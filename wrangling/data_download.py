# -*- coding: utf-8 -*-
"""Download_Data_OpenML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16BhejIhHOfFZPRzKeSMxadALa67WPnO8
"""

# Enter API key, server address, etc.
# You can obtain your API key by logging into OpenML and accessing "Your Profile."
import os
from tqdm import tqdm
import pandas as pd
import numpy as np

from openml import datasets, tasks, runs, flows, config, study
from openml.datasets import edit_dataset, fork_dataset, get_dataset
from openml.tasks import TaskType

config.apikey = '1d638eca3385a3acd1fb477970ade481'
config.server = 'https://www.openml.org/api/v1'
config.cache_directory = os.path.expanduser('~/.openml/cache ')
config.cachedir = '~/.openml/cache'

benchmark_suite = study.get_suite(suite_id=269) ## OBJECT : OpenMLBenchmarkSuite
benchmark_suite

print(benchmark_suite.description)

for id in np.arange(len(benchmark_suite.tasks)):
    # Retrieve dataset information
    dataset_id = benchmark_suite.data[id]

    # Get the task ID
    task_id = benchmark_suite.tasks[id]

    print(f"Dataset ID: {dataset_id}, Task ID: {task_id}")

#match task id to benchmark suite
dataset_task_dict = {}
for id in np.arange(len(benchmark_suite.tasks)):
    dataset_id = benchmark_suite.data[id]
    task_id = benchmark_suite.tasks[id]
    dataset_task_dict[dataset_id] = task_id

print(f'Task Dict: {dataset_task_dict}')

print('Number of tasks:', len(benchmark_suite.tasks))

print('Task ids:', benchmark_suite.tasks)

print('Number of data:', len(benchmark_suite.data))

print('Data ids:', benchmark_suite.data)

dlist = datasets.list_datasets(data_id=benchmark_suite.data, output_format='dataframe')

dlist['tid'] = ''
for i in range(len(list(dataset_task_dict.keys()))):
    k =  list(dataset_task_dict.keys())[i]
    v = dataset_task_dict[k]

    dlist.loc[dlist['did'] == k, 'tid'] = v  #match dataset id and task


dlist.to_csv('/home/mburu/Master_Thesis/master-thesis-da/datasets/269_data_info.csv')

#task list
tlist = tasks.get_tasks(benchmark_suite.tasks)

#dataset location
save_path = '/home/mburu/Master_Thesis/master-thesis-da/datasets/'


# Get data from every suit
for did in tqdm(dlist['did']):

    # get dataset name from dlist
    name = dlist.loc[dlist['did'] == did, 'name'].values[0]

    # get dataset from dlist
    odata = datasets.get_dataset(did)
    X, y, categorical_indicator, attribute_names = odata.get_data(
                                                            target=odata.default_target_attribute,
                                                            include_row_id=True,
                                                            include_ignore_attribute=True,
    )

    # data split using tasks from tlist
    task = tasks.get_task(dlist[dlist['did']==did]['tid'].values[0])
   
    data = pd.concat([X, y], axis=1)

    #download only categorical data only
    if True in categorical_indicator: # We will only collect datasets that contain categorical variables.
        # create a folder path
        os.makedirs(f'{save_path}{did}-{name}', exist_ok=True)
        
        data.to_csv(f'{save_path}{did}-{name}/raw_data.csv', index=False)
    else:
        pass
        
