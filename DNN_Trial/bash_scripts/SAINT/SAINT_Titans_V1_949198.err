[I 2025-01-08 09:52:44,688] A new study created in RDB with name: SAINT_SAT11
[I 2025-01-08 10:47:49,298] Trial 0 finished with value: 1446674.0370579709 and parameters: {'dim': 128, 'depth': 2, 'heads': 2, 'dropout': 0.8}. Best is trial 0 with value: 1446674.0370579709.
[I 2025-01-08 11:40:21,288] A new study created in RDB with name: SAINT_Diamonds
[I 2025-01-08 12:45:01,891] Trial 0 finished with value: 474150.7644489361 and parameters: {'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.3}. Best is trial 0 with value: 474150.7644489361.
[I 2025-01-08 13:40:18,552] A new study created in RDB with name: SAINT_House_Prices_Nominal
[I 2025-01-08 14:07:33,130] Trial 0 finished with value: 750054869.2640469 and parameters: {'dim': 256, 'depth': 6, 'heads': 8, 'dropout': 0.3}. Best is trial 0 with value: 750054869.2640469.
[I 2025-01-08 14:31:15,948] A new study created in RDB with name: SAINT_Mercedes_Benz
[W 2025-01-08 14:31:32,755] Trial 0 failed with parameters: {'dim': 64, 'depth': 3, 'heads': 2, 'dropout': 0.2} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 112.44 MiB is free. Including non-PyTorch memory, this process has 7.67 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 138.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 46, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/saint.py", line 132, in fit
    optimizer.step()
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 112.44 MiB is free. Including non-PyTorch memory, this process has 7.67 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 138.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[W 2025-01-08 14:31:32,757] Trial 0 failed with value None.
Traceback (most recent call last):
  File "train.py", line 185, in <module>
    main(arguments)
  File "train.py", line 156, in main
    study.optimize(Objective(args, model_name, X, y), n_trials=args.n_trials)
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/study.py", line 475, in optimize
    _optimize(
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 63, in _optimize
    _optimize_sequential(
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 248, in _run_trial
    raise func_err
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 134, in __call__
    sc, time = cross_validation(model, self.X, self.y, self.args, visual=False)
  File "train.py", line 46, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/saint.py", line 132, in fit
    optimizer.step()
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/mburu/.local/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 112.44 MiB is free. Including non-PyTorch memory, this process has 7.67 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 138.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: stud-000: task 0: Exited with exit code 1
[I 2025-01-08 14:31:50,968] A new study created in RDB with name: SAINT_Allstate_Claims
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=949198.4. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: stud-000: task 0: Out Of Memory
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=949198.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
