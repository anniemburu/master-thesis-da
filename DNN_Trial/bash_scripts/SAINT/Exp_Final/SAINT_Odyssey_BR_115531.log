

----------------------------------------------------------------------------
Training SAINT Vesion 1 with Dataset: config/abalone.yml 



----------------------------------------------------------------------------
Namespace(batch_size=64, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/abalone.yml', data_parallel=False, dataset='Abalone', direction='maximize', dropna_idx=None, early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='SAINT', n_trials=2, nominal_idx=[0], num_bins=10, num_classes=1, num_features=8, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=128, y_distribution='normal')
Start hyperparameter optimization
Loading dataset Abalone...
Dataset loaded! 

(4177, 8)
A new study created in RDB with name: SAINT_Abalone
In get_device
Using dim 128 and batch size 64
Fold 1
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793
  -1.1403358713163287 -0.6396371751892543 -0.6047860626063131
  -0.7242292401724562 -0.6378172121693153]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107 -1.268891079376386
  -1.2313829452130387 -1.1708398800472457 -1.207928885291349
  -1.2181336383713948]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793
  -0.3690046229559858 -0.6355349514802506 -0.6455419374620602
  -0.6044559947144446 -0.6015474355316853]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562 -1.5260014954965002
  -1.273430738230327 -1.2161241854425204 -1.2908488244545877
  -1.3269429682842848]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313 -1.1403358713163287
  -0.9729428515458055 -0.9829100126568561 -0.9407424146542464
  -0.8554358719950952]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843
  0.27377141734429983 -0.09916920152801632 -0.5481806808622199
  -0.3510895139378818 0.6678947467853636]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982 -0.3690046229559858
  -0.11865476414578383 -0.2900601401091547 -0.27738290134833615
  0.16011787385854406]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722 -0.3690046229559858
  -0.6488671785345128 -0.6410135069225328 -0.6182759845749843
  -0.5290078822564254]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084
  0.016661001224186022 -0.44990932864783295 -0.7429031940619006
  -0.2958095544957226 -0.20257989251775574]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084
  -0.11189420683587124 -0.5832315991904535 -0.6364850763830053
  -0.7795091996146154 -0.3476589990682755]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.7993472814559937
Epoch 1 loss 1.7366869449615479
Epoch 2 loss 1.6873561143875122
Epoch 3 loss 1.651672124862671
Epoch 4 loss 1.689892053604126
Epoch 5 loss 1.6457750797271729
Epoch 6 loss 1.6415717601776123
Epoch 7 loss 1.7077878713607788
Epoch 8 loss 1.6099549531936646
Epoch 9 loss 1.6209872961044312
Epoch 10 loss 1.6162608861923218
Epoch 11 loss 1.603166937828064
Epoch 12 loss 1.5945497751235962
Epoch 13 loss 1.6044151782989502
Epoch 14 loss 1.6027756929397583
Epoch 15 loss 1.6122907400131226
Epoch 16 loss 1.6921827793121338
Epoch 17 loss 1.6334091424942017
Epoch 18 loss 1.6119663715362549
Epoch 19 loss 1.615064263343811
Epoch 20 loss 1.616004228591919
Epoch 21 loss 1.589878797531128
Epoch 22 loss 1.6081337928771973
Epoch 23 loss 1.6019161939620972
Epoch 24 loss 1.6357601881027222
Epoch 25 loss 1.5987354516983032
Epoch 26 loss 1.630094051361084
Epoch 27 loss 1.6024633646011353
Epoch 28 loss 1.623689889907837
Epoch 29 loss 1.6411734819412231
Epoch 30 loss 1.6174017190933228
Epoch 31 loss 1.6109682321548462
Epoch 32 loss 1.5951342582702637
Epoch 33 loss 1.5967671871185303
Epoch 34 loss 1.607061505317688
Epoch 35 loss 1.610734462738037
Epoch 36 loss 1.6381490230560303
Epoch 37 loss 1.628365159034729
Epoch 38 loss 1.6270835399627686
Epoch 39 loss 1.5991557836532593
Epoch 40 loss 1.6212245225906372
Epoch 41 loss 1.6462610960006714
Epoch 42 loss 1.622530460357666
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (836,)
Probabilities shape : (836, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5925, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621
  -0.10725663378676703 -0.30947434175323335 -0.463709340293154
  -0.35856347602228766 -0.20891180319651964]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044
  -0.34231364492588795 -0.6402458656513711 -0.6493256004118436
  -0.6110682244346455 -0.6037444215028267]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755 -1.400070195051931
  -1.2791896167837988 -1.2197560583375726 -1.2951265428608514
  -1.3216219093324761]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927 -1.04748467834325
  -0.9782080748516424 -0.9866039267250722 -0.9462108905092297
  -0.855001542243204]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727
  0.24532888292191368 -0.1029989495062593 -0.5519902444959455
  -0.35856347602228766 0.6525411821990598]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692
  -0.34231364492588795 -0.12251652389776423 -0.29393837067240147
  -0.28510754921141984 0.15002694071830516]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856
  -0.34231364492588795 -0.6535999954981905 -0.6447983745552902
  -0.6248412107116832 -0.5319566727198618]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214 0.24532888292191368
  0.13737538773648675 -0.20113024061305662 -0.27133456293438213
  0.5807534334160948]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449 -0.694899161634569
  -0.8662388322898503 -0.866632441526407 -0.914073922529475
  -0.7473199190687565]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597
  -0.10725663378676703 -0.587856587021542 -0.6402711486987368
  -0.7855260506104564 -0.35248730076244944]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.702373743057251
Epoch 1 loss 1.6437426805496216
Epoch 2 loss 1.581969976425171
Epoch 3 loss 1.5493206977844238
Epoch 4 loss 1.5623482465744019
Epoch 5 loss 1.6174347400665283
Epoch 6 loss 1.5710800886154175
Epoch 7 loss 1.5656315088272095
Epoch 8 loss 1.5416761636734009
Epoch 9 loss 1.554831624031067
Epoch 10 loss 1.5597175359725952
Epoch 11 loss 1.5230079889297485
Epoch 12 loss 1.585754632949829
Epoch 13 loss 1.5542174577713013
Epoch 14 loss 1.53470778465271
Epoch 15 loss 1.5791226625442505
Epoch 16 loss 1.6073272228240967
Epoch 17 loss 1.5708718299865723
Epoch 18 loss 1.5493546724319458
Epoch 19 loss 1.5598167181015015
Epoch 20 loss 1.5527803897857666
Epoch 21 loss 1.5391042232513428
Epoch 22 loss 1.5814541578292847
Epoch 23 loss 1.5781385898590088
Epoch 24 loss 1.5537161827087402
Epoch 25 loss 1.5362231731414795
Epoch 26 loss 1.5311511754989624
Epoch 27 loss 1.5503464937210083
Epoch 28 loss 1.5692248344421387
Epoch 29 loss 1.519195795059204
Epoch 30 loss 1.5664774179458618
Epoch 31 loss 1.5537062883377075
Epoch 32 loss 1.5258228778839111
Epoch 33 loss 1.5443490743637085
Epoch 34 loss 1.540568470954895
Epoch 35 loss 1.5419234037399292
Epoch 36 loss 1.5643515586853027
Epoch 37 loss 1.542754888534546
Epoch 38 loss 1.5267767906188965
Epoch 39 loss 1.5654234886169434
Epoch 40 loss 1.5497139692306519
Epoch 41 loss 1.5423604249954224
Epoch 42 loss 1.5459940433502197
Epoch 43 loss 1.568394422531128
Epoch 44 loss 1.5623887777328491
Epoch 45 loss 1.5405919551849365
Epoch 46 loss 1.5826566219329834
Epoch 47 loss 1.526457667350769
Epoch 48 loss 1.5772737264633179
Epoch 49 loss 1.5738322734832764
Epoch 50 loss 1.5528441667556763
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (836,)
Probabilities shape : (836, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5645, 'Log Loss - std': 0.028000000000000025} 
 

Fold 3
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722 -1.0389280503395208
  -0.6347153561730201 -0.6005026730450418 -0.7223571148557317
  -0.6306854501095174]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578 -1.156125699169423
  -1.2182459667920271 -1.160980331721523 -1.1969622573764345
  -1.2017803823001665]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994
  -0.10134685970030412 -0.3050256177643609 -0.4570203924238625
  -0.3562331477683324 -0.2023642509665306]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722 -0.3357421573601085
  -0.6306700833091101 -0.6408570644697484 -0.6048358414696529
  -0.5949920168476018]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061 0.25024608678940174
  -0.10175065635288713 -0.5444549071773936 -0.3562331477683324
  0.6542781473194431]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284
  -0.3357421573601085 -0.6438172201168175 -0.6363732432003366
  -0.6183959883988158 -0.5236051503237706]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173 0.25024608678940174
  0.13489780618584366 -0.19695875879797511 -0.27035221721696706
  0.5828912807956119]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416
  0.015850789129598055 -0.44762148621718584 -0.7372592217621032
  -0.3019925600516806 -0.2023642509665306]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302 -0.6873351038498147
  -0.8531600908241561 -0.8560804854015173 -0.9031590739112376
  -0.737765749895264]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416
  -0.10134685970030412 -0.5790928542942586 -0.6318894219309248
  -0.7765977025723835 -0.3451379840141928]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.795082688331604
Epoch 1 loss 1.7418080568313599
Epoch 2 loss 1.7449842691421509
Epoch 3 loss 1.629133939743042
Epoch 4 loss 1.667019009590149
Epoch 5 loss 1.6243510246276855
Epoch 6 loss 1.6071569919586182
Epoch 7 loss 1.6100660562515259
Epoch 8 loss 1.6266580820083618
Epoch 9 loss 1.590615153312683
Epoch 10 loss 1.592515468597412
Epoch 11 loss 1.6045067310333252
Epoch 12 loss 1.603466272354126
Epoch 13 loss 1.6095097064971924
Epoch 14 loss 1.5829930305480957
Epoch 15 loss 1.5929509401321411
Epoch 16 loss 1.5723503828048706
Epoch 17 loss 1.5904258489608765
Epoch 18 loss 1.6351159811019897
Epoch 19 loss 1.6303654909133911
Epoch 20 loss 1.6015105247497559
Epoch 21 loss 1.586574912071228
Epoch 22 loss 1.6099355220794678
Epoch 23 loss 1.6429879665374756
Epoch 24 loss 1.555925965309143
Epoch 25 loss 1.617911696434021
Epoch 26 loss 1.5858269929885864
Epoch 27 loss 1.6088529825210571
Epoch 28 loss 1.587365746498108
Epoch 29 loss 1.590701937675476
Epoch 30 loss 1.610547661781311
Epoch 31 loss 1.623989224433899
Epoch 32 loss 1.56459379196167
Epoch 33 loss 1.5847976207733154
Epoch 34 loss 1.5695655345916748
Epoch 35 loss 1.5919286012649536
Epoch 36 loss 1.5788849592208862
Epoch 37 loss 1.5858296155929565
Epoch 38 loss 1.5710670948028564
Epoch 39 loss 1.5713191032409668
Epoch 40 loss 1.600736141204834
Epoch 41 loss 1.5954298973083496
Epoch 42 loss 1.5850069522857666
Epoch 43 loss 1.581687331199646
Epoch 44 loss 1.5892951488494873
Epoch 45 loss 1.5858161449432373
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5718666666666667, 'Log Loss - std': 0.025123738221495287} 
 

Fold 4
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443
  -1.0500768981385924 -0.6384214848190454 -0.6042936766305018
  -0.7195875197860123 -0.6349568100929689]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453 -1.167955269492567
  -1.225152073368887 -1.1667739623949986 -1.1982585500483505
  -1.2058724559266103]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614
  -0.10704992730679613 -0.30692378660890945 -0.4602987234747907
  -0.35032701072649475 -0.20677007571773787]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054 -1.403712012200516
  -1.266843501671389 -1.2117723852561584 -1.2803164409504653
  -1.312919139520418]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352 -1.0500768981385924
  -0.9689023189242423 -0.9800305075211857 -0.9338497904748684
  -0.8490501772805844]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604 0.24658518675512706
  -0.10253410151615713 -0.5480456480540523 -0.35032701072649475
  0.6496033930327244]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624
  -0.34280667001474535 -0.1218545195099993 -0.2915546377454418
  -0.277386663257948 0.1500522029282881]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656 0.24658518675512706
  0.13541209903958457 -0.1993078708800643 -0.2637103481075954
  0.5782389373035192]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541
  0.010828444047178483 -0.45030162540531793 -0.741538866357039
  -0.2956217501250846 -0.20677007571773787]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347 -0.6964417840766689
  -0.8580641314858839 -0.8607846869391123 -0.9019383884573792
  -0.7420034936867765]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.7806086540222168
Epoch 1 loss 1.6859612464904785
Epoch 2 loss 1.6634981632232666
Epoch 3 loss 1.6074122190475464
Epoch 4 loss 1.6643465757369995
Epoch 5 loss 1.6790673732757568
Epoch 6 loss 1.641071081161499
Epoch 7 loss 1.6327731609344482
Epoch 8 loss 1.630885362625122
Epoch 9 loss 1.5927263498306274
Epoch 10 loss 1.6762813329696655
Epoch 11 loss 1.6023730039596558
Epoch 12 loss 1.6309624910354614
Epoch 13 loss 1.6332945823669434
Epoch 14 loss 1.6158709526062012
Epoch 15 loss 1.6610502004623413
Epoch 16 loss 1.6271599531173706
Epoch 17 loss 1.5924632549285889
Epoch 18 loss 1.637932300567627
Epoch 19 loss 1.6177773475646973
Epoch 20 loss 1.6436303853988647
Epoch 21 loss 1.644934892654419
Epoch 22 loss 1.6254843473434448
Epoch 23 loss 1.6363033056259155
Epoch 24 loss 1.5950437784194946
Epoch 25 loss 1.6295329332351685
Epoch 26 loss 1.6108745336532593
Epoch 27 loss 1.6252936124801636
Epoch 28 loss 1.6536223888397217
Epoch 29 loss 1.6188371181488037
Epoch 30 loss 1.6000138521194458
Epoch 31 loss 1.6317241191864014
Epoch 32 loss 1.607784628868103
Epoch 33 loss 1.607448935508728
Epoch 34 loss 1.6200717687606812
Epoch 35 loss 1.6374316215515137
Epoch 36 loss 1.6296143531799316
Epoch 37 loss 1.5995029211044312
Epoch 38 loss 1.6406830549240112
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5805, 'Log Loss - std': 0.026400852258970757} 
 

Fold 5
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965 -1.0550394761935622
  -0.652531118203336 -0.6203614871572382 -0.7346462314941438
  -0.6481967718014128]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023 -1.172782811845699
  -1.2399035204261921 -1.1817207861619912 -1.2109175474628806
  -1.2256252900914224]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176
  -0.11309279097646885 -0.32067080082959054 -0.4766535066120213
  -0.3672369306039755 -0.21512538308390547]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965 -0.3485794622807424
  -0.6484592124686889 -0.6607793566855803 -0.6167123818256947
  -0.6121074894082871]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513 -1.4082694831499722
  -1.2816405542063258 -1.2266295300823715 -1.292564058771807
  -1.3338931372707994]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155 -1.0550394761935622
  -0.9833734591434197 -0.9953494988924132 -0.9478343443563404
  -0.8647324661601664]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708 -0.3485794622807424
  -0.13539908990314378 -0.30824571691059555 -0.29466225388492984
  0.14576744084735074]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115
  -0.3485794622807424 -0.6616929061062923 -0.6562884822935423
  -0.6303201337105158 -0.5399289246220358]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313 0.24013721597994078
  0.12214894781329044 -0.21618279187381598 -0.28105450200010873
  0.578838829564858]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044
  0.0046505446756679135 -0.4642054779759037 -0.7573331561143978
  -0.31280592306469124 -0.21512538308390547]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.6739799976348877
Epoch 1 loss 1.6033273935317993
Epoch 2 loss 1.5498780012130737
Epoch 3 loss 1.5520936250686646
Epoch 4 loss 1.5705169439315796
Epoch 5 loss 1.60055673122406
Epoch 6 loss 1.547397255897522
Epoch 7 loss 1.5687848329544067
Epoch 8 loss 1.5524992942810059
Epoch 9 loss 1.5582880973815918
Epoch 10 loss 1.5406773090362549
Epoch 11 loss 1.5464822053909302
Epoch 12 loss 1.581446886062622
Epoch 13 loss 1.5086125135421753
Epoch 14 loss 1.4962271451950073
Epoch 15 loss 1.5151563882827759
Epoch 16 loss 1.498097538948059
Epoch 17 loss 1.5297452211380005
Epoch 18 loss 1.5241401195526123
Epoch 19 loss 1.5206092596054077
Epoch 20 loss 1.5137460231781006
Epoch 21 loss 1.5054163932800293
Epoch 22 loss 1.5412631034851074
Epoch 23 loss 1.5117398500442505
Epoch 24 loss 1.5291465520858765
Epoch 25 loss 1.510013461112976
Epoch 26 loss 1.513466238975525
Epoch 27 loss 1.5286787748336792
Epoch 28 loss 1.483933925628662
Epoch 29 loss 1.499586582183838
Epoch 30 loss 1.5047667026519775
Epoch 31 loss 1.4899944067001343
Epoch 32 loss 1.5141026973724365
Epoch 33 loss 1.5086686611175537
Epoch 34 loss 1.519088625907898
Epoch 35 loss 1.5051329135894775
Epoch 36 loss 1.5014806985855103
Epoch 37 loss 1.5035215616226196
Epoch 38 loss 1.475354552268982
Epoch 39 loss 1.5217756032943726
Epoch 40 loss 1.5002081394195557
Epoch 41 loss 1.5033222436904907
Epoch 42 loss 1.50156569480896
Epoch 43 loss 1.4876723289489746
Epoch 44 loss 1.4948331117630005
Epoch 45 loss 1.4877361059188843
Epoch 46 loss 1.507470965385437
Epoch 47 loss 1.509555697441101
Epoch 48 loss 1.50704026222229
Epoch 49 loss 1.5225083827972412
Epoch 50 loss 1.5054905414581299
Epoch 51 loss 1.512871265411377
Epoch 52 loss 1.4907476902008057
Epoch 53 loss 1.4995779991149902
Epoch 54 loss 1.5041323900222778
Epoch 55 loss 1.5006569623947144
Epoch 56 loss 1.526243805885315
Epoch 57 loss 1.4894835948944092
Epoch 58 loss 1.5361629724502563
Epoch 59 loss 1.4974538087844849
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (835,)
Probabilities shape : (835, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5635, 'Log Loss - std': 0.04139570025980959} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 0 finished with value: 1.5635 and parameters: {'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}. Best is trial 0 with value: 1.5635.
In get_device
Using dim 32 and batch size 64
Fold 1
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793
  -1.1403358713163287 -0.6396371751892543 -0.6047860626063131
  -0.7242292401724562 -0.6378172121693153]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107 -1.268891079376386
  -1.2313829452130387 -1.1708398800472457 -1.207928885291349
  -1.2181336383713948]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793
  -0.3690046229559858 -0.6355349514802506 -0.6455419374620602
  -0.6044559947144446 -0.6015474355316853]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562 -1.5260014954965002
  -1.273430738230327 -1.2161241854425204 -1.2908488244545877
  -1.3269429682842848]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313 -1.1403358713163287
  -0.9729428515458055 -0.9829100126568561 -0.9407424146542464
  -0.8554358719950952]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843
  0.27377141734429983 -0.09916920152801632 -0.5481806808622199
  -0.3510895139378818 0.6678947467853636]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982 -0.3690046229559858
  -0.11865476414578383 -0.2900601401091547 -0.27738290134833615
  0.16011787385854406]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722 -0.3690046229559858
  -0.6488671785345128 -0.6410135069225328 -0.6182759845749843
  -0.5290078822564254]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084
  0.016661001224186022 -0.44990932864783295 -0.7429031940619006
  -0.2958095544957226 -0.20257989251775574]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084
  -0.11189420683587124 -0.5832315991904535 -0.6364850763830053
  -0.7795091996146154 -0.3476589990682755]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 32 and batch size 64
{'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}
Epoch 0 loss 1.8999249935150146
Epoch 1 loss 1.7916260957717896
Epoch 2 loss 1.7403590679168701
Epoch 3 loss 1.7006171941757202
Epoch 4 loss 1.6631693840026855
Epoch 5 loss 1.6520315408706665
Epoch 6 loss 1.6367506980895996
Epoch 7 loss 1.6207252740859985
Epoch 8 loss 1.5989415645599365
Epoch 9 loss 1.5973023176193237
Epoch 10 loss 1.608539342880249
Epoch 11 loss 1.6054586172103882
Epoch 12 loss 1.61561918258667
Epoch 13 loss 1.5876548290252686
Epoch 14 loss 1.5819436311721802
Epoch 15 loss 1.5812451839447021
Epoch 16 loss 1.5962365865707397
Epoch 17 loss 1.5694926977157593
Epoch 18 loss 1.595750331878662
Epoch 19 loss 1.5793648958206177
Epoch 20 loss 1.5841381549835205
Epoch 21 loss 1.5676782131195068
Epoch 22 loss 1.586517095565796
Epoch 23 loss 1.593392252922058
Epoch 24 loss 1.5859845876693726
Epoch 25 loss 1.5788357257843018
Epoch 26 loss 1.585498332977295
Epoch 27 loss 1.586018443107605
Epoch 28 loss 1.6040083169937134
Epoch 29 loss 1.5739548206329346
Epoch 30 loss 1.5806758403778076
Epoch 31 loss 1.5836626291275024
Epoch 32 loss 1.5821373462677002
Epoch 33 loss 1.5841275453567505
Epoch 34 loss 1.5718587636947632
Epoch 35 loss 1.5726078748703003
Epoch 36 loss 1.5861963033676147
Epoch 37 loss 1.5762840509414673
Epoch 38 loss 1.5747050046920776
Epoch 39 loss 1.576759696006775
Epoch 40 loss 1.5802415609359741
Epoch 41 loss 1.5876659154891968
Epoch 42 loss 1.5968631505966187
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (836,)
Probabilities shape : (836, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5856, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621
  -0.10725663378676703 -0.30947434175323335 -0.463709340293154
  -0.35856347602228766 -0.20891180319651964]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044
  -0.34231364492588795 -0.6402458656513711 -0.6493256004118436
  -0.6110682244346455 -0.6037444215028267]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755 -1.400070195051931
  -1.2791896167837988 -1.2197560583375726 -1.2951265428608514
  -1.3216219093324761]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927 -1.04748467834325
  -0.9782080748516424 -0.9866039267250722 -0.9462108905092297
  -0.855001542243204]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727
  0.24532888292191368 -0.1029989495062593 -0.5519902444959455
  -0.35856347602228766 0.6525411821990598]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692
  -0.34231364492588795 -0.12251652389776423 -0.29393837067240147
  -0.28510754921141984 0.15002694071830516]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856
  -0.34231364492588795 -0.6535999954981905 -0.6447983745552902
  -0.6248412107116832 -0.5319566727198618]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214 0.24532888292191368
  0.13737538773648675 -0.20113024061305662 -0.27133456293438213
  0.5807534334160948]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449 -0.694899161634569
  -0.8662388322898503 -0.866632441526407 -0.914073922529475
  -0.7473199190687565]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597
  -0.10725663378676703 -0.587856587021542 -0.6402711486987368
  -0.7855260506104564 -0.35248730076244944]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 32 and batch size 64
{'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}
Epoch 0 loss 1.7564115524291992
Epoch 1 loss 1.703228235244751
Epoch 2 loss 1.651212453842163
Epoch 3 loss 1.6082249879837036
Epoch 4 loss 1.5726507902145386
Epoch 5 loss 1.5630594491958618
Epoch 6 loss 1.5347450971603394
Epoch 7 loss 1.5312533378601074
Epoch 8 loss 1.527122974395752
Epoch 9 loss 1.5234787464141846
Epoch 10 loss 1.537628412246704
Epoch 11 loss 1.5181286334991455
Epoch 12 loss 1.506408452987671
Epoch 13 loss 1.5149149894714355
Epoch 14 loss 1.5149881839752197
Epoch 15 loss 1.5120822191238403
Epoch 16 loss 1.4984699487686157
Epoch 17 loss 1.4900778532028198
Epoch 18 loss 1.5071842670440674
Epoch 19 loss 1.5203016996383667
Epoch 20 loss 1.5261067152023315
Epoch 21 loss 1.4961822032928467
Epoch 22 loss 1.5008676052093506
Epoch 23 loss 1.5014530420303345
Epoch 24 loss 1.5078812837600708
Epoch 25 loss 1.4922388792037964
Epoch 26 loss 1.4919384717941284
Epoch 27 loss 1.4995967149734497
Epoch 28 loss 1.4778000116348267
Epoch 29 loss 1.5017951726913452
Epoch 30 loss 1.5010807514190674
Epoch 31 loss 1.5113171339035034
Epoch 32 loss 1.5118907690048218
Epoch 33 loss 1.4992039203643799
Epoch 34 loss 1.528014898300171
Epoch 35 loss 1.5106943845748901
Epoch 36 loss 1.5048242807388306
Epoch 37 loss 1.4916694164276123
Epoch 38 loss 1.5022143125534058
Epoch 39 loss 1.5031877756118774
Epoch 40 loss 1.5017808675765991
Epoch 41 loss 1.49411940574646
Epoch 42 loss 1.487059473991394
Epoch 43 loss 1.503752589225769
Epoch 44 loss 1.5317308902740479
Epoch 45 loss 1.4944301843643188
Epoch 46 loss 1.5162514448165894
Epoch 47 loss 1.5054755210876465
Epoch 48 loss 1.5049738883972168
Epoch 49 loss 1.5209076404571533
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (836,)
Probabilities shape : (836, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5413000000000001, 'Log Loss - std': 0.04429999999999989} 
 

Fold 3
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722 -1.0389280503395208
  -0.6347153561730201 -0.6005026730450418 -0.7223571148557317
  -0.6306854501095174]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578 -1.156125699169423
  -1.2182459667920271 -1.160980331721523 -1.1969622573764345
  -1.2017803823001665]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994
  -0.10134685970030412 -0.3050256177643609 -0.4570203924238625
  -0.3562331477683324 -0.2023642509665306]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722 -0.3357421573601085
  -0.6306700833091101 -0.6408570644697484 -0.6048358414696529
  -0.5949920168476018]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061 0.25024608678940174
  -0.10175065635288713 -0.5444549071773936 -0.3562331477683324
  0.6542781473194431]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284
  -0.3357421573601085 -0.6438172201168175 -0.6363732432003366
  -0.6183959883988158 -0.5236051503237706]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173 0.25024608678940174
  0.13489780618584366 -0.19695875879797511 -0.27035221721696706
  0.5828912807956119]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416
  0.015850789129598055 -0.44762148621718584 -0.7372592217621032
  -0.3019925600516806 -0.2023642509665306]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302 -0.6873351038498147
  -0.8531600908241561 -0.8560804854015173 -0.9031590739112376
  -0.737765749895264]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416
  -0.10134685970030412 -0.5790928542942586 -0.6318894219309248
  -0.7765977025723835 -0.3451379840141928]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 32 and batch size 64
{'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}
Epoch 0 loss 1.9492744207382202
Epoch 1 loss 1.7958133220672607
Epoch 2 loss 1.7378618717193604
Epoch 3 loss 1.6871198415756226
Epoch 4 loss 1.6552608013153076
Epoch 5 loss 1.620604395866394
Epoch 6 loss 1.6789098978042603
Epoch 7 loss 1.6335026025772095
Epoch 8 loss 1.5966742038726807
Epoch 9 loss 1.588253378868103
Epoch 10 loss 1.5997623205184937
Epoch 11 loss 1.5932596921920776
Epoch 12 loss 1.5902115106582642
Epoch 13 loss 1.585951566696167
Epoch 14 loss 1.5838449001312256
Epoch 15 loss 1.591671109199524
Epoch 16 loss 1.5790952444076538
Epoch 17 loss 1.5761237144470215
Epoch 18 loss 1.559195637702942
Epoch 19 loss 1.5775940418243408
Epoch 20 loss 1.561690092086792
Epoch 21 loss 1.5798054933547974
Epoch 22 loss 1.5641605854034424
Epoch 23 loss 1.568291425704956
Epoch 24 loss 1.570726990699768
Epoch 25 loss 1.5710619688034058
Epoch 26 loss 1.5721005201339722
Epoch 27 loss 1.5693668127059937
Epoch 28 loss 1.5698599815368652
Epoch 29 loss 1.5719294548034668
Epoch 30 loss 1.5565216541290283
Epoch 31 loss 1.5694535970687866
Epoch 32 loss 1.5747238397598267
Epoch 33 loss 1.5768519639968872
Epoch 34 loss 1.5607599020004272
Epoch 35 loss 1.5489474534988403
Epoch 36 loss 1.5631548166275024
Epoch 37 loss 1.55180025100708
Epoch 38 loss 1.565565824508667
Epoch 39 loss 1.563448190689087
Epoch 40 loss 1.5611833333969116
Epoch 41 loss 1.5463652610778809
Epoch 42 loss 1.5537564754486084
Epoch 43 loss 1.5537846088409424
Epoch 44 loss 1.5671932697296143
Epoch 45 loss 1.5605725049972534
Epoch 46 loss 1.5579499006271362
Epoch 47 loss 1.5627293586730957
Epoch 48 loss 1.5507969856262207
Epoch 49 loss 1.5640147924423218
Epoch 50 loss 1.559043049812317
Epoch 51 loss 1.5683246850967407
Epoch 52 loss 1.5506303310394287
Epoch 53 loss 1.5692849159240723
Epoch 54 loss 1.5661832094192505
Epoch 55 loss 1.5639313459396362
Epoch 56 loss 1.5729345083236694
Epoch 57 loss 1.5614055395126343
Epoch 58 loss 1.581838846206665
Epoch 59 loss 1.5487139225006104
Epoch 60 loss 1.562303066253662
Epoch 61 loss 1.5641429424285889
Epoch 62 loss 1.5436371564865112
Epoch 63 loss 1.5454429388046265
Epoch 64 loss 1.5622106790542603
Epoch 65 loss 1.5538837909698486
Epoch 66 loss 1.5569335222244263
Epoch 67 loss 1.5606085062026978
Epoch 68 loss 1.5583142042160034
Epoch 69 loss 1.5584787130355835
Epoch 70 loss 1.562754511833191
Epoch 71 loss 1.5594313144683838
Epoch 72 loss 1.5674837827682495
Epoch 73 loss 1.55666184425354
Epoch 74 loss 1.5678402185440063
Epoch 75 loss 1.5584161281585693
Epoch 76 loss 1.5460193157196045
Epoch 77 loss 1.5451375246047974
Epoch 78 loss 1.5567193031311035
Epoch 79 loss 1.5401772260665894
Epoch 80 loss 1.5551506280899048
Epoch 81 loss 1.5664962530136108
Epoch 82 loss 1.5624390840530396
Epoch 83 loss 1.555220365524292
Epoch 84 loss 1.559441328048706
Epoch 85 loss 1.5529673099517822
Epoch 86 loss 1.5527321100234985
Epoch 87 loss 1.559020757675171
Epoch 88 loss 1.557065486907959
Epoch 89 loss 1.5452446937561035
Epoch 90 loss 1.5583298206329346
Epoch 91 loss 1.5662741661071777
Epoch 92 loss 1.560232400894165
Epoch 93 loss 1.572288990020752
Epoch 94 loss 1.5539029836654663
Epoch 95 loss 1.5589594841003418
Epoch 96 loss 1.5617377758026123
Epoch 97 loss 1.557706356048584
Epoch 98 loss 1.5517699718475342
Epoch 99 loss 1.5690176486968994
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5487666666666666, 'Log Loss - std': 0.03768061688572632} 
 

Fold 4
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443
  -1.0500768981385924 -0.6384214848190454 -0.6042936766305018
  -0.7195875197860123 -0.6349568100929689]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453 -1.167955269492567
  -1.225152073368887 -1.1667739623949986 -1.1982585500483505
  -1.2058724559266103]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614
  -0.10704992730679613 -0.30692378660890945 -0.4602987234747907
  -0.35032701072649475 -0.20677007571773787]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054 -1.403712012200516
  -1.266843501671389 -1.2117723852561584 -1.2803164409504653
  -1.312919139520418]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352 -1.0500768981385924
  -0.9689023189242423 -0.9800305075211857 -0.9338497904748684
  -0.8490501772805844]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604 0.24658518675512706
  -0.10253410151615713 -0.5480456480540523 -0.35032701072649475
  0.6496033930327244]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624
  -0.34280667001474535 -0.1218545195099993 -0.2915546377454418
  -0.277386663257948 0.1500522029282881]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656 0.24658518675512706
  0.13541209903958457 -0.1993078708800643 -0.2637103481075954
  0.5782389373035192]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541
  0.010828444047178483 -0.45030162540531793 -0.741538866357039
  -0.2956217501250846 -0.20677007571773787]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347 -0.6964417840766689
  -0.8580641314858839 -0.8607846869391123 -0.9019383884573792
  -0.7420034936867765]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 32 and batch size 64
{'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}
Epoch 0 loss 1.8845727443695068
Epoch 1 loss 1.7673331499099731
Epoch 2 loss 1.7175401449203491
Epoch 3 loss 1.684354305267334
Epoch 4 loss 1.6403614282608032
Epoch 5 loss 1.616386890411377
Epoch 6 loss 1.6071165800094604
Epoch 7 loss 1.6001226902008057
Epoch 8 loss 1.596703052520752
Epoch 9 loss 1.6286342144012451
Epoch 10 loss 1.5795339345932007
Epoch 11 loss 1.6127129793167114
Epoch 12 loss 1.5816428661346436
Epoch 13 loss 1.5981287956237793
Epoch 14 loss 1.5786913633346558
Epoch 15 loss 1.5808675289154053
Epoch 16 loss 1.611141562461853
Epoch 17 loss 1.5878132581710815
Epoch 18 loss 1.5877355337142944
Epoch 19 loss 1.583788275718689
Epoch 20 loss 1.5848380327224731
Epoch 21 loss 1.5841091871261597
Epoch 22 loss 1.5846790075302124
Epoch 23 loss 1.5889023542404175
Epoch 24 loss 1.5763176679611206
Epoch 25 loss 1.5956681966781616
Epoch 26 loss 1.5863306522369385
Epoch 27 loss 1.5805336236953735
Epoch 28 loss 1.5881524085998535
Epoch 29 loss 1.5797096490859985
Epoch 30 loss 1.591295838356018
Epoch 31 loss 1.5826321840286255
Epoch 32 loss 1.5820353031158447
Epoch 33 loss 1.594361662864685
Epoch 34 loss 1.607262134552002
Epoch 35 loss 1.5823829174041748
Epoch 36 loss 1.595913052558899
Epoch 37 loss 1.5829532146453857
Epoch 38 loss 1.6060469150543213
Epoch 39 loss 1.5899087190628052
Epoch 40 loss 1.612036943435669
Epoch 41 loss 1.5953108072280884
Epoch 42 loss 1.5806175470352173
Epoch 43 loss 1.5908169746398926
Epoch 44 loss 1.6088050603866577
Epoch 45 loss 1.5878058671951294
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.55955, 'Log Loss - std': 0.03759936834575811} 
 

Fold 5
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965 -1.0550394761935622
  -0.652531118203336 -0.6203614871572382 -0.7346462314941438
  -0.6481967718014128]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023 -1.172782811845699
  -1.2399035204261921 -1.1817207861619912 -1.2109175474628806
  -1.2256252900914224]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176
  -0.11309279097646885 -0.32067080082959054 -0.4766535066120213
  -0.3672369306039755 -0.21512538308390547]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965 -0.3485794622807424
  -0.6484592124686889 -0.6607793566855803 -0.6167123818256947
  -0.6121074894082871]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513 -1.4082694831499722
  -1.2816405542063258 -1.2266295300823715 -1.292564058771807
  -1.3338931372707994]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155 -1.0550394761935622
  -0.9833734591434197 -0.9953494988924132 -0.9478343443563404
  -0.8647324661601664]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708 -0.3485794622807424
  -0.13539908990314378 -0.30824571691059555 -0.29466225388492984
  0.14576744084735074]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115
  -0.3485794622807424 -0.6616929061062923 -0.6562884822935423
  -0.6303201337105158 -0.5399289246220358]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313 0.24013721597994078
  0.12214894781329044 -0.21618279187381598 -0.28105450200010873
  0.578838829564858]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044
  0.0046505446756679135 -0.4642054779759037 -0.7573331561143978
  -0.31280592306469124 -0.21512538308390547]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 32 and batch size 64
{'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}
Epoch 0 loss 1.7708760499954224
Epoch 1 loss 1.658065915107727
Epoch 2 loss 1.6080944538116455
Epoch 3 loss 1.6084181070327759
Epoch 4 loss 1.5292632579803467
Epoch 5 loss 1.5224074125289917
Epoch 6 loss 1.5286171436309814
Epoch 7 loss 1.5048643350601196
Epoch 8 loss 1.511925458908081
Epoch 9 loss 1.4954813718795776
Epoch 10 loss 1.4878449440002441
Epoch 11 loss 1.4953514337539673
Epoch 12 loss 1.5032929182052612
Epoch 13 loss 1.481081247329712
Epoch 14 loss 1.4902020692825317
Epoch 15 loss 1.490575909614563
Epoch 16 loss 1.4875770807266235
Epoch 17 loss 1.4906977415084839
Epoch 18 loss 1.4949374198913574
Epoch 19 loss 1.4825170040130615
Epoch 20 loss 1.5095030069351196
Epoch 21 loss 1.4804109334945679
Epoch 22 loss 1.4896355867385864
Epoch 23 loss 1.5064669847488403
Epoch 24 loss 1.463560700416565
Epoch 25 loss 1.474994421005249
Epoch 26 loss 1.5019291639328003
Epoch 27 loss 1.4766148328781128
Epoch 28 loss 1.4863296747207642
Epoch 29 loss 1.475313663482666
Epoch 30 loss 1.4876080751419067
Epoch 31 loss 1.4816557168960571
Epoch 32 loss 1.4918577671051025
Epoch 33 loss 1.4779303073883057
Epoch 34 loss 1.4770169258117676
Epoch 35 loss 1.4753024578094482
Epoch 36 loss 1.4721195697784424
Epoch 37 loss 1.4682718515396118
Epoch 38 loss 1.4713844060897827
Epoch 39 loss 1.4793442487716675
Epoch 40 loss 1.4671175479888916
Epoch 41 loss 1.4666495323181152
Epoch 42 loss 1.4673447608947754
Epoch 43 loss 1.4712506532669067
Epoch 44 loss 1.4823200702667236
Epoch 45 loss 1.463867425918579
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (835,)
Probabilities shape : (835, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.54572, 'Log Loss - std': 0.04354360573034806} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 1 finished with value: 1.54572 and parameters: {'dim': 32, 'depth': 3, 'heads': 4, 'dropout': 0.6}. Best is trial 0 with value: 1.5635.
Best parameters After Trials: {'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Parameters saved to YAML file!!!
In get_device
Using dim 128 and batch size 64
Fold 1
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5711722952584546 -0.42686139059459793
  -1.1403358713163287 -0.6396371751892543 -0.6047860626063131
  -0.7242292401724562 -0.6378172121693153]
 [0.0 0.0 1.0 -1.4481230668108207 -1.4372003130391107 -1.268891079376386
  -1.2313829452130387 -1.1708398800472457 -1.207928885291349
  -1.2181336383713948]
 [0.0 0.0 1.0 -0.6964509769087927 -0.42686139059459793
  -0.3690046229559858 -0.6355349514802506 -0.6455419374620602
  -0.6044559947144446 -0.6015474355316853]
 [0.0 1.0 0.0 -1.6151613090112709 -1.538234205283562 -1.5260014954965002
  -1.273430738230327 -1.2161241854425204 -1.2908488244545877
  -1.3269429682842848]
 [0.0 1.0 0.0 -0.8217296585591307 -1.0835816901835313 -1.1403358713163287
  -0.9729428515458055 -0.9829100126568561 -0.9407424146542464
  -0.8554358719950952]
 [1.0 0.0 0.0 0.055221112993235226 0.07830807062765843
  0.27377141734429983 -0.09916920152801632 -0.5481806808622199
  -0.3510895139378818 0.6678947467853636]
 [1.0 0.0 0.0 0.1804997946435733 0.17934196287210982 -0.3690046229559858
  -0.11865476414578383 -0.2900601401091547 -0.27738290134833615
  0.16011787385854406]
 [0.0 0.0 1.0 -0.4041340530580043 -0.3763444444723722 -0.3690046229559858
  -0.6488671785345128 -0.6410135069225328 -0.6182759845749843
  -0.5290078822564254]
 [1.0 0.0 0.0 0.013461552443122539 -0.27531055222792084
  0.016661001224186022 -0.44990932864783295 -0.7429031940619006
  -0.2958095544957226 -0.20257989251775574]
 [0.0 0.0 1.0 -0.27885537140766625 -0.27531055222792084
  -0.11189420683587124 -0.5832315991904535 -0.6364850763830053
  -0.7795091996146154 -0.3476589990682755]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.8095022439956665
Epoch 1 loss 1.7602365016937256
Epoch 2 loss 1.6870453357696533
Epoch 3 loss 1.6471611261367798
Epoch 4 loss 1.6654179096221924
Epoch 5 loss 1.6248373985290527
Epoch 6 loss 1.6292794942855835
Epoch 7 loss 1.646058440208435
Epoch 8 loss 1.6220157146453857
Epoch 9 loss 1.62046480178833
Epoch 10 loss 1.6516579389572144
Epoch 11 loss 1.6067606210708618
Epoch 12 loss 1.61307692527771
Epoch 13 loss 1.5746835470199585
Epoch 14 loss 1.6291801929473877
Epoch 15 loss 1.641906499862671
Epoch 16 loss 1.6248860359191895
Epoch 17 loss 1.6312435865402222
Epoch 18 loss 1.6063282489776611
Epoch 19 loss 1.6428275108337402
Epoch 20 loss 1.6206800937652588
Epoch 21 loss 1.6552045345306396
Epoch 22 loss 1.624051570892334
Epoch 23 loss 1.6115639209747314
Epoch 24 loss 1.6166508197784424
Epoch 25 loss 1.5805728435516357
Epoch 26 loss 1.6267558336257935
Epoch 27 loss 1.5935895442962646
Epoch 28 loss 1.6021673679351807
Epoch 29 loss 1.5878655910491943
Epoch 30 loss 1.5889137983322144
Epoch 31 loss 1.6064660549163818
Epoch 32 loss 1.5911401510238647
Epoch 33 loss 1.6129871606826782
Epoch 34 loss 1.5861942768096924
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/Abalone/logging/loss_0.txt
File name : output/SAINT/Abalone/logging/loss_0.txt . The file was saved
Log file exists at: output/SAINT/Abalone/logging/val_loss_0.txt
File name : output/SAINT/Abalone/logging/val_loss_0.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (836,)
Probabilities shape : (836, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5878, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3341, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597]] 
 
 
Val : (3341, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.051808811977212994 0.1209546838869621
  -0.10725663378676703 -0.30947434175323335 -0.463709340293154
  -0.35856347602228766 -0.20891180319651964]
 [0.0 0.0 1.0 -0.6975125768360658 -0.43337947093655044
  -0.34231364492588795 -0.6402458656513711 -0.6493256004118436
  -0.6110682244346455 -0.6037444215028267]
 [0.0 1.0 0.0 -1.613349829830073 -1.5420477805835755 -1.400070195051931
  -1.2791896167837988 -1.2197560583375726 -1.2951265428608514
  -1.3216219093324761]
 [0.0 1.0 0.0 -0.8223994749716125 -1.0885016539097927 -1.04748467834325
  -0.9782080748516424 -0.9866039267250722 -0.9462108905092297
  -0.855001542243204]
 [1.0 0.0 0.0 0.051808811977212994 0.07056066981209727
  0.24532888292191368 -0.1029989495062593 -0.5519902444959455
  -0.35856347602228766 0.6525411821990598]
 [1.0 0.0 0.0 0.17669571011275956 0.17134869796182692
  -0.34231364492588795 -0.12251652389776423 -0.29393837067240147
  -0.28510754921141984 0.15002694071830516]
 [0.0 0.0 1.0 -0.40610981451979106 -0.3829854568616856
  -0.34231364492588795 -0.6535999954981905 -0.6447983745552902
  -0.6248412107116832 -0.5319566727198618]
 [1.0 0.0 0.0 0.21832467615794174 0.3225307401864214 0.24532888292191368
  0.13737538773648675 -0.20113024061305662 -0.27133456293438213
  0.5807534334160948]
 [0.0 0.0 1.0 -0.7807705089264303 -0.5845615131611449 -0.694899161634569
  -0.8662388322898503 -0.866632441526407 -0.914073922529475
  -0.7473199190687565]
 [0.0 0.0 1.0 -0.2812229163842445 -0.28219742871195597
  -0.10725663378676703 -0.587856587021542 -0.6402711486987368
  -0.7855260506104564 -0.35248730076244944]] 
 
 
Val : (3341, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.6906037330627441
Epoch 1 loss 1.668341040611267
Epoch 2 loss 1.5892287492752075
Epoch 3 loss 1.5877405405044556
Epoch 4 loss 1.5599626302719116
Epoch 5 loss 1.5639523267745972
Epoch 6 loss 1.5653635263442993
Epoch 7 loss 1.5706737041473389
Epoch 8 loss 1.5276730060577393
Epoch 9 loss 1.5831248760223389
Epoch 10 loss 1.5356669425964355
Epoch 11 loss 1.5294135808944702
Epoch 12 loss 1.544016718864441
Epoch 13 loss 1.5482189655303955
Epoch 14 loss 1.5388779640197754
Epoch 15 loss 1.5297331809997559
Epoch 16 loss 1.540625810623169
Epoch 17 loss 1.5434554815292358
Epoch 18 loss 1.5347769260406494
Epoch 19 loss 1.551471471786499
Epoch 20 loss 1.5198513269424438
Epoch 21 loss 1.5364117622375488
Epoch 22 loss 1.5479066371917725
Epoch 23 loss 1.5996310710906982
Epoch 24 loss 1.5769015550613403
Epoch 25 loss 1.6132915019989014
Epoch 26 loss 1.5808976888656616
Epoch 27 loss 1.5232489109039307
Epoch 28 loss 1.5615142583847046
Epoch 29 loss 1.5194189548492432
Epoch 30 loss 1.5566102266311646
Epoch 31 loss 1.5667370557785034
Epoch 32 loss 1.5447651147842407
Epoch 33 loss 1.5241135358810425
Epoch 34 loss 1.5168564319610596
Epoch 35 loss 1.5374338626861572
Epoch 36 loss 1.5756851434707642
Epoch 37 loss 1.5259743928909302
Epoch 38 loss 1.5365736484527588
Epoch 39 loss 1.5607616901397705
Epoch 40 loss 1.5344587564468384
Epoch 41 loss 1.5187305212020874
Epoch 42 loss 1.5596240758895874
Epoch 43 loss 1.5398306846618652
Epoch 44 loss 1.533701777458191
Epoch 45 loss 1.5103437900543213
Epoch 46 loss 1.5667264461517334
Epoch 47 loss 1.5640780925750732
Epoch 48 loss 1.544635534286499
Epoch 49 loss 1.551298975944519
Epoch 50 loss 1.527185082435608
Epoch 51 loss 1.5267747640609741
Epoch 52 loss 1.5450046062469482
Epoch 53 loss 1.5557204484939575
Epoch 54 loss 1.5444400310516357
Epoch 55 loss 1.5551913976669312
Epoch 56 loss 1.5408962965011597
Epoch 57 loss 1.5471404790878296
Epoch 58 loss 1.5251363515853882
Epoch 59 loss 1.5334773063659668
Epoch 60 loss 1.5345124006271362
Epoch 61 loss 1.5501500368118286
Epoch 62 loss 1.5486775636672974
Epoch 63 loss 1.5487195253372192
Epoch 64 loss 1.5046395063400269
Epoch 65 loss 1.568406343460083
Epoch 66 loss 1.5308390855789185
Epoch 67 loss 1.5380195379257202
Epoch 68 loss 1.5451666116714478
Epoch 69 loss 1.5685324668884277
Epoch 70 loss 1.5367095470428467
Epoch 71 loss 1.5260509252548218
Epoch 72 loss 1.5340389013290405
Epoch 73 loss 1.533727765083313
Epoch 74 loss 1.524666428565979
Epoch 75 loss 1.5911495685577393
Epoch 76 loss 1.5515202283859253
Epoch 77 loss 1.565057396888733
Epoch 78 loss 1.551437497138977
Epoch 79 loss 1.5520166158676147
Epoch 80 loss 1.5477478504180908
Epoch 81 loss 1.5939252376556396
Epoch 82 loss 1.580944538116455
Epoch 83 loss 1.5662654638290405
Epoch 84 loss 1.5416895151138306
Epoch 85 loss 1.5651112794876099
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/Abalone/logging/loss_1.txt
File name : output/SAINT/Abalone/logging/loss_1.txt . The file was saved
Log file exists at: output/SAINT/Abalone/logging/val_loss_1.txt
File name : output/SAINT/Abalone/logging/val_loss_1.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (836,)
Probabilities shape : (836, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5614, 'Log Loss - std': 0.02640000000000009} 
 

Fold 3
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5698320158539575 -0.4271699725795722 -1.0389280503395208
  -0.6347153561730201 -0.6005026730450418 -0.7223571148557317
  -0.6306854501095174]
 [0.0 0.0 1.0 -1.4435910400327316 -1.4336221198959578 -1.156125699169423
  -1.2182459667920271 -1.160980331721523 -1.1969622573764345
  -1.2017803823001665]
 [1.0 0.0 0.0 0.05428157284516671 0.12637870844443994
  -0.10134685970030412 -0.3050256177643609 -0.4570203924238625
  -0.3562331477683324 -0.2023642509665306]
 [0.0 0.0 1.0 -0.6946547335937824 -0.4271699725795722 -0.3357421573601085
  -0.6306700833091101 -0.6408570644697484 -0.6048358414696529
  -0.5949920168476018]
 [1.0 0.0 0.0 0.05428157284516671 0.07605610107862061 0.25024608678940174
  -0.10175065635288713 -0.5444549071773936 -0.3562331477683324
  0.6542781473194431]
 [0.0 0.0 1.0 -0.4034017255341914 -0.37684736521375284
  -0.3357421573601085 -0.6438172201168175 -0.6363732432003366
  -0.6183959883988158 -0.5236051503237706]
 [1.0 0.0 0.0 0.22071186316493327 0.3276691379077173 0.25024608678940174
  0.13489780618584366 -0.19695875879797511 -0.27035221721696706
  0.5828912807956119]
 [1.0 0.0 0.0 0.012674000265225062 -0.27620215048211416
  0.015850789129598055 -0.44762148621718584 -0.7372592217621032
  -0.3019925600516806 -0.2023642509665306]
 [0.0 0.0 1.0 -0.7778698787536658 -0.5781377946770302 -0.6873351038498147
  -0.8531600908241561 -0.8560804854015173 -0.9031590739112376
  -0.737765749895264]
 [0.0 0.0 1.0 -0.27857900779436645 -0.27620215048211416
  -0.10134685970030412 -0.5790928542942586 -0.6318894219309248
  -0.7765977025723835 -0.3451379840141928]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7., 8.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.8279756307601929
Epoch 1 loss 1.7297515869140625
Epoch 2 loss 1.7208064794540405
Epoch 3 loss 1.6458677053451538
Epoch 4 loss 1.654779314994812
Epoch 5 loss 1.672685146331787
Epoch 6 loss 1.6585627794265747
Epoch 7 loss 1.5914740562438965
Epoch 8 loss 1.613010287284851
Epoch 9 loss 1.6132904291152954
Epoch 10 loss 1.6031001806259155
Epoch 11 loss 1.6597743034362793
Epoch 12 loss 1.6194016933441162
Epoch 13 loss 1.6100705862045288
Epoch 14 loss 1.6051286458969116
Epoch 15 loss 1.6002944707870483
Epoch 16 loss 1.5947388410568237
Epoch 17 loss 1.5699740648269653
Epoch 18 loss 1.567611813545227
Epoch 19 loss 1.6016602516174316
Epoch 20 loss 1.5706099271774292
Epoch 21 loss 1.5869758129119873
Epoch 22 loss 1.5977967977523804
Epoch 23 loss 1.5744779109954834
Epoch 24 loss 1.5695472955703735
Epoch 25 loss 1.5894254446029663
Epoch 26 loss 1.583512544631958
Epoch 27 loss 1.6490647792816162
Epoch 28 loss 1.6041423082351685
Epoch 29 loss 1.5792168378829956
Epoch 30 loss 1.612535834312439
Epoch 31 loss 1.5905354022979736
Epoch 32 loss 1.5852837562561035
Epoch 33 loss 1.591367483139038
Epoch 34 loss 1.5942286252975464
Epoch 35 loss 1.6223732233047485
Epoch 36 loss 1.598151445388794
Epoch 37 loss 1.5883418321609497
Epoch 38 loss 1.5813145637512207
Epoch 39 loss 1.562959909439087
Epoch 40 loss 1.5625919103622437
Epoch 41 loss 1.584017276763916
Epoch 42 loss 1.5654126405715942
Epoch 43 loss 1.618122935295105
Epoch 44 loss 1.5782722234725952
Epoch 45 loss 1.5889934301376343
Epoch 46 loss 1.5978319644927979
Epoch 47 loss 1.5877641439437866
Epoch 48 loss 1.6048575639724731
Epoch 49 loss 1.6042500734329224
Epoch 50 loss 1.6195629835128784
Epoch 51 loss 1.5965956449508667
Epoch 52 loss 1.57889986038208
Epoch 53 loss 1.5913835763931274
Epoch 54 loss 1.5968550443649292
Epoch 55 loss 1.5914186239242554
Epoch 56 loss 1.5741544961929321
Epoch 57 loss 1.587100863456726
Epoch 58 loss 1.5706590414047241
Epoch 59 loss 1.6108602285385132
Epoch 60 loss 1.5661988258361816
Epoch 61 loss 1.5873149633407593
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/Abalone/logging/loss_2.txt
File name : output/SAINT/Abalone/logging/loss_2.txt . The file was saved
Log file exists at: output/SAINT/Abalone/logging/val_loss_2.txt
File name : output/SAINT/Abalone/logging/val_loss_2.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5729666666666666, 'Log Loss - std': 0.027059481805008015} 
 

Fold 4
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5690315405613264 -0.42535969270024443
  -1.0500768981385924 -0.6384214848190454 -0.6042936766305018
  -0.7195875197860123 -0.6349568100929689]
 [0.0 0.0 1.0 -1.4392625570934354 -1.4277233177208453 -1.167955269492567
  -1.225152073368887 -1.1667739623949986 -1.1982585500483505
  -1.2058724559266103]
 [1.0 0.0 0.0 0.05256204267589417 0.12594030106108614
  -0.10704992730679613 -0.30692378660890945 -0.4602987234747907
  -0.35032701072649475 -0.20677007571773787]
 [0.0 1.0 0.0 -1.605020845956694 -1.5279596802229054 -1.403712012200516
  -1.266843501671389 -1.2117723852561584 -1.2803164409504653
  -1.312919139520418]
 [0.0 1.0 0.0 -0.8176689738562148 -1.0768960489636352 -1.0500768981385924
  -0.9689023189242423 -0.9800305075211857 -0.9338497904748684
  -0.8490501772805844]
 [1.0 0.0 0.0 0.05256204267589417 0.07582211981005604 0.24658518675512706
  -0.10253410151615713 -0.5480456480540523 -0.35032701072649475
  0.6496033930327244]
 [1.0 0.0 0.0 0.17688075932333838 0.17605848231211624
  -0.34280667001474535 -0.1218545195099993 -0.2915546377454418
  -0.277386663257948 0.1500522029282881]
 [1.0 0.0 0.0 0.21832033153915312 0.32641302606520656 0.24658518675512706
  0.13541209903958457 -0.1993078708800643 -0.2637103481075954
  0.5782389373035192]
 [1.0 0.0 0.0 0.011122470460079437 -0.2750051489471541
  0.010828444047178483 -0.45030162540531793 -0.741538866357039
  -0.2956217501250846 -0.20677007571773787]
 [0.0 0.0 1.0 -0.7762294016404001 -0.5757142364533347 -0.6964417840766689
  -0.8580641314858839 -0.8607846869391123 -0.9019383884573792
  -0.7420034936867765]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
Unique values in y_test: (array([0., 1., 2., 3., 4., 6., 7., 8., 9.]), 9)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Test Labels: [0 1 2 3 4 5 6 7 8], Length: 9
Final Num Classes: 9
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Test after shift : [0 1 2 3 4 5 6 7 8], Length : 9
Number of Classes After Bin Verifier: 9
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.762868046760559
Epoch 1 loss 1.7178016901016235
Epoch 2 loss 1.6360925436019897
Epoch 3 loss 1.633620023727417
Epoch 4 loss 1.6281670331954956
Epoch 5 loss 1.6041995286941528
Epoch 6 loss 1.6552106142044067
Epoch 7 loss 1.613234281539917
Epoch 8 loss 1.604874849319458
Epoch 9 loss 1.6321719884872437
Epoch 10 loss 1.6614181995391846
Epoch 11 loss 1.6810535192489624
Epoch 12 loss 1.6358678340911865
Epoch 13 loss 1.640831470489502
Epoch 14 loss 1.6089617013931274
Epoch 15 loss 1.6252611875534058
Epoch 16 loss 1.6871148347854614
Epoch 17 loss 1.6494508981704712
Epoch 18 loss 1.6149952411651611
Epoch 19 loss 1.609342098236084
Epoch 20 loss 1.58805251121521
Epoch 21 loss 1.6070892810821533
Epoch 22 loss 1.6188948154449463
Epoch 23 loss 1.6096378564834595
Epoch 24 loss 1.6216360330581665
Epoch 25 loss 1.5916688442230225
Epoch 26 loss 1.6049977540969849
Epoch 27 loss 1.6129950284957886
Epoch 28 loss 1.6218866109848022
Epoch 29 loss 1.6318832635879517
Epoch 30 loss 1.6155258417129517
Epoch 31 loss 1.6061300039291382
Epoch 32 loss 1.6535698175430298
Epoch 33 loss 1.6113921403884888
Epoch 34 loss 1.6290991306304932
Epoch 35 loss 1.6160110235214233
Epoch 36 loss 1.627410650253296
Epoch 37 loss 1.6657941341400146
Epoch 38 loss 1.62373685836792
Epoch 39 loss 1.6513785123825073
Epoch 40 loss 1.6034590005874634
Epoch 41 loss 1.6170971393585205
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/Abalone/logging/loss_3.txt
File name : output/SAINT/Abalone/logging/loss_3.txt . The file was saved
Log file exists at: output/SAINT/Abalone/logging/val_loss_3.txt
File name : output/SAINT/Abalone/logging/val_loss_3.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 9
Class label len :9
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8]
Unique y_true : [0 1 2 3 4 5 6 7 8] 

Prediction shape : (835,)
Probabilities shape : (835, 9) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5816249999999998, 'Log Loss - std': 0.027821967489737344} 
 

Fold 5
num_features : 8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :8
num_classes : 1
cat_idx : [0]
nominal_idx : [0]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (3342, 8)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [1, 2, 3, 4, 5, 6, 7]
Cat Dims V1 : []
Cat Idx V1 : [0] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044]] 
 
 
Val : (3342, 10) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [1, 2, 3, 4, 5, 6, 7] 


OHE Idx : [0, 1, 2]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 1.0 -0.5902805359689328 -0.4481428509674965 -1.0550394761935622
  -0.652531118203336 -0.6203614871572382 -0.7346462314941438
  -0.6481967718014128]
 [0.0 0.0 1.0 -1.4674337379289928 -1.4602346254866023 -1.172782811845699
  -1.2399035204261921 -1.1817207861619912 -1.2109175474628806
  -1.2256252900914224]
 [1.0 0.0 0.0 0.036257465431109887 0.10850762501801176
  -0.11309279097646885 -0.32067080082959054 -0.4766535066120213
  -0.3672369306039755 -0.21512538308390547]
 [0.0 0.0 1.0 -0.7155881362489415 -0.4481428509674965 -0.3485794622807424
  -0.6484592124686889 -0.6607793566855803 -0.6167123818256947
  -0.6121074894082871]
 [0.0 1.0 0.0 -1.634510538302337 -1.561443802938513 -1.4082694831499722
  -1.2816405542063258 -1.2266295300823715 -1.292564058771807
  -1.3338931372707994]
 [0.0 1.0 0.0 -0.8408957365289501 -1.1060025044049155 -1.0550394761935622
  -0.9833734591434197 -0.9953494988924132 -0.9478343443563404
  -0.8647324661601664]
 [1.0 0.0 0.0 0.1615650657111185 0.15911221374396708 -0.3485794622807424
  -0.13539908990314378 -0.30824571691059555 -0.29466225388492984
  0.14576744084735074]
 [0.0 0.0 1.0 -0.42320373559558844 -0.39753826224154115
  -0.3485794622807424 -0.6616929061062923 -0.6562884822935423
  -0.6303201337105158 -0.5399289246220358]
 [1.0 0.0 0.0 0.20333426580445474 0.31092597992183313 0.24013721597994078
  0.12214894781329044 -0.21618279187381598 -0.28105450200010873
  0.578838829564858]
 [1.0 0.0 0.0 -0.0055117346622263206 -0.29632908478963044
  0.0046505446756679135 -0.4642054779759037 -0.7573331561143978
  -0.31280592306469124 -0.21512538308390547]] 
 
 
Val : (3342, 10) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 10
Unique values in y_train: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
Unique values in y_test: (array([0., 1., 2., 3., 4., 5., 6., 7.]), 8)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Test Labels: [0 1 2 3 4 5 6 7], Length: 8
Final Num Classes: 8
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7]
VERIFY SHIFT
Train after shift : [0 1 2 3 4 5 6 7], Length : 8
Test after shift : [0 1 2 3 4 5 6 7], Length : 8
Number of Classes After Bin Verifier: 8
In get_device
Using dim 128 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 4, 'dropout': 0.8}
Epoch 0 loss 1.6713029146194458
Epoch 1 loss 1.6089681386947632
Epoch 2 loss 1.587899923324585
Epoch 3 loss 1.5427221059799194
Epoch 4 loss 1.5853641033172607
Epoch 5 loss 1.5612808465957642
Epoch 6 loss 1.5723049640655518
Epoch 7 loss 1.5360757112503052
Epoch 8 loss 1.5558476448059082
Epoch 9 loss 1.5424381494522095
Epoch 10 loss 1.541297197341919
Epoch 11 loss 1.5323765277862549
Epoch 12 loss 1.5494061708450317
Epoch 13 loss 1.5332902669906616
Epoch 14 loss 1.5087555646896362
Epoch 15 loss 1.5292302370071411
Epoch 16 loss 1.536880373954773
Epoch 17 loss 1.5299042463302612
Epoch 18 loss 1.5100222826004028
Epoch 19 loss 1.5438646078109741
Epoch 20 loss 1.521475076675415
Epoch 21 loss 1.5321089029312134
Epoch 22 loss 1.522503137588501
Epoch 23 loss 1.532204270362854
Epoch 24 loss 1.508148431777954
Epoch 25 loss 1.5214238166809082
Epoch 26 loss 1.5457074642181396
Epoch 27 loss 1.4993271827697754
Epoch 28 loss 1.52003014087677
Epoch 29 loss 1.5370889902114868
Epoch 30 loss 1.5102407932281494
Epoch 31 loss 1.537962555885315
Epoch 32 loss 1.5079489946365356
Epoch 33 loss 1.5038994550704956
Epoch 34 loss 1.5035004615783691
Epoch 35 loss 1.4953243732452393
Epoch 36 loss 1.5136455297470093
Epoch 37 loss 1.506984829902649
Epoch 38 loss 1.5337789058685303
Epoch 39 loss 1.5104765892028809
Epoch 40 loss 1.5027109384536743
Epoch 41 loss 1.4966529607772827
Epoch 42 loss 1.4932606220245361
Epoch 43 loss 1.5096629858016968
Epoch 44 loss 1.4864600896835327
Epoch 45 loss 1.5008682012557983
Epoch 46 loss 1.4777944087982178
Epoch 47 loss 1.5023436546325684
Epoch 48 loss 1.507758617401123
Epoch 49 loss 1.510572910308838
Epoch 50 loss 1.514701008796692
Epoch 51 loss 1.5026775598526
Epoch 52 loss 1.4672255516052246
Epoch 53 loss 1.498629093170166
Epoch 54 loss 1.487147569656372
Epoch 55 loss 1.4969764947891235
Epoch 56 loss 1.5109752416610718
Epoch 57 loss 1.5005781650543213
Epoch 58 loss 1.4766252040863037
Epoch 59 loss 1.4835350513458252
Epoch 60 loss 1.5098512172698975
Epoch 61 loss 1.5074145793914795
Epoch 62 loss 1.5225626230239868
Epoch 63 loss 1.5093036890029907
Epoch 64 loss 1.503907561302185
Epoch 65 loss 1.5027434825897217
Epoch 66 loss 1.4916194677352905
Epoch 67 loss 1.5158483982086182
Epoch 68 loss 1.5206725597381592
Epoch 69 loss 1.5383721590042114
Epoch 70 loss 1.49583899974823
Epoch 71 loss 1.5070701837539673
Epoch 72 loss 1.5650333166122437
Epoch 73 loss 1.4987462759017944
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/Abalone/logging/loss_4.txt
File name : output/SAINT/Abalone/logging/loss_4.txt . The file was saved
Log file exists at: output/SAINT/Abalone/logging/val_loss_4.txt
File name : output/SAINT/Abalone/logging/val_loss_4.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 8
Class label len :8
Class labels : [0, 1, 2, 3, 4, 5, 6, 7]
Unique y_true : [0 1 2 3 4 5 6 7] 

Prediction shape : (835,)
Probabilities shape : (835, 8) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.56794, 'Log Loss - std': 0.036991436846924436} 
 

Saving model.....
Results After CV: {'Log Loss - mean': 1.56794, 'Log Loss - std': 0.036991436846924436}
Train time: 388.4369633253999
Inference time: 0.19263566659992648
Finished cross validation
Loss path :output/SAINT/Abalone/logging/
Plots saved successfully!


----------------------------------------------------------------------------
Training SAINT Vesion 1 with Dataset: config/house_sales.yml 



----------------------------------------------------------------------------
Namespace(batch_size=64, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/house_sales.yml', data_parallel=False, dataset='House_Sales', direction='maximize', dropna_idx=[0], early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='SAINT', n_trials=2, nominal_idx=[14], num_bins=10, num_classes=1, num_features=21, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=128, y_distribution='skewed')
Start hyperparameter optimization
Loading dataset House_Sales...
Dataset loaded! 

(21613, 21)
A new study created in RDB with name: SAINT_House_Sales
In get_device
Using dim 64 and batch size 64
Fold 1
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.3947275  -1.43868499
  -0.97562257 -0.22635649 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.73157015 -0.65849042 -0.53844893 -0.2111493  -0.35633908
  -0.30229791 -0.94097085 -0.26202697 -0.69194575  1.10260759 -0.31933766]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
   0.53685987 -0.18786591  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.46756773  0.24158981 -0.67496901  4.7231026   1.15996359
  -0.74230143 -0.43016988 -0.18783684 -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  1.15538522
  -0.12689142 -0.24207186 -0.91101164 -0.08969788 -0.30794673  2.43033886
  -0.55213581 -0.8890327   1.38919211 -0.19714873 -0.2111493  -0.28695631
  -1.26746693 -0.91178223 -0.2862721  -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -0.14164989
  -0.43156414 -0.16760517 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258 -0.12594496 -0.65849042  0.5537117  -0.2111493   0.40687141
   1.20223026 -0.26963243 -0.19290967  1.44520001 -1.46339083  0.26074556]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  3.10093788
   3.63799292  2.10145304 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   2.83973773  2.55091839  2.78431647  1.03153198 -0.2111493   0.69090714
   1.4861035   4.05028437  3.32923754 -0.69194575 -0.50114142 -0.4353543 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
  -0.39348005 -0.19809299  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.0835512  -0.65849042  0.82675186 -0.2111493  -1.81265457
  -0.79907608  0.3695985  -0.21842302 -0.69194575 -0.18039162  1.30489535]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -1.43868499
  -0.32275245 -0.18235344 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.8890327   0.98415601 -0.36779883 -0.2111493  -0.34838897
  -0.87004439 -0.29882106 -0.17015654  1.44520001 -0.82189122 -0.08730437]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
  -0.2030596  -0.20435496  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.12841762 -0.65849042  1.09979202 -0.2111493  -1.38840783
   1.3015859   0.59143207 -0.19041056  1.44520001 -1.14264102 -0.4353543 ]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
   1.61409556 -0.12611658 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258  0.09208011  3.16685057 -0.19714873 -0.2111493   0.29051072
   0.49254716  0.32873442 -0.13986877  1.44520001 -0.82189122 -1.47950409]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46579442 -1.43868499
  -0.99738491 -0.21789436 -0.91101164 -0.08969788 -0.30794673  0.89886576
  -0.55213581 -1.11917028  0.01656975 -0.98213919 -0.2111493   0.93591506
  -0.550687   -0.95556517 -0.24897189 -0.69194575 -0.50114142  1.30489535]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 178
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Num Classes: 176
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Number of Classes After Bin Verifier: 176
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.476016998291016
Epoch 1 loss 4.144652366638184
Epoch 2 loss 4.036501407623291
Epoch 3 loss 3.9858076572418213
Epoch 4 loss 3.952908754348755
Epoch 5 loss 3.943439245223999
Epoch 6 loss 3.9315316677093506
Epoch 7 loss 3.891359567642212
Epoch 8 loss 3.891836404800415
Epoch 9 loss 3.8749685287475586
Epoch 10 loss 3.8591599464416504
Epoch 11 loss 3.8611724376678467
Epoch 12 loss 3.919062852859497
Epoch 13 loss 3.866745948791504
Epoch 14 loss 3.8731861114501953
Epoch 15 loss 3.853748083114624
Epoch 16 loss 3.854633092880249
Epoch 17 loss 3.8507447242736816
Epoch 18 loss 3.876100540161133
Epoch 19 loss 3.8841023445129395
Epoch 20 loss 3.863689661026001
Epoch 21 loss 3.8669626712799072
Epoch 22 loss 3.911191701889038
Epoch 23 loss 3.8804819583892822
Epoch 24 loss 3.9183664321899414
Epoch 25 loss 3.9176743030548096
Epoch 26 loss 3.9086928367614746
Epoch 27 loss 3.942295789718628
Epoch 28 loss 3.953125
Epoch 29 loss 3.950347900390625
Epoch 30 loss 3.9771926403045654
Epoch 31 loss 4.014734268188477
Epoch 32 loss 4.0272440910339355
Epoch 33 loss 4.065616607666016
Epoch 34 loss 4.096246242523193
Epoch 35 loss 4.083767414093018
Epoch 36 loss 4.138689041137695
Epoch 37 loss 4.210792541503906
Epoch 38 loss 4.189436435699463
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 176
Class label len :176
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175] 

Prediction shape : (4323,)
Probabilities shape : (4323, 176) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8464, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39750492 -1.4464586
  -0.98108962 -0.22364056 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.73686418 -0.65800361 -0.53911901 -0.21352892 -0.35612463
  -0.30928015 -0.94471799 -0.26449285 -0.69176297  1.10470474 -0.31585982]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
   0.531249   -0.18624395  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.456921    0.24756427 -0.67481406  4.67126434  1.16025966
  -0.75053031 -0.43578594 -0.18976687 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.42717512 -0.12145761 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -1.4070134  -1.23126007 -0.65800361 -1.28544178 -0.21352892  1.28240883
  -0.13847364  1.06192837 -0.17387492  1.44558187 -1.46630395  1.0753261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  1.14789757
  -0.13243917 -0.23890927 -0.91453407 -0.08636167 -0.30417801  2.43580216
  -0.5589008  -0.89362385  1.40216332 -0.1998814  -0.21352892 -0.28673812
  -1.27718374 -0.91563616 -0.28891311 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  3.09366471
   3.6320872   2.03800863 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   2.83354957  2.53097202  2.80579354  1.02137403 -0.21352892  0.69117797
   1.48418826  4.02827516  3.35271046 -0.69176297 -0.50217569 -0.43179198]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
  -0.39900245 -0.19618036  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.09173785 -0.65800361  0.81783146 -0.21352892 -1.8125185
  -0.80746582  0.36105623 -0.22057397 -0.69176297 -0.18079961  1.30719042]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39750492 -0.79786956
  -1.11165123 -0.12824632 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.88156542 -0.65800361 -0.26772892 -0.21352892 -1.09118793
  -0.72206256 -0.49394961 -0.11192261  1.44558187 -1.78768004 -0.0839955 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
  -0.20860011 -0.20226435  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.11928478 -0.65800361  1.08922156 -0.21352892 -1.38824892
   1.29914787  0.58207815 -0.19235918  1.44558187 -1.14492787 -0.43179198]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
   1.60838227 -0.12624964 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   0.28921179  0.08310948  3.19065989 -0.1998814  -0.21352892  0.29076
   0.48781692  0.32034167 -0.14145234  1.44558187 -0.82355178 -1.47518142]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.00284989 -0.21541894 -0.91453407 -0.08636167 -0.30417801  0.9030092
  -0.5589008  -1.12273414  0.0211723  -0.98012792 -0.21352892  0.93619907
  -0.55837299 -0.95925891 -0.25134349 -0.69176297 -0.50217569  1.30719042]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 157
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
No need to shift labels.
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Number of Classes After Bin Verifier: 157
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.345434188842773
Epoch 1 loss 4.123791217803955
Epoch 2 loss 4.035742282867432
Epoch 3 loss 3.893615245819092
Epoch 4 loss 3.82452654838562
Epoch 5 loss 3.849102258682251
Epoch 6 loss 3.8127553462982178
Epoch 7 loss 3.8172171115875244
Epoch 8 loss 3.8007755279541016
Epoch 9 loss 3.8280622959136963
Epoch 10 loss 3.7733383178710938
Epoch 11 loss 3.7526063919067383
Epoch 12 loss 3.7649521827697754
Epoch 13 loss 3.761631727218628
Epoch 14 loss 3.7557053565979004
Epoch 15 loss 3.754187822341919
Epoch 16 loss 3.760067939758301
Epoch 17 loss 3.7622225284576416
Epoch 18 loss 3.8461058139801025
Epoch 19 loss 3.763305902481079
Epoch 20 loss 3.841914415359497
Epoch 21 loss 3.7973310947418213
Epoch 22 loss 3.7840702533721924
Epoch 23 loss 3.7745254039764404
Epoch 24 loss 3.8040268421173096
Epoch 25 loss 3.826873779296875
Epoch 26 loss 3.830828905105591
Epoch 27 loss 3.817608594894409
Epoch 28 loss 3.834728956222534
Epoch 29 loss 3.8386707305908203
Epoch 30 loss 3.8684957027435303
Epoch 31 loss 3.9057347774505615
Epoch 32 loss 3.9205260276794434
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 157
Class label len :157
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156] 

Prediction shape : (4323,)
Probabilities shape : (4323, 157) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.79805, 'Log Loss - std': 0.048350000000000115} 
 

Fold 3
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39741079 -1.45113378
  -0.98251964 -0.23408869 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.73776602 -0.65958158 -0.55261468 -0.20509978 -0.34621257
  -0.3075573  -0.94534346 -0.25697128 -0.68738047  1.09900948 -0.30696442]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
   0.53142887 -0.19516535  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.45948399  0.24387824 -0.68858798  4.86346877  1.16478766
  -0.74817205 -0.43462874 -0.18749435 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46903157 -1.45113378
  -1.42907999 -0.12773409 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -1.41023515 -1.23359683 -0.65958158 -1.30046783 -0.20509978  1.28650312
  -0.13699675  1.06833174 -0.17271872  1.45479839 -1.47542134  1.07875341]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     1.14652152
  -0.1329658  -0.24998075 -0.92135604 -0.08636167 -0.30822085  2.44891305
  -0.56095038 -0.89498067  1.39578951 -0.21268143 -0.20509978 -0.27707243
  -1.27406708 -0.91615976 -0.27967615 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -0.15230613
  -0.43793385 -0.17467681 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   0.28833439 -0.1330943  -0.65958158  0.53517172 -0.20509978  0.41432901
   1.19906088 -0.2741184  -0.19224491  1.45479839 -1.47542134  0.27041801]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     3.09476299
   3.63556789  2.11989316 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   2.8361887   2.53955472  2.79615222  1.01107827 -0.20509978  0.69737148
   1.48332846  4.04506898  3.10614501 -0.68738047 -0.51000978 -0.4224409 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
  -0.39981284 -0.20550742  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.09076728 -0.65958158  0.80711832 -0.20509978 -1.79743539
  -0.80502557  0.3650046  -0.21613743 -0.68738047 -0.18820593  1.30970638]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39741079 -0.80171996
  -1.11322023 -0.13479994 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.88288723 -0.65958158 -0.28066808 -0.20509978 -1.07866597
  -0.71974529 -0.49299613 -0.11511819  1.45479839 -1.7972252  -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -1.45113378
  -0.32901669 -0.1895909  -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.89498067  0.98923259 -0.38264805 -0.20509978 -0.33829026
  -0.87609246 -0.30330209 -0.17093726  1.45479839 -0.83181364 -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.49710769
  -0.20920781 -0.21183979  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.12086783 -0.65958158  1.07906492 -0.20509978 -1.37467222
   1.29855453  0.58680071 -0.18990456  1.45479839 -1.15361749 -0.4224409 ]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176., 177.]), 178)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,
        56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,
        67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,
        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,
        89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.,
       100., 101., 102., 103., 104., 105., 106., 107., 109., 110., 111.,
       112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,
       123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,
       134., 135., 136., 137., 138., 139., 140., 141., 142., 143., 144.,
       145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155.,
       156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166.,
       167., 168., 169., 170., 171., 172., 173., 174., 175., 176., 177.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 178
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 176
Final Num Classes: 178
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 178
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 176
Number of Classes After Bin Verifier: 178
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.663290500640869
Epoch 1 loss 4.171170234680176
Epoch 2 loss 4.025515556335449
Epoch 3 loss 4.025565147399902
Epoch 4 loss 3.9541046619415283
Epoch 5 loss 3.919886350631714
Epoch 6 loss 3.9052178859710693
Epoch 7 loss 3.9121735095977783
Epoch 8 loss 3.890953540802002
Epoch 9 loss 3.9069204330444336
Epoch 10 loss 3.8644938468933105
Epoch 11 loss 3.89342999458313
Epoch 12 loss 3.862652540206909
Epoch 13 loss 3.870375633239746
Epoch 14 loss 3.8688528537750244
Epoch 15 loss 3.8620834350585938
Epoch 16 loss 3.8978068828582764
Epoch 17 loss 3.866497755050659
Epoch 18 loss 3.874699831008911
Epoch 19 loss 3.861119508743286
Epoch 20 loss 3.9214420318603516
Epoch 21 loss 3.921095371246338
Epoch 22 loss 3.8986129760742188
Epoch 23 loss 3.9137766361236572
Epoch 24 loss 3.9841623306274414
Epoch 25 loss 3.919180154800415
Epoch 26 loss 3.945692300796509
Epoch 27 loss 3.9506337642669678
Epoch 28 loss 3.966543436050415
Epoch 29 loss 4.003310680389404
Epoch 30 loss 4.012463569641113
Epoch 31 loss 4.03939962387085
Epoch 32 loss 4.045851230621338
Epoch 33 loss 4.117457866668701
Epoch 34 loss 4.117702960968018
Epoch 35 loss 4.168560981750488
Epoch 36 loss 4.219090461730957
Epoch 37 loss 4.231520175933838
Epoch 38 loss 4.293095111846924
Epoch 39 loss 4.333591938018799
Epoch 40 loss 4.459508419036865
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 178
Class label len :178
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177] 

Prediction shape : (4323,)
Probabilities shape : (4323, 178) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8190666666666666, 'Log Loss - std': 0.04941540470564044} 
 

Fold 4
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.40689056 -1.45158542
  -0.9793635  -0.22406432 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.73483377 -0.65814255 -0.54856633 -0.21024459 -0.35275491
  -0.30866343 -0.94078658 -0.25941786 -0.69164201  1.09428478 -0.3072261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.16718528
   0.52910389 -0.1869132   0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.4529345   0.2497395  -0.68467086  4.7430623   1.15877468
  -0.74752539 -0.42980973 -0.18731522 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.42430711 -0.12255214 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -1.41302015 -1.2267378  -0.65814255 -1.29714122 -0.21024459  1.28053279
  -0.13878138  1.07392215 -0.17198117  1.44583467 -1.46983462  1.08105208]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   1.13844771
  -0.1328854  -0.23923281 -0.91901629 -0.08601865 -0.30335866  2.45803631
  -0.56114848 -0.89080334  1.40728911 -0.20830502 -0.21024459 -0.28359054
  -1.27132837 -0.9115879  -0.28298081 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -0.15656886
  -0.43674933 -0.16735752 -0.91901629 -0.08601865 -0.30335866 -0.62818566
   0.29072319 -0.1349508  -0.65814255  0.54026987 -0.21024459  0.40805312
   1.19196133 -0.269217   -0.19224531  1.44583467 -1.46983462  0.27122314]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   3.08097255
   3.62200464  2.0227386  -0.91901629 -0.08601865 -0.30335866 -0.62818566
   2.8463382   2.5165319   2.81450628  1.01663571 -0.21024459  0.69119475
   1.47509807  4.05218722  3.23079943 -0.69164201 -0.50828985 -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.40689056 -0.80407714
  -1.1095909  -0.12929629 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.87880568 -0.65814255 -0.27635728 -0.21024459 -1.08546492
  -0.71921171 -0.48820708 -0.11220377  1.44583467 -1.79034955 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -1.45158542
  -0.3282265  -0.18159256 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.89080334  0.99874218 -0.37843568 -0.21024459 -0.34482983
  -0.87493692 -0.29841568 -0.17013239  1.44583467 -0.82880477 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.49093942
  -0.20885138 -0.20282844  0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.11700004 -0.65814255  1.08468797 -0.21024459 -1.38157486
   1.29105919  0.59214397 -0.18981652  1.44583467 -1.1493197  -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.00106806 -0.21589668 -0.91901629 -0.08601865 -0.30335866  0.91492532
  -0.56114848 -1.11875887  0.02276898 -0.99090604 -0.21024459  0.93543142
  -0.55640808 -0.95538592 -0.24673011 -0.69164201 -0.50828985  1.31243178]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 181
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.493348598480225
Epoch 1 loss 4.105424404144287
Epoch 2 loss 4.016876697540283
Epoch 3 loss 3.9750890731811523
Epoch 4 loss 3.954087734222412
Epoch 5 loss 3.917888879776001
Epoch 6 loss 3.89412522315979
Epoch 7 loss 3.889399766921997
Epoch 8 loss 3.8764917850494385
Epoch 9 loss 3.885300636291504
Epoch 10 loss 3.865050792694092
Epoch 11 loss 3.898908853530884
Epoch 12 loss 3.852069139480591
Epoch 13 loss 3.8587520122528076
Epoch 14 loss 3.8678781986236572
Epoch 15 loss 3.863795042037964
Epoch 16 loss 3.857391834259033
Epoch 17 loss 3.8586511611938477
Epoch 18 loss 3.8683090209960938
Epoch 19 loss 3.877812147140503
Epoch 20 loss 3.8910772800445557
Epoch 21 loss 3.9139561653137207
Epoch 22 loss 3.8821096420288086
Epoch 23 loss 3.9024720191955566
Epoch 24 loss 3.929856300354004
Epoch 25 loss 3.927199363708496
Epoch 26 loss 3.9287588596343994
Epoch 27 loss 3.93933367729187
Epoch 28 loss 3.9437057971954346
Epoch 29 loss 3.957401990890503
Epoch 30 loss 3.975226402282715
Epoch 31 loss 3.9886317253112793
Epoch 32 loss 4.03916072845459
Epoch 33 loss 4.024586200714111
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.826775, 'Log Loss - std': 0.0448293082146045} 
 

Fold 5
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -1.46522841e+00 -1.44959243e+00
  -1.42903253e+00 -1.24024562e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -1.41774071e+00 -1.22950778e+00
  -6.59206226e-01 -1.29724427e+00 -2.10543891e-01  1.28617272e+00
  -1.32049290e-01  1.07354326e+00 -1.71578976e-01  1.44812997e+00
  -1.46713375e+00  1.08404858e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.45215728e-01
  -4.33784316e-01 -1.72861315e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01 -1.26495482e-01
  -6.59206226e-01  5.47241157e-01 -2.10543891e-01  4.11371019e-01
   1.20397428e+00 -2.72140904e-01 -1.92417981e-01  1.44812997e+00
  -1.46713375e+00  2.72452120e-01]
 [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  1.80878448e-01
  -3.95505538e-01 -2.04935871e-01  9.43881175e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.40719324e-02
  -6.59206226e-01  8.20498257e-01 -2.10543891e-01 -1.80705676e+00
  -8.00061074e-01  3.68521773e-01 -2.17916871e-01 -6.90545751e-01
  -1.84793849e-01  1.31593328e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00 -3.97401366e-01 -7.97404081e-01
  -1.11186552e+00 -1.31375511e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.77998365e-01
  -6.59206226e-01 -2.72530142e-01 -2.10543891e-01 -1.08612191e+00
  -7.14782974e-01 -4.91545931e-01 -1.10105776e-01  1.44812997e+00
  -1.78771873e+00 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -3.24416380e-01 -1.88377159e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.90119380e-01
   9.87298486e-01 -3.75001554e-01 -2.10543891e-01 -3.43515672e-01
  -8.71126158e-01 -3.01394908e-01 -1.69677743e-01  1.44812997e+00
  -8.25963800e-01 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  5.06972624e-01
   1.62233288e+00 -1.29213467e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01  9.16827741e-02
   3.17511982e+00 -2.04215867e-01 -2.10543891e-01  2.95067902e-01
   4.93323444e-01  3.27566168e-01 -1.39407132e-01  1.44812997e+00
  -8.25963800e-01 -1.46668315e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -7.07204156e-01  1.27815383e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -4.29520838e-01
  -6.59206226e-01 -1.50218709e+00 -2.10543891e-01  1.41547867e+00
  -1.03623257e-01 -3.01394908e-01  1.20955314e-03 -6.90545751e-01
  -5.05378825e-01  1.43187563e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -4.71309904e-01
  -7.72824917e-01 -1.32164021e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -5.02246924e-01
  -6.59206226e-01  2.05669783e-01 -2.10543891e-01  3.81753455e-01
   1.20397428e+00 -9.01101980e-01 -9.15780737e-02 -6.90545751e-01
   1.09754605e+00 -1.00291374e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01  1.15916098e+00
   9.55188471e-01 -2.51203607e-01  9.43881175e-01 -8.73729514e-02
   3.63073997e+00 -6.28575252e-01  1.15214655e+00  2.37134945e-01
   1.52861510e+00  2.73984057e-01 -2.10543891e-01  8.34106764e-02
  -1.14117347e+00  2.25177156e-01 -3.23006594e-01  1.44812997e+00
  -1.78771873e+00  9.68106228e-01]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01 -1.44959243e+00
  -5.21278664e-01 -2.69008674e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -2.23463596e-01
  -6.59206226e-01 -1.87791560e+00 -2.10543891e-01  7.58113231e-01
  -9.13765208e-01 -5.50053938e-01 -3.11822870e-01 -6.90545751e-01
  -5.05378825e-01  1.54781798e+00]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 114., 115., 116., 117., 118., 119., 120., 121.,
       122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,
       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,
       144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,
       155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,
       166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 176
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 176
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.460059642791748
Epoch 1 loss 4.112431049346924
Epoch 2 loss 4.014876365661621
Epoch 3 loss 3.983755588531494
Epoch 4 loss 3.945439577102661
Epoch 5 loss 3.928718090057373
Epoch 6 loss 3.9188272953033447
Epoch 7 loss 3.8799030780792236
Epoch 8 loss 3.8814198970794678
Epoch 9 loss 3.874924659729004
Epoch 10 loss 3.85353946685791
Epoch 11 loss 3.8920912742614746
Epoch 12 loss 3.8823633193969727
Epoch 13 loss 3.946978807449341
Epoch 14 loss 3.840837001800537
Epoch 15 loss 3.864011526107788
Epoch 16 loss 3.85827374458313
Epoch 17 loss 3.8733479976654053
Epoch 18 loss 3.848763942718506
Epoch 19 loss 3.8532907962799072
Epoch 20 loss 3.8632171154022217
Epoch 21 loss 3.872730016708374
Epoch 22 loss 3.8692312240600586
Epoch 23 loss 3.9033939838409424
Epoch 24 loss 3.8995375633239746
Epoch 25 loss 3.917311191558838
Epoch 26 loss 3.9522624015808105
Epoch 27 loss 3.9105167388916016
Epoch 28 loss 3.9389445781707764
Epoch 29 loss 3.938490152359009
Epoch 30 loss 3.975682497024536
Epoch 31 loss 3.99042010307312
Epoch 32 loss 4.007209777832031
Epoch 33 loss 4.017039775848389
Epoch 34 loss 4.0625410079956055
Epoch 35 loss 4.067348957061768
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.82958, 'Log Loss - std': 0.04048710411970713} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 0 finished with value: 3.82958 and parameters: {'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}. Best is trial 0 with value: 3.82958.
In get_device
Using dim 128 and batch size 64
Fold 1
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.3947275  -1.43868499
  -0.97562257 -0.22635649 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.73157015 -0.65849042 -0.53844893 -0.2111493  -0.35633908
  -0.30229791 -0.94097085 -0.26202697 -0.69194575  1.10260759 -0.31933766]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
   0.53685987 -0.18786591  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.46756773  0.24158981 -0.67496901  4.7231026   1.15996359
  -0.74230143 -0.43016988 -0.18783684 -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  1.15538522
  -0.12689142 -0.24207186 -0.91101164 -0.08969788 -0.30794673  2.43033886
  -0.55213581 -0.8890327   1.38919211 -0.19714873 -0.2111493  -0.28695631
  -1.26746693 -0.91178223 -0.2862721  -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -0.14164989
  -0.43156414 -0.16760517 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258 -0.12594496 -0.65849042  0.5537117  -0.2111493   0.40687141
   1.20223026 -0.26963243 -0.19290967  1.44520001 -1.46339083  0.26074556]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  3.10093788
   3.63799292  2.10145304 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   2.83973773  2.55091839  2.78431647  1.03153198 -0.2111493   0.69090714
   1.4861035   4.05028437  3.32923754 -0.69194575 -0.50114142 -0.4353543 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
  -0.39348005 -0.19809299  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.0835512  -0.65849042  0.82675186 -0.2111493  -1.81265457
  -0.79907608  0.3695985  -0.21842302 -0.69194575 -0.18039162  1.30489535]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -1.43868499
  -0.32275245 -0.18235344 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.8890327   0.98415601 -0.36779883 -0.2111493  -0.34838897
  -0.87004439 -0.29882106 -0.17015654  1.44520001 -0.82189122 -0.08730437]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
  -0.2030596  -0.20435496  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.12841762 -0.65849042  1.09979202 -0.2111493  -1.38840783
   1.3015859   0.59143207 -0.19041056  1.44520001 -1.14264102 -0.4353543 ]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
   1.61409556 -0.12611658 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258  0.09208011  3.16685057 -0.19714873 -0.2111493   0.29051072
   0.49254716  0.32873442 -0.13986877  1.44520001 -0.82189122 -1.47950409]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46579442 -1.43868499
  -0.99738491 -0.21789436 -0.91101164 -0.08969788 -0.30794673  0.89886576
  -0.55213581 -1.11917028  0.01656975 -0.98213919 -0.2111493   0.93591506
  -0.550687   -0.95556517 -0.24897189 -0.69194575 -0.50114142  1.30489535]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 178
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Num Classes: 176
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Number of Classes After Bin Verifier: 176
In get_device
Using dim 8 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}
Epoch 0 loss 4.77000093460083
Epoch 1 loss 4.356149196624756
Epoch 2 loss 4.102855205535889
Epoch 3 loss 4.053616046905518
Epoch 4 loss 4.012770175933838
Epoch 5 loss 3.976175308227539
Epoch 6 loss 3.9392054080963135
Epoch 7 loss 4.004401683807373
Epoch 8 loss 3.894692897796631
Epoch 9 loss 3.9844343662261963
Epoch 10 loss 3.8982126712799072
Epoch 11 loss 3.9001896381378174
Epoch 12 loss 3.8961141109466553
Epoch 13 loss 3.8640310764312744
Epoch 14 loss 3.851907253265381
Epoch 15 loss 3.8901193141937256
Epoch 16 loss 3.849100351333618
Epoch 17 loss 3.8406646251678467
Epoch 18 loss 3.88824200630188
Epoch 19 loss 3.841996669769287
Epoch 20 loss 3.8565237522125244
Epoch 21 loss 3.88448166847229
Epoch 22 loss 3.8895070552825928
Epoch 23 loss 3.8755664825439453
Epoch 24 loss 3.841055393218994
Epoch 25 loss 3.8717799186706543
Epoch 26 loss 3.862905740737915
Epoch 27 loss 3.871598243713379
Epoch 28 loss 3.870095729827881
Epoch 29 loss 3.909101724624634
Epoch 30 loss 3.8844218254089355
Epoch 31 loss 3.9063124656677246
Epoch 32 loss 3.9097774028778076
Epoch 33 loss 3.906433582305908
Epoch 34 loss 3.9391510486602783
Epoch 35 loss 3.908583641052246
Epoch 36 loss 3.9397175312042236
Epoch 37 loss 3.948688268661499
Epoch 38 loss 3.951601266860962
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 176
Class label len :176
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175] 

Prediction shape : (4323,)
Probabilities shape : (4323, 176) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8386, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39750492 -1.4464586
  -0.98108962 -0.22364056 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.73686418 -0.65800361 -0.53911901 -0.21352892 -0.35612463
  -0.30928015 -0.94471799 -0.26449285 -0.69176297  1.10470474 -0.31585982]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
   0.531249   -0.18624395  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.456921    0.24756427 -0.67481406  4.67126434  1.16025966
  -0.75053031 -0.43578594 -0.18976687 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.42717512 -0.12145761 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -1.4070134  -1.23126007 -0.65800361 -1.28544178 -0.21352892  1.28240883
  -0.13847364  1.06192837 -0.17387492  1.44558187 -1.46630395  1.0753261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  1.14789757
  -0.13243917 -0.23890927 -0.91453407 -0.08636167 -0.30417801  2.43580216
  -0.5589008  -0.89362385  1.40216332 -0.1998814  -0.21352892 -0.28673812
  -1.27718374 -0.91563616 -0.28891311 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  3.09366471
   3.6320872   2.03800863 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   2.83354957  2.53097202  2.80579354  1.02137403 -0.21352892  0.69117797
   1.48418826  4.02827516  3.35271046 -0.69176297 -0.50217569 -0.43179198]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
  -0.39900245 -0.19618036  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.09173785 -0.65800361  0.81783146 -0.21352892 -1.8125185
  -0.80746582  0.36105623 -0.22057397 -0.69176297 -0.18079961  1.30719042]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39750492 -0.79786956
  -1.11165123 -0.12824632 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.88156542 -0.65800361 -0.26772892 -0.21352892 -1.09118793
  -0.72206256 -0.49394961 -0.11192261  1.44558187 -1.78768004 -0.0839955 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
  -0.20860011 -0.20226435  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.11928478 -0.65800361  1.08922156 -0.21352892 -1.38824892
   1.29914787  0.58207815 -0.19235918  1.44558187 -1.14492787 -0.43179198]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
   1.60838227 -0.12624964 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   0.28921179  0.08310948  3.19065989 -0.1998814  -0.21352892  0.29076
   0.48781692  0.32034167 -0.14145234  1.44558187 -0.82355178 -1.47518142]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.00284989 -0.21541894 -0.91453407 -0.08636167 -0.30417801  0.9030092
  -0.5589008  -1.12273414  0.0211723  -0.98012792 -0.21352892  0.93619907
  -0.55837299 -0.95925891 -0.25134349 -0.69176297 -0.50217569  1.30719042]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 157
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
No need to shift labels.
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Number of Classes After Bin Verifier: 157
In get_device
Using dim 8 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}
Epoch 0 loss 4.662989139556885
Epoch 1 loss 4.1493144035339355
Epoch 2 loss 4.074267387390137
Epoch 3 loss 3.966214418411255
Epoch 4 loss 3.876312732696533
Epoch 5 loss 3.874207019805908
Epoch 6 loss 3.8559672832489014
Epoch 7 loss 3.817643642425537
Epoch 8 loss 3.798746347427368
Epoch 9 loss 3.821913719177246
Epoch 10 loss 3.7810373306274414
Epoch 11 loss 3.7710440158843994
Epoch 12 loss 3.774113416671753
Epoch 13 loss 3.7533786296844482
Epoch 14 loss 3.765558958053589
Epoch 15 loss 3.7604618072509766
Epoch 16 loss 3.786830186843872
Epoch 17 loss 3.774587392807007
Epoch 18 loss 3.754805564880371
Epoch 19 loss 3.7694437503814697
Epoch 20 loss 3.742968797683716
Epoch 21 loss 3.744007110595703
Epoch 22 loss 3.7428336143493652
Epoch 23 loss 3.7628581523895264
Epoch 24 loss 3.7535858154296875
Epoch 25 loss 3.76187801361084
Epoch 26 loss 3.7624313831329346
Epoch 27 loss 3.7716846466064453
Epoch 28 loss 3.749438524246216
Epoch 29 loss 3.7625391483306885
Epoch 30 loss 3.770655393600464
Epoch 31 loss 3.8111765384674072
Epoch 32 loss 3.7990946769714355
Epoch 33 loss 3.808985471725464
Epoch 34 loss 3.8090083599090576
Epoch 35 loss 3.7987167835235596
Epoch 36 loss 3.848493814468384
Epoch 37 loss 3.807851552963257
Epoch 38 loss 3.8055403232574463
Epoch 39 loss 3.8399531841278076
Epoch 40 loss 3.8567545413970947
Epoch 41 loss 3.8723678588867188
Epoch 42 loss 3.8854711055755615
Epoch 43 loss 3.875375986099243
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 157
Class label len :157
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156] 

Prediction shape : (4323,)
Probabilities shape : (4323, 157) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.78885, 'Log Loss - std': 0.04974999999999996} 
 

Fold 3
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39741079 -1.45113378
  -0.98251964 -0.23408869 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.73776602 -0.65958158 -0.55261468 -0.20509978 -0.34621257
  -0.3075573  -0.94534346 -0.25697128 -0.68738047  1.09900948 -0.30696442]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
   0.53142887 -0.19516535  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.45948399  0.24387824 -0.68858798  4.86346877  1.16478766
  -0.74817205 -0.43462874 -0.18749435 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46903157 -1.45113378
  -1.42907999 -0.12773409 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -1.41023515 -1.23359683 -0.65958158 -1.30046783 -0.20509978  1.28650312
  -0.13699675  1.06833174 -0.17271872  1.45479839 -1.47542134  1.07875341]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     1.14652152
  -0.1329658  -0.24998075 -0.92135604 -0.08636167 -0.30822085  2.44891305
  -0.56095038 -0.89498067  1.39578951 -0.21268143 -0.20509978 -0.27707243
  -1.27406708 -0.91615976 -0.27967615 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -0.15230613
  -0.43793385 -0.17467681 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   0.28833439 -0.1330943  -0.65958158  0.53517172 -0.20509978  0.41432901
   1.19906088 -0.2741184  -0.19224491  1.45479839 -1.47542134  0.27041801]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     3.09476299
   3.63556789  2.11989316 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   2.8361887   2.53955472  2.79615222  1.01107827 -0.20509978  0.69737148
   1.48332846  4.04506898  3.10614501 -0.68738047 -0.51000978 -0.4224409 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
  -0.39981284 -0.20550742  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.09076728 -0.65958158  0.80711832 -0.20509978 -1.79743539
  -0.80502557  0.3650046  -0.21613743 -0.68738047 -0.18820593  1.30970638]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39741079 -0.80171996
  -1.11322023 -0.13479994 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.88288723 -0.65958158 -0.28066808 -0.20509978 -1.07866597
  -0.71974529 -0.49299613 -0.11511819  1.45479839 -1.7972252  -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -1.45113378
  -0.32901669 -0.1895909  -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.89498067  0.98923259 -0.38264805 -0.20509978 -0.33829026
  -0.87609246 -0.30330209 -0.17093726  1.45479839 -0.83181364 -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.49710769
  -0.20920781 -0.21183979  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.12086783 -0.65958158  1.07906492 -0.20509978 -1.37467222
   1.29855453  0.58680071 -0.18990456  1.45479839 -1.15361749 -0.4224409 ]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176., 177.]), 178)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,
        56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,
        67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,
        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,
        89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.,
       100., 101., 102., 103., 104., 105., 106., 107., 109., 110., 111.,
       112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,
       123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,
       134., 135., 136., 137., 138., 139., 140., 141., 142., 143., 144.,
       145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155.,
       156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166.,
       167., 168., 169., 170., 171., 172., 173., 174., 175., 176., 177.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 178
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 176
Final Num Classes: 178
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 178
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 176
Number of Classes After Bin Verifier: 178
In get_device
Using dim 8 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}
Epoch 0 loss 4.623041152954102
Epoch 1 loss 4.187657356262207
Epoch 2 loss 4.101676940917969
Epoch 3 loss 3.999838352203369
Epoch 4 loss 3.968672037124634
Epoch 5 loss 3.953904628753662
Epoch 6 loss 3.931448459625244
Epoch 7 loss 3.9202253818511963
Epoch 8 loss 3.950360059738159
Epoch 9 loss 3.9183597564697266
Epoch 10 loss 3.9206650257110596
Epoch 11 loss 3.903435707092285
Epoch 12 loss 3.872279405593872
Epoch 13 loss 3.8763184547424316
Epoch 14 loss 3.885310411453247
Epoch 15 loss 3.897247314453125
Epoch 16 loss 3.8670969009399414
Epoch 17 loss 3.895841360092163
Epoch 18 loss 3.8525216579437256
Epoch 19 loss 3.8621740341186523
Epoch 20 loss 3.886791944503784
Epoch 21 loss 3.858147621154785
Epoch 22 loss 3.8841552734375
Epoch 23 loss 3.887608766555786
Epoch 24 loss 3.8923277854919434
Epoch 25 loss 3.923597574234009
Epoch 26 loss 3.874971389770508
Epoch 27 loss 3.8790955543518066
Epoch 28 loss 3.9283361434936523
Epoch 29 loss 3.909539222717285
Epoch 30 loss 3.9049956798553467
Epoch 31 loss 3.91332745552063
Epoch 32 loss 3.935540199279785
Epoch 33 loss 3.9309005737304688
Epoch 34 loss 3.9499566555023193
Epoch 35 loss 3.98926043510437
Epoch 36 loss 4.0025200843811035
Epoch 37 loss 4.001008987426758
Epoch 38 loss 4.010446071624756
Epoch 39 loss 4.031522274017334
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 178
Class label len :178
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177] 

Prediction shape : (4323,)
Probabilities shape : (4323, 178) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8108, 'Log Loss - std': 0.051123836580079324} 
 

Fold 4
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.40689056 -1.45158542
  -0.9793635  -0.22406432 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.73483377 -0.65814255 -0.54856633 -0.21024459 -0.35275491
  -0.30866343 -0.94078658 -0.25941786 -0.69164201  1.09428478 -0.3072261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.16718528
   0.52910389 -0.1869132   0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.4529345   0.2497395  -0.68467086  4.7430623   1.15877468
  -0.74752539 -0.42980973 -0.18731522 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.42430711 -0.12255214 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -1.41302015 -1.2267378  -0.65814255 -1.29714122 -0.21024459  1.28053279
  -0.13878138  1.07392215 -0.17198117  1.44583467 -1.46983462  1.08105208]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   1.13844771
  -0.1328854  -0.23923281 -0.91901629 -0.08601865 -0.30335866  2.45803631
  -0.56114848 -0.89080334  1.40728911 -0.20830502 -0.21024459 -0.28359054
  -1.27132837 -0.9115879  -0.28298081 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -0.15656886
  -0.43674933 -0.16735752 -0.91901629 -0.08601865 -0.30335866 -0.62818566
   0.29072319 -0.1349508  -0.65814255  0.54026987 -0.21024459  0.40805312
   1.19196133 -0.269217   -0.19224531  1.44583467 -1.46983462  0.27122314]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   3.08097255
   3.62200464  2.0227386  -0.91901629 -0.08601865 -0.30335866 -0.62818566
   2.8463382   2.5165319   2.81450628  1.01663571 -0.21024459  0.69119475
   1.47509807  4.05218722  3.23079943 -0.69164201 -0.50828985 -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.40689056 -0.80407714
  -1.1095909  -0.12929629 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.87880568 -0.65814255 -0.27635728 -0.21024459 -1.08546492
  -0.71921171 -0.48820708 -0.11220377  1.44583467 -1.79034955 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -1.45158542
  -0.3282265  -0.18159256 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.89080334  0.99874218 -0.37843568 -0.21024459 -0.34482983
  -0.87493692 -0.29841568 -0.17013239  1.44583467 -0.82880477 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.49093942
  -0.20885138 -0.20282844  0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.11700004 -0.65814255  1.08468797 -0.21024459 -1.38157486
   1.29105919  0.59214397 -0.18981652  1.44583467 -1.1493197  -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.00106806 -0.21589668 -0.91901629 -0.08601865 -0.30335866  0.91492532
  -0.56114848 -1.11875887  0.02276898 -0.99090604 -0.21024459  0.93543142
  -0.55640808 -0.95538592 -0.24673011 -0.69164201 -0.50828985  1.31243178]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 181
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}
Epoch 0 loss 4.787177085876465
Epoch 1 loss 4.288369655609131
Epoch 2 loss 4.125108242034912
Epoch 3 loss 4.046195983886719
Epoch 4 loss 4.033917427062988
Epoch 5 loss 4.004153251647949
Epoch 6 loss 3.9395666122436523
Epoch 7 loss 3.9367620944976807
Epoch 8 loss 3.9135258197784424
Epoch 9 loss 3.904772996902466
Epoch 10 loss 3.8897745609283447
Epoch 11 loss 3.870288610458374
Epoch 12 loss 3.8778040409088135
Epoch 13 loss 3.8656554222106934
Epoch 14 loss 3.866173267364502
Epoch 15 loss 3.860107421875
Epoch 16 loss 3.8398513793945312
Epoch 17 loss 3.853966236114502
Epoch 18 loss 3.8573927879333496
Epoch 19 loss 3.8687241077423096
Epoch 20 loss 3.8583643436431885
Epoch 21 loss 3.9319381713867188
Epoch 22 loss 3.859264612197876
Epoch 23 loss 3.893179416656494
Epoch 24 loss 3.8802073001861572
Epoch 25 loss 3.915261745452881
Epoch 26 loss 3.9012629985809326
Epoch 27 loss 3.866727590560913
Epoch 28 loss 3.8769302368164062
Epoch 29 loss 3.8949224948883057
Epoch 30 loss 3.8870656490325928
Epoch 31 loss 3.8895013332366943
Epoch 32 loss 3.9111530780792236
Epoch 33 loss 3.897467613220215
Epoch 34 loss 3.9186320304870605
Epoch 35 loss 3.903428077697754
Epoch 36 loss 3.9413111209869385
Epoch 37 loss 3.9808690547943115
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.81765, 'Log Loss - std': 0.045836693816199196} 
 

Fold 5
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -1.46522841e+00 -1.44959243e+00
  -1.42903253e+00 -1.24024562e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -1.41774071e+00 -1.22950778e+00
  -6.59206226e-01 -1.29724427e+00 -2.10543891e-01  1.28617272e+00
  -1.32049290e-01  1.07354326e+00 -1.71578976e-01  1.44812997e+00
  -1.46713375e+00  1.08404858e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.45215728e-01
  -4.33784316e-01 -1.72861315e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01 -1.26495482e-01
  -6.59206226e-01  5.47241157e-01 -2.10543891e-01  4.11371019e-01
   1.20397428e+00 -2.72140904e-01 -1.92417981e-01  1.44812997e+00
  -1.46713375e+00  2.72452120e-01]
 [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  1.80878448e-01
  -3.95505538e-01 -2.04935871e-01  9.43881175e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.40719324e-02
  -6.59206226e-01  8.20498257e-01 -2.10543891e-01 -1.80705676e+00
  -8.00061074e-01  3.68521773e-01 -2.17916871e-01 -6.90545751e-01
  -1.84793849e-01  1.31593328e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00 -3.97401366e-01 -7.97404081e-01
  -1.11186552e+00 -1.31375511e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.77998365e-01
  -6.59206226e-01 -2.72530142e-01 -2.10543891e-01 -1.08612191e+00
  -7.14782974e-01 -4.91545931e-01 -1.10105776e-01  1.44812997e+00
  -1.78771873e+00 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -3.24416380e-01 -1.88377159e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.90119380e-01
   9.87298486e-01 -3.75001554e-01 -2.10543891e-01 -3.43515672e-01
  -8.71126158e-01 -3.01394908e-01 -1.69677743e-01  1.44812997e+00
  -8.25963800e-01 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  5.06972624e-01
   1.62233288e+00 -1.29213467e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01  9.16827741e-02
   3.17511982e+00 -2.04215867e-01 -2.10543891e-01  2.95067902e-01
   4.93323444e-01  3.27566168e-01 -1.39407132e-01  1.44812997e+00
  -8.25963800e-01 -1.46668315e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -7.07204156e-01  1.27815383e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -4.29520838e-01
  -6.59206226e-01 -1.50218709e+00 -2.10543891e-01  1.41547867e+00
  -1.03623257e-01 -3.01394908e-01  1.20955314e-03 -6.90545751e-01
  -5.05378825e-01  1.43187563e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -4.71309904e-01
  -7.72824917e-01 -1.32164021e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -5.02246924e-01
  -6.59206226e-01  2.05669783e-01 -2.10543891e-01  3.81753455e-01
   1.20397428e+00 -9.01101980e-01 -9.15780737e-02 -6.90545751e-01
   1.09754605e+00 -1.00291374e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01  1.15916098e+00
   9.55188471e-01 -2.51203607e-01  9.43881175e-01 -8.73729514e-02
   3.63073997e+00 -6.28575252e-01  1.15214655e+00  2.37134945e-01
   1.52861510e+00  2.73984057e-01 -2.10543891e-01  8.34106764e-02
  -1.14117347e+00  2.25177156e-01 -3.23006594e-01  1.44812997e+00
  -1.78771873e+00  9.68106228e-01]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01 -1.44959243e+00
  -5.21278664e-01 -2.69008674e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -2.23463596e-01
  -6.59206226e-01 -1.87791560e+00 -2.10543891e-01  7.58113231e-01
  -9.13765208e-01 -5.50053938e-01 -3.11822870e-01 -6.90545751e-01
  -5.05378825e-01  1.54781798e+00]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 114., 115., 116., 117., 118., 119., 120., 121.,
       122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,
       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,
       144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,
       155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,
       166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 176
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 176
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}
Epoch 0 loss 4.928865909576416
Epoch 1 loss 4.411139011383057
Epoch 2 loss 4.107109546661377
Epoch 3 loss 4.027662754058838
Epoch 4 loss 4.001476764678955
Epoch 5 loss 3.9672582149505615
Epoch 6 loss 3.9316110610961914
Epoch 7 loss 3.9043710231781006
Epoch 8 loss 3.917466163635254
Epoch 9 loss 3.9248554706573486
Epoch 10 loss 3.898231029510498
Epoch 11 loss 3.868142604827881
Epoch 12 loss 3.913759469985962
Epoch 13 loss 3.8721063137054443
Epoch 14 loss 3.8747293949127197
Epoch 15 loss 3.8688409328460693
Epoch 16 loss 3.8407938480377197
Epoch 17 loss 3.8364267349243164
Epoch 18 loss 3.8488032817840576
Epoch 19 loss 3.8514404296875
Epoch 20 loss 3.8904221057891846
Epoch 21 loss 3.842357873916626
Epoch 22 loss 3.883474588394165
Epoch 23 loss 3.8627736568450928
Epoch 24 loss 3.8593852519989014
Epoch 25 loss 3.8559515476226807
Epoch 26 loss 3.851438283920288
Epoch 27 loss 3.941248655319214
Epoch 28 loss 3.8772614002227783
Epoch 29 loss 3.8744313716888428
Epoch 30 loss 3.874202013015747
Epoch 31 loss 3.8916423320770264
Epoch 32 loss 3.9255685806274414
Epoch 33 loss 3.906629800796509
Epoch 34 loss 3.901059865951538
Epoch 35 loss 3.9109973907470703
Epoch 36 loss 3.9224817752838135
Epoch 37 loss 3.943230628967285
Epoch 38 loss 3.946204662322998
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.82142, 'Log Loss - std': 0.04168517242377672} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 1 finished with value: 3.82142 and parameters: {'dim': 128, 'depth': 3, 'heads': 8, 'dropout': 0.6}. Best is trial 0 with value: 3.82958.
Best parameters After Trials: {'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Parameters saved to YAML file!!!
In get_device
Using dim 64 and batch size 64
Fold 1
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.3947275  -1.43868499
  -0.97562257 -0.22635649 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.73157015 -0.65849042 -0.53844893 -0.2111493  -0.35633908
  -0.30229791 -0.94097085 -0.26202697 -0.69194575  1.10260759 -0.31933766]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
   0.53685987 -0.18786591  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.46756773  0.24158981 -0.67496901  4.7231026   1.15996359
  -0.74230143 -0.43016988 -0.18783684 -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  1.15538522
  -0.12689142 -0.24207186 -0.91101164 -0.08969788 -0.30794673  2.43033886
  -0.55213581 -0.8890327   1.38919211 -0.19714873 -0.2111493  -0.28695631
  -1.26746693 -0.91178223 -0.2862721  -0.69194575  1.7441072  -0.78340423]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -0.14164989
  -0.43156414 -0.16760517 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258 -0.12594496 -0.65849042  0.5537117  -0.2111493   0.40687141
   1.20223026 -0.26963243 -0.19290967  1.44520001 -1.46339083  0.26074556]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67633942  3.10093788
   3.63799292  2.10145304 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   2.83973773  2.55091839  2.78431647  1.03153198 -0.2111493   0.69090714
   1.4861035   4.05028437  3.32923754 -0.69194575 -0.50114142 -0.4353543 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.18260889
  -0.39348005 -0.19809299  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.0835512  -0.65849042  0.82675186 -0.2111493  -1.81265457
  -0.79907608  0.3695985  -0.21842302 -0.69194575 -0.18039162  1.30489535]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275  -1.43868499
  -0.32275245 -0.18235344 -0.91101164 -0.08969788 -0.30794673 -0.63260734
  -0.55213581 -0.8890327   0.98415601 -0.36779883 -0.2111493  -0.34838897
  -0.87004439 -0.29882106 -0.17015654  1.44520001 -0.82189122 -0.08730437]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
  -0.2030596  -0.20435496  0.94460375 -0.08969788 -0.30794673 -0.63260734
  -0.55213581  0.12841762 -0.65849042  1.09979202 -0.2111493  -1.38840783
   1.3015859   0.59143207 -0.19041056  1.44520001 -1.14264102 -0.4353543 ]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.3947275   0.50686767
   1.61409556 -0.12611658 -0.91101164 -0.08969788 -0.30794673 -0.63260734
   0.29583258  0.09208011  3.16685057 -0.19714873 -0.2111493   0.29051072
   0.49254716  0.32873442 -0.13986877  1.44520001 -0.82189122 -1.47950409]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46579442 -1.43868499
  -0.99738491 -0.21789436 -0.91101164 -0.08969788 -0.30794673  0.89886576
  -0.55213581 -1.11917028  0.01656975 -0.98213919 -0.2111493   0.93591506
  -0.550687   -0.95556517 -0.24897189 -0.69194575 -0.50114142  1.30489535]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 178
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length: 176
Final Num Classes: 176
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175], Length : 176
Number of Classes After Bin Verifier: 176
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.507843971252441
Epoch 1 loss 4.219964504241943
Epoch 2 loss 4.028130054473877
Epoch 3 loss 3.9838857650756836
Epoch 4 loss 3.9708292484283447
Epoch 5 loss 3.9233992099761963
Epoch 6 loss 3.9383676052093506
Epoch 7 loss 3.8953802585601807
Epoch 8 loss 3.873757839202881
Epoch 9 loss 3.8724141120910645
Epoch 10 loss 3.894132375717163
Epoch 11 loss 3.8660995960235596
Epoch 12 loss 3.8770227432250977
Epoch 13 loss 3.8761448860168457
Epoch 14 loss 3.885510206222534
Epoch 15 loss 3.859423875808716
Epoch 16 loss 3.9613800048828125
Epoch 17 loss 3.888918161392212
Epoch 18 loss 3.8788232803344727
Epoch 19 loss 3.865307569503784
Epoch 20 loss 3.8650527000427246
Epoch 21 loss 3.8872487545013428
Epoch 22 loss 3.904855251312256
Epoch 23 loss 3.8913090229034424
Epoch 24 loss 3.902695655822754
Epoch 25 loss 3.921998977661133
Epoch 26 loss 3.923189401626587
Epoch 27 loss 3.962352991104126
Epoch 28 loss 3.9598257541656494
Epoch 29 loss 3.9696834087371826
Epoch 30 loss 3.975175380706787
Epoch 31 loss 3.990036964416504
Epoch 32 loss 4.039520263671875
Epoch 33 loss 4.04838228225708
Epoch 34 loss 4.099544525146484
Epoch 35 loss 4.129745960235596
Epoch 36 loss 4.147446155548096
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/House_Sales/logging/loss_0.txt
File name : output/SAINT/House_Sales/logging/loss_0.txt . The file was saved
Log file exists at: output/SAINT/House_Sales/logging/val_loss_0.txt
File name : output/SAINT/House_Sales/logging/val_loss_0.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 176
Class label len :176
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175] 

Prediction shape : (4323,)
Probabilities shape : (4323, 176) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8549, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39750492 -1.4464586
  -0.98108962 -0.22364056 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.73686418 -0.65800361 -0.53911901 -0.21352892 -0.35612463
  -0.30928015 -0.94471799 -0.26449285 -0.69176297  1.10470474 -0.31585982]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
   0.531249   -0.18624395  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.456921    0.24756427 -0.67481406  4.67126434  1.16025966
  -0.75053031 -0.43578594 -0.18976687 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.42717512 -0.12145761 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -1.4070134  -1.23126007 -0.65800361 -1.28544178 -0.21352892  1.28240883
  -0.13847364  1.06192837 -0.17387492  1.44558187 -1.46630395  1.0753261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  1.14789757
  -0.13243917 -0.23890927 -0.91453407 -0.08636167 -0.30417801  2.43580216
  -0.5589008  -0.89362385  1.40216332 -0.1998814  -0.21352892 -0.28673812
  -1.27718374 -0.91563616 -0.28891311 -0.69176297  1.74745691 -0.77958846]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67470414  3.09366471
   3.6320872   2.03800863 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   2.83354957  2.53097202  2.80579354  1.02137403 -0.21352892  0.69117797
   1.48418826  4.02827516  3.35271046 -0.69176297 -0.50217569 -0.43179198]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.17501401
  -0.39900245 -0.19618036  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.09173785 -0.65800361  0.81783146 -0.21352892 -1.8125185
  -0.80746582  0.36105623 -0.22057397 -0.69176297 -0.18079961  1.30719042]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39750492 -0.79786956
  -1.11165123 -0.12824632 -0.91453407 -0.08636167 -0.30417801 -0.62978376
  -0.5589008  -0.88156542 -0.65800361 -0.26772892 -0.21352892 -1.09118793
  -0.72206256 -0.49394961 -0.11192261  1.44558187 -1.78768004 -0.0839955 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
  -0.20860011 -0.20226435  0.93832439 -0.08636167 -0.30417801 -0.62978376
  -0.5589008   0.11928478 -0.65800361  1.08922156 -0.21352892 -1.38824892
   1.29914787  0.58207815 -0.19235918  1.44558187 -1.14492787 -0.43179198]
 [ 0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39750492  0.49930853
   1.60838227 -0.12624964 -0.91453407 -0.08636167 -0.30417801 -0.62978376
   0.28921179  0.08310948  3.19065989 -0.1998814  -0.21352892  0.29076
   0.48781692  0.32034167 -0.14145234  1.44558187 -0.82355178 -1.47518142]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46971399 -1.4464586
  -1.00284989 -0.21541894 -0.91453407 -0.08636167 -0.30417801  0.9030092
  -0.5589008  -1.12273414  0.0211723  -0.98012792 -0.21352892  0.93619907
  -0.55837299 -0.95925891 -0.25134349 -0.69176297 -0.50217569  1.30719042]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 157
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156.]), 157)
No need to shift labels.
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156], Length : 157
Number of Classes After Bin Verifier: 157
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.345574378967285
Epoch 1 loss 4.1086626052856445
Epoch 2 loss 3.9115803241729736
Epoch 3 loss 3.8946421146392822
Epoch 4 loss 3.8323562145233154
Epoch 5 loss 3.830162525177002
Epoch 6 loss 3.809678077697754
Epoch 7 loss 3.772223949432373
Epoch 8 loss 3.8031890392303467
Epoch 9 loss 3.8147451877593994
Epoch 10 loss 3.7547402381896973
Epoch 11 loss 3.774118423461914
Epoch 12 loss 3.76788592338562
Epoch 13 loss 3.7955358028411865
Epoch 14 loss 3.754828929901123
Epoch 15 loss 3.767259120941162
Epoch 16 loss 3.7580726146698
Epoch 17 loss 3.758345127105713
Epoch 18 loss 3.780771493911743
Epoch 19 loss 3.7866134643554688
Epoch 20 loss 3.7701685428619385
Epoch 21 loss 3.7882487773895264
Epoch 22 loss 3.784027099609375
Epoch 23 loss 3.805832862854004
Epoch 24 loss 3.7847537994384766
Epoch 25 loss 3.7878835201263428
Epoch 26 loss 3.846022605895996
Epoch 27 loss 3.8302855491638184
Epoch 28 loss 3.8718342781066895
Epoch 29 loss 3.8494954109191895
Epoch 30 loss 3.883941650390625
Epoch 31 loss 3.926103115081787
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/House_Sales/logging/loss_1.txt
File name : output/SAINT/House_Sales/logging/loss_1.txt . The file was saved
Log file exists at: output/SAINT/House_Sales/logging/val_loss_1.txt
File name : output/SAINT/House_Sales/logging/val_loss_1.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 157
Class label len :157
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156] 

Prediction shape : (4323,)
Probabilities shape : (4323, 157) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8035, 'Log Loss - std': 0.05140000000000011} 
 

Fold 3
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17290, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17290, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.39741079 -1.45113378
  -0.98251964 -0.23408869 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.73776602 -0.65958158 -0.55261468 -0.20509978 -0.34621257
  -0.3075573  -0.94534346 -0.25697128 -0.68738047  1.09900948 -0.30696442]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
   0.53142887 -0.19516535  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.45948399  0.24387824 -0.68858798  4.86346877  1.16478766
  -0.74817205 -0.43462874 -0.18749435 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.46903157 -1.45113378
  -1.42907999 -0.12773409 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -1.41023515 -1.23359683 -0.65958158 -1.30046783 -0.20509978  1.28650312
  -0.13699675  1.06833174 -0.17271872  1.45479839 -1.47542134  1.07875341]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     1.14652152
  -0.1329658  -0.24998075 -0.92135604 -0.08636167 -0.30822085  2.44891305
  -0.56095038 -0.89498067  1.39578951 -0.21268143 -0.20509978 -0.27707243
  -1.27406708 -0.91615976 -0.27967615 -0.68738047  1.74261719 -0.76887036]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -0.15230613
  -0.43793385 -0.17467681 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   0.28833439 -0.1330943  -0.65958158  0.53517172 -0.20509978  0.41432901
   1.19906088 -0.2741184  -0.19224491  1.45479839 -1.47542134  0.27041801]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.67421     3.09476299
   3.63556789  2.11989316 -0.92135604 -0.08636167 -0.30822085 -0.62679295
   2.8361887   2.53955472  2.79615222  1.01107827 -0.20509978  0.69737148
   1.48332846  4.04506898  3.10614501 -0.68738047 -0.51000978 -0.4224409 ]
 [ 0.          0.          1.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.17240078
  -0.39981284 -0.20550742  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.09076728 -0.65958158  0.80711832 -0.20509978 -1.79743539
  -0.80502557  0.3650046  -0.21613743 -0.68738047 -0.18820593  1.30970638]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.39741079 -0.80171996
  -1.11322023 -0.13479994 -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.88288723 -0.65958158 -0.28066808 -0.20509978 -1.07866597
  -0.71974529 -0.49299613 -0.11511819  1.45479839 -1.7972252  -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079 -1.45113378
  -0.32901669 -0.1895909  -0.92135604 -0.08636167 -0.30822085 -0.62679295
  -0.56095038 -0.89498067  0.98923259 -0.38264805 -0.20509978 -0.33829026
  -0.87609246 -0.30330209 -0.17093726  1.45479839 -0.83181364 -0.07601144]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.39741079  0.49710769
  -0.20920781 -0.21183979  0.92830967 -0.08636167 -0.30822085 -0.62679295
  -0.56095038  0.12086783 -0.65958158  1.07906492 -0.20509978 -1.37467222
   1.29855453  0.58680071 -0.18990456  1.45479839 -1.15361749 -0.4224409 ]] 
 
 
Val : (17290, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176., 177.]), 178)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,
        56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,
        67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,
        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,
        89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.,
       100., 101., 102., 103., 104., 105., 106., 107., 109., 110., 111.,
       112., 113., 114., 115., 116., 117., 118., 119., 120., 121., 122.,
       123., 124., 125., 126., 127., 128., 129., 130., 131., 132., 133.,
       134., 135., 136., 137., 138., 139., 140., 141., 142., 143., 144.,
       145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155.,
       156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166.,
       167., 168., 169., 170., 171., 172., 173., 174., 175., 176., 177.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 178
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length: 176
Final Num Classes: 178
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 178
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177], Length : 176
Number of Classes After Bin Verifier: 178
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.479282379150391
Epoch 1 loss 4.111973285675049
Epoch 2 loss 4.031932830810547
Epoch 3 loss 4.0026044845581055
Epoch 4 loss 3.983236789703369
Epoch 5 loss 3.917494773864746
Epoch 6 loss 3.893080234527588
Epoch 7 loss 3.8870904445648193
Epoch 8 loss 3.8916118144989014
Epoch 9 loss 3.8688929080963135
Epoch 10 loss 3.949357032775879
Epoch 11 loss 3.892286539077759
Epoch 12 loss 3.8543643951416016
Epoch 13 loss 3.8575072288513184
Epoch 14 loss 3.881922483444214
Epoch 15 loss 3.8657896518707275
Epoch 16 loss 3.852630138397217
Epoch 17 loss 3.8704309463500977
Epoch 18 loss 3.8870961666107178
Epoch 19 loss 3.8893306255340576
Epoch 20 loss 3.876063585281372
Epoch 21 loss 3.906477212905884
Epoch 22 loss 3.896629810333252
Epoch 23 loss 3.912057399749756
Epoch 24 loss 3.9113988876342773
Epoch 25 loss 3.9284870624542236
Epoch 26 loss 3.9385743141174316
Epoch 27 loss 3.968794345855713
Epoch 28 loss 3.96382737159729
Epoch 29 loss 3.987441062927246
Epoch 30 loss 3.993880271911621
Epoch 31 loss 4.024786472320557
Epoch 32 loss 4.033858299255371
Epoch 33 loss 4.091280937194824
Epoch 34 loss 4.153227806091309
Epoch 35 loss 4.119843006134033
Epoch 36 loss 4.173725128173828
Epoch 37 loss 4.226683139801025
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/House_Sales/logging/loss_2.txt
File name : output/SAINT/House_Sales/logging/loss_2.txt . The file was saved
Log file exists at: output/SAINT/House_Sales/logging/val_loss_2.txt
File name : output/SAINT/House_Sales/logging/val_loss_2.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 178
Class label len :178
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177] 

Prediction shape : (4323,)
Probabilities shape : (4323, 178) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8200333333333334, 'Log Loss - std': 0.04804174111008982} 
 

Fold 4
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.         -0.40689056 -1.45158542
  -0.9793635  -0.22406432 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.73483377 -0.65814255 -0.54856633 -0.21024459 -0.35275491
  -0.30866343 -0.94078658 -0.25941786 -0.69164201  1.09428478 -0.3072261 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.16718528
   0.52910389 -0.1869132   0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.4529345   0.2497395  -0.68467086  4.7430623   1.15877468
  -0.74752539 -0.42980973 -0.18731522 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.42430711 -0.12255214 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -1.41302015 -1.2267378  -0.65814255 -1.29714122 -0.21024459  1.28053279
  -0.13878138  1.07392215 -0.17198117  1.44583467 -1.46983462  1.08105208]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          1.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   1.13844771
  -0.1328854  -0.23923281 -0.91901629 -0.08601865 -0.30335866  2.45803631
  -0.56114848 -0.89080334  1.40728911 -0.20830502 -0.21024459 -0.28359054
  -1.27132837 -0.9115879  -0.28298081 -0.69164201  1.73531463 -0.76998549]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -0.15656886
  -0.43674933 -0.16735752 -0.91901629 -0.08601865 -0.30335866 -0.62818566
   0.29072319 -0.1349508  -0.65814255  0.54026987 -0.21024459  0.40805312
   1.19196133 -0.269217   -0.19224531  1.44583467 -1.46983462  0.27122314]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.6871142   3.08097255
   3.62200464  2.0227386  -0.91901629 -0.08601865 -0.30335866 -0.62818566
   2.8463382   2.5165319   2.81450628  1.01663571 -0.21024459  0.69119475
   1.47509807  4.05218722  3.23079943 -0.69164201 -0.50828985 -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          1.          0.         -0.40689056 -0.80407714
  -1.1095909  -0.12929629 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.87880568 -0.65814255 -0.27635728 -0.21024459 -1.08546492
  -0.71921171 -0.48820708 -0.11220377  1.44583467 -1.79034955 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   1.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056 -1.45158542
  -0.3282265  -0.18159256 -0.91901629 -0.08601865 -0.30335866 -0.62818566
  -0.56114848 -0.89080334  0.99874218 -0.37843568 -0.21024459 -0.34482983
  -0.87493692 -0.29841568 -0.17013239  1.44583467 -0.82880477 -0.0758464 ]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          1.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.40689056  0.49093942
  -0.20885138 -0.20282844  0.92755982 -0.08601865 -0.30335866 -0.62818566
  -0.56114848  0.11700004 -0.65814255  1.08468797 -0.21024459 -1.38157486
   1.29105919  0.59214397 -0.18981652  1.44583467 -1.1493197  -0.42291595]
 [ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          1.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -1.50089531 -1.45158542
  -1.00106806 -0.21589668 -0.91901629 -0.08601865 -0.30335866  0.91492532
  -0.56114848 -1.11875887  0.02276898 -0.99090604 -0.21024459  0.93543142
  -0.55640808 -0.95538592 -0.24673011 -0.69164201 -0.50828985  1.31243178]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 181
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.563717365264893
Epoch 1 loss 4.175202369689941
Epoch 2 loss 4.03532075881958
Epoch 3 loss 4.004167079925537
Epoch 4 loss 3.95941424369812
Epoch 5 loss 3.9508347511291504
Epoch 6 loss 3.931238889694214
Epoch 7 loss 3.90063738822937
Epoch 8 loss 3.9011576175689697
Epoch 9 loss 3.8832645416259766
Epoch 10 loss 3.8793277740478516
Epoch 11 loss 3.8844752311706543
Epoch 12 loss 3.903571128845215
Epoch 13 loss 3.873800754547119
Epoch 14 loss 3.8858957290649414
Epoch 15 loss 3.8613905906677246
Epoch 16 loss 3.85941219329834
Epoch 17 loss 3.8672165870666504
Epoch 18 loss 3.87200665473938
Epoch 19 loss 3.880990982055664
Epoch 20 loss 3.9019079208374023
Epoch 21 loss 3.885035753250122
Epoch 22 loss 3.892207384109497
Epoch 23 loss 3.8950607776641846
Epoch 24 loss 3.968266248703003
Epoch 25 loss 3.9109017848968506
Epoch 26 loss 3.9452946186065674
Epoch 27 loss 3.948883533477783
Epoch 28 loss 3.9657883644104004
Epoch 29 loss 3.9845662117004395
Epoch 30 loss 3.9895150661468506
Epoch 31 loss 3.992398977279663
Epoch 32 loss 4.025534629821777
Epoch 33 loss 4.020856857299805
Epoch 34 loss 4.078305244445801
Epoch 35 loss 4.092274188995361
Epoch 36 loss 4.129771709442139
Epoch 37 loss 4.1699652671813965
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/House_Sales/logging/loss_3.txt
File name : output/SAINT/House_Sales/logging/loss_3.txt . The file was saved
Log file exists at: output/SAINT/House_Sales/logging/val_loss_3.txt
File name : output/SAINT/House_Sales/logging/val_loss_3.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.828925, 'Log Loss - std': 0.044364308571192695} 
 

Fold 5
num_features : 21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :21
num_classes : 1
cat_idx : [0]
nominal_idx : [13]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (17291, 21)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20]
Cat Dims V1 : []
Cat Idx V1 : [13] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]] 
 
 
Val : (17291, 90) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -1.46522841e+00 -1.44959243e+00
  -1.42903253e+00 -1.24024562e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -1.41774071e+00 -1.22950778e+00
  -6.59206226e-01 -1.29724427e+00 -2.10543891e-01  1.28617272e+00
  -1.32049290e-01  1.07354326e+00 -1.71578976e-01  1.44812997e+00
  -1.46713375e+00  1.08404858e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.45215728e-01
  -4.33784316e-01 -1.72861315e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01 -1.26495482e-01
  -6.59206226e-01  5.47241157e-01 -2.10543891e-01  4.11371019e-01
   1.20397428e+00 -2.72140904e-01 -1.92417981e-01  1.44812997e+00
  -1.46713375e+00  2.72452120e-01]
 [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  1.80878448e-01
  -3.95505538e-01 -2.04935871e-01  9.43881175e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.40719324e-02
  -6.59206226e-01  8.20498257e-01 -2.10543891e-01 -1.80705676e+00
  -8.00061074e-01  3.68521773e-01 -2.17916871e-01 -6.90545751e-01
  -1.84793849e-01  1.31593328e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00 -3.97401366e-01 -7.97404081e-01
  -1.11186552e+00 -1.31375511e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.77998365e-01
  -6.59206226e-01 -2.72530142e-01 -2.10543891e-01 -1.08612191e+00
  -7.14782974e-01 -4.91545931e-01 -1.10105776e-01  1.44812997e+00
  -1.78771873e+00 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -3.24416380e-01 -1.88377159e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01 -5.61111621e-01 -8.90119380e-01
   9.87298486e-01 -3.75001554e-01 -2.10543891e-01 -3.43515672e-01
  -8.71126158e-01 -3.01394908e-01 -1.69677743e-01  1.44812997e+00
  -8.25963800e-01 -7.53749332e-02]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01  5.06972624e-01
   1.62233288e+00 -1.29213467e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01 -6.28575252e-01  2.95517466e-01  9.16827741e-02
   3.17511982e+00 -2.04215867e-01 -2.10543891e-01  2.95067902e-01
   4.93323444e-01  3.27566168e-01 -1.39407132e-01  1.44812997e+00
  -8.25963800e-01 -1.46668315e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -1.44959243e+00
  -7.07204156e-01  1.27815383e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -4.29520838e-01
  -6.59206226e-01 -1.50218709e+00 -2.10543891e-01  1.41547867e+00
  -1.03623257e-01 -3.01394908e-01  1.20955314e-03 -6.90545751e-01
  -5.05378825e-01  1.43187563e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -3.97401366e-01 -4.71309904e-01
  -7.72824917e-01 -1.32164021e-01 -9.11265105e-01 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -5.02246924e-01
  -6.59206226e-01  2.05669783e-01 -2.10543891e-01  3.81753455e-01
   1.20397428e+00 -9.01101980e-01 -9.15780737e-02 -6.90545751e-01
   1.09754605e+00 -1.00291374e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01  1.15916098e+00
   9.55188471e-01 -2.51203607e-01  9.43881175e-01 -8.73729514e-02
   3.63073997e+00 -6.28575252e-01  1.15214655e+00  2.37134945e-01
   1.52861510e+00  2.73984057e-01 -2.10543891e-01  8.34106764e-02
  -1.14117347e+00  2.25177156e-01 -3.23006594e-01  1.44812997e+00
  -1.78771873e+00  9.68106228e-01]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
   0.00000000e+00  0.00000000e+00  6.70425676e-01 -1.44959243e+00
  -5.21278664e-01 -2.69008674e-01  1.63080351e-02 -8.73729514e-02
  -3.05090810e-01  9.10028186e-01 -5.61111621e-01 -2.23463596e-01
  -6.59206226e-01 -1.87791560e+00 -2.10543891e-01  7.58113231e-01
  -9.13765208e-01 -5.50053938e-01 -3.11822870e-01 -6.90545751e-01
  -5.05378825e-01  1.54781798e+00]] 
 
 
Val : (17291, 90) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 180
Unique values in y_train: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,
       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,
       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,
       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,
       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,
       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,
       176.]), 177)
Unique values in y_test: (array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,
        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,
        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,
        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,
        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,
        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,
        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,
        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,
       110., 111., 112., 114., 115., 116., 117., 118., 119., 120., 121.,
       122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,
       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,
       144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,
       155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,
       166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.]), 176)
WE ARE IN THE GUTTERS!!!!!
Final Train Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 177
Final Test Labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length: 176
Final Num Classes: 177
Final Bin Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
VERIFY SHIFT
Train after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 177
Test after shift : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176], Length : 176
Number of Classes After Bin Verifier: 177
In get_device
Using dim 8 and batch size 64
{'dim': 64, 'depth': 2, 'heads': 2, 'dropout': 0.2}
Epoch 0 loss 4.495339870452881
Epoch 1 loss 4.112598419189453
Epoch 2 loss 4.0442094802856445
Epoch 3 loss 3.9921858310699463
Epoch 4 loss 3.968458414077759
Epoch 5 loss 3.9224233627319336
Epoch 6 loss 3.897588014602661
Epoch 7 loss 3.902761936187744
Epoch 8 loss 3.9063475131988525
Epoch 9 loss 3.8889856338500977
Epoch 10 loss 3.8669652938842773
Epoch 11 loss 3.8981566429138184
Epoch 12 loss 3.8704049587249756
Epoch 13 loss 3.880606174468994
Epoch 14 loss 3.8614726066589355
Epoch 15 loss 3.873670816421509
Epoch 16 loss 3.8617706298828125
Epoch 17 loss 3.872476577758789
Epoch 18 loss 3.890882968902588
Epoch 19 loss 3.8779749870300293
Epoch 20 loss 3.8594624996185303
Epoch 21 loss 3.8863275051116943
Epoch 22 loss 3.8829798698425293
Epoch 23 loss 3.8846516609191895
Epoch 24 loss 3.9056265354156494
Epoch 25 loss 3.900994300842285
Epoch 26 loss 3.9055957794189453
Epoch 27 loss 3.9018614292144775
Epoch 28 loss 3.914886951446533
Epoch 29 loss 3.935040235519409
Epoch 30 loss 3.98520565032959
Epoch 31 loss 4.000500679016113
Epoch 32 loss 3.964519739151001
Epoch 33 loss 3.9820830821990967
Epoch 34 loss 4.011275768280029
Epoch 35 loss 4.054294586181641
Epoch 36 loss 4.044763565063477
Epoch 37 loss 4.071378231048584
Epoch 38 loss 4.131795883178711
Epoch 39 loss 4.1765055656433105
Epoch 40 loss 4.185220241546631
Epoch 41 loss 4.253093719482422
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/House_Sales/logging/loss_4.txt
File name : output/SAINT/House_Sales/logging/loss_4.txt . The file was saved
Log file exists at: output/SAINT/House_Sales/logging/val_loss_4.txt
File name : output/SAINT/House_Sales/logging/val_loss_4.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 177
Class label len :177
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]
Unique y_true : [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176] 

Prediction shape : (4322,)
Probabilities shape : (4322, 177) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8348199999999997, 'Log Loss - std': 0.04139513981133536} 
 

Saving model.....
Results After CV: {'Log Loss - mean': 3.8348199999999997, 'Log Loss - std': 0.04139513981133536}
Train time: 1849.2249222061996
Inference time: 0.7949141497992969
Finished cross validation
Loss path :output/SAINT/House_Sales/logging/
Plots saved successfully!


----------------------------------------------------------------------------
Training SAINT Vesion 1 with Dataset: config/mip_2016.yml 



----------------------------------------------------------------------------
Namespace(batch_size=64, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/mip_2016.yml', data_parallel=False, dataset='MIP', direction='maximize', dropna_idx=[0], early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='SAINT', n_trials=2, nominal_idx=[145, 146], num_bins=10, num_classes=1, num_features=147, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=128, y_distribution='bimodial')
Start hyperparameter optimization
Loading dataset MIP...
Dataset loaded! 

(1090, 146)
A new study created in RDB with name: SAINT_MIP
In get_device
Using dim 8 and batch size 64
Fold 1
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 1.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 ...
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 1.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  1.,  3.,  6., 17., 20., 21., 22., 24., 27., 31., 38., 41.,
       42., 43., 44., 45., 46., 47., 50., 52., 54., 55., 58., 59., 60.,
       61., 62., 64., 65., 66., 67., 69., 70., 71., 72., 73., 75., 77.,
       78., 80., 84., 85., 86.]), 44)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86], Length : 44
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}
Epoch 0 loss 3.7805771827697754
Epoch 1 loss 3.348862648010254
Epoch 2 loss 3.175136089324951
Epoch 3 loss 3.089223861694336
Epoch 4 loss 2.9964499473571777
Epoch 5 loss 2.7685952186584473
Epoch 6 loss 2.64957857131958
Epoch 7 loss 2.5980372428894043
Epoch 8 loss 2.5825624465942383
Epoch 9 loss 2.603158473968506
Epoch 10 loss 2.5587339401245117
Epoch 11 loss 2.531881093978882
Epoch 12 loss 2.5404434204101562
Epoch 13 loss 2.4852912425994873
Epoch 14 loss 2.506434917449951
Epoch 15 loss 2.598951578140259
Epoch 16 loss 2.4916317462921143
Epoch 17 loss 2.4814791679382324
Epoch 18 loss 2.5427379608154297
Epoch 19 loss 2.505324363708496
Epoch 20 loss 2.4806132316589355
Epoch 21 loss 2.5529003143310547
Epoch 22 loss 2.538968086242676
Epoch 23 loss 2.510024070739746
Epoch 24 loss 2.573211669921875
Epoch 25 loss 2.436330556869507
Epoch 26 loss 2.5363640785217285
Epoch 27 loss 2.4328603744506836
Epoch 28 loss 2.464388370513916
Epoch 29 loss 2.4873809814453125
Epoch 30 loss 2.552677631378174
Epoch 31 loss 2.56854248046875
Epoch 32 loss 2.4392263889312744
Epoch 33 loss 2.526033401489258
Epoch 34 loss 2.552614688873291
Epoch 35 loss 2.4866437911987305
Epoch 36 loss 2.5185365676879883
Epoch 37 loss 2.6386399269104004
Epoch 38 loss 2.6963510513305664
Epoch 39 loss 2.661259651184082
Epoch 40 loss 2.693693161010742
Epoch 41 loss 2.6356735229492188
Epoch 42 loss 2.8139398097991943
Epoch 43 loss 2.7514472007751465
Epoch 44 loss 2.7201690673828125
Epoch 45 loss 2.7927675247192383
Epoch 46 loss 2.854781150817871
Epoch 47 loss 2.7798218727111816
Epoch 48 loss 2.899848461151123
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.4685, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 1.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 ...
 [0.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [1.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [0.0 1.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 83
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82.]), 83)
Unique values in y_test: (array([ 3.,  4.,  6., 11., 15., 16., 17., 22., 23., 26., 27., 34., 35.,
       36., 37., 38., 39., 42., 56., 57., 58., 59., 60., 61., 62., 63.,
       65., 66., 68., 69., 71., 73., 74., 75., 78., 79., 81., 82.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82], Length : 83
Test after shift : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82], Length : 38
Number of Classes After Bin Verifier: 83
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}
Epoch 0 loss 3.507711887359619
Epoch 1 loss 3.107365131378174
Epoch 2 loss 2.9221420288085938
Epoch 3 loss 2.7430617809295654
Epoch 4 loss 2.608696460723877
Epoch 5 loss 2.536505699157715
Epoch 6 loss 2.4631123542785645
Epoch 7 loss 2.469252586364746
Epoch 8 loss 2.4571595191955566
Epoch 9 loss 2.4312562942504883
Epoch 10 loss 2.4778366088867188
Epoch 11 loss 2.4476492404937744
Epoch 12 loss 2.3677210807800293
Epoch 13 loss 2.3386049270629883
Epoch 14 loss 2.368279457092285
Epoch 15 loss 2.403743267059326
Epoch 16 loss 2.3705825805664062
Epoch 17 loss 2.2701196670532227
Epoch 18 loss 2.392057180404663
Epoch 19 loss 2.342817783355713
Epoch 20 loss 2.309938907623291
Epoch 21 loss 2.3373818397521973
Epoch 22 loss 2.2481164932250977
Epoch 23 loss 2.3368992805480957
Epoch 24 loss 2.284569501876831
Epoch 25 loss 2.2979419231414795
Epoch 26 loss 2.2344131469726562
Epoch 27 loss 2.2710208892822266
Epoch 28 loss 2.279775381088257
Epoch 29 loss 2.3222711086273193
Epoch 30 loss 2.3029894828796387
Epoch 31 loss 2.2880523204803467
Epoch 32 loss 2.428217887878418
Epoch 33 loss 2.3533456325531006
Epoch 34 loss 2.402102470397949
Epoch 35 loss 2.2790772914886475
Epoch 36 loss 2.376110553741455
Epoch 37 loss 2.37669038772583
Epoch 38 loss 2.3473668098449707
Epoch 39 loss 2.3813722133636475
Epoch 40 loss 2.3322877883911133
Epoch 41 loss 2.3834900856018066
Epoch 42 loss 2.429522752761841
Epoch 43 loss 2.448111057281494
Epoch 44 loss 2.619478702545166
Epoch 45 loss 2.5843124389648438
Epoch 46 loss 2.4465017318725586
Epoch 47 loss 2.619781494140625
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 83
Class label len :83
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
Unique y_true : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82] 

Prediction shape : (218,)
Probabilities shape : (218, 83) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.36415, 'Log Loss - std': 0.10435000000000016} 
 

Fold 3
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 1.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 ...
 [0.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [1.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [0.0 0.0 0.0 ... -0.14328869268055042 0.9115819464298488
  0.05213966245741688]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 76
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75.]), 76)
Unique values in y_test: (array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 23., 24., 26., 27.,
       28., 32., 33., 36., 37., 38., 39., 40., 41., 42., 58., 59., 60.,
       61., 62., 63., 64., 65., 69., 70., 71., 72., 73., 74., 75.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75], Length : 76
Test after shift : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75], Length : 38
Number of Classes After Bin Verifier: 76
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}
Epoch 0 loss 3.612412214279175
Epoch 1 loss 3.0242788791656494
Epoch 2 loss 2.679043769836426
Epoch 3 loss 2.4146790504455566
Epoch 4 loss 2.3564960956573486
Epoch 5 loss 2.3109517097473145
Epoch 6 loss 2.3499436378479004
Epoch 7 loss 2.2641611099243164
Epoch 8 loss 2.2685837745666504
Epoch 9 loss 2.261366605758667
Epoch 10 loss 2.2930331230163574
Epoch 11 loss 2.261213779449463
Epoch 12 loss 2.2422523498535156
Epoch 13 loss 2.165282964706421
Epoch 14 loss 2.174649477005005
Epoch 15 loss 2.17722225189209
Epoch 16 loss 2.149491548538208
Epoch 17 loss 2.2228806018829346
Epoch 18 loss 2.1907763481140137
Epoch 19 loss 2.0565714836120605
Epoch 20 loss 2.0856406688690186
Epoch 21 loss 2.111398696899414
Epoch 22 loss 2.0615837574005127
Epoch 23 loss 2.06880259513855
Epoch 24 loss 2.002479076385498
Epoch 25 loss 2.0773072242736816
Epoch 26 loss 2.036227226257324
Epoch 27 loss 2.053910255432129
Epoch 28 loss 2.0956308841705322
Epoch 29 loss 1.982932686805725
Epoch 30 loss 2.0905184745788574
Epoch 31 loss 1.9333631992340088
Epoch 32 loss 2.0018670558929443
Epoch 33 loss 1.9544636011123657
Epoch 34 loss 1.9560925960540771
Epoch 35 loss 1.9677385091781616
Epoch 36 loss 1.9318015575408936
Epoch 37 loss 2.1651389598846436
Epoch 38 loss 2.044628143310547
Epoch 39 loss 2.0656955242156982
Epoch 40 loss 1.9735097885131836
Epoch 41 loss 1.9371297359466553
Epoch 42 loss 1.9974502325057983
Epoch 43 loss 1.945758581161499
Epoch 44 loss 2.1089494228363037
Epoch 45 loss 1.906484842300415
Epoch 46 loss 2.0057413578033447
Epoch 47 loss 2.039562225341797
Epoch 48 loss 2.0613231658935547
Epoch 49 loss 2.086575508117676
Epoch 50 loss 2.033921718597412
Epoch 51 loss 2.0380499362945557
Epoch 52 loss 2.229038715362549
Epoch 53 loss 2.128479480743408
Epoch 54 loss 2.159111499786377
Epoch 55 loss 2.143794298171997
Epoch 56 loss 2.2913312911987305
Epoch 57 loss 2.0937960147857666
Epoch 58 loss 2.0636796951293945
Epoch 59 loss 2.1775643825531006
Epoch 60 loss 2.094179391860962
Epoch 61 loss 2.26167368888855
Epoch 62 loss 2.238780975341797
Epoch 63 loss 2.372205972671509
Epoch 64 loss 2.179349422454834
Epoch 65 loss 2.4243507385253906
Epoch 66 loss 2.457550525665283
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 76
Class label len :76
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
Unique y_true : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75] 

Prediction shape : (218,)
Probabilities shape : (218, 76) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2228666666666665, 'Log Loss - std': 0.21721243569883897} 
 

Fold 4
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 1.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 ...
 [0.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [0.0 0.0 1.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [1.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  4.,  5.,  8.,  9., 13., 17., 19., 28., 32., 33., 39.,
       40., 41., 42., 43., 44., 45., 56., 57., 59., 60., 61., 62., 63.,
       64., 65., 66., 67., 68., 69., 71., 72., 73., 74., 75., 82., 85.,
       87., 88., 90.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}
Epoch 0 loss 4.097358703613281
Epoch 1 loss 3.4462852478027344
Epoch 2 loss 2.7815754413604736
Epoch 3 loss 2.636138916015625
Epoch 4 loss 2.5546436309814453
Epoch 5 loss 2.526843547821045
Epoch 6 loss 2.388864040374756
Epoch 7 loss 2.4505841732025146
Epoch 8 loss 2.466919422149658
Epoch 9 loss 2.429867744445801
Epoch 10 loss 2.4973268508911133
Epoch 11 loss 2.422365665435791
Epoch 12 loss 2.418320655822754
Epoch 13 loss 2.3828916549682617
Epoch 14 loss 2.3943893909454346
Epoch 15 loss 2.402125597000122
Epoch 16 loss 2.4191324710845947
Epoch 17 loss 2.372488021850586
Epoch 18 loss 2.3504366874694824
Epoch 19 loss 2.368350028991699
Epoch 20 loss 2.3746376037597656
Epoch 21 loss 2.350590229034424
Epoch 22 loss 2.336242198944092
Epoch 23 loss 2.4482555389404297
Epoch 24 loss 2.434630870819092
Epoch 25 loss 2.4032092094421387
Epoch 26 loss 2.3386876583099365
Epoch 27 loss 2.3247547149658203
Epoch 28 loss 2.2834362983703613
Epoch 29 loss 2.293562173843384
Epoch 30 loss 2.305452823638916
Epoch 31 loss 2.3191399574279785
Epoch 32 loss 2.352444648742676
Epoch 33 loss 2.296114683151245
Epoch 34 loss 2.296027660369873
Epoch 35 loss 2.3328232765197754
Epoch 36 loss 2.2387497425079346
Epoch 37 loss 2.327735662460327
Epoch 38 loss 2.4375154972076416
Epoch 39 loss 2.308222770690918
Epoch 40 loss 2.4354140758514404
Epoch 41 loss 2.435248851776123
Epoch 42 loss 2.3798060417175293
Epoch 43 loss 2.401381015777588
Epoch 44 loss 2.4911904335021973
Epoch 45 loss 2.4288487434387207
Epoch 46 loss 2.5192294120788574
Epoch 47 loss 2.5261762142181396
Epoch 48 loss 2.637556552886963
Epoch 49 loss 2.573275566101074
Epoch 50 loss 2.6801681518554688
Epoch 51 loss 2.592029571533203
Epoch 52 loss 2.792447566986084
Epoch 53 loss 2.7421157360076904
Epoch 54 loss 2.785940647125244
Epoch 55 loss 2.865248680114746
Epoch 56 loss 2.8250503540039062
Epoch 57 loss 2.9240803718566895
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.231275, 'Log Loss - std': 0.18867440704822694} 
 

Fold 5
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 1.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 0.0 0.0 ... -0.17343812715392498 0.8883649744794527
  -0.07440284126323692]
 ...
 [0.0 0.0 1.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [0.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [1.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  3.,  4.,  8., 10., 13., 15., 19., 31., 36., 37., 40.,
       41., 42., 43., 44., 45., 46., 47., 48., 49., 54., 59., 62., 63.,
       64., 65., 66., 67., 68., 69., 70., 71., 72., 79., 80., 83., 85.,
       87., 88., 89.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}
Epoch 0 loss 4.124877452850342
Epoch 1 loss 3.633939266204834
Epoch 2 loss 3.1932132244110107
Epoch 3 loss 3.018415927886963
Epoch 4 loss 2.888906478881836
Epoch 5 loss 2.727388858795166
Epoch 6 loss 2.705690622329712
Epoch 7 loss 2.621082305908203
Epoch 8 loss 2.6667838096618652
Epoch 9 loss 2.615250587463379
Epoch 10 loss 2.6134705543518066
Epoch 11 loss 2.5631184577941895
Epoch 12 loss 2.5939321517944336
Epoch 13 loss 2.5125975608825684
Epoch 14 loss 2.5081398487091064
Epoch 15 loss 2.529966354370117
Epoch 16 loss 2.5547561645507812
Epoch 17 loss 2.489889621734619
Epoch 18 loss 2.470775842666626
Epoch 19 loss 2.450087547302246
Epoch 20 loss 2.4435267448425293
Epoch 21 loss 2.4422216415405273
Epoch 22 loss 2.425337791442871
Epoch 23 loss 2.4665279388427734
Epoch 24 loss 2.392509937286377
Epoch 25 loss 2.3962440490722656
Epoch 26 loss 2.3411474227905273
Epoch 27 loss 2.3874459266662598
Epoch 28 loss 2.336740016937256
Epoch 29 loss 2.311563491821289
Epoch 30 loss 2.330110549926758
Epoch 31 loss 2.2937023639678955
Epoch 32 loss 2.327516555786133
Epoch 33 loss 2.299220561981201
Epoch 34 loss 2.258820056915283
Epoch 35 loss 2.3137526512145996
Epoch 36 loss 2.36039400100708
Epoch 37 loss 2.3678529262542725
Epoch 38 loss 2.3127236366271973
Epoch 39 loss 2.2914228439331055
Epoch 40 loss 2.3409957885742188
Epoch 41 loss 2.3095433712005615
Epoch 42 loss 2.3762779235839844
Epoch 43 loss 2.2563536167144775
Epoch 44 loss 2.407397747039795
Epoch 45 loss 2.2995896339416504
Epoch 46 loss 2.396165370941162
Epoch 47 loss 2.4761974811553955
Epoch 48 loss 2.45792555809021
Epoch 49 loss 2.4119529724121094
Epoch 50 loss 2.43660306930542
Epoch 51 loss 2.6309525966644287
Epoch 52 loss 2.4499640464782715
Epoch 53 loss 2.399336338043213
Epoch 54 loss 2.587049961090088
Epoch 55 loss 2.5808422565460205
Epoch 56 loss 2.607604742050171
Epoch 57 loss 2.5772504806518555
Epoch 58 loss 2.6230955123901367
Epoch 59 loss 2.669614315032959
Epoch 60 loss 2.6964619159698486
Epoch 61 loss 2.850166082382202
Epoch 62 loss 2.710723638534546
Epoch 63 loss 2.7726542949676514
Epoch 64 loss 2.8352856636047363
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2432600000000003, 'Log Loss - std': 0.1704493660885837} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 0 finished with value: 2.2432600000000003 and parameters: {'dim': 32, 'depth': 6, 'heads': 8, 'dropout': 0.4}. Best is trial 0 with value: 2.2432600000000003.
In get_device
Using dim 8 and batch size 64
Fold 1
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 1.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 ...
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 1.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  1.,  3.,  6., 17., 20., 21., 22., 24., 27., 31., 38., 41.,
       42., 43., 44., 45., 46., 47., 50., 52., 54., 55., 58., 59., 60.,
       61., 62., 64., 65., 66., 67., 69., 70., 71., 72., 73., 75., 77.,
       78., 80., 84., 85., 86.]), 44)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86], Length : 44
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.7591609954833984
Epoch 1 loss 3.3072288036346436
Epoch 2 loss 3.0502614974975586
Epoch 3 loss 2.73211407661438
Epoch 4 loss 2.5517334938049316
Epoch 5 loss 2.5075809955596924
Epoch 6 loss 2.5403432846069336
Epoch 7 loss 2.5141754150390625
Epoch 8 loss 2.5815393924713135
Epoch 9 loss 2.595447063446045
Epoch 10 loss 2.562525987625122
Epoch 11 loss 2.5355212688446045
Epoch 12 loss 2.515693187713623
Epoch 13 loss 2.48291015625
Epoch 14 loss 2.5149478912353516
Epoch 15 loss 2.4550724029541016
Epoch 16 loss 2.49910831451416
Epoch 17 loss 2.523977279663086
Epoch 18 loss 2.4616165161132812
Epoch 19 loss 2.448410987854004
Epoch 20 loss 2.574012517929077
Epoch 21 loss 2.4973349571228027
Epoch 22 loss 2.429388999938965
Epoch 23 loss 2.4822587966918945
Epoch 24 loss 2.429771900177002
Epoch 25 loss 2.554986000061035
Epoch 26 loss 2.430091619491577
Epoch 27 loss 2.4634146690368652
Epoch 28 loss 2.573967456817627
Epoch 29 loss 2.464995861053467
Epoch 30 loss 2.457277774810791
Epoch 31 loss 2.5012545585632324
Epoch 32 loss 2.4833176136016846
Epoch 33 loss 2.474851608276367
Epoch 34 loss 2.5458900928497314
Epoch 35 loss 2.551568031311035
Epoch 36 loss 2.4956202507019043
Epoch 37 loss 2.3949809074401855
Epoch 38 loss 2.5490715503692627
Epoch 39 loss 2.4706764221191406
Epoch 40 loss 2.540642261505127
Epoch 41 loss 2.6635446548461914
Epoch 42 loss 2.5612869262695312
Epoch 43 loss 2.5687241554260254
Epoch 44 loss 2.6795198917388916
Epoch 45 loss 2.4911763668060303
Epoch 46 loss 2.5315051078796387
Epoch 47 loss 2.5743727684020996
Epoch 48 loss 2.749917984008789
Epoch 49 loss 2.6398534774780273
Epoch 50 loss 2.6960806846618652
Epoch 51 loss 2.656768321990967
Epoch 52 loss 2.617161750793457
Epoch 53 loss 2.657306671142578
Epoch 54 loss 2.7700300216674805
Epoch 55 loss 2.7548272609710693
Epoch 56 loss 2.7968344688415527
Epoch 57 loss 2.7167086601257324
Epoch 58 loss 2.837327241897583
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.4402, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 1.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 ...
 [0.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [1.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [0.0 1.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 83
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82.]), 83)
Unique values in y_test: (array([ 3.,  4.,  6., 11., 15., 16., 17., 22., 23., 26., 27., 34., 35.,
       36., 37., 38., 39., 42., 56., 57., 58., 59., 60., 61., 62., 63.,
       65., 66., 68., 69., 71., 73., 74., 75., 78., 79., 81., 82.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82], Length : 83
Test after shift : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82], Length : 38
Number of Classes After Bin Verifier: 83
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.7139744758605957
Epoch 1 loss 3.231656551361084
Epoch 2 loss 2.901503562927246
Epoch 3 loss 2.5909643173217773
Epoch 4 loss 2.5094637870788574
Epoch 5 loss 2.511676549911499
Epoch 6 loss 2.4804530143737793
Epoch 7 loss 2.439850330352783
Epoch 8 loss 2.4580271244049072
Epoch 9 loss 2.4342782497406006
Epoch 10 loss 2.455686092376709
Epoch 11 loss 2.403109073638916
Epoch 12 loss 2.3828482627868652
Epoch 13 loss 2.3548667430877686
Epoch 14 loss 2.376103401184082
Epoch 15 loss 2.352846145629883
Epoch 16 loss 2.4124011993408203
Epoch 17 loss 2.3218798637390137
Epoch 18 loss 2.304161548614502
Epoch 19 loss 2.295644521713257
Epoch 20 loss 2.293168544769287
Epoch 21 loss 2.368216037750244
Epoch 22 loss 2.3281030654907227
Epoch 23 loss 2.2555782794952393
Epoch 24 loss 2.2792465686798096
Epoch 25 loss 2.269124984741211
Epoch 26 loss 2.2242932319641113
Epoch 27 loss 2.266873836517334
Epoch 28 loss 2.2673444747924805
Epoch 29 loss 2.274099826812744
Epoch 30 loss 2.380763053894043
Epoch 31 loss 2.260359287261963
Epoch 32 loss 2.236876964569092
Epoch 33 loss 2.228480815887451
Epoch 34 loss 2.3509926795959473
Epoch 35 loss 2.3934082984924316
Epoch 36 loss 2.2539479732513428
Epoch 37 loss 2.304138660430908
Epoch 38 loss 2.3676249980926514
Epoch 39 loss 2.343722343444824
Epoch 40 loss 2.406139612197876
Epoch 41 loss 2.423923969268799
Epoch 42 loss 2.473121166229248
Epoch 43 loss 2.415341377258301
Epoch 44 loss 2.398730754852295
Epoch 45 loss 2.4962854385375977
Epoch 46 loss 2.4744880199432373
Epoch 47 loss 2.554232120513916
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 83
Class label len :83
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
Unique y_true : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82] 

Prediction shape : (218,)
Probabilities shape : (218, 83) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.3556999999999997, 'Log Loss - std': 0.08450000000000002} 
 

Fold 3
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 1.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 ...
 [0.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [1.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [0.0 0.0 0.0 ... -0.14328869268055042 0.9115819464298488
  0.05213966245741688]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 76
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75.]), 76)
Unique values in y_test: (array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 23., 24., 26., 27.,
       28., 32., 33., 36., 37., 38., 39., 40., 41., 42., 58., 59., 60.,
       61., 62., 63., 64., 65., 69., 70., 71., 72., 73., 74., 75.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75], Length : 76
Test after shift : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75], Length : 38
Number of Classes After Bin Verifier: 76
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.875401020050049
Epoch 1 loss 3.265350818634033
Epoch 2 loss 2.768606185913086
Epoch 3 loss 2.7105507850646973
Epoch 4 loss 2.6582398414611816
Epoch 5 loss 2.5643155574798584
Epoch 6 loss 2.485074520111084
Epoch 7 loss 2.391659736633301
Epoch 8 loss 2.306347370147705
Epoch 9 loss 2.261824607849121
Epoch 10 loss 2.25585675239563
Epoch 11 loss 2.215024948120117
Epoch 12 loss 2.2466161251068115
Epoch 13 loss 2.240161418914795
Epoch 14 loss 2.177828311920166
Epoch 15 loss 2.1862082481384277
Epoch 16 loss 2.1525325775146484
Epoch 17 loss 2.1651928424835205
Epoch 18 loss 2.1611499786376953
Epoch 19 loss 2.2053184509277344
Epoch 20 loss 2.1519389152526855
Epoch 21 loss 2.079915761947632
Epoch 22 loss 2.1015920639038086
Epoch 23 loss 2.0040335655212402
Epoch 24 loss 2.065293788909912
Epoch 25 loss 2.073986530303955
Epoch 26 loss 2.0566630363464355
Epoch 27 loss 2.0315442085266113
Epoch 28 loss 2.062788963317871
Epoch 29 loss 2.1624741554260254
Epoch 30 loss 2.056443691253662
Epoch 31 loss 2.028120517730713
Epoch 32 loss 2.0453906059265137
Epoch 33 loss 2.040956497192383
Epoch 34 loss 2.016726016998291
Epoch 35 loss 2.0558390617370605
Epoch 36 loss 2.001500368118286
Epoch 37 loss 1.9664226770401
Epoch 38 loss 2.0885322093963623
Epoch 39 loss 1.9550127983093262
Epoch 40 loss 2.105714797973633
Epoch 41 loss 2.037515163421631
Epoch 42 loss 1.994800090789795
Epoch 43 loss 2.0182905197143555
Epoch 44 loss 1.9717118740081787
Epoch 45 loss 2.0336906909942627
Epoch 46 loss 1.9752557277679443
Epoch 47 loss 2.0461249351501465
Epoch 48 loss 1.965221643447876
Epoch 49 loss 2.0169341564178467
Epoch 50 loss 2.0748605728149414
Epoch 51 loss 1.887452483177185
Epoch 52 loss 2.022512912750244
Epoch 53 loss 2.0385522842407227
Epoch 54 loss 1.9911487102508545
Epoch 55 loss 2.0032548904418945
Epoch 56 loss 1.973532795906067
Epoch 57 loss 1.9723950624465942
Epoch 58 loss 2.018951416015625
Epoch 59 loss 2.0047388076782227
Epoch 60 loss 2.0001380443573
Epoch 61 loss 2.1784653663635254
Epoch 62 loss 2.043195962905884
Epoch 63 loss 2.042492628097534
Epoch 64 loss 2.177051544189453
Epoch 65 loss 2.0261335372924805
Epoch 66 loss 2.0470707416534424
Epoch 67 loss 2.1795573234558105
Epoch 68 loss 2.1304571628570557
Epoch 69 loss 2.0095021724700928
Epoch 70 loss 2.2156143188476562
Epoch 71 loss 2.140894889831543
Epoch 72 loss 2.217930793762207
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 76
Class label len :76
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
Unique y_true : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75] 

Prediction shape : (218,)
Probabilities shape : (218, 76) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2003, 'Log Loss - std': 0.23034427856290823} 
 

Fold 4
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 1.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 ...
 [0.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [0.0 0.0 1.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [1.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  4.,  5.,  8.,  9., 13., 17., 19., 28., 32., 33., 39.,
       40., 41., 42., 43., 44., 45., 56., 57., 59., 60., 61., 62., 63.,
       64., 65., 66., 67., 68., 69., 71., 72., 73., 74., 75., 82., 85.,
       87., 88., 90.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.637795925140381
Epoch 1 loss 3.267035961151123
Epoch 2 loss 3.005394220352173
Epoch 3 loss 2.6791443824768066
Epoch 4 loss 2.5137877464294434
Epoch 5 loss 2.4516611099243164
Epoch 6 loss 2.452606678009033
Epoch 7 loss 2.4478225708007812
Epoch 8 loss 2.4187769889831543
Epoch 9 loss 2.40706729888916
Epoch 10 loss 2.394300937652588
Epoch 11 loss 2.407118320465088
Epoch 12 loss 2.4135398864746094
Epoch 13 loss 2.4106411933898926
Epoch 14 loss 2.400947093963623
Epoch 15 loss 2.3641254901885986
Epoch 16 loss 2.3690407276153564
Epoch 17 loss 2.3847079277038574
Epoch 18 loss 2.35715389251709
Epoch 19 loss 2.354611396789551
Epoch 20 loss 2.382814407348633
Epoch 21 loss 2.38789701461792
Epoch 22 loss 2.342061996459961
Epoch 23 loss 2.3762195110321045
Epoch 24 loss 2.382843017578125
Epoch 25 loss 2.4138994216918945
Epoch 26 loss 2.338825225830078
Epoch 27 loss 2.407726287841797
Epoch 28 loss 2.2884793281555176
Epoch 29 loss 2.3387093544006348
Epoch 30 loss 2.3532557487487793
Epoch 31 loss 2.387098789215088
Epoch 32 loss 2.3823704719543457
Epoch 33 loss 2.3588366508483887
Epoch 34 loss 2.425302267074585
Epoch 35 loss 2.364656448364258
Epoch 36 loss 2.4510960578918457
Epoch 37 loss 2.3846309185028076
Epoch 38 loss 2.3758397102355957
Epoch 39 loss 2.4305615425109863
Epoch 40 loss 2.3997058868408203
Epoch 41 loss 2.430375576019287
Epoch 42 loss 2.464717149734497
Epoch 43 loss 2.4941718578338623
Epoch 44 loss 2.516352653503418
Epoch 45 loss 2.5715725421905518
Epoch 46 loss 2.4835829734802246
Epoch 47 loss 2.5154237747192383
Epoch 48 loss 2.5362324714660645
Epoch 49 loss 2.6298248767852783
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.234075, 'Log Loss - std': 0.20788486446829166} 
 

Fold 5
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 1.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 0.0 0.0 ... -0.17343812715392498 0.8883649744794527
  -0.07440284126323692]
 ...
 [0.0 0.0 1.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [0.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [1.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  3.,  4.,  8., 10., 13., 15., 19., 31., 36., 37., 40.,
       41., 42., 43., 44., 45., 46., 47., 48., 49., 54., 59., 62., 63.,
       64., 65., 66., 67., 68., 69., 70., 71., 72., 79., 80., 83., 85.,
       87., 88., 89.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.973917007446289
Epoch 1 loss 3.5327539443969727
Epoch 2 loss 3.2859411239624023
Epoch 3 loss 3.089988946914673
Epoch 4 loss 2.9883511066436768
Epoch 5 loss 2.7835607528686523
Epoch 6 loss 2.7152605056762695
Epoch 7 loss 2.6627988815307617
Epoch 8 loss 2.7506747245788574
Epoch 9 loss 2.5821380615234375
Epoch 10 loss 2.6292033195495605
Epoch 11 loss 2.57468843460083
Epoch 12 loss 2.6145272254943848
Epoch 13 loss 2.604532480239868
Epoch 14 loss 2.5782573223114014
Epoch 15 loss 2.6284239292144775
Epoch 16 loss 2.6564836502075195
Epoch 17 loss 2.516284227371216
Epoch 18 loss 2.5333681106567383
Epoch 19 loss 2.5884158611297607
Epoch 20 loss 2.562223434448242
Epoch 21 loss 2.5696616172790527
Epoch 22 loss 2.501323699951172
Epoch 23 loss 2.6251420974731445
Epoch 24 loss 2.521620750427246
Epoch 25 loss 2.5023367404937744
Epoch 26 loss 2.5178372859954834
Epoch 27 loss 2.44899320602417
Epoch 28 loss 2.444690465927124
Epoch 29 loss 2.509779930114746
Epoch 30 loss 2.5203137397766113
Epoch 31 loss 2.471334457397461
Epoch 32 loss 2.490084171295166
Epoch 33 loss 2.497603416442871
Epoch 34 loss 2.4949886798858643
Epoch 35 loss 2.5699429512023926
Epoch 36 loss 2.5100598335266113
Epoch 37 loss 2.4749810695648193
Epoch 38 loss 2.5680441856384277
Epoch 39 loss 2.5224480628967285
Epoch 40 loss 2.5678482055664062
Epoch 41 loss 2.5428614616394043
Epoch 42 loss 2.6151678562164307
Epoch 43 loss 2.6147804260253906
Epoch 44 loss 2.601332187652588
Epoch 45 loss 2.6488404273986816
Epoch 46 loss 2.6198418140411377
Epoch 47 loss 2.6144933700561523
Epoch 48 loss 2.578104019165039
Epoch 49 loss 2.581562042236328
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.28116, 'Log Loss - std': 0.2084247643635468} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 1 finished with value: 2.28116 and parameters: {'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}. Best is trial 1 with value: 2.28116.
Best parameters After Trials: {'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Parameters saved to YAML file!!!
In get_device
Using dim 8 and batch size 64
Fold 1
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 1.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 [0.0 0.0 0.0 ... -0.19374589191821132 -0.3222709797011101
  -0.1425475732690336]
 ...
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 1.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]
 [0.0 0.0 0.0 ... -0.17479792177884007 0.9407415789477349
  0.026954450218144525]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  1.,  3.,  6., 17., 20., 21., 22., 24., 27., 31., 38., 41.,
       42., 43., 44., 45., 46., 47., 50., 52., 54., 55., 58., 59., 60.,
       61., 62., 64., 65., 66., 67., 69., 70., 71., 72., 73., 75., 77.,
       78., 80., 84., 85., 86.]), 44)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86], Length : 44
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.727400541305542
Epoch 1 loss 3.3232078552246094
Epoch 2 loss 3.1306848526000977
Epoch 3 loss 3.0973267555236816
Epoch 4 loss 2.9886510372161865
Epoch 5 loss 2.778848648071289
Epoch 6 loss 2.637636661529541
Epoch 7 loss 2.6013548374176025
Epoch 8 loss 2.5617730617523193
Epoch 9 loss 2.471153974533081
Epoch 10 loss 2.5519022941589355
Epoch 11 loss 2.5779948234558105
Epoch 12 loss 2.5302066802978516
Epoch 13 loss 2.5430047512054443
Epoch 14 loss 2.5808486938476562
Epoch 15 loss 2.5723650455474854
Epoch 16 loss 2.497372627258301
Epoch 17 loss 2.548062801361084
Epoch 18 loss 2.5385990142822266
Epoch 19 loss 2.5432095527648926
Epoch 20 loss 2.5840110778808594
Epoch 21 loss 2.5588626861572266
Epoch 22 loss 2.5484941005706787
Epoch 23 loss 2.5663743019104004
Epoch 24 loss 2.495312452316284
Epoch 25 loss 2.5098748207092285
Epoch 26 loss 2.463545322418213
Epoch 27 loss 2.4980342388153076
Epoch 28 loss 2.60957670211792
Epoch 29 loss 2.5361480712890625
Epoch 30 loss 2.516098737716675
Epoch 31 loss 2.4811530113220215
Epoch 32 loss 2.5359983444213867
Epoch 33 loss 2.565495014190674
Epoch 34 loss 2.5213263034820557
Epoch 35 loss 2.591146469116211
Epoch 36 loss 2.570911407470703
Epoch 37 loss 2.51971435546875
Epoch 38 loss 2.519653797149658
Epoch 39 loss 2.572352647781372
Epoch 40 loss 2.5973968505859375
Epoch 41 loss 2.6048359870910645
Epoch 42 loss 2.63663911819458
Epoch 43 loss 2.694274663925171
Epoch 44 loss 2.7325563430786133
Epoch 45 loss 2.6999688148498535
Epoch 46 loss 2.637566089630127
Epoch 47 loss 2.6311185359954834
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/MIP/logging/loss_0.txt
File name : output/SAINT/MIP/logging/loss_0.txt . The file was saved
Log file exists at: output/SAINT/MIP/logging/val_loss_0.txt
File name : output/SAINT/MIP/logging/val_loss_0.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  1  3  6 17 20 21 22 24 27 31 38 41 42 43 44 45 46 47 50 52 54 55 58
 59 60 61 62 64 65 66 67 69 70 71 72 73 75 77 78 80 84 85 86] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.5039, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 1.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 [0.0 0.0 0.0 ... -0.17952568337830488 -0.31636181927831675
  -0.13934454596398707]
 ...
 [0.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [1.0 0.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]
 [0.0 1.0 0.0 ... -0.17952568337830488 0.8393834541148507
  -0.1108080396040536]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 83
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82.]), 83)
Unique values in y_test: (array([ 3.,  4.,  6., 11., 15., 16., 17., 22., 23., 26., 27., 34., 35.,
       36., 37., 38., 39., 42., 56., 57., 58., 59., 60., 61., 62., 63.,
       65., 66., 68., 69., 71., 73., 74., 75., 78., 79., 81., 82.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82], Length : 83
Test after shift : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82], Length : 38
Number of Classes After Bin Verifier: 83
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.817260265350342
Epoch 1 loss 3.413926124572754
Epoch 2 loss 3.048996925354004
Epoch 3 loss 2.617323398590088
Epoch 4 loss 2.655454635620117
Epoch 5 loss 2.454784870147705
Epoch 6 loss 2.4375691413879395
Epoch 7 loss 2.4173831939697266
Epoch 8 loss 2.4192309379577637
Epoch 9 loss 2.4143729209899902
Epoch 10 loss 2.3722586631774902
Epoch 11 loss 2.4122743606567383
Epoch 12 loss 2.3799033164978027
Epoch 13 loss 2.519207239151001
Epoch 14 loss 2.377239227294922
Epoch 15 loss 2.3971242904663086
Epoch 16 loss 2.361018657684326
Epoch 17 loss 2.402611255645752
Epoch 18 loss 2.5170023441314697
Epoch 19 loss 2.3866186141967773
Epoch 20 loss 2.3486721515655518
Epoch 21 loss 2.3417325019836426
Epoch 22 loss 2.374960422515869
Epoch 23 loss 2.2994372844696045
Epoch 24 loss 2.406113386154175
Epoch 25 loss 2.3085079193115234
Epoch 26 loss 2.2997584342956543
Epoch 27 loss 2.3625595569610596
Epoch 28 loss 2.3916399478912354
Epoch 29 loss 2.3433151245117188
Epoch 30 loss 2.2895431518554688
Epoch 31 loss 2.3200016021728516
Epoch 32 loss 2.293820858001709
Epoch 33 loss 2.320845365524292
Epoch 34 loss 2.367821455001831
Epoch 35 loss 2.4037985801696777
Epoch 36 loss 2.329075813293457
Epoch 37 loss 2.3318629264831543
Epoch 38 loss 2.361694812774658
Epoch 39 loss 2.489108085632324
Epoch 40 loss 2.349529504776001
Epoch 41 loss 2.3261756896972656
Epoch 42 loss 2.3855018615722656
Epoch 43 loss 2.4588675498962402
Epoch 44 loss 2.335979700088501
Epoch 45 loss 2.4054911136627197
Epoch 46 loss 2.4222209453582764
Epoch 47 loss 2.3516743183135986
Epoch 48 loss 2.4186654090881348
Epoch 49 loss 2.561201333999634
Epoch 50 loss 2.5212836265563965
Epoch 51 loss 2.3363661766052246
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/MIP/logging/loss_1.txt
File name : output/SAINT/MIP/logging/loss_1.txt . The file was saved
Log file exists at: output/SAINT/MIP/logging/val_loss_1.txt
File name : output/SAINT/MIP/logging/val_loss_1.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 83
Class label len :83
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82]
Unique y_true : [ 3  4  6 11 15 16 17 22 23 26 27 34 35 36 37 38 39 42 56 57 58 59 60 61
 62 63 65 66 68 69 71 73 74 75 78 79 81 82] 

Prediction shape : (218,)
Probabilities shape : (218, 83) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.4053, 'Log Loss - std': 0.0985999999999998} 
 

Fold 3
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 1.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 [0.0 0.0 0.0 ... -0.1611878054707481 -0.2996250596929442
  -0.08654600809530559]
 ...
 [0.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [1.0 0.0 0.0 ... -0.1611878054707481 0.7938257097234661
  -0.06343172966985185]
 [0.0 0.0 0.0 ... -0.14328869268055042 0.9115819464298488
  0.05213966245741688]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 76
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75.]), 76)
Unique values in y_test: (array([ 1.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 23., 24., 26., 27.,
       28., 32., 33., 36., 37., 38., 39., 40., 41., 42., 58., 59., 60.,
       61., 62., 63., 64., 65., 69., 70., 71., 72., 73., 74., 75.]), 38)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75], Length : 76
Test after shift : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75], Length : 38
Number of Classes After Bin Verifier: 76
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 4.024155139923096
Epoch 1 loss 3.3479771614074707
Epoch 2 loss 2.8140311241149902
Epoch 3 loss 2.592585563659668
Epoch 4 loss 2.4660797119140625
Epoch 5 loss 2.3532052040100098
Epoch 6 loss 2.337770462036133
Epoch 7 loss 2.3152217864990234
Epoch 8 loss 2.3329849243164062
Epoch 9 loss 2.3077707290649414
Epoch 10 loss 2.264223575592041
Epoch 11 loss 2.246965169906616
Epoch 12 loss 2.2319791316986084
Epoch 13 loss 2.37197208404541
Epoch 14 loss 2.2448463439941406
Epoch 15 loss 2.268031120300293
Epoch 16 loss 2.243600368499756
Epoch 17 loss 2.2517645359039307
Epoch 18 loss 2.2428221702575684
Epoch 19 loss 2.140054225921631
Epoch 20 loss 2.226273775100708
Epoch 21 loss 2.158560037612915
Epoch 22 loss 2.111628532409668
Epoch 23 loss 2.0846304893493652
Epoch 24 loss 2.1369833946228027
Epoch 25 loss 2.0873019695281982
Epoch 26 loss 2.1372487545013428
Epoch 27 loss 2.1230082511901855
Epoch 28 loss 2.1000914573669434
Epoch 29 loss 2.123445987701416
Epoch 30 loss 2.115999698638916
Epoch 31 loss 2.092679977416992
Epoch 32 loss 2.0622448921203613
Epoch 33 loss 2.123046398162842
Epoch 34 loss 2.1297378540039062
Epoch 35 loss 2.1272013187408447
Epoch 36 loss 2.108858585357666
Epoch 37 loss 2.265835762023926
Epoch 38 loss 2.149625301361084
Epoch 39 loss 2.2493441104888916
Epoch 40 loss 2.088153123855591
Epoch 41 loss 2.098365306854248
Epoch 42 loss 2.1771278381347656
Epoch 43 loss 2.157288074493408
Epoch 44 loss 2.0129058361053467
Epoch 45 loss 2.2748231887817383
Epoch 46 loss 2.123201608657837
Epoch 47 loss 2.173555850982666
Epoch 48 loss 2.09336519241333
Epoch 49 loss 2.1163365840911865
Epoch 50 loss 2.0993642807006836
Epoch 51 loss 2.1576716899871826
Epoch 52 loss 2.1782398223876953
Epoch 53 loss 2.148550510406494
Epoch 54 loss 2.3316328525543213
Epoch 55 loss 2.344707727432251
Epoch 56 loss 2.429450035095215
Epoch 57 loss 2.4483981132507324
Epoch 58 loss 2.385544538497925
Epoch 59 loss 2.1632871627807617
Epoch 60 loss 2.223473310470581
Epoch 61 loss 2.3089373111724854
Epoch 62 loss 2.2624306678771973
Epoch 63 loss 2.3153786659240723
Epoch 64 loss 2.1109859943389893
Epoch 65 loss 2.139606475830078
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/MIP/logging/loss_2.txt
File name : output/SAINT/MIP/logging/loss_2.txt . The file was saved
Log file exists at: output/SAINT/MIP/logging/val_loss_2.txt
File name : output/SAINT/MIP/logging/val_loss_2.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 76
Class label len :76
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]
Unique y_true : [ 1  3  4  5  6  7  8  9 11 23 24 26 27 28 32 33 36 37 38 39 40 41 42 58
 59 60 61 62 63 64 65 69 70 71 72 73 74 75] 

Prediction shape : (218,)
Probabilities shape : (218, 76) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2805, 'Log Loss - std': 0.19398810960125018} 
 

Fold 4
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 1.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 [0.0 0.0 0.0 ... -0.1632559473456111 -0.29118644171452457
  -0.0731320902969777]
 ...
 [0.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [0.0 0.0 1.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]
 [1.0 0.0 0.0 ... -0.14659405758842795 1.0204373456778975
  0.06702435528316418]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  4.,  5.,  8.,  9., 13., 17., 19., 28., 32., 33., 39.,
       40., 41., 42., 43., 44., 45., 56., 57., 59., 60., 61., 62., 63.,
       64., 65., 66., 67., 68., 69., 71., 72., 73., 74., 75., 82., 85.,
       87., 88., 90.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.9578917026519775
Epoch 1 loss 3.2369186878204346
Epoch 2 loss 2.929504871368408
Epoch 3 loss 2.838320255279541
Epoch 4 loss 2.7075705528259277
Epoch 5 loss 2.586118221282959
Epoch 6 loss 2.5678296089172363
Epoch 7 loss 2.4640231132507324
Epoch 8 loss 2.4384658336639404
Epoch 9 loss 2.463801383972168
Epoch 10 loss 2.3727922439575195
Epoch 11 loss 2.4081039428710938
Epoch 12 loss 2.420886993408203
Epoch 13 loss 2.4240190982818604
Epoch 14 loss 2.365096092224121
Epoch 15 loss 2.418823719024658
Epoch 16 loss 2.3173937797546387
Epoch 17 loss 2.402153968811035
Epoch 18 loss 2.2941808700561523
Epoch 19 loss 2.3280434608459473
Epoch 20 loss 2.410039186477661
Epoch 21 loss 2.3295516967773438
Epoch 22 loss 2.2825074195861816
Epoch 23 loss 2.2845911979675293
Epoch 24 loss 2.2738842964172363
Epoch 25 loss 2.3120622634887695
Epoch 26 loss 2.3067874908447266
Epoch 27 loss 2.3242483139038086
Epoch 28 loss 2.260551929473877
Epoch 29 loss 2.2707488536834717
Epoch 30 loss 2.2940735816955566
Epoch 31 loss 2.2865514755249023
Epoch 32 loss 2.295983076095581
Epoch 33 loss 2.2731709480285645
Epoch 34 loss 2.3546600341796875
Epoch 35 loss 2.3795437812805176
Epoch 36 loss 2.3227477073669434
Epoch 37 loss 2.3243637084960938
Epoch 38 loss 2.361171007156372
Epoch 39 loss 2.3407187461853027
Epoch 40 loss 2.4121694564819336
Epoch 41 loss 2.5151946544647217
Epoch 42 loss 2.3964600563049316
Epoch 43 loss 2.438814401626587
Epoch 44 loss 2.4529480934143066
Epoch 45 loss 2.5101988315582275
Epoch 46 loss 2.4448118209838867
Epoch 47 loss 2.4585134983062744
Epoch 48 loss 2.623446464538574
Epoch 49 loss 2.4620213508605957
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/MIP/logging/loss_3.txt
File name : output/SAINT/MIP/logging/loss_3.txt . The file was saved
Log file exists at: output/SAINT/MIP/logging/val_loss_3.txt
File name : output/SAINT/MIP/logging/val_loss_3.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  4  5  8  9 13 17 19 28 32 33 39 40 41 42 43 44 45 56 57 59 60 61
 62 63 64 65 66 67 68 69 71 72 73 74 75 82 85 87 88 90] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.27665, 'Log Loss - std': 0.16813092368746443} 
 

Fold 5
num_features : 146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :146
num_classes : 1
cat_idx : [0]
nominal_idx : [144, 145]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (872, 146)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]
Cat Dims V1 : []
Cat Idx V1 : [144, 145] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [1.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (872, 151) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[1.0 0.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 1.0 0.0 ... -0.17343812715392498 -0.3178376562796553
  -0.10019977071911866]
 [0.0 0.0 0.0 ... -0.17343812715392498 0.8883649744794527
  -0.07440284126323692]
 ...
 [0.0 0.0 1.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [0.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]
 [1.0 0.0 0.0 ... -0.15538129025995961 1.0182637193304336
  0.054581806016171804]] 
 
 
Val : (872, 151) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 91
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,
       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.,
       65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77.,
       78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.]), 91)
Unique values in y_test: (array([ 0.,  2.,  3.,  4.,  8., 10., 13., 15., 19., 31., 36., 37., 40.,
       41., 42., 43., 44., 45., 46., 47., 48., 49., 54., 59., 62., 63.,
       64., 65., 66., 67., 68., 69., 70., 71., 72., 79., 80., 83., 85.,
       87., 88., 89.]), 42)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90], Length : 91
Test after shift : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89], Length : 42
Number of Classes After Bin Verifier: 91
In get_device
Using dim 8 and batch size 64
{'dim': 32, 'depth': 6, 'heads': 2, 'dropout': 0.5}
Epoch 0 loss 3.809627056121826
Epoch 1 loss 3.3960025310516357
Epoch 2 loss 3.1379172801971436
Epoch 3 loss 2.961106777191162
Epoch 4 loss 2.7199201583862305
Epoch 5 loss 2.610745429992676
Epoch 6 loss 2.636051654815674
Epoch 7 loss 2.6203396320343018
Epoch 8 loss 2.586026668548584
Epoch 9 loss 2.5891456604003906
Epoch 10 loss 2.560741424560547
Epoch 11 loss 2.589191436767578
Epoch 12 loss 2.551487445831299
Epoch 13 loss 2.518458127975464
Epoch 14 loss 2.5806355476379395
Epoch 15 loss 2.5179965496063232
Epoch 16 loss 2.443786859512329
Epoch 17 loss 2.488877773284912
Epoch 18 loss 2.4218082427978516
Epoch 19 loss 2.46757173538208
Epoch 20 loss 2.4440670013427734
Epoch 21 loss 2.3851847648620605
Epoch 22 loss 2.4592766761779785
Epoch 23 loss 2.484175682067871
Epoch 24 loss 2.427147626876831
Epoch 25 loss 2.39974308013916
Epoch 26 loss 2.448897123336792
Epoch 27 loss 2.414430618286133
Epoch 28 loss 2.3507909774780273
Epoch 29 loss 2.402841091156006
Epoch 30 loss 2.356203317642212
Epoch 31 loss 2.398508071899414
Epoch 32 loss 2.4093666076660156
Epoch 33 loss 2.520531177520752
Epoch 34 loss 2.3603835105895996
Epoch 35 loss 2.3626153469085693
Epoch 36 loss 2.4200713634490967
Epoch 37 loss 2.4820053577423096
Epoch 38 loss 2.455415725708008
Epoch 39 loss 2.428353786468506
Epoch 40 loss 2.487041473388672
Epoch 41 loss 2.393069267272949
Epoch 42 loss 2.4290823936462402
Epoch 43 loss 2.364487648010254
Epoch 44 loss 2.570030450820923
Epoch 45 loss 2.437030792236328
Epoch 46 loss 2.592017650604248
Epoch 47 loss 2.5426454544067383
Epoch 48 loss 2.6419167518615723
Epoch 49 loss 2.519606828689575
Validation loss has not improved for 20 steps!
Early stopping applies.
State of save is True b4 loss saving
Log file exists at: output/SAINT/MIP/logging/loss_4.txt
File name : output/SAINT/MIP/logging/loss_4.txt . The file was saved
Log file exists at: output/SAINT/MIP/logging/val_loss_4.txt
File name : output/SAINT/MIP/logging/val_loss_4.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 91
Class label len :91
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]
Unique y_true : [ 0  2  3  4  8 10 13 15 19 31 36 37 40 41 42 43 44 45 46 47 48 49 54 59
 62 63 64 65 66 67 68 69 70 71 72 79 80 83 85 87 88 89] 

Prediction shape : (218,)
Probabilities shape : (218, 91) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.29976, 'Log Loss - std': 0.1573235341581163} 
 

Saving model.....
Results After CV: {'Log Loss - mean': 2.29976, 'Log Loss - std': 0.1573235341581163}
Train time: 249.8758051590001
Inference time: 0.26001048599991916
Finished cross validation
Loss path :output/SAINT/MIP/logging/
Plots saved successfully!
