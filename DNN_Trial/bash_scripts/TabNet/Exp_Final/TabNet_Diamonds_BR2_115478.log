Namespace(batch_size=128, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/diamonds.yml', data_parallel=False, dataset='Diamonds', direction='maximize', dropna_idx=None, early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabNet', n_trials=50, nominal_idx=None, num_bins=10, num_classes=1, num_features=9, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=False, optimize_hyperparameters=True, ordinal_encode=True, ordinal_idx=[1, 2, 3], scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256, y_distribution='normal')
Start hyperparameter optimization
Loading dataset Diamonds...
Dataset loaded! 

(53940, 9)
Using an existing study with name 'TabNet_Diamonds' instead of creating a new one.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.67002 | eval_custom_logloss: 7.6381  |  0:00:18s
epoch 1  | loss: 1.14903 | eval_custom_logloss: 8.27662 |  0:00:35s
epoch 2  | loss: 0.98994 | eval_custom_logloss: 4.79209 |  0:00:52s
epoch 3  | loss: 0.94944 | eval_custom_logloss: 4.45188 |  0:01:10s
epoch 4  | loss: 0.92103 | eval_custom_logloss: 2.19236 |  0:01:27s
epoch 5  | loss: 0.91585 | eval_custom_logloss: 2.29752 |  0:01:44s
epoch 6  | loss: 0.88535 | eval_custom_logloss: 1.26045 |  0:02:02s
epoch 7  | loss: 0.87178 | eval_custom_logloss: 2.75605 |  0:02:19s
epoch 8  | loss: 0.83951 | eval_custom_logloss: 1.24421 |  0:02:37s
epoch 9  | loss: 0.80488 | eval_custom_logloss: 3.41995 |  0:02:54s
epoch 10 | loss: 0.78422 | eval_custom_logloss: 1.55133 |  0:03:12s
epoch 11 | loss: 0.78854 | eval_custom_logloss: 1.60174 |  0:03:29s
epoch 12 | loss: 0.77479 | eval_custom_logloss: 1.00848 |  0:03:46s
epoch 13 | loss: 0.75136 | eval_custom_logloss: 1.17369 |  0:04:04s
epoch 14 | loss: 0.76427 | eval_custom_logloss: 1.33371 |  0:04:21s
epoch 15 | loss: 0.74665 | eval_custom_logloss: 1.38777 |  0:04:39s
epoch 16 | loss: 0.73508 | eval_custom_logloss: 0.92154 |  0:04:56s
epoch 17 | loss: 0.74161 | eval_custom_logloss: 1.07457 |  0:05:14s
epoch 18 | loss: 0.72464 | eval_custom_logloss: 1.09564 |  0:05:31s
epoch 19 | loss: 0.71962 | eval_custom_logloss: 0.96429 |  0:05:48s
epoch 20 | loss: 0.72482 | eval_custom_logloss: 1.27013 |  0:06:06s
epoch 21 | loss: 0.72231 | eval_custom_logloss: 1.0455  |  0:06:23s
epoch 22 | loss: 0.70026 | eval_custom_logloss: 0.90935 |  0:06:40s
epoch 23 | loss: 0.68783 | eval_custom_logloss: 1.27673 |  0:06:57s
epoch 24 | loss: 0.69942 | eval_custom_logloss: 0.84965 |  0:07:15s
epoch 25 | loss: 0.69128 | eval_custom_logloss: 0.76029 |  0:07:32s
epoch 26 | loss: 0.7012  | eval_custom_logloss: 1.14963 |  0:07:49s
epoch 27 | loss: 0.68933 | eval_custom_logloss: 0.92326 |  0:08:07s
epoch 28 | loss: 0.69324 | eval_custom_logloss: 1.02556 |  0:08:24s
epoch 29 | loss: 0.68883 | eval_custom_logloss: 1.04701 |  0:08:42s
epoch 30 | loss: 0.672   | eval_custom_logloss: 0.724   |  0:08:59s
epoch 31 | loss: 0.67839 | eval_custom_logloss: 0.95059 |  0:09:16s
epoch 32 | loss: 0.67292 | eval_custom_logloss: 1.31468 |  0:09:34s
epoch 33 | loss: 0.67075 | eval_custom_logloss: 0.70704 |  0:09:51s
epoch 34 | loss: 0.66221 | eval_custom_logloss: 0.92729 |  0:10:08s
epoch 35 | loss: 0.66578 | eval_custom_logloss: 0.80698 |  0:10:25s
epoch 36 | loss: 0.66016 | eval_custom_logloss: 0.85714 |  0:10:43s
epoch 37 | loss: 0.65516 | eval_custom_logloss: 1.07242 |  0:11:00s
epoch 38 | loss: 0.66369 | eval_custom_logloss: 0.63508 |  0:11:18s
epoch 39 | loss: 0.64701 | eval_custom_logloss: 1.17198 |  0:11:35s
epoch 40 | loss: 0.66098 | eval_custom_logloss: 0.81253 |  0:11:53s
epoch 41 | loss: 0.65474 | eval_custom_logloss: 0.70082 |  0:12:10s
epoch 42 | loss: 0.64578 | eval_custom_logloss: 1.01012 |  0:12:27s
epoch 43 | loss: 0.64656 | eval_custom_logloss: 0.80172 |  0:12:45s
epoch 44 | loss: 0.64485 | eval_custom_logloss: 0.62754 |  0:13:02s
epoch 45 | loss: 0.65806 | eval_custom_logloss: 0.7196  |  0:13:19s
epoch 46 | loss: 0.64783 | eval_custom_logloss: 0.97182 |  0:13:37s
epoch 47 | loss: 0.63899 | eval_custom_logloss: 0.75641 |  0:13:54s
epoch 48 | loss: 0.63089 | eval_custom_logloss: 0.88106 |  0:14:11s
epoch 49 | loss: 0.63757 | eval_custom_logloss: 0.77381 |  0:14:29s
epoch 50 | loss: 0.64468 | eval_custom_logloss: 0.70789 |  0:14:46s
epoch 51 | loss: 0.63162 | eval_custom_logloss: 0.59751 |  0:15:03s
epoch 52 | loss: 0.62336 | eval_custom_logloss: 1.06989 |  0:15:21s
epoch 53 | loss: 0.64537 | eval_custom_logloss: 0.86174 |  0:15:39s
epoch 54 | loss: 0.6258  | eval_custom_logloss: 0.79198 |  0:15:56s
epoch 55 | loss: 0.63337 | eval_custom_logloss: 0.71871 |  0:16:13s
epoch 56 | loss: 0.62706 | eval_custom_logloss: 0.66093 |  0:16:31s
epoch 57 | loss: 0.62728 | eval_custom_logloss: 1.02229 |  0:16:48s
epoch 58 | loss: 0.61908 | eval_custom_logloss: 0.71471 |  0:17:06s
epoch 59 | loss: 0.61762 | eval_custom_logloss: 0.69671 |  0:17:23s
epoch 60 | loss: 0.62111 | eval_custom_logloss: 0.80961 |  0:17:40s
epoch 61 | loss: 0.61741 | eval_custom_logloss: 0.8088  |  0:17:58s
epoch 62 | loss: 0.61925 | eval_custom_logloss: 0.97183 |  0:18:15s
epoch 63 | loss: 0.62419 | eval_custom_logloss: 0.59654 |  0:18:32s
epoch 64 | loss: 0.62849 | eval_custom_logloss: 0.84958 |  0:18:50s
epoch 65 | loss: 0.61572 | eval_custom_logloss: 0.78081 |  0:19:07s
epoch 66 | loss: 0.61322 | eval_custom_logloss: 0.94159 |  0:19:25s
epoch 67 | loss: 0.62258 | eval_custom_logloss: 0.69113 |  0:19:42s
epoch 68 | loss: 0.60502 | eval_custom_logloss: 0.63814 |  0:20:00s
epoch 69 | loss: 0.60562 | eval_custom_logloss: 0.67519 |  0:20:17s
epoch 70 | loss: 0.60344 | eval_custom_logloss: 0.61541 |  0:20:34s
epoch 71 | loss: 0.61114 | eval_custom_logloss: 0.79683 |  0:20:52s
epoch 72 | loss: 0.61634 | eval_custom_logloss: 0.9736  |  0:21:09s
epoch 73 | loss: 0.59903 | eval_custom_logloss: 0.64896 |  0:21:26s
epoch 74 | loss: 0.60078 | eval_custom_logloss: 0.89553 |  0:21:44s
epoch 75 | loss: 0.61253 | eval_custom_logloss: 0.70223 |  0:22:01s
epoch 76 | loss: 0.60356 | eval_custom_logloss: 0.6369  |  0:22:19s
epoch 77 | loss: 0.61001 | eval_custom_logloss: 0.58544 |  0:22:36s
epoch 78 | loss: 0.59882 | eval_custom_logloss: 0.66836 |  0:22:53s
epoch 79 | loss: 0.59438 | eval_custom_logloss: 0.60814 |  0:23:11s
epoch 80 | loss: 0.59496 | eval_custom_logloss: 0.66605 |  0:23:29s
epoch 81 | loss: 0.59141 | eval_custom_logloss: 0.91011 |  0:23:47s
epoch 82 | loss: 0.60047 | eval_custom_logloss: 0.64327 |  0:24:04s
epoch 83 | loss: 0.60528 | eval_custom_logloss: 0.75408 |  0:24:21s
epoch 84 | loss: 0.60423 | eval_custom_logloss: 1.01989 |  0:24:39s
epoch 85 | loss: 0.59269 | eval_custom_logloss: 0.82489 |  0:24:56s
epoch 86 | loss: 0.59939 | eval_custom_logloss: 0.77562 |  0:25:13s
epoch 87 | loss: 0.59465 | eval_custom_logloss: 1.05818 |  0:25:31s
epoch 88 | loss: 0.59765 | eval_custom_logloss: 0.75837 |  0:25:48s
epoch 89 | loss: 0.59674 | eval_custom_logloss: 0.81075 |  0:26:06s
epoch 90 | loss: 0.59032 | eval_custom_logloss: 0.67558 |  0:26:23s
epoch 91 | loss: 0.58755 | eval_custom_logloss: 0.96397 |  0:26:41s
epoch 92 | loss: 0.59068 | eval_custom_logloss: 0.68432 |  0:26:59s
epoch 93 | loss: 0.59029 | eval_custom_logloss: 0.61721 |  0:27:17s
epoch 94 | loss: 0.58891 | eval_custom_logloss: 0.8396  |  0:27:34s
epoch 95 | loss: 0.5886  | eval_custom_logloss: 0.8087  |  0:27:52s
epoch 96 | loss: 0.59274 | eval_custom_logloss: 0.64151 |  0:28:09s
epoch 97 | loss: 0.59827 | eval_custom_logloss: 0.62982 |  0:28:26s

Early stopping occurred at epoch 97 with best_epoch = 77 and best_eval_custom_logloss = 0.58544
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5843, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.6666  | eval_custom_logloss: 5.68884 |  0:00:17s
epoch 1  | loss: 1.18707 | eval_custom_logloss: 5.07123 |  0:00:34s
epoch 2  | loss: 1.0603  | eval_custom_logloss: 3.62562 |  0:00:52s
epoch 3  | loss: 0.98123 | eval_custom_logloss: 3.34379 |  0:01:09s
epoch 4  | loss: 0.97056 | eval_custom_logloss: 3.44191 |  0:01:27s
epoch 5  | loss: 0.96907 | eval_custom_logloss: 2.4929  |  0:01:44s
epoch 6  | loss: 0.95624 | eval_custom_logloss: 1.38471 |  0:02:02s
epoch 7  | loss: 0.93363 | eval_custom_logloss: 1.02082 |  0:02:19s
epoch 8  | loss: 0.90789 | eval_custom_logloss: 1.69731 |  0:02:37s
epoch 9  | loss: 0.87956 | eval_custom_logloss: 3.45653 |  0:02:54s
epoch 10 | loss: 0.87218 | eval_custom_logloss: 1.47746 |  0:03:11s
epoch 11 | loss: 0.84612 | eval_custom_logloss: 2.38296 |  0:03:29s
epoch 12 | loss: 0.83918 | eval_custom_logloss: 2.70097 |  0:03:46s
epoch 13 | loss: 0.82213 | eval_custom_logloss: 1.60645 |  0:04:04s
epoch 14 | loss: 0.82728 | eval_custom_logloss: 1.06257 |  0:04:21s
epoch 15 | loss: 0.81839 | eval_custom_logloss: 1.31115 |  0:04:38s
epoch 16 | loss: 0.8009  | eval_custom_logloss: 2.34986 |  0:04:56s
epoch 17 | loss: 0.79291 | eval_custom_logloss: 1.2232  |  0:05:13s
epoch 18 | loss: 0.77881 | eval_custom_logloss: 1.18947 |  0:05:31s
epoch 19 | loss: 0.77202 | eval_custom_logloss: 1.38208 |  0:05:48s
epoch 20 | loss: 0.76393 | eval_custom_logloss: 1.30129 |  0:06:06s
epoch 21 | loss: 0.76801 | eval_custom_logloss: 1.08932 |  0:06:23s
epoch 22 | loss: 0.75604 | eval_custom_logloss: 0.73364 |  0:06:41s
epoch 23 | loss: 0.76053 | eval_custom_logloss: 0.81658 |  0:06:58s
epoch 24 | loss: 0.752   | eval_custom_logloss: 0.90784 |  0:07:16s
epoch 25 | loss: 0.74231 | eval_custom_logloss: 0.79845 |  0:07:33s
epoch 26 | loss: 0.74526 | eval_custom_logloss: 0.77849 |  0:07:51s
epoch 27 | loss: 0.74465 | eval_custom_logloss: 1.0886  |  0:08:08s
epoch 28 | loss: 0.74151 | eval_custom_logloss: 0.71677 |  0:08:26s
epoch 29 | loss: 0.72852 | eval_custom_logloss: 0.92454 |  0:08:43s
epoch 30 | loss: 0.72158 | eval_custom_logloss: 1.04531 |  0:09:01s
epoch 31 | loss: 0.71336 | eval_custom_logloss: 0.95009 |  0:09:18s
epoch 32 | loss: 0.71998 | eval_custom_logloss: 0.76685 |  0:09:36s
epoch 33 | loss: 0.71246 | eval_custom_logloss: 0.96372 |  0:09:54s
epoch 34 | loss: 0.70479 | eval_custom_logloss: 1.47394 |  0:10:12s
epoch 35 | loss: 0.70739 | eval_custom_logloss: 0.77415 |  0:10:29s
epoch 36 | loss: 0.69372 | eval_custom_logloss: 0.88964 |  0:10:47s
epoch 37 | loss: 0.6893  | eval_custom_logloss: 0.89049 |  0:11:05s
epoch 38 | loss: 0.69184 | eval_custom_logloss: 1.27855 |  0:11:23s
epoch 39 | loss: 0.69519 | eval_custom_logloss: 1.22221 |  0:11:40s
epoch 40 | loss: 0.69264 | eval_custom_logloss: 0.81388 |  0:11:58s
epoch 41 | loss: 0.69361 | eval_custom_logloss: 0.77161 |  0:12:16s
epoch 42 | loss: 0.69709 | eval_custom_logloss: 0.76604 |  0:12:33s
epoch 43 | loss: 0.67784 | eval_custom_logloss: 0.9477  |  0:12:51s
epoch 44 | loss: 0.67524 | eval_custom_logloss: 0.87739 |  0:13:09s
epoch 45 | loss: 0.67845 | eval_custom_logloss: 0.69729 |  0:13:27s
epoch 46 | loss: 0.67326 | eval_custom_logloss: 0.82364 |  0:13:44s
epoch 47 | loss: 0.67316 | eval_custom_logloss: 0.84244 |  0:14:02s
epoch 48 | loss: 0.67043 | eval_custom_logloss: 0.9202  |  0:14:20s
epoch 49 | loss: 0.66419 | eval_custom_logloss: 1.06375 |  0:14:37s
epoch 50 | loss: 0.68005 | eval_custom_logloss: 1.00719 |  0:14:55s
epoch 51 | loss: 0.66794 | eval_custom_logloss: 1.05809 |  0:15:12s
epoch 52 | loss: 0.65732 | eval_custom_logloss: 1.01655 |  0:15:30s
epoch 53 | loss: 0.66539 | eval_custom_logloss: 0.82793 |  0:15:48s
epoch 54 | loss: 0.65891 | eval_custom_logloss: 1.04722 |  0:16:05s
epoch 55 | loss: 0.66328 | eval_custom_logloss: 0.92145 |  0:16:23s
epoch 56 | loss: 0.66532 | eval_custom_logloss: 0.8398  |  0:16:40s
epoch 57 | loss: 0.65597 | eval_custom_logloss: 0.94514 |  0:16:58s
epoch 58 | loss: 0.6571  | eval_custom_logloss: 0.75264 |  0:17:15s
epoch 59 | loss: 0.66508 | eval_custom_logloss: 1.26282 |  0:17:33s
epoch 60 | loss: 0.66007 | eval_custom_logloss: 0.79736 |  0:17:50s
epoch 61 | loss: 0.65203 | eval_custom_logloss: 0.81597 |  0:18:08s
epoch 62 | loss: 0.64656 | eval_custom_logloss: 0.85776 |  0:18:25s
epoch 63 | loss: 0.65007 | eval_custom_logloss: 0.62052 |  0:18:43s
epoch 64 | loss: 0.6594  | eval_custom_logloss: 0.82489 |  0:19:00s
epoch 65 | loss: 0.64933 | eval_custom_logloss: 0.84118 |  0:19:18s
epoch 66 | loss: 0.64427 | eval_custom_logloss: 0.74111 |  0:19:35s
epoch 67 | loss: 0.65019 | eval_custom_logloss: 0.85309 |  0:19:53s
epoch 68 | loss: 0.64285 | eval_custom_logloss: 0.83863 |  0:20:10s
epoch 69 | loss: 0.64466 | eval_custom_logloss: 0.77165 |  0:20:28s
epoch 70 | loss: 0.63119 | eval_custom_logloss: 0.91052 |  0:20:46s
epoch 71 | loss: 0.63986 | eval_custom_logloss: 0.86512 |  0:21:03s
epoch 72 | loss: 0.63433 | eval_custom_logloss: 0.91621 |  0:21:21s
epoch 73 | loss: 0.61985 | eval_custom_logloss: 0.66541 |  0:21:38s
epoch 74 | loss: 0.64548 | eval_custom_logloss: 1.00403 |  0:21:56s
epoch 75 | loss: 0.63647 | eval_custom_logloss: 0.77274 |  0:22:14s
epoch 76 | loss: 0.62919 | eval_custom_logloss: 0.80712 |  0:22:31s
epoch 77 | loss: 0.63538 | eval_custom_logloss: 0.75911 |  0:22:49s
epoch 78 | loss: 0.63315 | eval_custom_logloss: 0.65723 |  0:23:06s
epoch 79 | loss: 0.6254  | eval_custom_logloss: 0.83209 |  0:23:24s
epoch 80 | loss: 0.6271  | eval_custom_logloss: 0.85682 |  0:23:42s
epoch 81 | loss: 0.62272 | eval_custom_logloss: 0.94992 |  0:23:59s
epoch 82 | loss: 0.62464 | eval_custom_logloss: 2.68404 |  0:24:17s
epoch 83 | loss: 0.62962 | eval_custom_logloss: 2.27766 |  0:24:35s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 0.62052
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.60185, 'Log Loss - std': 0.017549999999999955} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.697   | eval_custom_logloss: 6.5556  |  0:00:17s
epoch 1  | loss: 1.1761  | eval_custom_logloss: 4.94893 |  0:00:35s
epoch 2  | loss: 1.06071 | eval_custom_logloss: 3.64589 |  0:00:52s
epoch 3  | loss: 1.01686 | eval_custom_logloss: 3.10799 |  0:01:10s
epoch 4  | loss: 0.98653 | eval_custom_logloss: 1.6886  |  0:01:28s
epoch 5  | loss: 0.96541 | eval_custom_logloss: 1.82193 |  0:01:45s
epoch 6  | loss: 0.96185 | eval_custom_logloss: 1.8005  |  0:02:03s
epoch 7  | loss: 0.91894 | eval_custom_logloss: 1.5831  |  0:02:21s
epoch 8  | loss: 0.9073  | eval_custom_logloss: 1.15062 |  0:02:38s
epoch 9  | loss: 0.89524 | eval_custom_logloss: 1.92488 |  0:02:56s
epoch 10 | loss: 0.86526 | eval_custom_logloss: 1.36745 |  0:03:14s
epoch 11 | loss: 0.87014 | eval_custom_logloss: 1.69257 |  0:03:32s
epoch 12 | loss: 0.8465  | eval_custom_logloss: 1.28523 |  0:03:50s
epoch 13 | loss: 0.82381 | eval_custom_logloss: 1.60096 |  0:04:08s
epoch 14 | loss: 0.82378 | eval_custom_logloss: 0.79048 |  0:04:25s
epoch 15 | loss: 0.80117 | eval_custom_logloss: 1.26157 |  0:04:43s
epoch 16 | loss: 0.79879 | eval_custom_logloss: 1.12469 |  0:05:01s
epoch 17 | loss: 0.78313 | eval_custom_logloss: 1.36764 |  0:05:19s
epoch 18 | loss: 0.76832 | eval_custom_logloss: 1.03686 |  0:05:36s
epoch 19 | loss: 0.75396 | eval_custom_logloss: 1.79752 |  0:05:54s
epoch 20 | loss: 0.742   | eval_custom_logloss: 0.81281 |  0:06:12s
epoch 21 | loss: 0.72496 | eval_custom_logloss: 1.05068 |  0:06:30s
epoch 22 | loss: 0.72638 | eval_custom_logloss: 0.95248 |  0:06:47s
epoch 23 | loss: 0.70363 | eval_custom_logloss: 1.04345 |  0:07:05s
epoch 24 | loss: 0.71785 | eval_custom_logloss: 1.13056 |  0:07:23s
epoch 25 | loss: 0.70564 | eval_custom_logloss: 1.03053 |  0:07:41s
epoch 26 | loss: 0.71735 | eval_custom_logloss: 0.77828 |  0:07:58s
epoch 27 | loss: 0.69214 | eval_custom_logloss: 0.83356 |  0:08:16s
epoch 28 | loss: 0.68937 | eval_custom_logloss: 1.03669 |  0:08:34s
epoch 29 | loss: 0.68679 | eval_custom_logloss: 0.72925 |  0:08:52s
epoch 30 | loss: 0.68207 | eval_custom_logloss: 0.66391 |  0:09:10s
epoch 31 | loss: 0.69594 | eval_custom_logloss: 0.88354 |  0:09:29s
epoch 32 | loss: 0.67445 | eval_custom_logloss: 0.75596 |  0:09:48s
epoch 33 | loss: 0.6795  | eval_custom_logloss: 0.67435 |  0:10:07s
epoch 34 | loss: 0.68287 | eval_custom_logloss: 0.981   |  0:10:26s
epoch 35 | loss: 0.66909 | eval_custom_logloss: 0.81068 |  0:10:45s
epoch 36 | loss: 0.6692  | eval_custom_logloss: 0.87143 |  0:11:04s
epoch 37 | loss: 0.66482 | eval_custom_logloss: 0.81552 |  0:11:23s
epoch 38 | loss: 0.67106 | eval_custom_logloss: 0.95061 |  0:11:42s
epoch 39 | loss: 0.67119 | eval_custom_logloss: 1.15526 |  0:12:00s
epoch 40 | loss: 0.67356 | eval_custom_logloss: 1.1352  |  0:12:18s
epoch 41 | loss: 0.6679  | eval_custom_logloss: 0.71615 |  0:12:35s
epoch 42 | loss: 0.65197 | eval_custom_logloss: 0.79198 |  0:12:53s
epoch 43 | loss: 0.66623 | eval_custom_logloss: 0.80532 |  0:13:10s
epoch 44 | loss: 0.65384 | eval_custom_logloss: 0.72135 |  0:13:28s
epoch 45 | loss: 0.64713 | eval_custom_logloss: 0.93974 |  0:13:46s
epoch 46 | loss: 0.64403 | eval_custom_logloss: 0.75513 |  0:14:03s
epoch 47 | loss: 0.67331 | eval_custom_logloss: 0.78567 |  0:14:21s
epoch 48 | loss: 0.6474  | eval_custom_logloss: 0.79422 |  0:14:39s
epoch 49 | loss: 0.64728 | eval_custom_logloss: 1.07044 |  0:14:56s
epoch 50 | loss: 0.65419 | eval_custom_logloss: 1.03558 |  0:15:14s

Early stopping occurred at epoch 50 with best_epoch = 30 and best_eval_custom_logloss = 0.66391
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6216333333333334, 'Log Loss - std': 0.03143398726785317} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66413 | eval_custom_logloss: 6.27809 |  0:00:17s
epoch 1  | loss: 1.15943 | eval_custom_logloss: 4.46109 |  0:00:35s
epoch 2  | loss: 1.05021 | eval_custom_logloss: 2.99656 |  0:00:52s
epoch 3  | loss: 0.98782 | eval_custom_logloss: 3.06659 |  0:01:10s
epoch 4  | loss: 0.93256 | eval_custom_logloss: 1.3579  |  0:01:28s
epoch 5  | loss: 0.91249 | eval_custom_logloss: 0.94112 |  0:01:45s
epoch 6  | loss: 0.90389 | eval_custom_logloss: 0.9037  |  0:02:03s
epoch 7  | loss: 0.88023 | eval_custom_logloss: 0.83452 |  0:02:21s
epoch 8  | loss: 0.87245 | eval_custom_logloss: 1.2245  |  0:02:38s
epoch 9  | loss: 0.84353 | eval_custom_logloss: 0.78039 |  0:02:56s
epoch 10 | loss: 0.81743 | eval_custom_logloss: 0.7874  |  0:03:13s
epoch 11 | loss: 0.82906 | eval_custom_logloss: 0.79941 |  0:03:31s
epoch 12 | loss: 0.81389 | eval_custom_logloss: 0.74604 |  0:03:49s
epoch 13 | loss: 0.79627 | eval_custom_logloss: 1.07267 |  0:04:06s
epoch 14 | loss: 0.78161 | eval_custom_logloss: 0.76867 |  0:04:24s
epoch 15 | loss: 0.79451 | eval_custom_logloss: 1.0312  |  0:04:42s
epoch 16 | loss: 0.76946 | eval_custom_logloss: 0.99249 |  0:04:59s
epoch 17 | loss: 0.77887 | eval_custom_logloss: 1.63479 |  0:05:17s
epoch 18 | loss: 0.77252 | eval_custom_logloss: 1.33528 |  0:05:34s
epoch 19 | loss: 0.74967 | eval_custom_logloss: 0.75997 |  0:05:52s
epoch 20 | loss: 0.75978 | eval_custom_logloss: 0.68347 |  0:06:10s
epoch 21 | loss: 0.738   | eval_custom_logloss: 1.32123 |  0:06:27s
epoch 22 | loss: 0.71986 | eval_custom_logloss: 0.75391 |  0:06:45s
epoch 23 | loss: 0.73005 | eval_custom_logloss: 1.17627 |  0:07:03s
epoch 24 | loss: 0.72868 | eval_custom_logloss: 0.72469 |  0:07:20s
epoch 25 | loss: 0.71768 | eval_custom_logloss: 0.86881 |  0:07:38s
epoch 26 | loss: 0.7083  | eval_custom_logloss: 0.86537 |  0:07:55s
epoch 27 | loss: 0.71073 | eval_custom_logloss: 0.9296  |  0:08:13s
epoch 28 | loss: 0.69722 | eval_custom_logloss: 0.72229 |  0:08:30s
epoch 29 | loss: 0.68988 | eval_custom_logloss: 0.73136 |  0:08:47s
epoch 30 | loss: 0.6776  | eval_custom_logloss: 0.70394 |  0:09:04s
epoch 31 | loss: 0.6909  | eval_custom_logloss: 0.64163 |  0:09:22s
epoch 32 | loss: 0.67879 | eval_custom_logloss: 0.90661 |  0:09:39s
epoch 33 | loss: 0.68447 | eval_custom_logloss: 0.88694 |  0:09:56s
epoch 34 | loss: 0.68263 | eval_custom_logloss: 0.9503  |  0:10:14s
epoch 35 | loss: 0.68206 | eval_custom_logloss: 0.87518 |  0:10:31s
epoch 36 | loss: 0.66907 | eval_custom_logloss: 0.62696 |  0:10:48s
epoch 37 | loss: 0.66557 | eval_custom_logloss: 0.67197 |  0:11:06s
epoch 38 | loss: 0.66924 | eval_custom_logloss: 0.78251 |  0:11:23s
epoch 39 | loss: 0.65548 | eval_custom_logloss: 0.68826 |  0:11:40s
epoch 40 | loss: 0.66735 | eval_custom_logloss: 0.78467 |  0:11:58s
epoch 41 | loss: 0.66642 | eval_custom_logloss: 0.69091 |  0:12:16s
epoch 42 | loss: 0.65063 | eval_custom_logloss: 0.61412 |  0:12:33s
epoch 43 | loss: 0.65901 | eval_custom_logloss: 0.71028 |  0:12:51s
epoch 44 | loss: 0.64923 | eval_custom_logloss: 0.83445 |  0:13:08s
epoch 45 | loss: 0.65435 | eval_custom_logloss: 0.61496 |  0:13:25s
epoch 46 | loss: 0.64595 | eval_custom_logloss: 0.72646 |  0:13:43s
epoch 47 | loss: 0.65669 | eval_custom_logloss: 0.63336 |  0:14:00s
epoch 48 | loss: 0.65196 | eval_custom_logloss: 0.98708 |  0:14:17s
epoch 49 | loss: 0.64827 | eval_custom_logloss: 0.65215 |  0:14:35s
epoch 50 | loss: 0.65092 | eval_custom_logloss: 0.71029 |  0:14:52s
epoch 51 | loss: 0.63682 | eval_custom_logloss: 0.93154 |  0:15:09s
epoch 52 | loss: 0.63295 | eval_custom_logloss: 1.0269  |  0:15:28s
epoch 53 | loss: 0.63555 | eval_custom_logloss: 0.82727 |  0:15:45s
epoch 54 | loss: 0.64051 | eval_custom_logloss: 0.76525 |  0:16:02s
epoch 55 | loss: 0.63504 | eval_custom_logloss: 0.61044 |  0:16:20s
epoch 56 | loss: 0.6373  | eval_custom_logloss: 0.80831 |  0:16:37s
epoch 57 | loss: 0.63229 | eval_custom_logloss: 0.78961 |  0:16:54s
epoch 58 | loss: 0.62028 | eval_custom_logloss: 0.78385 |  0:17:12s
epoch 59 | loss: 0.62438 | eval_custom_logloss: 0.62965 |  0:17:29s
epoch 60 | loss: 0.6293  | eval_custom_logloss: 0.885   |  0:17:47s
epoch 61 | loss: 0.61686 | eval_custom_logloss: 0.70574 |  0:18:06s
epoch 62 | loss: 0.62616 | eval_custom_logloss: 1.13475 |  0:18:24s
epoch 63 | loss: 0.624   | eval_custom_logloss: 0.81318 |  0:18:42s
epoch 64 | loss: 0.62815 | eval_custom_logloss: 0.7853  |  0:18:59s
epoch 65 | loss: 0.6146  | eval_custom_logloss: 0.89542 |  0:19:17s
epoch 66 | loss: 0.62035 | eval_custom_logloss: 0.89587 |  0:19:35s
epoch 67 | loss: 0.61545 | eval_custom_logloss: 0.64598 |  0:19:52s
epoch 68 | loss: 0.62564 | eval_custom_logloss: 0.68999 |  0:20:09s
epoch 69 | loss: 0.6256  | eval_custom_logloss: 0.69226 |  0:20:27s
epoch 70 | loss: 0.61645 | eval_custom_logloss: 0.7947  |  0:20:44s
epoch 71 | loss: 0.61052 | eval_custom_logloss: 1.06585 |  0:21:01s
epoch 72 | loss: 0.62168 | eval_custom_logloss: 1.21013 |  0:21:19s
epoch 73 | loss: 0.61729 | eval_custom_logloss: 0.89682 |  0:21:36s
epoch 74 | loss: 0.61562 | eval_custom_logloss: 0.78545 |  0:21:53s
epoch 75 | loss: 0.61966 | eval_custom_logloss: 0.86353 |  0:22:11s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.61044
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.61865, 'Log Loss - std': 0.027708708017516792} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66696 | eval_custom_logloss: 7.83165 |  0:00:17s
epoch 1  | loss: 1.12169 | eval_custom_logloss: 4.89515 |  0:00:34s
epoch 2  | loss: 1.00458 | eval_custom_logloss: 3.55038 |  0:00:52s
epoch 3  | loss: 0.93295 | eval_custom_logloss: 2.09854 |  0:01:09s
epoch 4  | loss: 0.93015 | eval_custom_logloss: 1.60465 |  0:01:26s
epoch 5  | loss: 0.91841 | eval_custom_logloss: 0.99407 |  0:01:44s
epoch 6  | loss: 0.89815 | eval_custom_logloss: 1.32121 |  0:02:01s
epoch 7  | loss: 0.88124 | eval_custom_logloss: 1.23804 |  0:02:18s
epoch 8  | loss: 0.88316 | eval_custom_logloss: 1.18601 |  0:02:36s
epoch 9  | loss: 0.8482  | eval_custom_logloss: 1.44994 |  0:02:53s
epoch 10 | loss: 0.84497 | eval_custom_logloss: 0.99467 |  0:03:10s
epoch 11 | loss: 0.87537 | eval_custom_logloss: 1.4043  |  0:03:28s
epoch 12 | loss: 0.84485 | eval_custom_logloss: 1.61495 |  0:03:45s
epoch 13 | loss: 0.78748 | eval_custom_logloss: 1.185   |  0:04:03s
epoch 14 | loss: 0.77677 | eval_custom_logloss: 0.74045 |  0:04:21s
epoch 15 | loss: 0.75776 | eval_custom_logloss: 1.37605 |  0:04:38s
epoch 16 | loss: 0.75819 | eval_custom_logloss: 1.67292 |  0:04:56s
epoch 17 | loss: 0.74343 | eval_custom_logloss: 1.43246 |  0:05:13s
epoch 18 | loss: 0.73634 | eval_custom_logloss: 0.88574 |  0:05:31s
epoch 19 | loss: 0.72395 | eval_custom_logloss: 1.02406 |  0:05:49s
epoch 20 | loss: 0.74492 | eval_custom_logloss: 0.86224 |  0:06:06s
epoch 21 | loss: 0.70871 | eval_custom_logloss: 0.82936 |  0:06:24s
epoch 22 | loss: 0.71031 | eval_custom_logloss: 1.02207 |  0:06:42s
epoch 23 | loss: 0.75697 | eval_custom_logloss: 1.20732 |  0:06:59s
epoch 24 | loss: 0.7356  | eval_custom_logloss: 0.97436 |  0:07:17s
epoch 25 | loss: 0.72112 | eval_custom_logloss: 1.48378 |  0:07:35s
epoch 26 | loss: 0.7141  | eval_custom_logloss: 0.70994 |  0:07:53s
epoch 27 | loss: 0.69948 | eval_custom_logloss: 0.76813 |  0:08:10s
epoch 28 | loss: 0.72042 | eval_custom_logloss: 1.0025  |  0:08:28s
epoch 29 | loss: 0.69885 | eval_custom_logloss: 0.86882 |  0:08:46s
epoch 30 | loss: 0.68285 | eval_custom_logloss: 0.66    |  0:09:03s
epoch 31 | loss: 0.68102 | eval_custom_logloss: 0.94533 |  0:09:21s
epoch 32 | loss: 0.68489 | eval_custom_logloss: 0.7531  |  0:09:39s
epoch 33 | loss: 0.68438 | eval_custom_logloss: 0.94535 |  0:09:57s
epoch 34 | loss: 0.67619 | eval_custom_logloss: 1.12992 |  0:10:14s
epoch 35 | loss: 0.66815 | eval_custom_logloss: 0.80499 |  0:10:32s
epoch 36 | loss: 0.66381 | eval_custom_logloss: 0.78483 |  0:10:50s
epoch 37 | loss: 0.65796 | eval_custom_logloss: 1.13224 |  0:11:08s
epoch 38 | loss: 0.66151 | eval_custom_logloss: 0.80748 |  0:11:26s
epoch 39 | loss: 0.66283 | eval_custom_logloss: 0.82916 |  0:11:44s
epoch 40 | loss: 0.65291 | eval_custom_logloss: 0.70263 |  0:12:02s
epoch 41 | loss: 0.65746 | eval_custom_logloss: 0.75616 |  0:12:19s
epoch 42 | loss: 0.65613 | eval_custom_logloss: 0.76741 |  0:12:37s
epoch 43 | loss: 0.64319 | eval_custom_logloss: 1.01035 |  0:12:55s
epoch 44 | loss: 0.66241 | eval_custom_logloss: 0.75892 |  0:13:13s
epoch 45 | loss: 0.64902 | eval_custom_logloss: 0.86371 |  0:13:30s
epoch 46 | loss: 0.64858 | eval_custom_logloss: 0.9418  |  0:13:48s
epoch 47 | loss: 0.64297 | eval_custom_logloss: 0.78252 |  0:14:06s
epoch 48 | loss: 0.64066 | eval_custom_logloss: 0.76971 |  0:14:24s
epoch 49 | loss: 0.64621 | eval_custom_logloss: 0.75179 |  0:14:41s
epoch 50 | loss: 0.64716 | eval_custom_logloss: 0.82113 |  0:14:59s

Early stopping occurred at epoch 50 with best_epoch = 30 and best_eval_custom_logloss = 0.66
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6266200000000001, 'Log Loss - std': 0.02946695776628458} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 0.6266200000000001 and parameters: {'n_d': 38, 'n_steps': 10, 'gamma': 1.330249081459642, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.002234609250146497, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6266200000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.31707 | eval_custom_logloss: 3.68888 |  0:00:10s
epoch 1  | loss: 1.02375 | eval_custom_logloss: 2.92406 |  0:00:18s
epoch 2  | loss: 0.9697  | eval_custom_logloss: 2.50254 |  0:00:28s
epoch 3  | loss: 0.92037 | eval_custom_logloss: 3.02366 |  0:00:39s
epoch 4  | loss: 0.90441 | eval_custom_logloss: 1.52664 |  0:00:49s
epoch 5  | loss: 0.86667 | eval_custom_logloss: 2.76782 |  0:00:59s
epoch 6  | loss: 0.88658 | eval_custom_logloss: 2.33082 |  0:01:09s
epoch 7  | loss: 0.83082 | eval_custom_logloss: 1.7812  |  0:01:19s
epoch 8  | loss: 0.83302 | eval_custom_logloss: 2.48361 |  0:01:29s
epoch 9  | loss: 0.82305 | eval_custom_logloss: 1.64146 |  0:01:39s
epoch 10 | loss: 0.81564 | eval_custom_logloss: 1.10188 |  0:01:49s
epoch 11 | loss: 0.81919 | eval_custom_logloss: 1.23101 |  0:01:59s
epoch 12 | loss: 0.78428 | eval_custom_logloss: 1.96316 |  0:02:09s
epoch 13 | loss: 0.77485 | eval_custom_logloss: 1.41638 |  0:02:19s
epoch 14 | loss: 0.78705 | eval_custom_logloss: 1.04817 |  0:02:28s
epoch 15 | loss: 0.78891 | eval_custom_logloss: 1.0451  |  0:02:38s
epoch 16 | loss: 0.77613 | eval_custom_logloss: 1.03426 |  0:02:48s
epoch 17 | loss: 0.74465 | eval_custom_logloss: 0.83606 |  0:02:58s
epoch 18 | loss: 0.75452 | eval_custom_logloss: 1.26847 |  0:03:08s
epoch 19 | loss: 0.75192 | eval_custom_logloss: 1.74444 |  0:03:18s
epoch 20 | loss: 0.74709 | eval_custom_logloss: 0.80519 |  0:03:28s
epoch 21 | loss: 0.73394 | eval_custom_logloss: 1.50107 |  0:03:38s
epoch 22 | loss: 0.74098 | eval_custom_logloss: 0.83539 |  0:03:48s
epoch 23 | loss: 0.73581 | eval_custom_logloss: 1.07956 |  0:03:59s
epoch 24 | loss: 0.72176 | eval_custom_logloss: 1.00904 |  0:04:09s
epoch 25 | loss: 0.72367 | eval_custom_logloss: 2.05061 |  0:04:18s
epoch 26 | loss: 0.72842 | eval_custom_logloss: 1.1237  |  0:04:28s
epoch 27 | loss: 0.70968 | eval_custom_logloss: 0.9722  |  0:04:38s
epoch 28 | loss: 0.70907 | eval_custom_logloss: 0.96758 |  0:04:49s
epoch 29 | loss: 0.70996 | eval_custom_logloss: 0.82188 |  0:04:59s
epoch 30 | loss: 0.70311 | eval_custom_logloss: 0.73808 |  0:05:09s
epoch 31 | loss: 0.70659 | eval_custom_logloss: 0.86519 |  0:05:19s
epoch 32 | loss: 0.71688 | eval_custom_logloss: 1.26058 |  0:05:29s
epoch 33 | loss: 0.70693 | eval_custom_logloss: 0.71394 |  0:05:39s
epoch 34 | loss: 0.68366 | eval_custom_logloss: 0.99945 |  0:05:49s
epoch 35 | loss: 0.69312 | eval_custom_logloss: 0.8648  |  0:05:59s
epoch 36 | loss: 0.69093 | eval_custom_logloss: 0.71054 |  0:06:09s
epoch 37 | loss: 0.69033 | eval_custom_logloss: 0.74792 |  0:06:19s
epoch 38 | loss: 0.68261 | eval_custom_logloss: 0.90278 |  0:06:28s
epoch 39 | loss: 0.68648 | eval_custom_logloss: 0.83859 |  0:06:35s
epoch 40 | loss: 0.68228 | eval_custom_logloss: 0.99718 |  0:06:43s
epoch 41 | loss: 0.67992 | eval_custom_logloss: 1.02409 |  0:06:52s
epoch 42 | loss: 0.675   | eval_custom_logloss: 0.809   |  0:07:02s
epoch 43 | loss: 0.68102 | eval_custom_logloss: 0.80508 |  0:07:11s
epoch 44 | loss: 0.68776 | eval_custom_logloss: 0.84973 |  0:07:20s
epoch 45 | loss: 0.68145 | eval_custom_logloss: 0.75175 |  0:07:30s
epoch 46 | loss: 0.66573 | eval_custom_logloss: 0.63863 |  0:07:40s
epoch 47 | loss: 0.67259 | eval_custom_logloss: 1.00028 |  0:07:51s
epoch 48 | loss: 0.67207 | eval_custom_logloss: 0.94545 |  0:08:01s
epoch 49 | loss: 0.67147 | eval_custom_logloss: 0.62622 |  0:08:11s
epoch 50 | loss: 0.67293 | eval_custom_logloss: 0.87154 |  0:08:21s
epoch 51 | loss: 0.6666  | eval_custom_logloss: 0.63797 |  0:08:32s
epoch 52 | loss: 0.67226 | eval_custom_logloss: 0.80338 |  0:08:40s
epoch 53 | loss: 0.67657 | eval_custom_logloss: 0.78436 |  0:08:50s
epoch 54 | loss: 0.67145 | eval_custom_logloss: 0.75894 |  0:09:00s
epoch 55 | loss: 0.66334 | eval_custom_logloss: 0.8536  |  0:09:10s
epoch 56 | loss: 0.6637  | eval_custom_logloss: 0.68968 |  0:09:21s
epoch 57 | loss: 0.66236 | eval_custom_logloss: 0.98982 |  0:09:31s
epoch 58 | loss: 0.65985 | eval_custom_logloss: 0.72189 |  0:09:42s
epoch 59 | loss: 0.66586 | eval_custom_logloss: 0.95274 |  0:09:51s
epoch 60 | loss: 0.65759 | eval_custom_logloss: 0.84611 |  0:10:02s
epoch 61 | loss: 0.66537 | eval_custom_logloss: 0.66996 |  0:10:12s
epoch 62 | loss: 0.66119 | eval_custom_logloss: 0.7858  |  0:10:22s
epoch 63 | loss: 0.64683 | eval_custom_logloss: 0.65945 |  0:10:32s
epoch 64 | loss: 0.6547  | eval_custom_logloss: 0.97533 |  0:10:42s
epoch 65 | loss: 0.66333 | eval_custom_logloss: 0.68288 |  0:10:52s
epoch 66 | loss: 0.65844 | eval_custom_logloss: 0.97343 |  0:11:02s
epoch 67 | loss: 0.64911 | eval_custom_logloss: 1.17154 |  0:11:12s
epoch 68 | loss: 0.65043 | eval_custom_logloss: 0.63159 |  0:11:20s
epoch 69 | loss: 0.65042 | eval_custom_logloss: 0.78835 |  0:11:29s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 0.62622
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6252, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.35817 | eval_custom_logloss: 5.95077 |  0:00:07s
epoch 1  | loss: 1.05662 | eval_custom_logloss: 4.32683 |  0:00:17s
epoch 2  | loss: 0.99968 | eval_custom_logloss: 2.29894 |  0:00:27s
epoch 3  | loss: 0.97287 | eval_custom_logloss: 1.57238 |  0:00:35s
epoch 4  | loss: 0.94214 | eval_custom_logloss: 1.32745 |  0:00:43s
epoch 5  | loss: 0.90842 | eval_custom_logloss: 1.86925 |  0:00:50s
epoch 6  | loss: 0.90096 | eval_custom_logloss: 1.41821 |  0:00:59s
epoch 7  | loss: 0.87463 | eval_custom_logloss: 1.34476 |  0:01:09s
epoch 8  | loss: 0.84754 | eval_custom_logloss: 1.43656 |  0:01:19s
epoch 9  | loss: 0.84332 | eval_custom_logloss: 1.03218 |  0:01:29s
epoch 10 | loss: 0.82365 | eval_custom_logloss: 1.13031 |  0:01:39s
epoch 11 | loss: 0.82554 | eval_custom_logloss: 1.02364 |  0:01:49s
epoch 12 | loss: 0.79026 | eval_custom_logloss: 1.23785 |  0:01:59s
epoch 13 | loss: 0.78678 | eval_custom_logloss: 0.96163 |  0:02:09s
epoch 14 | loss: 0.82278 | eval_custom_logloss: 1.17597 |  0:02:19s
epoch 15 | loss: 0.78429 | eval_custom_logloss: 1.12701 |  0:02:29s
epoch 16 | loss: 0.77603 | eval_custom_logloss: 1.41588 |  0:02:39s
epoch 17 | loss: 0.77608 | eval_custom_logloss: 0.82108 |  0:02:47s
epoch 18 | loss: 0.78093 | eval_custom_logloss: 0.96476 |  0:02:55s
epoch 19 | loss: 0.75961 | eval_custom_logloss: 1.35782 |  0:03:03s
epoch 20 | loss: 0.76351 | eval_custom_logloss: 0.85531 |  0:03:13s
epoch 21 | loss: 0.73904 | eval_custom_logloss: 0.81695 |  0:03:23s
epoch 22 | loss: 0.74366 | eval_custom_logloss: 1.29641 |  0:03:33s
epoch 23 | loss: 0.74552 | eval_custom_logloss: 0.92712 |  0:03:43s
epoch 24 | loss: 0.74824 | eval_custom_logloss: 1.05695 |  0:03:53s
epoch 25 | loss: 0.75758 | eval_custom_logloss: 0.97186 |  0:04:03s
epoch 26 | loss: 0.73272 | eval_custom_logloss: 0.92013 |  0:04:13s
epoch 27 | loss: 0.72814 | eval_custom_logloss: 1.05701 |  0:04:23s
epoch 28 | loss: 0.7496  | eval_custom_logloss: 0.90851 |  0:04:32s
epoch 29 | loss: 0.74567 | eval_custom_logloss: 0.96805 |  0:04:42s
epoch 30 | loss: 0.72103 | eval_custom_logloss: 0.80587 |  0:04:52s
epoch 31 | loss: 0.70885 | eval_custom_logloss: 0.99207 |  0:05:02s
epoch 32 | loss: 0.71307 | eval_custom_logloss: 0.89116 |  0:05:12s
epoch 33 | loss: 0.6958  | eval_custom_logloss: 0.8757  |  0:05:22s
epoch 34 | loss: 0.69475 | eval_custom_logloss: 0.76598 |  0:05:32s
epoch 35 | loss: 0.70342 | eval_custom_logloss: 0.65367 |  0:05:42s
epoch 36 | loss: 0.70948 | eval_custom_logloss: 1.04375 |  0:05:52s
epoch 37 | loss: 0.70066 | eval_custom_logloss: 0.84646 |  0:06:03s
epoch 38 | loss: 0.69067 | eval_custom_logloss: 1.31157 |  0:06:13s
epoch 39 | loss: 0.73473 | eval_custom_logloss: 0.71289 |  0:06:23s
epoch 40 | loss: 0.70039 | eval_custom_logloss: 0.78828 |  0:06:33s
epoch 41 | loss: 0.70405 | eval_custom_logloss: 0.92778 |  0:06:42s
epoch 42 | loss: 0.68966 | eval_custom_logloss: 0.76217 |  0:06:52s
epoch 43 | loss: 0.68972 | eval_custom_logloss: 0.72714 |  0:07:02s
epoch 44 | loss: 0.69384 | eval_custom_logloss: 1.05589 |  0:07:12s
epoch 45 | loss: 0.67688 | eval_custom_logloss: 0.73586 |  0:07:22s
epoch 46 | loss: 0.67292 | eval_custom_logloss: 0.80613 |  0:07:32s
epoch 47 | loss: 0.66915 | eval_custom_logloss: 0.9146  |  0:07:42s
epoch 48 | loss: 0.6677  | eval_custom_logloss: 1.0158  |  0:07:52s
epoch 49 | loss: 0.68177 | eval_custom_logloss: 0.78016 |  0:08:02s
epoch 50 | loss: 0.67174 | eval_custom_logloss: 0.84621 |  0:08:12s
epoch 51 | loss: 0.67196 | eval_custom_logloss: 0.73165 |  0:08:21s
epoch 52 | loss: 0.67051 | eval_custom_logloss: 0.68694 |  0:08:31s
epoch 53 | loss: 0.65934 | eval_custom_logloss: 0.96363 |  0:08:41s
epoch 54 | loss: 0.66596 | eval_custom_logloss: 0.77433 |  0:08:51s
epoch 55 | loss: 0.67057 | eval_custom_logloss: 0.79335 |  0:09:01s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 0.65367
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6388499999999999, 'Log Loss - std': 0.013649999999999995} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.38874 | eval_custom_logloss: 3.31115 |  0:00:09s
epoch 1  | loss: 1.03646 | eval_custom_logloss: 3.19733 |  0:00:20s
epoch 2  | loss: 0.99337 | eval_custom_logloss: 2.30775 |  0:00:30s
epoch 3  | loss: 0.93974 | eval_custom_logloss: 1.87545 |  0:00:40s
epoch 4  | loss: 0.94216 | eval_custom_logloss: 1.15723 |  0:00:50s
epoch 5  | loss: 0.92302 | eval_custom_logloss: 1.28479 |  0:01:00s
epoch 6  | loss: 0.88869 | eval_custom_logloss: 1.07614 |  0:01:09s
epoch 7  | loss: 0.88483 | eval_custom_logloss: 1.29315 |  0:01:20s
epoch 8  | loss: 0.85799 | eval_custom_logloss: 1.33571 |  0:01:30s
epoch 9  | loss: 0.85646 | eval_custom_logloss: 1.12493 |  0:01:39s
epoch 10 | loss: 0.81845 | eval_custom_logloss: 1.00003 |  0:01:46s
epoch 11 | loss: 0.82026 | eval_custom_logloss: 1.01217 |  0:01:54s
epoch 12 | loss: 0.79615 | eval_custom_logloss: 0.88939 |  0:02:04s
epoch 13 | loss: 0.79544 | eval_custom_logloss: 1.17443 |  0:02:14s
epoch 14 | loss: 0.79736 | eval_custom_logloss: 0.8572  |  0:02:24s
epoch 15 | loss: 0.77657 | eval_custom_logloss: 0.96142 |  0:02:34s
epoch 16 | loss: 0.79003 | eval_custom_logloss: 1.13062 |  0:02:42s
epoch 17 | loss: 0.76294 | eval_custom_logloss: 0.90393 |  0:02:52s
epoch 18 | loss: 0.766   | eval_custom_logloss: 0.73746 |  0:03:00s
epoch 19 | loss: 0.7469  | eval_custom_logloss: 1.17738 |  0:03:08s
epoch 20 | loss: 0.75269 | eval_custom_logloss: 0.85208 |  0:03:16s
epoch 21 | loss: 0.75012 | eval_custom_logloss: 0.84464 |  0:03:26s
epoch 22 | loss: 0.73704 | eval_custom_logloss: 0.93689 |  0:03:34s
epoch 23 | loss: 0.72356 | eval_custom_logloss: 0.93242 |  0:03:42s
epoch 24 | loss: 0.73236 | eval_custom_logloss: 0.96899 |  0:03:49s
epoch 25 | loss: 0.72954 | eval_custom_logloss: 0.94327 |  0:03:59s
epoch 26 | loss: 0.72924 | eval_custom_logloss: 0.87651 |  0:04:09s
epoch 27 | loss: 0.71196 | eval_custom_logloss: 0.9353  |  0:04:19s
epoch 28 | loss: 0.7115  | eval_custom_logloss: 0.72806 |  0:04:29s
epoch 29 | loss: 0.70923 | eval_custom_logloss: 0.87917 |  0:04:39s
epoch 30 | loss: 0.71053 | eval_custom_logloss: 0.7016  |  0:04:49s
epoch 31 | loss: 0.70169 | eval_custom_logloss: 0.7211  |  0:04:59s
epoch 32 | loss: 0.70176 | eval_custom_logloss: 0.82486 |  0:05:09s
epoch 33 | loss: 0.70204 | eval_custom_logloss: 1.11669 |  0:05:19s
epoch 34 | loss: 0.68133 | eval_custom_logloss: 0.86882 |  0:05:29s
epoch 35 | loss: 0.68283 | eval_custom_logloss: 1.01398 |  0:05:39s
epoch 36 | loss: 0.7073  | eval_custom_logloss: 1.07898 |  0:05:46s
epoch 37 | loss: 0.67211 | eval_custom_logloss: 0.82142 |  0:05:54s
epoch 38 | loss: 0.68153 | eval_custom_logloss: 0.78272 |  0:06:02s
epoch 39 | loss: 0.68093 | eval_custom_logloss: 0.81652 |  0:06:09s
epoch 40 | loss: 0.67536 | eval_custom_logloss: 1.10869 |  0:06:17s
epoch 41 | loss: 0.66585 | eval_custom_logloss: 0.67572 |  0:06:26s
epoch 42 | loss: 0.70165 | eval_custom_logloss: 0.73755 |  0:06:36s
epoch 43 | loss: 0.66809 | eval_custom_logloss: 1.04912 |  0:06:46s
epoch 44 | loss: 0.6766  | eval_custom_logloss: 1.02321 |  0:06:56s
epoch 45 | loss: 0.66651 | eval_custom_logloss: 0.83954 |  0:07:06s
epoch 46 | loss: 0.67011 | eval_custom_logloss: 0.99489 |  0:07:14s
epoch 47 | loss: 0.66663 | eval_custom_logloss: 0.83698 |  0:07:24s
epoch 48 | loss: 0.66436 | eval_custom_logloss: 0.77416 |  0:07:35s
epoch 49 | loss: 0.67182 | eval_custom_logloss: 1.0651  |  0:07:45s
epoch 50 | loss: 0.6781  | eval_custom_logloss: 0.97171 |  0:07:55s
epoch 51 | loss: 0.64967 | eval_custom_logloss: 0.79512 |  0:08:05s
epoch 52 | loss: 0.66299 | eval_custom_logloss: 1.19089 |  0:08:15s
epoch 53 | loss: 0.66019 | eval_custom_logloss: 0.68113 |  0:08:25s
epoch 54 | loss: 0.66173 | eval_custom_logloss: 0.67082 |  0:08:35s
epoch 55 | loss: 0.6551  | eval_custom_logloss: 0.79699 |  0:08:45s
epoch 56 | loss: 0.65826 | eval_custom_logloss: 0.73781 |  0:08:55s
epoch 57 | loss: 0.67323 | eval_custom_logloss: 0.66511 |  0:09:05s
epoch 58 | loss: 0.64565 | eval_custom_logloss: 0.85226 |  0:09:16s
epoch 59 | loss: 0.65472 | eval_custom_logloss: 0.73496 |  0:09:26s
epoch 60 | loss: 0.64217 | eval_custom_logloss: 0.69052 |  0:09:36s
epoch 61 | loss: 0.65099 | eval_custom_logloss: 0.7267  |  0:09:46s
epoch 62 | loss: 0.64436 | eval_custom_logloss: 0.65194 |  0:09:56s
epoch 63 | loss: 0.65376 | eval_custom_logloss: 1.0826  |  0:10:06s
epoch 64 | loss: 0.66551 | eval_custom_logloss: 0.78325 |  0:10:15s
epoch 65 | loss: 0.65383 | eval_custom_logloss: 0.85073 |  0:10:25s
epoch 66 | loss: 0.66744 | eval_custom_logloss: 0.87262 |  0:10:36s
epoch 67 | loss: 0.64717 | eval_custom_logloss: 0.6207  |  0:10:46s
epoch 68 | loss: 0.64704 | eval_custom_logloss: 0.95824 |  0:10:56s
epoch 69 | loss: 0.6523  | eval_custom_logloss: 0.69841 |  0:11:06s
epoch 70 | loss: 0.64902 | eval_custom_logloss: 0.59516 |  0:11:15s
epoch 71 | loss: 0.63993 | eval_custom_logloss: 0.84318 |  0:11:25s
epoch 72 | loss: 0.6419  | eval_custom_logloss: 0.91079 |  0:11:35s
epoch 73 | loss: 0.63949 | eval_custom_logloss: 1.09183 |  0:11:46s
epoch 74 | loss: 0.64497 | eval_custom_logloss: 0.63652 |  0:11:56s
epoch 75 | loss: 0.63259 | eval_custom_logloss: 0.83869 |  0:12:06s
epoch 76 | loss: 0.63166 | eval_custom_logloss: 0.79828 |  0:12:16s
epoch 77 | loss: 0.62831 | eval_custom_logloss: 0.8212  |  0:12:26s
epoch 78 | loss: 0.64244 | eval_custom_logloss: 0.75361 |  0:12:33s
epoch 79 | loss: 0.63888 | eval_custom_logloss: 0.91232 |  0:12:42s
epoch 80 | loss: 0.63563 | eval_custom_logloss: 1.05431 |  0:12:50s
epoch 81 | loss: 0.62442 | eval_custom_logloss: 0.72048 |  0:12:59s
epoch 82 | loss: 0.62176 | eval_custom_logloss: 0.73019 |  0:13:09s
epoch 83 | loss: 0.63665 | eval_custom_logloss: 1.23216 |  0:13:20s
epoch 84 | loss: 0.63028 | eval_custom_logloss: 1.03872 |  0:13:30s
epoch 85 | loss: 0.64385 | eval_custom_logloss: 0.6208  |  0:13:40s
epoch 86 | loss: 0.62502 | eval_custom_logloss: 0.61441 |  0:13:50s
epoch 87 | loss: 0.63219 | eval_custom_logloss: 0.98204 |  0:14:00s
epoch 88 | loss: 0.62816 | eval_custom_logloss: 0.90694 |  0:14:10s
epoch 89 | loss: 0.62339 | eval_custom_logloss: 0.9456  |  0:14:20s
epoch 90 | loss: 0.63586 | eval_custom_logloss: 0.98595 |  0:14:30s

Early stopping occurred at epoch 90 with best_epoch = 70 and best_eval_custom_logloss = 0.59516
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6236999999999999, 'Log Loss - std': 0.024150776385035714} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.34997 | eval_custom_logloss: 4.13807 |  0:00:10s
epoch 1  | loss: 1.01143 | eval_custom_logloss: 2.60134 |  0:00:20s
epoch 2  | loss: 0.96972 | eval_custom_logloss: 1.40837 |  0:00:27s
epoch 3  | loss: 0.92649 | eval_custom_logloss: 1.1041  |  0:00:35s
epoch 4  | loss: 0.89848 | eval_custom_logloss: 1.03778 |  0:00:44s
epoch 5  | loss: 0.87625 | eval_custom_logloss: 1.34965 |  0:00:54s
epoch 6  | loss: 0.88268 | eval_custom_logloss: 1.45892 |  0:01:01s
epoch 7  | loss: 0.8189  | eval_custom_logloss: 1.07705 |  0:01:10s
epoch 8  | loss: 0.81828 | eval_custom_logloss: 0.96929 |  0:01:17s
epoch 9  | loss: 0.81432 | eval_custom_logloss: 1.28397 |  0:01:25s
epoch 10 | loss: 0.775   | eval_custom_logloss: 1.14557 |  0:01:32s
epoch 11 | loss: 0.78226 | eval_custom_logloss: 0.81656 |  0:01:42s
epoch 12 | loss: 0.7611  | eval_custom_logloss: 0.82521 |  0:01:52s
epoch 13 | loss: 0.74951 | eval_custom_logloss: 0.73199 |  0:02:03s
epoch 14 | loss: 0.78143 | eval_custom_logloss: 0.93819 |  0:02:13s
epoch 15 | loss: 0.76139 | eval_custom_logloss: 0.97339 |  0:02:23s
epoch 16 | loss: 0.75374 | eval_custom_logloss: 0.99087 |  0:02:34s
epoch 17 | loss: 0.75445 | eval_custom_logloss: 1.04495 |  0:02:44s
epoch 18 | loss: 0.75664 | eval_custom_logloss: 0.93585 |  0:02:54s
epoch 19 | loss: 0.74078 | eval_custom_logloss: 0.76823 |  0:03:04s
epoch 20 | loss: 0.75625 | eval_custom_logloss: 0.92326 |  0:03:15s
epoch 21 | loss: 0.71658 | eval_custom_logloss: 0.91713 |  0:03:25s
epoch 22 | loss: 0.718   | eval_custom_logloss: 0.90481 |  0:03:35s
epoch 23 | loss: 0.72761 | eval_custom_logloss: 0.6957  |  0:03:43s
epoch 24 | loss: 0.72693 | eval_custom_logloss: 1.03119 |  0:03:51s
epoch 25 | loss: 0.72699 | eval_custom_logloss: 0.99691 |  0:04:00s
epoch 26 | loss: 0.704   | eval_custom_logloss: 0.69094 |  0:04:09s
epoch 27 | loss: 0.71024 | eval_custom_logloss: 0.86115 |  0:04:17s
epoch 28 | loss: 0.69762 | eval_custom_logloss: 0.91913 |  0:04:25s
epoch 29 | loss: 0.69991 | eval_custom_logloss: 1.03416 |  0:04:34s
epoch 30 | loss: 0.69637 | eval_custom_logloss: 0.89102 |  0:04:43s
epoch 31 | loss: 0.70678 | eval_custom_logloss: 0.69883 |  0:04:53s
epoch 32 | loss: 0.70182 | eval_custom_logloss: 0.71122 |  0:05:03s
epoch 33 | loss: 0.70297 | eval_custom_logloss: 0.84485 |  0:05:13s
epoch 34 | loss: 0.69947 | eval_custom_logloss: 0.73658 |  0:05:23s
epoch 35 | loss: 0.69539 | eval_custom_logloss: 0.79925 |  0:05:33s
epoch 36 | loss: 0.69028 | eval_custom_logloss: 0.94657 |  0:05:42s
epoch 37 | loss: 0.68654 | eval_custom_logloss: 0.95227 |  0:05:53s
epoch 38 | loss: 0.68692 | eval_custom_logloss: 1.26118 |  0:06:03s
epoch 39 | loss: 0.69476 | eval_custom_logloss: 0.65759 |  0:06:13s
epoch 40 | loss: 0.67596 | eval_custom_logloss: 0.96154 |  0:06:23s
epoch 41 | loss: 0.67991 | eval_custom_logloss: 1.27989 |  0:06:33s
epoch 42 | loss: 0.69026 | eval_custom_logloss: 0.7549  |  0:06:43s
epoch 43 | loss: 0.6851  | eval_custom_logloss: 0.64446 |  0:06:53s
epoch 44 | loss: 0.68033 | eval_custom_logloss: 0.67163 |  0:07:03s
epoch 45 | loss: 0.67764 | eval_custom_logloss: 1.0412  |  0:07:12s
epoch 46 | loss: 0.67032 | eval_custom_logloss: 0.98491 |  0:07:22s
epoch 47 | loss: 0.68288 | eval_custom_logloss: 0.86423 |  0:07:33s
epoch 48 | loss: 0.68174 | eval_custom_logloss: 0.71147 |  0:07:42s
epoch 49 | loss: 0.67964 | eval_custom_logloss: 0.73926 |  0:07:52s
epoch 50 | loss: 0.66484 | eval_custom_logloss: 0.76537 |  0:08:02s
epoch 51 | loss: 0.66365 | eval_custom_logloss: 0.88832 |  0:08:12s
epoch 52 | loss: 0.64555 | eval_custom_logloss: 0.632   |  0:08:22s
epoch 53 | loss: 0.66893 | eval_custom_logloss: 0.68779 |  0:08:32s
epoch 54 | loss: 0.66976 | eval_custom_logloss: 0.67076 |  0:08:42s
epoch 55 | loss: 0.66577 | eval_custom_logloss: 0.72002 |  0:08:52s
epoch 56 | loss: 0.65981 | eval_custom_logloss: 0.99569 |  0:09:02s
epoch 57 | loss: 0.6545  | eval_custom_logloss: 0.93889 |  0:09:12s
epoch 58 | loss: 0.654   | eval_custom_logloss: 0.99495 |  0:09:22s
epoch 59 | loss: 0.65505 | eval_custom_logloss: 0.89833 |  0:09:32s
epoch 60 | loss: 0.65988 | eval_custom_logloss: 1.04673 |  0:09:42s
epoch 61 | loss: 0.65914 | eval_custom_logloss: 0.79494 |  0:09:52s
epoch 62 | loss: 0.65296 | eval_custom_logloss: 0.60935 |  0:10:02s
epoch 63 | loss: 0.64884 | eval_custom_logloss: 0.60439 |  0:10:12s
epoch 64 | loss: 0.66281 | eval_custom_logloss: 0.83977 |  0:10:22s
epoch 65 | loss: 0.63445 | eval_custom_logloss: 0.74267 |  0:10:32s
epoch 66 | loss: 0.64326 | eval_custom_logloss: 0.66162 |  0:10:42s
epoch 67 | loss: 0.65135 | eval_custom_logloss: 1.28296 |  0:10:52s
epoch 68 | loss: 0.64379 | eval_custom_logloss: 0.91019 |  0:11:02s
epoch 69 | loss: 0.65246 | eval_custom_logloss: 0.72349 |  0:11:09s
epoch 70 | loss: 0.64993 | eval_custom_logloss: 0.71281 |  0:11:18s
epoch 71 | loss: 0.63364 | eval_custom_logloss: 0.83398 |  0:11:28s
epoch 72 | loss: 0.64031 | eval_custom_logloss: 0.80159 |  0:11:38s
epoch 73 | loss: 0.63211 | eval_custom_logloss: 0.69423 |  0:11:48s
epoch 74 | loss: 0.64616 | eval_custom_logloss: 0.66047 |  0:11:58s
epoch 75 | loss: 0.63192 | eval_custom_logloss: 0.96071 |  0:12:08s
epoch 76 | loss: 0.63621 | eval_custom_logloss: 1.01394 |  0:12:18s
epoch 77 | loss: 0.64067 | eval_custom_logloss: 0.63851 |  0:12:28s
epoch 78 | loss: 0.63694 | eval_custom_logloss: 0.66907 |  0:12:38s
epoch 79 | loss: 0.63447 | eval_custom_logloss: 0.86095 |  0:12:48s
epoch 80 | loss: 0.63578 | eval_custom_logloss: 0.71001 |  0:12:58s
epoch 81 | loss: 0.64458 | eval_custom_logloss: 0.68157 |  0:13:08s
epoch 82 | loss: 0.63158 | eval_custom_logloss: 1.1379  |  0:13:18s
epoch 83 | loss: 0.63632 | eval_custom_logloss: 0.58866 |  0:13:28s
epoch 84 | loss: 0.63698 | eval_custom_logloss: 0.61933 |  0:13:38s
epoch 85 | loss: 0.6365  | eval_custom_logloss: 1.19703 |  0:13:48s
epoch 86 | loss: 0.63588 | eval_custom_logloss: 0.67346 |  0:13:57s
epoch 87 | loss: 0.63804 | eval_custom_logloss: 0.58824 |  0:14:07s
epoch 88 | loss: 0.62804 | eval_custom_logloss: 1.39609 |  0:14:15s
epoch 89 | loss: 0.62744 | eval_custom_logloss: 0.92464 |  0:14:22s
epoch 90 | loss: 0.63259 | eval_custom_logloss: 0.70841 |  0:14:30s
epoch 91 | loss: 0.62723 | eval_custom_logloss: 0.65454 |  0:14:40s
epoch 92 | loss: 0.62031 | eval_custom_logloss: 0.63798 |  0:14:50s
epoch 93 | loss: 0.6116  | eval_custom_logloss: 0.74457 |  0:15:00s
epoch 94 | loss: 0.63244 | eval_custom_logloss: 0.6905  |  0:15:11s
epoch 95 | loss: 0.62464 | eval_custom_logloss: 0.63893 |  0:15:20s
epoch 96 | loss: 0.61898 | eval_custom_logloss: 0.91593 |  0:15:30s
epoch 97 | loss: 0.63149 | eval_custom_logloss: 0.68111 |  0:15:40s
epoch 98 | loss: 0.63395 | eval_custom_logloss: 0.76185 |  0:15:50s
epoch 99 | loss: 0.6188  | eval_custom_logloss: 0.60829 |  0:16:00s
Stop training because you reached max_epochs = 100 with best_epoch = 87 and best_eval_custom_logloss = 0.58824
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.614725, 'Log Loss - std': 0.026059487235937682} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.37004 | eval_custom_logloss: 2.94598 |  0:00:10s
epoch 1  | loss: 1.04851 | eval_custom_logloss: 3.16175 |  0:00:20s
epoch 2  | loss: 0.99864 | eval_custom_logloss: 2.14366 |  0:00:27s
epoch 3  | loss: 0.93785 | eval_custom_logloss: 1.80813 |  0:00:35s
epoch 4  | loss: 0.92549 | eval_custom_logloss: 2.62092 |  0:00:44s
epoch 5  | loss: 0.89834 | eval_custom_logloss: 1.41644 |  0:00:53s
epoch 6  | loss: 0.89383 | eval_custom_logloss: 1.28553 |  0:01:02s
epoch 7  | loss: 0.85537 | eval_custom_logloss: 1.72959 |  0:01:10s
epoch 8  | loss: 0.84561 | eval_custom_logloss: 0.96126 |  0:01:18s
epoch 9  | loss: 0.84482 | eval_custom_logloss: 1.02482 |  0:01:25s
epoch 10 | loss: 0.83374 | eval_custom_logloss: 1.05844 |  0:01:33s
epoch 11 | loss: 0.81408 | eval_custom_logloss: 0.99493 |  0:01:41s
epoch 12 | loss: 0.79561 | eval_custom_logloss: 0.98173 |  0:01:50s
epoch 13 | loss: 0.78528 | eval_custom_logloss: 1.05008 |  0:02:00s
epoch 14 | loss: 0.80117 | eval_custom_logloss: 1.2677  |  0:02:10s
epoch 15 | loss: 0.79365 | eval_custom_logloss: 1.01883 |  0:02:20s
epoch 16 | loss: 0.77076 | eval_custom_logloss: 0.84856 |  0:02:31s
epoch 17 | loss: 0.76364 | eval_custom_logloss: 1.06479 |  0:02:40s
epoch 18 | loss: 0.75757 | eval_custom_logloss: 0.99383 |  0:02:50s
epoch 19 | loss: 0.74713 | eval_custom_logloss: 1.01742 |  0:03:00s
epoch 20 | loss: 0.74677 | eval_custom_logloss: 0.80992 |  0:03:10s
epoch 21 | loss: 0.73413 | eval_custom_logloss: 0.89146 |  0:03:20s
epoch 22 | loss: 0.73012 | eval_custom_logloss: 0.89163 |  0:03:30s
epoch 23 | loss: 0.72595 | eval_custom_logloss: 0.88358 |  0:03:40s
epoch 24 | loss: 0.74241 | eval_custom_logloss: 1.3426  |  0:03:50s
epoch 25 | loss: 0.73611 | eval_custom_logloss: 0.97287 |  0:04:00s
epoch 26 | loss: 0.7249  | eval_custom_logloss: 0.76553 |  0:04:10s
epoch 27 | loss: 0.72336 | eval_custom_logloss: 0.81266 |  0:04:20s
epoch 28 | loss: 0.7265  | eval_custom_logloss: 0.85097 |  0:04:29s
epoch 29 | loss: 0.71429 | eval_custom_logloss: 0.91243 |  0:04:39s
epoch 30 | loss: 0.70526 | eval_custom_logloss: 0.86047 |  0:04:49s
epoch 31 | loss: 0.70632 | eval_custom_logloss: 0.89899 |  0:04:59s
epoch 32 | loss: 0.71306 | eval_custom_logloss: 0.97622 |  0:05:09s
epoch 33 | loss: 0.70943 | eval_custom_logloss: 0.84984 |  0:05:19s
epoch 34 | loss: 0.69615 | eval_custom_logloss: 0.70647 |  0:05:29s
epoch 35 | loss: 0.69763 | eval_custom_logloss: 0.97101 |  0:05:39s
epoch 36 | loss: 0.70478 | eval_custom_logloss: 0.90968 |  0:05:49s
epoch 37 | loss: 0.68614 | eval_custom_logloss: 0.82448 |  0:05:59s
epoch 38 | loss: 0.68943 | eval_custom_logloss: 0.73255 |  0:06:09s
epoch 39 | loss: 0.70581 | eval_custom_logloss: 0.89569 |  0:06:19s
epoch 40 | loss: 0.69664 | eval_custom_logloss: 0.64933 |  0:06:29s
epoch 41 | loss: 0.67568 | eval_custom_logloss: 0.70206 |  0:06:39s
epoch 42 | loss: 0.68602 | eval_custom_logloss: 0.89518 |  0:06:49s
epoch 43 | loss: 0.68402 | eval_custom_logloss: 0.71577 |  0:06:59s
epoch 44 | loss: 0.67995 | eval_custom_logloss: 0.8319  |  0:07:09s
epoch 45 | loss: 0.69102 | eval_custom_logloss: 1.29085 |  0:07:19s
epoch 46 | loss: 0.6769  | eval_custom_logloss: 0.65201 |  0:07:29s
epoch 47 | loss: 0.68504 | eval_custom_logloss: 0.78354 |  0:07:39s
epoch 48 | loss: 0.68233 | eval_custom_logloss: 0.98849 |  0:07:49s
epoch 49 | loss: 0.68096 | eval_custom_logloss: 0.96248 |  0:07:59s
epoch 50 | loss: 0.67854 | eval_custom_logloss: 0.86531 |  0:08:09s
epoch 51 | loss: 0.66862 | eval_custom_logloss: 0.77074 |  0:08:19s
epoch 52 | loss: 0.66594 | eval_custom_logloss: 0.85581 |  0:08:29s
epoch 53 | loss: 0.6749  | eval_custom_logloss: 0.72248 |  0:08:38s
epoch 54 | loss: 0.67804 | eval_custom_logloss: 0.99003 |  0:08:48s
epoch 55 | loss: 0.66473 | eval_custom_logloss: 1.0449  |  0:08:58s
epoch 56 | loss: 0.66526 | eval_custom_logloss: 0.77352 |  0:09:08s
epoch 57 | loss: 0.66992 | eval_custom_logloss: 0.7151  |  0:09:18s
epoch 58 | loss: 0.66054 | eval_custom_logloss: 0.92733 |  0:09:28s
epoch 59 | loss: 0.66036 | eval_custom_logloss: 1.09322 |  0:09:38s
epoch 60 | loss: 0.66017 | eval_custom_logloss: 0.88645 |  0:09:48s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.64933
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62134, 'Log Loss - std': 0.026801313400652584} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 0.62134 and parameters: {'n_d': 20, 'n_steps': 3, 'gamma': 1.6069349907047779, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0023270964946622195, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.6266200000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.57211 | eval_custom_logloss: 1.1941  |  0:00:16s
epoch 1  | loss: 1.15586 | eval_custom_logloss: 1.1133  |  0:00:32s
epoch 2  | loss: 1.02788 | eval_custom_logloss: 0.86563 |  0:00:49s
epoch 3  | loss: 0.96761 | eval_custom_logloss: 0.88298 |  0:01:05s
epoch 4  | loss: 0.95694 | eval_custom_logloss: 0.9906  |  0:01:21s
epoch 5  | loss: 0.94254 | eval_custom_logloss: 0.83407 |  0:01:38s
epoch 6  | loss: 0.92126 | eval_custom_logloss: 0.79264 |  0:01:54s
epoch 7  | loss: 0.88711 | eval_custom_logloss: 0.80267 |  0:02:11s
epoch 8  | loss: 0.87204 | eval_custom_logloss: 0.79099 |  0:02:27s
epoch 9  | loss: 0.87931 | eval_custom_logloss: 0.83762 |  0:02:43s
epoch 10 | loss: 0.85055 | eval_custom_logloss: 0.78381 |  0:03:00s
epoch 11 | loss: 0.83225 | eval_custom_logloss: 0.92352 |  0:03:16s
epoch 12 | loss: 0.82638 | eval_custom_logloss: 0.74889 |  0:03:33s
epoch 13 | loss: 0.83457 | eval_custom_logloss: 0.78761 |  0:03:49s
epoch 14 | loss: 0.83012 | eval_custom_logloss: 0.7658  |  0:04:05s
epoch 15 | loss: 0.81908 | eval_custom_logloss: 0.76462 |  0:04:22s
epoch 16 | loss: 0.82056 | eval_custom_logloss: 0.78431 |  0:04:38s
epoch 17 | loss: 0.832   | eval_custom_logloss: 1.29593 |  0:04:54s
epoch 18 | loss: 0.81134 | eval_custom_logloss: 0.73447 |  0:05:10s
epoch 19 | loss: 0.80897 | eval_custom_logloss: 0.73159 |  0:05:27s
epoch 20 | loss: 0.77908 | eval_custom_logloss: 0.76057 |  0:05:43s
epoch 21 | loss: 0.78095 | eval_custom_logloss: 0.78333 |  0:06:00s
epoch 22 | loss: 0.77354 | eval_custom_logloss: 0.74878 |  0:06:16s
epoch 23 | loss: 0.76591 | eval_custom_logloss: 0.75726 |  0:06:33s
epoch 24 | loss: 0.76967 | eval_custom_logloss: 0.70436 |  0:06:50s
epoch 25 | loss: 0.76065 | eval_custom_logloss: 0.92647 |  0:07:06s
epoch 26 | loss: 0.78329 | eval_custom_logloss: 0.70558 |  0:07:23s
epoch 27 | loss: 0.77186 | eval_custom_logloss: 0.77246 |  0:07:39s
epoch 28 | loss: 0.76571 | eval_custom_logloss: 0.83674 |  0:07:55s
epoch 29 | loss: 0.74806 | eval_custom_logloss: 0.71516 |  0:08:12s
epoch 30 | loss: 0.76806 | eval_custom_logloss: 1.35855 |  0:08:28s
epoch 31 | loss: 0.76848 | eval_custom_logloss: 0.83387 |  0:08:44s
epoch 32 | loss: 0.73608 | eval_custom_logloss: 0.67684 |  0:09:01s
epoch 33 | loss: 0.73939 | eval_custom_logloss: 0.75471 |  0:09:17s
epoch 34 | loss: 0.7254  | eval_custom_logloss: 0.8255  |  0:09:33s
epoch 35 | loss: 0.74148 | eval_custom_logloss: 1.1194  |  0:09:49s
epoch 36 | loss: 0.7175  | eval_custom_logloss: 0.79175 |  0:10:06s
epoch 37 | loss: 0.70822 | eval_custom_logloss: 0.66007 |  0:10:22s
epoch 38 | loss: 0.72093 | eval_custom_logloss: 0.69484 |  0:10:39s
epoch 39 | loss: 0.75227 | eval_custom_logloss: 0.80791 |  0:10:55s
epoch 40 | loss: 0.73119 | eval_custom_logloss: 1.32579 |  0:11:12s
epoch 41 | loss: 0.71923 | eval_custom_logloss: 0.67867 |  0:11:28s
epoch 42 | loss: 0.70917 | eval_custom_logloss: 0.66166 |  0:11:45s
epoch 43 | loss: 0.71142 | eval_custom_logloss: 0.67662 |  0:12:01s
epoch 44 | loss: 0.70001 | eval_custom_logloss: 0.74768 |  0:12:18s
epoch 45 | loss: 0.7082  | eval_custom_logloss: 0.7925  |  0:12:35s
epoch 46 | loss: 0.71395 | eval_custom_logloss: 0.79872 |  0:12:51s
epoch 47 | loss: 0.69896 | eval_custom_logloss: 0.67246 |  0:13:08s
epoch 48 | loss: 0.69454 | eval_custom_logloss: 0.86833 |  0:13:24s
epoch 49 | loss: 0.69928 | eval_custom_logloss: 0.69319 |  0:13:40s
epoch 50 | loss: 0.69747 | eval_custom_logloss: 0.67204 |  0:13:57s
epoch 51 | loss: 0.68825 | eval_custom_logloss: 0.6578  |  0:14:13s
epoch 52 | loss: 0.69947 | eval_custom_logloss: 1.01612 |  0:14:29s
epoch 53 | loss: 0.6884  | eval_custom_logloss: 0.67102 |  0:14:46s
epoch 54 | loss: 0.67983 | eval_custom_logloss: 0.63324 |  0:15:02s
epoch 55 | loss: 0.69834 | eval_custom_logloss: 0.70429 |  0:15:18s
epoch 56 | loss: 0.68515 | eval_custom_logloss: 0.68716 |  0:15:35s
epoch 57 | loss: 0.6691  | eval_custom_logloss: 0.7435  |  0:15:51s
epoch 58 | loss: 0.68653 | eval_custom_logloss: 0.71639 |  0:16:08s
epoch 59 | loss: 0.69508 | eval_custom_logloss: 0.72261 |  0:16:24s
epoch 60 | loss: 0.72467 | eval_custom_logloss: 1.0351  |  0:16:40s
epoch 61 | loss: 0.6973  | eval_custom_logloss: 0.68359 |  0:16:57s
epoch 62 | loss: 0.70066 | eval_custom_logloss: 0.6521  |  0:17:13s
epoch 63 | loss: 0.69373 | eval_custom_logloss: 0.6771  |  0:17:29s
epoch 64 | loss: 0.69207 | eval_custom_logloss: 0.83034 |  0:17:46s
epoch 65 | loss: 0.68608 | eval_custom_logloss: 0.78685 |  0:18:02s
epoch 66 | loss: 0.67694 | eval_custom_logloss: 0.86513 |  0:18:18s
epoch 67 | loss: 0.67754 | eval_custom_logloss: 0.67444 |  0:18:35s
epoch 68 | loss: 0.69388 | eval_custom_logloss: 0.67899 |  0:18:51s
epoch 69 | loss: 0.6772  | eval_custom_logloss: 0.63838 |  0:19:08s
epoch 70 | loss: 0.67578 | eval_custom_logloss: 0.66175 |  0:19:24s
epoch 71 | loss: 0.6789  | eval_custom_logloss: 0.65525 |  0:19:40s
epoch 72 | loss: 0.66761 | eval_custom_logloss: 0.63182 |  0:19:57s
epoch 73 | loss: 0.65961 | eval_custom_logloss: 0.65933 |  0:20:13s
epoch 74 | loss: 0.68092 | eval_custom_logloss: 0.76265 |  0:20:30s
epoch 75 | loss: 0.68577 | eval_custom_logloss: 0.70425 |  0:20:46s
epoch 76 | loss: 0.69706 | eval_custom_logloss: 0.77562 |  0:21:03s
epoch 77 | loss: 0.66338 | eval_custom_logloss: 0.70671 |  0:21:19s
epoch 78 | loss: 0.67282 | eval_custom_logloss: 0.6614  |  0:21:36s
epoch 79 | loss: 0.70785 | eval_custom_logloss: 0.72696 |  0:21:52s
epoch 80 | loss: 0.76508 | eval_custom_logloss: 0.78394 |  0:22:09s
epoch 81 | loss: 0.73158 | eval_custom_logloss: 0.75212 |  0:22:25s
epoch 82 | loss: 0.70444 | eval_custom_logloss: 0.72126 |  0:22:42s
epoch 83 | loss: 0.69177 | eval_custom_logloss: 0.70176 |  0:22:58s
epoch 84 | loss: 0.67835 | eval_custom_logloss: 0.73414 |  0:23:14s
epoch 85 | loss: 0.68231 | eval_custom_logloss: 0.6867  |  0:23:31s
epoch 86 | loss: 0.71979 | eval_custom_logloss: 0.67553 |  0:23:47s
epoch 87 | loss: 0.68818 | eval_custom_logloss: 0.65989 |  0:24:04s
epoch 88 | loss: 0.68195 | eval_custom_logloss: 0.66718 |  0:24:20s
epoch 89 | loss: 0.67514 | eval_custom_logloss: 0.6858  |  0:24:37s
epoch 90 | loss: 0.67072 | eval_custom_logloss: 0.64029 |  0:24:53s
epoch 91 | loss: 0.67591 | eval_custom_logloss: 0.66608 |  0:25:10s
epoch 92 | loss: 0.69248 | eval_custom_logloss: 0.63099 |  0:25:26s
epoch 93 | loss: 0.67653 | eval_custom_logloss: 0.67733 |  0:25:43s
epoch 94 | loss: 0.69539 | eval_custom_logloss: 0.65024 |  0:25:59s
epoch 95 | loss: 0.69091 | eval_custom_logloss: 0.63866 |  0:26:15s
epoch 96 | loss: 0.67372 | eval_custom_logloss: 0.65531 |  0:26:32s
epoch 97 | loss: 0.7506  | eval_custom_logloss: 0.67025 |  0:26:48s
epoch 98 | loss: 0.70101 | eval_custom_logloss: 0.73756 |  0:27:05s
epoch 99 | loss: 0.71596 | eval_custom_logloss: 0.89735 |  0:27:21s
Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_eval_custom_logloss = 0.63099
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6292, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.50393 | eval_custom_logloss: 1.15925 |  0:00:16s
epoch 1  | loss: 1.12997 | eval_custom_logloss: 1.08363 |  0:00:32s
epoch 2  | loss: 1.05089 | eval_custom_logloss: 0.94218 |  0:00:49s
epoch 3  | loss: 1.0132  | eval_custom_logloss: 0.92761 |  0:01:05s
epoch 4  | loss: 0.97915 | eval_custom_logloss: 0.83931 |  0:01:22s
epoch 5  | loss: 0.94342 | eval_custom_logloss: 0.79973 |  0:01:38s
epoch 6  | loss: 0.92994 | eval_custom_logloss: 0.86668 |  0:01:55s
epoch 7  | loss: 0.89718 | eval_custom_logloss: 0.78128 |  0:02:11s
epoch 8  | loss: 0.86379 | eval_custom_logloss: 0.91399 |  0:02:27s
epoch 9  | loss: 0.84226 | eval_custom_logloss: 0.89553 |  0:02:44s
epoch 10 | loss: 0.8102  | eval_custom_logloss: 0.98557 |  0:03:00s
epoch 11 | loss: 0.80609 | eval_custom_logloss: 0.76292 |  0:03:16s
epoch 12 | loss: 0.79563 | eval_custom_logloss: 0.69721 |  0:03:33s
epoch 13 | loss: 0.78009 | eval_custom_logloss: 0.82672 |  0:03:49s
epoch 14 | loss: 0.78219 | eval_custom_logloss: 0.69373 |  0:04:06s
epoch 15 | loss: 0.78977 | eval_custom_logloss: 0.9037  |  0:04:22s
epoch 16 | loss: 0.76246 | eval_custom_logloss: 0.67871 |  0:04:39s
epoch 17 | loss: 0.73392 | eval_custom_logloss: 0.65659 |  0:04:55s
epoch 18 | loss: 0.72511 | eval_custom_logloss: 0.81054 |  0:05:12s
epoch 19 | loss: 0.73953 | eval_custom_logloss: 0.73105 |  0:05:28s
epoch 20 | loss: 0.72078 | eval_custom_logloss: 0.7496  |  0:05:44s
epoch 21 | loss: 0.71872 | eval_custom_logloss: 0.79946 |  0:06:01s
epoch 22 | loss: 0.72572 | eval_custom_logloss: 0.70827 |  0:06:17s
epoch 23 | loss: 0.72383 | eval_custom_logloss: 0.81676 |  0:06:33s
epoch 24 | loss: 0.74128 | eval_custom_logloss: 0.84301 |  0:06:50s
epoch 25 | loss: 0.70662 | eval_custom_logloss: 0.82322 |  0:07:06s
epoch 26 | loss: 0.71549 | eval_custom_logloss: 0.81755 |  0:07:23s
epoch 27 | loss: 0.69259 | eval_custom_logloss: 0.92342 |  0:07:39s
epoch 28 | loss: 0.7077  | eval_custom_logloss: 0.99902 |  0:07:55s
epoch 29 | loss: 0.6868  | eval_custom_logloss: 0.94922 |  0:08:12s
epoch 30 | loss: 0.70505 | eval_custom_logloss: 0.84153 |  0:08:28s
epoch 31 | loss: 0.71326 | eval_custom_logloss: 0.68793 |  0:08:45s
epoch 32 | loss: 0.69299 | eval_custom_logloss: 0.91359 |  0:09:01s
epoch 33 | loss: 0.70665 | eval_custom_logloss: 0.80322 |  0:09:18s
epoch 34 | loss: 0.68324 | eval_custom_logloss: 0.79609 |  0:09:34s
epoch 35 | loss: 0.70621 | eval_custom_logloss: 0.69403 |  0:09:51s
epoch 36 | loss: 0.68185 | eval_custom_logloss: 0.62709 |  0:10:07s
epoch 37 | loss: 0.68645 | eval_custom_logloss: 0.90964 |  0:10:23s
epoch 38 | loss: 0.68376 | eval_custom_logloss: 0.75686 |  0:10:40s
epoch 39 | loss: 0.67127 | eval_custom_logloss: 0.7296  |  0:10:56s
epoch 40 | loss: 0.67436 | eval_custom_logloss: 0.69454 |  0:11:13s
epoch 41 | loss: 0.67612 | eval_custom_logloss: 0.67702 |  0:11:29s
epoch 42 | loss: 0.67325 | eval_custom_logloss: 0.63139 |  0:11:45s
epoch 43 | loss: 0.67066 | eval_custom_logloss: 0.66102 |  0:12:02s
epoch 44 | loss: 0.66465 | eval_custom_logloss: 0.68344 |  0:12:18s
epoch 45 | loss: 0.6662  | eval_custom_logloss: 0.72689 |  0:12:34s
epoch 46 | loss: 0.66234 | eval_custom_logloss: 0.66792 |  0:12:50s
epoch 47 | loss: 0.6728  | eval_custom_logloss: 0.6372  |  0:13:07s
epoch 48 | loss: 0.65627 | eval_custom_logloss: 0.64162 |  0:13:23s
epoch 49 | loss: 0.65373 | eval_custom_logloss: 0.64699 |  0:13:40s
epoch 50 | loss: 0.66006 | eval_custom_logloss: 0.85706 |  0:13:57s
epoch 51 | loss: 0.6656  | eval_custom_logloss: 0.63603 |  0:14:14s
epoch 52 | loss: 0.66204 | eval_custom_logloss: 0.9617  |  0:14:30s
epoch 53 | loss: 0.66349 | eval_custom_logloss: 0.70286 |  0:14:47s
epoch 54 | loss: 0.64978 | eval_custom_logloss: 0.8035  |  0:15:04s
epoch 55 | loss: 0.66022 | eval_custom_logloss: 0.63183 |  0:15:22s
epoch 56 | loss: 0.64827 | eval_custom_logloss: 0.81163 |  0:15:39s

Early stopping occurred at epoch 56 with best_epoch = 36 and best_eval_custom_logloss = 0.62709
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62735, 'Log Loss - std': 0.0018500000000000183} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.55421 | eval_custom_logloss: 1.10294 |  0:00:17s
epoch 1  | loss: 1.14746 | eval_custom_logloss: 1.03676 |  0:00:35s
epoch 2  | loss: 1.05572 | eval_custom_logloss: 0.91095 |  0:00:52s
epoch 3  | loss: 0.98821 | eval_custom_logloss: 0.83202 |  0:01:10s
epoch 4  | loss: 0.98279 | eval_custom_logloss: 0.92403 |  0:01:28s
epoch 5  | loss: 0.95431 | eval_custom_logloss: 0.84319 |  0:01:44s
epoch 6  | loss: 0.92729 | eval_custom_logloss: 0.81341 |  0:02:01s
epoch 7  | loss: 0.91879 | eval_custom_logloss: 0.7947  |  0:02:17s
epoch 8  | loss: 0.89764 | eval_custom_logloss: 0.83438 |  0:02:33s
epoch 9  | loss: 0.90723 | eval_custom_logloss: 0.80473 |  0:02:50s
epoch 10 | loss: 0.91211 | eval_custom_logloss: 0.78396 |  0:03:06s
epoch 11 | loss: 0.87308 | eval_custom_logloss: 0.80569 |  0:03:23s
epoch 12 | loss: 0.87545 | eval_custom_logloss: 0.74902 |  0:03:39s
epoch 13 | loss: 0.85197 | eval_custom_logloss: 0.80784 |  0:03:55s
epoch 14 | loss: 0.83707 | eval_custom_logloss: 0.74832 |  0:04:12s
epoch 15 | loss: 0.88438 | eval_custom_logloss: 0.77426 |  0:04:28s
epoch 16 | loss: 0.85536 | eval_custom_logloss: 0.7819  |  0:04:45s
epoch 17 | loss: 0.83578 | eval_custom_logloss: 0.76911 |  0:05:01s
epoch 18 | loss: 0.82253 | eval_custom_logloss: 0.76377 |  0:05:18s
epoch 19 | loss: 0.80502 | eval_custom_logloss: 0.75731 |  0:05:34s
epoch 20 | loss: 0.81277 | eval_custom_logloss: 0.75997 |  0:05:51s
epoch 21 | loss: 0.79979 | eval_custom_logloss: 0.74318 |  0:06:07s
epoch 22 | loss: 0.80052 | eval_custom_logloss: 0.72235 |  0:06:24s
epoch 23 | loss: 0.79389 | eval_custom_logloss: 0.71129 |  0:06:40s
epoch 24 | loss: 0.78802 | eval_custom_logloss: 0.75387 |  0:06:57s
epoch 25 | loss: 0.77935 | eval_custom_logloss: 0.77839 |  0:07:13s
epoch 26 | loss: 0.78121 | eval_custom_logloss: 0.72198 |  0:07:30s
epoch 27 | loss: 0.77226 | eval_custom_logloss: 1.02138 |  0:07:46s
epoch 28 | loss: 0.77174 | eval_custom_logloss: 0.75692 |  0:08:02s
epoch 29 | loss: 0.77581 | eval_custom_logloss: 0.84615 |  0:08:19s
epoch 30 | loss: 0.77175 | eval_custom_logloss: 1.08598 |  0:08:35s
epoch 31 | loss: 0.77351 | eval_custom_logloss: 0.81665 |  0:08:51s
epoch 32 | loss: 0.75829 | eval_custom_logloss: 0.83092 |  0:09:08s
epoch 33 | loss: 0.76457 | eval_custom_logloss: 0.71143 |  0:09:24s
epoch 34 | loss: 0.7545  | eval_custom_logloss: 0.89712 |  0:09:41s
epoch 35 | loss: 0.75018 | eval_custom_logloss: 0.81975 |  0:09:57s
epoch 36 | loss: 0.74185 | eval_custom_logloss: 0.78467 |  0:10:13s
epoch 37 | loss: 0.73384 | eval_custom_logloss: 0.7744  |  0:10:30s
epoch 38 | loss: 0.73196 | eval_custom_logloss: 0.73554 |  0:10:46s
epoch 39 | loss: 0.74427 | eval_custom_logloss: 1.01863 |  0:11:03s
epoch 40 | loss: 0.7425  | eval_custom_logloss: 0.76499 |  0:11:19s
epoch 41 | loss: 0.73319 | eval_custom_logloss: 0.74376 |  0:11:36s
epoch 42 | loss: 0.73369 | eval_custom_logloss: 0.72884 |  0:11:52s
epoch 43 | loss: 0.73034 | eval_custom_logloss: 0.87867 |  0:12:09s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.71129
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6549666666666667, 'Log Loss - std': 0.03908506392756989} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.57186 | eval_custom_logloss: 1.22911 |  0:00:16s
epoch 1  | loss: 1.15772 | eval_custom_logloss: 1.01235 |  0:00:33s
epoch 2  | loss: 1.07898 | eval_custom_logloss: 0.90892 |  0:00:49s
epoch 3  | loss: 1.01792 | eval_custom_logloss: 1.05849 |  0:01:06s
epoch 4  | loss: 1.0277  | eval_custom_logloss: 1.11407 |  0:01:22s
epoch 5  | loss: 0.95076 | eval_custom_logloss: 0.814   |  0:01:38s
epoch 6  | loss: 0.90166 | eval_custom_logloss: 2.02941 |  0:01:55s
epoch 7  | loss: 0.87825 | eval_custom_logloss: 1.39843 |  0:02:11s
epoch 8  | loss: 0.87653 | eval_custom_logloss: 1.34791 |  0:02:28s
epoch 9  | loss: 0.85907 | eval_custom_logloss: 0.81038 |  0:02:44s
epoch 10 | loss: 0.82383 | eval_custom_logloss: 0.78405 |  0:03:00s
epoch 11 | loss: 0.81048 | eval_custom_logloss: 1.00346 |  0:03:17s
epoch 12 | loss: 0.79388 | eval_custom_logloss: 0.79798 |  0:03:33s
epoch 13 | loss: 0.78346 | eval_custom_logloss: 0.77143 |  0:03:50s
epoch 14 | loss: 0.79455 | eval_custom_logloss: 1.00745 |  0:04:07s
epoch 15 | loss: 0.79318 | eval_custom_logloss: 0.67627 |  0:04:23s
epoch 16 | loss: 0.77169 | eval_custom_logloss: 1.2796  |  0:04:40s
epoch 17 | loss: 0.73198 | eval_custom_logloss: 1.2266  |  0:04:56s
epoch 18 | loss: 0.7422  | eval_custom_logloss: 1.07621 |  0:05:12s
epoch 19 | loss: 0.74019 | eval_custom_logloss: 0.87282 |  0:05:29s
epoch 20 | loss: 0.71655 | eval_custom_logloss: 1.01865 |  0:05:46s
epoch 21 | loss: 0.70232 | eval_custom_logloss: 0.78601 |  0:06:03s
epoch 22 | loss: 0.69779 | eval_custom_logloss: 1.02695 |  0:06:20s
epoch 23 | loss: 0.71194 | eval_custom_logloss: 1.10028 |  0:06:36s
epoch 24 | loss: 0.71675 | eval_custom_logloss: 1.16009 |  0:06:53s
epoch 25 | loss: 0.70067 | eval_custom_logloss: 0.80477 |  0:07:09s
epoch 26 | loss: 0.70946 | eval_custom_logloss: 0.67738 |  0:07:25s
epoch 27 | loss: 0.69599 | eval_custom_logloss: 1.00289 |  0:07:42s
epoch 28 | loss: 0.68467 | eval_custom_logloss: 1.1339  |  0:07:58s
epoch 29 | loss: 0.68509 | eval_custom_logloss: 0.80835 |  0:08:15s
epoch 30 | loss: 0.6783  | eval_custom_logloss: 1.29876 |  0:08:31s
epoch 31 | loss: 0.68214 | eval_custom_logloss: 0.88356 |  0:08:47s
epoch 32 | loss: 0.67454 | eval_custom_logloss: 0.73175 |  0:09:04s
epoch 33 | loss: 0.66537 | eval_custom_logloss: 0.77565 |  0:09:20s
epoch 34 | loss: 0.65261 | eval_custom_logloss: 1.09895 |  0:09:37s
epoch 35 | loss: 0.66252 | eval_custom_logloss: 0.67104 |  0:09:53s
epoch 36 | loss: 0.66338 | eval_custom_logloss: 0.74829 |  0:10:09s
epoch 37 | loss: 0.65455 | eval_custom_logloss: 0.79541 |  0:10:26s
epoch 38 | loss: 0.65232 | eval_custom_logloss: 1.00679 |  0:10:43s
epoch 39 | loss: 0.66698 | eval_custom_logloss: 0.78322 |  0:10:59s
epoch 40 | loss: 0.66274 | eval_custom_logloss: 0.88124 |  0:11:15s
epoch 41 | loss: 0.64419 | eval_custom_logloss: 1.29319 |  0:11:32s
epoch 42 | loss: 0.65702 | eval_custom_logloss: 0.91481 |  0:11:48s
epoch 43 | loss: 0.65356 | eval_custom_logloss: 0.82731 |  0:12:05s
epoch 44 | loss: 0.64115 | eval_custom_logloss: 1.01809 |  0:12:21s
epoch 45 | loss: 0.638   | eval_custom_logloss: 0.73867 |  0:12:37s
epoch 46 | loss: 0.65393 | eval_custom_logloss: 1.7901  |  0:12:53s
epoch 47 | loss: 0.62936 | eval_custom_logloss: 0.93876 |  0:13:10s
epoch 48 | loss: 0.63488 | eval_custom_logloss: 0.9942  |  0:13:26s
epoch 49 | loss: 0.63457 | eval_custom_logloss: 1.02161 |  0:13:42s
epoch 50 | loss: 0.70797 | eval_custom_logloss: 0.60243 |  0:13:58s
epoch 51 | loss: 0.67545 | eval_custom_logloss: 0.9305  |  0:14:14s
epoch 52 | loss: 0.63509 | eval_custom_logloss: 0.89119 |  0:14:31s
epoch 53 | loss: 0.62922 | eval_custom_logloss: 0.61966 |  0:14:47s
epoch 54 | loss: 0.63745 | eval_custom_logloss: 1.37621 |  0:15:03s
epoch 55 | loss: 0.63384 | eval_custom_logloss: 0.93782 |  0:15:20s
epoch 56 | loss: 0.63341 | eval_custom_logloss: 0.95703 |  0:15:36s
epoch 57 | loss: 0.62692 | eval_custom_logloss: 0.89727 |  0:15:53s
epoch 58 | loss: 0.62519 | eval_custom_logloss: 1.22459 |  0:16:09s
epoch 59 | loss: 0.62146 | eval_custom_logloss: 1.09333 |  0:16:26s
epoch 60 | loss: 0.61094 | eval_custom_logloss: 1.00201 |  0:16:42s
epoch 61 | loss: 0.61819 | eval_custom_logloss: 0.89606 |  0:16:59s
epoch 62 | loss: 0.62052 | eval_custom_logloss: 1.06847 |  0:17:15s
epoch 63 | loss: 0.60838 | eval_custom_logloss: 0.90686 |  0:17:32s
epoch 64 | loss: 0.60617 | eval_custom_logloss: 0.89306 |  0:17:48s
epoch 65 | loss: 0.61062 | eval_custom_logloss: 0.90598 |  0:18:05s
epoch 66 | loss: 0.61029 | eval_custom_logloss: 0.87043 |  0:18:21s
epoch 67 | loss: 0.60801 | eval_custom_logloss: 0.85012 |  0:18:37s
epoch 68 | loss: 0.6139  | eval_custom_logloss: 1.0864  |  0:18:54s
epoch 69 | loss: 0.59952 | eval_custom_logloss: 0.57957 |  0:19:10s
epoch 70 | loss: 0.613   | eval_custom_logloss: 0.93059 |  0:19:27s
epoch 71 | loss: 0.60537 | eval_custom_logloss: 0.69691 |  0:19:43s
epoch 72 | loss: 0.61562 | eval_custom_logloss: 1.28855 |  0:20:00s
epoch 73 | loss: 0.59117 | eval_custom_logloss: 0.6905  |  0:20:16s
epoch 74 | loss: 0.63272 | eval_custom_logloss: 0.61973 |  0:20:33s
epoch 75 | loss: 0.61774 | eval_custom_logloss: 1.08259 |  0:20:49s
epoch 76 | loss: 0.61534 | eval_custom_logloss: 0.94816 |  0:21:06s
epoch 77 | loss: 0.59877 | eval_custom_logloss: 0.86025 |  0:21:22s
epoch 78 | loss: 0.58894 | eval_custom_logloss: 1.41616 |  0:21:39s
epoch 79 | loss: 0.59486 | eval_custom_logloss: 0.81749 |  0:21:55s
epoch 80 | loss: 0.59461 | eval_custom_logloss: 1.04857 |  0:22:11s
epoch 81 | loss: 0.58964 | eval_custom_logloss: 0.86812 |  0:22:28s
epoch 82 | loss: 0.59838 | eval_custom_logloss: 0.95417 |  0:22:44s
epoch 83 | loss: 0.59859 | eval_custom_logloss: 1.10197 |  0:23:01s
epoch 84 | loss: 0.57946 | eval_custom_logloss: 1.17426 |  0:23:17s
epoch 85 | loss: 0.5954  | eval_custom_logloss: 0.85233 |  0:23:33s
epoch 86 | loss: 0.58673 | eval_custom_logloss: 0.92858 |  0:23:50s
epoch 87 | loss: 0.59008 | eval_custom_logloss: 1.01256 |  0:24:06s
epoch 88 | loss: 0.58215 | eval_custom_logloss: 0.75796 |  0:24:23s
epoch 89 | loss: 0.58023 | eval_custom_logloss: 1.05448 |  0:24:39s

Early stopping occurred at epoch 89 with best_epoch = 69 and best_eval_custom_logloss = 0.57957
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.635225, 'Log Loss - std': 0.04811373894221903} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.5014  | eval_custom_logloss: 1.1529  |  0:00:16s
epoch 1  | loss: 1.1171  | eval_custom_logloss: 1.0529  |  0:00:32s
epoch 2  | loss: 1.04419 | eval_custom_logloss: 0.93697 |  0:00:49s
epoch 3  | loss: 1.02822 | eval_custom_logloss: 0.92862 |  0:01:05s
epoch 4  | loss: 0.9947  | eval_custom_logloss: 0.96632 |  0:01:22s
epoch 5  | loss: 0.9799  | eval_custom_logloss: 0.88364 |  0:01:38s
epoch 6  | loss: 0.94895 | eval_custom_logloss: 0.88697 |  0:01:55s
epoch 7  | loss: 0.96637 | eval_custom_logloss: 0.85392 |  0:02:11s
epoch 8  | loss: 0.93825 | eval_custom_logloss: 0.79712 |  0:02:28s
epoch 9  | loss: 0.92882 | eval_custom_logloss: 0.86737 |  0:02:44s
epoch 10 | loss: 0.89906 | eval_custom_logloss: 0.79853 |  0:03:01s
epoch 11 | loss: 0.89355 | eval_custom_logloss: 0.83623 |  0:03:17s
epoch 12 | loss: 0.88606 | eval_custom_logloss: 0.80168 |  0:03:33s
epoch 13 | loss: 0.88208 | eval_custom_logloss: 0.80501 |  0:03:50s
epoch 14 | loss: 0.85833 | eval_custom_logloss: 0.76504 |  0:04:06s
epoch 15 | loss: 0.87528 | eval_custom_logloss: 0.82287 |  0:04:23s
epoch 16 | loss: 0.87058 | eval_custom_logloss: 0.80396 |  0:04:39s
epoch 17 | loss: 0.8364  | eval_custom_logloss: 0.91697 |  0:04:55s
epoch 18 | loss: 0.84378 | eval_custom_logloss: 0.95875 |  0:05:12s
epoch 19 | loss: 0.83647 | eval_custom_logloss: 0.7989  |  0:05:29s
epoch 20 | loss: 0.8163  | eval_custom_logloss: 0.73485 |  0:05:45s
epoch 21 | loss: 0.80474 | eval_custom_logloss: 0.97375 |  0:06:02s
epoch 22 | loss: 0.80738 | eval_custom_logloss: 0.81616 |  0:06:18s
epoch 23 | loss: 0.79785 | eval_custom_logloss: 0.77824 |  0:06:35s
epoch 24 | loss: 0.80351 | eval_custom_logloss: 0.7716  |  0:06:52s
epoch 25 | loss: 0.81194 | eval_custom_logloss: 0.73007 |  0:07:10s
epoch 26 | loss: 0.83887 | eval_custom_logloss: 0.7472  |  0:07:27s
epoch 27 | loss: 0.83705 | eval_custom_logloss: 0.94762 |  0:07:44s
epoch 28 | loss: 0.79813 | eval_custom_logloss: 0.76452 |  0:08:00s
epoch 29 | loss: 0.77284 | eval_custom_logloss: 0.82899 |  0:08:17s
epoch 30 | loss: 0.79066 | eval_custom_logloss: 0.9219  |  0:08:33s
epoch 31 | loss: 0.79169 | eval_custom_logloss: 0.81511 |  0:08:50s
epoch 32 | loss: 0.74837 | eval_custom_logloss: 0.74287 |  0:09:06s
epoch 33 | loss: 0.75052 | eval_custom_logloss: 0.71071 |  0:09:23s
epoch 34 | loss: 0.74226 | eval_custom_logloss: 0.89088 |  0:09:39s
epoch 35 | loss: 0.74266 | eval_custom_logloss: 0.73335 |  0:09:56s
epoch 36 | loss: 0.73664 | eval_custom_logloss: 0.73201 |  0:10:13s
epoch 37 | loss: 0.71784 | eval_custom_logloss: 0.78848 |  0:10:29s
epoch 38 | loss: 0.73073 | eval_custom_logloss: 0.8797  |  0:10:46s
epoch 39 | loss: 0.73336 | eval_custom_logloss: 0.84657 |  0:11:02s
epoch 40 | loss: 0.73737 | eval_custom_logloss: 0.84058 |  0:11:19s
epoch 41 | loss: 0.70612 | eval_custom_logloss: 0.77204 |  0:11:35s
epoch 42 | loss: 0.72026 | eval_custom_logloss: 0.8544  |  0:11:52s
epoch 43 | loss: 0.71227 | eval_custom_logloss: 0.68861 |  0:12:08s
epoch 44 | loss: 0.70326 | eval_custom_logloss: 0.74224 |  0:12:25s
epoch 45 | loss: 0.70818 | eval_custom_logloss: 0.81715 |  0:12:41s
epoch 46 | loss: 0.71287 | eval_custom_logloss: 0.65643 |  0:12:58s
epoch 47 | loss: 0.7126  | eval_custom_logloss: 0.79073 |  0:13:14s
epoch 48 | loss: 0.72318 | eval_custom_logloss: 0.82631 |  0:13:31s
epoch 49 | loss: 0.69849 | eval_custom_logloss: 1.01642 |  0:13:47s
epoch 50 | loss: 0.68338 | eval_custom_logloss: 0.8647  |  0:14:04s
epoch 51 | loss: 0.6901  | eval_custom_logloss: 0.68114 |  0:14:20s
epoch 52 | loss: 0.69724 | eval_custom_logloss: 0.86574 |  0:14:37s
epoch 53 | loss: 0.67892 | eval_custom_logloss: 0.62284 |  0:14:53s
epoch 54 | loss: 0.6879  | eval_custom_logloss: 0.76896 |  0:15:09s
epoch 55 | loss: 0.69211 | eval_custom_logloss: 0.78591 |  0:15:26s
epoch 56 | loss: 0.67846 | eval_custom_logloss: 0.79684 |  0:15:42s
epoch 57 | loss: 0.68518 | eval_custom_logloss: 0.90711 |  0:15:59s
epoch 58 | loss: 0.66769 | eval_custom_logloss: 0.87326 |  0:16:15s
epoch 59 | loss: 0.69131 | eval_custom_logloss: 0.68307 |  0:16:32s
epoch 60 | loss: 0.658   | eval_custom_logloss: 0.81871 |  0:16:48s
epoch 61 | loss: 0.66682 | eval_custom_logloss: 1.00826 |  0:17:05s
epoch 62 | loss: 0.66492 | eval_custom_logloss: 0.96829 |  0:17:21s
epoch 63 | loss: 0.65695 | eval_custom_logloss: 0.81165 |  0:17:37s
epoch 64 | loss: 0.66413 | eval_custom_logloss: 0.66759 |  0:17:54s
epoch 65 | loss: 0.66394 | eval_custom_logloss: 0.77386 |  0:18:10s
epoch 66 | loss: 0.6626  | eval_custom_logloss: 0.84118 |  0:18:26s
epoch 67 | loss: 0.66208 | eval_custom_logloss: 0.79523 |  0:18:43s
epoch 68 | loss: 0.65741 | eval_custom_logloss: 0.82875 |  0:18:59s
epoch 69 | loss: 0.67879 | eval_custom_logloss: 0.63918 |  0:19:16s
epoch 70 | loss: 0.65706 | eval_custom_logloss: 0.63615 |  0:19:32s
epoch 71 | loss: 0.64094 | eval_custom_logloss: 0.76421 |  0:19:49s
epoch 72 | loss: 0.65707 | eval_custom_logloss: 0.64625 |  0:20:06s
epoch 73 | loss: 0.6528  | eval_custom_logloss: 0.782   |  0:20:23s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.62284
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6324400000000001, 'Log Loss - std': 0.04339320684162445} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 0.6324400000000001 and parameters: {'n_d': 59, 'n_steps': 7, 'gamma': 1.039894832455389, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.07975638320679328, 'mask_type': 'sparsemax'}. Best is trial 5 with value: 0.6324400000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.55555 | eval_custom_logloss: 1.20357 |  0:00:18s
epoch 1  | loss: 1.12508 | eval_custom_logloss: 0.95715 |  0:00:36s
epoch 2  | loss: 1.01099 | eval_custom_logloss: 0.916   |  0:00:54s
epoch 3  | loss: 0.96179 | eval_custom_logloss: 0.85903 |  0:01:13s
epoch 4  | loss: 0.94057 | eval_custom_logloss: 0.82109 |  0:01:31s
epoch 5  | loss: 0.92924 | eval_custom_logloss: 0.82554 |  0:01:49s
epoch 6  | loss: 0.934   | eval_custom_logloss: 0.80565 |  0:02:07s
epoch 7  | loss: 0.9131  | eval_custom_logloss: 0.8604  |  0:02:25s
epoch 8  | loss: 0.89246 | eval_custom_logloss: 0.81589 |  0:02:44s
epoch 9  | loss: 0.87777 | eval_custom_logloss: 0.74747 |  0:03:02s
epoch 10 | loss: 0.8432  | eval_custom_logloss: 0.93189 |  0:03:20s
epoch 11 | loss: 0.85234 | eval_custom_logloss: 1.38836 |  0:03:38s
epoch 12 | loss: 0.80506 | eval_custom_logloss: 0.78793 |  0:03:56s
epoch 13 | loss: 0.79811 | eval_custom_logloss: 0.93369 |  0:04:14s
epoch 14 | loss: 0.78215 | eval_custom_logloss: 0.76463 |  0:04:32s
epoch 15 | loss: 0.77489 | eval_custom_logloss: 0.72551 |  0:04:50s
epoch 16 | loss: 0.76166 | eval_custom_logloss: 0.79015 |  0:05:08s
epoch 17 | loss: 0.76046 | eval_custom_logloss: 0.69642 |  0:05:26s
epoch 18 | loss: 0.75274 | eval_custom_logloss: 0.72067 |  0:05:44s
epoch 19 | loss: 0.75294 | eval_custom_logloss: 0.83579 |  0:06:02s
epoch 20 | loss: 0.75    | eval_custom_logloss: 1.20986 |  0:06:20s
epoch 21 | loss: 0.737   | eval_custom_logloss: 0.78269 |  0:06:38s
epoch 22 | loss: 0.72561 | eval_custom_logloss: 0.7195  |  0:06:56s
epoch 23 | loss: 0.72384 | eval_custom_logloss: 0.70972 |  0:07:14s
epoch 24 | loss: 0.72867 | eval_custom_logloss: 0.73786 |  0:07:32s
epoch 25 | loss: 0.72376 | eval_custom_logloss: 0.70019 |  0:07:50s
epoch 26 | loss: 0.71971 | eval_custom_logloss: 0.6566  |  0:08:09s
epoch 27 | loss: 0.71956 | eval_custom_logloss: 0.90715 |  0:08:27s
epoch 28 | loss: 0.70755 | eval_custom_logloss: 0.72064 |  0:08:45s
epoch 29 | loss: 0.70996 | eval_custom_logloss: 0.64701 |  0:09:03s
epoch 30 | loss: 0.70219 | eval_custom_logloss: 1.05149 |  0:09:21s
epoch 31 | loss: 0.68871 | eval_custom_logloss: 0.91018 |  0:09:39s
epoch 32 | loss: 0.6953  | eval_custom_logloss: 0.79076 |  0:09:57s
epoch 33 | loss: 0.69035 | eval_custom_logloss: 0.90099 |  0:10:15s
epoch 34 | loss: 0.70322 | eval_custom_logloss: 0.71072 |  0:10:34s
epoch 35 | loss: 0.68204 | eval_custom_logloss: 0.72463 |  0:10:52s
epoch 36 | loss: 0.67157 | eval_custom_logloss: 0.81988 |  0:11:11s
epoch 37 | loss: 0.68699 | eval_custom_logloss: 0.86366 |  0:11:29s
epoch 38 | loss: 0.68063 | eval_custom_logloss: 0.82482 |  0:11:47s
epoch 39 | loss: 0.68434 | eval_custom_logloss: 0.66275 |  0:12:05s
epoch 40 | loss: 0.68352 | eval_custom_logloss: 0.77912 |  0:12:23s
epoch 41 | loss: 0.67324 | eval_custom_logloss: 0.83566 |  0:12:41s
epoch 42 | loss: 0.67147 | eval_custom_logloss: 0.64811 |  0:12:59s
epoch 43 | loss: 0.66889 | eval_custom_logloss: 0.76393 |  0:13:17s
epoch 44 | loss: 0.66887 | eval_custom_logloss: 0.80878 |  0:13:35s
epoch 45 | loss: 0.66657 | eval_custom_logloss: 0.81626 |  0:13:53s
epoch 46 | loss: 0.66387 | eval_custom_logloss: 0.84218 |  0:14:11s
epoch 47 | loss: 0.6623  | eval_custom_logloss: 0.77457 |  0:14:29s
epoch 48 | loss: 0.659   | eval_custom_logloss: 0.64828 |  0:14:47s
epoch 49 | loss: 0.66536 | eval_custom_logloss: 0.67343 |  0:15:05s

Early stopping occurred at epoch 49 with best_epoch = 29 and best_eval_custom_logloss = 0.64701
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6453, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.61596 | eval_custom_logloss: 1.27404 |  0:00:18s
epoch 1  | loss: 1.10793 | eval_custom_logloss: 0.93721 |  0:00:36s
epoch 2  | loss: 1.0289  | eval_custom_logloss: 0.88114 |  0:00:54s
epoch 3  | loss: 0.97391 | eval_custom_logloss: 0.85056 |  0:01:12s
epoch 4  | loss: 0.9327  | eval_custom_logloss: 0.83594 |  0:01:31s
epoch 5  | loss: 0.92145 | eval_custom_logloss: 0.80935 |  0:01:49s
epoch 6  | loss: 0.91751 | eval_custom_logloss: 0.89082 |  0:02:07s
epoch 7  | loss: 0.88906 | eval_custom_logloss: 0.79343 |  0:02:25s
epoch 8  | loss: 0.87297 | eval_custom_logloss: 2.22762 |  0:02:44s
epoch 9  | loss: 0.84295 | eval_custom_logloss: 1.3161  |  0:03:02s
epoch 10 | loss: 0.84    | eval_custom_logloss: 0.77309 |  0:03:20s
epoch 11 | loss: 0.83579 | eval_custom_logloss: 0.80883 |  0:03:38s
epoch 12 | loss: 0.82104 | eval_custom_logloss: 0.77099 |  0:03:57s
epoch 13 | loss: 0.80002 | eval_custom_logloss: 0.74076 |  0:04:15s
epoch 14 | loss: 0.80167 | eval_custom_logloss: 0.75519 |  0:04:33s
epoch 15 | loss: 0.78974 | eval_custom_logloss: 0.91357 |  0:04:51s
epoch 16 | loss: 0.77775 | eval_custom_logloss: 0.7867  |  0:05:09s
epoch 17 | loss: 0.80034 | eval_custom_logloss: 0.71885 |  0:05:27s
epoch 18 | loss: 0.79331 | eval_custom_logloss: 0.76235 |  0:05:45s
epoch 19 | loss: 0.76847 | eval_custom_logloss: 0.76758 |  0:06:03s
epoch 20 | loss: 0.78445 | eval_custom_logloss: 0.7564  |  0:06:21s
epoch 21 | loss: 0.76806 | eval_custom_logloss: 0.84666 |  0:06:39s
epoch 22 | loss: 0.75513 | eval_custom_logloss: 0.82919 |  0:06:58s
epoch 23 | loss: 0.75316 | eval_custom_logloss: 0.67171 |  0:07:16s
epoch 24 | loss: 0.7294  | eval_custom_logloss: 0.81966 |  0:07:34s
epoch 25 | loss: 0.7468  | eval_custom_logloss: 0.78269 |  0:07:52s
epoch 26 | loss: 0.73323 | eval_custom_logloss: 1.10414 |  0:08:10s
epoch 27 | loss: 0.73739 | eval_custom_logloss: 0.88673 |  0:08:28s
epoch 28 | loss: 0.75155 | eval_custom_logloss: 0.69455 |  0:08:46s
epoch 29 | loss: 0.74291 | eval_custom_logloss: 0.99218 |  0:09:05s
epoch 30 | loss: 0.70283 | eval_custom_logloss: 0.74702 |  0:09:23s
epoch 31 | loss: 0.71285 | eval_custom_logloss: 0.80772 |  0:09:41s
epoch 32 | loss: 0.70132 | eval_custom_logloss: 0.77765 |  0:09:59s
epoch 33 | loss: 0.69272 | eval_custom_logloss: 0.64688 |  0:10:17s
epoch 34 | loss: 0.71477 | eval_custom_logloss: 0.8502  |  0:10:36s
epoch 35 | loss: 0.74598 | eval_custom_logloss: 0.73793 |  0:10:54s
epoch 36 | loss: 0.73565 | eval_custom_logloss: 0.84633 |  0:11:12s
epoch 37 | loss: 0.71505 | eval_custom_logloss: 0.79339 |  0:11:31s
epoch 38 | loss: 0.70197 | eval_custom_logloss: 0.67957 |  0:11:49s
epoch 39 | loss: 0.71533 | eval_custom_logloss: 0.69548 |  0:12:07s
epoch 40 | loss: 0.69746 | eval_custom_logloss: 0.67872 |  0:12:25s
epoch 41 | loss: 0.68882 | eval_custom_logloss: 0.78902 |  0:12:43s
epoch 42 | loss: 0.69441 | eval_custom_logloss: 0.90652 |  0:13:02s
epoch 43 | loss: 0.69754 | eval_custom_logloss: 0.759   |  0:13:20s
epoch 44 | loss: 0.67406 | eval_custom_logloss: 0.8212  |  0:13:38s
epoch 45 | loss: 0.68074 | eval_custom_logloss: 0.95876 |  0:13:56s
epoch 46 | loss: 0.66986 | eval_custom_logloss: 0.64837 |  0:14:14s
epoch 47 | loss: 0.66462 | eval_custom_logloss: 1.07178 |  0:14:32s
epoch 48 | loss: 0.65641 | eval_custom_logloss: 0.69185 |  0:14:51s
epoch 49 | loss: 0.65657 | eval_custom_logloss: 1.04255 |  0:15:09s
epoch 50 | loss: 0.64658 | eval_custom_logloss: 0.78965 |  0:15:27s
epoch 51 | loss: 0.65553 | eval_custom_logloss: 0.6647  |  0:15:45s
epoch 52 | loss: 0.66106 | eval_custom_logloss: 0.83189 |  0:16:03s
epoch 53 | loss: 0.65075 | eval_custom_logloss: 0.75954 |  0:16:21s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.64688
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.64615, 'Log Loss - std': 0.0008500000000000174} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.54722 | eval_custom_logloss: 1.17862 |  0:00:18s
epoch 1  | loss: 1.07092 | eval_custom_logloss: 0.96216 |  0:00:36s
epoch 2  | loss: 0.98144 | eval_custom_logloss: 0.96062 |  0:00:54s
epoch 3  | loss: 0.97313 | eval_custom_logloss: 0.86676 |  0:01:12s
epoch 4  | loss: 0.93923 | eval_custom_logloss: 0.99653 |  0:01:31s
epoch 5  | loss: 0.92126 | eval_custom_logloss: 0.80567 |  0:01:50s
epoch 6  | loss: 0.90259 | eval_custom_logloss: 0.88887 |  0:02:10s
epoch 7  | loss: 0.90555 | eval_custom_logloss: 0.80757 |  0:02:29s
epoch 8  | loss: 0.89279 | eval_custom_logloss: 0.86232 |  0:02:49s
epoch 9  | loss: 0.8702  | eval_custom_logloss: 0.82586 |  0:03:08s
epoch 10 | loss: 0.8607  | eval_custom_logloss: 1.3777  |  0:03:28s
epoch 11 | loss: 0.8577  | eval_custom_logloss: 1.11307 |  0:03:47s
epoch 12 | loss: 0.81241 | eval_custom_logloss: 0.78798 |  0:04:07s
epoch 13 | loss: 0.81027 | eval_custom_logloss: 0.91872 |  0:04:26s
epoch 14 | loss: 0.79492 | eval_custom_logloss: 0.8354  |  0:04:45s
epoch 15 | loss: 0.76875 | eval_custom_logloss: 0.86064 |  0:05:03s
epoch 16 | loss: 0.76405 | eval_custom_logloss: 0.74649 |  0:05:21s
epoch 17 | loss: 0.76592 | eval_custom_logloss: 0.89908 |  0:05:39s
epoch 18 | loss: 0.73915 | eval_custom_logloss: 0.67839 |  0:05:58s
epoch 19 | loss: 0.7537  | eval_custom_logloss: 0.88241 |  0:06:16s
epoch 20 | loss: 0.74449 | eval_custom_logloss: 1.01533 |  0:06:34s
epoch 21 | loss: 0.74181 | eval_custom_logloss: 1.0753  |  0:06:53s
epoch 22 | loss: 0.74284 | eval_custom_logloss: 0.63803 |  0:07:11s
epoch 23 | loss: 0.73332 | eval_custom_logloss: 0.8247  |  0:07:29s
epoch 24 | loss: 0.73555 | eval_custom_logloss: 1.04229 |  0:07:48s
epoch 25 | loss: 0.74351 | eval_custom_logloss: 0.68732 |  0:08:06s
epoch 26 | loss: 0.72071 | eval_custom_logloss: 0.81502 |  0:08:24s
epoch 27 | loss: 0.72112 | eval_custom_logloss: 0.84978 |  0:08:43s
epoch 28 | loss: 0.71827 | eval_custom_logloss: 0.76667 |  0:09:01s
epoch 29 | loss: 0.70714 | eval_custom_logloss: 0.81969 |  0:09:19s
epoch 30 | loss: 0.70943 | eval_custom_logloss: 0.90726 |  0:09:38s
epoch 31 | loss: 0.71002 | eval_custom_logloss: 0.90218 |  0:09:56s
epoch 32 | loss: 0.70859 | eval_custom_logloss: 0.63704 |  0:10:15s
epoch 33 | loss: 0.70278 | eval_custom_logloss: 0.68399 |  0:10:33s
epoch 34 | loss: 0.7089  | eval_custom_logloss: 0.70417 |  0:10:52s
epoch 35 | loss: 0.69421 | eval_custom_logloss: 0.84074 |  0:11:10s
epoch 36 | loss: 0.69206 | eval_custom_logloss: 0.81661 |  0:11:28s
epoch 37 | loss: 0.68623 | eval_custom_logloss: 0.67235 |  0:11:47s
epoch 38 | loss: 0.67697 | eval_custom_logloss: 0.73542 |  0:12:05s
epoch 39 | loss: 0.6858  | eval_custom_logloss: 0.78322 |  0:12:24s
epoch 40 | loss: 0.69067 | eval_custom_logloss: 0.83909 |  0:12:42s
epoch 41 | loss: 0.67745 | eval_custom_logloss: 0.81397 |  0:13:00s
epoch 42 | loss: 0.68673 | eval_custom_logloss: 0.83996 |  0:13:19s
epoch 43 | loss: 0.66979 | eval_custom_logloss: 0.72537 |  0:13:37s
epoch 44 | loss: 0.67439 | eval_custom_logloss: 0.65585 |  0:13:55s
epoch 45 | loss: 0.67418 | eval_custom_logloss: 1.04777 |  0:14:13s
epoch 46 | loss: 0.67025 | eval_custom_logloss: 0.95085 |  0:14:32s
epoch 47 | loss: 0.66097 | eval_custom_logloss: 0.85327 |  0:14:50s
epoch 48 | loss: 0.6652  | eval_custom_logloss: 0.66897 |  0:15:08s
epoch 49 | loss: 0.65761 | eval_custom_logloss: 0.9421  |  0:15:27s
epoch 50 | loss: 0.67193 | eval_custom_logloss: 0.81632 |  0:15:45s
epoch 51 | loss: 0.65931 | eval_custom_logloss: 0.74123 |  0:16:03s
epoch 52 | loss: 0.66934 | eval_custom_logloss: 0.80573 |  0:16:21s

Early stopping occurred at epoch 52 with best_epoch = 32 and best_eval_custom_logloss = 0.63704
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6425333333333333, 'Log Loss - std': 0.005161610429141506} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58499 | eval_custom_logloss: 1.42533 |  0:00:18s
epoch 1  | loss: 1.13858 | eval_custom_logloss: 0.93814 |  0:00:36s
epoch 2  | loss: 1.01126 | eval_custom_logloss: 0.91408 |  0:00:55s
epoch 3  | loss: 0.98173 | eval_custom_logloss: 0.88863 |  0:01:13s
epoch 4  | loss: 0.95993 | eval_custom_logloss: 0.86958 |  0:01:31s
epoch 5  | loss: 0.93009 | eval_custom_logloss: 0.77895 |  0:01:50s
epoch 6  | loss: 0.93393 | eval_custom_logloss: 0.93288 |  0:02:08s
epoch 7  | loss: 0.91966 | eval_custom_logloss: 0.98454 |  0:02:27s
epoch 8  | loss: 0.90112 | eval_custom_logloss: 0.75444 |  0:02:45s
epoch 9  | loss: 0.87633 | eval_custom_logloss: 0.75574 |  0:03:03s
epoch 10 | loss: 0.83355 | eval_custom_logloss: 0.98801 |  0:03:22s
epoch 11 | loss: 0.83963 | eval_custom_logloss: 0.88831 |  0:03:40s
epoch 12 | loss: 0.80867 | eval_custom_logloss: 0.83444 |  0:03:59s
epoch 13 | loss: 0.81842 | eval_custom_logloss: 1.05574 |  0:04:17s
epoch 14 | loss: 0.77612 | eval_custom_logloss: 0.87438 |  0:04:35s
epoch 15 | loss: 0.79217 | eval_custom_logloss: 0.77401 |  0:04:54s
epoch 16 | loss: 0.76369 | eval_custom_logloss: 0.86937 |  0:05:12s
epoch 17 | loss: 0.75774 | eval_custom_logloss: 0.87318 |  0:05:31s
epoch 18 | loss: 0.7498  | eval_custom_logloss: 0.79425 |  0:05:49s
epoch 19 | loss: 0.74784 | eval_custom_logloss: 0.86428 |  0:06:08s
epoch 20 | loss: 0.73789 | eval_custom_logloss: 0.65435 |  0:06:26s
epoch 21 | loss: 0.72984 | eval_custom_logloss: 0.69024 |  0:06:45s
epoch 22 | loss: 0.72594 | eval_custom_logloss: 0.68412 |  0:07:03s
epoch 23 | loss: 0.71915 | eval_custom_logloss: 0.9155  |  0:07:22s
epoch 24 | loss: 0.73085 | eval_custom_logloss: 0.68329 |  0:07:40s
epoch 25 | loss: 0.71733 | eval_custom_logloss: 0.6825  |  0:07:59s
epoch 26 | loss: 0.68963 | eval_custom_logloss: 0.65693 |  0:08:18s
epoch 27 | loss: 0.68772 | eval_custom_logloss: 0.64175 |  0:08:36s
epoch 28 | loss: 0.68979 | eval_custom_logloss: 0.65011 |  0:08:55s
epoch 29 | loss: 0.69118 | eval_custom_logloss: 0.66171 |  0:09:13s
epoch 30 | loss: 0.66888 | eval_custom_logloss: 0.63552 |  0:09:32s
epoch 31 | loss: 0.66207 | eval_custom_logloss: 0.63164 |  0:09:50s
epoch 32 | loss: 0.67634 | eval_custom_logloss: 0.93429 |  0:10:09s
epoch 33 | loss: 0.65878 | eval_custom_logloss: 0.74849 |  0:10:27s
epoch 34 | loss: 0.66689 | eval_custom_logloss: 0.64315 |  0:10:46s
epoch 35 | loss: 0.64464 | eval_custom_logloss: 0.69579 |  0:11:04s
epoch 36 | loss: 0.64828 | eval_custom_logloss: 0.84736 |  0:11:23s
epoch 37 | loss: 0.65453 | eval_custom_logloss: 0.83802 |  0:11:41s
epoch 38 | loss: 0.64763 | eval_custom_logloss: 0.59624 |  0:12:00s
epoch 39 | loss: 0.65283 | eval_custom_logloss: 0.83101 |  0:12:18s
epoch 40 | loss: 0.65847 | eval_custom_logloss: 0.61259 |  0:12:36s
epoch 41 | loss: 0.63127 | eval_custom_logloss: 0.6178  |  0:12:55s
epoch 42 | loss: 0.64109 | eval_custom_logloss: 0.63882 |  0:13:13s
epoch 43 | loss: 0.63231 | eval_custom_logloss: 0.69297 |  0:13:32s
epoch 44 | loss: 0.62451 | eval_custom_logloss: 0.68952 |  0:13:50s
epoch 45 | loss: 0.62288 | eval_custom_logloss: 0.94687 |  0:14:08s
epoch 46 | loss: 0.61632 | eval_custom_logloss: 0.76181 |  0:14:27s
epoch 47 | loss: 0.61377 | eval_custom_logloss: 0.84698 |  0:14:45s
epoch 48 | loss: 0.61357 | eval_custom_logloss: 0.64573 |  0:15:03s
epoch 49 | loss: 0.61722 | eval_custom_logloss: 0.65007 |  0:15:22s
epoch 50 | loss: 0.606   | eval_custom_logloss: 1.03928 |  0:15:40s
epoch 51 | loss: 0.61094 | eval_custom_logloss: 0.90993 |  0:15:58s
epoch 52 | loss: 0.61339 | eval_custom_logloss: 0.58918 |  0:16:16s
epoch 53 | loss: 0.60564 | eval_custom_logloss: 0.5987  |  0:16:35s
epoch 54 | loss: 0.6085  | eval_custom_logloss: 0.6183  |  0:16:53s
epoch 55 | loss: 0.60178 | eval_custom_logloss: 0.77575 |  0:17:11s
epoch 56 | loss: 0.58872 | eval_custom_logloss: 0.73579 |  0:17:30s
epoch 57 | loss: 0.61407 | eval_custom_logloss: 0.62746 |  0:17:48s
epoch 58 | loss: 0.59625 | eval_custom_logloss: 0.7226  |  0:18:07s
epoch 59 | loss: 0.58557 | eval_custom_logloss: 0.83302 |  0:18:25s
epoch 60 | loss: 0.58955 | eval_custom_logloss: 0.6947  |  0:18:44s
epoch 61 | loss: 0.59125 | eval_custom_logloss: 0.78296 |  0:19:02s
epoch 62 | loss: 0.57922 | eval_custom_logloss: 0.74465 |  0:19:21s
epoch 63 | loss: 0.59584 | eval_custom_logloss: 0.8464  |  0:19:39s
epoch 64 | loss: 0.58781 | eval_custom_logloss: 0.68241 |  0:19:58s
epoch 65 | loss: 0.59203 | eval_custom_logloss: 0.818   |  0:20:16s
epoch 66 | loss: 0.57551 | eval_custom_logloss: 1.02847 |  0:20:35s
epoch 67 | loss: 0.57892 | eval_custom_logloss: 0.60624 |  0:20:53s
epoch 68 | loss: 0.58377 | eval_custom_logloss: 0.96067 |  0:21:12s
epoch 69 | loss: 0.58648 | eval_custom_logloss: 0.81084 |  0:21:30s
epoch 70 | loss: 0.57974 | eval_custom_logloss: 0.7513  |  0:21:48s
epoch 71 | loss: 0.57908 | eval_custom_logloss: 0.88433 |  0:22:07s
epoch 72 | loss: 0.57023 | eval_custom_logloss: 0.82741 |  0:22:25s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 0.58918
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62905, 'Log Loss - std': 0.02377777323468284} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.51311 | eval_custom_logloss: 1.07759 |  0:00:18s
epoch 1  | loss: 1.13253 | eval_custom_logloss: 1.10838 |  0:00:36s
epoch 2  | loss: 1.01778 | eval_custom_logloss: 0.86403 |  0:00:54s
epoch 3  | loss: 0.96423 | eval_custom_logloss: 0.88987 |  0:01:13s
epoch 4  | loss: 0.9481  | eval_custom_logloss: 0.95873 |  0:01:32s
epoch 5  | loss: 0.91096 | eval_custom_logloss: 0.79475 |  0:01:50s
epoch 6  | loss: 0.89006 | eval_custom_logloss: 0.94018 |  0:02:08s
epoch 7  | loss: 0.85585 | eval_custom_logloss: 0.77155 |  0:02:26s
epoch 8  | loss: 0.85295 | eval_custom_logloss: 0.75141 |  0:02:44s
epoch 9  | loss: 0.82347 | eval_custom_logloss: 1.17271 |  0:03:03s
epoch 10 | loss: 0.80179 | eval_custom_logloss: 0.82953 |  0:03:21s
epoch 11 | loss: 0.7993  | eval_custom_logloss: 0.99694 |  0:03:39s
epoch 12 | loss: 0.80169 | eval_custom_logloss: 0.88392 |  0:03:58s
epoch 13 | loss: 0.77792 | eval_custom_logloss: 0.81936 |  0:04:16s
epoch 14 | loss: 0.7761  | eval_custom_logloss: 0.86009 |  0:04:35s
epoch 15 | loss: 0.77395 | eval_custom_logloss: 0.80651 |  0:04:53s
epoch 16 | loss: 0.76399 | eval_custom_logloss: 0.77744 |  0:05:11s
epoch 17 | loss: 0.75952 | eval_custom_logloss: 0.76252 |  0:05:30s
epoch 18 | loss: 0.76933 | eval_custom_logloss: 0.96209 |  0:05:48s
epoch 19 | loss: 0.74423 | eval_custom_logloss: 0.7907  |  0:06:06s
epoch 20 | loss: 0.73905 | eval_custom_logloss: 0.79081 |  0:06:25s
epoch 21 | loss: 0.73193 | eval_custom_logloss: 0.83726 |  0:06:43s
epoch 22 | loss: 0.71907 | eval_custom_logloss: 0.92004 |  0:07:01s
epoch 23 | loss: 0.72446 | eval_custom_logloss: 0.78135 |  0:07:19s
epoch 24 | loss: 0.71855 | eval_custom_logloss: 0.88371 |  0:07:38s
epoch 25 | loss: 0.71471 | eval_custom_logloss: 1.27188 |  0:07:56s
epoch 26 | loss: 0.70635 | eval_custom_logloss: 0.88728 |  0:08:14s
epoch 27 | loss: 0.71754 | eval_custom_logloss: 0.87718 |  0:08:33s
epoch 28 | loss: 0.71456 | eval_custom_logloss: 1.14556 |  0:08:51s

Early stopping occurred at epoch 28 with best_epoch = 8 and best_eval_custom_logloss = 0.75141
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6532600000000001, 'Log Loss - std': 0.052884803110156316} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 0.6532600000000001 and parameters: {'n_d': 52, 'n_steps': 5, 'gamma': 1.8583073545470548, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.03353409790255242, 'mask_type': 'sparsemax'}. Best is trial 6 with value: 0.6532600000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.32957 | eval_custom_logloss: 8.4226  |  0:00:14s
epoch 1  | loss: 0.99269 | eval_custom_logloss: 2.99494 |  0:00:28s
epoch 2  | loss: 0.94183 | eval_custom_logloss: 1.84805 |  0:00:42s
epoch 3  | loss: 0.90706 | eval_custom_logloss: 1.01323 |  0:00:57s
epoch 4  | loss: 0.90421 | eval_custom_logloss: 1.19665 |  0:01:12s
epoch 5  | loss: 0.88354 | eval_custom_logloss: 1.10185 |  0:01:26s
epoch 6  | loss: 0.86853 | eval_custom_logloss: 0.94329 |  0:01:39s
epoch 7  | loss: 0.85067 | eval_custom_logloss: 1.01289 |  0:01:54s
epoch 8  | loss: 0.81918 | eval_custom_logloss: 0.89247 |  0:02:08s
epoch 9  | loss: 0.81202 | eval_custom_logloss: 0.8823  |  0:02:21s
epoch 10 | loss: 0.78715 | eval_custom_logloss: 0.6968  |  0:02:35s
epoch 11 | loss: 0.78651 | eval_custom_logloss: 1.82401 |  0:02:49s
epoch 12 | loss: 0.76979 | eval_custom_logloss: 0.81921 |  0:03:03s
epoch 13 | loss: 0.76692 | eval_custom_logloss: 0.92856 |  0:03:17s
epoch 14 | loss: 0.76498 | eval_custom_logloss: 1.05407 |  0:03:32s
epoch 15 | loss: 0.77831 | eval_custom_logloss: 0.84782 |  0:03:47s
epoch 16 | loss: 0.74744 | eval_custom_logloss: 0.70331 |  0:04:02s
epoch 17 | loss: 0.74251 | eval_custom_logloss: 1.32694 |  0:04:17s
epoch 18 | loss: 0.72794 | eval_custom_logloss: 0.68126 |  0:04:30s
epoch 19 | loss: 0.72691 | eval_custom_logloss: 0.77434 |  0:04:44s
epoch 20 | loss: 0.71625 | eval_custom_logloss: 0.80257 |  0:04:58s
epoch 21 | loss: 0.72429 | eval_custom_logloss: 0.95191 |  0:05:12s
epoch 22 | loss: 0.72017 | eval_custom_logloss: 0.74223 |  0:05:25s
epoch 23 | loss: 0.713   | eval_custom_logloss: 1.08512 |  0:05:40s
epoch 24 | loss: 0.70924 | eval_custom_logloss: 0.64407 |  0:05:53s
epoch 25 | loss: 0.7077  | eval_custom_logloss: 0.73414 |  0:06:06s
epoch 26 | loss: 0.70226 | eval_custom_logloss: 0.65737 |  0:06:21s
epoch 27 | loss: 0.68549 | eval_custom_logloss: 1.03444 |  0:06:35s
epoch 28 | loss: 0.69396 | eval_custom_logloss: 0.82487 |  0:06:49s
epoch 29 | loss: 0.68745 | eval_custom_logloss: 0.64528 |  0:07:03s
epoch 30 | loss: 0.68946 | eval_custom_logloss: 1.2588  |  0:07:17s
epoch 31 | loss: 0.67902 | eval_custom_logloss: 0.65351 |  0:07:31s
epoch 32 | loss: 0.69952 | eval_custom_logloss: 0.66896 |  0:07:46s
epoch 33 | loss: 0.68924 | eval_custom_logloss: 0.67943 |  0:08:00s
epoch 34 | loss: 0.67503 | eval_custom_logloss: 0.69335 |  0:08:15s
epoch 35 | loss: 0.67737 | eval_custom_logloss: 0.65233 |  0:08:29s
epoch 36 | loss: 0.68116 | eval_custom_logloss: 0.74023 |  0:08:44s
epoch 37 | loss: 0.67879 | eval_custom_logloss: 0.79249 |  0:08:59s
epoch 38 | loss: 0.66268 | eval_custom_logloss: 0.9459  |  0:09:13s
epoch 39 | loss: 0.67619 | eval_custom_logloss: 0.78201 |  0:09:27s
epoch 40 | loss: 0.66134 | eval_custom_logloss: 0.87875 |  0:09:41s
epoch 41 | loss: 0.67356 | eval_custom_logloss: 0.6537  |  0:09:55s
epoch 42 | loss: 0.66503 | eval_custom_logloss: 0.76636 |  0:10:09s
epoch 43 | loss: 0.66336 | eval_custom_logloss: 0.69202 |  0:10:22s
epoch 44 | loss: 0.65202 | eval_custom_logloss: 0.9224  |  0:10:37s

Early stopping occurred at epoch 44 with best_epoch = 24 and best_eval_custom_logloss = 0.64407
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.643, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.28852 | eval_custom_logloss: 4.88793 |  0:00:12s
epoch 1  | loss: 0.99509 | eval_custom_logloss: 2.24585 |  0:00:24s
epoch 2  | loss: 0.95305 | eval_custom_logloss: 1.46831 |  0:00:38s
epoch 3  | loss: 0.89947 | eval_custom_logloss: 1.4297  |  0:00:52s
epoch 4  | loss: 0.91342 | eval_custom_logloss: 1.20751 |  0:01:06s
epoch 5  | loss: 0.89583 | eval_custom_logloss: 0.96951 |  0:01:20s
epoch 6  | loss: 0.87004 | eval_custom_logloss: 1.09871 |  0:01:34s
epoch 7  | loss: 0.85124 | eval_custom_logloss: 0.85858 |  0:01:47s
epoch 8  | loss: 0.83165 | eval_custom_logloss: 1.3368  |  0:02:03s
epoch 9  | loss: 0.83674 | eval_custom_logloss: 1.19335 |  0:02:16s
epoch 10 | loss: 0.8238  | eval_custom_logloss: 0.84432 |  0:02:30s
epoch 11 | loss: 0.80712 | eval_custom_logloss: 0.75566 |  0:02:44s
epoch 12 | loss: 0.79611 | eval_custom_logloss: 2.26051 |  0:02:58s
epoch 13 | loss: 0.78353 | eval_custom_logloss: 1.21378 |  0:03:12s
epoch 14 | loss: 0.7732  | eval_custom_logloss: 0.87724 |  0:03:26s
epoch 15 | loss: 0.76829 | eval_custom_logloss: 0.87931 |  0:03:40s
epoch 16 | loss: 0.73476 | eval_custom_logloss: 0.94155 |  0:03:54s
epoch 17 | loss: 0.73497 | eval_custom_logloss: 0.86871 |  0:04:08s
epoch 18 | loss: 0.73363 | eval_custom_logloss: 0.7107  |  0:04:21s
epoch 19 | loss: 0.71692 | eval_custom_logloss: 1.08735 |  0:04:36s
epoch 20 | loss: 0.71621 | eval_custom_logloss: 0.74968 |  0:04:50s
epoch 21 | loss: 0.69815 | eval_custom_logloss: 0.86524 |  0:05:04s
epoch 22 | loss: 0.70904 | eval_custom_logloss: 0.95557 |  0:05:18s
epoch 23 | loss: 0.70992 | eval_custom_logloss: 0.78084 |  0:05:32s
epoch 24 | loss: 0.72885 | eval_custom_logloss: 0.94966 |  0:05:46s
epoch 25 | loss: 0.72279 | eval_custom_logloss: 1.12188 |  0:06:00s
epoch 26 | loss: 0.71061 | eval_custom_logloss: 1.29219 |  0:06:14s
epoch 27 | loss: 0.70691 | eval_custom_logloss: 0.98985 |  0:06:27s
epoch 28 | loss: 0.70089 | eval_custom_logloss: 1.19127 |  0:06:41s
epoch 29 | loss: 0.67187 | eval_custom_logloss: 0.8253  |  0:06:55s
epoch 30 | loss: 0.6652  | eval_custom_logloss: 0.7864  |  0:07:09s
epoch 31 | loss: 0.6558  | eval_custom_logloss: 1.33572 |  0:07:23s
epoch 32 | loss: 0.66986 | eval_custom_logloss: 1.03433 |  0:07:38s
epoch 33 | loss: 0.66215 | eval_custom_logloss: 0.63127 |  0:07:52s
epoch 34 | loss: 0.65266 | eval_custom_logloss: 0.93045 |  0:08:05s
epoch 35 | loss: 0.65517 | eval_custom_logloss: 0.95048 |  0:08:19s
epoch 36 | loss: 0.66157 | eval_custom_logloss: 1.26808 |  0:08:33s
epoch 37 | loss: 0.6749  | eval_custom_logloss: 0.74912 |  0:08:47s
epoch 38 | loss: 0.66735 | eval_custom_logloss: 0.79051 |  0:09:01s
epoch 39 | loss: 0.64102 | eval_custom_logloss: 0.87877 |  0:09:15s
epoch 40 | loss: 0.64345 | eval_custom_logloss: 1.30422 |  0:09:28s
epoch 41 | loss: 0.64959 | eval_custom_logloss: 0.96538 |  0:09:42s
epoch 42 | loss: 0.64088 | eval_custom_logloss: 0.89455 |  0:09:56s
epoch 43 | loss: 0.63285 | eval_custom_logloss: 0.79475 |  0:10:11s
epoch 44 | loss: 0.62916 | eval_custom_logloss: 0.80462 |  0:10:25s
epoch 45 | loss: 0.63304 | eval_custom_logloss: 1.04064 |  0:10:38s
epoch 46 | loss: 0.61791 | eval_custom_logloss: 0.99177 |  0:10:52s
epoch 47 | loss: 0.62656 | eval_custom_logloss: 1.05057 |  0:11:06s
epoch 48 | loss: 0.63025 | eval_custom_logloss: 1.3964  |  0:11:19s
epoch 49 | loss: 0.6153  | eval_custom_logloss: 0.86656 |  0:11:33s
epoch 50 | loss: 0.62579 | eval_custom_logloss: 0.9808  |  0:11:47s
epoch 51 | loss: 0.61326 | eval_custom_logloss: 0.87394 |  0:12:01s
epoch 52 | loss: 0.61051 | eval_custom_logloss: 0.86554 |  0:12:16s
epoch 53 | loss: 0.61972 | eval_custom_logloss: 1.09955 |  0:12:29s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.63127
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6368, 'Log Loss - std': 0.006199999999999983} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.25733 | eval_custom_logloss: 3.35406 |  0:00:14s
epoch 1  | loss: 0.96716 | eval_custom_logloss: 2.571   |  0:00:28s
epoch 2  | loss: 0.92313 | eval_custom_logloss: 1.76285 |  0:00:42s
epoch 3  | loss: 0.8908  | eval_custom_logloss: 1.66795 |  0:00:57s
epoch 4  | loss: 0.87849 | eval_custom_logloss: 3.14707 |  0:01:11s
epoch 5  | loss: 0.85666 | eval_custom_logloss: 2.0912  |  0:01:25s
epoch 6  | loss: 0.83897 | eval_custom_logloss: 1.83736 |  0:01:39s
epoch 7  | loss: 0.81544 | eval_custom_logloss: 1.68646 |  0:01:54s
epoch 8  | loss: 0.80668 | eval_custom_logloss: 0.88805 |  0:02:07s
epoch 9  | loss: 0.80756 | eval_custom_logloss: 1.63779 |  0:02:20s
epoch 10 | loss: 0.79315 | eval_custom_logloss: 2.01379 |  0:02:33s
epoch 11 | loss: 0.79812 | eval_custom_logloss: 1.43646 |  0:02:48s
epoch 12 | loss: 0.79973 | eval_custom_logloss: 1.3124  |  0:03:02s
epoch 13 | loss: 0.78808 | eval_custom_logloss: 0.93941 |  0:03:16s
epoch 14 | loss: 0.77071 | eval_custom_logloss: 1.28059 |  0:03:31s
epoch 15 | loss: 0.76717 | eval_custom_logloss: 0.87106 |  0:03:45s
epoch 16 | loss: 0.76707 | eval_custom_logloss: 1.02615 |  0:04:00s
epoch 17 | loss: 0.7448  | eval_custom_logloss: 1.18136 |  0:04:14s
epoch 18 | loss: 0.7403  | eval_custom_logloss: 1.23681 |  0:04:27s
epoch 19 | loss: 0.7406  | eval_custom_logloss: 1.30662 |  0:04:41s
epoch 20 | loss: 0.72721 | eval_custom_logloss: 3.26358 |  0:04:55s
epoch 21 | loss: 0.73197 | eval_custom_logloss: 0.73762 |  0:05:09s
epoch 22 | loss: 0.72069 | eval_custom_logloss: 1.06939 |  0:05:23s
epoch 23 | loss: 0.72861 | eval_custom_logloss: 2.21393 |  0:05:37s
epoch 24 | loss: 0.70723 | eval_custom_logloss: 0.97314 |  0:05:52s
epoch 25 | loss: 0.70992 | eval_custom_logloss: 1.68834 |  0:06:05s
epoch 26 | loss: 0.70116 | eval_custom_logloss: 0.88981 |  0:06:19s
epoch 27 | loss: 0.70608 | eval_custom_logloss: 0.79738 |  0:06:33s
epoch 28 | loss: 0.69626 | eval_custom_logloss: 1.13837 |  0:06:47s
epoch 29 | loss: 0.6891  | eval_custom_logloss: 1.17047 |  0:07:01s
epoch 30 | loss: 0.68964 | eval_custom_logloss: 0.91984 |  0:07:15s
epoch 31 | loss: 0.67816 | eval_custom_logloss: 0.67011 |  0:07:29s
epoch 32 | loss: 0.69255 | eval_custom_logloss: 0.97879 |  0:07:42s
epoch 33 | loss: 0.67103 | eval_custom_logloss: 0.6341  |  0:07:56s
epoch 34 | loss: 0.66285 | eval_custom_logloss: 1.05244 |  0:08:11s
epoch 35 | loss: 0.66513 | eval_custom_logloss: 0.67743 |  0:08:25s
epoch 36 | loss: 0.66586 | eval_custom_logloss: 1.01973 |  0:08:39s
epoch 37 | loss: 0.67187 | eval_custom_logloss: 0.75011 |  0:08:53s
epoch 38 | loss: 0.65508 | eval_custom_logloss: 0.81149 |  0:09:08s
epoch 39 | loss: 0.66673 | eval_custom_logloss: 0.85332 |  0:09:21s
epoch 40 | loss: 0.69227 | eval_custom_logloss: 0.75164 |  0:09:35s
epoch 41 | loss: 0.66425 | eval_custom_logloss: 0.75686 |  0:09:49s
epoch 42 | loss: 0.64847 | eval_custom_logloss: 1.03923 |  0:10:03s
epoch 43 | loss: 0.63947 | eval_custom_logloss: 0.86569 |  0:10:16s
epoch 44 | loss: 0.64259 | eval_custom_logloss: 0.9597  |  0:10:30s
epoch 45 | loss: 0.64728 | eval_custom_logloss: 0.83003 |  0:10:44s
epoch 46 | loss: 0.64863 | eval_custom_logloss: 0.68062 |  0:10:59s
epoch 47 | loss: 0.63858 | eval_custom_logloss: 0.88082 |  0:11:13s
epoch 48 | loss: 0.63474 | eval_custom_logloss: 0.78215 |  0:11:26s
epoch 49 | loss: 0.62961 | eval_custom_logloss: 0.71561 |  0:11:40s
epoch 50 | loss: 0.63443 | eval_custom_logloss: 0.89712 |  0:11:54s
epoch 51 | loss: 0.63211 | eval_custom_logloss: 1.05859 |  0:12:08s
epoch 52 | loss: 0.62293 | eval_custom_logloss: 1.00593 |  0:12:23s
epoch 53 | loss: 0.62413 | eval_custom_logloss: 1.00027 |  0:12:37s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.6341
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6355333333333334, 'Log Loss - std': 0.0053698748174939295} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.29109 | eval_custom_logloss: 2.88786 |  0:00:13s
epoch 1  | loss: 0.98648 | eval_custom_logloss: 2.61018 |  0:00:28s
epoch 2  | loss: 0.92508 | eval_custom_logloss: 1.19133 |  0:00:42s
epoch 3  | loss: 0.90969 | eval_custom_logloss: 1.02061 |  0:00:56s
epoch 4  | loss: 0.89324 | eval_custom_logloss: 1.28342 |  0:01:11s
epoch 5  | loss: 0.87038 | eval_custom_logloss: 1.16124 |  0:01:25s
epoch 6  | loss: 0.8605  | eval_custom_logloss: 1.02369 |  0:01:39s
epoch 7  | loss: 0.84481 | eval_custom_logloss: 0.80743 |  0:01:52s
epoch 8  | loss: 0.82612 | eval_custom_logloss: 0.9555  |  0:02:06s
epoch 9  | loss: 0.82473 | eval_custom_logloss: 0.86491 |  0:02:20s
epoch 10 | loss: 0.80833 | eval_custom_logloss: 1.032   |  0:02:34s
epoch 11 | loss: 0.79047 | eval_custom_logloss: 0.7752  |  0:02:48s
epoch 12 | loss: 0.78519 | eval_custom_logloss: 1.10264 |  0:03:02s
epoch 13 | loss: 0.7853  | eval_custom_logloss: 0.82531 |  0:03:16s
epoch 14 | loss: 0.75708 | eval_custom_logloss: 0.73605 |  0:03:30s
epoch 15 | loss: 0.75122 | eval_custom_logloss: 0.95947 |  0:03:43s
epoch 16 | loss: 0.75644 | eval_custom_logloss: 0.75384 |  0:03:57s
epoch 17 | loss: 0.71754 | eval_custom_logloss: 0.74684 |  0:04:11s
epoch 18 | loss: 0.71568 | eval_custom_logloss: 0.77796 |  0:04:24s
epoch 19 | loss: 0.72329 | eval_custom_logloss: 0.74479 |  0:04:38s
epoch 20 | loss: 0.69788 | eval_custom_logloss: 0.72397 |  0:04:52s
epoch 21 | loss: 0.70369 | eval_custom_logloss: 0.77601 |  0:05:06s
epoch 22 | loss: 0.69105 | eval_custom_logloss: 0.80282 |  0:05:19s
epoch 23 | loss: 0.69376 | eval_custom_logloss: 0.86011 |  0:05:34s
epoch 24 | loss: 0.69981 | eval_custom_logloss: 0.7796  |  0:05:48s
epoch 25 | loss: 0.69294 | eval_custom_logloss: 0.81654 |  0:06:02s
epoch 26 | loss: 0.68676 | eval_custom_logloss: 0.81987 |  0:06:16s
epoch 27 | loss: 0.68244 | eval_custom_logloss: 0.78512 |  0:06:30s
epoch 28 | loss: 0.67504 | eval_custom_logloss: 0.8651  |  0:06:45s
epoch 29 | loss: 0.6683  | eval_custom_logloss: 0.80842 |  0:06:59s
epoch 30 | loss: 0.67463 | eval_custom_logloss: 0.76862 |  0:07:13s
epoch 31 | loss: 0.666   | eval_custom_logloss: 1.0993  |  0:07:27s
epoch 32 | loss: 0.67532 | eval_custom_logloss: 0.88977 |  0:07:41s
epoch 33 | loss: 0.66479 | eval_custom_logloss: 1.0822  |  0:07:55s
epoch 34 | loss: 0.6648  | eval_custom_logloss: 0.74942 |  0:08:09s
epoch 35 | loss: 0.66057 | eval_custom_logloss: 0.9947  |  0:08:24s
epoch 36 | loss: 0.66857 | eval_custom_logloss: 1.42251 |  0:08:38s
epoch 37 | loss: 0.6617  | eval_custom_logloss: 0.7309  |  0:08:52s
epoch 38 | loss: 0.65852 | eval_custom_logloss: 0.84519 |  0:09:06s
epoch 39 | loss: 0.65557 | eval_custom_logloss: 0.86813 |  0:09:20s
epoch 40 | loss: 0.65392 | eval_custom_logloss: 1.03081 |  0:09:34s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 0.72397
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6575500000000001, 'Log Loss - std': 0.038416500361172924} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.26418 | eval_custom_logloss: 4.13818 |  0:00:14s
epoch 1  | loss: 0.99983 | eval_custom_logloss: 1.84802 |  0:00:28s
epoch 2  | loss: 0.92429 | eval_custom_logloss: 1.56851 |  0:00:42s
epoch 3  | loss: 0.9108  | eval_custom_logloss: 1.01339 |  0:00:57s
epoch 4  | loss: 0.90292 | eval_custom_logloss: 0.97375 |  0:01:10s
epoch 5  | loss: 0.86733 | eval_custom_logloss: 0.9654  |  0:01:24s
epoch 6  | loss: 0.85913 | eval_custom_logloss: 0.92029 |  0:01:36s
epoch 7  | loss: 0.86243 | eval_custom_logloss: 0.93743 |  0:01:51s
epoch 8  | loss: 0.83817 | eval_custom_logloss: 0.8412  |  0:02:05s
epoch 9  | loss: 0.82661 | eval_custom_logloss: 1.4185  |  0:02:20s
epoch 10 | loss: 0.82196 | eval_custom_logloss: 0.81617 |  0:02:34s
epoch 11 | loss: 0.80508 | eval_custom_logloss: 1.03216 |  0:02:48s
epoch 12 | loss: 0.77893 | eval_custom_logloss: 0.81762 |  0:03:01s
epoch 13 | loss: 0.75506 | eval_custom_logloss: 0.76667 |  0:03:15s
epoch 14 | loss: 0.76936 | eval_custom_logloss: 0.83267 |  0:03:29s
epoch 15 | loss: 0.75937 | eval_custom_logloss: 0.90632 |  0:03:43s
epoch 16 | loss: 0.7503  | eval_custom_logloss: 0.79233 |  0:03:56s
epoch 17 | loss: 0.73122 | eval_custom_logloss: 0.91735 |  0:04:10s
epoch 18 | loss: 0.7256  | eval_custom_logloss: 0.93644 |  0:04:24s
epoch 19 | loss: 0.73343 | eval_custom_logloss: 0.73704 |  0:04:38s
epoch 20 | loss: 0.71404 | eval_custom_logloss: 0.6269  |  0:04:52s
epoch 21 | loss: 0.72292 | eval_custom_logloss: 0.71639 |  0:05:06s
epoch 22 | loss: 0.70294 | eval_custom_logloss: 0.92943 |  0:05:20s
epoch 23 | loss: 0.71732 | eval_custom_logloss: 0.78063 |  0:05:35s
epoch 24 | loss: 0.69214 | eval_custom_logloss: 0.78    |  0:05:49s
epoch 25 | loss: 0.68734 | eval_custom_logloss: 1.00965 |  0:06:04s
epoch 26 | loss: 0.69506 | eval_custom_logloss: 0.78863 |  0:06:18s
epoch 27 | loss: 0.68488 | eval_custom_logloss: 0.66924 |  0:06:32s
epoch 28 | loss: 0.66833 | eval_custom_logloss: 0.64111 |  0:06:46s
epoch 29 | loss: 0.67375 | eval_custom_logloss: 0.62687 |  0:07:00s
epoch 30 | loss: 0.66559 | eval_custom_logloss: 0.75578 |  0:07:14s
epoch 31 | loss: 0.6641  | eval_custom_logloss: 0.94763 |  0:07:28s
epoch 32 | loss: 0.673   | eval_custom_logloss: 0.65693 |  0:07:41s
epoch 33 | loss: 0.66319 | eval_custom_logloss: 0.83524 |  0:07:54s
epoch 34 | loss: 0.65193 | eval_custom_logloss: 0.67636 |  0:08:07s
epoch 35 | loss: 0.65042 | eval_custom_logloss: 0.65942 |  0:08:22s
epoch 36 | loss: 0.65673 | eval_custom_logloss: 0.73439 |  0:08:37s
epoch 37 | loss: 0.65908 | eval_custom_logloss: 0.84337 |  0:08:51s
epoch 38 | loss: 0.65956 | eval_custom_logloss: 0.7776  |  0:09:07s
epoch 39 | loss: 0.65393 | eval_custom_logloss: 0.83262 |  0:09:22s
epoch 40 | loss: 0.63476 | eval_custom_logloss: 0.74404 |  0:09:37s
epoch 41 | loss: 0.64623 | eval_custom_logloss: 0.66378 |  0:09:53s
epoch 42 | loss: 0.62364 | eval_custom_logloss: 0.8156  |  0:10:08s
epoch 43 | loss: 0.63496 | eval_custom_logloss: 0.64524 |  0:10:23s
epoch 44 | loss: 0.63303 | eval_custom_logloss: 0.78897 |  0:10:37s
epoch 45 | loss: 0.63547 | eval_custom_logloss: 0.68235 |  0:10:52s
epoch 46 | loss: 0.62569 | eval_custom_logloss: 0.79641 |  0:11:07s
epoch 47 | loss: 0.62282 | eval_custom_logloss: 0.95879 |  0:11:22s
epoch 48 | loss: 0.61618 | eval_custom_logloss: 0.93948 |  0:11:37s
epoch 49 | loss: 0.61678 | eval_custom_logloss: 0.66688 |  0:11:50s

Early stopping occurred at epoch 49 with best_epoch = 29 and best_eval_custom_logloss = 0.62687
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6511000000000001, 'Log Loss - std': 0.036702479480274906} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 0.6511000000000001 and parameters: {'n_d': 17, 'n_steps': 3, 'gamma': 1.012586963647991, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.006196321153188687, 'mask_type': 'entmax'}. Best is trial 6 with value: 0.6532600000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.57046 | eval_custom_logloss: 3.11835 |  0:00:14s
epoch 1  | loss: 1.12171 | eval_custom_logloss: 3.2339  |  0:00:29s
epoch 2  | loss: 1.00661 | eval_custom_logloss: 2.66209 |  0:00:46s
epoch 3  | loss: 0.97477 | eval_custom_logloss: 1.46122 |  0:01:00s
epoch 4  | loss: 0.93472 | eval_custom_logloss: 1.0247  |  0:01:14s
epoch 5  | loss: 0.93399 | eval_custom_logloss: 1.35918 |  0:01:28s
epoch 6  | loss: 0.8964  | eval_custom_logloss: 1.61364 |  0:01:44s
epoch 7  | loss: 0.8878  | eval_custom_logloss: 1.46292 |  0:02:00s
epoch 8  | loss: 0.87811 | eval_custom_logloss: 0.96729 |  0:02:15s
epoch 9  | loss: 0.89032 | eval_custom_logloss: 1.42817 |  0:02:30s
epoch 10 | loss: 0.87534 | eval_custom_logloss: 1.39076 |  0:02:45s
epoch 11 | loss: 0.88947 | eval_custom_logloss: 0.94512 |  0:03:00s
epoch 12 | loss: 0.84704 | eval_custom_logloss: 2.1443  |  0:03:16s
epoch 13 | loss: 0.82737 | eval_custom_logloss: 0.99214 |  0:03:32s
epoch 14 | loss: 0.85051 | eval_custom_logloss: 2.57619 |  0:03:47s
epoch 15 | loss: 0.82856 | eval_custom_logloss: 0.93784 |  0:04:02s
epoch 16 | loss: 0.82257 | eval_custom_logloss: 1.13232 |  0:04:17s
epoch 17 | loss: 0.81635 | eval_custom_logloss: 0.8028  |  0:04:32s
epoch 18 | loss: 0.79569 | eval_custom_logloss: 1.16787 |  0:04:47s
epoch 19 | loss: 0.80356 | eval_custom_logloss: 2.33347 |  0:05:02s
epoch 20 | loss: 0.8028  | eval_custom_logloss: 0.89478 |  0:05:18s
epoch 21 | loss: 0.80178 | eval_custom_logloss: 0.88155 |  0:05:33s
epoch 22 | loss: 0.78717 | eval_custom_logloss: 1.71128 |  0:05:47s
epoch 23 | loss: 0.78206 | eval_custom_logloss: 0.8674  |  0:06:02s
epoch 24 | loss: 0.78186 | eval_custom_logloss: 0.90856 |  0:06:17s
epoch 25 | loss: 0.77629 | eval_custom_logloss: 0.77953 |  0:06:32s
epoch 26 | loss: 0.77072 | eval_custom_logloss: 0.8727  |  0:06:47s
epoch 27 | loss: 0.77358 | eval_custom_logloss: 0.75949 |  0:07:01s
epoch 28 | loss: 0.76997 | eval_custom_logloss: 0.97985 |  0:07:16s
epoch 29 | loss: 0.74324 | eval_custom_logloss: 1.84435 |  0:07:32s
epoch 30 | loss: 0.76173 | eval_custom_logloss: 0.90712 |  0:07:46s
epoch 31 | loss: 0.73533 | eval_custom_logloss: 1.11978 |  0:08:01s
epoch 32 | loss: 0.73045 | eval_custom_logloss: 1.01065 |  0:08:16s
epoch 33 | loss: 0.73573 | eval_custom_logloss: 1.68788 |  0:08:31s
epoch 34 | loss: 0.73307 | eval_custom_logloss: 0.94993 |  0:08:46s
epoch 35 | loss: 0.7215  | eval_custom_logloss: 0.89828 |  0:09:01s
epoch 36 | loss: 0.71678 | eval_custom_logloss: 1.04747 |  0:09:16s
epoch 37 | loss: 0.72657 | eval_custom_logloss: 1.04043 |  0:09:32s
epoch 38 | loss: 0.70972 | eval_custom_logloss: 0.74582 |  0:09:49s
epoch 39 | loss: 0.71469 | eval_custom_logloss: 0.86251 |  0:10:04s
epoch 40 | loss: 0.71132 | eval_custom_logloss: 2.26326 |  0:10:19s
epoch 41 | loss: 0.69375 | eval_custom_logloss: 1.97239 |  0:10:34s
epoch 42 | loss: 0.70973 | eval_custom_logloss: 1.27041 |  0:10:49s
epoch 43 | loss: 0.70362 | eval_custom_logloss: 0.71945 |  0:11:04s
epoch 44 | loss: 0.68803 | eval_custom_logloss: 0.87584 |  0:11:19s
epoch 45 | loss: 0.69702 | eval_custom_logloss: 0.65599 |  0:11:34s
epoch 46 | loss: 0.679   | eval_custom_logloss: 0.7421  |  0:11:49s
epoch 47 | loss: 0.68775 | eval_custom_logloss: 0.77186 |  0:12:04s
epoch 48 | loss: 0.68052 | eval_custom_logloss: 0.85904 |  0:12:18s
epoch 49 | loss: 0.68166 | eval_custom_logloss: 0.75133 |  0:12:33s
epoch 50 | loss: 0.67675 | eval_custom_logloss: 0.68986 |  0:12:48s
epoch 51 | loss: 0.67395 | eval_custom_logloss: 0.86852 |  0:13:03s
epoch 52 | loss: 0.68574 | eval_custom_logloss: 0.69662 |  0:13:18s
epoch 53 | loss: 0.67692 | eval_custom_logloss: 1.17832 |  0:13:33s
epoch 54 | loss: 0.68482 | eval_custom_logloss: 0.64431 |  0:13:50s
epoch 55 | loss: 0.66988 | eval_custom_logloss: 0.87909 |  0:14:05s
epoch 56 | loss: 0.67041 | eval_custom_logloss: 0.75047 |  0:14:20s
epoch 57 | loss: 0.66679 | eval_custom_logloss: 0.65186 |  0:14:35s
epoch 58 | loss: 0.67024 | eval_custom_logloss: 0.76732 |  0:14:50s
epoch 59 | loss: 0.65782 | eval_custom_logloss: 0.706   |  0:15:05s
epoch 60 | loss: 0.67651 | eval_custom_logloss: 0.73612 |  0:15:20s
epoch 61 | loss: 0.65889 | eval_custom_logloss: 0.65379 |  0:15:36s
epoch 62 | loss: 0.66006 | eval_custom_logloss: 0.80631 |  0:15:51s
epoch 63 | loss: 0.65923 | eval_custom_logloss: 0.73122 |  0:16:06s
epoch 64 | loss: 0.6685  | eval_custom_logloss: 1.31662 |  0:16:22s
epoch 65 | loss: 0.65583 | eval_custom_logloss: 3.4949  |  0:16:37s
epoch 66 | loss: 0.66533 | eval_custom_logloss: 0.64715 |  0:16:51s
epoch 67 | loss: 0.66579 | eval_custom_logloss: 0.89999 |  0:17:06s
epoch 68 | loss: 0.64993 | eval_custom_logloss: 1.15576 |  0:17:22s
epoch 69 | loss: 0.65693 | eval_custom_logloss: 1.18998 |  0:17:37s
epoch 70 | loss: 0.66628 | eval_custom_logloss: 1.19299 |  0:17:52s
epoch 71 | loss: 0.65049 | eval_custom_logloss: 0.8547  |  0:18:07s
epoch 72 | loss: 0.65608 | eval_custom_logloss: 0.66313 |  0:18:23s
epoch 73 | loss: 0.65805 | eval_custom_logloss: 0.68774 |  0:18:37s
epoch 74 | loss: 0.65253 | eval_custom_logloss: 1.19503 |  0:18:53s

Early stopping occurred at epoch 74 with best_epoch = 54 and best_eval_custom_logloss = 0.64431
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6436, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.50561 | eval_custom_logloss: 2.74612 |  0:00:15s
epoch 1  | loss: 1.12677 | eval_custom_logloss: 1.61608 |  0:00:30s
epoch 2  | loss: 1.01293 | eval_custom_logloss: 1.69181 |  0:00:47s
epoch 3  | loss: 0.9829  | eval_custom_logloss: 1.37539 |  0:01:01s
epoch 4  | loss: 0.92994 | eval_custom_logloss: 0.93014 |  0:01:16s
epoch 5  | loss: 0.91506 | eval_custom_logloss: 0.99777 |  0:01:30s
epoch 6  | loss: 0.906   | eval_custom_logloss: 1.01947 |  0:01:46s
epoch 7  | loss: 0.89517 | eval_custom_logloss: 0.86209 |  0:02:02s
epoch 8  | loss: 0.87979 | eval_custom_logloss: 1.20877 |  0:02:17s
epoch 9  | loss: 0.86033 | eval_custom_logloss: 0.87279 |  0:02:32s
epoch 10 | loss: 0.86604 | eval_custom_logloss: 1.05312 |  0:02:48s
epoch 11 | loss: 0.85644 | eval_custom_logloss: 1.19542 |  0:03:03s
epoch 12 | loss: 0.8567  | eval_custom_logloss: 0.90907 |  0:03:18s
epoch 13 | loss: 0.85906 | eval_custom_logloss: 1.11055 |  0:03:33s
epoch 14 | loss: 0.8415  | eval_custom_logloss: 0.94256 |  0:03:48s
epoch 15 | loss: 0.83122 | eval_custom_logloss: 0.88749 |  0:04:04s
epoch 16 | loss: 0.83247 | eval_custom_logloss: 0.8563  |  0:04:19s
epoch 17 | loss: 0.81289 | eval_custom_logloss: 0.93618 |  0:04:34s
epoch 18 | loss: 0.81339 | eval_custom_logloss: 0.84843 |  0:04:49s
epoch 19 | loss: 0.8146  | eval_custom_logloss: 1.16839 |  0:05:03s
epoch 20 | loss: 0.80223 | eval_custom_logloss: 0.74499 |  0:05:18s
epoch 21 | loss: 0.81557 | eval_custom_logloss: 1.07208 |  0:05:33s
epoch 22 | loss: 0.79803 | eval_custom_logloss: 0.71092 |  0:05:49s
epoch 23 | loss: 0.78899 | eval_custom_logloss: 0.82721 |  0:06:06s
epoch 24 | loss: 0.77663 | eval_custom_logloss: 0.716   |  0:06:23s
epoch 25 | loss: 0.77408 | eval_custom_logloss: 0.67358 |  0:06:38s
epoch 26 | loss: 0.773   | eval_custom_logloss: 1.62142 |  0:06:53s
epoch 27 | loss: 0.76997 | eval_custom_logloss: 0.80164 |  0:07:08s
epoch 28 | loss: 0.78279 | eval_custom_logloss: 0.82473 |  0:07:24s
epoch 29 | loss: 0.7591  | eval_custom_logloss: 0.70117 |  0:07:40s
epoch 30 | loss: 0.77009 | eval_custom_logloss: 0.6966  |  0:07:55s
epoch 31 | loss: 0.76141 | eval_custom_logloss: 0.73812 |  0:08:12s
epoch 32 | loss: 0.768   | eval_custom_logloss: 0.67051 |  0:08:27s
epoch 33 | loss: 0.75154 | eval_custom_logloss: 0.73896 |  0:08:42s
epoch 34 | loss: 0.76847 | eval_custom_logloss: 0.6949  |  0:08:57s
epoch 35 | loss: 0.75992 | eval_custom_logloss: 0.73055 |  0:09:12s
epoch 36 | loss: 0.75053 | eval_custom_logloss: 0.90274 |  0:09:27s
epoch 37 | loss: 0.77178 | eval_custom_logloss: 0.86537 |  0:09:42s
epoch 38 | loss: 0.73997 | eval_custom_logloss: 0.74599 |  0:09:57s
epoch 39 | loss: 0.74141 | eval_custom_logloss: 0.9165  |  0:10:13s
epoch 40 | loss: 0.73108 | eval_custom_logloss: 0.77393 |  0:10:28s
epoch 41 | loss: 0.7349  | eval_custom_logloss: 0.70508 |  0:10:45s
epoch 42 | loss: 0.73809 | eval_custom_logloss: 0.70327 |  0:11:00s
epoch 43 | loss: 0.7447  | eval_custom_logloss: 1.46297 |  0:11:16s
epoch 44 | loss: 0.73996 | eval_custom_logloss: 0.91071 |  0:11:32s
epoch 45 | loss: 0.73699 | eval_custom_logloss: 0.81432 |  0:11:48s
epoch 46 | loss: 0.73515 | eval_custom_logloss: 0.92574 |  0:12:05s
epoch 47 | loss: 0.72447 | eval_custom_logloss: 0.77884 |  0:12:21s
epoch 48 | loss: 0.73511 | eval_custom_logloss: 1.6701  |  0:12:35s
epoch 49 | loss: 0.7351  | eval_custom_logloss: 0.71931 |  0:12:51s
epoch 50 | loss: 0.71988 | eval_custom_logloss: 0.70146 |  0:13:07s
epoch 51 | loss: 0.72661 | eval_custom_logloss: 1.40253 |  0:13:22s
epoch 52 | loss: 0.72368 | eval_custom_logloss: 0.68545 |  0:13:37s

Early stopping occurred at epoch 52 with best_epoch = 32 and best_eval_custom_logloss = 0.67051
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6565, 'Log Loss - std': 0.012900000000000023} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.5358  | eval_custom_logloss: 5.89493 |  0:00:16s
epoch 1  | loss: 1.10966 | eval_custom_logloss: 2.796   |  0:00:32s
epoch 2  | loss: 1.02305 | eval_custom_logloss: 2.45822 |  0:00:47s
epoch 3  | loss: 0.97546 | eval_custom_logloss: 1.02384 |  0:01:03s
epoch 4  | loss: 0.94187 | eval_custom_logloss: 1.28057 |  0:01:18s
epoch 5  | loss: 0.91947 | eval_custom_logloss: 0.98961 |  0:01:34s
epoch 6  | loss: 0.89838 | eval_custom_logloss: 0.84338 |  0:01:51s
epoch 7  | loss: 0.88499 | eval_custom_logloss: 1.36331 |  0:02:07s
epoch 8  | loss: 0.86906 | eval_custom_logloss: 0.97503 |  0:02:22s
epoch 9  | loss: 0.88656 | eval_custom_logloss: 1.40452 |  0:02:37s
epoch 10 | loss: 0.8682  | eval_custom_logloss: 4.27261 |  0:02:52s
epoch 11 | loss: 0.84268 | eval_custom_logloss: 1.14488 |  0:03:07s
epoch 12 | loss: 0.83127 | eval_custom_logloss: 1.08662 |  0:03:22s
epoch 13 | loss: 0.81683 | eval_custom_logloss: 1.63573 |  0:03:37s
epoch 14 | loss: 0.80696 | eval_custom_logloss: 0.98022 |  0:03:52s
epoch 15 | loss: 0.79258 | eval_custom_logloss: 1.57524 |  0:04:08s
epoch 16 | loss: 0.79317 | eval_custom_logloss: 1.38622 |  0:04:23s
epoch 17 | loss: 0.76824 | eval_custom_logloss: 1.24777 |  0:04:38s
epoch 18 | loss: 0.75592 | eval_custom_logloss: 0.88914 |  0:04:53s
epoch 19 | loss: 0.75913 | eval_custom_logloss: 0.93546 |  0:05:09s
epoch 20 | loss: 0.75072 | eval_custom_logloss: 0.91846 |  0:05:24s
epoch 21 | loss: 0.74279 | eval_custom_logloss: 1.08685 |  0:05:40s
epoch 22 | loss: 0.72849 | eval_custom_logloss: 0.90155 |  0:05:55s
epoch 23 | loss: 0.73811 | eval_custom_logloss: 0.71927 |  0:06:10s
epoch 24 | loss: 0.71851 | eval_custom_logloss: 0.74248 |  0:06:26s
epoch 25 | loss: 0.72515 | eval_custom_logloss: 0.91916 |  0:06:42s
epoch 26 | loss: 0.72182 | eval_custom_logloss: 1.06061 |  0:06:57s
epoch 27 | loss: 0.71934 | eval_custom_logloss: 0.93001 |  0:07:12s
epoch 28 | loss: 0.71963 | eval_custom_logloss: 1.67669 |  0:07:29s
epoch 29 | loss: 0.71164 | eval_custom_logloss: 0.90568 |  0:07:44s
epoch 30 | loss: 0.72202 | eval_custom_logloss: 0.87051 |  0:07:59s
epoch 31 | loss: 0.70811 | eval_custom_logloss: 0.86738 |  0:08:14s
epoch 32 | loss: 0.69872 | eval_custom_logloss: 0.72118 |  0:08:29s
epoch 33 | loss: 0.69538 | eval_custom_logloss: 0.80581 |  0:08:43s
epoch 34 | loss: 0.70766 | eval_custom_logloss: 2.09729 |  0:08:58s
epoch 35 | loss: 0.69279 | eval_custom_logloss: 0.80635 |  0:09:13s
epoch 36 | loss: 0.68491 | eval_custom_logloss: 0.72296 |  0:09:29s
epoch 37 | loss: 0.69769 | eval_custom_logloss: 0.86984 |  0:09:44s
epoch 38 | loss: 0.68665 | eval_custom_logloss: 0.80943 |  0:09:58s
epoch 39 | loss: 0.70672 | eval_custom_logloss: 1.24659 |  0:10:13s
epoch 40 | loss: 0.68379 | eval_custom_logloss: 0.82439 |  0:10:28s
epoch 41 | loss: 0.67883 | eval_custom_logloss: 0.93307 |  0:10:42s
epoch 42 | loss: 0.67622 | eval_custom_logloss: 1.07344 |  0:10:57s
epoch 43 | loss: 0.69342 | eval_custom_logloss: 0.89192 |  0:11:13s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.71927
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6767666666666666, 'Log Loss - std': 0.030535480273864334} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.54298 | eval_custom_logloss: 3.48418 |  0:00:16s
epoch 1  | loss: 1.12395 | eval_custom_logloss: 1.96785 |  0:00:32s
epoch 2  | loss: 1.02095 | eval_custom_logloss: 1.17889 |  0:00:47s
epoch 3  | loss: 0.97869 | eval_custom_logloss: 1.57171 |  0:01:02s
epoch 4  | loss: 0.96136 | eval_custom_logloss: 1.11223 |  0:01:17s
epoch 5  | loss: 0.9101  | eval_custom_logloss: 1.28789 |  0:01:32s
epoch 6  | loss: 0.85658 | eval_custom_logloss: 1.04939 |  0:01:48s
epoch 7  | loss: 0.85114 | eval_custom_logloss: 1.24683 |  0:02:03s
epoch 8  | loss: 0.83784 | eval_custom_logloss: 1.42896 |  0:02:18s
epoch 9  | loss: 0.83707 | eval_custom_logloss: 1.03429 |  0:02:33s
epoch 10 | loss: 0.83623 | eval_custom_logloss: 0.7453  |  0:02:48s
epoch 11 | loss: 0.82574 | eval_custom_logloss: 0.84951 |  0:03:03s
epoch 12 | loss: 0.82519 | eval_custom_logloss: 0.79875 |  0:03:18s
epoch 13 | loss: 0.81367 | eval_custom_logloss: 1.05073 |  0:03:33s
epoch 14 | loss: 0.80902 | eval_custom_logloss: 1.05656 |  0:03:48s
epoch 15 | loss: 0.78025 | eval_custom_logloss: 1.03243 |  0:04:03s
epoch 16 | loss: 0.79088 | eval_custom_logloss: 1.67201 |  0:04:18s
epoch 17 | loss: 0.78185 | eval_custom_logloss: 1.20562 |  0:04:32s
epoch 18 | loss: 0.76326 | eval_custom_logloss: 0.87145 |  0:04:47s
epoch 19 | loss: 0.77778 | eval_custom_logloss: 0.87867 |  0:05:02s
epoch 20 | loss: 0.7507  | eval_custom_logloss: 0.85917 |  0:05:17s
epoch 21 | loss: 0.75601 | eval_custom_logloss: 1.1011  |  0:05:32s
epoch 22 | loss: 0.74803 | eval_custom_logloss: 0.994   |  0:05:47s
epoch 23 | loss: 0.74001 | eval_custom_logloss: 0.842   |  0:06:01s
epoch 24 | loss: 0.73164 | eval_custom_logloss: 0.90036 |  0:06:16s
epoch 25 | loss: 0.73356 | eval_custom_logloss: 0.90947 |  0:06:32s
epoch 26 | loss: 0.7415  | eval_custom_logloss: 1.0997  |  0:06:48s
epoch 27 | loss: 0.74133 | eval_custom_logloss: 0.78262 |  0:07:03s
epoch 28 | loss: 0.72159 | eval_custom_logloss: 1.24699 |  0:07:18s
epoch 29 | loss: 0.71784 | eval_custom_logloss: 0.80587 |  0:07:33s
epoch 30 | loss: 0.73681 | eval_custom_logloss: 1.19195 |  0:07:48s

Early stopping occurred at epoch 30 with best_epoch = 10 and best_eval_custom_logloss = 0.7453
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.693825, 'Log Loss - std': 0.039651883624867074} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.52843 | eval_custom_logloss: 3.8694  |  0:00:16s
epoch 1  | loss: 1.11251 | eval_custom_logloss: 2.72294 |  0:00:31s
epoch 2  | loss: 1.02779 | eval_custom_logloss: 2.63957 |  0:00:47s
epoch 3  | loss: 0.98938 | eval_custom_logloss: 1.31507 |  0:01:03s
epoch 4  | loss: 0.94404 | eval_custom_logloss: 1.59479 |  0:01:18s
epoch 5  | loss: 0.94233 | eval_custom_logloss: 0.94064 |  0:01:33s
epoch 6  | loss: 0.92883 | eval_custom_logloss: 2.04032 |  0:01:49s
epoch 7  | loss: 0.89799 | eval_custom_logloss: 1.27134 |  0:02:04s
epoch 8  | loss: 0.88116 | eval_custom_logloss: 1.28987 |  0:02:19s
epoch 9  | loss: 0.87595 | eval_custom_logloss: 1.12864 |  0:02:34s
epoch 10 | loss: 0.87382 | eval_custom_logloss: 1.40348 |  0:02:49s
epoch 11 | loss: 0.87815 | eval_custom_logloss: 0.82803 |  0:03:04s
epoch 12 | loss: 0.87387 | eval_custom_logloss: 1.19504 |  0:03:19s
epoch 13 | loss: 0.86741 | eval_custom_logloss: 0.88395 |  0:03:34s
epoch 14 | loss: 0.86219 | eval_custom_logloss: 0.90129 |  0:03:51s
epoch 15 | loss: 0.84542 | eval_custom_logloss: 0.77513 |  0:04:06s
epoch 16 | loss: 0.84083 | eval_custom_logloss: 1.77289 |  0:04:22s
epoch 17 | loss: 0.83352 | eval_custom_logloss: 0.76861 |  0:04:37s
epoch 18 | loss: 0.84645 | eval_custom_logloss: 0.83794 |  0:04:52s
epoch 19 | loss: 0.84432 | eval_custom_logloss: 0.85934 |  0:05:07s
epoch 20 | loss: 0.81914 | eval_custom_logloss: 0.87219 |  0:05:22s
epoch 21 | loss: 0.81976 | eval_custom_logloss: 1.10787 |  0:05:38s
epoch 22 | loss: 0.79659 | eval_custom_logloss: 1.20399 |  0:05:53s
epoch 23 | loss: 0.79694 | eval_custom_logloss: 1.16666 |  0:06:08s
epoch 24 | loss: 0.79409 | eval_custom_logloss: 1.10155 |  0:06:23s
epoch 25 | loss: 0.76642 | eval_custom_logloss: 0.90346 |  0:06:38s
epoch 26 | loss: 0.76766 | eval_custom_logloss: 0.79051 |  0:06:53s
epoch 27 | loss: 0.76838 | eval_custom_logloss: 0.7434  |  0:07:08s
epoch 28 | loss: 0.76899 | eval_custom_logloss: 0.99881 |  0:07:23s
epoch 29 | loss: 0.75819 | eval_custom_logloss: 0.85035 |  0:07:38s
epoch 30 | loss: 0.76362 | eval_custom_logloss: 1.15517 |  0:07:53s
epoch 31 | loss: 0.74793 | eval_custom_logloss: 0.87785 |  0:08:08s
epoch 32 | loss: 0.75088 | eval_custom_logloss: 1.39824 |  0:08:23s
epoch 33 | loss: 0.74415 | eval_custom_logloss: 0.93264 |  0:08:38s
epoch 34 | loss: 0.75693 | eval_custom_logloss: 0.76041 |  0:08:53s
epoch 35 | loss: 0.71795 | eval_custom_logloss: 0.92159 |  0:09:08s
epoch 36 | loss: 0.73664 | eval_custom_logloss: 0.83416 |  0:09:23s
epoch 37 | loss: 0.73365 | eval_custom_logloss: 0.82078 |  0:09:39s
epoch 38 | loss: 0.72851 | eval_custom_logloss: 0.69833 |  0:09:54s
epoch 39 | loss: 0.7322  | eval_custom_logloss: 0.69505 |  0:10:09s
epoch 40 | loss: 0.7511  | eval_custom_logloss: 0.89091 |  0:10:25s
epoch 41 | loss: 0.72561 | eval_custom_logloss: 0.91678 |  0:10:40s
epoch 42 | loss: 0.73081 | eval_custom_logloss: 0.93484 |  0:10:55s
epoch 43 | loss: 0.72976 | eval_custom_logloss: 1.18665 |  0:11:10s
epoch 44 | loss: 0.73245 | eval_custom_logloss: 0.89538 |  0:11:27s
epoch 45 | loss: 0.71757 | eval_custom_logloss: 0.92714 |  0:11:42s
epoch 46 | loss: 0.72225 | eval_custom_logloss: 0.82478 |  0:11:57s
epoch 47 | loss: 0.72983 | eval_custom_logloss: 0.90339 |  0:12:12s
epoch 48 | loss: 0.71315 | eval_custom_logloss: 1.23178 |  0:12:29s
epoch 49 | loss: 0.728   | eval_custom_logloss: 0.77138 |  0:12:45s
epoch 50 | loss: 0.70696 | eval_custom_logloss: 0.70611 |  0:13:00s
epoch 51 | loss: 0.71106 | eval_custom_logloss: 0.69235 |  0:13:15s
epoch 52 | loss: 0.7056  | eval_custom_logloss: 0.81906 |  0:13:31s
epoch 53 | loss: 0.7033  | eval_custom_logloss: 0.87548 |  0:13:46s
epoch 54 | loss: 0.71819 | eval_custom_logloss: 1.06979 |  0:14:01s
epoch 55 | loss: 0.70691 | eval_custom_logloss: 1.23811 |  0:14:17s
epoch 56 | loss: 0.68944 | eval_custom_logloss: 0.75734 |  0:14:33s
epoch 57 | loss: 0.70273 | eval_custom_logloss: 0.81955 |  0:14:50s
epoch 58 | loss: 0.69982 | eval_custom_logloss: 0.9483  |  0:15:05s
epoch 59 | loss: 0.69931 | eval_custom_logloss: 1.21358 |  0:15:21s
epoch 60 | loss: 0.69049 | eval_custom_logloss: 0.6583  |  0:15:37s
epoch 61 | loss: 0.70625 | eval_custom_logloss: 1.21163 |  0:15:52s
epoch 62 | loss: 0.69714 | eval_custom_logloss: 0.83453 |  0:16:08s
epoch 63 | loss: 0.69641 | eval_custom_logloss: 0.9924  |  0:16:23s
epoch 64 | loss: 0.68665 | eval_custom_logloss: 0.78101 |  0:16:38s
epoch 65 | loss: 0.7034  | eval_custom_logloss: 0.819   |  0:16:54s
epoch 66 | loss: 0.69332 | eval_custom_logloss: 0.89755 |  0:17:09s
epoch 67 | loss: 0.69975 | eval_custom_logloss: 0.79859 |  0:17:24s
epoch 68 | loss: 0.69258 | eval_custom_logloss: 1.13481 |  0:17:41s
epoch 69 | loss: 0.69678 | eval_custom_logloss: 0.89011 |  0:17:56s
epoch 70 | loss: 0.70982 | eval_custom_logloss: 0.83718 |  0:18:11s
epoch 71 | loss: 0.68373 | eval_custom_logloss: 0.92638 |  0:18:27s
epoch 72 | loss: 0.68766 | eval_custom_logloss: 0.93308 |  0:18:42s
epoch 73 | loss: 0.69166 | eval_custom_logloss: 1.10315 |  0:18:57s
epoch 74 | loss: 0.69023 | eval_custom_logloss: 1.2053  |  0:19:13s
epoch 75 | loss: 0.66925 | eval_custom_logloss: 0.63964 |  0:19:30s
epoch 76 | loss: 0.68004 | eval_custom_logloss: 0.82498 |  0:19:45s
epoch 77 | loss: 0.68401 | eval_custom_logloss: 0.83518 |  0:20:02s
epoch 78 | loss: 0.67917 | eval_custom_logloss: 1.00712 |  0:20:17s
epoch 79 | loss: 0.67344 | eval_custom_logloss: 0.92456 |  0:20:33s
epoch 80 | loss: 0.68146 | eval_custom_logloss: 0.84968 |  0:20:47s
epoch 81 | loss: 0.6723  | eval_custom_logloss: 1.18721 |  0:21:01s
epoch 82 | loss: 0.67596 | eval_custom_logloss: 0.71055 |  0:21:15s
epoch 83 | loss: 0.67639 | eval_custom_logloss: 0.96945 |  0:21:30s
epoch 84 | loss: 0.67499 | eval_custom_logloss: 1.15498 |  0:21:45s
epoch 85 | loss: 0.67575 | eval_custom_logloss: 1.12249 |  0:22:00s
epoch 86 | loss: 0.66292 | eval_custom_logloss: 0.81272 |  0:22:15s
epoch 87 | loss: 0.68623 | eval_custom_logloss: 1.04876 |  0:22:31s
epoch 88 | loss: 0.67516 | eval_custom_logloss: 0.75202 |  0:22:47s
epoch 89 | loss: 0.67258 | eval_custom_logloss: 0.68974 |  0:23:02s
epoch 90 | loss: 0.66985 | eval_custom_logloss: 1.35027 |  0:23:17s
epoch 91 | loss: 0.68154 | eval_custom_logloss: 1.18937 |  0:23:32s
epoch 92 | loss: 0.68435 | eval_custom_logloss: 0.77267 |  0:23:48s
epoch 93 | loss: 0.67668 | eval_custom_logloss: 0.68376 |  0:24:04s
epoch 94 | loss: 0.68256 | eval_custom_logloss: 1.13261 |  0:24:20s
epoch 95 | loss: 0.68807 | eval_custom_logloss: 0.9041  |  0:24:35s

Early stopping occurred at epoch 95 with best_epoch = 75 and best_eval_custom_logloss = 0.63964
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6826800000000001, 'Log Loss - std': 0.04188868104870338} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 0.6826800000000001 and parameters: {'n_d': 12, 'n_steps': 5, 'gamma': 1.263859320313395, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.005078815902864435, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.41902 | eval_custom_logloss: 2.51616 |  0:00:11s
epoch 1  | loss: 1.07009 | eval_custom_logloss: 1.0307  |  0:00:23s
epoch 2  | loss: 0.96799 | eval_custom_logloss: 0.95424 |  0:00:35s
epoch 3  | loss: 0.95393 | eval_custom_logloss: 1.00904 |  0:00:47s
epoch 4  | loss: 0.94305 | eval_custom_logloss: 0.82809 |  0:00:58s
epoch 5  | loss: 0.90969 | eval_custom_logloss: 0.77735 |  0:01:10s
epoch 6  | loss: 0.87947 | eval_custom_logloss: 1.43889 |  0:01:22s
epoch 7  | loss: 0.85599 | eval_custom_logloss: 0.74692 |  0:01:34s
epoch 8  | loss: 0.83367 | eval_custom_logloss: 0.90404 |  0:01:43s
epoch 9  | loss: 0.83388 | eval_custom_logloss: 1.01527 |  0:01:53s
epoch 10 | loss: 0.83742 | eval_custom_logloss: 1.11538 |  0:02:05s
epoch 11 | loss: 0.79521 | eval_custom_logloss: 0.79461 |  0:02:16s
epoch 12 | loss: 0.79298 | eval_custom_logloss: 0.77368 |  0:02:28s
epoch 13 | loss: 0.78506 | eval_custom_logloss: 0.70316 |  0:02:40s
epoch 14 | loss: 0.77223 | eval_custom_logloss: 0.83922 |  0:02:51s
epoch 15 | loss: 0.77107 | eval_custom_logloss: 0.91096 |  0:03:03s
epoch 16 | loss: 0.76464 | eval_custom_logloss: 0.80969 |  0:03:14s
epoch 17 | loss: 0.75231 | eval_custom_logloss: 0.66223 |  0:03:26s
epoch 18 | loss: 0.73606 | eval_custom_logloss: 0.66977 |  0:03:36s
epoch 19 | loss: 0.72175 | eval_custom_logloss: 0.75514 |  0:03:48s
epoch 20 | loss: 0.72827 | eval_custom_logloss: 0.82683 |  0:03:59s
epoch 21 | loss: 0.72634 | eval_custom_logloss: 0.99633 |  0:04:11s
epoch 22 | loss: 0.70524 | eval_custom_logloss: 0.64598 |  0:04:22s
epoch 23 | loss: 0.72277 | eval_custom_logloss: 0.99537 |  0:04:34s
epoch 24 | loss: 0.70511 | eval_custom_logloss: 0.88983 |  0:04:46s
epoch 25 | loss: 0.70685 | eval_custom_logloss: 0.6806  |  0:04:58s
epoch 26 | loss: 0.70335 | eval_custom_logloss: 0.99639 |  0:05:09s
epoch 27 | loss: 0.69811 | eval_custom_logloss: 0.64876 |  0:05:21s
epoch 28 | loss: 0.68854 | eval_custom_logloss: 0.65667 |  0:05:33s
epoch 29 | loss: 0.68137 | eval_custom_logloss: 0.73012 |  0:05:44s
epoch 30 | loss: 0.67529 | eval_custom_logloss: 0.79468 |  0:05:55s
epoch 31 | loss: 0.67911 | eval_custom_logloss: 0.85261 |  0:06:07s
epoch 32 | loss: 0.67493 | eval_custom_logloss: 0.612   |  0:06:19s
epoch 33 | loss: 0.67239 | eval_custom_logloss: 0.67433 |  0:06:31s
epoch 34 | loss: 0.67294 | eval_custom_logloss: 0.91771 |  0:06:42s
epoch 35 | loss: 0.66244 | eval_custom_logloss: 0.77411 |  0:06:54s
epoch 36 | loss: 0.67673 | eval_custom_logloss: 0.72625 |  0:07:06s
epoch 37 | loss: 0.65952 | eval_custom_logloss: 0.90796 |  0:07:17s
epoch 38 | loss: 0.66341 | eval_custom_logloss: 0.88286 |  0:07:29s
epoch 39 | loss: 0.66294 | eval_custom_logloss: 0.6169  |  0:07:40s
epoch 40 | loss: 0.6515  | eval_custom_logloss: 0.71076 |  0:07:52s
epoch 41 | loss: 0.65147 | eval_custom_logloss: 0.61871 |  0:08:03s
epoch 42 | loss: 0.65783 | eval_custom_logloss: 0.83473 |  0:08:15s
epoch 43 | loss: 0.65613 | eval_custom_logloss: 0.88944 |  0:08:27s
epoch 44 | loss: 0.65597 | eval_custom_logloss: 0.76309 |  0:08:38s
epoch 45 | loss: 0.66294 | eval_custom_logloss: 0.84202 |  0:08:47s
epoch 46 | loss: 0.6386  | eval_custom_logloss: 0.73516 |  0:08:58s
epoch 47 | loss: 0.64136 | eval_custom_logloss: 0.83159 |  0:09:10s
epoch 48 | loss: 0.66094 | eval_custom_logloss: 0.83939 |  0:09:21s
epoch 49 | loss: 0.65614 | eval_custom_logloss: 0.71217 |  0:09:33s
epoch 50 | loss: 0.66438 | eval_custom_logloss: 0.79733 |  0:09:45s
epoch 51 | loss: 0.64757 | eval_custom_logloss: 1.26487 |  0:09:56s
epoch 52 | loss: 0.63604 | eval_custom_logloss: 0.59883 |  0:10:08s
epoch 53 | loss: 0.64311 | eval_custom_logloss: 0.64397 |  0:10:20s
epoch 54 | loss: 0.63154 | eval_custom_logloss: 0.79787 |  0:10:31s
epoch 55 | loss: 0.64934 | eval_custom_logloss: 0.85325 |  0:10:42s
epoch 56 | loss: 0.63882 | eval_custom_logloss: 0.60402 |  0:10:53s
epoch 57 | loss: 0.64857 | eval_custom_logloss: 0.69202 |  0:11:05s
epoch 58 | loss: 0.63692 | eval_custom_logloss: 1.09244 |  0:11:16s
epoch 59 | loss: 0.63771 | eval_custom_logloss: 0.65277 |  0:11:27s
epoch 60 | loss: 0.63328 | eval_custom_logloss: 0.73222 |  0:11:38s
epoch 61 | loss: 0.6587  | eval_custom_logloss: 0.78982 |  0:11:50s
epoch 62 | loss: 0.6448  | eval_custom_logloss: 0.65284 |  0:12:02s
epoch 63 | loss: 0.65591 | eval_custom_logloss: 0.76891 |  0:12:13s
epoch 64 | loss: 0.62618 | eval_custom_logloss: 0.66903 |  0:12:25s
epoch 65 | loss: 0.61432 | eval_custom_logloss: 1.13507 |  0:12:37s
epoch 66 | loss: 0.62038 | eval_custom_logloss: 0.70095 |  0:12:49s
epoch 67 | loss: 0.63069 | eval_custom_logloss: 0.61737 |  0:13:00s
epoch 68 | loss: 0.62226 | eval_custom_logloss: 0.68956 |  0:13:11s
epoch 69 | loss: 0.62199 | eval_custom_logloss: 0.63195 |  0:13:23s
epoch 70 | loss: 0.61761 | eval_custom_logloss: 0.7303  |  0:13:34s
epoch 71 | loss: 0.62621 | eval_custom_logloss: 0.65312 |  0:13:46s
epoch 72 | loss: 0.62311 | eval_custom_logloss: 0.59197 |  0:13:58s
epoch 73 | loss: 0.63302 | eval_custom_logloss: 0.63771 |  0:14:09s
epoch 74 | loss: 0.61941 | eval_custom_logloss: 1.07964 |  0:14:21s
epoch 75 | loss: 0.6219  | eval_custom_logloss: 1.0269  |  0:14:32s
epoch 76 | loss: 0.61916 | eval_custom_logloss: 0.762   |  0:14:42s
epoch 77 | loss: 0.61226 | eval_custom_logloss: 0.70094 |  0:14:54s
epoch 78 | loss: 0.61031 | eval_custom_logloss: 0.68162 |  0:15:05s
epoch 79 | loss: 0.61999 | eval_custom_logloss: 0.76231 |  0:15:17s
epoch 80 | loss: 0.61235 | eval_custom_logloss: 0.82343 |  0:15:28s
epoch 81 | loss: 0.61906 | eval_custom_logloss: 0.61725 |  0:15:40s
epoch 82 | loss: 0.61686 | eval_custom_logloss: 0.72883 |  0:15:52s
epoch 83 | loss: 0.60492 | eval_custom_logloss: 0.76143 |  0:16:03s
epoch 84 | loss: 0.61069 | eval_custom_logloss: 0.73536 |  0:16:15s
epoch 85 | loss: 0.61751 | eval_custom_logloss: 0.8847  |  0:16:26s
epoch 86 | loss: 0.6204  | eval_custom_logloss: 0.65515 |  0:16:37s
epoch 87 | loss: 0.6128  | eval_custom_logloss: 0.7332  |  0:16:47s
epoch 88 | loss: 0.60508 | eval_custom_logloss: 0.68331 |  0:16:59s
epoch 89 | loss: 0.60485 | eval_custom_logloss: 0.61366 |  0:17:10s
epoch 90 | loss: 0.61094 | eval_custom_logloss: 0.82282 |  0:17:21s
epoch 91 | loss: 0.59836 | eval_custom_logloss: 0.65385 |  0:17:33s
epoch 92 | loss: 0.61164 | eval_custom_logloss: 0.64955 |  0:17:44s

Early stopping occurred at epoch 92 with best_epoch = 72 and best_eval_custom_logloss = 0.59197
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5899, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.3826  | eval_custom_logloss: 0.96919 |  0:00:11s
epoch 1  | loss: 1.01293 | eval_custom_logloss: 0.8618  |  0:00:23s
epoch 2  | loss: 0.93098 | eval_custom_logloss: 0.8196  |  0:00:35s
epoch 3  | loss: 0.89376 | eval_custom_logloss: 0.84827 |  0:00:46s
epoch 4  | loss: 0.88001 | eval_custom_logloss: 0.75067 |  0:00:57s
epoch 5  | loss: 0.86539 | eval_custom_logloss: 0.82012 |  0:01:09s
epoch 6  | loss: 0.82363 | eval_custom_logloss: 0.93011 |  0:01:20s
epoch 7  | loss: 0.81689 | eval_custom_logloss: 1.36063 |  0:01:31s
epoch 8  | loss: 0.79603 | eval_custom_logloss: 1.36396 |  0:01:43s
epoch 9  | loss: 0.79276 | eval_custom_logloss: 1.68599 |  0:01:55s
epoch 10 | loss: 0.77598 | eval_custom_logloss: 0.70784 |  0:02:07s
epoch 11 | loss: 0.75966 | eval_custom_logloss: 0.89363 |  0:02:18s
epoch 12 | loss: 0.74011 | eval_custom_logloss: 0.68632 |  0:02:30s
epoch 13 | loss: 0.73956 | eval_custom_logloss: 0.78515 |  0:02:42s
epoch 14 | loss: 0.73319 | eval_custom_logloss: 1.09096 |  0:02:51s
epoch 15 | loss: 0.74071 | eval_custom_logloss: 0.93118 |  0:03:00s
epoch 16 | loss: 0.72364 | eval_custom_logloss: 0.79294 |  0:03:09s
epoch 17 | loss: 0.72889 | eval_custom_logloss: 1.15713 |  0:03:18s
epoch 18 | loss: 0.69749 | eval_custom_logloss: 0.85253 |  0:03:27s
epoch 19 | loss: 0.71434 | eval_custom_logloss: 0.94165 |  0:03:37s
epoch 20 | loss: 0.70334 | eval_custom_logloss: 0.82835 |  0:03:47s
epoch 21 | loss: 0.69194 | eval_custom_logloss: 0.80459 |  0:03:59s
epoch 22 | loss: 0.70086 | eval_custom_logloss: 0.80986 |  0:04:11s
epoch 23 | loss: 0.68639 | eval_custom_logloss: 0.69827 |  0:04:22s
epoch 24 | loss: 0.68543 | eval_custom_logloss: 0.70318 |  0:04:34s
epoch 25 | loss: 0.6805  | eval_custom_logloss: 0.76604 |  0:04:46s
epoch 26 | loss: 0.6822  | eval_custom_logloss: 1.05344 |  0:04:57s
epoch 27 | loss: 0.68703 | eval_custom_logloss: 0.72594 |  0:05:09s
epoch 28 | loss: 0.68117 | eval_custom_logloss: 0.75816 |  0:05:20s
epoch 29 | loss: 0.66487 | eval_custom_logloss: 1.02739 |  0:05:32s
epoch 30 | loss: 0.67206 | eval_custom_logloss: 0.81071 |  0:05:43s
epoch 31 | loss: 0.66098 | eval_custom_logloss: 0.80818 |  0:05:55s
epoch 32 | loss: 0.65076 | eval_custom_logloss: 1.03605 |  0:06:07s

Early stopping occurred at epoch 32 with best_epoch = 12 and best_eval_custom_logloss = 0.68632
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.63795, 'Log Loss - std': 0.04805000000000004} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.37311 | eval_custom_logloss: 1.0175  |  0:00:11s
epoch 1  | loss: 0.99247 | eval_custom_logloss: 0.89576 |  0:00:23s
epoch 2  | loss: 0.93844 | eval_custom_logloss: 1.05368 |  0:00:35s
epoch 3  | loss: 0.9087  | eval_custom_logloss: 0.77695 |  0:00:46s
epoch 4  | loss: 0.8749  | eval_custom_logloss: 0.81136 |  0:00:58s
epoch 5  | loss: 0.86335 | eval_custom_logloss: 0.81188 |  0:01:10s
epoch 6  | loss: 0.85558 | eval_custom_logloss: 0.758   |  0:01:22s
epoch 7  | loss: 0.84138 | eval_custom_logloss: 0.74517 |  0:01:34s
epoch 8  | loss: 0.83169 | eval_custom_logloss: 0.81756 |  0:01:45s
epoch 9  | loss: 0.81911 | eval_custom_logloss: 0.71679 |  0:01:54s
epoch 10 | loss: 0.83187 | eval_custom_logloss: 0.80112 |  0:02:04s
epoch 11 | loss: 0.79795 | eval_custom_logloss: 0.80347 |  0:02:13s
epoch 12 | loss: 0.78781 | eval_custom_logloss: 0.77733 |  0:02:23s
epoch 13 | loss: 0.77579 | eval_custom_logloss: 0.95435 |  0:02:32s
epoch 14 | loss: 0.75135 | eval_custom_logloss: 0.71681 |  0:02:43s
epoch 15 | loss: 0.7424  | eval_custom_logloss: 0.72748 |  0:02:54s
epoch 16 | loss: 0.74581 | eval_custom_logloss: 0.828   |  0:03:06s
epoch 17 | loss: 0.72857 | eval_custom_logloss: 0.66048 |  0:03:18s
epoch 18 | loss: 0.71145 | eval_custom_logloss: 0.64071 |  0:03:30s
epoch 19 | loss: 0.71635 | eval_custom_logloss: 0.92314 |  0:03:42s
epoch 20 | loss: 0.72079 | eval_custom_logloss: 1.12743 |  0:03:51s
epoch 21 | loss: 0.72286 | eval_custom_logloss: 0.84289 |  0:04:01s
epoch 22 | loss: 0.71779 | eval_custom_logloss: 0.78743 |  0:04:13s
epoch 23 | loss: 0.6988  | eval_custom_logloss: 0.72884 |  0:04:24s
epoch 24 | loss: 0.71281 | eval_custom_logloss: 1.44328 |  0:04:36s
epoch 25 | loss: 0.69438 | eval_custom_logloss: 0.64296 |  0:04:48s
epoch 26 | loss: 0.6936  | eval_custom_logloss: 0.69384 |  0:04:59s
epoch 27 | loss: 0.68256 | eval_custom_logloss: 0.83094 |  0:05:11s
epoch 28 | loss: 0.69711 | eval_custom_logloss: 0.9195  |  0:05:23s
epoch 29 | loss: 0.68588 | eval_custom_logloss: 0.76134 |  0:05:35s
epoch 30 | loss: 0.67956 | eval_custom_logloss: 0.72638 |  0:05:46s
epoch 31 | loss: 0.67231 | eval_custom_logloss: 0.62406 |  0:05:58s
epoch 32 | loss: 0.67214 | eval_custom_logloss: 0.79417 |  0:06:10s
epoch 33 | loss: 0.67263 | eval_custom_logloss: 0.66304 |  0:06:21s
epoch 34 | loss: 0.67283 | eval_custom_logloss: 0.8269  |  0:06:33s
epoch 35 | loss: 0.67621 | eval_custom_logloss: 0.82547 |  0:06:44s
epoch 36 | loss: 0.66635 | eval_custom_logloss: 0.67082 |  0:06:55s
epoch 37 | loss: 0.65913 | eval_custom_logloss: 1.27235 |  0:07:07s
epoch 38 | loss: 0.64993 | eval_custom_logloss: 0.94106 |  0:07:17s
epoch 39 | loss: 0.6635  | eval_custom_logloss: 0.90468 |  0:07:29s
epoch 40 | loss: 0.64319 | eval_custom_logloss: 0.61294 |  0:07:41s
epoch 41 | loss: 0.65618 | eval_custom_logloss: 0.97777 |  0:07:53s
epoch 42 | loss: 0.65474 | eval_custom_logloss: 0.66165 |  0:08:03s
epoch 43 | loss: 0.65124 | eval_custom_logloss: 1.33392 |  0:08:14s
epoch 44 | loss: 0.66165 | eval_custom_logloss: 0.77023 |  0:08:26s
epoch 45 | loss: 0.64153 | eval_custom_logloss: 0.61157 |  0:08:37s
epoch 46 | loss: 0.64654 | eval_custom_logloss: 0.74369 |  0:08:49s
epoch 47 | loss: 0.65639 | eval_custom_logloss: 1.24075 |  0:09:00s
epoch 48 | loss: 0.636   | eval_custom_logloss: 0.96075 |  0:09:12s
epoch 49 | loss: 0.64479 | eval_custom_logloss: 0.9709  |  0:09:23s
epoch 50 | loss: 0.6356  | eval_custom_logloss: 0.89166 |  0:09:35s
epoch 51 | loss: 0.63085 | eval_custom_logloss: 0.78913 |  0:09:47s
epoch 52 | loss: 0.6367  | eval_custom_logloss: 0.71533 |  0:09:58s
epoch 53 | loss: 0.64507 | eval_custom_logloss: 1.12286 |  0:10:10s
epoch 54 | loss: 0.63539 | eval_custom_logloss: 0.97586 |  0:10:22s
epoch 55 | loss: 0.64611 | eval_custom_logloss: 0.69167 |  0:10:33s
epoch 56 | loss: 0.62745 | eval_custom_logloss: 0.78745 |  0:10:45s
epoch 57 | loss: 0.6385  | eval_custom_logloss: 0.87835 |  0:10:56s
epoch 58 | loss: 0.63185 | eval_custom_logloss: 0.91565 |  0:11:07s
epoch 59 | loss: 0.62296 | eval_custom_logloss: 0.72562 |  0:11:19s
epoch 60 | loss: 0.63883 | eval_custom_logloss: 1.0258  |  0:11:31s
epoch 61 | loss: 0.62722 | eval_custom_logloss: 0.67666 |  0:11:42s
epoch 62 | loss: 0.6193  | eval_custom_logloss: 0.66569 |  0:11:53s
epoch 63 | loss: 0.63432 | eval_custom_logloss: 0.82333 |  0:12:04s
epoch 64 | loss: 0.62997 | eval_custom_logloss: 1.10778 |  0:12:15s
epoch 65 | loss: 0.61519 | eval_custom_logloss: 1.03697 |  0:12:26s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 0.61157
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6285, 'Log Loss - std': 0.04144643128987909} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.34633 | eval_custom_logloss: 1.38846 |  0:00:11s
epoch 1  | loss: 1.00072 | eval_custom_logloss: 0.92171 |  0:00:22s
epoch 2  | loss: 0.98453 | eval_custom_logloss: 0.91623 |  0:00:34s
epoch 3  | loss: 0.92491 | eval_custom_logloss: 0.85527 |  0:00:45s
epoch 4  | loss: 0.90904 | eval_custom_logloss: 0.79428 |  0:00:57s
epoch 5  | loss: 0.87883 | eval_custom_logloss: 0.75145 |  0:01:08s
epoch 6  | loss: 0.85412 | eval_custom_logloss: 0.87141 |  0:01:20s
epoch 7  | loss: 0.82382 | eval_custom_logloss: 0.98877 |  0:01:32s
epoch 8  | loss: 0.82333 | eval_custom_logloss: 0.70769 |  0:01:43s
epoch 9  | loss: 0.79312 | eval_custom_logloss: 0.73256 |  0:01:55s
epoch 10 | loss: 0.8114  | eval_custom_logloss: 1.01126 |  0:02:06s
epoch 11 | loss: 0.77823 | eval_custom_logloss: 0.69323 |  0:02:18s
epoch 12 | loss: 0.77547 | eval_custom_logloss: 0.77462 |  0:02:29s
epoch 13 | loss: 0.77196 | eval_custom_logloss: 1.08292 |  0:02:41s
epoch 14 | loss: 0.75767 | eval_custom_logloss: 0.82815 |  0:02:53s
epoch 15 | loss: 0.75003 | eval_custom_logloss: 0.95316 |  0:03:04s
epoch 16 | loss: 0.73789 | eval_custom_logloss: 0.74127 |  0:03:16s
epoch 17 | loss: 0.74904 | eval_custom_logloss: 0.70588 |  0:03:27s
epoch 18 | loss: 0.73077 | eval_custom_logloss: 0.79791 |  0:03:39s
epoch 19 | loss: 0.71707 | eval_custom_logloss: 0.72706 |  0:03:50s
epoch 20 | loss: 0.71491 | eval_custom_logloss: 0.67179 |  0:04:01s
epoch 21 | loss: 0.70149 | eval_custom_logloss: 0.991   |  0:04:13s
epoch 22 | loss: 0.72055 | eval_custom_logloss: 0.66197 |  0:04:24s
epoch 23 | loss: 0.7077  | eval_custom_logloss: 0.69211 |  0:04:36s
epoch 24 | loss: 0.70015 | eval_custom_logloss: 0.6982  |  0:04:48s
epoch 25 | loss: 0.68986 | eval_custom_logloss: 0.79689 |  0:05:00s
epoch 26 | loss: 0.69567 | eval_custom_logloss: 0.82006 |  0:05:11s
epoch 27 | loss: 0.68007 | eval_custom_logloss: 0.7348  |  0:05:22s
epoch 28 | loss: 0.68421 | eval_custom_logloss: 0.82935 |  0:05:32s
epoch 29 | loss: 0.68169 | eval_custom_logloss: 0.74202 |  0:05:41s
epoch 30 | loss: 0.68289 | eval_custom_logloss: 0.65005 |  0:05:53s
epoch 31 | loss: 0.67172 | eval_custom_logloss: 0.71522 |  0:06:05s
epoch 32 | loss: 0.67188 | eval_custom_logloss: 0.84817 |  0:06:16s
epoch 33 | loss: 0.67264 | eval_custom_logloss: 0.62209 |  0:06:28s
epoch 34 | loss: 0.67033 | eval_custom_logloss: 0.76816 |  0:06:40s
epoch 35 | loss: 0.66704 | eval_custom_logloss: 0.74203 |  0:06:51s
epoch 36 | loss: 0.66908 | eval_custom_logloss: 0.67744 |  0:07:03s
epoch 37 | loss: 0.662   | eval_custom_logloss: 0.66495 |  0:07:14s
epoch 38 | loss: 0.66686 | eval_custom_logloss: 1.05031 |  0:07:26s
epoch 39 | loss: 0.6737  | eval_custom_logloss: 0.73692 |  0:07:37s
epoch 40 | loss: 0.65729 | eval_custom_logloss: 0.79602 |  0:07:49s
epoch 41 | loss: 0.66153 | eval_custom_logloss: 0.85304 |  0:08:00s
epoch 42 | loss: 0.66674 | eval_custom_logloss: 1.05483 |  0:08:10s
epoch 43 | loss: 0.65819 | eval_custom_logloss: 0.6907  |  0:08:22s
epoch 44 | loss: 0.65599 | eval_custom_logloss: 0.78387 |  0:08:33s
epoch 45 | loss: 0.66256 | eval_custom_logloss: 0.70488 |  0:08:45s
epoch 46 | loss: 0.67124 | eval_custom_logloss: 0.83339 |  0:08:57s
epoch 47 | loss: 0.65905 | eval_custom_logloss: 0.70331 |  0:09:08s
epoch 48 | loss: 0.65206 | eval_custom_logloss: 0.79532 |  0:09:20s
epoch 49 | loss: 0.65334 | eval_custom_logloss: 0.78769 |  0:09:32s
epoch 50 | loss: 0.64927 | eval_custom_logloss: 0.97816 |  0:09:43s
epoch 51 | loss: 0.64174 | eval_custom_logloss: 0.72852 |  0:09:55s
epoch 52 | loss: 0.63802 | eval_custom_logloss: 0.8931  |  0:10:07s
epoch 53 | loss: 0.65125 | eval_custom_logloss: 0.69495 |  0:10:19s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.62209
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62675, 'Log Loss - std': 0.03602141724030305} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.35948 | eval_custom_logloss: 1.09496 |  0:00:11s
epoch 1  | loss: 1.03587 | eval_custom_logloss: 0.87989 |  0:00:23s
epoch 2  | loss: 0.98652 | eval_custom_logloss: 1.37361 |  0:00:34s
epoch 3  | loss: 0.93658 | eval_custom_logloss: 0.88576 |  0:00:45s
epoch 4  | loss: 0.89164 | eval_custom_logloss: 0.78511 |  0:00:57s
epoch 5  | loss: 0.87781 | eval_custom_logloss: 0.82029 |  0:01:07s
epoch 6  | loss: 0.86859 | eval_custom_logloss: 1.29127 |  0:01:19s
epoch 7  | loss: 0.84068 | eval_custom_logloss: 1.18207 |  0:01:31s
epoch 8  | loss: 0.81732 | eval_custom_logloss: 0.78006 |  0:01:42s
epoch 9  | loss: 0.83046 | eval_custom_logloss: 0.73172 |  0:01:54s
epoch 10 | loss: 0.80015 | eval_custom_logloss: 0.97958 |  0:02:05s
epoch 11 | loss: 0.78575 | eval_custom_logloss: 0.84965 |  0:02:17s
epoch 12 | loss: 0.78253 | eval_custom_logloss: 0.73394 |  0:02:29s
epoch 13 | loss: 0.78957 | eval_custom_logloss: 0.77458 |  0:02:40s
epoch 14 | loss: 0.78115 | eval_custom_logloss: 0.69468 |  0:02:52s
epoch 15 | loss: 0.76725 | eval_custom_logloss: 0.76685 |  0:03:04s
epoch 16 | loss: 0.77303 | eval_custom_logloss: 0.85673 |  0:03:16s
epoch 17 | loss: 0.75581 | eval_custom_logloss: 0.73546 |  0:03:27s
epoch 18 | loss: 0.72994 | eval_custom_logloss: 0.99792 |  0:03:38s
epoch 19 | loss: 0.73814 | eval_custom_logloss: 0.68191 |  0:03:50s
epoch 20 | loss: 0.74497 | eval_custom_logloss: 0.80631 |  0:04:02s
epoch 21 | loss: 0.72364 | eval_custom_logloss: 0.96999 |  0:04:14s
epoch 22 | loss: 0.7216  | eval_custom_logloss: 0.63993 |  0:04:23s
epoch 23 | loss: 0.72226 | eval_custom_logloss: 0.71761 |  0:04:32s
epoch 24 | loss: 0.72415 | eval_custom_logloss: 0.76569 |  0:04:41s
epoch 25 | loss: 0.7053  | eval_custom_logloss: 0.61795 |  0:04:50s
epoch 26 | loss: 0.70141 | eval_custom_logloss: 0.77941 |  0:05:01s
epoch 27 | loss: 0.71267 | eval_custom_logloss: 0.69644 |  0:05:13s
epoch 28 | loss: 0.69873 | eval_custom_logloss: 0.69203 |  0:05:24s
epoch 29 | loss: 0.69652 | eval_custom_logloss: 0.70191 |  0:05:36s
epoch 30 | loss: 0.68812 | eval_custom_logloss: 0.64789 |  0:05:47s
epoch 31 | loss: 0.68871 | eval_custom_logloss: 0.62962 |  0:05:58s
epoch 32 | loss: 0.67232 | eval_custom_logloss: 0.62098 |  0:06:09s
epoch 33 | loss: 0.67936 | eval_custom_logloss: 0.66801 |  0:06:21s
epoch 34 | loss: 0.66642 | eval_custom_logloss: 0.80948 |  0:06:33s
epoch 35 | loss: 0.67726 | eval_custom_logloss: 0.64933 |  0:06:45s
epoch 36 | loss: 0.66786 | eval_custom_logloss: 0.66931 |  0:06:57s
epoch 37 | loss: 0.66494 | eval_custom_logloss: 0.63298 |  0:07:08s
epoch 38 | loss: 0.66428 | eval_custom_logloss: 0.90787 |  0:07:20s
epoch 39 | loss: 0.66449 | eval_custom_logloss: 0.70724 |  0:07:31s
epoch 40 | loss: 0.65805 | eval_custom_logloss: 0.73795 |  0:07:42s
epoch 41 | loss: 0.6578  | eval_custom_logloss: 0.80054 |  0:07:54s
epoch 42 | loss: 0.66706 | eval_custom_logloss: 1.23206 |  0:08:05s
epoch 43 | loss: 0.66194 | eval_custom_logloss: 1.29443 |  0:08:17s
epoch 44 | loss: 0.66058 | eval_custom_logloss: 0.67772 |  0:08:29s
epoch 45 | loss: 0.66289 | eval_custom_logloss: 0.79866 |  0:08:39s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 0.61795
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62476, 'Log Loss - std': 0.032463431734799715} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 0.62476 and parameters: {'n_d': 29, 'n_steps': 5, 'gamma': 1.7885460367192891, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 2, 'momentum': 0.00963576540025324, 'mask_type': 'entmax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax', 'n_a': 60, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.96273 | eval_custom_logloss: 1.35045 |  0:00:33s
epoch 1  | loss: 1.20183 | eval_custom_logloss: 0.98796 |  0:01:06s
epoch 2  | loss: 1.05535 | eval_custom_logloss: 0.90979 |  0:01:39s
epoch 3  | loss: 0.98301 | eval_custom_logloss: 0.86673 |  0:02:12s
epoch 4  | loss: 0.95776 | eval_custom_logloss: 0.86125 |  0:02:46s
epoch 5  | loss: 0.92863 | eval_custom_logloss: 0.79179 |  0:03:19s
epoch 6  | loss: 0.90563 | eval_custom_logloss: 0.75625 |  0:03:52s
epoch 7  | loss: 0.89458 | eval_custom_logloss: 0.85208 |  0:04:26s
epoch 8  | loss: 0.86699 | eval_custom_logloss: 0.76283 |  0:04:59s
epoch 9  | loss: 0.83705 | eval_custom_logloss: 0.79371 |  0:05:33s
epoch 10 | loss: 0.83137 | eval_custom_logloss: 0.77352 |  0:06:07s
epoch 11 | loss: 0.82477 | eval_custom_logloss: 1.56781 |  0:06:40s
epoch 12 | loss: 0.82439 | eval_custom_logloss: 0.66778 |  0:07:13s
epoch 13 | loss: 0.80102 | eval_custom_logloss: 0.70212 |  0:07:47s
epoch 14 | loss: 0.7929  | eval_custom_logloss: 0.81628 |  0:08:21s
epoch 15 | loss: 0.79363 | eval_custom_logloss: 0.76069 |  0:08:54s
epoch 16 | loss: 0.77311 | eval_custom_logloss: 0.74125 |  0:09:27s
epoch 17 | loss: 0.76971 | eval_custom_logloss: 0.74061 |  0:10:01s
epoch 18 | loss: 0.77753 | eval_custom_logloss: 0.71642 |  0:10:34s
epoch 19 | loss: 0.76082 | eval_custom_logloss: 0.66906 |  0:11:07s
epoch 20 | loss: 0.76066 | eval_custom_logloss: 0.751   |  0:11:41s
epoch 21 | loss: 0.73877 | eval_custom_logloss: 0.82732 |  0:12:15s
epoch 22 | loss: 0.73989 | eval_custom_logloss: 0.67385 |  0:12:48s
epoch 23 | loss: 0.74763 | eval_custom_logloss: 0.72416 |  0:13:21s
epoch 24 | loss: 0.74046 | eval_custom_logloss: 0.71574 |  0:13:54s
epoch 25 | loss: 0.72599 | eval_custom_logloss: 0.80232 |  0:14:28s
epoch 26 | loss: 0.72062 | eval_custom_logloss: 0.67525 |  0:15:01s
epoch 27 | loss: 0.69904 | eval_custom_logloss: 0.85113 |  0:15:35s
epoch 28 | loss: 0.69921 | eval_custom_logloss: 0.6866  |  0:16:08s
epoch 29 | loss: 0.70223 | eval_custom_logloss: 0.69129 |  0:16:42s
epoch 30 | loss: 0.71138 | eval_custom_logloss: 1.02563 |  0:17:15s
epoch 31 | loss: 0.69238 | eval_custom_logloss: 1.0258  |  0:17:48s
epoch 32 | loss: 0.69732 | eval_custom_logloss: 0.82818 |  0:18:22s

Early stopping occurred at epoch 32 with best_epoch = 12 and best_eval_custom_logloss = 0.66778
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6668, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax', 'n_a': 60, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.96035 | eval_custom_logloss: 1.32464 |  0:00:33s
epoch 1  | loss: 1.3103  | eval_custom_logloss: 1.12383 |  0:01:06s
epoch 2  | loss: 1.17017 | eval_custom_logloss: 1.05181 |  0:01:40s
epoch 3  | loss: 1.06216 | eval_custom_logloss: 0.92919 |  0:02:14s
epoch 4  | loss: 0.99335 | eval_custom_logloss: 0.87425 |  0:02:48s
epoch 5  | loss: 0.93999 | eval_custom_logloss: 0.81613 |  0:03:21s
epoch 6  | loss: 0.90032 | eval_custom_logloss: 0.85095 |  0:03:55s
epoch 7  | loss: 0.8927  | eval_custom_logloss: 1.00607 |  0:04:28s
epoch 8  | loss: 0.87995 | eval_custom_logloss: 0.89598 |  0:05:02s
epoch 9  | loss: 0.86394 | eval_custom_logloss: 1.17927 |  0:05:35s
epoch 10 | loss: 0.85249 | eval_custom_logloss: 0.82943 |  0:06:08s
epoch 11 | loss: 0.82805 | eval_custom_logloss: 0.90526 |  0:06:42s
epoch 12 | loss: 0.82213 | eval_custom_logloss: 0.78108 |  0:07:16s
epoch 13 | loss: 0.81194 | eval_custom_logloss: 0.7383  |  0:07:49s
epoch 14 | loss: 0.80432 | eval_custom_logloss: 0.723   |  0:08:23s
epoch 15 | loss: 0.78523 | eval_custom_logloss: 0.804   |  0:08:57s
epoch 16 | loss: 0.77111 | eval_custom_logloss: 0.71125 |  0:09:30s
epoch 17 | loss: 0.7849  | eval_custom_logloss: 1.17068 |  0:10:04s
epoch 18 | loss: 0.78827 | eval_custom_logloss: 0.70996 |  0:10:37s
epoch 19 | loss: 0.75797 | eval_custom_logloss: 0.64452 |  0:11:11s
epoch 20 | loss: 0.74137 | eval_custom_logloss: 0.86007 |  0:11:44s
epoch 21 | loss: 0.75498 | eval_custom_logloss: 0.66537 |  0:12:18s
epoch 22 | loss: 0.74498 | eval_custom_logloss: 0.81979 |  0:12:51s
epoch 23 | loss: 0.73139 | eval_custom_logloss: 0.6887  |  0:13:25s
epoch 24 | loss: 0.73947 | eval_custom_logloss: 0.68772 |  0:13:59s
epoch 25 | loss: 0.72962 | eval_custom_logloss: 0.98211 |  0:14:33s
epoch 26 | loss: 0.71916 | eval_custom_logloss: 0.84109 |  0:15:07s
epoch 27 | loss: 0.70766 | eval_custom_logloss: 0.75786 |  0:15:41s
epoch 28 | loss: 0.69252 | eval_custom_logloss: 0.69295 |  0:16:14s
epoch 29 | loss: 0.69915 | eval_custom_logloss: 0.79431 |  0:16:48s
epoch 30 | loss: 0.70177 | eval_custom_logloss: 0.61691 |  0:17:22s
epoch 31 | loss: 0.68162 | eval_custom_logloss: 0.64173 |  0:17:55s
epoch 32 | loss: 0.70495 | eval_custom_logloss: 1.11271 |  0:18:29s
epoch 33 | loss: 0.70091 | eval_custom_logloss: 0.74858 |  0:19:02s
epoch 34 | loss: 0.68332 | eval_custom_logloss: 0.74763 |  0:19:36s
epoch 35 | loss: 0.67272 | eval_custom_logloss: 0.81644 |  0:20:09s
epoch 36 | loss: 0.68114 | eval_custom_logloss: 0.87771 |  0:20:43s
epoch 37 | loss: 0.67644 | eval_custom_logloss: 0.61868 |  0:21:16s
epoch 38 | loss: 0.68168 | eval_custom_logloss: 0.62761 |  0:21:49s
epoch 39 | loss: 0.67265 | eval_custom_logloss: 0.78045 |  0:22:23s
epoch 40 | loss: 0.6676  | eval_custom_logloss: 0.63138 |  0:22:57s
epoch 41 | loss: 0.66294 | eval_custom_logloss: 0.70419 |  0:23:30s
epoch 42 | loss: 0.67171 | eval_custom_logloss: 0.81189 |  0:24:03s
epoch 43 | loss: 0.65289 | eval_custom_logloss: 0.59055 |  0:24:37s
epoch 44 | loss: 0.66217 | eval_custom_logloss: 0.78339 |  0:25:12s
epoch 45 | loss: 0.65734 | eval_custom_logloss: 0.80481 |  0:25:45s
epoch 46 | loss: 0.67037 | eval_custom_logloss: 0.77356 |  0:26:19s
epoch 47 | loss: 0.65479 | eval_custom_logloss: 0.72796 |  0:26:52s
epoch 48 | loss: 0.65427 | eval_custom_logloss: 0.60483 |  0:27:26s
epoch 49 | loss: 0.64519 | eval_custom_logloss: 0.92133 |  0:28:01s
epoch 50 | loss: 0.65442 | eval_custom_logloss: 0.79644 |  0:28:35s
epoch 51 | loss: 0.64451 | eval_custom_logloss: 0.80221 |  0:29:08s
epoch 52 | loss: 0.64892 | eval_custom_logloss: 0.7035  |  0:29:42s
epoch 53 | loss: 0.65173 | eval_custom_logloss: 0.80057 |  0:30:15s
epoch 54 | loss: 0.66219 | eval_custom_logloss: 0.79868 |  0:30:49s
epoch 55 | loss: 0.65893 | eval_custom_logloss: 0.83052 |  0:31:23s
epoch 56 | loss: 0.65313 | eval_custom_logloss: 0.79882 |  0:31:56s
epoch 57 | loss: 0.63219 | eval_custom_logloss: 0.9297  |  0:32:30s
epoch 58 | loss: 0.64036 | eval_custom_logloss: 0.72224 |  0:33:04s
epoch 59 | loss: 0.64219 | eval_custom_logloss: 1.33595 |  0:33:38s
epoch 60 | loss: 0.63139 | eval_custom_logloss: 0.70783 |  0:34:12s
epoch 61 | loss: 0.65128 | eval_custom_logloss: 0.7322  |  0:34:46s
epoch 62 | loss: 0.64421 | eval_custom_logloss: 0.59392 |  0:35:22s
epoch 63 | loss: 0.62952 | eval_custom_logloss: 0.73089 |  0:35:57s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.59055
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6275999999999999, 'Log Loss - std': 0.03919999999999996} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax', 'n_a': 60, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.95722 | eval_custom_logloss: 1.43029 |  0:00:35s
epoch 1  | loss: 1.23837 | eval_custom_logloss: 1.03603 |  0:01:10s
epoch 2  | loss: 1.08235 | eval_custom_logloss: 0.9568  |  0:01:46s
epoch 3  | loss: 1.05452 | eval_custom_logloss: 0.98796 |  0:02:22s
epoch 4  | loss: 1.0584  | eval_custom_logloss: 0.93877 |  0:02:58s
epoch 5  | loss: 1.01595 | eval_custom_logloss: 0.89975 |  0:03:33s
epoch 6  | loss: 0.99234 | eval_custom_logloss: 0.86324 |  0:04:07s
epoch 7  | loss: 0.98694 | eval_custom_logloss: 1.01116 |  0:04:41s
epoch 8  | loss: 0.96504 | eval_custom_logloss: 0.81996 |  0:05:15s
epoch 9  | loss: 0.93261 | eval_custom_logloss: 0.87394 |  0:05:49s
epoch 10 | loss: 0.94283 | eval_custom_logloss: 0.85118 |  0:06:23s
epoch 11 | loss: 0.92282 | eval_custom_logloss: 0.80458 |  0:06:57s
epoch 12 | loss: 0.90866 | eval_custom_logloss: 0.8098  |  0:07:31s
epoch 13 | loss: 0.89048 | eval_custom_logloss: 0.97159 |  0:08:05s
epoch 14 | loss: 0.84701 | eval_custom_logloss: 0.83826 |  0:08:38s
epoch 15 | loss: 0.83839 | eval_custom_logloss: 0.78357 |  0:09:12s
epoch 16 | loss: 0.82523 | eval_custom_logloss: 0.89449 |  0:09:46s
epoch 17 | loss: 0.80151 | eval_custom_logloss: 0.69516 |  0:10:19s
epoch 18 | loss: 0.80426 | eval_custom_logloss: 0.71805 |  0:10:53s
epoch 19 | loss: 0.79512 | eval_custom_logloss: 0.67901 |  0:11:27s
epoch 20 | loss: 0.77105 | eval_custom_logloss: 0.7117  |  0:12:00s
epoch 21 | loss: 0.77588 | eval_custom_logloss: 0.74666 |  0:12:34s
epoch 22 | loss: 0.74185 | eval_custom_logloss: 0.99211 |  0:13:08s
epoch 23 | loss: 0.75314 | eval_custom_logloss: 0.70963 |  0:13:41s
epoch 24 | loss: 0.75579 | eval_custom_logloss: 0.67908 |  0:14:15s
epoch 25 | loss: 0.73374 | eval_custom_logloss: 0.72998 |  0:14:49s
epoch 26 | loss: 0.73542 | eval_custom_logloss: 0.79201 |  0:15:22s
epoch 27 | loss: 0.72641 | eval_custom_logloss: 0.69156 |  0:15:56s
epoch 28 | loss: 0.70436 | eval_custom_logloss: 0.80465 |  0:16:30s
epoch 29 | loss: 0.71424 | eval_custom_logloss: 0.73213 |  0:17:04s
epoch 30 | loss: 0.72768 | eval_custom_logloss: 0.65536 |  0:17:38s
epoch 31 | loss: 0.70447 | eval_custom_logloss: 0.69542 |  0:18:12s
epoch 32 | loss: 0.70392 | eval_custom_logloss: 0.77571 |  0:18:46s
epoch 33 | loss: 0.70357 | eval_custom_logloss: 0.8832  |  0:19:20s
epoch 34 | loss: 0.69332 | eval_custom_logloss: 0.66705 |  0:19:53s
epoch 35 | loss: 0.69833 | eval_custom_logloss: 0.64268 |  0:20:27s
epoch 36 | loss: 0.69656 | eval_custom_logloss: 0.86063 |  0:21:01s
epoch 37 | loss: 0.6812  | eval_custom_logloss: 0.88777 |  0:21:34s
epoch 38 | loss: 0.68449 | eval_custom_logloss: 0.72369 |  0:22:08s
epoch 39 | loss: 0.68971 | eval_custom_logloss: 0.7131  |  0:22:42s
epoch 40 | loss: 0.68698 | eval_custom_logloss: 0.71377 |  0:23:17s
epoch 41 | loss: 0.68987 | eval_custom_logloss: 0.73129 |  0:23:50s
epoch 42 | loss: 0.69473 | eval_custom_logloss: 0.76987 |  0:24:24s
epoch 43 | loss: 0.68137 | eval_custom_logloss: 0.65045 |  0:24:58s
epoch 44 | loss: 0.66907 | eval_custom_logloss: 0.70699 |  0:25:31s
epoch 45 | loss: 0.67326 | eval_custom_logloss: 0.68303 |  0:26:05s
epoch 46 | loss: 0.69041 | eval_custom_logloss: 0.68823 |  0:26:38s
epoch 47 | loss: 0.67425 | eval_custom_logloss: 0.7806  |  0:27:12s
epoch 48 | loss: 0.6646  | eval_custom_logloss: 0.67197 |  0:27:45s
epoch 49 | loss: 0.66176 | eval_custom_logloss: 0.80493 |  0:28:19s
epoch 50 | loss: 0.67787 | eval_custom_logloss: 0.6978  |  0:28:52s
epoch 51 | loss: 0.66885 | eval_custom_logloss: 0.83985 |  0:29:26s
epoch 52 | loss: 0.67261 | eval_custom_logloss: 0.78627 |  0:29:59s
epoch 53 | loss: 0.6662  | eval_custom_logloss: 0.64745 |  0:30:33s
epoch 54 | loss: 0.65548 | eval_custom_logloss: 0.80481 |  0:31:06s
epoch 55 | loss: 0.65185 | eval_custom_logloss: 0.72449 |  0:31:40s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 0.64268
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6320666666666667, 'Log Loss - std': 0.03262405383898338} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax', 'n_a': 60, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.00297 | eval_custom_logloss: 1.25678 |  0:00:33s
epoch 1  | loss: 1.30162 | eval_custom_logloss: 1.0848  |  0:01:06s
epoch 2  | loss: 1.18017 | eval_custom_logloss: 1.05188 |  0:01:40s
epoch 3  | loss: 1.059   | eval_custom_logloss: 0.90384 |  0:02:13s
epoch 4  | loss: 1.06508 | eval_custom_logloss: 0.92426 |  0:02:47s
epoch 5  | loss: 1.07219 | eval_custom_logloss: 0.91471 |  0:03:20s
epoch 6  | loss: 0.98085 | eval_custom_logloss: 0.88147 |  0:03:54s
epoch 7  | loss: 0.9861  | eval_custom_logloss: 0.86578 |  0:04:27s
epoch 8  | loss: 0.96797 | eval_custom_logloss: 0.85769 |  0:05:01s
epoch 9  | loss: 0.96366 | eval_custom_logloss: 0.85271 |  0:05:34s
epoch 10 | loss: 0.93195 | eval_custom_logloss: 0.81325 |  0:06:08s
epoch 11 | loss: 0.91939 | eval_custom_logloss: 0.98451 |  0:06:42s
epoch 12 | loss: 0.88835 | eval_custom_logloss: 0.81056 |  0:07:15s
epoch 13 | loss: 0.86736 | eval_custom_logloss: 0.70137 |  0:07:49s
epoch 14 | loss: 0.84279 | eval_custom_logloss: 0.76953 |  0:08:23s
epoch 15 | loss: 0.82863 | eval_custom_logloss: 0.89564 |  0:08:56s
epoch 16 | loss: 0.84986 | eval_custom_logloss: 0.78237 |  0:09:30s
epoch 17 | loss: 0.83219 | eval_custom_logloss: 0.75685 |  0:10:04s
epoch 18 | loss: 0.8251  | eval_custom_logloss: 0.85002 |  0:10:38s
epoch 19 | loss: 0.807   | eval_custom_logloss: 0.81697 |  0:11:12s
epoch 20 | loss: 0.79479 | eval_custom_logloss: 0.85387 |  0:11:45s
epoch 21 | loss: 0.79785 | eval_custom_logloss: 0.72009 |  0:12:19s
epoch 22 | loss: 0.78594 | eval_custom_logloss: 0.9584  |  0:12:54s
epoch 23 | loss: 0.78337 | eval_custom_logloss: 0.82477 |  0:13:28s
epoch 24 | loss: 0.79235 | eval_custom_logloss: 0.76846 |  0:14:01s
epoch 25 | loss: 0.79609 | eval_custom_logloss: 0.77628 |  0:14:35s
epoch 26 | loss: 0.77694 | eval_custom_logloss: 0.84216 |  0:15:09s
epoch 27 | loss: 0.75601 | eval_custom_logloss: 0.73733 |  0:15:42s
epoch 28 | loss: 0.7537  | eval_custom_logloss: 0.81326 |  0:16:16s
epoch 29 | loss: 0.76523 | eval_custom_logloss: 0.74374 |  0:16:50s
epoch 30 | loss: 0.76624 | eval_custom_logloss: 0.7622  |  0:17:23s
epoch 31 | loss: 0.74871 | eval_custom_logloss: 0.74741 |  0:17:57s
epoch 32 | loss: 0.75514 | eval_custom_logloss: 0.68532 |  0:18:32s
epoch 33 | loss: 0.75628 | eval_custom_logloss: 0.82418 |  0:19:07s
epoch 34 | loss: 0.7477  | eval_custom_logloss: 0.74076 |  0:19:41s
epoch 35 | loss: 0.7573  | eval_custom_logloss: 0.78907 |  0:20:16s
epoch 36 | loss: 0.74898 | eval_custom_logloss: 0.80982 |  0:20:50s
epoch 37 | loss: 0.73209 | eval_custom_logloss: 0.64383 |  0:21:25s
epoch 38 | loss: 0.73751 | eval_custom_logloss: 0.90592 |  0:21:59s
epoch 39 | loss: 0.73426 | eval_custom_logloss: 0.79799 |  0:22:33s
epoch 40 | loss: 0.71437 | eval_custom_logloss: 0.68307 |  0:23:06s
epoch 41 | loss: 0.71666 | eval_custom_logloss: 0.76459 |  0:23:40s
epoch 42 | loss: 0.71058 | eval_custom_logloss: 0.65195 |  0:24:13s
epoch 43 | loss: 0.71211 | eval_custom_logloss: 0.8354  |  0:24:47s
epoch 44 | loss: 0.71271 | eval_custom_logloss: 0.83979 |  0:25:20s
epoch 45 | loss: 0.71474 | eval_custom_logloss: 0.99431 |  0:25:54s
epoch 46 | loss: 0.71327 | eval_custom_logloss: 0.61354 |  0:26:28s
epoch 47 | loss: 0.69606 | eval_custom_logloss: 0.7285  |  0:27:01s
epoch 48 | loss: 0.69682 | eval_custom_logloss: 0.76926 |  0:27:35s
epoch 49 | loss: 0.69229 | eval_custom_logloss: 0.68305 |  0:28:09s
epoch 50 | loss: 0.7148  | eval_custom_logloss: 0.70205 |  0:28:42s
epoch 51 | loss: 0.69394 | eval_custom_logloss: 0.71281 |  0:29:16s
epoch 52 | loss: 0.68676 | eval_custom_logloss: 0.6442  |  0:29:50s
epoch 53 | loss: 0.68139 | eval_custom_logloss: 0.76728 |  0:30:24s
epoch 54 | loss: 0.67892 | eval_custom_logloss: 0.67942 |  0:30:58s
epoch 55 | loss: 0.68178 | eval_custom_logloss: 0.76977 |  0:31:32s
epoch 56 | loss: 0.68184 | eval_custom_logloss: 0.61101 |  0:32:05s
epoch 57 | loss: 0.67293 | eval_custom_logloss: 0.82639 |  0:32:39s
epoch 58 | loss: 0.66033 | eval_custom_logloss: 0.76407 |  0:33:13s
epoch 59 | loss: 0.67058 | eval_custom_logloss: 0.66623 |  0:33:47s
epoch 60 | loss: 0.6664  | eval_custom_logloss: 0.74376 |  0:34:21s
epoch 61 | loss: 0.66566 | eval_custom_logloss: 1.24322 |  0:34:55s
epoch 62 | loss: 0.65838 | eval_custom_logloss: 0.72927 |  0:35:29s
epoch 63 | loss: 0.65917 | eval_custom_logloss: 0.78198 |  0:36:03s
epoch 64 | loss: 0.67349 | eval_custom_logloss: 1.07319 |  0:36:37s
epoch 65 | loss: 0.67347 | eval_custom_logloss: 0.80775 |  0:37:11s
epoch 66 | loss: 0.68523 | eval_custom_logloss: 0.74651 |  0:37:45s
epoch 67 | loss: 0.71658 | eval_custom_logloss: 0.73866 |  0:38:19s
epoch 68 | loss: 0.71249 | eval_custom_logloss: 0.67641 |  0:38:52s
epoch 69 | loss: 0.68825 | eval_custom_logloss: 0.60034 |  0:39:26s
epoch 70 | loss: 0.68459 | eval_custom_logloss: 0.6031  |  0:40:00s
epoch 71 | loss: 0.67508 | eval_custom_logloss: 0.69102 |  0:40:34s
epoch 72 | loss: 0.66846 | eval_custom_logloss: 0.81785 |  0:41:07s
epoch 73 | loss: 0.67012 | eval_custom_logloss: 0.64064 |  0:41:41s
epoch 74 | loss: 0.65756 | eval_custom_logloss: 0.80239 |  0:42:15s
epoch 75 | loss: 0.65393 | eval_custom_logloss: 0.72209 |  0:42:49s
epoch 76 | loss: 0.66723 | eval_custom_logloss: 0.67187 |  0:43:22s
epoch 77 | loss: 0.66607 | eval_custom_logloss: 0.6426  |  0:43:56s
epoch 78 | loss: 0.66413 | eval_custom_logloss: 0.82792 |  0:44:30s
epoch 79 | loss: 0.65619 | eval_custom_logloss: 0.6796  |  0:45:03s
epoch 80 | loss: 0.65073 | eval_custom_logloss: 0.87879 |  0:45:37s
epoch 81 | loss: 0.65876 | eval_custom_logloss: 0.83065 |  0:46:10s
epoch 82 | loss: 0.64319 | eval_custom_logloss: 0.89519 |  0:46:44s
epoch 83 | loss: 0.65987 | eval_custom_logloss: 0.6059  |  0:47:18s
epoch 84 | loss: 0.66411 | eval_custom_logloss: 2.08888 |  0:47:52s
epoch 85 | loss: 0.64578 | eval_custom_logloss: 0.64216 |  0:48:25s
epoch 86 | loss: 0.64477 | eval_custom_logloss: 0.65711 |  0:48:59s
epoch 87 | loss: 0.64807 | eval_custom_logloss: 0.85725 |  0:49:33s
epoch 88 | loss: 0.63448 | eval_custom_logloss: 0.78881 |  0:50:07s
epoch 89 | loss: 0.63718 | eval_custom_logloss: 0.73666 |  0:50:41s

Early stopping occurred at epoch 89 with best_epoch = 69 and best_eval_custom_logloss = 0.60034
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62415, 'Log Loss - std': 0.03140489611509641} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax', 'n_a': 60, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.89849 | eval_custom_logloss: 1.16882 |  0:00:33s
epoch 1  | loss: 1.20683 | eval_custom_logloss: 1.09688 |  0:01:07s
epoch 2  | loss: 1.13614 | eval_custom_logloss: 1.00905 |  0:01:41s
epoch 3  | loss: 1.06331 | eval_custom_logloss: 0.97247 |  0:02:15s
epoch 4  | loss: 1.06023 | eval_custom_logloss: 0.94656 |  0:02:49s
epoch 5  | loss: 1.03714 | eval_custom_logloss: 0.87464 |  0:03:22s
epoch 6  | loss: 0.98326 | eval_custom_logloss: 0.85605 |  0:03:56s
epoch 7  | loss: 1.00295 | eval_custom_logloss: 0.92831 |  0:04:30s
epoch 8  | loss: 0.98209 | eval_custom_logloss: 0.86709 |  0:05:03s
epoch 9  | loss: 0.95023 | eval_custom_logloss: 0.87623 |  0:05:37s
epoch 10 | loss: 0.96406 | eval_custom_logloss: 0.90976 |  0:06:11s
epoch 11 | loss: 0.94727 | eval_custom_logloss: 1.00591 |  0:06:44s
epoch 12 | loss: 0.93187 | eval_custom_logloss: 0.85198 |  0:07:18s
epoch 13 | loss: 0.91731 | eval_custom_logloss: 0.84023 |  0:07:53s
epoch 14 | loss: 0.92357 | eval_custom_logloss: 0.85855 |  0:08:27s
epoch 15 | loss: 0.89923 | eval_custom_logloss: 0.82126 |  0:09:00s
epoch 16 | loss: 0.88529 | eval_custom_logloss: 0.84831 |  0:09:35s
epoch 17 | loss: 0.85529 | eval_custom_logloss: 0.77459 |  0:10:10s
epoch 18 | loss: 0.85674 | eval_custom_logloss: 0.76532 |  0:10:46s
epoch 19 | loss: 0.83727 | eval_custom_logloss: 0.82182 |  0:11:22s
epoch 20 | loss: 0.84397 | eval_custom_logloss: 0.77321 |  0:11:57s
epoch 21 | loss: 0.81936 | eval_custom_logloss: 0.73857 |  0:12:33s
epoch 22 | loss: 0.79757 | eval_custom_logloss: 0.75642 |  0:13:09s
epoch 23 | loss: 0.79794 | eval_custom_logloss: 0.782   |  0:13:45s
epoch 24 | loss: 0.7897  | eval_custom_logloss: 0.77277 |  0:14:20s
epoch 25 | loss: 0.7568  | eval_custom_logloss: 0.66674 |  0:14:54s
epoch 26 | loss: 0.75165 | eval_custom_logloss: 0.65054 |  0:15:28s
epoch 27 | loss: 0.73613 | eval_custom_logloss: 0.76638 |  0:16:03s
epoch 28 | loss: 0.71256 | eval_custom_logloss: 0.71594 |  0:16:37s
epoch 29 | loss: 0.72597 | eval_custom_logloss: 0.69905 |  0:17:11s
epoch 30 | loss: 0.72419 | eval_custom_logloss: 0.77364 |  0:17:45s
epoch 31 | loss: 0.69767 | eval_custom_logloss: 0.7052  |  0:18:19s
epoch 32 | loss: 0.70485 | eval_custom_logloss: 0.70528 |  0:18:53s
epoch 33 | loss: 0.6942  | eval_custom_logloss: 0.77807 |  0:19:26s
epoch 34 | loss: 0.70239 | eval_custom_logloss: 0.6565  |  0:20:00s
epoch 35 | loss: 0.70934 | eval_custom_logloss: 0.70774 |  0:20:34s
epoch 36 | loss: 0.68665 | eval_custom_logloss: 0.61245 |  0:21:07s
epoch 37 | loss: 0.68666 | eval_custom_logloss: 0.63301 |  0:21:41s
epoch 38 | loss: 0.68596 | eval_custom_logloss: 0.75913 |  0:22:14s
epoch 39 | loss: 0.68814 | eval_custom_logloss: 0.62753 |  0:22:48s
epoch 40 | loss: 0.66999 | eval_custom_logloss: 0.73516 |  0:23:21s
epoch 41 | loss: 0.67996 | eval_custom_logloss: 0.71247 |  0:23:55s
epoch 42 | loss: 0.67867 | eval_custom_logloss: 0.64815 |  0:24:29s
epoch 43 | loss: 0.65456 | eval_custom_logloss: 0.6513  |  0:25:02s
epoch 44 | loss: 0.67153 | eval_custom_logloss: 0.75403 |  0:25:36s
epoch 45 | loss: 0.68432 | eval_custom_logloss: 0.70999 |  0:26:09s
epoch 46 | loss: 0.65298 | eval_custom_logloss: 0.78844 |  0:26:43s
epoch 47 | loss: 0.64927 | eval_custom_logloss: 0.77373 |  0:27:16s
epoch 48 | loss: 0.65978 | eval_custom_logloss: 0.81034 |  0:27:50s
epoch 49 | loss: 0.65199 | eval_custom_logloss: 0.72404 |  0:28:23s
epoch 50 | loss: 0.66713 | eval_custom_logloss: 0.7483  |  0:28:58s
epoch 51 | loss: 0.65866 | eval_custom_logloss: 1.06885 |  0:29:33s
epoch 52 | loss: 0.65181 | eval_custom_logloss: 0.64985 |  0:30:06s
epoch 53 | loss: 0.6447  | eval_custom_logloss: 0.67396 |  0:30:40s
epoch 54 | loss: 0.64685 | eval_custom_logloss: 0.83409 |  0:31:14s
epoch 55 | loss: 0.6416  | eval_custom_logloss: 0.70673 |  0:31:48s
epoch 56 | loss: 0.64821 | eval_custom_logloss: 0.6665  |  0:32:21s

Early stopping occurred at epoch 56 with best_epoch = 36 and best_eval_custom_logloss = 0.61245
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62158, 'Log Loss - std': 0.02855579801021149} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 0.62158 and parameters: {'n_d': 60, 'n_steps': 10, 'gamma': 1.8476525501746845, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.1865835523276842, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.61518 | eval_custom_logloss: 1.41172 |  0:00:18s
epoch 1  | loss: 1.17435 | eval_custom_logloss: 1.23817 |  0:00:36s
epoch 2  | loss: 1.08406 | eval_custom_logloss: 1.65983 |  0:00:54s
epoch 3  | loss: 1.04111 | eval_custom_logloss: 1.03731 |  0:01:13s
epoch 4  | loss: 1.01706 | eval_custom_logloss: 1.06461 |  0:01:31s
epoch 5  | loss: 0.97967 | eval_custom_logloss: 1.87925 |  0:01:49s
epoch 6  | loss: 0.96229 | eval_custom_logloss: 0.9939  |  0:02:07s
epoch 7  | loss: 0.9769  | eval_custom_logloss: 0.89122 |  0:02:26s
epoch 8  | loss: 0.93365 | eval_custom_logloss: 0.94743 |  0:02:44s
epoch 9  | loss: 0.92097 | eval_custom_logloss: 0.85187 |  0:03:02s
epoch 10 | loss: 0.89501 | eval_custom_logloss: 0.98091 |  0:03:20s
epoch 11 | loss: 0.89104 | eval_custom_logloss: 0.7716  |  0:03:38s
epoch 12 | loss: 0.88517 | eval_custom_logloss: 1.01362 |  0:03:57s
epoch 13 | loss: 0.86424 | eval_custom_logloss: 0.96714 |  0:04:15s
epoch 14 | loss: 0.87698 | eval_custom_logloss: 0.97657 |  0:04:33s
epoch 15 | loss: 0.85588 | eval_custom_logloss: 0.93483 |  0:04:51s
epoch 16 | loss: 0.87052 | eval_custom_logloss: 0.94643 |  0:05:09s
epoch 17 | loss: 0.83793 | eval_custom_logloss: 0.80651 |  0:05:27s
epoch 18 | loss: 0.82845 | eval_custom_logloss: 0.78702 |  0:05:45s
epoch 19 | loss: 0.84422 | eval_custom_logloss: 0.83951 |  0:06:03s
epoch 20 | loss: 0.8295  | eval_custom_logloss: 0.78454 |  0:06:21s
epoch 21 | loss: 0.81672 | eval_custom_logloss: 0.81249 |  0:06:40s
epoch 22 | loss: 0.81336 | eval_custom_logloss: 0.81398 |  0:06:58s
epoch 23 | loss: 0.79485 | eval_custom_logloss: 0.78179 |  0:07:16s
epoch 24 | loss: 0.79501 | eval_custom_logloss: 0.80563 |  0:07:34s
epoch 25 | loss: 0.8061  | eval_custom_logloss: 0.80257 |  0:07:52s
epoch 26 | loss: 0.7821  | eval_custom_logloss: 0.75882 |  0:08:10s
epoch 27 | loss: 0.79224 | eval_custom_logloss: 0.81412 |  0:08:28s
epoch 28 | loss: 0.80116 | eval_custom_logloss: 0.82904 |  0:08:47s
epoch 29 | loss: 0.78066 | eval_custom_logloss: 1.1239  |  0:09:04s
epoch 30 | loss: 0.76209 | eval_custom_logloss: 0.80009 |  0:09:22s
epoch 31 | loss: 0.78994 | eval_custom_logloss: 0.77011 |  0:09:41s
epoch 32 | loss: 0.7575  | eval_custom_logloss: 0.76476 |  0:09:59s
epoch 33 | loss: 0.77983 | eval_custom_logloss: 0.68768 |  0:10:17s
epoch 34 | loss: 0.76122 | eval_custom_logloss: 0.72415 |  0:10:35s
epoch 35 | loss: 0.76329 | eval_custom_logloss: 1.15643 |  0:10:54s
epoch 36 | loss: 0.76116 | eval_custom_logloss: 0.75758 |  0:11:12s
epoch 37 | loss: 0.74912 | eval_custom_logloss: 0.74821 |  0:11:30s
epoch 38 | loss: 0.74589 | eval_custom_logloss: 0.74201 |  0:11:48s
epoch 39 | loss: 0.74652 | eval_custom_logloss: 0.80668 |  0:12:06s
epoch 40 | loss: 0.7479  | eval_custom_logloss: 0.68703 |  0:12:24s
epoch 41 | loss: 0.74235 | eval_custom_logloss: 0.77372 |  0:12:43s
epoch 42 | loss: 0.74742 | eval_custom_logloss: 0.68679 |  0:13:01s
epoch 43 | loss: 0.7405  | eval_custom_logloss: 0.75067 |  0:13:19s
epoch 44 | loss: 0.75459 | eval_custom_logloss: 0.7585  |  0:13:37s
epoch 45 | loss: 0.73461 | eval_custom_logloss: 0.8037  |  0:13:55s
epoch 46 | loss: 0.73712 | eval_custom_logloss: 0.7777  |  0:14:13s
epoch 47 | loss: 0.73511 | eval_custom_logloss: 0.70263 |  0:14:31s
epoch 48 | loss: 0.73623 | eval_custom_logloss: 0.71901 |  0:14:49s
epoch 49 | loss: 0.72968 | eval_custom_logloss: 0.87398 |  0:15:07s
epoch 50 | loss: 0.74175 | eval_custom_logloss: 0.75308 |  0:15:25s
epoch 51 | loss: 0.72944 | eval_custom_logloss: 0.81515 |  0:15:44s
epoch 52 | loss: 0.72713 | eval_custom_logloss: 1.06178 |  0:16:02s
epoch 53 | loss: 0.73174 | eval_custom_logloss: 0.90591 |  0:16:20s
epoch 54 | loss: 0.72495 | eval_custom_logloss: 0.74844 |  0:16:38s
epoch 55 | loss: 0.72677 | eval_custom_logloss: 0.77985 |  0:16:56s
epoch 56 | loss: 0.73309 | eval_custom_logloss: 0.77705 |  0:17:15s
epoch 57 | loss: 0.7271  | eval_custom_logloss: 0.7205  |  0:17:33s
epoch 58 | loss: 0.72484 | eval_custom_logloss: 0.7877  |  0:17:51s
epoch 59 | loss: 0.71041 | eval_custom_logloss: 0.74111 |  0:18:09s
epoch 60 | loss: 0.72393 | eval_custom_logloss: 0.74381 |  0:18:28s
epoch 61 | loss: 0.70894 | eval_custom_logloss: 0.88642 |  0:18:46s
epoch 62 | loss: 0.71749 | eval_custom_logloss: 0.75502 |  0:19:04s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.68679
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6864, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56219 | eval_custom_logloss: 1.31312 |  0:00:18s
epoch 1  | loss: 1.19194 | eval_custom_logloss: 1.18639 |  0:00:36s
epoch 2  | loss: 1.10917 | eval_custom_logloss: 1.1643  |  0:00:55s
epoch 3  | loss: 1.10205 | eval_custom_logloss: 1.0948  |  0:01:13s
epoch 4  | loss: 1.01326 | eval_custom_logloss: 1.01813 |  0:01:31s
epoch 5  | loss: 0.99387 | eval_custom_logloss: 0.94589 |  0:01:50s
epoch 6  | loss: 1.00401 | eval_custom_logloss: 1.42965 |  0:02:08s
epoch 7  | loss: 1.00887 | eval_custom_logloss: 1.08095 |  0:02:26s
epoch 8  | loss: 0.94754 | eval_custom_logloss: 0.84307 |  0:02:45s
epoch 9  | loss: 0.91613 | eval_custom_logloss: 0.90438 |  0:03:03s
epoch 10 | loss: 0.90382 | eval_custom_logloss: 1.30202 |  0:03:22s
epoch 11 | loss: 0.89149 | eval_custom_logloss: 0.90714 |  0:03:40s
epoch 12 | loss: 0.87305 | eval_custom_logloss: 1.01218 |  0:03:58s
epoch 13 | loss: 0.8265  | eval_custom_logloss: 2.31824 |  0:04:17s
epoch 14 | loss: 0.82896 | eval_custom_logloss: 1.22384 |  0:04:35s
epoch 15 | loss: 0.83172 | eval_custom_logloss: 1.49666 |  0:04:53s
epoch 16 | loss: 0.82705 | eval_custom_logloss: 0.84095 |  0:05:11s
epoch 17 | loss: 0.81635 | eval_custom_logloss: 0.83557 |  0:05:29s
epoch 18 | loss: 0.79174 | eval_custom_logloss: 0.9629  |  0:05:47s
epoch 19 | loss: 0.80259 | eval_custom_logloss: 2.33338 |  0:06:05s
epoch 20 | loss: 0.77345 | eval_custom_logloss: 0.89565 |  0:06:23s
epoch 21 | loss: 0.77553 | eval_custom_logloss: 0.71511 |  0:06:42s
epoch 22 | loss: 0.77799 | eval_custom_logloss: 1.77792 |  0:07:00s
epoch 23 | loss: 0.75654 | eval_custom_logloss: 0.8888  |  0:07:18s
epoch 24 | loss: 0.76754 | eval_custom_logloss: 1.4239  |  0:07:36s
epoch 25 | loss: 0.75741 | eval_custom_logloss: 0.89393 |  0:07:54s
epoch 26 | loss: 0.76812 | eval_custom_logloss: 1.16009 |  0:08:12s
epoch 27 | loss: 0.75922 | eval_custom_logloss: 1.08371 |  0:08:30s
epoch 28 | loss: 0.73893 | eval_custom_logloss: 1.14263 |  0:08:49s
epoch 29 | loss: 0.751   | eval_custom_logloss: 0.72052 |  0:09:07s
epoch 30 | loss: 0.7331  | eval_custom_logloss: 0.81658 |  0:09:25s
epoch 31 | loss: 0.73381 | eval_custom_logloss: 0.80899 |  0:09:43s
epoch 32 | loss: 0.71343 | eval_custom_logloss: 1.48968 |  0:10:01s
epoch 33 | loss: 0.73367 | eval_custom_logloss: 1.11725 |  0:10:19s
epoch 34 | loss: 0.70821 | eval_custom_logloss: 1.0137  |  0:10:37s
epoch 35 | loss: 0.71939 | eval_custom_logloss: 0.80566 |  0:10:55s
epoch 36 | loss: 0.72083 | eval_custom_logloss: 1.02112 |  0:11:13s
epoch 37 | loss: 0.71281 | eval_custom_logloss: 1.10312 |  0:11:31s
epoch 38 | loss: 0.71358 | eval_custom_logloss: 1.26655 |  0:11:49s
epoch 39 | loss: 0.69946 | eval_custom_logloss: 0.9443  |  0:12:08s
epoch 40 | loss: 0.73258 | eval_custom_logloss: 0.84716 |  0:12:26s
epoch 41 | loss: 0.70462 | eval_custom_logloss: 1.14721 |  0:12:44s

Early stopping occurred at epoch 41 with best_epoch = 21 and best_eval_custom_logloss = 0.71511
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7, 'Log Loss - std': 0.013600000000000001} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.6469  | eval_custom_logloss: 1.32231 |  0:00:17s
epoch 1  | loss: 1.27779 | eval_custom_logloss: 1.09942 |  0:00:35s
epoch 2  | loss: 1.12125 | eval_custom_logloss: 1.08492 |  0:00:53s
epoch 3  | loss: 1.03907 | eval_custom_logloss: 1.12972 |  0:01:12s
epoch 4  | loss: 0.99149 | eval_custom_logloss: 1.06402 |  0:01:30s
epoch 5  | loss: 0.99765 | eval_custom_logloss: 0.89916 |  0:01:48s
epoch 6  | loss: 0.96305 | eval_custom_logloss: 0.84361 |  0:02:06s
epoch 7  | loss: 0.94385 | eval_custom_logloss: 0.91395 |  0:02:24s
epoch 8  | loss: 0.8978  | eval_custom_logloss: 0.84798 |  0:02:42s
epoch 9  | loss: 0.91671 | eval_custom_logloss: 0.83421 |  0:03:00s
epoch 10 | loss: 0.90812 | eval_custom_logloss: 1.20445 |  0:03:19s
epoch 11 | loss: 0.89558 | eval_custom_logloss: 0.89422 |  0:03:37s
epoch 12 | loss: 0.87209 | eval_custom_logloss: 0.86818 |  0:03:55s
epoch 13 | loss: 0.8628  | eval_custom_logloss: 0.79544 |  0:04:13s
epoch 14 | loss: 0.86194 | eval_custom_logloss: 0.82459 |  0:04:32s
epoch 15 | loss: 0.84897 | eval_custom_logloss: 0.80594 |  0:04:50s
epoch 16 | loss: 0.85076 | eval_custom_logloss: 0.91196 |  0:05:08s
epoch 17 | loss: 0.83216 | eval_custom_logloss: 0.86635 |  0:05:26s
epoch 18 | loss: 0.82987 | eval_custom_logloss: 0.77435 |  0:05:44s
epoch 19 | loss: 0.82472 | eval_custom_logloss: 0.81952 |  0:06:02s
epoch 20 | loss: 0.80865 | eval_custom_logloss: 0.86154 |  0:06:20s
epoch 21 | loss: 0.80396 | eval_custom_logloss: 0.81186 |  0:06:38s
epoch 22 | loss: 0.8006  | eval_custom_logloss: 1.18067 |  0:06:56s
epoch 23 | loss: 0.83818 | eval_custom_logloss: 0.78487 |  0:07:14s
epoch 24 | loss: 0.8177  | eval_custom_logloss: 1.26743 |  0:07:32s
epoch 25 | loss: 0.77728 | eval_custom_logloss: 0.77431 |  0:07:50s
epoch 26 | loss: 0.77897 | eval_custom_logloss: 0.7626  |  0:08:08s
epoch 27 | loss: 0.7831  | eval_custom_logloss: 0.99877 |  0:08:26s
epoch 28 | loss: 0.79262 | eval_custom_logloss: 0.80449 |  0:08:44s
epoch 29 | loss: 0.77275 | eval_custom_logloss: 1.49468 |  0:09:02s
epoch 30 | loss: 0.77885 | eval_custom_logloss: 0.92647 |  0:09:20s
epoch 31 | loss: 0.81175 | eval_custom_logloss: 0.76852 |  0:09:38s
epoch 32 | loss: 0.7906  | eval_custom_logloss: 0.92519 |  0:09:57s
epoch 33 | loss: 0.77914 | eval_custom_logloss: 0.72812 |  0:10:14s
epoch 34 | loss: 0.78032 | eval_custom_logloss: 0.91233 |  0:10:32s
epoch 35 | loss: 0.76872 | eval_custom_logloss: 0.82698 |  0:10:50s
epoch 36 | loss: 0.74137 | eval_custom_logloss: 0.90621 |  0:11:09s
epoch 37 | loss: 0.74201 | eval_custom_logloss: 0.79866 |  0:11:28s
epoch 38 | loss: 0.75599 | eval_custom_logloss: 0.89141 |  0:11:48s
epoch 39 | loss: 0.76971 | eval_custom_logloss: 0.81875 |  0:12:07s
epoch 40 | loss: 0.75239 | eval_custom_logloss: 0.83186 |  0:12:27s
epoch 41 | loss: 0.73331 | eval_custom_logloss: 0.71611 |  0:12:46s
epoch 42 | loss: 0.75704 | eval_custom_logloss: 0.86007 |  0:13:06s
epoch 43 | loss: 0.75395 | eval_custom_logloss: 0.77048 |  0:13:25s
epoch 44 | loss: 0.74416 | eval_custom_logloss: 0.69789 |  0:13:45s
epoch 45 | loss: 0.72014 | eval_custom_logloss: 0.70245 |  0:14:04s
epoch 46 | loss: 0.71543 | eval_custom_logloss: 0.96234 |  0:14:24s
epoch 47 | loss: 0.73105 | eval_custom_logloss: 0.71187 |  0:14:43s
epoch 48 | loss: 0.72929 | eval_custom_logloss: 0.6641  |  0:15:02s
epoch 49 | loss: 0.71614 | eval_custom_logloss: 0.68169 |  0:15:22s
epoch 50 | loss: 0.71852 | eval_custom_logloss: 0.87923 |  0:15:41s
epoch 51 | loss: 0.71426 | eval_custom_logloss: 0.78342 |  0:16:01s
epoch 52 | loss: 0.7408  | eval_custom_logloss: 0.74688 |  0:16:19s
epoch 53 | loss: 0.71804 | eval_custom_logloss: 0.83951 |  0:16:37s
epoch 54 | loss: 0.74234 | eval_custom_logloss: 0.89437 |  0:16:55s
epoch 55 | loss: 0.73067 | eval_custom_logloss: 1.19606 |  0:17:14s
epoch 56 | loss: 0.72606 | eval_custom_logloss: 0.72065 |  0:17:32s
epoch 57 | loss: 0.73098 | eval_custom_logloss: 0.92157 |  0:17:50s
epoch 58 | loss: 0.7172  | eval_custom_logloss: 1.07696 |  0:18:08s
epoch 59 | loss: 0.71746 | eval_custom_logloss: 0.88768 |  0:18:26s
epoch 60 | loss: 0.74932 | eval_custom_logloss: 1.28416 |  0:18:44s
epoch 61 | loss: 0.70873 | eval_custom_logloss: 0.8116  |  0:19:02s
epoch 62 | loss: 0.69937 | eval_custom_logloss: 0.72425 |  0:19:20s
epoch 63 | loss: 0.69802 | eval_custom_logloss: 0.67257 |  0:19:39s
epoch 64 | loss: 0.6929  | eval_custom_logloss: 0.79602 |  0:19:57s
epoch 65 | loss: 0.68957 | eval_custom_logloss: 0.66246 |  0:20:15s
epoch 66 | loss: 0.70384 | eval_custom_logloss: 0.80936 |  0:20:33s
epoch 67 | loss: 0.69055 | eval_custom_logloss: 0.74855 |  0:20:52s
epoch 68 | loss: 0.69911 | eval_custom_logloss: 0.99621 |  0:21:10s
epoch 69 | loss: 0.67675 | eval_custom_logloss: 0.87414 |  0:21:28s
epoch 70 | loss: 0.68406 | eval_custom_logloss: 0.75878 |  0:21:46s
epoch 71 | loss: 0.69645 | eval_custom_logloss: 0.69353 |  0:22:05s
epoch 72 | loss: 0.70789 | eval_custom_logloss: 0.90798 |  0:22:23s
epoch 73 | loss: 0.68675 | eval_custom_logloss: 0.66118 |  0:22:41s
epoch 74 | loss: 0.67477 | eval_custom_logloss: 1.08204 |  0:22:59s
epoch 75 | loss: 0.68276 | eval_custom_logloss: 1.18047 |  0:23:17s
epoch 76 | loss: 0.689   | eval_custom_logloss: 0.89676 |  0:23:35s
epoch 77 | loss: 0.67521 | eval_custom_logloss: 0.69095 |  0:23:54s
epoch 78 | loss: 0.68325 | eval_custom_logloss: 0.86964 |  0:24:12s
epoch 79 | loss: 0.676   | eval_custom_logloss: 0.843   |  0:24:30s
epoch 80 | loss: 0.68745 | eval_custom_logloss: 0.68723 |  0:24:48s
epoch 81 | loss: 0.6691  | eval_custom_logloss: 0.8869  |  0:25:06s
epoch 82 | loss: 0.68981 | eval_custom_logloss: 0.79345 |  0:25:24s
epoch 83 | loss: 0.67468 | eval_custom_logloss: 0.79907 |  0:25:42s
epoch 84 | loss: 0.66267 | eval_custom_logloss: 0.62084 |  0:26:00s
epoch 85 | loss: 0.6626  | eval_custom_logloss: 0.76899 |  0:26:18s
epoch 86 | loss: 0.66756 | eval_custom_logloss: 0.7442  |  0:26:36s
epoch 87 | loss: 0.66818 | eval_custom_logloss: 1.04281 |  0:26:54s
epoch 88 | loss: 0.66077 | eval_custom_logloss: 0.77974 |  0:27:12s
epoch 89 | loss: 0.67554 | eval_custom_logloss: 0.69249 |  0:27:30s
epoch 90 | loss: 0.65602 | eval_custom_logloss: 0.71439 |  0:27:49s
epoch 91 | loss: 0.66379 | eval_custom_logloss: 0.74868 |  0:28:07s
epoch 92 | loss: 0.66794 | eval_custom_logloss: 0.87475 |  0:28:25s
epoch 93 | loss: 0.66234 | eval_custom_logloss: 0.74182 |  0:28:43s
epoch 94 | loss: 0.67459 | eval_custom_logloss: 0.83483 |  0:29:02s
epoch 95 | loss: 0.65823 | eval_custom_logloss: 0.76181 |  0:29:20s
epoch 96 | loss: 0.6605  | eval_custom_logloss: 0.85318 |  0:29:38s
epoch 97 | loss: 0.68527 | eval_custom_logloss: 0.7088  |  0:29:56s
epoch 98 | loss: 0.65515 | eval_custom_logloss: 0.86871 |  0:30:14s
epoch 99 | loss: 0.65723 | eval_custom_logloss: 0.88504 |  0:30:32s
Stop training because you reached max_epochs = 100 with best_epoch = 84 and best_eval_custom_logloss = 0.62084
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6728999999999999, 'Log Loss - std': 0.03990146196152049} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.61499 | eval_custom_logloss: 1.31423 |  0:00:17s
epoch 1  | loss: 1.25225 | eval_custom_logloss: 1.57913 |  0:00:35s
epoch 2  | loss: 1.20908 | eval_custom_logloss: 1.14595 |  0:00:53s
epoch 3  | loss: 1.15398 | eval_custom_logloss: 1.16626 |  0:01:12s
epoch 4  | loss: 1.07749 | eval_custom_logloss: 1.09972 |  0:01:30s
epoch 5  | loss: 1.07103 | eval_custom_logloss: 0.9732  |  0:01:48s
epoch 6  | loss: 1.02055 | eval_custom_logloss: 0.89764 |  0:02:06s
epoch 7  | loss: 1.00095 | eval_custom_logloss: 0.92608 |  0:02:23s
epoch 8  | loss: 0.96129 | eval_custom_logloss: 1.25649 |  0:02:41s
epoch 9  | loss: 0.95865 | eval_custom_logloss: 0.89907 |  0:02:59s
epoch 10 | loss: 0.94747 | eval_custom_logloss: 0.82305 |  0:03:17s
epoch 11 | loss: 0.91391 | eval_custom_logloss: 0.89634 |  0:03:35s
epoch 12 | loss: 0.90987 | eval_custom_logloss: 0.94616 |  0:03:53s
epoch 13 | loss: 0.93465 | eval_custom_logloss: 1.04662 |  0:04:11s
epoch 14 | loss: 0.92962 | eval_custom_logloss: 0.85792 |  0:04:28s
epoch 15 | loss: 0.90764 | eval_custom_logloss: 0.85999 |  0:04:46s
epoch 16 | loss: 0.89715 | eval_custom_logloss: 0.93864 |  0:05:04s
epoch 17 | loss: 0.88232 | eval_custom_logloss: 0.79334 |  0:05:22s
epoch 18 | loss: 0.86332 | eval_custom_logloss: 0.80246 |  0:05:40s
epoch 19 | loss: 0.85116 | eval_custom_logloss: 0.89733 |  0:05:58s
epoch 20 | loss: 0.83493 | eval_custom_logloss: 1.15011 |  0:06:16s
epoch 21 | loss: 0.83102 | eval_custom_logloss: 0.82656 |  0:06:34s
epoch 22 | loss: 0.82513 | eval_custom_logloss: 0.84487 |  0:06:51s
epoch 23 | loss: 0.79443 | eval_custom_logloss: 1.42004 |  0:07:09s
epoch 24 | loss: 0.80762 | eval_custom_logloss: 0.78286 |  0:07:27s
epoch 25 | loss: 0.78429 | eval_custom_logloss: 0.95339 |  0:07:45s
epoch 26 | loss: 0.79065 | eval_custom_logloss: 1.25614 |  0:08:03s
epoch 27 | loss: 0.78862 | eval_custom_logloss: 0.76082 |  0:08:21s
epoch 28 | loss: 0.77617 | eval_custom_logloss: 0.84503 |  0:08:38s
epoch 29 | loss: 0.79113 | eval_custom_logloss: 1.20579 |  0:08:56s
epoch 30 | loss: 0.77726 | eval_custom_logloss: 0.86529 |  0:09:15s
epoch 31 | loss: 0.76387 | eval_custom_logloss: 0.77393 |  0:09:33s
epoch 32 | loss: 0.76182 | eval_custom_logloss: 0.68935 |  0:09:51s
epoch 33 | loss: 0.76463 | eval_custom_logloss: 0.72678 |  0:10:09s
epoch 34 | loss: 0.74828 | eval_custom_logloss: 0.88197 |  0:10:27s
epoch 35 | loss: 0.75134 | eval_custom_logloss: 0.7425  |  0:10:45s
epoch 36 | loss: 0.74418 | eval_custom_logloss: 0.66486 |  0:11:03s
epoch 37 | loss: 0.74836 | eval_custom_logloss: 0.88512 |  0:11:21s
epoch 38 | loss: 0.74658 | eval_custom_logloss: 0.68673 |  0:11:39s
epoch 39 | loss: 0.7293  | eval_custom_logloss: 1.61379 |  0:11:57s
epoch 40 | loss: 0.73034 | eval_custom_logloss: 0.90548 |  0:12:15s
epoch 41 | loss: 0.71295 | eval_custom_logloss: 0.7882  |  0:12:33s
epoch 42 | loss: 0.72641 | eval_custom_logloss: 0.69782 |  0:12:51s
epoch 43 | loss: 0.72648 | eval_custom_logloss: 0.85072 |  0:13:09s
epoch 44 | loss: 0.72609 | eval_custom_logloss: 0.90203 |  0:13:27s
epoch 45 | loss: 0.70715 | eval_custom_logloss: 0.71107 |  0:13:45s
epoch 46 | loss: 0.71719 | eval_custom_logloss: 0.86036 |  0:14:03s
epoch 47 | loss: 0.71444 | eval_custom_logloss: 0.6398  |  0:14:21s
epoch 48 | loss: 0.72623 | eval_custom_logloss: 0.85805 |  0:14:39s
epoch 49 | loss: 0.70266 | eval_custom_logloss: 1.29261 |  0:14:57s
epoch 50 | loss: 0.71053 | eval_custom_logloss: 0.73988 |  0:15:15s
epoch 51 | loss: 0.70591 | eval_custom_logloss: 0.83521 |  0:15:33s
epoch 52 | loss: 0.71623 | eval_custom_logloss: 0.70854 |  0:15:51s
epoch 53 | loss: 0.70584 | eval_custom_logloss: 0.86307 |  0:16:09s
epoch 54 | loss: 0.69593 | eval_custom_logloss: 0.73221 |  0:16:27s
epoch 55 | loss: 0.72343 | eval_custom_logloss: 0.74472 |  0:16:45s
epoch 56 | loss: 0.70368 | eval_custom_logloss: 0.80332 |  0:17:03s
epoch 57 | loss: 0.69913 | eval_custom_logloss: 1.53606 |  0:17:21s
epoch 58 | loss: 0.70328 | eval_custom_logloss: 0.7536  |  0:17:39s
epoch 59 | loss: 0.69474 | eval_custom_logloss: 0.88122 |  0:17:57s
epoch 60 | loss: 0.69157 | eval_custom_logloss: 0.64849 |  0:18:15s
epoch 61 | loss: 0.6886  | eval_custom_logloss: 0.82671 |  0:18:33s
epoch 62 | loss: 0.69905 | eval_custom_logloss: 0.93775 |  0:18:51s
epoch 63 | loss: 0.69334 | eval_custom_logloss: 1.15005 |  0:19:09s
epoch 64 | loss: 0.68887 | eval_custom_logloss: 0.75008 |  0:19:27s
epoch 65 | loss: 0.67843 | eval_custom_logloss: 0.7328  |  0:19:45s
epoch 66 | loss: 0.68033 | eval_custom_logloss: 0.91209 |  0:20:03s
epoch 67 | loss: 0.67756 | eval_custom_logloss: 0.89516 |  0:20:21s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.6398
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.664225, 'Log Loss - std': 0.0376810545898068} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66633 | eval_custom_logloss: 2.17751 |  0:00:18s
epoch 1  | loss: 1.27456 | eval_custom_logloss: 1.17785 |  0:00:36s
epoch 2  | loss: 1.21914 | eval_custom_logloss: 1.74911 |  0:00:54s
epoch 3  | loss: 1.15486 | eval_custom_logloss: 1.09698 |  0:01:12s
epoch 4  | loss: 1.11196 | eval_custom_logloss: 1.40838 |  0:01:30s
epoch 5  | loss: 1.09892 | eval_custom_logloss: 1.08928 |  0:01:48s
epoch 6  | loss: 1.06837 | eval_custom_logloss: 1.02922 |  0:02:06s
epoch 7  | loss: 1.05239 | eval_custom_logloss: 0.96299 |  0:02:24s
epoch 8  | loss: 1.03171 | eval_custom_logloss: 0.94911 |  0:02:42s
epoch 9  | loss: 1.03842 | eval_custom_logloss: 1.02467 |  0:03:00s
epoch 10 | loss: 1.00103 | eval_custom_logloss: 0.94012 |  0:03:18s
epoch 11 | loss: 0.97924 | eval_custom_logloss: 0.94847 |  0:03:36s
epoch 12 | loss: 0.96077 | eval_custom_logloss: 1.04175 |  0:03:54s
epoch 13 | loss: 0.94073 | eval_custom_logloss: 0.86033 |  0:04:12s
epoch 14 | loss: 0.92529 | eval_custom_logloss: 1.10597 |  0:04:30s
epoch 15 | loss: 0.90106 | eval_custom_logloss: 0.89992 |  0:04:48s
epoch 16 | loss: 0.90571 | eval_custom_logloss: 0.85493 |  0:05:06s
epoch 17 | loss: 0.88136 | eval_custom_logloss: 1.21082 |  0:05:24s
epoch 18 | loss: 0.86506 | eval_custom_logloss: 0.89286 |  0:05:42s
epoch 19 | loss: 0.86311 | eval_custom_logloss: 0.80294 |  0:06:00s
epoch 20 | loss: 0.85372 | eval_custom_logloss: 0.95119 |  0:06:19s
epoch 21 | loss: 0.88091 | eval_custom_logloss: 0.90298 |  0:06:37s
epoch 22 | loss: 0.86454 | eval_custom_logloss: 0.8439  |  0:06:55s
epoch 23 | loss: 0.84141 | eval_custom_logloss: 0.89906 |  0:07:13s
epoch 24 | loss: 0.85538 | eval_custom_logloss: 0.82474 |  0:07:31s
epoch 25 | loss: 0.83269 | eval_custom_logloss: 0.95303 |  0:07:49s
epoch 26 | loss: 0.82289 | eval_custom_logloss: 0.787   |  0:08:07s
epoch 27 | loss: 0.82393 | eval_custom_logloss: 0.88198 |  0:08:25s
epoch 28 | loss: 0.83421 | eval_custom_logloss: 1.32383 |  0:08:43s
epoch 29 | loss: 0.82275 | eval_custom_logloss: 0.93486 |  0:09:02s
epoch 30 | loss: 0.80663 | eval_custom_logloss: 1.05633 |  0:09:20s
epoch 31 | loss: 0.82327 | eval_custom_logloss: 0.95699 |  0:09:38s
epoch 32 | loss: 0.82293 | eval_custom_logloss: 0.82796 |  0:09:56s
epoch 33 | loss: 0.80817 | eval_custom_logloss: 0.80776 |  0:10:14s
epoch 34 | loss: 0.77997 | eval_custom_logloss: 0.79865 |  0:10:32s
epoch 35 | loss: 0.7854  | eval_custom_logloss: 0.83153 |  0:10:50s
epoch 36 | loss: 0.76013 | eval_custom_logloss: 0.74434 |  0:11:08s
epoch 37 | loss: 0.76036 | eval_custom_logloss: 0.89761 |  0:11:26s
epoch 38 | loss: 0.75265 | eval_custom_logloss: 0.94515 |  0:11:44s
epoch 39 | loss: 0.74729 | eval_custom_logloss: 0.82345 |  0:12:02s
epoch 40 | loss: 0.75883 | eval_custom_logloss: 0.94949 |  0:12:21s
epoch 41 | loss: 0.7395  | eval_custom_logloss: 0.77507 |  0:12:39s
epoch 42 | loss: 0.74768 | eval_custom_logloss: 0.91636 |  0:12:57s
epoch 43 | loss: 0.73493 | eval_custom_logloss: 0.78515 |  0:13:15s
epoch 44 | loss: 0.72246 | eval_custom_logloss: 0.90113 |  0:13:33s
epoch 45 | loss: 0.73184 | eval_custom_logloss: 0.88005 |  0:13:51s
epoch 46 | loss: 0.73228 | eval_custom_logloss: 0.91175 |  0:14:09s
epoch 47 | loss: 0.73878 | eval_custom_logloss: 1.11002 |  0:14:27s
epoch 48 | loss: 0.72784 | eval_custom_logloss: 0.69882 |  0:14:47s
epoch 49 | loss: 0.72717 | eval_custom_logloss: 1.12492 |  0:15:06s
epoch 50 | loss: 0.71973 | eval_custom_logloss: 0.702   |  0:15:26s
epoch 51 | loss: 0.72888 | eval_custom_logloss: 0.73313 |  0:15:45s
epoch 52 | loss: 0.73441 | eval_custom_logloss: 0.82316 |  0:16:05s
epoch 53 | loss: 0.73364 | eval_custom_logloss: 0.6836  |  0:16:24s
epoch 54 | loss: 0.72235 | eval_custom_logloss: 0.95134 |  0:16:43s
epoch 55 | loss: 0.75375 | eval_custom_logloss: 0.81818 |  0:17:02s
epoch 56 | loss: 0.73427 | eval_custom_logloss: 0.67121 |  0:17:20s
epoch 57 | loss: 0.73222 | eval_custom_logloss: 1.02994 |  0:17:38s
epoch 58 | loss: 0.75315 | eval_custom_logloss: 0.99158 |  0:17:56s
epoch 59 | loss: 0.7296  | eval_custom_logloss: 0.82005 |  0:18:14s
epoch 60 | loss: 0.72723 | eval_custom_logloss: 0.73294 |  0:18:32s
epoch 61 | loss: 0.72037 | eval_custom_logloss: 0.77961 |  0:18:50s
epoch 62 | loss: 0.72073 | eval_custom_logloss: 0.79613 |  0:19:08s
epoch 63 | loss: 0.71467 | eval_custom_logloss: 0.804   |  0:19:27s
epoch 64 | loss: 0.70878 | eval_custom_logloss: 0.83347 |  0:19:45s
epoch 65 | loss: 0.71955 | eval_custom_logloss: 0.91011 |  0:20:03s
epoch 66 | loss: 0.71072 | eval_custom_logloss: 0.79135 |  0:20:21s
epoch 67 | loss: 0.71494 | eval_custom_logloss: 0.81031 |  0:20:39s
epoch 68 | loss: 0.71211 | eval_custom_logloss: 0.87017 |  0:20:57s
epoch 69 | loss: 0.7147  | eval_custom_logloss: 0.85586 |  0:21:16s
epoch 70 | loss: 0.70911 | eval_custom_logloss: 0.67375 |  0:21:34s
epoch 71 | loss: 0.71166 | eval_custom_logloss: 0.68409 |  0:21:52s
epoch 72 | loss: 0.71654 | eval_custom_logloss: 0.79155 |  0:22:10s
epoch 73 | loss: 0.69727 | eval_custom_logloss: 1.24234 |  0:22:28s
epoch 74 | loss: 0.69069 | eval_custom_logloss: 0.67502 |  0:22:46s
epoch 75 | loss: 0.69395 | eval_custom_logloss: 0.69741 |  0:23:04s
epoch 76 | loss: 0.70502 | eval_custom_logloss: 0.75835 |  0:23:22s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.67121
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6651999999999999, 'Log Loss - std': 0.03375932463779452} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 0.6651999999999999 and parameters: {'n_d': 47, 'n_steps': 6, 'gamma': 1.8016980795485913, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.017113069815339367, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax', 'n_a': 11, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65627 | eval_custom_logloss: 1.15838 |  0:00:18s
epoch 1  | loss: 1.21275 | eval_custom_logloss: 1.30169 |  0:00:37s
epoch 2  | loss: 1.06163 | eval_custom_logloss: 0.88014 |  0:00:55s
epoch 3  | loss: 0.99597 | eval_custom_logloss: 0.85053 |  0:01:14s
epoch 4  | loss: 0.95541 | eval_custom_logloss: 0.8047  |  0:01:32s
epoch 5  | loss: 0.93192 | eval_custom_logloss: 0.82044 |  0:01:51s
epoch 6  | loss: 0.93164 | eval_custom_logloss: 0.88648 |  0:02:09s
epoch 7  | loss: 0.91678 | eval_custom_logloss: 0.85181 |  0:02:28s
epoch 8  | loss: 0.90291 | eval_custom_logloss: 0.77968 |  0:02:47s
epoch 9  | loss: 0.90243 | eval_custom_logloss: 0.79977 |  0:03:05s
epoch 10 | loss: 0.89287 | eval_custom_logloss: 0.87153 |  0:03:24s
epoch 11 | loss: 0.87465 | eval_custom_logloss: 0.79599 |  0:03:43s
epoch 12 | loss: 0.85458 | eval_custom_logloss: 0.70686 |  0:04:01s
epoch 13 | loss: 0.83061 | eval_custom_logloss: 1.00153 |  0:04:20s
epoch 14 | loss: 0.82044 | eval_custom_logloss: 0.76158 |  0:04:39s
epoch 15 | loss: 0.80353 | eval_custom_logloss: 0.85831 |  0:04:57s
epoch 16 | loss: 0.80406 | eval_custom_logloss: 0.80178 |  0:05:16s
epoch 17 | loss: 0.78427 | eval_custom_logloss: 0.95364 |  0:05:34s
epoch 18 | loss: 0.7837  | eval_custom_logloss: 0.8375  |  0:05:53s
epoch 19 | loss: 0.79221 | eval_custom_logloss: 0.67249 |  0:06:12s
epoch 20 | loss: 0.77304 | eval_custom_logloss: 0.6848  |  0:06:30s
epoch 21 | loss: 0.79795 | eval_custom_logloss: 0.7804  |  0:06:49s
epoch 22 | loss: 0.77448 | eval_custom_logloss: 0.97666 |  0:07:07s
epoch 23 | loss: 0.77024 | eval_custom_logloss: 0.7023  |  0:07:26s
epoch 24 | loss: 0.75351 | eval_custom_logloss: 0.77191 |  0:07:45s
epoch 25 | loss: 0.77032 | eval_custom_logloss: 1.08534 |  0:08:03s
epoch 26 | loss: 0.78619 | eval_custom_logloss: 0.97799 |  0:08:22s
epoch 27 | loss: 0.76712 | eval_custom_logloss: 0.6807  |  0:08:41s
epoch 28 | loss: 0.75587 | eval_custom_logloss: 0.78064 |  0:09:00s
epoch 29 | loss: 0.73497 | eval_custom_logloss: 0.86325 |  0:09:19s
epoch 30 | loss: 0.77767 | eval_custom_logloss: 0.71886 |  0:09:37s
epoch 31 | loss: 0.75075 | eval_custom_logloss: 0.97349 |  0:09:56s
epoch 32 | loss: 0.72752 | eval_custom_logloss: 0.71712 |  0:10:15s
epoch 33 | loss: 0.74521 | eval_custom_logloss: 0.66267 |  0:10:33s
epoch 34 | loss: 0.72043 | eval_custom_logloss: 0.69002 |  0:10:52s
epoch 35 | loss: 0.71634 | eval_custom_logloss: 0.77623 |  0:11:11s
epoch 36 | loss: 0.70314 | eval_custom_logloss: 0.83602 |  0:11:29s
epoch 37 | loss: 0.70856 | eval_custom_logloss: 0.62498 |  0:11:48s
epoch 38 | loss: 0.69423 | eval_custom_logloss: 0.73821 |  0:12:07s
epoch 39 | loss: 0.71404 | eval_custom_logloss: 0.6599  |  0:12:26s
epoch 40 | loss: 0.69125 | eval_custom_logloss: 0.71468 |  0:12:44s
epoch 41 | loss: 0.69465 | eval_custom_logloss: 0.73635 |  0:13:03s
epoch 42 | loss: 0.69368 | eval_custom_logloss: 0.81078 |  0:13:22s
epoch 43 | loss: 0.71406 | eval_custom_logloss: 0.79229 |  0:13:40s
epoch 44 | loss: 0.71044 | eval_custom_logloss: 0.72132 |  0:13:59s
epoch 45 | loss: 0.70133 | eval_custom_logloss: 0.69085 |  0:14:17s
epoch 46 | loss: 0.71672 | eval_custom_logloss: 0.9903  |  0:14:36s
epoch 47 | loss: 0.70334 | eval_custom_logloss: 0.81544 |  0:14:54s
epoch 48 | loss: 0.70042 | eval_custom_logloss: 0.75247 |  0:15:13s
epoch 49 | loss: 0.6921  | eval_custom_logloss: 0.6613  |  0:15:32s
epoch 50 | loss: 0.72005 | eval_custom_logloss: 1.02399 |  0:15:50s
epoch 51 | loss: 0.70362 | eval_custom_logloss: 0.74418 |  0:16:09s
epoch 52 | loss: 0.69382 | eval_custom_logloss: 0.70024 |  0:16:27s
epoch 53 | loss: 0.69234 | eval_custom_logloss: 0.82248 |  0:16:46s
epoch 54 | loss: 0.69138 | eval_custom_logloss: 0.93884 |  0:17:04s
epoch 55 | loss: 0.69538 | eval_custom_logloss: 1.10968 |  0:17:23s
epoch 56 | loss: 0.69136 | eval_custom_logloss: 0.61984 |  0:17:41s
epoch 57 | loss: 0.69646 | eval_custom_logloss: 0.77945 |  0:18:00s
epoch 58 | loss: 0.71357 | eval_custom_logloss: 0.7196  |  0:18:19s
epoch 59 | loss: 0.69944 | eval_custom_logloss: 0.91853 |  0:18:37s
epoch 60 | loss: 0.69192 | eval_custom_logloss: 0.76599 |  0:18:56s
epoch 61 | loss: 0.68778 | eval_custom_logloss: 0.74109 |  0:19:14s
epoch 62 | loss: 0.68806 | eval_custom_logloss: 0.66095 |  0:19:32s
epoch 63 | loss: 0.68524 | eval_custom_logloss: 0.70188 |  0:19:51s
epoch 64 | loss: 0.68154 | eval_custom_logloss: 0.61992 |  0:20:09s
epoch 65 | loss: 0.68455 | eval_custom_logloss: 0.64019 |  0:20:28s
epoch 66 | loss: 0.67412 | eval_custom_logloss: 0.71563 |  0:20:47s
epoch 67 | loss: 0.73005 | eval_custom_logloss: 0.69447 |  0:21:05s
epoch 68 | loss: 0.69391 | eval_custom_logloss: 0.64421 |  0:21:24s
epoch 69 | loss: 0.69383 | eval_custom_logloss: 0.69464 |  0:21:43s
epoch 70 | loss: 0.68866 | eval_custom_logloss: 0.72404 |  0:22:01s
epoch 71 | loss: 0.69103 | eval_custom_logloss: 0.63125 |  0:22:20s
epoch 72 | loss: 0.69603 | eval_custom_logloss: 0.66302 |  0:22:38s
epoch 73 | loss: 0.7071  | eval_custom_logloss: 0.65292 |  0:22:57s
epoch 74 | loss: 0.68471 | eval_custom_logloss: 0.69114 |  0:23:15s
epoch 75 | loss: 0.69698 | eval_custom_logloss: 0.67003 |  0:23:34s
epoch 76 | loss: 0.69152 | eval_custom_logloss: 0.77593 |  0:23:52s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.61984
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6187, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax', 'n_a': 11, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71364 | eval_custom_logloss: 1.2312  |  0:00:18s
epoch 1  | loss: 1.24507 | eval_custom_logloss: 1.10548 |  0:00:36s
epoch 2  | loss: 1.09266 | eval_custom_logloss: 0.90369 |  0:00:55s
epoch 3  | loss: 0.99555 | eval_custom_logloss: 0.88628 |  0:01:13s
epoch 4  | loss: 0.94787 | eval_custom_logloss: 0.87579 |  0:01:32s
epoch 5  | loss: 0.92915 | eval_custom_logloss: 0.82579 |  0:01:51s
epoch 6  | loss: 0.9084  | eval_custom_logloss: 0.85791 |  0:02:09s
epoch 7  | loss: 0.92349 | eval_custom_logloss: 0.7959  |  0:02:28s
epoch 8  | loss: 0.9018  | eval_custom_logloss: 0.78939 |  0:02:46s
epoch 9  | loss: 0.9     | eval_custom_logloss: 0.76389 |  0:03:04s
epoch 10 | loss: 0.86893 | eval_custom_logloss: 0.86516 |  0:03:23s
epoch 11 | loss: 0.8662  | eval_custom_logloss: 0.77218 |  0:03:41s
epoch 12 | loss: 0.85093 | eval_custom_logloss: 0.8767  |  0:03:59s
epoch 13 | loss: 0.8455  | eval_custom_logloss: 0.77834 |  0:04:18s
epoch 14 | loss: 0.84192 | eval_custom_logloss: 0.88664 |  0:04:36s
epoch 15 | loss: 0.82247 | eval_custom_logloss: 0.76989 |  0:04:55s
epoch 16 | loss: 0.81097 | eval_custom_logloss: 0.74061 |  0:05:13s
epoch 17 | loss: 0.80692 | eval_custom_logloss: 0.84642 |  0:05:32s
epoch 18 | loss: 0.80174 | eval_custom_logloss: 0.87378 |  0:05:50s
epoch 19 | loss: 0.78863 | eval_custom_logloss: 0.90803 |  0:06:09s
epoch 20 | loss: 0.78253 | eval_custom_logloss: 0.71316 |  0:06:27s
epoch 21 | loss: 0.76401 | eval_custom_logloss: 0.66906 |  0:06:46s
epoch 22 | loss: 0.75949 | eval_custom_logloss: 0.87542 |  0:07:04s
epoch 23 | loss: 0.75081 | eval_custom_logloss: 0.71008 |  0:07:23s
epoch 24 | loss: 0.74268 | eval_custom_logloss: 0.80741 |  0:07:41s
epoch 25 | loss: 0.75182 | eval_custom_logloss: 0.72591 |  0:08:00s
epoch 26 | loss: 0.73418 | eval_custom_logloss: 0.72472 |  0:08:18s
epoch 27 | loss: 0.72337 | eval_custom_logloss: 0.87086 |  0:08:37s
epoch 28 | loss: 0.72724 | eval_custom_logloss: 0.74979 |  0:08:55s
epoch 29 | loss: 0.72737 | eval_custom_logloss: 0.75419 |  0:09:14s
epoch 30 | loss: 0.71201 | eval_custom_logloss: 0.7089  |  0:09:32s
epoch 31 | loss: 0.71621 | eval_custom_logloss: 0.85953 |  0:09:51s
epoch 32 | loss: 0.71159 | eval_custom_logloss: 0.92666 |  0:10:09s
epoch 33 | loss: 0.71246 | eval_custom_logloss: 0.75109 |  0:10:28s
epoch 34 | loss: 0.70698 | eval_custom_logloss: 0.73592 |  0:10:46s
epoch 35 | loss: 0.71204 | eval_custom_logloss: 0.73985 |  0:11:05s
epoch 36 | loss: 0.68287 | eval_custom_logloss: 0.76633 |  0:11:23s
epoch 37 | loss: 0.69422 | eval_custom_logloss: 0.75509 |  0:11:42s
epoch 38 | loss: 0.68114 | eval_custom_logloss: 0.74115 |  0:12:00s
epoch 39 | loss: 0.68598 | eval_custom_logloss: 0.85621 |  0:12:19s
epoch 40 | loss: 0.68143 | eval_custom_logloss: 0.64462 |  0:12:37s
epoch 41 | loss: 0.69042 | eval_custom_logloss: 0.69043 |  0:12:56s
epoch 42 | loss: 0.67858 | eval_custom_logloss: 0.75942 |  0:13:14s
epoch 43 | loss: 0.67469 | eval_custom_logloss: 0.87859 |  0:13:33s
epoch 44 | loss: 0.68698 | eval_custom_logloss: 0.79115 |  0:13:51s
epoch 45 | loss: 0.6739  | eval_custom_logloss: 0.8551  |  0:14:09s
epoch 46 | loss: 0.68145 | eval_custom_logloss: 0.64271 |  0:14:28s
epoch 47 | loss: 0.68106 | eval_custom_logloss: 0.70817 |  0:14:46s
epoch 48 | loss: 0.67994 | eval_custom_logloss: 0.62878 |  0:15:05s
epoch 49 | loss: 0.66363 | eval_custom_logloss: 0.67131 |  0:15:23s
epoch 50 | loss: 0.67444 | eval_custom_logloss: 0.85546 |  0:15:41s
epoch 51 | loss: 0.66603 | eval_custom_logloss: 0.90656 |  0:16:00s
epoch 52 | loss: 0.66864 | eval_custom_logloss: 0.83337 |  0:16:18s
epoch 53 | loss: 0.67112 | eval_custom_logloss: 0.827   |  0:16:37s
epoch 54 | loss: 0.67288 | eval_custom_logloss: 0.70836 |  0:16:55s
epoch 55 | loss: 0.66737 | eval_custom_logloss: 0.64349 |  0:17:14s
epoch 56 | loss: 0.66385 | eval_custom_logloss: 0.86607 |  0:17:32s
epoch 57 | loss: 0.67255 | eval_custom_logloss: 0.78285 |  0:17:50s
epoch 58 | loss: 0.67235 | eval_custom_logloss: 0.66627 |  0:18:09s
epoch 59 | loss: 0.6605  | eval_custom_logloss: 0.93212 |  0:18:27s
epoch 60 | loss: 0.66427 | eval_custom_logloss: 0.79033 |  0:18:45s
epoch 61 | loss: 0.66182 | eval_custom_logloss: 0.81407 |  0:19:04s
epoch 62 | loss: 0.65345 | eval_custom_logloss: 0.77141 |  0:19:22s
epoch 63 | loss: 0.6586  | eval_custom_logloss: 0.71809 |  0:19:41s
epoch 64 | loss: 0.65669 | eval_custom_logloss: 0.65846 |  0:19:59s
epoch 65 | loss: 0.65452 | eval_custom_logloss: 0.62347 |  0:20:18s
epoch 66 | loss: 0.66164 | eval_custom_logloss: 0.6497  |  0:20:36s
epoch 67 | loss: 0.65435 | eval_custom_logloss: 0.66274 |  0:20:54s
epoch 68 | loss: 0.66036 | eval_custom_logloss: 0.62774 |  0:21:13s
epoch 69 | loss: 0.67381 | eval_custom_logloss: 0.80216 |  0:21:31s
epoch 70 | loss: 0.64514 | eval_custom_logloss: 0.71179 |  0:21:50s
epoch 71 | loss: 0.65731 | eval_custom_logloss: 0.81628 |  0:22:08s
epoch 72 | loss: 0.65251 | eval_custom_logloss: 0.63823 |  0:22:27s
epoch 73 | loss: 0.64894 | eval_custom_logloss: 0.63333 |  0:22:45s
epoch 74 | loss: 0.64361 | eval_custom_logloss: 0.78074 |  0:23:03s
epoch 75 | loss: 0.64762 | eval_custom_logloss: 0.7168  |  0:23:22s
epoch 76 | loss: 0.66071 | eval_custom_logloss: 0.79561 |  0:23:40s
epoch 77 | loss: 0.65083 | eval_custom_logloss: 0.80578 |  0:23:59s
epoch 78 | loss: 0.65161 | eval_custom_logloss: 0.69294 |  0:24:17s
epoch 79 | loss: 0.63755 | eval_custom_logloss: 0.7356  |  0:24:36s
epoch 80 | loss: 0.64808 | eval_custom_logloss: 0.67757 |  0:24:54s
epoch 81 | loss: 0.6364  | eval_custom_logloss: 0.68567 |  0:25:13s
epoch 82 | loss: 0.62923 | eval_custom_logloss: 0.78279 |  0:25:31s
epoch 83 | loss: 0.64869 | eval_custom_logloss: 0.62576 |  0:25:49s
epoch 84 | loss: 0.62581 | eval_custom_logloss: 0.72364 |  0:26:08s
epoch 85 | loss: 0.63962 | eval_custom_logloss: 0.61079 |  0:26:26s
epoch 86 | loss: 0.65436 | eval_custom_logloss: 0.80239 |  0:26:45s
epoch 87 | loss: 0.63816 | eval_custom_logloss: 0.69175 |  0:27:03s
epoch 88 | loss: 0.63637 | eval_custom_logloss: 0.77841 |  0:27:21s
epoch 89 | loss: 0.64019 | eval_custom_logloss: 0.80116 |  0:27:41s
epoch 90 | loss: 0.63341 | eval_custom_logloss: 0.95208 |  0:28:00s
epoch 91 | loss: 0.63647 | eval_custom_logloss: 0.81222 |  0:28:20s
epoch 92 | loss: 0.64669 | eval_custom_logloss: 0.70093 |  0:28:40s
epoch 93 | loss: 0.63734 | eval_custom_logloss: 0.776   |  0:29:00s
epoch 94 | loss: 0.63618 | eval_custom_logloss: 0.81503 |  0:29:20s
epoch 95 | loss: 0.63844 | eval_custom_logloss: 0.79371 |  0:29:40s
epoch 96 | loss: 0.63584 | eval_custom_logloss: 0.7219  |  0:29:59s
epoch 97 | loss: 0.65109 | eval_custom_logloss: 0.81941 |  0:30:18s
epoch 98 | loss: 0.63216 | eval_custom_logloss: 0.79484 |  0:30:36s
epoch 99 | loss: 0.64119 | eval_custom_logloss: 0.72524 |  0:30:55s
Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_eval_custom_logloss = 0.61079
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6143000000000001, 'Log Loss - std': 0.004400000000000015} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax', 'n_a': 11, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.6568  | eval_custom_logloss: 1.23878 |  0:00:18s
epoch 1  | loss: 1.19702 | eval_custom_logloss: 0.98607 |  0:00:37s
epoch 2  | loss: 1.07806 | eval_custom_logloss: 0.89437 |  0:00:55s
epoch 3  | loss: 1.02674 | eval_custom_logloss: 0.86524 |  0:01:14s
epoch 4  | loss: 0.97746 | eval_custom_logloss: 0.86765 |  0:01:33s
epoch 5  | loss: 0.96911 | eval_custom_logloss: 0.82719 |  0:01:51s
epoch 6  | loss: 0.95076 | eval_custom_logloss: 0.83508 |  0:02:10s
epoch 7  | loss: 0.933   | eval_custom_logloss: 0.82677 |  0:02:29s
epoch 8  | loss: 0.92488 | eval_custom_logloss: 0.79413 |  0:02:47s
epoch 9  | loss: 0.92013 | eval_custom_logloss: 0.85405 |  0:03:06s
epoch 10 | loss: 0.8843  | eval_custom_logloss: 0.9476  |  0:03:25s
epoch 11 | loss: 0.84385 | eval_custom_logloss: 0.82549 |  0:03:43s
epoch 12 | loss: 0.85191 | eval_custom_logloss: 1.11931 |  0:04:02s
epoch 13 | loss: 0.85809 | eval_custom_logloss: 0.78731 |  0:04:21s
epoch 14 | loss: 0.83356 | eval_custom_logloss: 0.85305 |  0:04:39s
epoch 15 | loss: 0.80909 | eval_custom_logloss: 0.76952 |  0:04:58s
epoch 16 | loss: 0.7881  | eval_custom_logloss: 0.70206 |  0:05:17s
epoch 17 | loss: 0.79224 | eval_custom_logloss: 0.80147 |  0:05:35s
epoch 18 | loss: 0.78353 | eval_custom_logloss: 0.79201 |  0:05:54s
epoch 19 | loss: 0.76635 | eval_custom_logloss: 0.80428 |  0:06:13s
epoch 20 | loss: 0.77789 | eval_custom_logloss: 0.99218 |  0:06:31s
epoch 21 | loss: 0.75251 | eval_custom_logloss: 0.72135 |  0:06:50s
epoch 22 | loss: 0.75185 | eval_custom_logloss: 0.82961 |  0:07:08s
epoch 23 | loss: 0.74721 | eval_custom_logloss: 0.84396 |  0:07:27s
epoch 24 | loss: 0.74059 | eval_custom_logloss: 0.67403 |  0:07:46s
epoch 25 | loss: 0.75    | eval_custom_logloss: 0.80964 |  0:08:04s
epoch 26 | loss: 0.72928 | eval_custom_logloss: 0.93912 |  0:08:23s
epoch 27 | loss: 0.73707 | eval_custom_logloss: 0.65916 |  0:08:41s
epoch 28 | loss: 0.7284  | eval_custom_logloss: 0.90794 |  0:09:00s
epoch 29 | loss: 0.72731 | eval_custom_logloss: 0.85823 |  0:09:19s
epoch 30 | loss: 0.73168 | eval_custom_logloss: 0.74793 |  0:09:37s
epoch 31 | loss: 0.74052 | eval_custom_logloss: 0.64778 |  0:09:56s
epoch 32 | loss: 0.71771 | eval_custom_logloss: 0.80221 |  0:10:15s
epoch 33 | loss: 0.71591 | eval_custom_logloss: 0.76498 |  0:10:33s
epoch 34 | loss: 0.71433 | eval_custom_logloss: 0.73854 |  0:10:52s
epoch 35 | loss: 0.7097  | eval_custom_logloss: 0.89098 |  0:11:11s
epoch 36 | loss: 0.71228 | eval_custom_logloss: 0.82624 |  0:11:29s
epoch 37 | loss: 0.69777 | eval_custom_logloss: 0.73037 |  0:11:48s
epoch 38 | loss: 0.71411 | eval_custom_logloss: 0.81199 |  0:12:07s
epoch 39 | loss: 0.69419 | eval_custom_logloss: 0.79432 |  0:12:26s
epoch 40 | loss: 0.70034 | eval_custom_logloss: 0.64039 |  0:12:44s
epoch 41 | loss: 0.69751 | eval_custom_logloss: 0.78454 |  0:13:03s
epoch 42 | loss: 0.69625 | eval_custom_logloss: 0.95776 |  0:13:21s
epoch 43 | loss: 0.70879 | eval_custom_logloss: 0.71845 |  0:13:40s
epoch 44 | loss: 0.70546 | eval_custom_logloss: 0.6729  |  0:13:58s
epoch 45 | loss: 0.68736 | eval_custom_logloss: 0.7147  |  0:14:17s
epoch 46 | loss: 0.70834 | eval_custom_logloss: 0.71995 |  0:14:35s
epoch 47 | loss: 0.68824 | eval_custom_logloss: 0.74993 |  0:14:54s
epoch 48 | loss: 0.68862 | eval_custom_logloss: 0.76683 |  0:15:12s
epoch 49 | loss: 0.68592 | eval_custom_logloss: 0.8504  |  0:15:30s
epoch 50 | loss: 0.69456 | eval_custom_logloss: 0.65587 |  0:15:49s
epoch 51 | loss: 0.69226 | eval_custom_logloss: 0.81083 |  0:16:07s
epoch 52 | loss: 0.67561 | eval_custom_logloss: 0.65126 |  0:16:26s
epoch 53 | loss: 0.70231 | eval_custom_logloss: 0.74856 |  0:16:44s
epoch 54 | loss: 0.68702 | eval_custom_logloss: 0.66153 |  0:17:03s
epoch 55 | loss: 0.68338 | eval_custom_logloss: 0.80859 |  0:17:21s
epoch 56 | loss: 0.67811 | eval_custom_logloss: 0.87014 |  0:17:40s
epoch 57 | loss: 0.68059 | eval_custom_logloss: 0.7506  |  0:17:58s
epoch 58 | loss: 0.6833  | eval_custom_logloss: 0.63753 |  0:18:17s
epoch 59 | loss: 0.67115 | eval_custom_logloss: 0.70266 |  0:18:36s
epoch 60 | loss: 0.67466 | eval_custom_logloss: 0.90327 |  0:18:54s
epoch 61 | loss: 0.67069 | eval_custom_logloss: 0.77862 |  0:19:12s
epoch 62 | loss: 0.67348 | eval_custom_logloss: 0.70491 |  0:19:31s
epoch 63 | loss: 0.66831 | eval_custom_logloss: 0.82391 |  0:19:49s
epoch 64 | loss: 0.67633 | eval_custom_logloss: 0.79744 |  0:20:08s
epoch 65 | loss: 0.66597 | eval_custom_logloss: 0.65026 |  0:20:26s
epoch 66 | loss: 0.66847 | eval_custom_logloss: 0.79626 |  0:20:45s
epoch 67 | loss: 0.67327 | eval_custom_logloss: 0.72513 |  0:21:04s
epoch 68 | loss: 0.67329 | eval_custom_logloss: 0.85207 |  0:21:22s
epoch 69 | loss: 0.67429 | eval_custom_logloss: 0.79496 |  0:21:41s
epoch 70 | loss: 0.66553 | eval_custom_logloss: 0.66498 |  0:21:59s
epoch 71 | loss: 0.66042 | eval_custom_logloss: 0.6873  |  0:22:18s
epoch 72 | loss: 0.65695 | eval_custom_logloss: 0.68283 |  0:22:36s
epoch 73 | loss: 0.66053 | eval_custom_logloss: 0.69958 |  0:22:54s
epoch 74 | loss: 0.65752 | eval_custom_logloss: 0.77735 |  0:23:13s
epoch 75 | loss: 0.68322 | eval_custom_logloss: 0.75989 |  0:23:31s
epoch 76 | loss: 0.65933 | eval_custom_logloss: 0.75483 |  0:23:50s
epoch 77 | loss: 0.66276 | eval_custom_logloss: 0.80376 |  0:24:09s
epoch 78 | loss: 0.66371 | eval_custom_logloss: 0.66321 |  0:24:27s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.63753
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6212666666666667, 'Log Loss - std': 0.010486923073788394} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax', 'n_a': 11, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.67675 | eval_custom_logloss: 1.41414 |  0:00:18s
epoch 1  | loss: 1.26708 | eval_custom_logloss: 1.31243 |  0:00:37s
epoch 2  | loss: 1.18538 | eval_custom_logloss: 1.02518 |  0:00:55s
epoch 3  | loss: 1.0654  | eval_custom_logloss: 0.89583 |  0:01:13s
epoch 4  | loss: 1.01309 | eval_custom_logloss: 0.9254  |  0:01:32s
epoch 5  | loss: 1.03093 | eval_custom_logloss: 1.03573 |  0:01:50s
epoch 6  | loss: 1.00682 | eval_custom_logloss: 0.84658 |  0:02:09s
epoch 7  | loss: 0.96942 | eval_custom_logloss: 0.94428 |  0:02:27s
epoch 8  | loss: 0.95045 | eval_custom_logloss: 0.82095 |  0:02:45s
epoch 9  | loss: 0.97425 | eval_custom_logloss: 0.9657  |  0:03:04s
epoch 10 | loss: 0.93595 | eval_custom_logloss: 0.9296  |  0:03:22s
epoch 11 | loss: 0.91549 | eval_custom_logloss: 0.79057 |  0:03:40s
epoch 12 | loss: 0.89547 | eval_custom_logloss: 0.78732 |  0:03:59s
epoch 13 | loss: 0.89328 | eval_custom_logloss: 0.80927 |  0:04:17s
epoch 14 | loss: 0.88897 | eval_custom_logloss: 0.83016 |  0:04:36s
epoch 15 | loss: 0.87071 | eval_custom_logloss: 0.75162 |  0:04:54s
epoch 16 | loss: 0.87278 | eval_custom_logloss: 0.74512 |  0:05:13s
epoch 17 | loss: 0.88116 | eval_custom_logloss: 0.76339 |  0:05:31s
epoch 18 | loss: 0.86663 | eval_custom_logloss: 0.75811 |  0:05:50s
epoch 19 | loss: 0.84315 | eval_custom_logloss: 0.91448 |  0:06:08s
epoch 20 | loss: 0.84802 | eval_custom_logloss: 0.83705 |  0:06:27s
epoch 21 | loss: 0.84222 | eval_custom_logloss: 0.74399 |  0:06:46s
epoch 22 | loss: 0.83264 | eval_custom_logloss: 0.765   |  0:07:04s
epoch 23 | loss: 0.84328 | eval_custom_logloss: 0.81318 |  0:07:23s
epoch 24 | loss: 0.82979 | eval_custom_logloss: 0.74143 |  0:07:41s
epoch 25 | loss: 0.82238 | eval_custom_logloss: 0.75388 |  0:08:00s
epoch 26 | loss: 0.81658 | eval_custom_logloss: 0.78408 |  0:08:19s
epoch 27 | loss: 0.80175 | eval_custom_logloss: 0.79494 |  0:08:37s
epoch 28 | loss: 0.80796 | eval_custom_logloss: 0.73979 |  0:08:56s
epoch 29 | loss: 0.78712 | eval_custom_logloss: 0.79702 |  0:09:14s
epoch 30 | loss: 0.77892 | eval_custom_logloss: 0.7268  |  0:09:33s
epoch 31 | loss: 0.78361 | eval_custom_logloss: 0.74885 |  0:09:51s
epoch 32 | loss: 0.77139 | eval_custom_logloss: 0.72786 |  0:10:10s
epoch 33 | loss: 0.76749 | eval_custom_logloss: 0.79067 |  0:10:28s
epoch 34 | loss: 0.74677 | eval_custom_logloss: 0.71782 |  0:10:47s
epoch 35 | loss: 0.74311 | eval_custom_logloss: 0.67592 |  0:11:06s
epoch 36 | loss: 0.73139 | eval_custom_logloss: 0.74461 |  0:11:24s
epoch 37 | loss: 0.75253 | eval_custom_logloss: 0.76984 |  0:11:43s
epoch 38 | loss: 0.7297  | eval_custom_logloss: 0.78624 |  0:12:01s
epoch 39 | loss: 0.74352 | eval_custom_logloss: 0.74662 |  0:12:20s
epoch 40 | loss: 0.73622 | eval_custom_logloss: 0.70456 |  0:12:38s
epoch 41 | loss: 0.71519 | eval_custom_logloss: 0.64927 |  0:12:57s
epoch 42 | loss: 0.71442 | eval_custom_logloss: 0.70607 |  0:13:15s
epoch 43 | loss: 0.73215 | eval_custom_logloss: 0.73779 |  0:13:34s
epoch 44 | loss: 0.71868 | eval_custom_logloss: 0.64829 |  0:13:53s
epoch 45 | loss: 0.71517 | eval_custom_logloss: 0.70052 |  0:14:11s
epoch 46 | loss: 0.72281 | eval_custom_logloss: 0.68128 |  0:14:30s
epoch 47 | loss: 0.71358 | eval_custom_logloss: 0.69185 |  0:14:48s
epoch 48 | loss: 0.71571 | eval_custom_logloss: 0.65174 |  0:15:07s
epoch 49 | loss: 0.71091 | eval_custom_logloss: 0.73203 |  0:15:25s
epoch 50 | loss: 0.71725 | eval_custom_logloss: 0.70404 |  0:15:44s
epoch 51 | loss: 0.71559 | eval_custom_logloss: 0.6196  |  0:16:02s
epoch 52 | loss: 0.71066 | eval_custom_logloss: 0.74567 |  0:16:21s
epoch 53 | loss: 0.70471 | eval_custom_logloss: 0.78114 |  0:16:39s
epoch 54 | loss: 0.71584 | eval_custom_logloss: 0.81008 |  0:16:58s
epoch 55 | loss: 0.71112 | eval_custom_logloss: 0.7283  |  0:17:17s
epoch 56 | loss: 0.71291 | eval_custom_logloss: 0.82806 |  0:17:35s
epoch 57 | loss: 0.70852 | eval_custom_logloss: 0.61157 |  0:17:54s
epoch 58 | loss: 0.70505 | eval_custom_logloss: 0.64447 |  0:18:13s
epoch 59 | loss: 0.69863 | eval_custom_logloss: 1.1924  |  0:18:32s
epoch 60 | loss: 0.70192 | eval_custom_logloss: 0.73495 |  0:18:50s
epoch 61 | loss: 0.6864  | eval_custom_logloss: 0.80819 |  0:19:09s
epoch 62 | loss: 0.69362 | eval_custom_logloss: 0.82822 |  0:19:28s
epoch 63 | loss: 0.68912 | eval_custom_logloss: 0.75345 |  0:19:46s
epoch 64 | loss: 0.69528 | eval_custom_logloss: 0.80211 |  0:20:04s
epoch 65 | loss: 0.69334 | eval_custom_logloss: 0.71546 |  0:20:23s
epoch 66 | loss: 0.6934  | eval_custom_logloss: 0.8538  |  0:20:41s
epoch 67 | loss: 0.6872  | eval_custom_logloss: 0.88682 |  0:21:00s
epoch 68 | loss: 0.70399 | eval_custom_logloss: 0.7565  |  0:21:19s
epoch 69 | loss: 0.6953  | eval_custom_logloss: 0.74681 |  0:21:37s
epoch 70 | loss: 0.68071 | eval_custom_logloss: 0.75955 |  0:21:56s
epoch 71 | loss: 0.67871 | eval_custom_logloss: 0.93162 |  0:22:15s
epoch 72 | loss: 0.69999 | eval_custom_logloss: 0.76995 |  0:22:33s
epoch 73 | loss: 0.69271 | eval_custom_logloss: 0.89527 |  0:22:52s
epoch 74 | loss: 0.68456 | eval_custom_logloss: 0.75263 |  0:23:11s
epoch 75 | loss: 0.70317 | eval_custom_logloss: 0.83925 |  0:23:29s
epoch 76 | loss: 0.67867 | eval_custom_logloss: 0.61902 |  0:23:48s
epoch 77 | loss: 0.6787  | eval_custom_logloss: 0.66748 |  0:24:06s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.61157
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.61865, 'Log Loss - std': 0.010149999999999994} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax', 'n_a': 11, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.67266 | eval_custom_logloss: 1.27283 |  0:00:18s
epoch 1  | loss: 1.20285 | eval_custom_logloss: 1.13489 |  0:00:36s
epoch 2  | loss: 1.05593 | eval_custom_logloss: 0.97881 |  0:00:55s
epoch 3  | loss: 1.00789 | eval_custom_logloss: 0.98251 |  0:01:13s
epoch 4  | loss: 0.96081 | eval_custom_logloss: 0.97159 |  0:01:32s
epoch 5  | loss: 0.94784 | eval_custom_logloss: 0.83022 |  0:01:51s
epoch 6  | loss: 0.93734 | eval_custom_logloss: 0.89653 |  0:02:09s
epoch 7  | loss: 0.94203 | eval_custom_logloss: 0.85979 |  0:02:27s
epoch 8  | loss: 0.90331 | eval_custom_logloss: 0.78997 |  0:02:46s
epoch 9  | loss: 0.90688 | eval_custom_logloss: 0.82623 |  0:03:04s
epoch 10 | loss: 0.90634 | eval_custom_logloss: 0.90368 |  0:03:23s
epoch 11 | loss: 0.88151 | eval_custom_logloss: 0.89717 |  0:03:41s
epoch 12 | loss: 0.87423 | eval_custom_logloss: 0.87566 |  0:04:00s
epoch 13 | loss: 0.86506 | eval_custom_logloss: 0.78405 |  0:04:18s
epoch 14 | loss: 0.8664  | eval_custom_logloss: 0.78709 |  0:04:37s
epoch 15 | loss: 0.84932 | eval_custom_logloss: 0.75107 |  0:04:55s
epoch 16 | loss: 0.82905 | eval_custom_logloss: 0.77814 |  0:05:14s
epoch 17 | loss: 0.82181 | eval_custom_logloss: 0.76995 |  0:05:32s
epoch 18 | loss: 0.83322 | eval_custom_logloss: 0.75826 |  0:05:51s
epoch 19 | loss: 0.81181 | eval_custom_logloss: 0.79554 |  0:06:10s
epoch 20 | loss: 0.79493 | eval_custom_logloss: 0.78601 |  0:06:28s
epoch 21 | loss: 0.8133  | eval_custom_logloss: 0.75633 |  0:06:47s
epoch 22 | loss: 0.78692 | eval_custom_logloss: 0.84635 |  0:07:05s
epoch 23 | loss: 0.80089 | eval_custom_logloss: 0.87529 |  0:07:24s
epoch 24 | loss: 0.77247 | eval_custom_logloss: 0.72081 |  0:07:43s
epoch 25 | loss: 0.76461 | eval_custom_logloss: 0.69094 |  0:08:02s
epoch 26 | loss: 0.75582 | eval_custom_logloss: 0.68739 |  0:08:20s
epoch 27 | loss: 0.76472 | eval_custom_logloss: 0.8815  |  0:08:39s
epoch 28 | loss: 0.75842 | eval_custom_logloss: 0.82129 |  0:08:57s
epoch 29 | loss: 0.73716 | eval_custom_logloss: 0.71842 |  0:09:16s
epoch 30 | loss: 0.76045 | eval_custom_logloss: 0.73562 |  0:09:34s
epoch 31 | loss: 0.74986 | eval_custom_logloss: 0.74815 |  0:09:53s
epoch 32 | loss: 0.75646 | eval_custom_logloss: 0.67741 |  0:10:11s
epoch 33 | loss: 0.73988 | eval_custom_logloss: 0.79523 |  0:10:30s
epoch 34 | loss: 0.74203 | eval_custom_logloss: 1.07412 |  0:10:48s
epoch 35 | loss: 0.74918 | eval_custom_logloss: 0.78104 |  0:11:07s
epoch 36 | loss: 0.72931 | eval_custom_logloss: 0.67395 |  0:11:25s
epoch 37 | loss: 0.72464 | eval_custom_logloss: 0.65283 |  0:11:44s
epoch 38 | loss: 0.73126 | eval_custom_logloss: 0.66428 |  0:12:02s
epoch 39 | loss: 0.72275 | eval_custom_logloss: 0.9398  |  0:12:21s
epoch 40 | loss: 0.72908 | eval_custom_logloss: 0.75133 |  0:12:39s
epoch 41 | loss: 0.72018 | eval_custom_logloss: 0.63127 |  0:12:58s
epoch 42 | loss: 0.71592 | eval_custom_logloss: 0.73964 |  0:13:16s
epoch 43 | loss: 0.71662 | eval_custom_logloss: 0.68079 |  0:13:35s
epoch 44 | loss: 0.71929 | eval_custom_logloss: 0.80894 |  0:13:53s
epoch 45 | loss: 0.72299 | eval_custom_logloss: 0.72182 |  0:14:11s
epoch 46 | loss: 0.70992 | eval_custom_logloss: 0.80095 |  0:14:30s
epoch 47 | loss: 0.71459 | eval_custom_logloss: 0.82982 |  0:14:48s
epoch 48 | loss: 0.71518 | eval_custom_logloss: 0.68449 |  0:15:06s
epoch 49 | loss: 0.69622 | eval_custom_logloss: 0.73729 |  0:15:25s
epoch 50 | loss: 0.7033  | eval_custom_logloss: 0.67867 |  0:15:43s
epoch 51 | loss: 0.71    | eval_custom_logloss: 0.9553  |  0:16:02s
epoch 52 | loss: 0.71046 | eval_custom_logloss: 0.76696 |  0:16:20s
epoch 53 | loss: 0.70064 | eval_custom_logloss: 0.69702 |  0:16:38s
epoch 54 | loss: 0.70551 | eval_custom_logloss: 0.75095 |  0:16:57s
epoch 55 | loss: 0.70788 | eval_custom_logloss: 0.79219 |  0:17:16s
epoch 56 | loss: 0.69508 | eval_custom_logloss: 0.89345 |  0:17:34s
epoch 57 | loss: 0.70818 | eval_custom_logloss: 1.42851 |  0:17:53s
epoch 58 | loss: 0.70705 | eval_custom_logloss: 0.71285 |  0:18:11s
epoch 59 | loss: 0.70049 | eval_custom_logloss: 0.69975 |  0:18:30s
epoch 60 | loss: 0.69108 | eval_custom_logloss: 0.70257 |  0:18:48s
epoch 61 | loss: 0.7007  | eval_custom_logloss: 1.17981 |  0:19:07s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 0.63127
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.621, 'Log Loss - std': 0.010222915435432288} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 0.621 and parameters: {'n_d': 11, 'n_steps': 8, 'gamma': 1.2399473546027948, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.04072693325411094, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.3701  | eval_custom_logloss: 8.02774 |  0:00:11s
epoch 1  | loss: 1.0521  | eval_custom_logloss: 6.54412 |  0:00:22s
epoch 2  | loss: 0.99306 | eval_custom_logloss: 6.44313 |  0:00:33s
epoch 3  | loss: 0.95529 | eval_custom_logloss: 6.18383 |  0:00:46s
epoch 4  | loss: 0.90545 | eval_custom_logloss: 5.93896 |  0:00:56s
epoch 5  | loss: 0.89137 | eval_custom_logloss: 5.7627  |  0:01:09s
epoch 6  | loss: 0.84139 | eval_custom_logloss: 5.265   |  0:01:20s
epoch 7  | loss: 0.83146 | eval_custom_logloss: 3.76134 |  0:01:31s
epoch 8  | loss: 0.81103 | eval_custom_logloss: 3.23701 |  0:01:42s
epoch 9  | loss: 0.82197 | eval_custom_logloss: 4.08144 |  0:01:53s
epoch 10 | loss: 0.7933  | eval_custom_logloss: 2.66098 |  0:02:04s
epoch 11 | loss: 0.80463 | eval_custom_logloss: 3.41665 |  0:02:17s
epoch 12 | loss: 0.82205 | eval_custom_logloss: 1.86855 |  0:02:28s
epoch 13 | loss: 0.80121 | eval_custom_logloss: 2.18033 |  0:02:39s
epoch 14 | loss: 0.77732 | eval_custom_logloss: 2.22069 |  0:02:50s
epoch 15 | loss: 0.76312 | eval_custom_logloss: 1.60144 |  0:03:01s
epoch 16 | loss: 0.76735 | eval_custom_logloss: 2.03584 |  0:03:12s
epoch 17 | loss: 0.75101 | eval_custom_logloss: 2.24494 |  0:03:23s
epoch 18 | loss: 0.75762 | eval_custom_logloss: 1.04408 |  0:03:35s
epoch 19 | loss: 0.73871 | eval_custom_logloss: 1.20035 |  0:03:46s
epoch 20 | loss: 0.74411 | eval_custom_logloss: 1.72445 |  0:03:58s
epoch 21 | loss: 0.74772 | eval_custom_logloss: 2.11955 |  0:04:09s
epoch 22 | loss: 0.73772 | eval_custom_logloss: 1.24612 |  0:04:20s
epoch 23 | loss: 0.72446 | eval_custom_logloss: 1.00456 |  0:04:31s
epoch 24 | loss: 0.72901 | eval_custom_logloss: 1.58224 |  0:04:42s
epoch 25 | loss: 0.72424 | eval_custom_logloss: 1.10593 |  0:04:53s
epoch 26 | loss: 0.75593 | eval_custom_logloss: 1.46822 |  0:05:05s
epoch 27 | loss: 0.74667 | eval_custom_logloss: 1.72373 |  0:05:16s
epoch 28 | loss: 0.73017 | eval_custom_logloss: 2.09217 |  0:05:27s
epoch 29 | loss: 0.70411 | eval_custom_logloss: 1.25151 |  0:05:38s
epoch 30 | loss: 0.69997 | eval_custom_logloss: 0.95352 |  0:05:50s
epoch 31 | loss: 0.68468 | eval_custom_logloss: 1.25677 |  0:06:01s
epoch 32 | loss: 0.69223 | eval_custom_logloss: 0.86923 |  0:06:12s
epoch 33 | loss: 0.68649 | eval_custom_logloss: 1.36147 |  0:06:23s
epoch 34 | loss: 0.69166 | eval_custom_logloss: 1.16884 |  0:06:34s
epoch 35 | loss: 0.68988 | eval_custom_logloss: 0.80548 |  0:06:45s
epoch 36 | loss: 0.69288 | eval_custom_logloss: 1.03889 |  0:06:56s
epoch 37 | loss: 0.68869 | eval_custom_logloss: 1.08146 |  0:07:08s
epoch 38 | loss: 0.69077 | eval_custom_logloss: 0.83925 |  0:07:19s
epoch 39 | loss: 0.67585 | eval_custom_logloss: 1.06866 |  0:07:30s
epoch 40 | loss: 0.67413 | eval_custom_logloss: 0.7623  |  0:07:41s
epoch 41 | loss: 0.68208 | eval_custom_logloss: 0.77373 |  0:07:52s
epoch 42 | loss: 0.6647  | eval_custom_logloss: 0.98192 |  0:08:03s
epoch 43 | loss: 0.66968 | eval_custom_logloss: 1.06517 |  0:08:14s
epoch 44 | loss: 0.65978 | eval_custom_logloss: 0.99239 |  0:08:26s
epoch 45 | loss: 0.68817 | eval_custom_logloss: 0.82809 |  0:08:38s
epoch 46 | loss: 0.65812 | eval_custom_logloss: 0.85766 |  0:08:49s
epoch 47 | loss: 0.66059 | eval_custom_logloss: 1.01553 |  0:09:01s
epoch 48 | loss: 0.64974 | eval_custom_logloss: 0.68412 |  0:09:12s
epoch 49 | loss: 0.65126 | eval_custom_logloss: 0.77042 |  0:09:23s
epoch 50 | loss: 0.65588 | eval_custom_logloss: 1.1351  |  0:09:34s
epoch 51 | loss: 0.65892 | eval_custom_logloss: 0.94981 |  0:09:45s
epoch 52 | loss: 0.6536  | eval_custom_logloss: 1.06795 |  0:09:56s
epoch 53 | loss: 0.65496 | eval_custom_logloss: 1.2338  |  0:10:07s
epoch 54 | loss: 0.65178 | eval_custom_logloss: 0.88288 |  0:10:19s
epoch 55 | loss: 0.67027 | eval_custom_logloss: 0.8681  |  0:10:31s
epoch 56 | loss: 0.65358 | eval_custom_logloss: 0.66608 |  0:10:42s
epoch 57 | loss: 0.66158 | eval_custom_logloss: 0.74432 |  0:10:53s
epoch 58 | loss: 0.65452 | eval_custom_logloss: 0.91881 |  0:11:04s
epoch 59 | loss: 0.65991 | eval_custom_logloss: 0.65236 |  0:11:16s
epoch 60 | loss: 0.65056 | eval_custom_logloss: 0.92433 |  0:11:28s
epoch 61 | loss: 0.65326 | eval_custom_logloss: 1.02217 |  0:11:40s
epoch 62 | loss: 0.64252 | eval_custom_logloss: 1.19706 |  0:11:51s
epoch 63 | loss: 0.65617 | eval_custom_logloss: 0.69613 |  0:12:03s
epoch 64 | loss: 0.64    | eval_custom_logloss: 0.70601 |  0:12:15s
epoch 65 | loss: 0.64818 | eval_custom_logloss: 0.62039 |  0:12:26s
epoch 66 | loss: 0.63498 | eval_custom_logloss: 0.63808 |  0:12:37s
epoch 67 | loss: 0.64336 | eval_custom_logloss: 0.74313 |  0:12:48s
epoch 68 | loss: 0.65551 | eval_custom_logloss: 0.83966 |  0:12:59s
epoch 69 | loss: 0.64116 | eval_custom_logloss: 1.04137 |  0:13:10s
epoch 70 | loss: 0.63684 | eval_custom_logloss: 0.73844 |  0:13:22s
epoch 71 | loss: 0.63711 | eval_custom_logloss: 1.45892 |  0:13:34s
epoch 72 | loss: 0.64101 | eval_custom_logloss: 0.66845 |  0:13:45s
epoch 73 | loss: 0.64101 | eval_custom_logloss: 0.88165 |  0:13:56s
epoch 74 | loss: 0.66387 | eval_custom_logloss: 0.84389 |  0:14:08s
epoch 75 | loss: 0.64936 | eval_custom_logloss: 0.83628 |  0:14:19s
epoch 76 | loss: 0.65073 | eval_custom_logloss: 0.77577 |  0:14:30s
epoch 77 | loss: 0.63616 | eval_custom_logloss: 0.77452 |  0:14:41s
epoch 78 | loss: 0.63333 | eval_custom_logloss: 0.76314 |  0:14:53s
epoch 79 | loss: 0.63446 | eval_custom_logloss: 1.00287 |  0:15:04s
epoch 80 | loss: 0.62361 | eval_custom_logloss: 0.68106 |  0:15:15s
epoch 81 | loss: 0.62372 | eval_custom_logloss: 0.77601 |  0:15:27s
epoch 82 | loss: 0.6406  | eval_custom_logloss: 0.65244 |  0:15:39s
epoch 83 | loss: 0.63624 | eval_custom_logloss: 0.85104 |  0:15:51s
epoch 84 | loss: 0.63909 | eval_custom_logloss: 0.67198 |  0:16:03s
epoch 85 | loss: 0.63699 | eval_custom_logloss: 0.62353 |  0:16:14s

Early stopping occurred at epoch 85 with best_epoch = 65 and best_eval_custom_logloss = 0.62039
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6194, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.43437 | eval_custom_logloss: 11.18178|  0:00:11s
epoch 1  | loss: 1.05861 | eval_custom_logloss: 8.98384 |  0:00:22s
epoch 2  | loss: 0.9742  | eval_custom_logloss: 9.65478 |  0:00:34s
epoch 3  | loss: 0.9287  | eval_custom_logloss: 7.69843 |  0:00:45s
epoch 4  | loss: 0.91449 | eval_custom_logloss: 6.5344  |  0:00:57s
epoch 5  | loss: 0.93892 | eval_custom_logloss: 4.98732 |  0:01:09s
epoch 6  | loss: 0.88074 | eval_custom_logloss: 3.40188 |  0:01:21s
epoch 7  | loss: 0.89639 | eval_custom_logloss: 2.25649 |  0:01:33s
epoch 8  | loss: 0.84494 | eval_custom_logloss: 2.18715 |  0:01:45s
epoch 9  | loss: 0.83319 | eval_custom_logloss: 3.55427 |  0:01:56s
epoch 10 | loss: 0.79526 | eval_custom_logloss: 2.21887 |  0:02:08s
epoch 11 | loss: 0.7836  | eval_custom_logloss: 2.39466 |  0:02:20s
epoch 12 | loss: 0.79069 | eval_custom_logloss: 1.8938  |  0:02:32s
epoch 13 | loss: 0.75839 | eval_custom_logloss: 1.8989  |  0:02:44s
epoch 14 | loss: 0.74668 | eval_custom_logloss: 1.58988 |  0:02:56s
epoch 15 | loss: 0.73363 | eval_custom_logloss: 1.27937 |  0:03:07s
epoch 16 | loss: 0.73108 | eval_custom_logloss: 1.6555  |  0:03:18s
epoch 17 | loss: 0.72415 | eval_custom_logloss: 0.97406 |  0:03:29s
epoch 18 | loss: 0.71536 | eval_custom_logloss: 2.03302 |  0:03:41s
epoch 19 | loss: 0.71246 | eval_custom_logloss: 0.88897 |  0:03:53s
epoch 20 | loss: 0.73071 | eval_custom_logloss: 1.09261 |  0:04:05s
epoch 21 | loss: 0.70758 | eval_custom_logloss: 0.89964 |  0:04:17s
epoch 22 | loss: 0.71209 | eval_custom_logloss: 1.35143 |  0:04:28s
epoch 23 | loss: 0.70925 | eval_custom_logloss: 1.443   |  0:04:39s
epoch 24 | loss: 0.69463 | eval_custom_logloss: 0.96469 |  0:04:50s
epoch 25 | loss: 0.69236 | eval_custom_logloss: 1.14627 |  0:05:01s
epoch 26 | loss: 0.69219 | eval_custom_logloss: 1.60042 |  0:05:13s
epoch 27 | loss: 0.68956 | eval_custom_logloss: 1.66616 |  0:05:24s
epoch 28 | loss: 0.68741 | eval_custom_logloss: 0.81243 |  0:05:36s
epoch 29 | loss: 0.68313 | eval_custom_logloss: 1.37701 |  0:05:47s
epoch 30 | loss: 0.69504 | eval_custom_logloss: 1.24047 |  0:05:59s
epoch 31 | loss: 0.68317 | eval_custom_logloss: 0.98565 |  0:06:10s
epoch 32 | loss: 0.70442 | eval_custom_logloss: 0.97721 |  0:06:21s
epoch 33 | loss: 0.67761 | eval_custom_logloss: 0.86649 |  0:06:32s
epoch 34 | loss: 0.67008 | eval_custom_logloss: 0.77098 |  0:06:44s
epoch 35 | loss: 0.68074 | eval_custom_logloss: 0.85774 |  0:06:55s
epoch 36 | loss: 0.66953 | eval_custom_logloss: 0.99549 |  0:07:06s
epoch 37 | loss: 0.67597 | eval_custom_logloss: 0.8757  |  0:07:17s
epoch 38 | loss: 0.65659 | eval_custom_logloss: 0.8369  |  0:07:28s
epoch 39 | loss: 0.66501 | eval_custom_logloss: 1.0869  |  0:07:39s
epoch 40 | loss: 0.66023 | eval_custom_logloss: 1.3987  |  0:07:51s
epoch 41 | loss: 0.66329 | eval_custom_logloss: 0.74858 |  0:08:02s
epoch 42 | loss: 0.66648 | eval_custom_logloss: 0.69671 |  0:08:13s
epoch 43 | loss: 0.67877 | eval_custom_logloss: 0.93528 |  0:08:24s
epoch 44 | loss: 0.65111 | eval_custom_logloss: 0.80344 |  0:08:35s
epoch 45 | loss: 0.65034 | eval_custom_logloss: 1.2207  |  0:08:46s
epoch 46 | loss: 0.68118 | eval_custom_logloss: 1.09329 |  0:08:58s
epoch 47 | loss: 0.65378 | eval_custom_logloss: 1.04189 |  0:09:09s
epoch 48 | loss: 0.64506 | eval_custom_logloss: 1.01549 |  0:09:21s
epoch 49 | loss: 0.65367 | eval_custom_logloss: 0.82497 |  0:09:32s
epoch 50 | loss: 0.64575 | eval_custom_logloss: 0.7372  |  0:09:43s
epoch 51 | loss: 0.64887 | eval_custom_logloss: 0.64904 |  0:09:55s
epoch 52 | loss: 0.64904 | eval_custom_logloss: 0.81156 |  0:10:06s
epoch 53 | loss: 0.64012 | eval_custom_logloss: 0.73239 |  0:10:17s
epoch 54 | loss: 0.63755 | eval_custom_logloss: 0.68309 |  0:10:29s
epoch 55 | loss: 0.64266 | eval_custom_logloss: 0.89789 |  0:10:41s
epoch 56 | loss: 0.64708 | eval_custom_logloss: 0.68409 |  0:10:53s
epoch 57 | loss: 0.62933 | eval_custom_logloss: 0.82773 |  0:11:05s
epoch 58 | loss: 0.63816 | eval_custom_logloss: 0.95641 |  0:11:17s
epoch 59 | loss: 0.63916 | eval_custom_logloss: 0.78965 |  0:11:29s
epoch 60 | loss: 0.62819 | eval_custom_logloss: 0.88551 |  0:11:40s
epoch 61 | loss: 0.63044 | eval_custom_logloss: 0.99037 |  0:11:51s
epoch 62 | loss: 0.63426 | eval_custom_logloss: 0.70294 |  0:12:03s
epoch 63 | loss: 0.62432 | eval_custom_logloss: 0.7884  |  0:12:15s
epoch 64 | loss: 0.62602 | eval_custom_logloss: 1.01299 |  0:12:26s
epoch 65 | loss: 0.62377 | eval_custom_logloss: 1.24109 |  0:12:37s
epoch 66 | loss: 0.63347 | eval_custom_logloss: 1.00538 |  0:12:48s
epoch 67 | loss: 0.6296  | eval_custom_logloss: 0.887   |  0:12:59s
epoch 68 | loss: 0.62464 | eval_custom_logloss: 0.79208 |  0:13:10s
epoch 69 | loss: 0.61695 | eval_custom_logloss: 0.79456 |  0:13:22s
epoch 70 | loss: 0.61471 | eval_custom_logloss: 0.96117 |  0:13:33s
epoch 71 | loss: 0.62173 | eval_custom_logloss: 1.35083 |  0:13:44s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 0.64904
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6335999999999999, 'Log Loss - std': 0.014200000000000046} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.47141 | eval_custom_logloss: 10.09383|  0:00:11s
epoch 1  | loss: 1.07523 | eval_custom_logloss: 9.00039 |  0:00:23s
epoch 2  | loss: 0.97674 | eval_custom_logloss: 9.25343 |  0:00:34s
epoch 3  | loss: 0.93343 | eval_custom_logloss: 7.1554  |  0:00:46s
epoch 4  | loss: 0.91986 | eval_custom_logloss: 6.19892 |  0:00:57s
epoch 5  | loss: 0.90109 | eval_custom_logloss: 5.74858 |  0:01:08s
epoch 6  | loss: 0.86952 | eval_custom_logloss: 4.95011 |  0:01:19s
epoch 7  | loss: 0.8526  | eval_custom_logloss: 4.6906  |  0:01:30s
epoch 8  | loss: 0.8246  | eval_custom_logloss: 5.14847 |  0:01:41s
epoch 9  | loss: 0.83327 | eval_custom_logloss: 4.46315 |  0:01:53s
epoch 10 | loss: 0.79967 | eval_custom_logloss: 4.59652 |  0:02:04s
epoch 11 | loss: 0.7962  | eval_custom_logloss: 4.65206 |  0:02:15s
epoch 12 | loss: 0.80095 | eval_custom_logloss: 3.75982 |  0:02:27s
epoch 13 | loss: 0.77066 | eval_custom_logloss: 4.61848 |  0:02:39s
epoch 14 | loss: 0.76567 | eval_custom_logloss: 2.83045 |  0:02:50s
epoch 15 | loss: 0.75524 | eval_custom_logloss: 3.30332 |  0:03:01s
epoch 16 | loss: 0.74386 | eval_custom_logloss: 2.85101 |  0:03:12s
epoch 17 | loss: 0.75279 | eval_custom_logloss: 2.4575  |  0:03:24s
epoch 18 | loss: 0.74877 | eval_custom_logloss: 2.99942 |  0:03:35s
epoch 19 | loss: 0.72656 | eval_custom_logloss: 2.85397 |  0:03:46s
epoch 20 | loss: 0.71793 | eval_custom_logloss: 2.08991 |  0:03:57s
epoch 21 | loss: 0.72722 | eval_custom_logloss: 2.88005 |  0:04:08s
epoch 22 | loss: 0.70667 | eval_custom_logloss: 1.95053 |  0:04:19s
epoch 23 | loss: 0.70518 | eval_custom_logloss: 1.921   |  0:04:30s
epoch 24 | loss: 0.70506 | eval_custom_logloss: 1.35772 |  0:04:41s
epoch 25 | loss: 0.6902  | eval_custom_logloss: 1.26293 |  0:04:53s
epoch 26 | loss: 0.70564 | eval_custom_logloss: 1.77783 |  0:05:04s
epoch 27 | loss: 0.70174 | eval_custom_logloss: 1.60904 |  0:05:15s
epoch 28 | loss: 0.69227 | eval_custom_logloss: 1.89296 |  0:05:26s
epoch 29 | loss: 0.68146 | eval_custom_logloss: 0.91762 |  0:05:38s
epoch 30 | loss: 0.679   | eval_custom_logloss: 1.40334 |  0:05:50s
epoch 31 | loss: 0.67604 | eval_custom_logloss: 1.73133 |  0:06:02s
epoch 32 | loss: 0.68557 | eval_custom_logloss: 1.68507 |  0:06:13s
epoch 33 | loss: 0.68315 | eval_custom_logloss: 2.08563 |  0:06:24s
epoch 34 | loss: 0.67417 | eval_custom_logloss: 1.6242  |  0:06:35s
epoch 35 | loss: 0.67031 | eval_custom_logloss: 1.23737 |  0:06:47s
epoch 36 | loss: 0.67162 | eval_custom_logloss: 1.47164 |  0:06:58s
epoch 37 | loss: 0.66925 | eval_custom_logloss: 1.71041 |  0:07:09s
epoch 38 | loss: 0.66974 | eval_custom_logloss: 1.37105 |  0:07:20s
epoch 39 | loss: 0.66363 | eval_custom_logloss: 1.41651 |  0:07:31s
epoch 40 | loss: 0.68035 | eval_custom_logloss: 1.21134 |  0:07:42s
epoch 41 | loss: 0.69008 | eval_custom_logloss: 1.67274 |  0:07:53s
epoch 42 | loss: 0.67262 | eval_custom_logloss: 0.92897 |  0:08:04s
epoch 43 | loss: 0.66375 | eval_custom_logloss: 0.83036 |  0:08:15s
epoch 44 | loss: 0.66349 | eval_custom_logloss: 0.86892 |  0:08:26s
epoch 45 | loss: 0.65306 | eval_custom_logloss: 2.23316 |  0:08:37s
epoch 46 | loss: 0.66675 | eval_custom_logloss: 1.33554 |  0:08:48s
epoch 47 | loss: 0.66234 | eval_custom_logloss: 1.54566 |  0:08:59s
epoch 48 | loss: 0.65762 | eval_custom_logloss: 1.55452 |  0:09:10s
epoch 49 | loss: 0.66356 | eval_custom_logloss: 0.92273 |  0:09:21s
epoch 50 | loss: 0.65887 | eval_custom_logloss: 1.25617 |  0:09:32s
epoch 51 | loss: 0.65146 | eval_custom_logloss: 0.8373  |  0:09:43s
epoch 52 | loss: 0.64958 | eval_custom_logloss: 2.06722 |  0:09:54s
epoch 53 | loss: 0.63991 | eval_custom_logloss: 1.18692 |  0:10:05s
epoch 54 | loss: 0.64692 | eval_custom_logloss: 1.09159 |  0:10:16s
epoch 55 | loss: 0.64554 | eval_custom_logloss: 1.18502 |  0:10:27s
epoch 56 | loss: 0.63408 | eval_custom_logloss: 0.8251  |  0:10:38s
epoch 57 | loss: 0.64697 | eval_custom_logloss: 1.10323 |  0:10:49s
epoch 58 | loss: 0.63242 | eval_custom_logloss: 1.44821 |  0:11:00s
epoch 59 | loss: 0.63841 | eval_custom_logloss: 0.80939 |  0:11:11s
epoch 60 | loss: 0.6314  | eval_custom_logloss: 1.48261 |  0:11:22s
epoch 61 | loss: 0.6371  | eval_custom_logloss: 0.83136 |  0:11:34s
epoch 62 | loss: 0.63765 | eval_custom_logloss: 0.84843 |  0:11:44s
epoch 63 | loss: 0.63254 | eval_custom_logloss: 1.42087 |  0:11:55s
epoch 64 | loss: 0.63739 | eval_custom_logloss: 0.94547 |  0:12:06s
epoch 65 | loss: 0.63638 | eval_custom_logloss: 0.85512 |  0:12:18s
epoch 66 | loss: 0.62084 | eval_custom_logloss: 1.34075 |  0:12:30s
epoch 67 | loss: 0.63825 | eval_custom_logloss: 1.47197 |  0:12:42s
epoch 68 | loss: 0.63356 | eval_custom_logloss: 1.72822 |  0:12:54s
epoch 69 | loss: 0.62815 | eval_custom_logloss: 0.83859 |  0:13:05s
epoch 70 | loss: 0.62557 | eval_custom_logloss: 1.15037 |  0:13:16s
epoch 71 | loss: 0.6359  | eval_custom_logloss: 0.88394 |  0:13:28s
epoch 72 | loss: 0.6164  | eval_custom_logloss: 1.37949 |  0:13:40s
epoch 73 | loss: 0.61754 | eval_custom_logloss: 1.4799  |  0:13:51s
epoch 74 | loss: 0.61522 | eval_custom_logloss: 1.40934 |  0:14:02s
epoch 75 | loss: 0.61796 | eval_custom_logloss: 0.88761 |  0:14:13s
epoch 76 | loss: 0.62027 | eval_custom_logloss: 0.71747 |  0:14:24s
epoch 77 | loss: 0.61547 | eval_custom_logloss: 1.06923 |  0:14:35s
epoch 78 | loss: 0.61652 | eval_custom_logloss: 1.36172 |  0:14:46s
epoch 79 | loss: 0.62505 | eval_custom_logloss: 0.75979 |  0:14:56s
epoch 80 | loss: 0.60631 | eval_custom_logloss: 0.93774 |  0:15:08s
epoch 81 | loss: 0.61078 | eval_custom_logloss: 1.13579 |  0:15:19s
epoch 82 | loss: 0.61889 | eval_custom_logloss: 1.03795 |  0:15:30s
epoch 83 | loss: 0.61357 | eval_custom_logloss: 0.92126 |  0:15:42s
epoch 84 | loss: 0.60688 | eval_custom_logloss: 1.61331 |  0:15:53s
epoch 85 | loss: 0.62229 | eval_custom_logloss: 0.88487 |  0:16:03s
epoch 86 | loss: 0.61094 | eval_custom_logloss: 1.07936 |  0:16:15s
epoch 87 | loss: 0.59985 | eval_custom_logloss: 0.85718 |  0:16:26s
epoch 88 | loss: 0.61391 | eval_custom_logloss: 0.85848 |  0:16:37s
epoch 89 | loss: 0.61493 | eval_custom_logloss: 1.06088 |  0:16:48s
epoch 90 | loss: 0.61649 | eval_custom_logloss: 1.2027  |  0:16:59s
epoch 91 | loss: 0.61881 | eval_custom_logloss: 2.5662  |  0:17:10s
epoch 92 | loss: 0.60993 | eval_custom_logloss: 0.79833 |  0:17:21s
epoch 93 | loss: 0.60524 | eval_custom_logloss: 1.1993  |  0:17:32s
epoch 94 | loss: 0.60259 | eval_custom_logloss: 1.44916 |  0:17:43s
epoch 95 | loss: 0.60292 | eval_custom_logloss: 0.91432 |  0:17:55s
epoch 96 | loss: 0.60352 | eval_custom_logloss: 0.8232  |  0:18:06s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 0.71747
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6610333333333333, 'Log Loss - std': 0.04049200195374665} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.49968 | eval_custom_logloss: 7.72991 |  0:00:12s
epoch 1  | loss: 1.10276 | eval_custom_logloss: 7.7525  |  0:00:23s
epoch 2  | loss: 1.00329 | eval_custom_logloss: 7.09539 |  0:00:35s
epoch 3  | loss: 0.95586 | eval_custom_logloss: 4.82949 |  0:00:47s
epoch 4  | loss: 0.93708 | eval_custom_logloss: 6.23893 |  0:00:58s
epoch 5  | loss: 0.92598 | eval_custom_logloss: 4.35349 |  0:01:10s
epoch 6  | loss: 0.90377 | eval_custom_logloss: 3.70961 |  0:01:20s
epoch 7  | loss: 0.89366 | eval_custom_logloss: 2.99005 |  0:01:32s
epoch 8  | loss: 0.85256 | eval_custom_logloss: 3.17532 |  0:01:43s
epoch 9  | loss: 0.86772 | eval_custom_logloss: 4.32364 |  0:01:55s
epoch 10 | loss: 0.82697 | eval_custom_logloss: 2.79157 |  0:02:06s
epoch 11 | loss: 0.83271 | eval_custom_logloss: 4.16578 |  0:02:18s
epoch 12 | loss: 0.82937 | eval_custom_logloss: 2.3739  |  0:02:29s
epoch 13 | loss: 0.79571 | eval_custom_logloss: 2.67835 |  0:02:41s
epoch 14 | loss: 0.79284 | eval_custom_logloss: 2.16302 |  0:02:53s
epoch 15 | loss: 0.7928  | eval_custom_logloss: 2.61605 |  0:03:05s
epoch 16 | loss: 0.77396 | eval_custom_logloss: 1.71828 |  0:03:16s
epoch 17 | loss: 0.76525 | eval_custom_logloss: 2.11189 |  0:03:27s
epoch 18 | loss: 0.76041 | eval_custom_logloss: 1.49314 |  0:03:39s
epoch 19 | loss: 0.75499 | eval_custom_logloss: 1.64553 |  0:03:50s
epoch 20 | loss: 0.75715 | eval_custom_logloss: 2.08599 |  0:04:01s
epoch 21 | loss: 0.74741 | eval_custom_logloss: 1.26179 |  0:04:12s
epoch 22 | loss: 0.73592 | eval_custom_logloss: 1.54729 |  0:04:23s
epoch 23 | loss: 0.73255 | eval_custom_logloss: 1.17835 |  0:04:34s
epoch 24 | loss: 0.72307 | eval_custom_logloss: 1.44604 |  0:04:45s
epoch 25 | loss: 0.71483 | eval_custom_logloss: 1.7845  |  0:04:56s
epoch 26 | loss: 0.72081 | eval_custom_logloss: 1.41405 |  0:05:08s
epoch 27 | loss: 0.71418 | eval_custom_logloss: 1.19634 |  0:05:20s
epoch 28 | loss: 0.72329 | eval_custom_logloss: 1.13583 |  0:05:32s
epoch 29 | loss: 0.70645 | eval_custom_logloss: 0.88447 |  0:05:43s
epoch 30 | loss: 0.69613 | eval_custom_logloss: 0.76804 |  0:05:54s
epoch 31 | loss: 0.69728 | eval_custom_logloss: 1.31727 |  0:06:06s
epoch 32 | loss: 0.70092 | eval_custom_logloss: 0.99513 |  0:06:18s
epoch 33 | loss: 0.69618 | eval_custom_logloss: 1.44088 |  0:06:30s
epoch 34 | loss: 0.69955 | eval_custom_logloss: 0.94942 |  0:06:41s
epoch 35 | loss: 0.68822 | eval_custom_logloss: 0.9175  |  0:06:52s
epoch 36 | loss: 0.69622 | eval_custom_logloss: 0.89145 |  0:07:03s
epoch 37 | loss: 0.70181 | eval_custom_logloss: 1.05051 |  0:07:15s
epoch 38 | loss: 0.68517 | eval_custom_logloss: 2.98358 |  0:07:27s
epoch 39 | loss: 0.69335 | eval_custom_logloss: 0.82179 |  0:07:39s
epoch 40 | loss: 0.69849 | eval_custom_logloss: 0.72371 |  0:07:51s
epoch 41 | loss: 0.6925  | eval_custom_logloss: 1.24224 |  0:08:02s
epoch 42 | loss: 0.68925 | eval_custom_logloss: 1.19183 |  0:08:14s
epoch 43 | loss: 0.67899 | eval_custom_logloss: 1.04573 |  0:08:26s
epoch 44 | loss: 0.68133 | eval_custom_logloss: 0.8622  |  0:08:37s
epoch 45 | loss: 0.67268 | eval_custom_logloss: 0.90567 |  0:08:48s
epoch 46 | loss: 0.67936 | eval_custom_logloss: 0.89762 |  0:08:59s
epoch 47 | loss: 0.68207 | eval_custom_logloss: 1.00949 |  0:09:10s
epoch 48 | loss: 0.6811  | eval_custom_logloss: 1.01664 |  0:09:22s
epoch 49 | loss: 0.68631 | eval_custom_logloss: 1.08852 |  0:09:33s
epoch 50 | loss: 0.66981 | eval_custom_logloss: 1.51627 |  0:09:44s
epoch 51 | loss: 0.66621 | eval_custom_logloss: 0.91102 |  0:09:55s
epoch 52 | loss: 0.66276 | eval_custom_logloss: 1.10676 |  0:10:07s
epoch 53 | loss: 0.65663 | eval_custom_logloss: 1.34755 |  0:10:18s
epoch 54 | loss: 0.67387 | eval_custom_logloss: 1.82737 |  0:10:29s
epoch 55 | loss: 0.66229 | eval_custom_logloss: 0.73461 |  0:10:40s
epoch 56 | loss: 0.65489 | eval_custom_logloss: 1.15891 |  0:10:51s
epoch 57 | loss: 0.66086 | eval_custom_logloss: 0.86306 |  0:11:02s
epoch 58 | loss: 0.66462 | eval_custom_logloss: 1.11521 |  0:11:13s
epoch 59 | loss: 0.66757 | eval_custom_logloss: 1.02056 |  0:11:24s
epoch 60 | loss: 0.65879 | eval_custom_logloss: 0.72938 |  0:11:36s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.72371
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6765, 'Log Loss - std': 0.04412884544150232} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.44916 | eval_custom_logloss: 9.90517 |  0:00:10s
epoch 1  | loss: 1.13631 | eval_custom_logloss: 7.73679 |  0:00:22s
epoch 2  | loss: 1.02647 | eval_custom_logloss: 8.83957 |  0:00:34s
epoch 3  | loss: 0.96949 | eval_custom_logloss: 6.54955 |  0:00:46s
epoch 4  | loss: 0.93955 | eval_custom_logloss: 6.3999  |  0:00:58s
epoch 5  | loss: 0.90296 | eval_custom_logloss: 4.27788 |  0:01:09s
epoch 6  | loss: 0.85241 | eval_custom_logloss: 3.95824 |  0:01:20s
epoch 7  | loss: 0.87308 | eval_custom_logloss: 2.67641 |  0:01:32s
epoch 8  | loss: 0.83597 | eval_custom_logloss: 2.55818 |  0:01:43s
epoch 9  | loss: 0.83469 | eval_custom_logloss: 2.19417 |  0:01:54s
epoch 10 | loss: 0.79194 | eval_custom_logloss: 1.77846 |  0:02:05s
epoch 11 | loss: 0.8078  | eval_custom_logloss: 2.11357 |  0:02:17s
epoch 12 | loss: 0.79912 | eval_custom_logloss: 1.32079 |  0:02:28s
epoch 13 | loss: 0.77073 | eval_custom_logloss: 2.08411 |  0:02:39s
epoch 14 | loss: 0.77451 | eval_custom_logloss: 1.08205 |  0:02:50s
epoch 15 | loss: 0.7615  | eval_custom_logloss: 1.20105 |  0:03:01s
epoch 16 | loss: 0.75374 | eval_custom_logloss: 1.46103 |  0:03:13s
epoch 17 | loss: 0.73926 | eval_custom_logloss: 1.20468 |  0:03:24s
epoch 18 | loss: 0.76467 | eval_custom_logloss: 1.08415 |  0:03:36s
epoch 19 | loss: 0.74798 | eval_custom_logloss: 1.62234 |  0:03:47s
epoch 20 | loss: 0.7359  | eval_custom_logloss: 1.13832 |  0:03:58s
epoch 21 | loss: 0.73013 | eval_custom_logloss: 0.97418 |  0:04:10s
epoch 22 | loss: 0.71563 | eval_custom_logloss: 0.89772 |  0:04:21s
epoch 23 | loss: 0.70217 | eval_custom_logloss: 1.21951 |  0:04:32s
epoch 24 | loss: 0.71615 | eval_custom_logloss: 1.09673 |  0:04:43s
epoch 25 | loss: 0.70995 | eval_custom_logloss: 1.02694 |  0:04:55s
epoch 26 | loss: 0.71436 | eval_custom_logloss: 1.34621 |  0:05:06s
epoch 27 | loss: 0.70315 | eval_custom_logloss: 1.12032 |  0:05:18s
epoch 28 | loss: 0.70254 | eval_custom_logloss: 0.92289 |  0:05:30s
epoch 29 | loss: 0.69443 | eval_custom_logloss: 0.92005 |  0:05:42s
epoch 30 | loss: 0.70704 | eval_custom_logloss: 0.94152 |  0:05:53s
epoch 31 | loss: 0.69247 | eval_custom_logloss: 0.97351 |  0:06:04s
epoch 32 | loss: 0.68404 | eval_custom_logloss: 1.54147 |  0:06:15s
epoch 33 | loss: 0.69238 | eval_custom_logloss: 1.51002 |  0:06:27s
epoch 34 | loss: 0.67376 | eval_custom_logloss: 0.9509  |  0:06:38s
epoch 35 | loss: 0.6818  | eval_custom_logloss: 1.06276 |  0:06:49s
epoch 36 | loss: 0.68517 | eval_custom_logloss: 1.0183  |  0:07:00s
epoch 37 | loss: 0.67508 | eval_custom_logloss: 1.23947 |  0:07:12s
epoch 38 | loss: 0.67251 | eval_custom_logloss: 0.99345 |  0:07:24s
epoch 39 | loss: 0.67857 | eval_custom_logloss: 0.83326 |  0:07:35s
epoch 40 | loss: 0.68369 | eval_custom_logloss: 0.84476 |  0:07:47s
epoch 41 | loss: 0.69496 | eval_custom_logloss: 0.73169 |  0:07:58s
epoch 42 | loss: 0.67607 | eval_custom_logloss: 0.78535 |  0:08:10s
epoch 43 | loss: 0.67347 | eval_custom_logloss: 0.93937 |  0:08:20s
epoch 44 | loss: 0.67722 | eval_custom_logloss: 0.80419 |  0:08:32s
epoch 45 | loss: 0.6507  | eval_custom_logloss: 1.00922 |  0:08:44s
epoch 46 | loss: 0.66934 | eval_custom_logloss: 0.94077 |  0:08:55s
epoch 47 | loss: 0.65713 | eval_custom_logloss: 0.82117 |  0:09:07s
epoch 48 | loss: 0.67491 | eval_custom_logloss: 1.56281 |  0:09:18s
epoch 49 | loss: 0.6504  | eval_custom_logloss: 0.67636 |  0:09:30s
epoch 50 | loss: 0.65997 | eval_custom_logloss: 0.73251 |  0:09:41s
epoch 51 | loss: 0.67106 | eval_custom_logloss: 0.75456 |  0:09:54s
epoch 52 | loss: 0.64566 | eval_custom_logloss: 0.97471 |  0:10:06s
epoch 53 | loss: 0.65816 | eval_custom_logloss: 0.74328 |  0:10:18s
epoch 54 | loss: 0.66062 | eval_custom_logloss: 0.73842 |  0:10:30s
epoch 55 | loss: 0.65272 | eval_custom_logloss: 1.30933 |  0:10:41s
epoch 56 | loss: 0.64649 | eval_custom_logloss: 1.00554 |  0:10:54s
epoch 57 | loss: 0.63524 | eval_custom_logloss: 0.93739 |  0:11:05s
epoch 58 | loss: 0.64008 | eval_custom_logloss: 1.03769 |  0:11:17s
epoch 59 | loss: 0.65888 | eval_custom_logloss: 0.8672  |  0:11:29s
epoch 60 | loss: 0.63896 | eval_custom_logloss: 0.65258 |  0:11:41s
epoch 61 | loss: 0.64314 | eval_custom_logloss: 0.71226 |  0:11:53s
epoch 62 | loss: 0.6378  | eval_custom_logloss: 0.74166 |  0:12:05s
epoch 63 | loss: 0.63799 | eval_custom_logloss: 0.78852 |  0:12:16s
epoch 64 | loss: 0.64982 | eval_custom_logloss: 0.80508 |  0:12:27s
epoch 65 | loss: 0.6522  | eval_custom_logloss: 0.77648 |  0:12:38s
epoch 66 | loss: 0.62973 | eval_custom_logloss: 0.75529 |  0:12:50s
epoch 67 | loss: 0.64149 | eval_custom_logloss: 0.63873 |  0:13:01s
epoch 68 | loss: 0.6402  | eval_custom_logloss: 0.67973 |  0:13:12s
epoch 69 | loss: 0.64389 | eval_custom_logloss: 0.82318 |  0:13:23s
epoch 70 | loss: 0.62207 | eval_custom_logloss: 0.91385 |  0:13:35s
epoch 71 | loss: 0.66791 | eval_custom_logloss: 0.81711 |  0:13:46s
epoch 72 | loss: 0.62242 | eval_custom_logloss: 0.79305 |  0:13:57s
epoch 73 | loss: 0.62878 | eval_custom_logloss: 0.74214 |  0:14:10s
epoch 74 | loss: 0.63204 | eval_custom_logloss: 0.68845 |  0:14:21s
epoch 75 | loss: 0.6261  | eval_custom_logloss: 0.90136 |  0:14:33s
epoch 76 | loss: 0.63881 | eval_custom_logloss: 0.76804 |  0:14:45s
epoch 77 | loss: 0.62445 | eval_custom_logloss: 0.65928 |  0:14:56s
epoch 78 | loss: 0.62513 | eval_custom_logloss: 0.63849 |  0:15:07s
epoch 79 | loss: 0.63272 | eval_custom_logloss: 1.08296 |  0:15:18s
epoch 80 | loss: 0.62016 | eval_custom_logloss: 0.98276 |  0:15:30s
epoch 81 | loss: 0.62533 | eval_custom_logloss: 0.70527 |  0:15:42s
epoch 82 | loss: 0.62486 | eval_custom_logloss: 0.63696 |  0:15:53s
epoch 83 | loss: 0.62314 | eval_custom_logloss: 1.04569 |  0:16:05s
epoch 84 | loss: 0.62906 | eval_custom_logloss: 0.66562 |  0:16:17s
epoch 85 | loss: 0.62845 | eval_custom_logloss: 0.6122  |  0:16:28s
epoch 86 | loss: 0.62973 | eval_custom_logloss: 0.7088  |  0:16:39s
epoch 87 | loss: 0.61948 | eval_custom_logloss: 0.64565 |  0:16:51s
epoch 88 | loss: 0.61949 | eval_custom_logloss: 0.98189 |  0:17:02s
epoch 89 | loss: 0.62046 | eval_custom_logloss: 0.62364 |  0:17:13s
epoch 90 | loss: 0.61504 | eval_custom_logloss: 0.83229 |  0:17:24s
epoch 91 | loss: 0.62269 | eval_custom_logloss: 1.00436 |  0:17:35s
epoch 92 | loss: 0.61006 | eval_custom_logloss: 0.89776 |  0:17:46s
epoch 93 | loss: 0.61954 | eval_custom_logloss: 0.76623 |  0:17:57s
epoch 94 | loss: 0.60874 | eval_custom_logloss: 1.0127  |  0:18:08s
epoch 95 | loss: 0.60442 | eval_custom_logloss: 0.63295 |  0:18:19s
epoch 96 | loss: 0.62018 | eval_custom_logloss: 0.6327  |  0:18:30s
epoch 97 | loss: 0.62059 | eval_custom_logloss: 1.13972 |  0:18:41s
epoch 98 | loss: 0.60827 | eval_custom_logloss: 1.10161 |  0:18:52s
epoch 99 | loss: 0.60476 | eval_custom_logloss: 0.78332 |  0:19:04s
Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_eval_custom_logloss = 0.6122
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.66318, 'Log Loss - std': 0.047619046609523805} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 0.66318 and parameters: {'n_d': 29, 'n_steps': 4, 'gamma': 1.5130281819769742, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0013343268047619314, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax', 'n_a': 44, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56316 | eval_custom_logloss: 2.87257 |  0:00:16s
epoch 1  | loss: 1.23861 | eval_custom_logloss: 1.11847 |  0:00:32s
epoch 2  | loss: 1.15594 | eval_custom_logloss: 1.07617 |  0:00:48s
epoch 3  | loss: 1.14299 | eval_custom_logloss: 1.36657 |  0:01:04s
epoch 4  | loss: 1.10673 | eval_custom_logloss: 1.02407 |  0:01:20s
epoch 5  | loss: 1.03249 | eval_custom_logloss: 0.90441 |  0:01:36s
epoch 6  | loss: 1.0212  | eval_custom_logloss: 0.94973 |  0:01:52s
epoch 7  | loss: 1.01839 | eval_custom_logloss: 0.93168 |  0:02:09s
epoch 8  | loss: 0.99668 | eval_custom_logloss: 1.06114 |  0:02:25s
epoch 9  | loss: 0.97472 | eval_custom_logloss: 0.91398 |  0:02:41s
epoch 10 | loss: 0.93658 | eval_custom_logloss: 0.84362 |  0:02:57s
epoch 11 | loss: 0.93023 | eval_custom_logloss: 0.81082 |  0:03:13s
epoch 12 | loss: 0.9053  | eval_custom_logloss: 0.88015 |  0:03:29s
epoch 13 | loss: 0.89607 | eval_custom_logloss: 0.8561  |  0:03:45s
epoch 14 | loss: 0.87239 | eval_custom_logloss: 0.84904 |  0:04:01s
epoch 15 | loss: 0.86172 | eval_custom_logloss: 0.76488 |  0:04:17s
epoch 16 | loss: 0.8569  | eval_custom_logloss: 0.85489 |  0:04:33s
epoch 17 | loss: 0.84915 | eval_custom_logloss: 0.89955 |  0:04:49s
epoch 18 | loss: 0.84263 | eval_custom_logloss: 0.89689 |  0:05:05s
epoch 19 | loss: 0.8203  | eval_custom_logloss: 0.96979 |  0:05:21s
epoch 20 | loss: 0.79089 | eval_custom_logloss: 1.04892 |  0:05:37s
epoch 21 | loss: 0.79721 | eval_custom_logloss: 0.70353 |  0:05:54s
epoch 22 | loss: 0.80637 | eval_custom_logloss: 1.06208 |  0:06:10s
epoch 23 | loss: 0.82197 | eval_custom_logloss: 0.81572 |  0:06:26s
epoch 24 | loss: 0.78296 | eval_custom_logloss: 0.69194 |  0:06:42s
epoch 25 | loss: 0.81491 | eval_custom_logloss: 0.98039 |  0:06:58s
epoch 26 | loss: 0.79889 | eval_custom_logloss: 0.84277 |  0:07:14s
epoch 27 | loss: 0.7878  | eval_custom_logloss: 0.85938 |  0:07:30s
epoch 28 | loss: 0.7642  | eval_custom_logloss: 0.68187 |  0:07:46s
epoch 29 | loss: 0.78775 | eval_custom_logloss: 0.75772 |  0:08:02s
epoch 30 | loss: 0.7758  | eval_custom_logloss: 1.23322 |  0:08:18s
epoch 31 | loss: 0.76348 | eval_custom_logloss: 0.86493 |  0:08:34s
epoch 32 | loss: 0.77296 | eval_custom_logloss: 0.84487 |  0:08:50s
epoch 33 | loss: 0.7531  | eval_custom_logloss: 0.70951 |  0:09:06s
epoch 34 | loss: 0.73475 | eval_custom_logloss: 0.9293  |  0:09:22s
epoch 35 | loss: 0.73938 | eval_custom_logloss: 0.82032 |  0:09:38s
epoch 36 | loss: 0.74636 | eval_custom_logloss: 0.67493 |  0:09:54s
epoch 37 | loss: 0.7329  | eval_custom_logloss: 1.07196 |  0:10:10s
epoch 38 | loss: 0.73919 | eval_custom_logloss: 0.75666 |  0:10:26s
epoch 39 | loss: 0.75587 | eval_custom_logloss: 1.00033 |  0:10:43s
epoch 40 | loss: 0.74062 | eval_custom_logloss: 0.78314 |  0:10:59s
epoch 41 | loss: 0.72394 | eval_custom_logloss: 0.83482 |  0:11:15s
epoch 42 | loss: 0.72536 | eval_custom_logloss: 0.81048 |  0:11:30s
epoch 43 | loss: 0.73201 | eval_custom_logloss: 0.84869 |  0:11:47s
epoch 44 | loss: 0.7154  | eval_custom_logloss: 0.70816 |  0:12:03s
epoch 45 | loss: 0.72233 | eval_custom_logloss: 0.71688 |  0:12:19s
epoch 46 | loss: 0.71499 | eval_custom_logloss: 0.66108 |  0:12:35s
epoch 47 | loss: 0.71944 | eval_custom_logloss: 0.89646 |  0:12:50s
epoch 48 | loss: 0.7036  | eval_custom_logloss: 0.89659 |  0:13:05s
epoch 49 | loss: 0.70636 | eval_custom_logloss: 0.87757 |  0:13:20s
epoch 50 | loss: 0.71846 | eval_custom_logloss: 1.02665 |  0:13:35s
epoch 51 | loss: 0.70894 | eval_custom_logloss: 0.8211  |  0:13:50s
epoch 52 | loss: 0.71061 | eval_custom_logloss: 0.98146 |  0:14:06s
epoch 53 | loss: 0.71773 | eval_custom_logloss: 1.0136  |  0:14:21s
epoch 54 | loss: 0.70302 | eval_custom_logloss: 0.80546 |  0:14:36s
epoch 55 | loss: 0.71248 | eval_custom_logloss: 1.15948 |  0:14:51s
epoch 56 | loss: 0.70739 | eval_custom_logloss: 0.91816 |  0:15:06s
epoch 57 | loss: 0.70235 | eval_custom_logloss: 0.86045 |  0:15:21s
epoch 58 | loss: 0.71973 | eval_custom_logloss: 1.10259 |  0:15:36s
epoch 59 | loss: 0.69148 | eval_custom_logloss: 0.90738 |  0:15:51s
epoch 60 | loss: 0.68536 | eval_custom_logloss: 0.7393  |  0:16:06s
epoch 61 | loss: 0.6936  | eval_custom_logloss: 0.91555 |  0:16:21s
epoch 62 | loss: 0.69602 | eval_custom_logloss: 0.70493 |  0:16:37s
epoch 63 | loss: 0.68899 | eval_custom_logloss: 0.82556 |  0:16:52s
epoch 64 | loss: 0.69746 | eval_custom_logloss: 0.9112  |  0:17:07s
epoch 65 | loss: 0.6852  | eval_custom_logloss: 0.7223  |  0:17:22s
epoch 66 | loss: 0.69106 | eval_custom_logloss: 0.71763 |  0:17:37s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 0.66108
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6594, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax', 'n_a': 44, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58304 | eval_custom_logloss: 2.80885 |  0:00:15s
epoch 1  | loss: 1.2558  | eval_custom_logloss: 1.19461 |  0:00:30s
epoch 2  | loss: 1.16662 | eval_custom_logloss: 1.16644 |  0:00:45s
epoch 3  | loss: 1.15396 | eval_custom_logloss: 1.26338 |  0:01:00s
epoch 4  | loss: 1.12861 | eval_custom_logloss: 1.17196 |  0:01:15s
epoch 5  | loss: 1.06214 | eval_custom_logloss: 1.07217 |  0:01:30s
epoch 6  | loss: 1.02355 | eval_custom_logloss: 0.94202 |  0:01:45s
epoch 7  | loss: 1.0048  | eval_custom_logloss: 0.86478 |  0:02:00s
epoch 8  | loss: 0.99134 | eval_custom_logloss: 0.96181 |  0:02:16s
epoch 9  | loss: 1.01059 | eval_custom_logloss: 0.98844 |  0:02:31s
epoch 10 | loss: 0.95856 | eval_custom_logloss: 0.88989 |  0:02:46s
epoch 11 | loss: 0.96327 | eval_custom_logloss: 0.86358 |  0:03:01s
epoch 12 | loss: 0.95695 | eval_custom_logloss: 1.21211 |  0:03:16s
epoch 13 | loss: 0.9376  | eval_custom_logloss: 0.85349 |  0:03:32s
epoch 14 | loss: 0.90394 | eval_custom_logloss: 0.89767 |  0:03:47s
epoch 15 | loss: 0.91453 | eval_custom_logloss: 0.80102 |  0:04:02s
epoch 16 | loss: 0.88306 | eval_custom_logloss: 0.91156 |  0:04:17s
epoch 17 | loss: 0.88383 | eval_custom_logloss: 0.82219 |  0:04:33s
epoch 18 | loss: 0.87179 | eval_custom_logloss: 0.88266 |  0:04:48s
epoch 19 | loss: 0.85538 | eval_custom_logloss: 0.85581 |  0:05:03s
epoch 20 | loss: 0.86844 | eval_custom_logloss: 0.83078 |  0:05:17s
epoch 21 | loss: 0.84522 | eval_custom_logloss: 0.81014 |  0:05:32s
epoch 22 | loss: 0.85798 | eval_custom_logloss: 0.81424 |  0:05:47s
epoch 23 | loss: 0.83017 | eval_custom_logloss: 0.78062 |  0:06:02s
epoch 24 | loss: 0.83244 | eval_custom_logloss: 0.85819 |  0:06:17s
epoch 25 | loss: 0.81494 | eval_custom_logloss: 0.76772 |  0:06:32s
epoch 26 | loss: 0.80035 | eval_custom_logloss: 0.81885 |  0:06:47s
epoch 27 | loss: 0.80548 | eval_custom_logloss: 0.75968 |  0:07:02s
epoch 28 | loss: 0.79621 | eval_custom_logloss: 0.80833 |  0:07:18s
epoch 29 | loss: 0.76922 | eval_custom_logloss: 1.02339 |  0:07:33s
epoch 30 | loss: 0.77699 | eval_custom_logloss: 1.01836 |  0:07:48s
epoch 31 | loss: 0.74477 | eval_custom_logloss: 0.96244 |  0:08:03s
epoch 32 | loss: 0.75282 | eval_custom_logloss: 0.81142 |  0:08:18s
epoch 33 | loss: 0.7601  | eval_custom_logloss: 0.85898 |  0:08:33s
epoch 34 | loss: 0.73948 | eval_custom_logloss: 0.85125 |  0:08:48s
epoch 35 | loss: 0.74757 | eval_custom_logloss: 0.80534 |  0:09:03s
epoch 36 | loss: 0.74901 | eval_custom_logloss: 0.87534 |  0:09:18s
epoch 37 | loss: 0.74505 | eval_custom_logloss: 0.74649 |  0:09:33s
epoch 38 | loss: 0.73111 | eval_custom_logloss: 0.70536 |  0:09:48s
epoch 39 | loss: 0.73621 | eval_custom_logloss: 0.91766 |  0:10:03s
epoch 40 | loss: 0.7315  | eval_custom_logloss: 0.73319 |  0:10:18s
epoch 41 | loss: 0.70823 | eval_custom_logloss: 0.73718 |  0:10:34s
epoch 42 | loss: 0.71819 | eval_custom_logloss: 0.71236 |  0:10:50s
epoch 43 | loss: 0.73945 | eval_custom_logloss: 0.76577 |  0:11:07s
epoch 44 | loss: 0.71521 | eval_custom_logloss: 0.81462 |  0:11:24s
epoch 45 | loss: 0.72816 | eval_custom_logloss: 0.91744 |  0:11:40s
epoch 46 | loss: 0.71735 | eval_custom_logloss: 0.75403 |  0:11:57s
epoch 47 | loss: 0.71178 | eval_custom_logloss: 0.76521 |  0:12:14s
epoch 48 | loss: 0.71035 | eval_custom_logloss: 0.80545 |  0:12:31s
epoch 49 | loss: 0.70671 | eval_custom_logloss: 0.70347 |  0:12:47s
epoch 50 | loss: 0.70754 | eval_custom_logloss: 0.79155 |  0:13:03s
epoch 51 | loss: 0.71331 | eval_custom_logloss: 0.78971 |  0:13:18s
epoch 52 | loss: 0.7072  | eval_custom_logloss: 0.71188 |  0:13:33s
epoch 53 | loss: 0.71133 | eval_custom_logloss: 1.07281 |  0:13:48s
epoch 54 | loss: 0.69158 | eval_custom_logloss: 0.89711 |  0:14:03s
epoch 55 | loss: 0.69031 | eval_custom_logloss: 0.68592 |  0:14:18s
epoch 56 | loss: 0.72147 | eval_custom_logloss: 1.07447 |  0:14:33s
epoch 57 | loss: 0.70671 | eval_custom_logloss: 0.83655 |  0:14:48s
epoch 58 | loss: 0.69121 | eval_custom_logloss: 0.8544  |  0:15:03s
epoch 59 | loss: 0.69845 | eval_custom_logloss: 0.84315 |  0:15:18s
epoch 60 | loss: 0.7064  | eval_custom_logloss: 0.89222 |  0:15:33s
epoch 61 | loss: 0.6855  | eval_custom_logloss: 0.89145 |  0:15:48s
epoch 62 | loss: 0.71035 | eval_custom_logloss: 0.76701 |  0:16:03s
epoch 63 | loss: 0.6906  | eval_custom_logloss: 1.12593 |  0:16:18s
epoch 64 | loss: 0.71046 | eval_custom_logloss: 0.92658 |  0:16:33s
epoch 65 | loss: 0.72934 | eval_custom_logloss: 0.93462 |  0:16:49s
epoch 66 | loss: 0.72708 | eval_custom_logloss: 0.8016  |  0:17:04s
epoch 67 | loss: 0.70399 | eval_custom_logloss: 0.80742 |  0:17:19s
epoch 68 | loss: 0.69885 | eval_custom_logloss: 1.02515 |  0:17:34s
epoch 69 | loss: 0.7216  | eval_custom_logloss: 0.87339 |  0:17:49s
epoch 70 | loss: 0.72016 | eval_custom_logloss: 1.24324 |  0:18:05s
epoch 71 | loss: 0.7     | eval_custom_logloss: 0.79187 |  0:18:20s
epoch 72 | loss: 0.69628 | eval_custom_logloss: 1.06045 |  0:18:35s
epoch 73 | loss: 0.69538 | eval_custom_logloss: 0.70786 |  0:18:50s
epoch 74 | loss: 0.71105 | eval_custom_logloss: 0.8273  |  0:19:05s
epoch 75 | loss: 0.70621 | eval_custom_logloss: 0.83799 |  0:19:20s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.68592
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6718500000000001, 'Log Loss - std': 0.012450000000000017} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax', 'n_a': 44, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56182 | eval_custom_logloss: 1.9551  |  0:00:15s
epoch 1  | loss: 1.24748 | eval_custom_logloss: 1.27039 |  0:00:30s
epoch 2  | loss: 1.10756 | eval_custom_logloss: 1.04096 |  0:00:45s
epoch 3  | loss: 1.05998 | eval_custom_logloss: 1.03591 |  0:01:00s
epoch 4  | loss: 1.04139 | eval_custom_logloss: 0.96429 |  0:01:15s
epoch 5  | loss: 1.00062 | eval_custom_logloss: 1.04721 |  0:01:31s
epoch 6  | loss: 0.963   | eval_custom_logloss: 1.18153 |  0:01:46s
epoch 7  | loss: 0.94391 | eval_custom_logloss: 0.94239 |  0:02:01s
epoch 8  | loss: 0.92015 | eval_custom_logloss: 0.92665 |  0:02:16s
epoch 9  | loss: 0.90708 | eval_custom_logloss: 0.8965  |  0:02:31s
epoch 10 | loss: 0.88476 | eval_custom_logloss: 0.95346 |  0:02:46s
epoch 11 | loss: 0.86725 | eval_custom_logloss: 1.06156 |  0:03:01s
epoch 12 | loss: 0.85647 | eval_custom_logloss: 0.90153 |  0:03:16s
epoch 13 | loss: 0.84174 | eval_custom_logloss: 0.86974 |  0:03:32s
epoch 14 | loss: 0.82299 | eval_custom_logloss: 1.10768 |  0:03:47s
epoch 15 | loss: 0.82003 | eval_custom_logloss: 0.80128 |  0:04:02s
epoch 16 | loss: 0.79773 | eval_custom_logloss: 0.88591 |  0:04:17s
epoch 17 | loss: 0.8028  | eval_custom_logloss: 0.91646 |  0:04:32s
epoch 18 | loss: 0.78101 | eval_custom_logloss: 0.81981 |  0:04:47s
epoch 19 | loss: 0.75509 | eval_custom_logloss: 0.89919 |  0:05:02s
epoch 20 | loss: 0.73848 | eval_custom_logloss: 0.73711 |  0:05:17s
epoch 21 | loss: 0.74325 | eval_custom_logloss: 0.94907 |  0:05:33s
epoch 22 | loss: 0.74259 | eval_custom_logloss: 0.84444 |  0:05:48s
epoch 23 | loss: 0.73431 | eval_custom_logloss: 0.86642 |  0:06:03s
epoch 24 | loss: 0.72598 | eval_custom_logloss: 0.81321 |  0:06:18s
epoch 25 | loss: 0.71853 | eval_custom_logloss: 1.03846 |  0:06:33s
epoch 26 | loss: 0.72302 | eval_custom_logloss: 0.72932 |  0:06:48s
epoch 27 | loss: 0.70361 | eval_custom_logloss: 0.78627 |  0:07:03s
epoch 28 | loss: 0.70733 | eval_custom_logloss: 0.86    |  0:07:19s
epoch 29 | loss: 0.70245 | eval_custom_logloss: 0.71398 |  0:07:34s
epoch 30 | loss: 0.69898 | eval_custom_logloss: 0.91286 |  0:07:49s
epoch 31 | loss: 0.69224 | eval_custom_logloss: 0.68365 |  0:08:04s
epoch 32 | loss: 0.69875 | eval_custom_logloss: 0.93339 |  0:08:19s
epoch 33 | loss: 0.68011 | eval_custom_logloss: 0.71615 |  0:08:35s
epoch 34 | loss: 0.67547 | eval_custom_logloss: 0.88328 |  0:08:50s
epoch 35 | loss: 0.68782 | eval_custom_logloss: 0.76444 |  0:09:05s
epoch 36 | loss: 0.67373 | eval_custom_logloss: 1.30532 |  0:09:20s
epoch 37 | loss: 0.67136 | eval_custom_logloss: 0.67513 |  0:09:35s
epoch 38 | loss: 0.6771  | eval_custom_logloss: 1.05523 |  0:09:50s
epoch 39 | loss: 0.68981 | eval_custom_logloss: 0.71395 |  0:10:06s
epoch 40 | loss: 0.67678 | eval_custom_logloss: 1.12857 |  0:10:21s
epoch 41 | loss: 0.66697 | eval_custom_logloss: 1.14472 |  0:10:36s
epoch 42 | loss: 0.64838 | eval_custom_logloss: 0.85116 |  0:10:51s
epoch 43 | loss: 0.67081 | eval_custom_logloss: 0.86939 |  0:11:06s
epoch 44 | loss: 0.65478 | eval_custom_logloss: 0.76253 |  0:11:21s
epoch 45 | loss: 0.65204 | eval_custom_logloss: 0.84199 |  0:11:36s
epoch 46 | loss: 0.64423 | eval_custom_logloss: 0.92077 |  0:11:52s
epoch 47 | loss: 0.65209 | eval_custom_logloss: 0.83255 |  0:12:07s
epoch 48 | loss: 0.64366 | eval_custom_logloss: 0.78759 |  0:12:22s
epoch 49 | loss: 0.6569  | eval_custom_logloss: 0.674   |  0:12:37s
epoch 50 | loss: 0.64532 | eval_custom_logloss: 0.68099 |  0:12:52s
epoch 51 | loss: 0.6477  | eval_custom_logloss: 0.8336  |  0:13:08s
epoch 52 | loss: 0.65757 | eval_custom_logloss: 0.63119 |  0:13:23s
epoch 53 | loss: 0.64408 | eval_custom_logloss: 0.74956 |  0:13:38s
epoch 54 | loss: 0.65183 | eval_custom_logloss: 0.77341 |  0:13:53s
epoch 55 | loss: 0.64623 | eval_custom_logloss: 0.8348  |  0:14:09s
epoch 56 | loss: 0.6369  | eval_custom_logloss: 0.64035 |  0:14:24s
epoch 57 | loss: 0.64216 | eval_custom_logloss: 0.65595 |  0:14:39s
epoch 58 | loss: 0.64755 | eval_custom_logloss: 0.85119 |  0:14:54s
epoch 59 | loss: 0.6383  | eval_custom_logloss: 0.78245 |  0:15:10s
epoch 60 | loss: 0.63222 | eval_custom_logloss: 0.63    |  0:15:25s
epoch 61 | loss: 0.64365 | eval_custom_logloss: 0.93765 |  0:15:40s
epoch 62 | loss: 0.63233 | eval_custom_logloss: 0.89727 |  0:15:55s
epoch 63 | loss: 0.62568 | eval_custom_logloss: 0.86273 |  0:16:11s
epoch 64 | loss: 0.63836 | eval_custom_logloss: 0.86686 |  0:16:26s
epoch 65 | loss: 0.64057 | eval_custom_logloss: 0.94483 |  0:16:41s
epoch 66 | loss: 0.64508 | eval_custom_logloss: 1.03232 |  0:16:56s
epoch 67 | loss: 0.633   | eval_custom_logloss: 0.71053 |  0:17:11s
epoch 68 | loss: 0.6284  | eval_custom_logloss: 0.62815 |  0:17:27s
epoch 69 | loss: 0.63408 | eval_custom_logloss: 0.87182 |  0:17:42s
epoch 70 | loss: 0.62797 | eval_custom_logloss: 0.82085 |  0:17:57s
epoch 71 | loss: 0.62659 | eval_custom_logloss: 1.0504  |  0:18:12s
epoch 72 | loss: 0.62288 | eval_custom_logloss: 0.85747 |  0:18:27s
epoch 73 | loss: 0.6208  | eval_custom_logloss: 0.8837  |  0:18:43s
epoch 74 | loss: 0.6195  | eval_custom_logloss: 1.36257 |  0:18:58s
epoch 75 | loss: 0.62562 | eval_custom_logloss: 0.87942 |  0:19:13s
epoch 76 | loss: 0.64263 | eval_custom_logloss: 1.08357 |  0:19:28s
epoch 77 | loss: 0.62438 | eval_custom_logloss: 0.81513 |  0:19:43s
epoch 78 | loss: 0.61842 | eval_custom_logloss: 0.82104 |  0:19:59s
epoch 79 | loss: 0.61875 | eval_custom_logloss: 0.66301 |  0:20:14s
epoch 80 | loss: 0.61344 | eval_custom_logloss: 0.89326 |  0:20:29s
epoch 81 | loss: 0.62189 | eval_custom_logloss: 0.83885 |  0:20:44s
epoch 82 | loss: 0.61241 | eval_custom_logloss: 0.872   |  0:20:59s
epoch 83 | loss: 0.6347  | eval_custom_logloss: 1.17454 |  0:21:15s
epoch 84 | loss: 0.6173  | eval_custom_logloss: 0.80058 |  0:21:30s
epoch 85 | loss: 0.61109 | eval_custom_logloss: 0.9805  |  0:21:45s
epoch 86 | loss: 0.61374 | eval_custom_logloss: 1.29293 |  0:22:00s
epoch 87 | loss: 0.61244 | eval_custom_logloss: 0.773   |  0:22:15s
epoch 88 | loss: 0.5993  | eval_custom_logloss: 0.60794 |  0:22:31s
epoch 89 | loss: 0.61086 | eval_custom_logloss: 1.44734 |  0:22:46s
epoch 90 | loss: 0.62202 | eval_custom_logloss: 0.79054 |  0:23:01s
epoch 91 | loss: 0.60708 | eval_custom_logloss: 0.80994 |  0:23:17s
epoch 92 | loss: 0.60378 | eval_custom_logloss: 0.94426 |  0:23:32s
epoch 93 | loss: 0.60686 | eval_custom_logloss: 0.7832  |  0:23:47s
epoch 94 | loss: 0.60978 | eval_custom_logloss: 1.11347 |  0:24:02s
epoch 95 | loss: 0.60519 | eval_custom_logloss: 0.89601 |  0:24:18s
epoch 96 | loss: 0.59391 | eval_custom_logloss: 1.64753 |  0:24:33s
epoch 97 | loss: 0.59342 | eval_custom_logloss: 0.90236 |  0:24:48s
epoch 98 | loss: 0.60887 | eval_custom_logloss: 0.72068 |  0:25:04s
epoch 99 | loss: 0.59395 | eval_custom_logloss: 0.79553 |  0:25:19s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 0.60794
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6496666666666667, 'Log Loss - std': 0.032977803983218125} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax', 'n_a': 44, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.55284 | eval_custom_logloss: 2.18363 |  0:00:15s
epoch 1  | loss: 1.25072 | eval_custom_logloss: 1.68043 |  0:00:30s
epoch 2  | loss: 1.12651 | eval_custom_logloss: 1.00204 |  0:00:45s
epoch 3  | loss: 1.06475 | eval_custom_logloss: 1.08566 |  0:01:00s
epoch 4  | loss: 1.04964 | eval_custom_logloss: 1.59543 |  0:01:15s
epoch 5  | loss: 1.04879 | eval_custom_logloss: 1.07772 |  0:01:31s
epoch 6  | loss: 0.99885 | eval_custom_logloss: 0.88657 |  0:01:46s
epoch 7  | loss: 1.00255 | eval_custom_logloss: 0.93285 |  0:02:01s
epoch 8  | loss: 0.99446 | eval_custom_logloss: 0.91384 |  0:02:16s
epoch 9  | loss: 0.9848  | eval_custom_logloss: 0.93545 |  0:02:32s
epoch 10 | loss: 0.96934 | eval_custom_logloss: 1.06661 |  0:02:47s
epoch 11 | loss: 0.9428  | eval_custom_logloss: 2.00694 |  0:03:02s
epoch 12 | loss: 0.92993 | eval_custom_logloss: 0.98338 |  0:03:17s
epoch 13 | loss: 0.93866 | eval_custom_logloss: 1.3253  |  0:03:32s
epoch 14 | loss: 0.91602 | eval_custom_logloss: 0.98543 |  0:03:48s
epoch 15 | loss: 0.91649 | eval_custom_logloss: 1.95096 |  0:04:03s
epoch 16 | loss: 0.89613 | eval_custom_logloss: 0.97165 |  0:04:18s
epoch 17 | loss: 0.88916 | eval_custom_logloss: 0.91086 |  0:04:33s
epoch 18 | loss: 0.87426 | eval_custom_logloss: 0.79064 |  0:04:49s
epoch 19 | loss: 0.88647 | eval_custom_logloss: 0.86533 |  0:05:05s
epoch 20 | loss: 0.85554 | eval_custom_logloss: 1.7538  |  0:05:21s
epoch 21 | loss: 0.86614 | eval_custom_logloss: 0.88818 |  0:05:36s
epoch 22 | loss: 0.86672 | eval_custom_logloss: 1.33389 |  0:05:51s
epoch 23 | loss: 0.84454 | eval_custom_logloss: 0.95788 |  0:06:06s
epoch 24 | loss: 0.84854 | eval_custom_logloss: 1.85051 |  0:06:21s
epoch 25 | loss: 0.84509 | eval_custom_logloss: 1.06369 |  0:06:36s
epoch 26 | loss: 0.84586 | eval_custom_logloss: 1.60342 |  0:06:52s
epoch 27 | loss: 0.85259 | eval_custom_logloss: 0.82241 |  0:07:07s
epoch 28 | loss: 0.84645 | eval_custom_logloss: 0.80805 |  0:07:22s
epoch 29 | loss: 0.85632 | eval_custom_logloss: 1.14033 |  0:07:37s
epoch 30 | loss: 0.83807 | eval_custom_logloss: 0.78394 |  0:07:52s
epoch 31 | loss: 0.83671 | eval_custom_logloss: 1.43169 |  0:08:07s
epoch 32 | loss: 0.83843 | eval_custom_logloss: 0.8184  |  0:08:22s
epoch 33 | loss: 0.83308 | eval_custom_logloss: 0.9272  |  0:08:37s
epoch 34 | loss: 0.82201 | eval_custom_logloss: 0.86401 |  0:08:53s
epoch 35 | loss: 0.8173  | eval_custom_logloss: 1.03447 |  0:09:08s
epoch 36 | loss: 0.81294 | eval_custom_logloss: 1.18303 |  0:09:23s
epoch 37 | loss: 0.84113 | eval_custom_logloss: 1.18182 |  0:09:39s
epoch 38 | loss: 0.82232 | eval_custom_logloss: 1.43285 |  0:09:54s
epoch 39 | loss: 0.82035 | eval_custom_logloss: 1.14022 |  0:10:09s
epoch 40 | loss: 0.80289 | eval_custom_logloss: 0.76365 |  0:10:24s
epoch 41 | loss: 0.7917  | eval_custom_logloss: 0.89677 |  0:10:39s
epoch 42 | loss: 0.78684 | eval_custom_logloss: 0.92709 |  0:10:54s
epoch 43 | loss: 0.8095  | eval_custom_logloss: 0.79219 |  0:11:09s
epoch 44 | loss: 0.79727 | eval_custom_logloss: 0.85161 |  0:11:24s
epoch 45 | loss: 0.81421 | eval_custom_logloss: 0.75774 |  0:11:40s
epoch 46 | loss: 0.80839 | eval_custom_logloss: 0.73494 |  0:11:55s
epoch 47 | loss: 0.81034 | eval_custom_logloss: 0.92248 |  0:12:10s
epoch 48 | loss: 0.80576 | eval_custom_logloss: 0.81884 |  0:12:25s
epoch 49 | loss: 0.79923 | eval_custom_logloss: 0.99165 |  0:12:40s
epoch 50 | loss: 0.79831 | eval_custom_logloss: 0.84633 |  0:12:55s
epoch 51 | loss: 0.79431 | eval_custom_logloss: 0.72473 |  0:13:10s
epoch 52 | loss: 0.79973 | eval_custom_logloss: 1.34427 |  0:13:25s
epoch 53 | loss: 0.79129 | eval_custom_logloss: 1.85004 |  0:13:40s
epoch 54 | loss: 0.78787 | eval_custom_logloss: 0.95575 |  0:13:56s
epoch 55 | loss: 0.78363 | eval_custom_logloss: 1.47636 |  0:14:11s
epoch 56 | loss: 0.77098 | eval_custom_logloss: 0.80142 |  0:14:26s
epoch 57 | loss: 0.78801 | eval_custom_logloss: 0.9409  |  0:14:41s
epoch 58 | loss: 0.76997 | eval_custom_logloss: 0.72836 |  0:14:57s
epoch 59 | loss: 0.76655 | eval_custom_logloss: 1.06664 |  0:15:12s
epoch 60 | loss: 0.77866 | eval_custom_logloss: 0.7968  |  0:15:27s
epoch 61 | loss: 0.77837 | eval_custom_logloss: 0.92861 |  0:15:42s
epoch 62 | loss: 0.77778 | eval_custom_logloss: 1.54447 |  0:15:57s
epoch 63 | loss: 0.76826 | eval_custom_logloss: 0.82568 |  0:16:13s
epoch 64 | loss: 0.76259 | eval_custom_logloss: 1.01472 |  0:16:28s
epoch 65 | loss: 0.78239 | eval_custom_logloss: 0.79771 |  0:16:43s
epoch 66 | loss: 0.77155 | eval_custom_logloss: 0.86111 |  0:16:58s
epoch 67 | loss: 0.76681 | eval_custom_logloss: 0.8458  |  0:17:13s
epoch 68 | loss: 0.76452 | eval_custom_logloss: 0.72689 |  0:17:28s
epoch 69 | loss: 0.76246 | eval_custom_logloss: 0.88337 |  0:17:43s
epoch 70 | loss: 0.7609  | eval_custom_logloss: 0.91144 |  0:17:58s
epoch 71 | loss: 0.74767 | eval_custom_logloss: 0.70857 |  0:18:14s
epoch 72 | loss: 0.75637 | eval_custom_logloss: 0.72605 |  0:18:29s
epoch 73 | loss: 0.76781 | eval_custom_logloss: 0.74132 |  0:18:45s
epoch 74 | loss: 0.75758 | eval_custom_logloss: 1.23429 |  0:19:00s
epoch 75 | loss: 0.76048 | eval_custom_logloss: 1.47325 |  0:19:15s
epoch 76 | loss: 0.78047 | eval_custom_logloss: 1.0768  |  0:19:30s
epoch 77 | loss: 0.77528 | eval_custom_logloss: 1.36198 |  0:19:45s
epoch 78 | loss: 0.75851 | eval_custom_logloss: 0.75309 |  0:20:00s
epoch 79 | loss: 0.75288 | eval_custom_logloss: 0.71512 |  0:20:16s
epoch 80 | loss: 0.75361 | eval_custom_logloss: 0.71062 |  0:20:31s
epoch 81 | loss: 0.74402 | eval_custom_logloss: 0.78656 |  0:20:46s
epoch 82 | loss: 0.74768 | eval_custom_logloss: 1.39769 |  0:21:02s
epoch 83 | loss: 0.76291 | eval_custom_logloss: 1.03701 |  0:21:17s
epoch 84 | loss: 0.76028 | eval_custom_logloss: 0.9544  |  0:21:33s
epoch 85 | loss: 0.76308 | eval_custom_logloss: 0.88725 |  0:21:48s
epoch 86 | loss: 0.75546 | eval_custom_logloss: 0.749   |  0:22:03s
epoch 87 | loss: 0.75472 | eval_custom_logloss: 0.76667 |  0:22:18s
epoch 88 | loss: 0.75031 | eval_custom_logloss: 0.70207 |  0:22:34s
epoch 89 | loss: 0.74259 | eval_custom_logloss: 1.05927 |  0:22:49s
epoch 90 | loss: 0.7507  | eval_custom_logloss: 0.71894 |  0:23:04s
epoch 91 | loss: 0.74374 | eval_custom_logloss: 1.15153 |  0:23:19s
epoch 92 | loss: 0.7471  | eval_custom_logloss: 0.76327 |  0:23:34s
epoch 93 | loss: 0.75231 | eval_custom_logloss: 0.72631 |  0:23:50s
epoch 94 | loss: 0.74316 | eval_custom_logloss: 0.68396 |  0:24:05s
epoch 95 | loss: 0.73667 | eval_custom_logloss: 0.74879 |  0:24:20s
epoch 96 | loss: 0.74774 | eval_custom_logloss: 2.2573  |  0:24:36s
epoch 97 | loss: 0.7518  | eval_custom_logloss: 0.80118 |  0:24:51s
epoch 98 | loss: 0.74267 | eval_custom_logloss: 0.77088 |  0:25:06s
epoch 99 | loss: 0.74409 | eval_custom_logloss: 0.76629 |  0:25:21s
Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_eval_custom_logloss = 0.68396
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.65805, 'Log Loss - std': 0.03203892164227757} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax', 'n_a': 44, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.59585 | eval_custom_logloss: 1.58028 |  0:00:15s
epoch 1  | loss: 1.17438 | eval_custom_logloss: 2.02957 |  0:00:30s
epoch 2  | loss: 1.08131 | eval_custom_logloss: 1.44089 |  0:00:45s
epoch 3  | loss: 1.06169 | eval_custom_logloss: 2.09049 |  0:01:00s
epoch 4  | loss: 1.01032 | eval_custom_logloss: 1.52113 |  0:01:15s
epoch 5  | loss: 0.99531 | eval_custom_logloss: 0.88294 |  0:01:31s
epoch 6  | loss: 0.97976 | eval_custom_logloss: 0.84113 |  0:01:46s
epoch 7  | loss: 0.96279 | eval_custom_logloss: 1.58321 |  0:02:01s
epoch 8  | loss: 0.93254 | eval_custom_logloss: 0.97918 |  0:02:17s
epoch 9  | loss: 0.93649 | eval_custom_logloss: 0.8376  |  0:02:32s
epoch 10 | loss: 0.91855 | eval_custom_logloss: 0.94329 |  0:02:47s
epoch 11 | loss: 0.91097 | eval_custom_logloss: 0.94884 |  0:03:03s
epoch 12 | loss: 0.90627 | eval_custom_logloss: 1.23808 |  0:03:18s
epoch 13 | loss: 0.90354 | eval_custom_logloss: 1.61336 |  0:03:34s
epoch 14 | loss: 0.87317 | eval_custom_logloss: 1.29343 |  0:03:49s
epoch 15 | loss: 0.85036 | eval_custom_logloss: 1.02893 |  0:04:05s
epoch 16 | loss: 0.84205 | eval_custom_logloss: 0.76083 |  0:04:20s
epoch 17 | loss: 0.8574  | eval_custom_logloss: 1.40673 |  0:04:35s
epoch 18 | loss: 0.83287 | eval_custom_logloss: 1.00714 |  0:04:50s
epoch 19 | loss: 0.826   | eval_custom_logloss: 0.77915 |  0:05:06s
epoch 20 | loss: 0.8004  | eval_custom_logloss: 1.20181 |  0:05:21s
epoch 21 | loss: 0.7998  | eval_custom_logloss: 0.96753 |  0:05:37s
epoch 22 | loss: 0.80466 | eval_custom_logloss: 0.9116  |  0:05:52s
epoch 23 | loss: 0.78002 | eval_custom_logloss: 0.79498 |  0:06:08s
epoch 24 | loss: 0.77881 | eval_custom_logloss: 0.79479 |  0:06:24s
epoch 25 | loss: 0.77602 | eval_custom_logloss: 1.15762 |  0:06:39s
epoch 26 | loss: 0.79568 | eval_custom_logloss: 0.82418 |  0:06:54s
epoch 27 | loss: 0.77696 | eval_custom_logloss: 0.90799 |  0:07:09s
epoch 28 | loss: 0.76689 | eval_custom_logloss: 1.06623 |  0:07:25s
epoch 29 | loss: 0.77725 | eval_custom_logloss: 0.81425 |  0:07:40s
epoch 30 | loss: 0.77124 | eval_custom_logloss: 1.69607 |  0:07:56s
epoch 31 | loss: 0.75906 | eval_custom_logloss: 1.0015  |  0:08:11s
epoch 32 | loss: 0.75758 | eval_custom_logloss: 0.99699 |  0:08:26s
epoch 33 | loss: 0.73136 | eval_custom_logloss: 0.81237 |  0:08:41s
epoch 34 | loss: 0.73407 | eval_custom_logloss: 0.95678 |  0:08:57s
epoch 35 | loss: 0.73232 | eval_custom_logloss: 0.94089 |  0:09:12s
epoch 36 | loss: 0.72786 | eval_custom_logloss: 1.20873 |  0:09:27s

Early stopping occurred at epoch 36 with best_epoch = 16 and best_eval_custom_logloss = 0.76083
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6782600000000001, 'Log Loss - std': 0.04954765786593754} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 0.6782600000000001 and parameters: {'n_d': 44, 'n_steps': 6, 'gamma': 1.2955211870155094, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.009222147367324957, 'mask_type': 'sparsemax'}. Best is trial 8 with value: 0.6826800000000001.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.62913 | eval_custom_logloss: 3.09215 |  0:00:15s
epoch 1  | loss: 1.25101 | eval_custom_logloss: 4.16955 |  0:00:30s
epoch 2  | loss: 1.11896 | eval_custom_logloss: 1.39676 |  0:00:46s
epoch 3  | loss: 1.05222 | eval_custom_logloss: 1.28046 |  0:01:01s
epoch 4  | loss: 1.05075 | eval_custom_logloss: 1.39415 |  0:01:17s
epoch 5  | loss: 1.02654 | eval_custom_logloss: 1.25794 |  0:01:32s
epoch 6  | loss: 0.98964 | eval_custom_logloss: 1.2931  |  0:01:48s
epoch 7  | loss: 0.97833 | eval_custom_logloss: 0.96324 |  0:02:03s
epoch 8  | loss: 0.96658 | eval_custom_logloss: 0.85094 |  0:02:19s
epoch 9  | loss: 0.93635 | eval_custom_logloss: 1.25754 |  0:02:34s
epoch 10 | loss: 0.95297 | eval_custom_logloss: 0.89511 |  0:02:50s
epoch 11 | loss: 0.93669 | eval_custom_logloss: 0.95604 |  0:03:05s
epoch 12 | loss: 0.90733 | eval_custom_logloss: 1.01466 |  0:03:21s
epoch 13 | loss: 0.89293 | eval_custom_logloss: 0.90048 |  0:03:36s
epoch 14 | loss: 0.88689 | eval_custom_logloss: 0.94669 |  0:03:52s
epoch 15 | loss: 0.88168 | eval_custom_logloss: 0.82905 |  0:04:08s
epoch 16 | loss: 0.86037 | eval_custom_logloss: 0.99014 |  0:04:24s
epoch 17 | loss: 0.85012 | eval_custom_logloss: 0.8643  |  0:04:39s
epoch 18 | loss: 0.85    | eval_custom_logloss: 0.98479 |  0:04:55s
epoch 19 | loss: 0.84606 | eval_custom_logloss: 0.78838 |  0:05:11s
epoch 20 | loss: 0.84395 | eval_custom_logloss: 0.87765 |  0:05:26s
epoch 21 | loss: 0.83299 | eval_custom_logloss: 0.80126 |  0:05:42s
epoch 22 | loss: 0.82888 | eval_custom_logloss: 0.77324 |  0:05:57s
epoch 23 | loss: 0.82501 | eval_custom_logloss: 0.97798 |  0:06:13s
epoch 24 | loss: 0.79489 | eval_custom_logloss: 0.87583 |  0:06:29s
epoch 25 | loss: 0.76482 | eval_custom_logloss: 1.16942 |  0:06:44s
epoch 26 | loss: 0.76069 | eval_custom_logloss: 1.20575 |  0:07:00s
epoch 27 | loss: 0.74911 | eval_custom_logloss: 1.04038 |  0:07:15s
epoch 28 | loss: 0.74574 | eval_custom_logloss: 1.59555 |  0:07:31s
epoch 29 | loss: 0.73812 | eval_custom_logloss: 0.67825 |  0:07:46s
epoch 30 | loss: 0.73622 | eval_custom_logloss: 0.75501 |  0:08:02s
epoch 31 | loss: 0.72522 | eval_custom_logloss: 0.88174 |  0:08:18s
epoch 32 | loss: 0.71394 | eval_custom_logloss: 1.09542 |  0:08:33s
epoch 33 | loss: 0.71458 | eval_custom_logloss: 0.91151 |  0:08:49s
epoch 34 | loss: 0.74005 | eval_custom_logloss: 0.69071 |  0:09:04s
epoch 35 | loss: 0.71088 | eval_custom_logloss: 0.79834 |  0:09:20s
epoch 36 | loss: 0.71329 | eval_custom_logloss: 1.49369 |  0:09:35s
epoch 37 | loss: 0.69713 | eval_custom_logloss: 0.72535 |  0:09:51s
epoch 38 | loss: 0.7063  | eval_custom_logloss: 0.75962 |  0:10:06s
epoch 39 | loss: 0.70126 | eval_custom_logloss: 0.99263 |  0:10:22s
epoch 40 | loss: 0.73914 | eval_custom_logloss: 0.92264 |  0:10:37s
epoch 41 | loss: 0.71252 | eval_custom_logloss: 0.97113 |  0:10:53s
epoch 42 | loss: 0.70478 | eval_custom_logloss: 0.90132 |  0:11:08s
epoch 43 | loss: 0.69135 | eval_custom_logloss: 0.85959 |  0:11:24s
epoch 44 | loss: 0.69684 | eval_custom_logloss: 1.10072 |  0:11:39s
epoch 45 | loss: 0.69275 | eval_custom_logloss: 1.79788 |  0:11:55s
epoch 46 | loss: 0.7027  | eval_custom_logloss: 0.89266 |  0:12:10s
epoch 47 | loss: 0.68901 | eval_custom_logloss: 0.83903 |  0:12:26s
epoch 48 | loss: 0.68012 | eval_custom_logloss: 1.16568 |  0:12:42s
epoch 49 | loss: 0.69155 | eval_custom_logloss: 1.20682 |  0:12:58s

Early stopping occurred at epoch 49 with best_epoch = 29 and best_eval_custom_logloss = 0.67825
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6772, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.62004 | eval_custom_logloss: 3.04985 |  0:00:15s
epoch 1  | loss: 1.25879 | eval_custom_logloss: 2.24447 |  0:00:31s
epoch 2  | loss: 1.15069 | eval_custom_logloss: 1.30345 |  0:00:47s
epoch 3  | loss: 1.06469 | eval_custom_logloss: 1.11321 |  0:01:03s
epoch 4  | loss: 1.0337  | eval_custom_logloss: 1.14852 |  0:01:19s
epoch 5  | loss: 0.99867 | eval_custom_logloss: 0.95012 |  0:01:35s
epoch 6  | loss: 1.01135 | eval_custom_logloss: 1.03649 |  0:01:51s
epoch 7  | loss: 0.96365 | eval_custom_logloss: 1.13124 |  0:02:06s
epoch 8  | loss: 0.96071 | eval_custom_logloss: 1.04519 |  0:02:22s
epoch 9  | loss: 0.94065 | eval_custom_logloss: 1.38301 |  0:02:37s
epoch 10 | loss: 0.93441 | eval_custom_logloss: 1.08843 |  0:02:53s
epoch 11 | loss: 0.91787 | eval_custom_logloss: 0.89978 |  0:03:09s
epoch 12 | loss: 0.90173 | eval_custom_logloss: 0.84829 |  0:03:25s
epoch 13 | loss: 0.89769 | eval_custom_logloss: 0.83486 |  0:03:41s
epoch 14 | loss: 0.89523 | eval_custom_logloss: 0.84777 |  0:03:57s
epoch 15 | loss: 0.88071 | eval_custom_logloss: 0.79546 |  0:04:13s
epoch 16 | loss: 0.86686 | eval_custom_logloss: 0.80702 |  0:04:28s
epoch 17 | loss: 0.84721 | eval_custom_logloss: 0.84001 |  0:04:44s
epoch 18 | loss: 0.85291 | eval_custom_logloss: 0.82307 |  0:04:59s
epoch 19 | loss: 0.84389 | eval_custom_logloss: 1.34452 |  0:05:15s
epoch 20 | loss: 0.85268 | eval_custom_logloss: 1.05777 |  0:05:31s
epoch 21 | loss: 0.81366 | eval_custom_logloss: 0.76407 |  0:05:46s
epoch 22 | loss: 0.83694 | eval_custom_logloss: 0.79174 |  0:06:02s
epoch 23 | loss: 0.82915 | eval_custom_logloss: 0.843   |  0:06:17s
epoch 24 | loss: 0.82348 | eval_custom_logloss: 0.76803 |  0:06:33s
epoch 25 | loss: 0.81048 | eval_custom_logloss: 0.83375 |  0:06:49s
epoch 26 | loss: 0.79761 | eval_custom_logloss: 0.80422 |  0:07:04s
epoch 27 | loss: 0.79547 | eval_custom_logloss: 0.73742 |  0:07:20s
epoch 28 | loss: 0.78682 | eval_custom_logloss: 0.74483 |  0:07:35s
epoch 29 | loss: 0.79333 | eval_custom_logloss: 0.87744 |  0:07:51s
epoch 30 | loss: 0.78455 | eval_custom_logloss: 0.84077 |  0:08:07s
epoch 31 | loss: 0.75133 | eval_custom_logloss: 0.723   |  0:08:22s
epoch 32 | loss: 0.73845 | eval_custom_logloss: 0.85661 |  0:08:38s
epoch 33 | loss: 0.71895 | eval_custom_logloss: 0.77648 |  0:08:54s
epoch 34 | loss: 0.72721 | eval_custom_logloss: 0.85082 |  0:09:10s
epoch 35 | loss: 0.70502 | eval_custom_logloss: 0.77031 |  0:09:25s
epoch 36 | loss: 0.7322  | eval_custom_logloss: 0.7229  |  0:09:41s
epoch 37 | loss: 0.71167 | eval_custom_logloss: 0.96105 |  0:09:56s
epoch 38 | loss: 0.70521 | eval_custom_logloss: 0.70081 |  0:10:12s
epoch 39 | loss: 0.71568 | eval_custom_logloss: 0.7035  |  0:10:27s
epoch 40 | loss: 0.70969 | eval_custom_logloss: 0.92577 |  0:10:43s
epoch 41 | loss: 0.69909 | eval_custom_logloss: 1.21422 |  0:10:58s
epoch 42 | loss: 0.69857 | eval_custom_logloss: 0.83481 |  0:11:14s
epoch 43 | loss: 0.68268 | eval_custom_logloss: 0.68628 |  0:11:29s
epoch 44 | loss: 0.69314 | eval_custom_logloss: 0.70208 |  0:11:45s
epoch 45 | loss: 0.68766 | eval_custom_logloss: 0.77583 |  0:12:00s
epoch 46 | loss: 0.6796  | eval_custom_logloss: 0.95266 |  0:12:16s
epoch 47 | loss: 0.68571 | eval_custom_logloss: 0.87124 |  0:12:32s
epoch 48 | loss: 0.69354 | eval_custom_logloss: 0.71324 |  0:12:47s
epoch 49 | loss: 0.67365 | eval_custom_logloss: 0.74094 |  0:13:03s
epoch 50 | loss: 0.69019 | eval_custom_logloss: 0.78476 |  0:13:18s
epoch 51 | loss: 0.69473 | eval_custom_logloss: 0.70053 |  0:13:34s
epoch 52 | loss: 0.67897 | eval_custom_logloss: 0.87671 |  0:13:50s
epoch 53 | loss: 0.69874 | eval_custom_logloss: 1.60708 |  0:14:05s
epoch 54 | loss: 0.67664 | eval_custom_logloss: 0.74465 |  0:14:21s
epoch 55 | loss: 0.66897 | eval_custom_logloss: 0.73602 |  0:14:36s
epoch 56 | loss: 0.6789  | eval_custom_logloss: 1.11524 |  0:14:52s
epoch 57 | loss: 0.67238 | eval_custom_logloss: 0.65977 |  0:15:08s
epoch 58 | loss: 0.66594 | eval_custom_logloss: 0.8649  |  0:15:23s
epoch 59 | loss: 0.67388 | eval_custom_logloss: 1.02621 |  0:15:39s
epoch 60 | loss: 0.65881 | eval_custom_logloss: 1.5392  |  0:15:54s
epoch 61 | loss: 0.67576 | eval_custom_logloss: 0.67978 |  0:16:10s
epoch 62 | loss: 0.65825 | eval_custom_logloss: 0.70945 |  0:16:25s
epoch 63 | loss: 0.65251 | eval_custom_logloss: 0.89252 |  0:16:41s
epoch 64 | loss: 0.65443 | eval_custom_logloss: 0.66676 |  0:16:56s
epoch 65 | loss: 0.64699 | eval_custom_logloss: 0.95493 |  0:17:12s
epoch 66 | loss: 0.65628 | eval_custom_logloss: 0.70852 |  0:17:27s
epoch 67 | loss: 0.65597 | eval_custom_logloss: 0.75256 |  0:17:43s
epoch 68 | loss: 0.64838 | eval_custom_logloss: 0.77992 |  0:17:58s
epoch 69 | loss: 0.6447  | eval_custom_logloss: 0.84871 |  0:18:13s
epoch 70 | loss: 0.64171 | eval_custom_logloss: 0.66481 |  0:18:29s
epoch 71 | loss: 0.66732 | eval_custom_logloss: 0.75375 |  0:18:44s
epoch 72 | loss: 0.65333 | eval_custom_logloss: 1.36351 |  0:19:00s
epoch 73 | loss: 0.63844 | eval_custom_logloss: 0.78104 |  0:19:15s
epoch 74 | loss: 0.63262 | eval_custom_logloss: 0.70867 |  0:19:31s
epoch 75 | loss: 0.64542 | eval_custom_logloss: 0.67591 |  0:19:46s
epoch 76 | loss: 0.64813 | eval_custom_logloss: 0.82794 |  0:20:01s
epoch 77 | loss: 0.64856 | eval_custom_logloss: 1.21214 |  0:20:17s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.65977
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6678999999999999, 'Log Loss - std': 0.00930000000000003} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.62385 | eval_custom_logloss: 3.03573 |  0:00:15s
epoch 1  | loss: 1.25902 | eval_custom_logloss: 2.49223 |  0:00:30s
epoch 2  | loss: 1.12569 | eval_custom_logloss: 1.91662 |  0:00:46s
epoch 3  | loss: 1.05374 | eval_custom_logloss: 1.00293 |  0:01:01s
epoch 4  | loss: 1.0275  | eval_custom_logloss: 1.25939 |  0:01:17s
epoch 5  | loss: 0.98596 | eval_custom_logloss: 0.98511 |  0:01:32s
epoch 6  | loss: 0.99931 | eval_custom_logloss: 1.19603 |  0:01:48s
epoch 7  | loss: 0.98035 | eval_custom_logloss: 0.90535 |  0:02:03s
epoch 8  | loss: 0.97566 | eval_custom_logloss: 0.88196 |  0:02:18s
epoch 9  | loss: 0.96656 | eval_custom_logloss: 0.95666 |  0:02:34s
epoch 10 | loss: 0.9601  | eval_custom_logloss: 0.86787 |  0:02:50s
epoch 11 | loss: 0.93838 | eval_custom_logloss: 0.85494 |  0:03:05s
epoch 12 | loss: 0.91891 | eval_custom_logloss: 0.87936 |  0:03:21s
epoch 13 | loss: 0.92298 | eval_custom_logloss: 0.93866 |  0:03:36s
epoch 14 | loss: 0.91982 | eval_custom_logloss: 0.97513 |  0:03:51s
epoch 15 | loss: 0.93247 | eval_custom_logloss: 0.94782 |  0:04:07s
epoch 16 | loss: 0.89665 | eval_custom_logloss: 0.87084 |  0:04:22s
epoch 17 | loss: 0.89217 | eval_custom_logloss: 0.90466 |  0:04:38s
epoch 18 | loss: 0.89496 | eval_custom_logloss: 0.8383  |  0:04:53s
epoch 19 | loss: 0.89471 | eval_custom_logloss: 0.7964  |  0:05:10s
epoch 20 | loss: 0.87814 | eval_custom_logloss: 0.7923  |  0:05:27s
epoch 21 | loss: 0.87259 | eval_custom_logloss: 0.86077 |  0:05:44s
epoch 22 | loss: 0.86964 | eval_custom_logloss: 0.83865 |  0:06:02s
epoch 23 | loss: 0.87694 | eval_custom_logloss: 0.78552 |  0:06:19s
epoch 24 | loss: 0.88179 | eval_custom_logloss: 0.79392 |  0:06:36s
epoch 25 | loss: 0.85343 | eval_custom_logloss: 0.82436 |  0:06:54s
epoch 26 | loss: 0.85605 | eval_custom_logloss: 0.98906 |  0:07:11s
epoch 27 | loss: 0.85174 | eval_custom_logloss: 0.8327  |  0:07:27s
epoch 28 | loss: 0.83144 | eval_custom_logloss: 0.7957  |  0:07:43s
epoch 29 | loss: 0.83185 | eval_custom_logloss: 0.7636  |  0:07:58s
epoch 30 | loss: 0.85782 | eval_custom_logloss: 0.77726 |  0:08:14s
epoch 31 | loss: 0.82864 | eval_custom_logloss: 0.78456 |  0:08:29s
epoch 32 | loss: 0.82121 | eval_custom_logloss: 0.81433 |  0:08:45s
epoch 33 | loss: 0.81965 | eval_custom_logloss: 0.84571 |  0:09:01s
epoch 34 | loss: 0.82736 | eval_custom_logloss: 0.75895 |  0:09:16s
epoch 35 | loss: 0.81671 | eval_custom_logloss: 0.99746 |  0:09:32s
epoch 36 | loss: 0.83001 | eval_custom_logloss: 0.78942 |  0:09:48s
epoch 37 | loss: 0.81957 | eval_custom_logloss: 0.81936 |  0:10:03s
epoch 38 | loss: 0.81839 | eval_custom_logloss: 0.97887 |  0:10:19s
epoch 39 | loss: 0.82    | eval_custom_logloss: 0.79535 |  0:10:34s
epoch 40 | loss: 0.81391 | eval_custom_logloss: 0.79562 |  0:10:50s
epoch 41 | loss: 0.8053  | eval_custom_logloss: 0.85046 |  0:11:06s
epoch 42 | loss: 0.81479 | eval_custom_logloss: 0.77969 |  0:11:21s
epoch 43 | loss: 0.80477 | eval_custom_logloss: 0.8675  |  0:11:36s
epoch 44 | loss: 0.80773 | eval_custom_logloss: 1.53753 |  0:11:52s
epoch 45 | loss: 0.79948 | eval_custom_logloss: 0.75551 |  0:12:07s
epoch 46 | loss: 0.80939 | eval_custom_logloss: 0.83947 |  0:12:23s
epoch 47 | loss: 0.80641 | eval_custom_logloss: 0.75467 |  0:12:38s
epoch 48 | loss: 0.78423 | eval_custom_logloss: 0.80059 |  0:12:54s
epoch 49 | loss: 0.78879 | eval_custom_logloss: 0.77879 |  0:13:10s
epoch 50 | loss: 0.79475 | eval_custom_logloss: 0.77337 |  0:13:25s
epoch 51 | loss: 0.78107 | eval_custom_logloss: 0.76924 |  0:13:41s
epoch 52 | loss: 0.79004 | eval_custom_logloss: 0.76058 |  0:13:56s
epoch 53 | loss: 0.78916 | eval_custom_logloss: 0.76128 |  0:14:11s
epoch 54 | loss: 0.78475 | eval_custom_logloss: 0.82448 |  0:14:27s
epoch 55 | loss: 0.78187 | eval_custom_logloss: 0.80362 |  0:14:42s
epoch 56 | loss: 0.78153 | eval_custom_logloss: 0.89075 |  0:14:58s
epoch 57 | loss: 0.78907 | eval_custom_logloss: 0.77782 |  0:15:13s
epoch 58 | loss: 0.77207 | eval_custom_logloss: 0.83447 |  0:15:29s
epoch 59 | loss: 0.77351 | eval_custom_logloss: 0.71481 |  0:15:44s
epoch 60 | loss: 0.76524 | eval_custom_logloss: 0.74106 |  0:16:00s
epoch 61 | loss: 0.77799 | eval_custom_logloss: 0.76892 |  0:16:15s
epoch 62 | loss: 0.7594  | eval_custom_logloss: 0.8323  |  0:16:31s
epoch 63 | loss: 0.77845 | eval_custom_logloss: 0.78439 |  0:16:46s
epoch 64 | loss: 0.75995 | eval_custom_logloss: 0.82749 |  0:17:02s
epoch 65 | loss: 0.7749  | eval_custom_logloss: 0.73481 |  0:17:17s
epoch 66 | loss: 0.76385 | eval_custom_logloss: 1.01165 |  0:17:33s
epoch 67 | loss: 0.7678  | eval_custom_logloss: 0.79014 |  0:17:48s
epoch 68 | loss: 0.75532 | eval_custom_logloss: 0.74458 |  0:18:03s
epoch 69 | loss: 0.76272 | eval_custom_logloss: 0.81398 |  0:18:19s
epoch 70 | loss: 0.76003 | eval_custom_logloss: 0.70946 |  0:18:34s
epoch 71 | loss: 0.75853 | eval_custom_logloss: 0.72802 |  0:18:50s
epoch 72 | loss: 0.7692  | eval_custom_logloss: 0.75868 |  0:19:05s
epoch 73 | loss: 0.78859 | eval_custom_logloss: 0.7462  |  0:19:20s
epoch 74 | loss: 0.78123 | eval_custom_logloss: 0.89601 |  0:19:36s
epoch 75 | loss: 0.76617 | eval_custom_logloss: 0.78045 |  0:19:51s
epoch 76 | loss: 0.75681 | eval_custom_logloss: 0.78875 |  0:20:07s
epoch 77 | loss: 0.76246 | eval_custom_logloss: 0.81393 |  0:20:22s
epoch 78 | loss: 0.75122 | eval_custom_logloss: 0.76113 |  0:20:38s
epoch 79 | loss: 0.75591 | eval_custom_logloss: 0.89315 |  0:20:53s
epoch 80 | loss: 0.74793 | eval_custom_logloss: 0.7653  |  0:21:09s
epoch 81 | loss: 0.75791 | eval_custom_logloss: 0.73536 |  0:21:24s
epoch 82 | loss: 0.76455 | eval_custom_logloss: 0.77905 |  0:21:40s
epoch 83 | loss: 0.75168 | eval_custom_logloss: 0.77636 |  0:21:55s
epoch 84 | loss: 0.75099 | eval_custom_logloss: 0.78291 |  0:22:11s
epoch 85 | loss: 0.74461 | eval_custom_logloss: 1.1013  |  0:22:26s
epoch 86 | loss: 0.7542  | eval_custom_logloss: 0.80642 |  0:22:42s
epoch 87 | loss: 0.74249 | eval_custom_logloss: 0.89155 |  0:22:57s
epoch 88 | loss: 0.74103 | eval_custom_logloss: 0.80704 |  0:23:13s
epoch 89 | loss: 0.7464  | eval_custom_logloss: 0.80605 |  0:23:28s
epoch 90 | loss: 0.74218 | eval_custom_logloss: 0.76304 |  0:23:44s

Early stopping occurred at epoch 90 with best_epoch = 70 and best_eval_custom_logloss = 0.70946
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6813333333333333, 'Log Loss - std': 0.02045895620233081} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.54763 | eval_custom_logloss: 3.00889 |  0:00:15s
epoch 1  | loss: 1.17413 | eval_custom_logloss: 1.52304 |  0:00:31s
epoch 2  | loss: 1.06343 | eval_custom_logloss: 1.69064 |  0:00:46s
epoch 3  | loss: 1.01219 | eval_custom_logloss: 1.05238 |  0:01:02s
epoch 4  | loss: 0.96465 | eval_custom_logloss: 1.22084 |  0:01:17s
epoch 5  | loss: 0.92089 | eval_custom_logloss: 1.01615 |  0:01:32s
epoch 6  | loss: 0.92769 | eval_custom_logloss: 1.07286 |  0:01:48s
epoch 7  | loss: 0.90408 | eval_custom_logloss: 0.89226 |  0:02:03s
epoch 8  | loss: 0.92581 | eval_custom_logloss: 0.79056 |  0:02:19s
epoch 9  | loss: 0.9031  | eval_custom_logloss: 1.56447 |  0:02:34s
epoch 10 | loss: 0.92543 | eval_custom_logloss: 1.02419 |  0:02:50s
epoch 11 | loss: 0.89909 | eval_custom_logloss: 1.03441 |  0:03:05s
epoch 12 | loss: 0.88624 | eval_custom_logloss: 1.1706  |  0:03:21s
epoch 13 | loss: 0.86061 | eval_custom_logloss: 0.91079 |  0:03:36s
epoch 14 | loss: 0.86475 | eval_custom_logloss: 0.79897 |  0:03:52s
epoch 15 | loss: 0.8635  | eval_custom_logloss: 1.03036 |  0:04:07s
epoch 16 | loss: 0.83429 | eval_custom_logloss: 0.76938 |  0:04:23s
epoch 17 | loss: 0.82596 | eval_custom_logloss: 0.75789 |  0:04:38s
epoch 18 | loss: 0.83816 | eval_custom_logloss: 1.02747 |  0:04:53s
epoch 19 | loss: 0.83564 | eval_custom_logloss: 0.8808  |  0:05:09s
epoch 20 | loss: 0.82253 | eval_custom_logloss: 1.18675 |  0:05:24s
epoch 21 | loss: 0.7977  | eval_custom_logloss: 0.81216 |  0:05:40s
epoch 22 | loss: 0.80388 | eval_custom_logloss: 0.83648 |  0:05:55s
epoch 23 | loss: 0.81904 | eval_custom_logloss: 0.70774 |  0:06:11s
epoch 24 | loss: 0.79706 | eval_custom_logloss: 1.09121 |  0:06:27s
epoch 25 | loss: 0.7868  | eval_custom_logloss: 0.95016 |  0:06:42s
epoch 26 | loss: 0.77854 | eval_custom_logloss: 1.15677 |  0:06:58s
epoch 27 | loss: 0.77102 | eval_custom_logloss: 0.84202 |  0:07:13s
epoch 28 | loss: 0.77901 | eval_custom_logloss: 1.05963 |  0:07:28s
epoch 29 | loss: 0.79046 | eval_custom_logloss: 1.00947 |  0:07:44s
epoch 30 | loss: 0.78538 | eval_custom_logloss: 0.91516 |  0:07:59s
epoch 31 | loss: 0.7526  | eval_custom_logloss: 1.01685 |  0:08:15s
epoch 32 | loss: 0.76019 | eval_custom_logloss: 0.92387 |  0:08:31s
epoch 33 | loss: 0.74606 | eval_custom_logloss: 0.75894 |  0:08:46s
epoch 34 | loss: 0.761   | eval_custom_logloss: 0.90082 |  0:09:02s
epoch 35 | loss: 0.75901 | eval_custom_logloss: 0.82761 |  0:09:17s
epoch 36 | loss: 0.75723 | eval_custom_logloss: 0.71348 |  0:09:33s
epoch 37 | loss: 0.72479 | eval_custom_logloss: 1.01661 |  0:09:48s
epoch 38 | loss: 0.73354 | eval_custom_logloss: 0.82408 |  0:10:04s
epoch 39 | loss: 0.7378  | eval_custom_logloss: 0.92386 |  0:10:19s
epoch 40 | loss: 0.74506 | eval_custom_logloss: 0.90673 |  0:10:35s
epoch 41 | loss: 0.73454 | eval_custom_logloss: 0.87535 |  0:10:50s
epoch 42 | loss: 0.72386 | eval_custom_logloss: 0.80654 |  0:11:06s
epoch 43 | loss: 0.7286  | eval_custom_logloss: 0.82568 |  0:11:21s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.70774
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.68765, 'Log Loss - std': 0.020823724450731693} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56901 | eval_custom_logloss: 2.28193 |  0:00:15s
epoch 1  | loss: 1.23451 | eval_custom_logloss: 1.97028 |  0:00:31s
epoch 2  | loss: 1.16061 | eval_custom_logloss: 1.25146 |  0:00:46s
epoch 3  | loss: 1.08299 | eval_custom_logloss: 1.10386 |  0:01:01s
epoch 4  | loss: 1.06169 | eval_custom_logloss: 1.13568 |  0:01:17s
epoch 5  | loss: 0.99621 | eval_custom_logloss: 1.09057 |  0:01:32s
epoch 6  | loss: 0.99152 | eval_custom_logloss: 1.15731 |  0:01:48s
epoch 7  | loss: 0.97188 | eval_custom_logloss: 0.8462  |  0:02:03s
epoch 8  | loss: 0.94993 | eval_custom_logloss: 0.89375 |  0:02:19s
epoch 9  | loss: 0.91739 | eval_custom_logloss: 0.88787 |  0:02:34s
epoch 10 | loss: 0.91419 | eval_custom_logloss: 0.79077 |  0:02:50s
epoch 11 | loss: 0.90075 | eval_custom_logloss: 1.10018 |  0:03:05s
epoch 12 | loss: 0.89506 | eval_custom_logloss: 0.85863 |  0:03:21s
epoch 13 | loss: 0.86779 | eval_custom_logloss: 1.37864 |  0:03:36s
epoch 14 | loss: 0.85967 | eval_custom_logloss: 0.78062 |  0:03:52s
epoch 15 | loss: 0.84749 | eval_custom_logloss: 0.90525 |  0:04:07s
epoch 16 | loss: 0.83823 | eval_custom_logloss: 0.8362  |  0:04:23s
epoch 17 | loss: 0.85432 | eval_custom_logloss: 0.8997  |  0:04:38s
epoch 18 | loss: 0.85024 | eval_custom_logloss: 0.84875 |  0:04:53s
epoch 19 | loss: 0.83356 | eval_custom_logloss: 0.99547 |  0:05:09s
epoch 20 | loss: 0.83102 | eval_custom_logloss: 0.80925 |  0:05:24s
epoch 21 | loss: 0.83253 | eval_custom_logloss: 0.82513 |  0:05:40s
epoch 22 | loss: 0.8007  | eval_custom_logloss: 1.03828 |  0:05:55s
epoch 23 | loss: 0.80184 | eval_custom_logloss: 0.88471 |  0:06:11s
epoch 24 | loss: 0.80258 | eval_custom_logloss: 1.11545 |  0:06:26s
epoch 25 | loss: 0.78908 | eval_custom_logloss: 1.60995 |  0:06:42s
epoch 26 | loss: 0.79191 | eval_custom_logloss: 2.12905 |  0:06:57s
epoch 27 | loss: 0.80239 | eval_custom_logloss: 1.03547 |  0:07:12s
epoch 28 | loss: 0.77237 | eval_custom_logloss: 0.76496 |  0:07:28s
epoch 29 | loss: 0.77294 | eval_custom_logloss: 1.62401 |  0:07:43s
epoch 30 | loss: 0.78419 | eval_custom_logloss: 1.74798 |  0:07:59s
epoch 31 | loss: 0.75711 | eval_custom_logloss: 1.10694 |  0:08:14s
epoch 32 | loss: 0.76391 | eval_custom_logloss: 0.7665  |  0:08:30s
epoch 33 | loss: 0.76411 | eval_custom_logloss: 0.99807 |  0:08:45s
epoch 34 | loss: 0.75561 | eval_custom_logloss: 0.91665 |  0:09:01s
epoch 35 | loss: 0.77521 | eval_custom_logloss: 0.70904 |  0:09:16s
epoch 36 | loss: 0.76857 | eval_custom_logloss: 0.83311 |  0:09:32s
epoch 37 | loss: 0.76376 | eval_custom_logloss: 0.82061 |  0:09:47s
epoch 38 | loss: 0.75748 | eval_custom_logloss: 0.91944 |  0:10:03s
epoch 39 | loss: 0.76257 | eval_custom_logloss: 1.08534 |  0:10:18s
epoch 40 | loss: 0.75009 | eval_custom_logloss: 0.95432 |  0:10:33s
epoch 41 | loss: 0.75417 | eval_custom_logloss: 0.73498 |  0:10:49s
epoch 42 | loss: 0.75746 | eval_custom_logloss: 0.69472 |  0:11:04s
epoch 43 | loss: 0.7499  | eval_custom_logloss: 0.70483 |  0:11:20s
epoch 44 | loss: 0.74536 | eval_custom_logloss: 0.93976 |  0:11:35s
epoch 45 | loss: 0.74181 | eval_custom_logloss: 0.74936 |  0:11:51s
epoch 46 | loss: 0.75794 | eval_custom_logloss: 0.68143 |  0:12:06s
epoch 47 | loss: 0.74368 | eval_custom_logloss: 1.02771 |  0:12:22s
epoch 48 | loss: 0.74946 | eval_custom_logloss: 1.2345  |  0:12:37s
epoch 49 | loss: 0.74046 | eval_custom_logloss: 0.713   |  0:12:53s
epoch 50 | loss: 0.73218 | eval_custom_logloss: 0.70945 |  0:13:08s
epoch 51 | loss: 0.73264 | eval_custom_logloss: 0.74409 |  0:13:23s
epoch 52 | loss: 0.74462 | eval_custom_logloss: 0.78524 |  0:13:39s
epoch 53 | loss: 0.75617 | eval_custom_logloss: 0.79216 |  0:13:54s
epoch 54 | loss: 0.79078 | eval_custom_logloss: 0.80636 |  0:14:09s
epoch 55 | loss: 0.77402 | eval_custom_logloss: 0.83304 |  0:14:25s
epoch 56 | loss: 0.75786 | eval_custom_logloss: 0.96393 |  0:14:40s
epoch 57 | loss: 0.73352 | eval_custom_logloss: 0.9617  |  0:14:55s
epoch 58 | loss: 0.71469 | eval_custom_logloss: 0.71738 |  0:15:11s
epoch 59 | loss: 0.71367 | eval_custom_logloss: 1.00716 |  0:15:26s
epoch 60 | loss: 0.72554 | eval_custom_logloss: 0.69339 |  0:15:41s
epoch 61 | loss: 0.71884 | eval_custom_logloss: 0.76823 |  0:15:57s
epoch 62 | loss: 0.70925 | eval_custom_logloss: 0.70035 |  0:16:12s
epoch 63 | loss: 0.7232  | eval_custom_logloss: 0.70092 |  0:16:28s
epoch 64 | loss: 0.71064 | eval_custom_logloss: 0.75612 |  0:16:43s
epoch 65 | loss: 0.71707 | eval_custom_logloss: 0.77472 |  0:16:58s
epoch 66 | loss: 0.71421 | eval_custom_logloss: 0.76322 |  0:17:14s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 0.68143
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6860799999999999, 'Log Loss - std': 0.018888133841118367} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 0.6860799999999999 and parameters: {'n_d': 41, 'n_steps': 7, 'gamma': 1.27105823046316, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.005505971559725207, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 0.6860799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax', 'n_a': 32, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.6918  | eval_custom_logloss: 4.47569 |  0:00:19s
epoch 1  | loss: 1.23739 | eval_custom_logloss: 2.54856 |  0:00:38s
epoch 2  | loss: 1.08361 | eval_custom_logloss: 1.84557 |  0:00:57s
epoch 3  | loss: 1.02512 | eval_custom_logloss: 1.4332  |  0:01:17s
epoch 4  | loss: 0.99535 | eval_custom_logloss: 1.15047 |  0:01:36s
epoch 5  | loss: 0.95588 | eval_custom_logloss: 1.53018 |  0:01:55s
epoch 6  | loss: 0.9601  | eval_custom_logloss: 0.94705 |  0:02:14s
epoch 7  | loss: 0.95411 | eval_custom_logloss: 0.8734  |  0:02:34s
epoch 8  | loss: 0.93072 | eval_custom_logloss: 0.94839 |  0:02:53s
epoch 9  | loss: 0.93781 | eval_custom_logloss: 2.12576 |  0:03:13s
epoch 10 | loss: 0.9344  | eval_custom_logloss: 0.87787 |  0:03:32s
epoch 11 | loss: 0.88953 | eval_custom_logloss: 1.34224 |  0:03:51s
epoch 12 | loss: 0.88354 | eval_custom_logloss: 0.84751 |  0:04:10s
epoch 13 | loss: 0.88031 | eval_custom_logloss: 0.99508 |  0:04:30s
epoch 14 | loss: 0.90278 | eval_custom_logloss: 1.17766 |  0:04:49s
epoch 15 | loss: 0.86523 | eval_custom_logloss: 0.72924 |  0:05:08s
epoch 16 | loss: 0.84564 | eval_custom_logloss: 0.93233 |  0:05:27s
epoch 17 | loss: 0.83657 | eval_custom_logloss: 0.89406 |  0:05:47s
epoch 18 | loss: 0.84714 | eval_custom_logloss: 0.90412 |  0:06:06s
epoch 19 | loss: 0.82228 | eval_custom_logloss: 0.72498 |  0:06:25s
epoch 20 | loss: 0.80201 | eval_custom_logloss: 0.79506 |  0:06:45s
epoch 21 | loss: 0.80261 | eval_custom_logloss: 0.91234 |  0:07:04s
epoch 22 | loss: 0.76587 | eval_custom_logloss: 0.9288  |  0:07:23s
epoch 23 | loss: 0.77774 | eval_custom_logloss: 0.86709 |  0:07:43s
epoch 24 | loss: 0.76137 | eval_custom_logloss: 0.78306 |  0:08:02s
epoch 25 | loss: 0.7659  | eval_custom_logloss: 0.79202 |  0:08:21s
epoch 26 | loss: 0.761   | eval_custom_logloss: 0.74413 |  0:08:41s
epoch 27 | loss: 0.76991 | eval_custom_logloss: 0.9239  |  0:09:00s
epoch 28 | loss: 0.75038 | eval_custom_logloss: 0.85162 |  0:09:19s
epoch 29 | loss: 0.7452  | eval_custom_logloss: 0.68167 |  0:09:39s
epoch 30 | loss: 0.76033 | eval_custom_logloss: 0.80721 |  0:09:58s
epoch 31 | loss: 0.74629 | eval_custom_logloss: 0.88437 |  0:10:18s
epoch 32 | loss: 0.73559 | eval_custom_logloss: 0.77492 |  0:10:37s
epoch 33 | loss: 0.71305 | eval_custom_logloss: 1.09582 |  0:10:56s
epoch 34 | loss: 0.73242 | eval_custom_logloss: 0.83863 |  0:11:15s
epoch 35 | loss: 0.73326 | eval_custom_logloss: 0.64313 |  0:11:35s
epoch 36 | loss: 0.71426 | eval_custom_logloss: 0.68725 |  0:11:54s
epoch 37 | loss: 0.72332 | eval_custom_logloss: 0.75816 |  0:12:13s
epoch 38 | loss: 0.72529 | eval_custom_logloss: 0.77953 |  0:12:33s
epoch 39 | loss: 0.71283 | eval_custom_logloss: 0.71828 |  0:12:52s
epoch 40 | loss: 0.70345 | eval_custom_logloss: 1.06041 |  0:13:11s
epoch 41 | loss: 0.70355 | eval_custom_logloss: 0.65949 |  0:13:30s
epoch 42 | loss: 0.69042 | eval_custom_logloss: 0.64362 |  0:13:49s
epoch 43 | loss: 0.70615 | eval_custom_logloss: 0.87223 |  0:14:09s
epoch 44 | loss: 0.70138 | eval_custom_logloss: 0.66993 |  0:14:28s
epoch 45 | loss: 0.70845 | eval_custom_logloss: 0.62715 |  0:14:47s
epoch 46 | loss: 0.69241 | eval_custom_logloss: 0.95382 |  0:15:07s
epoch 47 | loss: 0.69215 | eval_custom_logloss: 0.6508  |  0:15:26s
epoch 48 | loss: 0.69756 | eval_custom_logloss: 1.09029 |  0:15:45s
epoch 49 | loss: 0.68077 | eval_custom_logloss: 0.82599 |  0:16:05s
epoch 50 | loss: 0.6848  | eval_custom_logloss: 0.66011 |  0:16:24s
epoch 51 | loss: 0.67064 | eval_custom_logloss: 0.63011 |  0:16:44s
epoch 52 | loss: 0.67862 | eval_custom_logloss: 0.66709 |  0:17:03s
epoch 53 | loss: 0.67755 | eval_custom_logloss: 0.78282 |  0:17:22s
epoch 54 | loss: 0.68232 | eval_custom_logloss: 0.96161 |  0:17:42s
epoch 55 | loss: 0.67675 | eval_custom_logloss: 0.75792 |  0:18:01s
epoch 56 | loss: 0.67668 | eval_custom_logloss: 0.62483 |  0:18:21s
epoch 57 | loss: 0.67339 | eval_custom_logloss: 0.88232 |  0:18:40s
epoch 58 | loss: 0.68559 | eval_custom_logloss: 0.66833 |  0:18:59s
epoch 59 | loss: 0.68257 | eval_custom_logloss: 0.78578 |  0:19:19s
epoch 60 | loss: 0.67333 | eval_custom_logloss: 0.6104  |  0:19:38s
epoch 61 | loss: 0.65779 | eval_custom_logloss: 0.71727 |  0:19:57s
epoch 62 | loss: 0.66188 | eval_custom_logloss: 0.79644 |  0:20:17s
epoch 63 | loss: 0.66606 | eval_custom_logloss: 0.75262 |  0:20:36s
epoch 64 | loss: 0.65119 | eval_custom_logloss: 0.75007 |  0:20:55s
epoch 65 | loss: 0.65799 | eval_custom_logloss: 0.66906 |  0:21:15s
epoch 66 | loss: 0.66763 | eval_custom_logloss: 0.76947 |  0:21:36s
epoch 67 | loss: 0.66803 | eval_custom_logloss: 0.87396 |  0:21:58s
epoch 68 | loss: 0.6552  | eval_custom_logloss: 0.88019 |  0:22:19s
epoch 69 | loss: 0.654   | eval_custom_logloss: 0.81351 |  0:22:41s
epoch 70 | loss: 0.65598 | eval_custom_logloss: 0.77382 |  0:23:02s
epoch 71 | loss: 0.64721 | eval_custom_logloss: 0.59876 |  0:23:24s
epoch 72 | loss: 0.65278 | eval_custom_logloss: 0.65387 |  0:23:43s
epoch 73 | loss: 0.64997 | eval_custom_logloss: 0.64354 |  0:24:03s
epoch 74 | loss: 0.64648 | eval_custom_logloss: 0.84366 |  0:24:22s
epoch 75 | loss: 0.6526  | eval_custom_logloss: 0.63824 |  0:24:41s
epoch 76 | loss: 0.66322 | eval_custom_logloss: 0.59469 |  0:25:01s
epoch 77 | loss: 0.65234 | eval_custom_logloss: 1.05003 |  0:25:20s
epoch 78 | loss: 0.64914 | eval_custom_logloss: 0.66945 |  0:25:39s
epoch 79 | loss: 0.63925 | eval_custom_logloss: 0.65525 |  0:25:59s
epoch 80 | loss: 0.6458  | eval_custom_logloss: 0.65249 |  0:26:18s
epoch 81 | loss: 0.65143 | eval_custom_logloss: 0.78529 |  0:26:37s
epoch 82 | loss: 0.63462 | eval_custom_logloss: 0.80427 |  0:26:57s
epoch 83 | loss: 0.63643 | eval_custom_logloss: 0.85299 |  0:27:16s
epoch 84 | loss: 0.63733 | eval_custom_logloss: 0.60413 |  0:27:35s
epoch 85 | loss: 0.6329  | eval_custom_logloss: 0.72406 |  0:27:55s
epoch 86 | loss: 0.64172 | eval_custom_logloss: 0.94979 |  0:28:14s
epoch 87 | loss: 0.62921 | eval_custom_logloss: 0.71686 |  0:28:34s
epoch 88 | loss: 0.6398  | eval_custom_logloss: 1.16266 |  0:28:53s
epoch 89 | loss: 0.63374 | eval_custom_logloss: 0.85938 |  0:29:12s
epoch 90 | loss: 0.64297 | eval_custom_logloss: 1.08593 |  0:29:31s
epoch 91 | loss: 0.63551 | eval_custom_logloss: 0.76543 |  0:29:51s
epoch 92 | loss: 0.64811 | eval_custom_logloss: 1.13247 |  0:30:10s
epoch 93 | loss: 0.62807 | eval_custom_logloss: 0.73039 |  0:30:29s
epoch 94 | loss: 0.63268 | eval_custom_logloss: 0.60672 |  0:30:49s
epoch 95 | loss: 0.62894 | eval_custom_logloss: 0.91349 |  0:31:08s
epoch 96 | loss: 0.63875 | eval_custom_logloss: 0.6078  |  0:31:27s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 0.59469
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5923, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax', 'n_a': 32, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68812 | eval_custom_logloss: 5.42473 |  0:00:19s
epoch 1  | loss: 1.15689 | eval_custom_logloss: 2.56203 |  0:00:38s
epoch 2  | loss: 1.03216 | eval_custom_logloss: 1.51076 |  0:00:57s
epoch 3  | loss: 1.01015 | eval_custom_logloss: 2.33726 |  0:01:17s
epoch 4  | loss: 0.99268 | eval_custom_logloss: 1.09347 |  0:01:36s
epoch 5  | loss: 0.95296 | eval_custom_logloss: 2.14968 |  0:01:56s
epoch 6  | loss: 0.93623 | eval_custom_logloss: 1.03078 |  0:02:15s
epoch 7  | loss: 0.92814 | eval_custom_logloss: 0.81328 |  0:02:34s
epoch 8  | loss: 0.91964 | eval_custom_logloss: 0.80895 |  0:02:53s
epoch 9  | loss: 0.89826 | eval_custom_logloss: 1.27853 |  0:03:13s
epoch 10 | loss: 0.89392 | eval_custom_logloss: 1.05579 |  0:03:32s
epoch 11 | loss: 0.89683 | eval_custom_logloss: 0.95266 |  0:03:51s
epoch 12 | loss: 0.89259 | eval_custom_logloss: 0.80856 |  0:04:11s
epoch 13 | loss: 0.86377 | eval_custom_logloss: 1.31896 |  0:04:30s
epoch 14 | loss: 0.89209 | eval_custom_logloss: 1.15864 |  0:04:49s
epoch 15 | loss: 0.86999 | eval_custom_logloss: 0.92178 |  0:05:08s
epoch 16 | loss: 0.85598 | eval_custom_logloss: 1.17252 |  0:05:28s
epoch 17 | loss: 0.84107 | eval_custom_logloss: 1.25783 |  0:05:47s
epoch 18 | loss: 0.85812 | eval_custom_logloss: 0.81401 |  0:06:06s
epoch 19 | loss: 0.82948 | eval_custom_logloss: 0.75524 |  0:06:25s
epoch 20 | loss: 0.81617 | eval_custom_logloss: 0.96126 |  0:06:45s
epoch 21 | loss: 0.82269 | eval_custom_logloss: 0.93569 |  0:07:04s
epoch 22 | loss: 0.81377 | eval_custom_logloss: 0.8425  |  0:07:23s
epoch 23 | loss: 0.81787 | eval_custom_logloss: 2.12483 |  0:07:43s
epoch 24 | loss: 0.81176 | eval_custom_logloss: 0.87893 |  0:08:02s
epoch 25 | loss: 0.81185 | eval_custom_logloss: 0.79322 |  0:08:21s
epoch 26 | loss: 0.83495 | eval_custom_logloss: 0.86108 |  0:08:41s
epoch 27 | loss: 0.81501 | eval_custom_logloss: 0.89538 |  0:09:00s
epoch 28 | loss: 0.81081 | eval_custom_logloss: 1.85188 |  0:09:19s
epoch 29 | loss: 0.81314 | eval_custom_logloss: 0.80008 |  0:09:39s
epoch 30 | loss: 0.80309 | eval_custom_logloss: 0.79834 |  0:09:58s
epoch 31 | loss: 0.79929 | eval_custom_logloss: 0.87445 |  0:10:17s
epoch 32 | loss: 0.79683 | eval_custom_logloss: 0.7776  |  0:10:37s
epoch 33 | loss: 0.79601 | eval_custom_logloss: 0.73333 |  0:10:56s
epoch 34 | loss: 0.78892 | eval_custom_logloss: 0.76881 |  0:11:15s
epoch 35 | loss: 0.78814 | eval_custom_logloss: 0.71661 |  0:11:35s
epoch 36 | loss: 0.78226 | eval_custom_logloss: 0.76252 |  0:11:54s
epoch 37 | loss: 0.79003 | eval_custom_logloss: 0.79314 |  0:12:13s
epoch 38 | loss: 0.78595 | eval_custom_logloss: 0.7713  |  0:12:32s
epoch 39 | loss: 0.77595 | eval_custom_logloss: 0.88487 |  0:12:52s
epoch 40 | loss: 0.77541 | eval_custom_logloss: 0.74962 |  0:13:11s
epoch 41 | loss: 0.78785 | eval_custom_logloss: 0.73716 |  0:13:30s
epoch 42 | loss: 0.77892 | eval_custom_logloss: 0.94597 |  0:13:50s
epoch 43 | loss: 0.77863 | eval_custom_logloss: 0.88361 |  0:14:09s
epoch 44 | loss: 0.77558 | eval_custom_logloss: 0.71693 |  0:14:28s
epoch 45 | loss: 0.77046 | eval_custom_logloss: 0.70537 |  0:14:47s
epoch 46 | loss: 0.78093 | eval_custom_logloss: 0.79526 |  0:15:07s
epoch 47 | loss: 0.78862 | eval_custom_logloss: 0.74403 |  0:15:26s
epoch 48 | loss: 0.79356 | eval_custom_logloss: 0.82094 |  0:15:45s
epoch 49 | loss: 0.77361 | eval_custom_logloss: 0.75819 |  0:16:04s
epoch 50 | loss: 0.78485 | eval_custom_logloss: 0.77174 |  0:16:24s
epoch 51 | loss: 0.77399 | eval_custom_logloss: 0.75294 |  0:16:43s
epoch 52 | loss: 0.76749 | eval_custom_logloss: 1.75425 |  0:17:02s
epoch 53 | loss: 0.76495 | eval_custom_logloss: 0.78901 |  0:17:22s
epoch 54 | loss: 0.78257 | eval_custom_logloss: 0.75859 |  0:17:41s
epoch 55 | loss: 0.79185 | eval_custom_logloss: 0.92606 |  0:18:01s
epoch 56 | loss: 0.77697 | eval_custom_logloss: 0.79192 |  0:18:20s
epoch 57 | loss: 0.76999 | eval_custom_logloss: 0.87084 |  0:18:40s
epoch 58 | loss: 0.77886 | eval_custom_logloss: 0.83676 |  0:18:59s
epoch 59 | loss: 0.76856 | eval_custom_logloss: 0.78368 |  0:19:19s
epoch 60 | loss: 0.75763 | eval_custom_logloss: 0.7895  |  0:19:38s
epoch 61 | loss: 0.76779 | eval_custom_logloss: 0.75576 |  0:19:58s
epoch 62 | loss: 0.79532 | eval_custom_logloss: 0.79422 |  0:20:17s
epoch 63 | loss: 0.79301 | eval_custom_logloss: 0.89765 |  0:20:37s
epoch 64 | loss: 0.78245 | eval_custom_logloss: 0.79007 |  0:20:56s
epoch 65 | loss: 0.78976 | eval_custom_logloss: 0.88419 |  0:21:15s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 0.70537
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.64865, 'Log Loss - std': 0.056349999999999956} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax', 'n_a': 32, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66415 | eval_custom_logloss: 3.60747 |  0:00:19s
epoch 1  | loss: 1.23014 | eval_custom_logloss: 1.3737  |  0:00:40s
epoch 2  | loss: 1.0544  | eval_custom_logloss: 1.6988  |  0:01:00s
epoch 3  | loss: 1.01721 | eval_custom_logloss: 1.06601 |  0:01:21s
epoch 4  | loss: 0.98397 | eval_custom_logloss: 1.12017 |  0:01:42s
epoch 5  | loss: 0.96021 | eval_custom_logloss: 1.07007 |  0:02:03s
epoch 6  | loss: 0.93716 | eval_custom_logloss: 0.93616 |  0:02:23s
epoch 7  | loss: 0.9265  | eval_custom_logloss: 0.84645 |  0:02:44s
epoch 8  | loss: 0.91489 | eval_custom_logloss: 1.16108 |  0:03:05s
epoch 9  | loss: 0.8975  | eval_custom_logloss: 1.22956 |  0:03:26s
epoch 10 | loss: 0.88163 | eval_custom_logloss: 0.83698 |  0:03:47s
epoch 11 | loss: 0.86647 | eval_custom_logloss: 0.82338 |  0:04:07s
epoch 12 | loss: 0.84834 | eval_custom_logloss: 0.83122 |  0:04:28s
epoch 13 | loss: 0.83115 | eval_custom_logloss: 0.80954 |  0:04:48s
epoch 14 | loss: 0.84212 | eval_custom_logloss: 1.16435 |  0:05:09s
epoch 15 | loss: 0.82876 | eval_custom_logloss: 1.22673 |  0:05:30s
epoch 16 | loss: 0.82368 | eval_custom_logloss: 1.1733  |  0:05:51s
epoch 17 | loss: 0.80696 | eval_custom_logloss: 0.88254 |  0:06:11s
epoch 18 | loss: 0.78373 | eval_custom_logloss: 0.74614 |  0:06:32s
epoch 19 | loss: 0.79626 | eval_custom_logloss: 0.93573 |  0:06:53s
epoch 20 | loss: 0.79417 | eval_custom_logloss: 0.92242 |  0:07:14s
epoch 21 | loss: 0.79835 | eval_custom_logloss: 0.85328 |  0:07:34s
epoch 22 | loss: 0.78022 | eval_custom_logloss: 0.88458 |  0:07:55s
epoch 23 | loss: 0.78727 | eval_custom_logloss: 1.18805 |  0:08:16s
epoch 24 | loss: 0.78369 | eval_custom_logloss: 1.57583 |  0:08:37s
epoch 25 | loss: 0.7779  | eval_custom_logloss: 1.86818 |  0:08:57s
epoch 26 | loss: 0.77314 | eval_custom_logloss: 0.79895 |  0:09:18s
epoch 27 | loss: 0.75301 | eval_custom_logloss: 0.73117 |  0:09:39s
epoch 28 | loss: 0.76024 | eval_custom_logloss: 0.78224 |  0:10:00s
epoch 29 | loss: 0.75047 | eval_custom_logloss: 0.7355  |  0:10:21s
epoch 30 | loss: 0.7508  | eval_custom_logloss: 0.93152 |  0:10:41s
epoch 31 | loss: 0.76563 | eval_custom_logloss: 0.89371 |  0:11:02s
epoch 32 | loss: 0.74987 | eval_custom_logloss: 0.77007 |  0:11:23s
epoch 33 | loss: 0.76787 | eval_custom_logloss: 0.97618 |  0:11:44s
epoch 34 | loss: 0.75565 | eval_custom_logloss: 0.98331 |  0:12:05s
epoch 35 | loss: 0.7588  | eval_custom_logloss: 1.40942 |  0:12:25s
epoch 36 | loss: 0.74209 | eval_custom_logloss: 0.73392 |  0:12:46s
epoch 37 | loss: 0.73501 | eval_custom_logloss: 0.7508  |  0:13:07s
epoch 38 | loss: 0.73558 | eval_custom_logloss: 1.8979  |  0:13:27s
epoch 39 | loss: 0.73268 | eval_custom_logloss: 0.86185 |  0:13:48s
epoch 40 | loss: 0.73164 | eval_custom_logloss: 0.79967 |  0:14:09s
epoch 41 | loss: 0.72446 | eval_custom_logloss: 1.22408 |  0:14:29s
epoch 42 | loss: 0.73752 | eval_custom_logloss: 0.87467 |  0:14:50s
epoch 43 | loss: 0.76276 | eval_custom_logloss: 1.44388 |  0:15:11s
epoch 44 | loss: 0.73559 | eval_custom_logloss: 0.71676 |  0:15:31s
epoch 45 | loss: 0.73095 | eval_custom_logloss: 1.23424 |  0:15:52s
epoch 46 | loss: 0.74523 | eval_custom_logloss: 0.83877 |  0:16:13s
epoch 47 | loss: 0.72872 | eval_custom_logloss: 0.77101 |  0:16:33s
epoch 48 | loss: 0.74341 | eval_custom_logloss: 1.12013 |  0:16:54s
epoch 49 | loss: 0.72849 | eval_custom_logloss: 0.84054 |  0:17:15s
epoch 50 | loss: 0.71626 | eval_custom_logloss: 0.98556 |  0:17:36s
epoch 51 | loss: 0.72416 | eval_custom_logloss: 0.74828 |  0:17:57s
epoch 52 | loss: 0.73117 | eval_custom_logloss: 0.73584 |  0:18:17s
epoch 53 | loss: 0.7163  | eval_custom_logloss: 1.08113 |  0:18:38s
epoch 54 | loss: 0.72173 | eval_custom_logloss: 1.13154 |  0:18:59s
epoch 55 | loss: 0.70645 | eval_custom_logloss: 1.13953 |  0:19:19s
epoch 56 | loss: 0.71484 | eval_custom_logloss: 0.94989 |  0:19:40s
epoch 57 | loss: 0.706   | eval_custom_logloss: 1.49334 |  0:20:01s
epoch 58 | loss: 0.71709 | eval_custom_logloss: 0.70341 |  0:20:21s
epoch 59 | loss: 0.70955 | eval_custom_logloss: 1.50444 |  0:20:42s
epoch 60 | loss: 0.69802 | eval_custom_logloss: 0.76991 |  0:21:03s
epoch 61 | loss: 0.69787 | eval_custom_logloss: 0.99605 |  0:21:23s
epoch 62 | loss: 0.69495 | eval_custom_logloss: 0.80814 |  0:21:44s
epoch 63 | loss: 0.70864 | eval_custom_logloss: 0.68935 |  0:22:05s
epoch 64 | loss: 0.73036 | eval_custom_logloss: 2.12612 |  0:22:25s
epoch 65 | loss: 0.71613 | eval_custom_logloss: 0.71464 |  0:22:46s
epoch 66 | loss: 0.71858 | eval_custom_logloss: 0.845   |  0:23:07s
epoch 67 | loss: 0.71043 | eval_custom_logloss: 1.00601 |  0:23:27s
epoch 68 | loss: 0.69153 | eval_custom_logloss: 0.82728 |  0:23:48s
epoch 69 | loss: 0.70644 | eval_custom_logloss: 1.78853 |  0:24:09s
epoch 70 | loss: 0.71184 | eval_custom_logloss: 1.6714  |  0:24:29s
epoch 71 | loss: 0.69305 | eval_custom_logloss: 1.68069 |  0:24:50s
epoch 72 | loss: 0.69058 | eval_custom_logloss: 0.75924 |  0:25:10s
epoch 73 | loss: 0.68842 | eval_custom_logloss: 0.87945 |  0:25:31s
epoch 74 | loss: 0.72236 | eval_custom_logloss: 0.75263 |  0:25:52s
epoch 75 | loss: 0.74544 | eval_custom_logloss: 0.82815 |  0:26:13s
epoch 76 | loss: 0.71582 | eval_custom_logloss: 0.80436 |  0:26:34s
epoch 77 | loss: 0.70266 | eval_custom_logloss: 0.68903 |  0:26:55s
epoch 78 | loss: 0.69622 | eval_custom_logloss: 1.42133 |  0:27:16s
epoch 79 | loss: 0.68751 | eval_custom_logloss: 1.01447 |  0:27:36s
epoch 80 | loss: 0.68182 | eval_custom_logloss: 0.90417 |  0:27:57s
epoch 81 | loss: 0.68906 | eval_custom_logloss: 2.95785 |  0:28:18s
epoch 82 | loss: 0.6799  | eval_custom_logloss: 0.90987 |  0:28:39s
epoch 83 | loss: 0.68232 | eval_custom_logloss: 0.82545 |  0:29:00s
epoch 84 | loss: 0.67679 | eval_custom_logloss: 0.77304 |  0:29:21s
epoch 85 | loss: 0.67326 | eval_custom_logloss: 0.99855 |  0:29:42s
epoch 86 | loss: 0.67277 | eval_custom_logloss: 0.79975 |  0:30:03s
epoch 87 | loss: 0.6747  | eval_custom_logloss: 1.09933 |  0:30:24s
epoch 88 | loss: 0.664   | eval_custom_logloss: 0.66679 |  0:30:45s
epoch 89 | loss: 0.66712 | eval_custom_logloss: 0.71427 |  0:31:05s
epoch 90 | loss: 0.66658 | eval_custom_logloss: 0.9348  |  0:31:26s
epoch 91 | loss: 0.67963 | eval_custom_logloss: 0.75005 |  0:31:47s
epoch 92 | loss: 0.67696 | eval_custom_logloss: 0.68797 |  0:32:08s
epoch 93 | loss: 0.67287 | eval_custom_logloss: 1.46938 |  0:32:29s
epoch 94 | loss: 0.66975 | eval_custom_logloss: 0.72597 |  0:32:50s
epoch 95 | loss: 0.67285 | eval_custom_logloss: 1.00389 |  0:33:11s
epoch 96 | loss: 0.66749 | eval_custom_logloss: 0.74464 |  0:33:32s
epoch 97 | loss: 0.672   | eval_custom_logloss: 0.95559 |  0:33:52s
epoch 98 | loss: 0.66735 | eval_custom_logloss: 0.90937 |  0:34:13s
epoch 99 | loss: 0.67468 | eval_custom_logloss: 0.86277 |  0:34:34s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 0.66679
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6541, 'Log Loss - std': 0.04665068774055388} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax', 'n_a': 32, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68599 | eval_custom_logloss: 4.74276 |  0:00:20s
epoch 1  | loss: 1.16933 | eval_custom_logloss: 5.55432 |  0:00:41s
epoch 2  | loss: 1.10325 | eval_custom_logloss: 3.89728 |  0:01:02s
epoch 3  | loss: 1.02037 | eval_custom_logloss: 2.33077 |  0:01:23s
epoch 4  | loss: 0.98808 | eval_custom_logloss: 2.28843 |  0:01:44s
epoch 5  | loss: 0.96333 | eval_custom_logloss: 1.04352 |  0:02:04s
epoch 6  | loss: 0.96444 | eval_custom_logloss: 1.66769 |  0:02:25s
epoch 7  | loss: 0.92587 | eval_custom_logloss: 1.14547 |  0:02:46s
epoch 8  | loss: 0.92098 | eval_custom_logloss: 0.84737 |  0:03:07s
epoch 9  | loss: 0.8999  | eval_custom_logloss: 1.58807 |  0:03:28s
epoch 10 | loss: 0.88311 | eval_custom_logloss: 1.04101 |  0:03:48s
epoch 11 | loss: 0.86741 | eval_custom_logloss: 0.89892 |  0:04:09s
epoch 12 | loss: 0.87373 | eval_custom_logloss: 0.83107 |  0:04:30s
epoch 13 | loss: 0.85289 | eval_custom_logloss: 1.26273 |  0:04:51s
epoch 14 | loss: 0.85005 | eval_custom_logloss: 0.81573 |  0:05:12s
epoch 15 | loss: 0.84344 | eval_custom_logloss: 0.93579 |  0:05:32s
epoch 16 | loss: 0.84213 | eval_custom_logloss: 0.85249 |  0:05:53s
epoch 17 | loss: 0.86414 | eval_custom_logloss: 1.00376 |  0:06:14s
epoch 18 | loss: 0.86051 | eval_custom_logloss: 1.17091 |  0:06:35s
epoch 19 | loss: 0.8223  | eval_custom_logloss: 0.79948 |  0:06:56s
epoch 20 | loss: 0.82964 | eval_custom_logloss: 0.80362 |  0:07:17s
epoch 21 | loss: 0.82837 | eval_custom_logloss: 0.7697  |  0:07:37s
epoch 22 | loss: 0.80335 | eval_custom_logloss: 1.09711 |  0:07:58s
epoch 23 | loss: 0.81317 | eval_custom_logloss: 0.76823 |  0:08:19s
epoch 24 | loss: 0.80167 | eval_custom_logloss: 0.75865 |  0:08:40s
epoch 25 | loss: 0.7992  | eval_custom_logloss: 0.75486 |  0:09:01s
epoch 26 | loss: 0.81198 | eval_custom_logloss: 1.19014 |  0:09:21s
epoch 27 | loss: 0.80785 | eval_custom_logloss: 0.81065 |  0:09:42s
epoch 28 | loss: 0.80644 | eval_custom_logloss: 0.73424 |  0:10:03s
epoch 29 | loss: 0.79825 | eval_custom_logloss: 0.80067 |  0:10:24s
epoch 30 | loss: 0.80088 | eval_custom_logloss: 0.9305  |  0:10:44s
epoch 31 | loss: 0.80575 | eval_custom_logloss: 0.85547 |  0:11:05s
epoch 32 | loss: 0.78014 | eval_custom_logloss: 0.76811 |  0:11:26s
epoch 33 | loss: 0.77511 | eval_custom_logloss: 0.78414 |  0:11:47s
epoch 34 | loss: 0.78643 | eval_custom_logloss: 0.79958 |  0:12:08s
epoch 35 | loss: 0.78239 | eval_custom_logloss: 0.79738 |  0:12:28s
epoch 36 | loss: 0.76757 | eval_custom_logloss: 0.78362 |  0:12:49s
epoch 37 | loss: 0.79403 | eval_custom_logloss: 1.283   |  0:13:10s
epoch 38 | loss: 0.77223 | eval_custom_logloss: 1.00705 |  0:13:31s
epoch 39 | loss: 0.78562 | eval_custom_logloss: 0.81151 |  0:13:51s
epoch 40 | loss: 0.77688 | eval_custom_logloss: 0.86738 |  0:14:12s
epoch 41 | loss: 0.77626 | eval_custom_logloss: 0.75366 |  0:14:33s
epoch 42 | loss: 0.75972 | eval_custom_logloss: 0.85878 |  0:14:54s
epoch 43 | loss: 0.76397 | eval_custom_logloss: 0.92267 |  0:15:15s
epoch 44 | loss: 0.77044 | eval_custom_logloss: 0.76489 |  0:15:35s
epoch 45 | loss: 0.76541 | eval_custom_logloss: 0.71019 |  0:15:56s
epoch 46 | loss: 0.772   | eval_custom_logloss: 0.78423 |  0:16:18s
epoch 47 | loss: 0.76033 | eval_custom_logloss: 0.87925 |  0:16:40s
epoch 48 | loss: 0.76919 | eval_custom_logloss: 0.82864 |  0:17:02s
epoch 49 | loss: 0.75732 | eval_custom_logloss: 0.72762 |  0:17:24s
epoch 50 | loss: 0.75778 | eval_custom_logloss: 0.82612 |  0:17:46s
epoch 51 | loss: 0.74798 | eval_custom_logloss: 0.79207 |  0:18:08s
epoch 52 | loss: 0.75494 | eval_custom_logloss: 0.84063 |  0:18:30s
epoch 53 | loss: 0.75569 | eval_custom_logloss: 1.14462 |  0:18:50s
epoch 54 | loss: 0.751   | eval_custom_logloss: 0.70623 |  0:19:11s
epoch 55 | loss: 0.74684 | eval_custom_logloss: 0.79435 |  0:19:32s
epoch 56 | loss: 0.75183 | eval_custom_logloss: 0.92509 |  0:19:52s
epoch 57 | loss: 0.75521 | eval_custom_logloss: 0.73016 |  0:20:13s
epoch 58 | loss: 0.74823 | eval_custom_logloss: 0.75905 |  0:20:33s
epoch 59 | loss: 0.78379 | eval_custom_logloss: 0.86677 |  0:20:54s
epoch 60 | loss: 0.77426 | eval_custom_logloss: 1.08918 |  0:21:14s
epoch 61 | loss: 0.75641 | eval_custom_logloss: 0.75662 |  0:21:35s
epoch 62 | loss: 0.77052 | eval_custom_logloss: 0.73452 |  0:21:56s
epoch 63 | loss: 0.7563  | eval_custom_logloss: 0.92271 |  0:22:16s
epoch 64 | loss: 0.75052 | eval_custom_logloss: 0.76157 |  0:22:37s
epoch 65 | loss: 0.7628  | eval_custom_logloss: 1.00258 |  0:22:58s
epoch 66 | loss: 0.75749 | eval_custom_logloss: 0.9398  |  0:23:18s
epoch 67 | loss: 0.75547 | eval_custom_logloss: 0.71599 |  0:23:39s
epoch 68 | loss: 0.74154 | eval_custom_logloss: 0.69721 |  0:24:00s
epoch 69 | loss: 0.7392  | eval_custom_logloss: 1.06786 |  0:24:21s
epoch 70 | loss: 0.73919 | eval_custom_logloss: 0.77195 |  0:24:41s
epoch 71 | loss: 0.73084 | eval_custom_logloss: 0.7289  |  0:25:02s
epoch 72 | loss: 0.75742 | eval_custom_logloss: 0.86925 |  0:25:23s
epoch 73 | loss: 0.74108 | eval_custom_logloss: 0.83093 |  0:25:44s
epoch 74 | loss: 0.74632 | eval_custom_logloss: 0.73316 |  0:26:04s
epoch 75 | loss: 0.73502 | eval_custom_logloss: 0.7129  |  0:26:25s
epoch 76 | loss: 0.73917 | eval_custom_logloss: 0.70404 |  0:26:46s
epoch 77 | loss: 0.73966 | eval_custom_logloss: 0.96784 |  0:27:07s
epoch 78 | loss: 0.73708 | eval_custom_logloss: 0.73543 |  0:27:28s
epoch 79 | loss: 0.73796 | eval_custom_logloss: 0.75359 |  0:27:48s
epoch 80 | loss: 0.7318  | eval_custom_logloss: 0.78817 |  0:28:09s
epoch 81 | loss: 0.73015 | eval_custom_logloss: 0.71505 |  0:28:29s
epoch 82 | loss: 0.73099 | eval_custom_logloss: 0.74236 |  0:28:50s
epoch 83 | loss: 0.73491 | eval_custom_logloss: 0.71613 |  0:29:10s
epoch 84 | loss: 0.72664 | eval_custom_logloss: 0.72806 |  0:29:31s
epoch 85 | loss: 0.72933 | eval_custom_logloss: 1.29145 |  0:29:51s
epoch 86 | loss: 0.73515 | eval_custom_logloss: 0.73289 |  0:30:12s
epoch 87 | loss: 0.72606 | eval_custom_logloss: 0.78207 |  0:30:33s
epoch 88 | loss: 0.7323  | eval_custom_logloss: 0.91384 |  0:30:53s

Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_custom_logloss = 0.69721
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.664275, 'Log Loss - std': 0.04407728298114571} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax', 'n_a': 32, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.59116 | eval_custom_logloss: 4.79135 |  0:00:20s
epoch 1  | loss: 1.14779 | eval_custom_logloss: 1.74141 |  0:00:41s
epoch 2  | loss: 1.02302 | eval_custom_logloss: 1.73135 |  0:01:02s
epoch 3  | loss: 0.9723  | eval_custom_logloss: 1.65842 |  0:01:22s
epoch 4  | loss: 0.93799 | eval_custom_logloss: 1.10731 |  0:01:43s
epoch 5  | loss: 0.93401 | eval_custom_logloss: 0.89072 |  0:02:04s
epoch 6  | loss: 0.90764 | eval_custom_logloss: 0.80913 |  0:02:24s
epoch 7  | loss: 0.88116 | eval_custom_logloss: 0.94587 |  0:02:45s
epoch 8  | loss: 0.85806 | eval_custom_logloss: 0.82388 |  0:03:06s
epoch 9  | loss: 0.86306 | eval_custom_logloss: 0.77751 |  0:03:27s
epoch 10 | loss: 0.82812 | eval_custom_logloss: 0.83124 |  0:03:48s
epoch 11 | loss: 0.81042 | eval_custom_logloss: 0.91147 |  0:04:09s
epoch 12 | loss: 0.80348 | eval_custom_logloss: 0.89948 |  0:04:30s
epoch 13 | loss: 0.78414 | eval_custom_logloss: 0.71835 |  0:04:51s
epoch 14 | loss: 0.78429 | eval_custom_logloss: 0.74013 |  0:05:12s
epoch 15 | loss: 0.75957 | eval_custom_logloss: 0.78192 |  0:05:33s
epoch 16 | loss: 0.75406 | eval_custom_logloss: 0.79536 |  0:05:53s
epoch 17 | loss: 0.76856 | eval_custom_logloss: 1.04011 |  0:06:14s
epoch 18 | loss: 0.74855 | eval_custom_logloss: 0.76295 |  0:06:35s
epoch 19 | loss: 0.74245 | eval_custom_logloss: 0.80317 |  0:06:56s
epoch 20 | loss: 0.73934 | eval_custom_logloss: 0.7188  |  0:07:16s
epoch 21 | loss: 0.7299  | eval_custom_logloss: 0.80911 |  0:07:37s
epoch 22 | loss: 0.72236 | eval_custom_logloss: 1.19234 |  0:07:58s
epoch 23 | loss: 0.71289 | eval_custom_logloss: 0.87162 |  0:08:19s
epoch 24 | loss: 0.70472 | eval_custom_logloss: 0.84984 |  0:08:39s
epoch 25 | loss: 0.72309 | eval_custom_logloss: 1.06361 |  0:09:00s
epoch 26 | loss: 0.701   | eval_custom_logloss: 0.88552 |  0:09:21s
epoch 27 | loss: 0.69409 | eval_custom_logloss: 0.71648 |  0:09:42s
epoch 28 | loss: 0.69042 | eval_custom_logloss: 0.96964 |  0:10:02s
epoch 29 | loss: 0.69421 | eval_custom_logloss: 0.85184 |  0:10:23s
epoch 30 | loss: 0.70281 | eval_custom_logloss: 0.73745 |  0:10:43s
epoch 31 | loss: 0.69451 | eval_custom_logloss: 0.84637 |  0:11:04s
epoch 32 | loss: 0.68353 | eval_custom_logloss: 0.98134 |  0:11:24s
epoch 33 | loss: 0.68111 | eval_custom_logloss: 0.6801  |  0:11:45s
epoch 34 | loss: 0.68275 | eval_custom_logloss: 0.89901 |  0:12:06s
epoch 35 | loss: 0.66573 | eval_custom_logloss: 0.86572 |  0:12:27s
epoch 36 | loss: 0.67774 | eval_custom_logloss: 1.00467 |  0:12:48s
epoch 37 | loss: 0.69312 | eval_custom_logloss: 0.83983 |  0:13:08s
epoch 38 | loss: 0.66419 | eval_custom_logloss: 0.79605 |  0:13:29s
epoch 39 | loss: 0.66425 | eval_custom_logloss: 0.94137 |  0:13:50s
epoch 40 | loss: 0.65602 | eval_custom_logloss: 0.74155 |  0:14:11s
epoch 41 | loss: 0.67793 | eval_custom_logloss: 0.79766 |  0:14:32s
epoch 42 | loss: 0.65181 | eval_custom_logloss: 0.7156  |  0:14:52s
epoch 43 | loss: 0.67688 | eval_custom_logloss: 0.76611 |  0:15:13s
epoch 44 | loss: 0.658   | eval_custom_logloss: 0.75505 |  0:15:34s
epoch 45 | loss: 0.65773 | eval_custom_logloss: 0.98435 |  0:15:55s
epoch 46 | loss: 0.64943 | eval_custom_logloss: 0.80764 |  0:16:16s
epoch 47 | loss: 0.64842 | eval_custom_logloss: 0.77592 |  0:16:37s
epoch 48 | loss: 0.65469 | eval_custom_logloss: 0.79633 |  0:16:58s
epoch 49 | loss: 0.64089 | eval_custom_logloss: 0.84624 |  0:17:19s
epoch 50 | loss: 0.65044 | eval_custom_logloss: 0.91273 |  0:17:39s
epoch 51 | loss: 0.65518 | eval_custom_logloss: 0.74369 |  0:18:00s
epoch 52 | loss: 0.6415  | eval_custom_logloss: 0.81431 |  0:18:21s
epoch 53 | loss: 0.64386 | eval_custom_logloss: 0.84669 |  0:18:42s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.6801
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.66714, 'Log Loss - std': 0.039838152567607824} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 0.66714 and parameters: {'n_d': 32, 'n_steps': 8, 'gamma': 1.1803146088295313, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0046250739367755395, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 0.6860799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax', 'n_a': 22, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68061 | eval_custom_logloss: 3.71347 |  0:00:18s
epoch 1  | loss: 1.24163 | eval_custom_logloss: 2.3103  |  0:00:37s
epoch 2  | loss: 1.14215 | eval_custom_logloss: 1.55767 |  0:00:55s
epoch 3  | loss: 1.0952  | eval_custom_logloss: 1.17951 |  0:01:14s
epoch 4  | loss: 1.03832 | eval_custom_logloss: 1.49063 |  0:01:33s
epoch 5  | loss: 1.02159 | eval_custom_logloss: 1.17902 |  0:01:52s
epoch 6  | loss: 0.99752 | eval_custom_logloss: 1.50175 |  0:02:10s
epoch 7  | loss: 0.99595 | eval_custom_logloss: 0.91685 |  0:02:29s
epoch 8  | loss: 0.99304 | eval_custom_logloss: 1.01208 |  0:02:48s
epoch 9  | loss: 0.95697 | eval_custom_logloss: 0.95071 |  0:03:06s
epoch 10 | loss: 0.9437  | eval_custom_logloss: 0.84209 |  0:03:25s
epoch 11 | loss: 0.93588 | eval_custom_logloss: 1.23273 |  0:03:43s
epoch 12 | loss: 0.94837 | eval_custom_logloss: 1.04249 |  0:04:02s
epoch 13 | loss: 0.94855 | eval_custom_logloss: 0.91714 |  0:04:20s
epoch 14 | loss: 0.91262 | eval_custom_logloss: 0.86973 |  0:04:39s
epoch 15 | loss: 0.91966 | eval_custom_logloss: 1.04066 |  0:04:58s
epoch 16 | loss: 0.90626 | eval_custom_logloss: 0.86272 |  0:05:16s
epoch 17 | loss: 0.91053 | eval_custom_logloss: 1.23816 |  0:05:35s
epoch 18 | loss: 0.9226  | eval_custom_logloss: 0.9134  |  0:05:54s
epoch 19 | loss: 0.91923 | eval_custom_logloss: 0.90563 |  0:06:12s
epoch 20 | loss: 0.89407 | eval_custom_logloss: 0.89839 |  0:06:31s
epoch 21 | loss: 0.90794 | eval_custom_logloss: 0.91213 |  0:06:49s
epoch 22 | loss: 0.92064 | eval_custom_logloss: 0.84988 |  0:07:08s
epoch 23 | loss: 0.88303 | eval_custom_logloss: 0.9138  |  0:07:27s
epoch 24 | loss: 0.89735 | eval_custom_logloss: 1.4519  |  0:07:45s
epoch 25 | loss: 0.87336 | eval_custom_logloss: 0.83966 |  0:08:04s
epoch 26 | loss: 0.89639 | eval_custom_logloss: 0.93587 |  0:08:23s
epoch 27 | loss: 0.88765 | eval_custom_logloss: 0.80745 |  0:08:41s
epoch 28 | loss: 0.87473 | eval_custom_logloss: 0.93706 |  0:09:00s
epoch 29 | loss: 0.86289 | eval_custom_logloss: 0.84319 |  0:09:19s
epoch 30 | loss: 0.85142 | eval_custom_logloss: 0.91387 |  0:09:37s
epoch 31 | loss: 0.85133 | eval_custom_logloss: 1.20124 |  0:09:56s
epoch 32 | loss: 0.84478 | eval_custom_logloss: 0.75344 |  0:10:15s
epoch 33 | loss: 0.82242 | eval_custom_logloss: 1.05616 |  0:10:33s
epoch 34 | loss: 0.82134 | eval_custom_logloss: 1.10418 |  0:10:52s
epoch 35 | loss: 0.80363 | eval_custom_logloss: 0.78082 |  0:11:11s
epoch 36 | loss: 0.81069 | eval_custom_logloss: 1.14766 |  0:11:29s
epoch 37 | loss: 0.79848 | eval_custom_logloss: 0.80019 |  0:11:48s
epoch 38 | loss: 0.79852 | eval_custom_logloss: 0.72583 |  0:12:07s
epoch 39 | loss: 0.80135 | eval_custom_logloss: 0.82085 |  0:12:25s
epoch 40 | loss: 0.82022 | eval_custom_logloss: 1.2321  |  0:12:44s
epoch 41 | loss: 0.82341 | eval_custom_logloss: 0.99351 |  0:13:03s
epoch 42 | loss: 0.80073 | eval_custom_logloss: 0.80762 |  0:13:21s
epoch 43 | loss: 0.8082  | eval_custom_logloss: 0.82242 |  0:13:40s
epoch 44 | loss: 0.78354 | eval_custom_logloss: 0.77871 |  0:13:59s
epoch 45 | loss: 0.77498 | eval_custom_logloss: 0.83173 |  0:14:17s
epoch 46 | loss: 0.79593 | eval_custom_logloss: 0.91102 |  0:14:36s
epoch 47 | loss: 0.80876 | eval_custom_logloss: 0.93119 |  0:14:54s
epoch 48 | loss: 0.78634 | eval_custom_logloss: 0.81092 |  0:15:13s
epoch 49 | loss: 0.76738 | eval_custom_logloss: 0.69841 |  0:15:31s
epoch 50 | loss: 0.76982 | eval_custom_logloss: 0.79595 |  0:15:50s
epoch 51 | loss: 0.76789 | eval_custom_logloss: 0.75891 |  0:16:09s
epoch 52 | loss: 0.78468 | eval_custom_logloss: 1.02126 |  0:16:27s
epoch 53 | loss: 0.76647 | eval_custom_logloss: 0.93829 |  0:16:46s
epoch 54 | loss: 0.76232 | eval_custom_logloss: 0.73604 |  0:17:05s
epoch 55 | loss: 0.75776 | eval_custom_logloss: 0.82643 |  0:17:24s
epoch 56 | loss: 0.75654 | eval_custom_logloss: 0.95626 |  0:17:42s
epoch 57 | loss: 0.74987 | eval_custom_logloss: 0.88164 |  0:18:01s
epoch 58 | loss: 0.76518 | eval_custom_logloss: 0.70489 |  0:18:20s
epoch 59 | loss: 0.75082 | eval_custom_logloss: 0.72995 |  0:18:39s
epoch 60 | loss: 0.74694 | eval_custom_logloss: 0.74599 |  0:18:57s
epoch 61 | loss: 0.75521 | eval_custom_logloss: 0.68513 |  0:19:16s
epoch 62 | loss: 0.74605 | eval_custom_logloss: 0.89959 |  0:19:35s
epoch 63 | loss: 0.74016 | eval_custom_logloss: 1.09454 |  0:19:53s
epoch 64 | loss: 0.72162 | eval_custom_logloss: 0.77011 |  0:20:12s
epoch 65 | loss: 0.73941 | eval_custom_logloss: 0.66775 |  0:20:31s
epoch 66 | loss: 0.73432 | eval_custom_logloss: 0.77055 |  0:20:50s
epoch 67 | loss: 0.7358  | eval_custom_logloss: 0.79867 |  0:21:08s
epoch 68 | loss: 0.72819 | eval_custom_logloss: 0.67749 |  0:21:27s
epoch 69 | loss: 0.73519 | eval_custom_logloss: 1.00659 |  0:21:46s
epoch 70 | loss: 0.73492 | eval_custom_logloss: 0.79095 |  0:22:05s
epoch 71 | loss: 0.72207 | eval_custom_logloss: 0.88403 |  0:22:24s
epoch 72 | loss: 0.71982 | eval_custom_logloss: 0.75037 |  0:22:42s
epoch 73 | loss: 0.7263  | eval_custom_logloss: 0.79668 |  0:23:01s
epoch 74 | loss: 0.7162  | eval_custom_logloss: 0.76147 |  0:23:20s
epoch 75 | loss: 0.74051 | eval_custom_logloss: 0.72956 |  0:23:39s
epoch 76 | loss: 0.71885 | eval_custom_logloss: 0.72531 |  0:23:58s
epoch 77 | loss: 0.72081 | eval_custom_logloss: 0.98863 |  0:24:16s
epoch 78 | loss: 0.72146 | eval_custom_logloss: 0.78127 |  0:24:35s
epoch 79 | loss: 0.71984 | eval_custom_logloss: 0.73339 |  0:24:54s
epoch 80 | loss: 0.7181  | eval_custom_logloss: 0.72646 |  0:25:12s
epoch 81 | loss: 0.73151 | eval_custom_logloss: 0.77532 |  0:25:31s
epoch 82 | loss: 0.72094 | eval_custom_logloss: 0.78875 |  0:25:50s
epoch 83 | loss: 0.72871 | eval_custom_logloss: 0.76759 |  0:26:08s
epoch 84 | loss: 0.70537 | eval_custom_logloss: 0.85423 |  0:26:27s
epoch 85 | loss: 0.70208 | eval_custom_logloss: 1.20023 |  0:26:46s

Early stopping occurred at epoch 85 with best_epoch = 65 and best_eval_custom_logloss = 0.66775
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6658, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax', 'n_a': 22, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.67265 | eval_custom_logloss: 5.57585 |  0:00:18s
epoch 1  | loss: 1.29028 | eval_custom_logloss: 2.75342 |  0:00:37s
epoch 2  | loss: 1.05515 | eval_custom_logloss: 1.07676 |  0:00:56s
epoch 3  | loss: 0.98921 | eval_custom_logloss: 1.35982 |  0:01:14s
epoch 4  | loss: 0.93805 | eval_custom_logloss: 1.40175 |  0:01:33s
epoch 5  | loss: 0.92464 | eval_custom_logloss: 0.95074 |  0:01:52s
epoch 6  | loss: 0.86343 | eval_custom_logloss: 0.93447 |  0:02:11s
epoch 7  | loss: 0.87951 | eval_custom_logloss: 1.26662 |  0:02:29s
epoch 8  | loss: 0.85773 | eval_custom_logloss: 0.883   |  0:02:48s
epoch 9  | loss: 0.83652 | eval_custom_logloss: 1.73729 |  0:03:07s
epoch 10 | loss: 0.81463 | eval_custom_logloss: 1.01656 |  0:03:25s
epoch 11 | loss: 0.81318 | eval_custom_logloss: 1.20286 |  0:03:44s
epoch 12 | loss: 0.80388 | eval_custom_logloss: 0.82723 |  0:04:03s
epoch 13 | loss: 0.81115 | eval_custom_logloss: 0.87639 |  0:04:22s
epoch 14 | loss: 0.78932 | eval_custom_logloss: 1.01147 |  0:04:40s
epoch 15 | loss: 0.79953 | eval_custom_logloss: 0.91449 |  0:04:59s
epoch 16 | loss: 0.79181 | eval_custom_logloss: 0.73641 |  0:05:18s
epoch 17 | loss: 0.76949 | eval_custom_logloss: 0.79805 |  0:05:36s
epoch 18 | loss: 0.77598 | eval_custom_logloss: 0.86014 |  0:05:55s
epoch 19 | loss: 0.7678  | eval_custom_logloss: 0.80693 |  0:06:13s
epoch 20 | loss: 0.7579  | eval_custom_logloss: 1.22215 |  0:06:32s
epoch 21 | loss: 0.75792 | eval_custom_logloss: 0.9277  |  0:06:51s
epoch 22 | loss: 0.76657 | eval_custom_logloss: 1.06861 |  0:07:09s
epoch 23 | loss: 0.77224 | eval_custom_logloss: 0.83075 |  0:07:28s
epoch 24 | loss: 0.73636 | eval_custom_logloss: 0.80596 |  0:07:47s
epoch 25 | loss: 0.73926 | eval_custom_logloss: 0.7566  |  0:08:05s
epoch 26 | loss: 0.74409 | eval_custom_logloss: 0.69742 |  0:08:24s
epoch 27 | loss: 0.74957 | eval_custom_logloss: 0.83968 |  0:08:43s
epoch 28 | loss: 0.74938 | eval_custom_logloss: 0.76155 |  0:09:01s
epoch 29 | loss: 0.72571 | eval_custom_logloss: 0.83696 |  0:09:20s
epoch 30 | loss: 0.73285 | eval_custom_logloss: 0.84326 |  0:09:39s
epoch 31 | loss: 0.72953 | eval_custom_logloss: 0.75249 |  0:09:57s
epoch 32 | loss: 0.73318 | eval_custom_logloss: 0.77455 |  0:10:16s
epoch 33 | loss: 0.70866 | eval_custom_logloss: 1.08326 |  0:10:35s
epoch 34 | loss: 0.72419 | eval_custom_logloss: 0.96998 |  0:10:53s
epoch 35 | loss: 0.73401 | eval_custom_logloss: 1.18706 |  0:11:12s
epoch 36 | loss: 0.69714 | eval_custom_logloss: 0.6397  |  0:11:30s
epoch 37 | loss: 0.72156 | eval_custom_logloss: 0.84817 |  0:11:49s
epoch 38 | loss: 0.71423 | eval_custom_logloss: 0.71341 |  0:12:08s
epoch 39 | loss: 0.72149 | eval_custom_logloss: 0.70077 |  0:12:26s
epoch 40 | loss: 0.71654 | eval_custom_logloss: 0.75211 |  0:12:45s
epoch 41 | loss: 0.71609 | eval_custom_logloss: 1.06391 |  0:13:03s
epoch 42 | loss: 0.70115 | eval_custom_logloss: 0.81262 |  0:13:22s
epoch 43 | loss: 0.70588 | eval_custom_logloss: 0.74754 |  0:13:40s
epoch 44 | loss: 0.70503 | eval_custom_logloss: 0.8809  |  0:13:59s
epoch 45 | loss: 0.68447 | eval_custom_logloss: 0.63855 |  0:14:17s
epoch 46 | loss: 0.70535 | eval_custom_logloss: 0.72205 |  0:14:36s
epoch 47 | loss: 0.70628 | eval_custom_logloss: 0.85845 |  0:14:54s
epoch 48 | loss: 0.6911  | eval_custom_logloss: 0.98286 |  0:15:13s
epoch 49 | loss: 0.70659 | eval_custom_logloss: 0.94694 |  0:15:31s
epoch 50 | loss: 0.68114 | eval_custom_logloss: 0.98409 |  0:15:50s
epoch 51 | loss: 0.69091 | eval_custom_logloss: 0.75457 |  0:16:09s
epoch 52 | loss: 0.69769 | eval_custom_logloss: 1.00932 |  0:16:27s
epoch 53 | loss: 0.69076 | eval_custom_logloss: 0.90787 |  0:16:45s
epoch 54 | loss: 0.66851 | eval_custom_logloss: 0.73978 |  0:17:04s
epoch 55 | loss: 0.70423 | eval_custom_logloss: 1.20148 |  0:17:22s
epoch 56 | loss: 0.68361 | eval_custom_logloss: 0.73192 |  0:17:41s
epoch 57 | loss: 0.67953 | eval_custom_logloss: 0.78432 |  0:17:59s
epoch 58 | loss: 0.69581 | eval_custom_logloss: 0.78664 |  0:18:18s
epoch 59 | loss: 0.67936 | eval_custom_logloss: 0.83368 |  0:18:37s
epoch 60 | loss: 0.68322 | eval_custom_logloss: 0.68321 |  0:18:55s
epoch 61 | loss: 0.68986 | eval_custom_logloss: 0.63237 |  0:19:14s
epoch 62 | loss: 0.68324 | eval_custom_logloss: 1.104   |  0:19:32s
epoch 63 | loss: 0.68156 | eval_custom_logloss: 0.76907 |  0:19:51s
epoch 64 | loss: 0.66585 | eval_custom_logloss: 1.36555 |  0:20:09s
epoch 65 | loss: 0.68095 | eval_custom_logloss: 1.05058 |  0:20:28s
epoch 66 | loss: 0.67663 | eval_custom_logloss: 1.10982 |  0:20:46s
epoch 67 | loss: 0.67814 | eval_custom_logloss: 1.46533 |  0:21:05s
epoch 68 | loss: 0.67789 | eval_custom_logloss: 0.85319 |  0:21:23s
epoch 69 | loss: 0.66912 | eval_custom_logloss: 0.87458 |  0:21:42s
epoch 70 | loss: 0.67949 | eval_custom_logloss: 0.68594 |  0:22:00s
epoch 71 | loss: 0.6737  | eval_custom_logloss: 0.7571  |  0:22:19s
epoch 72 | loss: 0.67798 | eval_custom_logloss: 0.98124 |  0:22:38s
epoch 73 | loss: 0.68714 | eval_custom_logloss: 0.72844 |  0:22:56s
epoch 74 | loss: 0.65118 | eval_custom_logloss: 0.69239 |  0:23:15s
epoch 75 | loss: 0.6703  | eval_custom_logloss: 0.74361 |  0:23:33s
epoch 76 | loss: 0.67521 | eval_custom_logloss: 0.82863 |  0:23:52s
epoch 77 | loss: 0.68741 | eval_custom_logloss: 0.69046 |  0:24:10s
epoch 78 | loss: 0.69586 | eval_custom_logloss: 0.82298 |  0:24:29s
epoch 79 | loss: 0.66169 | eval_custom_logloss: 1.10022 |  0:24:47s
epoch 80 | loss: 0.68092 | eval_custom_logloss: 0.78405 |  0:25:06s
epoch 81 | loss: 0.66351 | eval_custom_logloss: 0.76679 |  0:25:24s

Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_custom_logloss = 0.63237
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.64865, 'Log Loss - std': 0.01715} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax', 'n_a': 22, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68287 | eval_custom_logloss: 4.05172 |  0:00:18s
epoch 1  | loss: 1.25403 | eval_custom_logloss: 1.5731  |  0:00:37s
epoch 2  | loss: 1.16441 | eval_custom_logloss: 1.78784 |  0:00:56s
epoch 3  | loss: 1.08476 | eval_custom_logloss: 2.4721  |  0:01:14s
epoch 4  | loss: 1.03029 | eval_custom_logloss: 1.33282 |  0:01:33s
epoch 5  | loss: 0.99827 | eval_custom_logloss: 2.51728 |  0:01:52s
epoch 6  | loss: 0.95759 | eval_custom_logloss: 0.93596 |  0:02:11s
epoch 7  | loss: 0.97018 | eval_custom_logloss: 1.4984  |  0:02:30s
epoch 8  | loss: 0.96226 | eval_custom_logloss: 1.1494  |  0:02:48s
epoch 9  | loss: 0.94955 | eval_custom_logloss: 1.7427  |  0:03:07s
epoch 10 | loss: 0.92251 | eval_custom_logloss: 0.83635 |  0:03:26s
epoch 11 | loss: 0.91166 | eval_custom_logloss: 0.93549 |  0:03:45s
epoch 12 | loss: 0.90612 | eval_custom_logloss: 0.78334 |  0:04:03s
epoch 13 | loss: 0.89886 | eval_custom_logloss: 0.99595 |  0:04:22s
epoch 14 | loss: 0.88133 | eval_custom_logloss: 1.3658  |  0:04:41s
epoch 15 | loss: 0.86628 | eval_custom_logloss: 1.14957 |  0:04:59s
epoch 16 | loss: 0.86055 | eval_custom_logloss: 1.52425 |  0:05:18s
epoch 17 | loss: 0.83179 | eval_custom_logloss: 0.80478 |  0:05:37s
epoch 18 | loss: 0.84649 | eval_custom_logloss: 0.80582 |  0:05:56s
epoch 19 | loss: 0.80999 | eval_custom_logloss: 0.71324 |  0:06:14s
epoch 20 | loss: 0.78791 | eval_custom_logloss: 1.08037 |  0:06:33s
epoch 21 | loss: 0.78564 | eval_custom_logloss: 1.44178 |  0:06:52s
epoch 22 | loss: 0.79577 | eval_custom_logloss: 0.85971 |  0:07:11s
epoch 23 | loss: 0.77424 | eval_custom_logloss: 1.33066 |  0:07:29s
epoch 24 | loss: 0.75745 | eval_custom_logloss: 0.93921 |  0:07:48s
epoch 25 | loss: 0.75259 | eval_custom_logloss: 1.36237 |  0:08:07s
epoch 26 | loss: 0.75381 | eval_custom_logloss: 1.69496 |  0:08:25s
epoch 27 | loss: 0.7598  | eval_custom_logloss: 0.83873 |  0:08:44s
epoch 28 | loss: 0.73511 | eval_custom_logloss: 0.93159 |  0:09:02s
epoch 29 | loss: 0.74474 | eval_custom_logloss: 0.8481  |  0:09:21s
epoch 30 | loss: 0.73635 | eval_custom_logloss: 0.88884 |  0:09:40s
epoch 31 | loss: 0.73198 | eval_custom_logloss: 0.79396 |  0:09:58s
epoch 32 | loss: 0.74561 | eval_custom_logloss: 0.93891 |  0:10:17s
epoch 33 | loss: 0.73242 | eval_custom_logloss: 3.60851 |  0:10:35s
epoch 34 | loss: 0.72346 | eval_custom_logloss: 1.24265 |  0:10:54s
epoch 35 | loss: 0.72736 | eval_custom_logloss: 0.84366 |  0:11:13s
epoch 36 | loss: 0.72228 | eval_custom_logloss: 0.82688 |  0:11:31s
epoch 37 | loss: 0.71713 | eval_custom_logloss: 0.76418 |  0:11:50s
epoch 38 | loss: 0.719   | eval_custom_logloss: 1.0831  |  0:12:09s
epoch 39 | loss: 0.71154 | eval_custom_logloss: 1.0223  |  0:12:27s

Early stopping occurred at epoch 39 with best_epoch = 19 and best_eval_custom_logloss = 0.71324
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6695333333333333, 'Log Loss - std': 0.032684994858327446} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax', 'n_a': 22, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.63249 | eval_custom_logloss: 5.76118 |  0:00:18s
epoch 1  | loss: 1.21716 | eval_custom_logloss: 3.96598 |  0:00:37s
epoch 2  | loss: 1.07915 | eval_custom_logloss: 1.54505 |  0:00:56s
epoch 3  | loss: 1.01521 | eval_custom_logloss: 1.46727 |  0:01:14s
epoch 4  | loss: 0.97078 | eval_custom_logloss: 3.67888 |  0:01:33s
epoch 5  | loss: 0.94341 | eval_custom_logloss: 1.26599 |  0:01:52s
epoch 6  | loss: 0.92686 | eval_custom_logloss: 1.01653 |  0:02:10s
epoch 7  | loss: 0.93678 | eval_custom_logloss: 1.7632  |  0:02:29s
epoch 8  | loss: 0.91227 | eval_custom_logloss: 0.82365 |  0:02:47s
epoch 9  | loss: 0.90147 | eval_custom_logloss: 1.5049  |  0:03:06s
epoch 10 | loss: 0.86949 | eval_custom_logloss: 1.94622 |  0:03:25s
epoch 11 | loss: 0.8521  | eval_custom_logloss: 1.32776 |  0:03:44s
epoch 12 | loss: 0.82216 | eval_custom_logloss: 0.8558  |  0:04:02s
epoch 13 | loss: 0.80339 | eval_custom_logloss: 1.0665  |  0:04:21s
epoch 14 | loss: 0.78733 | eval_custom_logloss: 1.21705 |  0:04:40s
epoch 15 | loss: 0.78434 | eval_custom_logloss: 1.16812 |  0:04:58s
epoch 16 | loss: 0.79497 | eval_custom_logloss: 1.38153 |  0:05:17s
epoch 17 | loss: 0.84745 | eval_custom_logloss: 1.76034 |  0:05:36s
epoch 18 | loss: 0.78403 | eval_custom_logloss: 1.15659 |  0:05:54s
epoch 19 | loss: 0.76821 | eval_custom_logloss: 1.22093 |  0:06:13s
epoch 20 | loss: 0.75578 | eval_custom_logloss: 1.14948 |  0:06:32s
epoch 21 | loss: 0.74623 | eval_custom_logloss: 1.4114  |  0:06:50s
epoch 22 | loss: 0.73805 | eval_custom_logloss: 0.76613 |  0:07:09s
epoch 23 | loss: 0.74805 | eval_custom_logloss: 0.90584 |  0:07:28s
epoch 24 | loss: 0.72587 | eval_custom_logloss: 1.501   |  0:07:47s
epoch 25 | loss: 0.71742 | eval_custom_logloss: 1.17117 |  0:08:05s
epoch 26 | loss: 0.72004 | eval_custom_logloss: 1.16908 |  0:08:24s
epoch 27 | loss: 0.72281 | eval_custom_logloss: 0.83127 |  0:08:43s
epoch 28 | loss: 0.70572 | eval_custom_logloss: 1.00177 |  0:09:01s
epoch 29 | loss: 0.71572 | eval_custom_logloss: 1.06582 |  0:09:20s
epoch 30 | loss: 0.70909 | eval_custom_logloss: 0.95238 |  0:09:38s
epoch 31 | loss: 0.7047  | eval_custom_logloss: 0.76964 |  0:09:57s
epoch 32 | loss: 0.69471 | eval_custom_logloss: 0.71126 |  0:10:16s
epoch 33 | loss: 0.69702 | eval_custom_logloss: 0.93454 |  0:10:34s
epoch 34 | loss: 0.68484 | eval_custom_logloss: 0.97279 |  0:10:53s
epoch 35 | loss: 0.68876 | eval_custom_logloss: 1.03798 |  0:11:12s
epoch 36 | loss: 0.6835  | eval_custom_logloss: 0.82346 |  0:11:31s
epoch 37 | loss: 0.69216 | eval_custom_logloss: 0.93819 |  0:11:49s
epoch 38 | loss: 0.69225 | eval_custom_logloss: 1.14968 |  0:12:08s
epoch 39 | loss: 0.68827 | eval_custom_logloss: 0.82537 |  0:12:27s
epoch 40 | loss: 0.6843  | eval_custom_logloss: 0.87296 |  0:12:46s
epoch 41 | loss: 0.68527 | eval_custom_logloss: 0.74046 |  0:13:06s
epoch 42 | loss: 0.67922 | eval_custom_logloss: 0.65318 |  0:13:26s
epoch 43 | loss: 0.68916 | eval_custom_logloss: 1.02464 |  0:13:46s
epoch 44 | loss: 0.68035 | eval_custom_logloss: 0.83131 |  0:14:06s
epoch 45 | loss: 0.66604 | eval_custom_logloss: 0.86205 |  0:14:26s
epoch 46 | loss: 0.69507 | eval_custom_logloss: 1.06153 |  0:14:46s
epoch 47 | loss: 0.68322 | eval_custom_logloss: 0.8022  |  0:15:06s
epoch 48 | loss: 0.66864 | eval_custom_logloss: 0.84682 |  0:15:25s
epoch 49 | loss: 0.67549 | eval_custom_logloss: 0.72619 |  0:15:44s
epoch 50 | loss: 0.66213 | eval_custom_logloss: 1.00834 |  0:16:03s
epoch 51 | loss: 0.67208 | eval_custom_logloss: 0.98729 |  0:16:22s
epoch 52 | loss: 0.65553 | eval_custom_logloss: 1.16962 |  0:16:41s
epoch 53 | loss: 0.65784 | eval_custom_logloss: 0.86053 |  0:16:59s
epoch 54 | loss: 0.65202 | eval_custom_logloss: 0.78554 |  0:17:18s
epoch 55 | loss: 0.65906 | eval_custom_logloss: 1.0047  |  0:17:37s
epoch 56 | loss: 0.65276 | eval_custom_logloss: 0.67599 |  0:17:56s
epoch 57 | loss: 0.65262 | eval_custom_logloss: 0.81604 |  0:18:14s
epoch 58 | loss: 0.66673 | eval_custom_logloss: 0.77783 |  0:18:33s
epoch 59 | loss: 0.65099 | eval_custom_logloss: 0.78846 |  0:18:52s
epoch 60 | loss: 0.64801 | eval_custom_logloss: 0.82784 |  0:19:11s
epoch 61 | loss: 0.6408  | eval_custom_logloss: 0.63026 |  0:19:30s
epoch 62 | loss: 0.64648 | eval_custom_logloss: 0.81993 |  0:19:48s
epoch 63 | loss: 0.63979 | eval_custom_logloss: 0.94933 |  0:20:07s
epoch 64 | loss: 0.63589 | eval_custom_logloss: 0.77204 |  0:20:26s
epoch 65 | loss: 0.63865 | eval_custom_logloss: 0.87257 |  0:20:45s
epoch 66 | loss: 0.64388 | eval_custom_logloss: 0.79951 |  0:21:03s
epoch 67 | loss: 0.64947 | eval_custom_logloss: 0.8265  |  0:21:22s
epoch 68 | loss: 0.63179 | eval_custom_logloss: 0.66255 |  0:21:41s
epoch 69 | loss: 0.63378 | eval_custom_logloss: 0.91759 |  0:22:00s
epoch 70 | loss: 0.64073 | eval_custom_logloss: 0.71046 |  0:22:18s
epoch 71 | loss: 0.63312 | eval_custom_logloss: 0.74302 |  0:22:37s
epoch 72 | loss: 0.6346  | eval_custom_logloss: 0.69501 |  0:22:56s
epoch 73 | loss: 0.6322  | eval_custom_logloss: 0.96623 |  0:23:15s
epoch 74 | loss: 0.63318 | eval_custom_logloss: 0.78843 |  0:23:34s
epoch 75 | loss: 0.6306  | eval_custom_logloss: 0.73293 |  0:23:52s
epoch 76 | loss: 0.63293 | eval_custom_logloss: 0.75509 |  0:24:11s
epoch 77 | loss: 0.63067 | eval_custom_logloss: 0.86278 |  0:24:30s
epoch 78 | loss: 0.63468 | eval_custom_logloss: 0.87573 |  0:24:49s
epoch 79 | loss: 0.63073 | eval_custom_logloss: 0.77237 |  0:25:07s
epoch 80 | loss: 0.62551 | eval_custom_logloss: 0.9125  |  0:25:26s
epoch 81 | loss: 0.61357 | eval_custom_logloss: 0.73399 |  0:25:45s

Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_custom_logloss = 0.63026
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6595, 'Log Loss - std': 0.03321498155953127} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax', 'n_a': 22, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.70676 | eval_custom_logloss: 7.72767 |  0:00:18s
epoch 1  | loss: 1.26254 | eval_custom_logloss: 3.94092 |  0:00:37s
epoch 2  | loss: 1.03656 | eval_custom_logloss: 2.00373 |  0:00:56s
epoch 3  | loss: 0.97584 | eval_custom_logloss: 2.55524 |  0:01:15s
epoch 4  | loss: 0.95409 | eval_custom_logloss: 1.74989 |  0:01:33s
epoch 5  | loss: 0.93198 | eval_custom_logloss: 1.22676 |  0:01:52s
epoch 6  | loss: 0.89401 | eval_custom_logloss: 0.86825 |  0:02:11s
epoch 7  | loss: 0.91062 | eval_custom_logloss: 0.83174 |  0:02:29s
epoch 8  | loss: 0.90564 | eval_custom_logloss: 1.49353 |  0:02:48s
epoch 9  | loss: 0.89047 | eval_custom_logloss: 1.51959 |  0:03:06s
epoch 10 | loss: 0.86664 | eval_custom_logloss: 1.29736 |  0:03:25s
epoch 11 | loss: 0.8604  | eval_custom_logloss: 1.38262 |  0:03:43s
epoch 12 | loss: 0.85557 | eval_custom_logloss: 2.5427  |  0:04:02s
epoch 13 | loss: 0.84221 | eval_custom_logloss: 0.79427 |  0:04:21s
epoch 14 | loss: 0.82773 | eval_custom_logloss: 1.13427 |  0:04:39s
epoch 15 | loss: 0.82837 | eval_custom_logloss: 1.97027 |  0:04:58s
epoch 16 | loss: 0.80796 | eval_custom_logloss: 0.78964 |  0:05:16s
epoch 17 | loss: 0.79933 | eval_custom_logloss: 2.34201 |  0:05:35s
epoch 18 | loss: 0.78453 | eval_custom_logloss: 0.84387 |  0:05:54s
epoch 19 | loss: 0.77942 | eval_custom_logloss: 0.7373  |  0:06:12s
epoch 20 | loss: 0.78232 | eval_custom_logloss: 0.90368 |  0:06:31s
epoch 21 | loss: 0.78179 | eval_custom_logloss: 1.45241 |  0:06:50s
epoch 22 | loss: 0.79365 | eval_custom_logloss: 1.19987 |  0:07:09s
epoch 23 | loss: 0.79269 | eval_custom_logloss: 2.37548 |  0:07:27s
epoch 24 | loss: 0.7931  | eval_custom_logloss: 1.09745 |  0:07:46s
epoch 25 | loss: 0.76388 | eval_custom_logloss: 2.59524 |  0:08:05s
epoch 26 | loss: 0.75084 | eval_custom_logloss: 0.91502 |  0:08:23s
epoch 27 | loss: 0.78192 | eval_custom_logloss: 0.88668 |  0:08:42s
epoch 28 | loss: 0.77057 | eval_custom_logloss: 0.71622 |  0:09:00s
epoch 29 | loss: 0.77095 | eval_custom_logloss: 0.82389 |  0:09:19s
epoch 30 | loss: 0.75176 | eval_custom_logloss: 2.44525 |  0:09:38s
epoch 31 | loss: 0.76137 | eval_custom_logloss: 1.1948  |  0:09:56s
epoch 32 | loss: 0.76422 | eval_custom_logloss: 3.12192 |  0:10:15s
epoch 33 | loss: 0.76933 | eval_custom_logloss: 1.8094  |  0:10:33s
epoch 34 | loss: 0.75092 | eval_custom_logloss: 3.09534 |  0:10:52s
epoch 35 | loss: 0.75519 | eval_custom_logloss: 2.68689 |  0:11:10s
epoch 36 | loss: 0.76731 | eval_custom_logloss: 0.91804 |  0:11:29s
epoch 37 | loss: 0.75763 | eval_custom_logloss: 2.4189  |  0:11:47s
epoch 38 | loss: 0.75291 | eval_custom_logloss: 0.74511 |  0:12:06s
epoch 39 | loss: 0.75351 | eval_custom_logloss: 1.08628 |  0:12:24s
epoch 40 | loss: 0.74437 | eval_custom_logloss: 2.09992 |  0:12:43s
epoch 41 | loss: 0.75993 | eval_custom_logloss: 0.93217 |  0:13:02s
epoch 42 | loss: 0.74751 | eval_custom_logloss: 1.39189 |  0:13:20s
epoch 43 | loss: 0.76186 | eval_custom_logloss: 1.20752 |  0:13:39s
epoch 44 | loss: 0.74602 | eval_custom_logloss: 1.06283 |  0:13:58s
epoch 45 | loss: 0.74629 | eval_custom_logloss: 0.92277 |  0:14:16s
epoch 46 | loss: 0.77347 | eval_custom_logloss: 0.93524 |  0:14:35s
epoch 47 | loss: 0.75661 | eval_custom_logloss: 1.91156 |  0:14:54s
epoch 48 | loss: 0.74785 | eval_custom_logloss: 2.87389 |  0:15:12s

Early stopping occurred at epoch 48 with best_epoch = 28 and best_eval_custom_logloss = 0.71622
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.67054, 'Log Loss - std': 0.03701505639601272} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 0.67054 and parameters: {'n_d': 22, 'n_steps': 8, 'gamma': 1.4135922343414449, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0035795845073108186, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 0.6860799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.69088 | eval_custom_logloss: 13.67922|  0:00:20s
epoch 1  | loss: 1.34426 | eval_custom_logloss: 14.19951|  0:00:40s
epoch 2  | loss: 1.18066 | eval_custom_logloss: 11.79514|  0:01:01s
epoch 3  | loss: 1.0929  | eval_custom_logloss: 10.74204|  0:01:21s
epoch 4  | loss: 1.06691 | eval_custom_logloss: 8.90423 |  0:01:41s
epoch 5  | loss: 1.06112 | eval_custom_logloss: 7.07956 |  0:02:02s
epoch 6  | loss: 1.05798 | eval_custom_logloss: 7.48933 |  0:02:22s
epoch 7  | loss: 1.05435 | eval_custom_logloss: 7.91224 |  0:02:42s
epoch 8  | loss: 1.01138 | eval_custom_logloss: 7.92575 |  0:03:03s
epoch 9  | loss: 1.00172 | eval_custom_logloss: 7.85276 |  0:03:23s
epoch 10 | loss: 1.0462  | eval_custom_logloss: 7.29825 |  0:03:43s
epoch 11 | loss: 1.01926 | eval_custom_logloss: 8.77943 |  0:04:04s
epoch 12 | loss: 0.9944  | eval_custom_logloss: 7.6495  |  0:04:24s
epoch 13 | loss: 1.011   | eval_custom_logloss: 6.98603 |  0:04:45s
epoch 14 | loss: 0.94715 | eval_custom_logloss: 7.65471 |  0:05:05s
epoch 15 | loss: 0.93045 | eval_custom_logloss: 5.3645  |  0:05:25s
epoch 16 | loss: 0.93224 | eval_custom_logloss: 5.94829 |  0:05:46s
epoch 17 | loss: 0.93941 | eval_custom_logloss: 6.42412 |  0:06:06s
epoch 18 | loss: 0.91816 | eval_custom_logloss: 3.76861 |  0:06:26s
epoch 19 | loss: 0.89853 | eval_custom_logloss: 7.33406 |  0:06:47s
epoch 20 | loss: 0.894   | eval_custom_logloss: 6.23435 |  0:07:08s
epoch 21 | loss: 0.86333 | eval_custom_logloss: 3.83865 |  0:07:28s
epoch 22 | loss: 0.8773  | eval_custom_logloss: 1.59189 |  0:07:49s
epoch 23 | loss: 0.88124 | eval_custom_logloss: 3.63545 |  0:08:09s
epoch 24 | loss: 0.8519  | eval_custom_logloss: 3.43915 |  0:08:30s
epoch 25 | loss: 0.8455  | eval_custom_logloss: 2.9123  |  0:08:50s
epoch 26 | loss: 0.83263 | eval_custom_logloss: 1.21386 |  0:09:10s
epoch 27 | loss: 0.83736 | eval_custom_logloss: 2.04003 |  0:09:31s
epoch 28 | loss: 0.82539 | eval_custom_logloss: 1.27817 |  0:09:51s
epoch 29 | loss: 0.8211  | eval_custom_logloss: 1.31755 |  0:10:12s
epoch 30 | loss: 0.81847 | eval_custom_logloss: 4.06905 |  0:10:32s
epoch 31 | loss: 0.80794 | eval_custom_logloss: 3.73508 |  0:10:53s
epoch 32 | loss: 0.84683 | eval_custom_logloss: 5.11347 |  0:11:13s
epoch 33 | loss: 0.86119 | eval_custom_logloss: 3.711   |  0:11:33s
epoch 34 | loss: 0.82705 | eval_custom_logloss: 3.23451 |  0:11:54s
epoch 35 | loss: 0.82101 | eval_custom_logloss: 3.8359  |  0:12:14s
epoch 36 | loss: 0.82301 | eval_custom_logloss: 2.95148 |  0:12:35s
epoch 37 | loss: 0.80338 | eval_custom_logloss: 2.12823 |  0:12:55s
epoch 38 | loss: 0.82555 | eval_custom_logloss: 3.17327 |  0:13:15s
epoch 39 | loss: 0.80616 | eval_custom_logloss: 1.51333 |  0:13:36s
epoch 40 | loss: 0.80441 | eval_custom_logloss: 3.03724 |  0:13:56s
epoch 41 | loss: 0.78872 | eval_custom_logloss: 2.27947 |  0:14:17s
epoch 42 | loss: 0.77465 | eval_custom_logloss: 2.16741 |  0:14:37s
epoch 43 | loss: 0.79638 | eval_custom_logloss: 3.65672 |  0:14:57s
epoch 44 | loss: 0.78253 | eval_custom_logloss: 5.04085 |  0:15:18s
epoch 45 | loss: 0.7772  | eval_custom_logloss: 1.63707 |  0:15:38s
epoch 46 | loss: 0.78609 | eval_custom_logloss: 1.38685 |  0:15:59s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 1.21386
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.2089, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.6741  | eval_custom_logloss: 10.48046|  0:00:20s
epoch 1  | loss: 1.33612 | eval_custom_logloss: 11.27821|  0:00:41s
epoch 2  | loss: 1.1431  | eval_custom_logloss: 9.8487  |  0:01:01s
epoch 3  | loss: 1.03119 | eval_custom_logloss: 9.43053 |  0:01:22s
epoch 4  | loss: 0.99305 | eval_custom_logloss: 11.1109 |  0:01:43s
epoch 5  | loss: 0.97984 | eval_custom_logloss: 7.50993 |  0:02:03s
epoch 6  | loss: 0.98668 | eval_custom_logloss: 6.51126 |  0:02:24s
epoch 7  | loss: 0.9557  | eval_custom_logloss: 5.78977 |  0:02:44s
epoch 8  | loss: 0.96356 | eval_custom_logloss: 5.06904 |  0:03:05s
epoch 9  | loss: 0.96376 | eval_custom_logloss: 4.14752 |  0:03:25s
epoch 10 | loss: 0.96168 | eval_custom_logloss: 3.08728 |  0:03:46s
epoch 11 | loss: 0.94726 | eval_custom_logloss: 2.91011 |  0:04:06s
epoch 12 | loss: 0.92715 | eval_custom_logloss: 2.69963 |  0:04:27s
epoch 13 | loss: 0.95456 | eval_custom_logloss: 3.301   |  0:04:47s
epoch 14 | loss: 0.90437 | eval_custom_logloss: 3.83164 |  0:05:08s
epoch 15 | loss: 0.90376 | eval_custom_logloss: 3.96976 |  0:05:29s
epoch 16 | loss: 0.91762 | eval_custom_logloss: 2.18696 |  0:05:49s
epoch 17 | loss: 0.92103 | eval_custom_logloss: 2.99923 |  0:06:10s
epoch 18 | loss: 0.90028 | eval_custom_logloss: 2.36098 |  0:06:30s
epoch 19 | loss: 0.88684 | eval_custom_logloss: 1.37507 |  0:06:50s
epoch 20 | loss: 0.89745 | eval_custom_logloss: 3.7222  |  0:07:11s
epoch 21 | loss: 0.86698 | eval_custom_logloss: 2.84939 |  0:07:32s
epoch 22 | loss: 0.86827 | eval_custom_logloss: 1.87231 |  0:07:53s
epoch 23 | loss: 0.87711 | eval_custom_logloss: 1.68041 |  0:08:15s
epoch 24 | loss: 0.87507 | eval_custom_logloss: 3.35966 |  0:08:37s
epoch 25 | loss: 0.86562 | eval_custom_logloss: 2.72124 |  0:08:59s
epoch 26 | loss: 0.85301 | eval_custom_logloss: 1.40669 |  0:09:21s
epoch 27 | loss: 0.87516 | eval_custom_logloss: 1.31384 |  0:09:42s
epoch 28 | loss: 0.9002  | eval_custom_logloss: 2.00445 |  0:10:03s
epoch 29 | loss: 0.86812 | eval_custom_logloss: 1.21116 |  0:10:23s
epoch 30 | loss: 0.85496 | eval_custom_logloss: 1.31363 |  0:10:44s
epoch 31 | loss: 0.84572 | eval_custom_logloss: 1.11827 |  0:11:04s
epoch 32 | loss: 0.85519 | eval_custom_logloss: 1.45298 |  0:11:25s
epoch 33 | loss: 0.84371 | eval_custom_logloss: 0.94365 |  0:11:45s
epoch 34 | loss: 0.83203 | eval_custom_logloss: 0.96324 |  0:12:06s
epoch 35 | loss: 0.83461 | eval_custom_logloss: 1.1593  |  0:12:26s
epoch 36 | loss: 0.84421 | eval_custom_logloss: 0.97317 |  0:12:47s
epoch 37 | loss: 0.83544 | eval_custom_logloss: 1.23939 |  0:13:07s
epoch 38 | loss: 0.84056 | eval_custom_logloss: 1.24277 |  0:13:28s
epoch 39 | loss: 0.83264 | eval_custom_logloss: 0.85879 |  0:13:48s
epoch 40 | loss: 0.83346 | eval_custom_logloss: 2.45606 |  0:14:09s
epoch 41 | loss: 0.83159 | eval_custom_logloss: 0.93955 |  0:14:29s
epoch 42 | loss: 0.82819 | eval_custom_logloss: 1.20774 |  0:14:50s
epoch 43 | loss: 0.82474 | eval_custom_logloss: 1.44808 |  0:15:10s
epoch 44 | loss: 0.8315  | eval_custom_logloss: 1.60487 |  0:15:31s
epoch 45 | loss: 0.82075 | eval_custom_logloss: 0.95733 |  0:15:51s
epoch 46 | loss: 0.88183 | eval_custom_logloss: 1.33062 |  0:16:12s
epoch 47 | loss: 0.82304 | eval_custom_logloss: 1.45333 |  0:16:33s
epoch 48 | loss: 0.8249  | eval_custom_logloss: 1.80626 |  0:16:53s
epoch 49 | loss: 0.83699 | eval_custom_logloss: 1.56255 |  0:17:14s
epoch 50 | loss: 0.82717 | eval_custom_logloss: 1.23615 |  0:17:34s
epoch 51 | loss: 0.82343 | eval_custom_logloss: 1.44852 |  0:17:55s
epoch 52 | loss: 0.82922 | eval_custom_logloss: 1.01446 |  0:18:15s
epoch 53 | loss: 0.82402 | eval_custom_logloss: 1.32477 |  0:18:36s
epoch 54 | loss: 0.81475 | eval_custom_logloss: 0.88466 |  0:18:56s
epoch 55 | loss: 0.81871 | eval_custom_logloss: 0.98964 |  0:19:17s
epoch 56 | loss: 0.82015 | eval_custom_logloss: 1.55767 |  0:19:37s
epoch 57 | loss: 0.93548 | eval_custom_logloss: 1.55178 |  0:19:58s
epoch 58 | loss: 1.08836 | eval_custom_logloss: 1.16244 |  0:20:19s
epoch 59 | loss: 1.09272 | eval_custom_logloss: 1.30151 |  0:20:39s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.85879
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.03325, 'Log Loss - std': 0.17565000000000003} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.7114  | eval_custom_logloss: 11.16883|  0:00:20s
epoch 1  | loss: 1.28344 | eval_custom_logloss: 11.06744|  0:00:41s
epoch 2  | loss: 1.10651 | eval_custom_logloss: 8.46984 |  0:01:01s
epoch 3  | loss: 1.06973 | eval_custom_logloss: 7.18351 |  0:01:22s
epoch 4  | loss: 1.04891 | eval_custom_logloss: 4.60675 |  0:01:42s
epoch 5  | loss: 1.03616 | eval_custom_logloss: 3.46483 |  0:02:03s
epoch 6  | loss: 1.02277 | eval_custom_logloss: 3.33182 |  0:02:24s
epoch 7  | loss: 1.03184 | eval_custom_logloss: 4.09719 |  0:02:44s
epoch 8  | loss: 0.98533 | eval_custom_logloss: 4.16265 |  0:03:05s
epoch 9  | loss: 1.00185 | eval_custom_logloss: 3.82146 |  0:03:26s
epoch 10 | loss: 0.97746 | eval_custom_logloss: 2.90934 |  0:03:47s
epoch 11 | loss: 0.96428 | eval_custom_logloss: 3.95966 |  0:04:08s
epoch 12 | loss: 0.95061 | eval_custom_logloss: 2.96107 |  0:04:28s
epoch 13 | loss: 0.96175 | eval_custom_logloss: 3.89992 |  0:04:49s
epoch 14 | loss: 0.94163 | eval_custom_logloss: 2.99764 |  0:05:10s
epoch 15 | loss: 0.93138 | eval_custom_logloss: 2.9366  |  0:05:31s
epoch 16 | loss: 0.94673 | eval_custom_logloss: 2.35078 |  0:05:52s
epoch 17 | loss: 0.93003 | eval_custom_logloss: 2.36725 |  0:06:12s
epoch 18 | loss: 0.90937 | eval_custom_logloss: 1.71909 |  0:06:33s
epoch 19 | loss: 0.90822 | eval_custom_logloss: 1.87135 |  0:06:54s
epoch 20 | loss: 0.93393 | eval_custom_logloss: 1.55136 |  0:07:14s
epoch 21 | loss: 0.89881 | eval_custom_logloss: 1.497   |  0:07:35s
epoch 22 | loss: 0.8967  | eval_custom_logloss: 1.6351  |  0:07:56s
epoch 23 | loss: 0.9222  | eval_custom_logloss: 1.83575 |  0:08:17s
epoch 24 | loss: 0.8886  | eval_custom_logloss: 1.27888 |  0:08:37s
epoch 25 | loss: 0.88194 | eval_custom_logloss: 1.34663 |  0:08:58s
epoch 26 | loss: 0.88002 | eval_custom_logloss: 2.91772 |  0:09:19s
epoch 27 | loss: 0.8852  | eval_custom_logloss: 1.83009 |  0:09:39s
epoch 28 | loss: 0.8659  | eval_custom_logloss: 2.08942 |  0:10:00s
epoch 29 | loss: 0.86416 | eval_custom_logloss: 1.48695 |  0:10:21s
epoch 30 | loss: 0.86205 | eval_custom_logloss: 1.07511 |  0:10:41s
epoch 31 | loss: 0.84238 | eval_custom_logloss: 1.60292 |  0:11:02s
epoch 32 | loss: 0.86478 | eval_custom_logloss: 2.03587 |  0:11:22s
epoch 33 | loss: 0.85597 | eval_custom_logloss: 1.23584 |  0:11:43s
epoch 34 | loss: 0.84513 | eval_custom_logloss: 1.24349 |  0:12:04s
epoch 35 | loss: 0.89389 | eval_custom_logloss: 1.15379 |  0:12:24s
epoch 36 | loss: 0.87052 | eval_custom_logloss: 1.29703 |  0:12:45s
epoch 37 | loss: 0.85142 | eval_custom_logloss: 1.38861 |  0:13:05s
epoch 38 | loss: 0.85253 | eval_custom_logloss: 1.41057 |  0:13:26s
epoch 39 | loss: 0.86413 | eval_custom_logloss: 1.57521 |  0:13:47s
epoch 40 | loss: 0.84064 | eval_custom_logloss: 1.31537 |  0:14:07s
epoch 41 | loss: 0.82319 | eval_custom_logloss: 1.21275 |  0:14:28s
epoch 42 | loss: 0.82665 | eval_custom_logloss: 0.96544 |  0:14:49s
epoch 43 | loss: 0.82762 | eval_custom_logloss: 1.15553 |  0:15:09s
epoch 44 | loss: 0.82724 | eval_custom_logloss: 1.43242 |  0:15:30s
epoch 45 | loss: 0.80957 | eval_custom_logloss: 0.95342 |  0:15:51s
epoch 46 | loss: 0.82513 | eval_custom_logloss: 1.18411 |  0:16:12s
epoch 47 | loss: 0.814   | eval_custom_logloss: 1.45178 |  0:16:32s
epoch 48 | loss: 0.81575 | eval_custom_logloss: 1.09172 |  0:16:53s
epoch 49 | loss: 0.83424 | eval_custom_logloss: 1.19101 |  0:17:14s
epoch 50 | loss: 0.82949 | eval_custom_logloss: 1.16737 |  0:17:35s
epoch 51 | loss: 0.81651 | eval_custom_logloss: 1.18945 |  0:17:56s
epoch 52 | loss: 0.81349 | eval_custom_logloss: 2.29835 |  0:18:17s
epoch 53 | loss: 0.82098 | eval_custom_logloss: 2.41836 |  0:18:38s
epoch 54 | loss: 0.81221 | eval_custom_logloss: 1.65018 |  0:18:59s
epoch 55 | loss: 0.81332 | eval_custom_logloss: 1.11403 |  0:19:19s
epoch 56 | loss: 0.80589 | eval_custom_logloss: 0.93624 |  0:19:40s
epoch 57 | loss: 0.81492 | eval_custom_logloss: 1.33923 |  0:20:01s
epoch 58 | loss: 0.80868 | eval_custom_logloss: 0.95803 |  0:20:21s
epoch 59 | loss: 0.81654 | eval_custom_logloss: 0.99147 |  0:20:42s
epoch 60 | loss: 0.80717 | eval_custom_logloss: 1.18077 |  0:21:02s
epoch 61 | loss: 0.81458 | eval_custom_logloss: 0.93552 |  0:21:23s
epoch 62 | loss: 0.80968 | eval_custom_logloss: 1.31573 |  0:21:44s
epoch 63 | loss: 0.81373 | eval_custom_logloss: 1.43873 |  0:22:04s
epoch 64 | loss: 0.79321 | eval_custom_logloss: 1.32778 |  0:22:24s
epoch 65 | loss: 0.80017 | eval_custom_logloss: 2.20892 |  0:22:45s
epoch 66 | loss: 0.80205 | eval_custom_logloss: 2.18361 |  0:23:06s
epoch 67 | loss: 0.80918 | eval_custom_logloss: 2.61008 |  0:23:27s
epoch 68 | loss: 0.80165 | eval_custom_logloss: 1.51824 |  0:23:47s
epoch 69 | loss: 0.78281 | eval_custom_logloss: 0.84793 |  0:24:08s
epoch 70 | loss: 0.77571 | eval_custom_logloss: 1.03221 |  0:24:28s
epoch 71 | loss: 0.77426 | eval_custom_logloss: 1.25963 |  0:24:49s
epoch 72 | loss: 0.77545 | eval_custom_logloss: 1.35136 |  0:25:10s
epoch 73 | loss: 0.79332 | eval_custom_logloss: 1.35467 |  0:25:30s
epoch 74 | loss: 0.78529 | eval_custom_logloss: 1.10263 |  0:25:51s
epoch 75 | loss: 0.8094  | eval_custom_logloss: 0.88708 |  0:26:12s
epoch 76 | loss: 0.79394 | eval_custom_logloss: 1.83513 |  0:26:33s
epoch 77 | loss: 0.79642 | eval_custom_logloss: 0.90209 |  0:26:53s
epoch 78 | loss: 0.79265 | eval_custom_logloss: 1.6371  |  0:27:14s
epoch 79 | loss: 0.78699 | eval_custom_logloss: 0.93035 |  0:27:35s
epoch 80 | loss: 0.77955 | eval_custom_logloss: 1.1147  |  0:27:55s
epoch 81 | loss: 0.78039 | eval_custom_logloss: 1.43565 |  0:28:16s
epoch 82 | loss: 0.78345 | eval_custom_logloss: 0.81422 |  0:28:37s
epoch 83 | loss: 0.80158 | eval_custom_logloss: 1.51963 |  0:28:58s
epoch 84 | loss: 0.79112 | eval_custom_logloss: 0.89446 |  0:29:19s
epoch 85 | loss: 0.80316 | eval_custom_logloss: 1.29943 |  0:29:39s
epoch 86 | loss: 0.77325 | eval_custom_logloss: 0.79149 |  0:30:00s
epoch 87 | loss: 0.77953 | eval_custom_logloss: 0.86403 |  0:30:21s
epoch 88 | loss: 0.79203 | eval_custom_logloss: 1.69217 |  0:30:42s
epoch 89 | loss: 0.7894  | eval_custom_logloss: 0.87497 |  0:31:03s
epoch 90 | loss: 0.77772 | eval_custom_logloss: 0.94672 |  0:31:24s
epoch 91 | loss: 0.77495 | eval_custom_logloss: 1.06719 |  0:31:45s
epoch 92 | loss: 0.77815 | eval_custom_logloss: 0.86462 |  0:32:06s
epoch 93 | loss: 0.77552 | eval_custom_logloss: 1.15816 |  0:32:26s
epoch 94 | loss: 0.78599 | eval_custom_logloss: 1.52099 |  0:32:47s
epoch 95 | loss: 0.771   | eval_custom_logloss: 1.07403 |  0:33:08s
epoch 96 | loss: 0.77117 | eval_custom_logloss: 1.10598 |  0:33:29s
epoch 97 | loss: 0.77353 | eval_custom_logloss: 1.0254  |  0:33:50s
epoch 98 | loss: 0.79622 | eval_custom_logloss: 1.4949  |  0:34:10s
epoch 99 | loss: 0.78244 | eval_custom_logloss: 0.99832 |  0:34:31s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.79149
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9522666666666666, 'Log Loss - std': 0.18353532508908352} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66331 | eval_custom_logloss: 13.34506|  0:00:20s
epoch 1  | loss: 1.28046 | eval_custom_logloss: 10.19531|  0:00:41s
epoch 2  | loss: 1.17176 | eval_custom_logloss: 11.27524|  0:01:02s
epoch 3  | loss: 1.10707 | eval_custom_logloss: 11.87561|  0:01:22s
epoch 4  | loss: 1.03172 | eval_custom_logloss: 10.64838|  0:01:43s
epoch 5  | loss: 1.03071 | eval_custom_logloss: 10.40162|  0:02:04s
epoch 6  | loss: 1.00461 | eval_custom_logloss: 9.71221 |  0:02:25s
epoch 7  | loss: 1.01734 | eval_custom_logloss: 8.98078 |  0:02:46s
epoch 8  | loss: 0.97581 | eval_custom_logloss: 8.4913  |  0:03:06s
epoch 9  | loss: 0.95731 | eval_custom_logloss: 8.58323 |  0:03:27s
epoch 10 | loss: 0.96959 | eval_custom_logloss: 7.34837 |  0:03:48s
epoch 11 | loss: 0.94391 | eval_custom_logloss: 6.94299 |  0:04:09s
epoch 12 | loss: 0.94558 | eval_custom_logloss: 5.95545 |  0:04:29s
epoch 13 | loss: 0.94788 | eval_custom_logloss: 5.23264 |  0:04:50s
epoch 14 | loss: 0.93538 | eval_custom_logloss: 5.08622 |  0:05:11s
epoch 15 | loss: 0.90876 | eval_custom_logloss: 6.02953 |  0:05:31s
epoch 16 | loss: 0.91027 | eval_custom_logloss: 4.93271 |  0:05:52s
epoch 17 | loss: 0.91535 | eval_custom_logloss: 2.15943 |  0:06:13s
epoch 18 | loss: 0.89316 | eval_custom_logloss: 2.70295 |  0:06:33s
epoch 19 | loss: 0.87992 | eval_custom_logloss: 3.45985 |  0:06:54s
epoch 20 | loss: 0.90045 | eval_custom_logloss: 3.96241 |  0:07:14s
epoch 21 | loss: 0.85876 | eval_custom_logloss: 5.0241  |  0:07:35s
epoch 22 | loss: 0.86343 | eval_custom_logloss: 4.09645 |  0:07:56s
epoch 23 | loss: 0.87743 | eval_custom_logloss: 2.36759 |  0:08:17s
epoch 24 | loss: 0.85271 | eval_custom_logloss: 3.26694 |  0:08:37s
epoch 25 | loss: 0.83581 | eval_custom_logloss: 2.71429 |  0:08:58s
epoch 26 | loss: 0.8421  | eval_custom_logloss: 2.69732 |  0:09:19s
epoch 27 | loss: 0.84519 | eval_custom_logloss: 2.44321 |  0:09:39s
epoch 28 | loss: 0.84383 | eval_custom_logloss: 1.78197 |  0:10:00s
epoch 29 | loss: 0.81222 | eval_custom_logloss: 1.42273 |  0:10:21s
epoch 30 | loss: 0.83936 | eval_custom_logloss: 2.94018 |  0:10:42s
epoch 31 | loss: 0.80764 | eval_custom_logloss: 2.48436 |  0:11:02s
epoch 32 | loss: 0.81538 | eval_custom_logloss: 1.42761 |  0:11:23s
epoch 33 | loss: 0.81601 | eval_custom_logloss: 1.98875 |  0:11:43s
epoch 34 | loss: 0.79662 | eval_custom_logloss: 1.48751 |  0:12:04s
epoch 35 | loss: 0.79165 | eval_custom_logloss: 1.97779 |  0:12:24s
epoch 36 | loss: 0.78752 | eval_custom_logloss: 1.29088 |  0:12:45s
epoch 37 | loss: 0.77696 | eval_custom_logloss: 1.52353 |  0:13:05s
epoch 38 | loss: 0.79675 | eval_custom_logloss: 2.1272  |  0:13:26s
epoch 39 | loss: 0.78192 | eval_custom_logloss: 2.05974 |  0:13:47s
epoch 40 | loss: 0.77194 | eval_custom_logloss: 1.46874 |  0:14:07s
epoch 41 | loss: 0.78171 | eval_custom_logloss: 1.69983 |  0:14:28s
epoch 42 | loss: 0.78281 | eval_custom_logloss: 1.12401 |  0:14:48s
epoch 43 | loss: 0.77618 | eval_custom_logloss: 2.02028 |  0:15:09s
epoch 44 | loss: 0.78213 | eval_custom_logloss: 1.50159 |  0:15:29s
epoch 45 | loss: 0.76784 | eval_custom_logloss: 1.11971 |  0:15:50s
epoch 46 | loss: 0.77807 | eval_custom_logloss: 1.24958 |  0:16:10s
epoch 47 | loss: 0.76642 | eval_custom_logloss: 1.18974 |  0:16:31s
epoch 48 | loss: 0.77104 | eval_custom_logloss: 0.93458 |  0:16:52s
epoch 49 | loss: 0.75986 | eval_custom_logloss: 0.9651  |  0:17:12s
epoch 50 | loss: 0.77671 | eval_custom_logloss: 2.10335 |  0:17:33s
epoch 51 | loss: 0.79247 | eval_custom_logloss: 0.81006 |  0:17:53s
epoch 52 | loss: 0.77786 | eval_custom_logloss: 1.40943 |  0:18:14s
epoch 53 | loss: 0.77293 | eval_custom_logloss: 1.15788 |  0:18:34s
epoch 54 | loss: 0.75034 | eval_custom_logloss: 1.59631 |  0:18:55s
epoch 55 | loss: 0.76509 | eval_custom_logloss: 1.45398 |  0:19:15s
epoch 56 | loss: 0.72799 | eval_custom_logloss: 1.23439 |  0:19:36s
epoch 57 | loss: 0.74014 | eval_custom_logloss: 1.526   |  0:19:56s
epoch 58 | loss: 0.73364 | eval_custom_logloss: 1.67658 |  0:20:17s
epoch 59 | loss: 0.75601 | eval_custom_logloss: 1.05061 |  0:20:37s
epoch 60 | loss: 0.75128 | eval_custom_logloss: 1.37173 |  0:20:58s
epoch 61 | loss: 0.74494 | eval_custom_logloss: 1.33809 |  0:21:18s
epoch 62 | loss: 0.7267  | eval_custom_logloss: 1.18931 |  0:21:38s
epoch 63 | loss: 0.73043 | eval_custom_logloss: 1.04902 |  0:21:59s
epoch 64 | loss: 0.71876 | eval_custom_logloss: 1.17729 |  0:22:19s
epoch 65 | loss: 0.72968 | eval_custom_logloss: 1.02159 |  0:22:40s
epoch 66 | loss: 0.74235 | eval_custom_logloss: 1.1798  |  0:23:01s
epoch 67 | loss: 0.73898 | eval_custom_logloss: 1.57936 |  0:23:21s
epoch 68 | loss: 0.72063 | eval_custom_logloss: 0.9099  |  0:23:42s
epoch 69 | loss: 0.71504 | eval_custom_logloss: 1.23033 |  0:24:03s
epoch 70 | loss: 0.71866 | eval_custom_logloss: 1.34412 |  0:24:23s
epoch 71 | loss: 0.71671 | eval_custom_logloss: 1.17793 |  0:24:44s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 0.81006
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9160999999999999, 'Log Loss - std': 0.1708449443208666} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65651 | eval_custom_logloss: 11.34531|  0:00:20s
epoch 1  | loss: 1.3423  | eval_custom_logloss: 11.61013|  0:00:40s
epoch 2  | loss: 1.17735 | eval_custom_logloss: 12.98806|  0:01:01s
epoch 3  | loss: 1.09784 | eval_custom_logloss: 11.86326|  0:01:21s
epoch 4  | loss: 1.0696  | eval_custom_logloss: 10.53116|  0:01:42s
epoch 5  | loss: 1.02932 | eval_custom_logloss: 9.1326  |  0:02:02s
epoch 6  | loss: 1.00981 | eval_custom_logloss: 7.66523 |  0:02:23s
epoch 7  | loss: 0.97637 | eval_custom_logloss: 7.28658 |  0:02:44s
epoch 8  | loss: 0.96059 | eval_custom_logloss: 7.58622 |  0:03:05s
epoch 9  | loss: 0.9606  | eval_custom_logloss: 6.44629 |  0:03:26s
epoch 10 | loss: 0.95582 | eval_custom_logloss: 5.02788 |  0:03:47s
epoch 11 | loss: 0.93742 | eval_custom_logloss: 4.96128 |  0:04:07s
epoch 12 | loss: 0.93009 | eval_custom_logloss: 4.90617 |  0:04:28s
epoch 13 | loss: 0.93415 | eval_custom_logloss: 3.97035 |  0:04:48s
epoch 14 | loss: 0.92684 | eval_custom_logloss: 4.04913 |  0:05:09s
epoch 15 | loss: 0.91269 | eval_custom_logloss: 5.33447 |  0:05:29s
epoch 16 | loss: 0.91988 | eval_custom_logloss: 3.37418 |  0:05:50s
epoch 17 | loss: 0.91357 | eval_custom_logloss: 2.6145  |  0:06:11s
epoch 18 | loss: 0.90284 | eval_custom_logloss: 2.34052 |  0:06:31s
epoch 19 | loss: 0.89892 | eval_custom_logloss: 2.52388 |  0:06:52s
epoch 20 | loss: 0.90569 | eval_custom_logloss: 3.33652 |  0:07:12s
epoch 21 | loss: 0.87649 | eval_custom_logloss: 2.13556 |  0:07:33s
epoch 22 | loss: 0.87718 | eval_custom_logloss: 2.41656 |  0:07:53s
epoch 23 | loss: 0.89348 | eval_custom_logloss: 1.54495 |  0:08:14s
epoch 24 | loss: 0.86093 | eval_custom_logloss: 2.08737 |  0:08:35s
epoch 25 | loss: 0.86885 | eval_custom_logloss: 2.74452 |  0:08:55s
epoch 26 | loss: 0.87073 | eval_custom_logloss: 3.07079 |  0:09:16s
epoch 27 | loss: 0.84946 | eval_custom_logloss: 4.53466 |  0:09:37s
epoch 28 | loss: 0.86228 | eval_custom_logloss: 2.691   |  0:09:57s
epoch 29 | loss: 0.83703 | eval_custom_logloss: 3.43652 |  0:10:18s
epoch 30 | loss: 0.84238 | eval_custom_logloss: 1.63222 |  0:10:38s
epoch 31 | loss: 0.83782 | eval_custom_logloss: 1.13551 |  0:10:59s
epoch 32 | loss: 0.82677 | eval_custom_logloss: 3.87556 |  0:11:19s
epoch 33 | loss: 0.90199 | eval_custom_logloss: 2.11694 |  0:11:40s
epoch 34 | loss: 0.83813 | eval_custom_logloss: 2.65111 |  0:12:00s
epoch 35 | loss: 0.83462 | eval_custom_logloss: 2.00105 |  0:12:21s
epoch 36 | loss: 0.83782 | eval_custom_logloss: 1.99207 |  0:12:42s
epoch 37 | loss: 0.82465 | eval_custom_logloss: 1.54708 |  0:13:03s
epoch 38 | loss: 0.84243 | eval_custom_logloss: 1.53142 |  0:13:23s
epoch 39 | loss: 0.81906 | eval_custom_logloss: 0.98315 |  0:13:44s
epoch 40 | loss: 0.8148  | eval_custom_logloss: 1.31896 |  0:14:05s
epoch 41 | loss: 0.81663 | eval_custom_logloss: 1.25196 |  0:14:25s
epoch 42 | loss: 0.81521 | eval_custom_logloss: 1.25695 |  0:14:46s
epoch 43 | loss: 0.82293 | eval_custom_logloss: 2.24758 |  0:15:06s
epoch 44 | loss: 0.82056 | eval_custom_logloss: 2.09074 |  0:15:27s
epoch 45 | loss: 0.80628 | eval_custom_logloss: 3.93966 |  0:15:48s
epoch 46 | loss: 0.81849 | eval_custom_logloss: 1.30196 |  0:16:09s
epoch 47 | loss: 0.8091  | eval_custom_logloss: 1.09142 |  0:16:29s
epoch 48 | loss: 0.80331 | eval_custom_logloss: 0.92651 |  0:16:50s
epoch 49 | loss: 0.8027  | eval_custom_logloss: 0.79383 |  0:17:10s
epoch 50 | loss: 0.78849 | eval_custom_logloss: 0.98714 |  0:17:31s
epoch 51 | loss: 0.7953  | eval_custom_logloss: 1.27322 |  0:17:52s
epoch 52 | loss: 0.81059 | eval_custom_logloss: 1.23354 |  0:18:12s
epoch 53 | loss: 0.80123 | eval_custom_logloss: 2.72831 |  0:18:33s
epoch 54 | loss: 0.78065 | eval_custom_logloss: 1.93101 |  0:18:54s
epoch 55 | loss: 0.81381 | eval_custom_logloss: 0.7772  |  0:19:14s
epoch 56 | loss: 0.78272 | eval_custom_logloss: 1.42587 |  0:19:35s
epoch 57 | loss: 0.78158 | eval_custom_logloss: 1.18846 |  0:19:56s
epoch 58 | loss: 0.79906 | eval_custom_logloss: 1.04727 |  0:20:16s
epoch 59 | loss: 0.78004 | eval_custom_logloss: 3.02555 |  0:20:37s
epoch 60 | loss: 0.77034 | eval_custom_logloss: 0.81148 |  0:20:58s
epoch 61 | loss: 0.7787  | eval_custom_logloss: 1.17016 |  0:21:18s
epoch 62 | loss: 0.78444 | eval_custom_logloss: 1.11457 |  0:21:39s
epoch 63 | loss: 0.77016 | eval_custom_logloss: 0.90144 |  0:22:00s
epoch 64 | loss: 0.77521 | eval_custom_logloss: 1.69656 |  0:22:20s
epoch 65 | loss: 0.77075 | eval_custom_logloss: 1.20605 |  0:22:41s
epoch 66 | loss: 0.78468 | eval_custom_logloss: 1.37589 |  0:23:02s
epoch 67 | loss: 0.78496 | eval_custom_logloss: 1.56889 |  0:23:22s
epoch 68 | loss: 0.78981 | eval_custom_logloss: 1.06693 |  0:23:43s
epoch 69 | loss: 0.77719 | eval_custom_logloss: 0.81141 |  0:24:04s
epoch 70 | loss: 0.83383 | eval_custom_logloss: 2.107   |  0:24:24s
epoch 71 | loss: 0.80166 | eval_custom_logloss: 1.88425 |  0:24:45s
epoch 72 | loss: 0.80137 | eval_custom_logloss: 1.75227 |  0:25:06s
epoch 73 | loss: 0.80082 | eval_custom_logloss: 1.23787 |  0:25:27s
epoch 74 | loss: 0.78523 | eval_custom_logloss: 2.40558 |  0:25:47s
epoch 75 | loss: 0.78379 | eval_custom_logloss: 1.46766 |  0:26:08s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.7772
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8880799999999999, 'Log Loss - std': 0.1627601843203675} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 0.8880799999999999 and parameters: {'n_d': 8, 'n_steps': 7, 'gamma': 1.1367554225728589, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010038774033580649, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.53379 | eval_custom_logloss: 10.9849 |  0:00:15s
epoch 1  | loss: 1.1838  | eval_custom_logloss: 9.59028 |  0:00:30s
epoch 2  | loss: 1.06389 | eval_custom_logloss: 5.78393 |  0:00:45s
epoch 3  | loss: 1.03384 | eval_custom_logloss: 5.97967 |  0:01:00s
epoch 4  | loss: 0.99455 | eval_custom_logloss: 6.76017 |  0:01:15s
epoch 5  | loss: 0.97404 | eval_custom_logloss: 5.39211 |  0:01:30s
epoch 6  | loss: 0.98544 | eval_custom_logloss: 5.42061 |  0:01:45s
epoch 7  | loss: 0.95631 | eval_custom_logloss: 4.42836 |  0:02:01s
epoch 8  | loss: 0.95215 | eval_custom_logloss: 4.08492 |  0:02:16s
epoch 9  | loss: 0.91823 | eval_custom_logloss: 2.93562 |  0:02:31s
epoch 10 | loss: 0.91786 | eval_custom_logloss: 2.80931 |  0:02:46s
epoch 11 | loss: 0.92276 | eval_custom_logloss: 3.6096  |  0:03:01s
epoch 12 | loss: 0.90106 | eval_custom_logloss: 3.13115 |  0:03:16s
epoch 13 | loss: 0.89705 | eval_custom_logloss: 1.89991 |  0:03:31s
epoch 14 | loss: 0.8884  | eval_custom_logloss: 2.97762 |  0:03:46s
epoch 15 | loss: 0.86872 | eval_custom_logloss: 2.90345 |  0:04:01s
epoch 16 | loss: 0.89054 | eval_custom_logloss: 2.08965 |  0:04:16s
epoch 17 | loss: 0.86841 | eval_custom_logloss: 1.35762 |  0:04:32s
epoch 18 | loss: 0.84725 | eval_custom_logloss: 2.04107 |  0:04:47s
epoch 19 | loss: 0.84917 | eval_custom_logloss: 1.4488  |  0:05:02s
epoch 20 | loss: 0.84977 | eval_custom_logloss: 1.26801 |  0:05:17s
epoch 21 | loss: 0.85278 | eval_custom_logloss: 1.45916 |  0:05:32s
epoch 22 | loss: 0.84359 | eval_custom_logloss: 1.30826 |  0:05:47s
epoch 23 | loss: 0.83238 | eval_custom_logloss: 1.26574 |  0:06:02s
epoch 24 | loss: 0.82879 | eval_custom_logloss: 0.92728 |  0:06:17s
epoch 25 | loss: 0.84612 | eval_custom_logloss: 1.18603 |  0:06:32s
epoch 26 | loss: 0.81535 | eval_custom_logloss: 1.09722 |  0:06:48s
epoch 27 | loss: 0.81251 | eval_custom_logloss: 1.09137 |  0:07:03s
epoch 28 | loss: 0.82625 | eval_custom_logloss: 1.42668 |  0:07:18s
epoch 29 | loss: 0.82722 | eval_custom_logloss: 1.37129 |  0:07:33s
epoch 30 | loss: 0.81516 | eval_custom_logloss: 0.83198 |  0:07:48s
epoch 31 | loss: 0.81173 | eval_custom_logloss: 0.95155 |  0:08:03s
epoch 32 | loss: 0.79981 | eval_custom_logloss: 0.97552 |  0:08:18s
epoch 33 | loss: 0.80999 | eval_custom_logloss: 1.67659 |  0:08:33s
epoch 34 | loss: 0.80028 | eval_custom_logloss: 0.90204 |  0:08:48s
epoch 35 | loss: 0.83396 | eval_custom_logloss: 1.43415 |  0:09:03s
epoch 36 | loss: 0.84848 | eval_custom_logloss: 0.98103 |  0:09:19s
epoch 37 | loss: 0.82771 | eval_custom_logloss: 1.07563 |  0:09:34s
epoch 38 | loss: 0.8119  | eval_custom_logloss: 0.80543 |  0:09:49s
epoch 39 | loss: 0.80796 | eval_custom_logloss: 1.05495 |  0:10:04s
epoch 40 | loss: 0.83285 | eval_custom_logloss: 0.85525 |  0:10:19s
epoch 41 | loss: 0.80358 | eval_custom_logloss: 0.98822 |  0:10:34s
epoch 42 | loss: 0.80376 | eval_custom_logloss: 0.98382 |  0:10:49s
epoch 43 | loss: 0.77699 | eval_custom_logloss: 0.98482 |  0:11:04s
epoch 44 | loss: 0.76251 | eval_custom_logloss: 0.82036 |  0:11:19s
epoch 45 | loss: 0.76746 | eval_custom_logloss: 0.91275 |  0:11:34s
epoch 46 | loss: 0.74749 | eval_custom_logloss: 0.82477 |  0:11:49s
epoch 47 | loss: 0.73841 | eval_custom_logloss: 1.06831 |  0:12:04s
epoch 48 | loss: 0.75407 | eval_custom_logloss: 1.02421 |  0:12:19s
epoch 49 | loss: 0.73953 | eval_custom_logloss: 0.84611 |  0:12:35s
epoch 50 | loss: 0.73831 | eval_custom_logloss: 0.94756 |  0:12:50s
epoch 51 | loss: 0.73134 | eval_custom_logloss: 0.88671 |  0:13:05s
epoch 52 | loss: 0.72676 | eval_custom_logloss: 0.96496 |  0:13:19s
epoch 53 | loss: 0.73116 | eval_custom_logloss: 0.89626 |  0:13:34s
epoch 54 | loss: 0.72579 | eval_custom_logloss: 0.87726 |  0:13:49s
epoch 55 | loss: 0.73268 | eval_custom_logloss: 0.88753 |  0:14:04s
epoch 56 | loss: 0.70735 | eval_custom_logloss: 1.15485 |  0:14:19s
epoch 57 | loss: 0.72735 | eval_custom_logloss: 0.68327 |  0:14:34s
epoch 58 | loss: 0.71052 | eval_custom_logloss: 1.19402 |  0:14:49s
epoch 59 | loss: 0.70586 | eval_custom_logloss: 0.99933 |  0:15:04s
epoch 60 | loss: 0.69599 | eval_custom_logloss: 1.00035 |  0:15:19s
epoch 61 | loss: 0.70253 | eval_custom_logloss: 0.75559 |  0:15:34s
epoch 62 | loss: 0.6998  | eval_custom_logloss: 0.8337  |  0:15:49s
epoch 63 | loss: 0.69449 | eval_custom_logloss: 0.83958 |  0:16:04s
epoch 64 | loss: 0.68406 | eval_custom_logloss: 1.12741 |  0:16:19s
epoch 65 | loss: 0.69613 | eval_custom_logloss: 0.76717 |  0:16:34s
epoch 66 | loss: 0.68171 | eval_custom_logloss: 0.80059 |  0:16:49s
epoch 67 | loss: 0.69054 | eval_custom_logloss: 0.70759 |  0:17:05s
epoch 68 | loss: 0.68042 | eval_custom_logloss: 0.98708 |  0:17:20s
epoch 69 | loss: 0.67777 | eval_custom_logloss: 0.77404 |  0:17:35s
epoch 70 | loss: 0.67846 | eval_custom_logloss: 0.67803 |  0:17:50s
epoch 71 | loss: 0.68477 | eval_custom_logloss: 0.74977 |  0:18:05s
epoch 72 | loss: 0.68623 | eval_custom_logloss: 0.72098 |  0:18:20s
epoch 73 | loss: 0.67698 | eval_custom_logloss: 0.92877 |  0:18:35s
epoch 74 | loss: 0.6821  | eval_custom_logloss: 0.68469 |  0:18:50s
epoch 75 | loss: 0.67532 | eval_custom_logloss: 0.88252 |  0:19:05s
epoch 76 | loss: 0.67343 | eval_custom_logloss: 0.69003 |  0:19:20s
epoch 77 | loss: 0.68434 | eval_custom_logloss: 0.8434  |  0:19:35s
epoch 78 | loss: 0.67658 | eval_custom_logloss: 1.27859 |  0:19:50s
epoch 79 | loss: 0.67167 | eval_custom_logloss: 0.64192 |  0:20:05s
epoch 80 | loss: 0.67046 | eval_custom_logloss: 1.04328 |  0:20:20s
epoch 81 | loss: 0.6852  | eval_custom_logloss: 0.73812 |  0:20:35s
epoch 82 | loss: 0.67177 | eval_custom_logloss: 1.80736 |  0:20:50s
epoch 83 | loss: 0.6823  | eval_custom_logloss: 0.96817 |  0:21:05s
epoch 84 | loss: 0.66039 | eval_custom_logloss: 0.76101 |  0:21:20s
epoch 85 | loss: 0.67574 | eval_custom_logloss: 0.88413 |  0:21:35s
epoch 86 | loss: 0.67879 | eval_custom_logloss: 0.8698  |  0:21:50s
epoch 87 | loss: 0.66299 | eval_custom_logloss: 0.90203 |  0:22:05s
epoch 88 | loss: 0.65492 | eval_custom_logloss: 0.87442 |  0:22:20s
epoch 89 | loss: 0.66973 | eval_custom_logloss: 0.76924 |  0:22:35s
epoch 90 | loss: 0.67067 | eval_custom_logloss: 0.72113 |  0:22:51s
epoch 91 | loss: 0.66645 | eval_custom_logloss: 0.73408 |  0:23:06s
epoch 92 | loss: 0.64753 | eval_custom_logloss: 1.06982 |  0:23:21s
epoch 93 | loss: 0.68629 | eval_custom_logloss: 0.69417 |  0:23:36s
epoch 94 | loss: 0.67917 | eval_custom_logloss: 0.83002 |  0:23:51s
epoch 95 | loss: 0.66369 | eval_custom_logloss: 0.7159  |  0:24:06s
epoch 96 | loss: 0.66518 | eval_custom_logloss: 0.77772 |  0:24:21s
epoch 97 | loss: 0.66415 | eval_custom_logloss: 1.25312 |  0:24:36s
epoch 98 | loss: 0.65668 | eval_custom_logloss: 0.87155 |  0:24:52s
epoch 99 | loss: 0.65097 | eval_custom_logloss: 0.81815 |  0:25:07s

Early stopping occurred at epoch 99 with best_epoch = 79 and best_eval_custom_logloss = 0.64192
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6404, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.49365 | eval_custom_logloss: 12.80818|  0:00:15s
epoch 1  | loss: 1.12981 | eval_custom_logloss: 8.53567 |  0:00:30s
epoch 2  | loss: 1.04954 | eval_custom_logloss: 8.20176 |  0:00:45s
epoch 3  | loss: 1.00388 | eval_custom_logloss: 6.05498 |  0:01:00s
epoch 4  | loss: 0.95436 | eval_custom_logloss: 5.29961 |  0:01:15s
epoch 5  | loss: 0.93922 | eval_custom_logloss: 4.70616 |  0:01:30s
epoch 6  | loss: 0.95577 | eval_custom_logloss: 6.01945 |  0:01:45s
epoch 7  | loss: 0.91555 | eval_custom_logloss: 4.91925 |  0:02:00s
epoch 8  | loss: 0.90185 | eval_custom_logloss: 4.20627 |  0:02:15s
epoch 9  | loss: 0.86633 | eval_custom_logloss: 2.77277 |  0:02:30s
epoch 10 | loss: 0.86545 | eval_custom_logloss: 2.47823 |  0:02:44s
epoch 11 | loss: 0.89812 | eval_custom_logloss: 3.7471  |  0:02:59s
epoch 12 | loss: 0.91636 | eval_custom_logloss: 2.08454 |  0:03:14s
epoch 13 | loss: 0.88526 | eval_custom_logloss: 1.96699 |  0:03:29s
epoch 14 | loss: 0.87532 | eval_custom_logloss: 1.65555 |  0:03:44s
epoch 15 | loss: 0.83333 | eval_custom_logloss: 2.24419 |  0:03:59s
epoch 16 | loss: 0.82286 | eval_custom_logloss: 1.3943  |  0:04:14s
epoch 17 | loss: 0.81532 | eval_custom_logloss: 1.82865 |  0:04:29s
epoch 18 | loss: 0.7903  | eval_custom_logloss: 2.22625 |  0:04:44s
epoch 19 | loss: 0.80471 | eval_custom_logloss: 2.03688 |  0:04:59s
epoch 20 | loss: 0.78447 | eval_custom_logloss: 1.04627 |  0:05:15s
epoch 21 | loss: 0.78905 | eval_custom_logloss: 1.02253 |  0:05:30s
epoch 22 | loss: 0.76712 | eval_custom_logloss: 0.99335 |  0:05:46s
epoch 23 | loss: 0.76343 | eval_custom_logloss: 1.07097 |  0:06:02s
epoch 24 | loss: 0.74206 | eval_custom_logloss: 0.93731 |  0:06:17s
epoch 25 | loss: 0.76179 | eval_custom_logloss: 1.21712 |  0:06:33s
epoch 26 | loss: 0.74279 | eval_custom_logloss: 1.43827 |  0:06:49s
epoch 27 | loss: 0.72652 | eval_custom_logloss: 1.00219 |  0:07:04s
epoch 28 | loss: 0.73087 | eval_custom_logloss: 1.04613 |  0:07:20s
epoch 29 | loss: 0.71193 | eval_custom_logloss: 1.28842 |  0:07:36s
epoch 30 | loss: 0.72191 | eval_custom_logloss: 1.56422 |  0:07:51s
epoch 31 | loss: 0.71025 | eval_custom_logloss: 0.92174 |  0:08:07s
epoch 32 | loss: 0.72578 | eval_custom_logloss: 1.25893 |  0:08:23s
epoch 33 | loss: 0.70302 | eval_custom_logloss: 0.9838  |  0:08:39s
epoch 34 | loss: 0.70974 | eval_custom_logloss: 1.06816 |  0:08:54s
epoch 35 | loss: 0.6906  | eval_custom_logloss: 0.80114 |  0:09:10s
epoch 36 | loss: 0.68923 | eval_custom_logloss: 0.75013 |  0:09:26s
epoch 37 | loss: 0.68792 | eval_custom_logloss: 1.01669 |  0:09:42s
epoch 38 | loss: 0.68942 | eval_custom_logloss: 1.02352 |  0:09:58s
epoch 39 | loss: 0.70008 | eval_custom_logloss: 1.08398 |  0:10:13s
epoch 40 | loss: 0.68431 | eval_custom_logloss: 1.0132  |  0:10:29s
epoch 41 | loss: 0.6826  | eval_custom_logloss: 0.84777 |  0:10:45s
epoch 42 | loss: 0.69426 | eval_custom_logloss: 0.86215 |  0:11:01s
epoch 43 | loss: 0.67942 | eval_custom_logloss: 0.9967  |  0:11:16s
epoch 44 | loss: 0.67194 | eval_custom_logloss: 0.91413 |  0:11:32s
epoch 45 | loss: 0.65923 | eval_custom_logloss: 0.9249  |  0:11:47s
epoch 46 | loss: 0.65659 | eval_custom_logloss: 1.37295 |  0:12:02s
epoch 47 | loss: 0.66412 | eval_custom_logloss: 0.73461 |  0:12:17s
epoch 48 | loss: 0.67801 | eval_custom_logloss: 0.88026 |  0:12:32s
epoch 49 | loss: 0.66147 | eval_custom_logloss: 0.90539 |  0:12:46s
epoch 50 | loss: 0.65488 | eval_custom_logloss: 0.76555 |  0:13:01s
epoch 51 | loss: 0.65467 | eval_custom_logloss: 0.99921 |  0:13:16s
epoch 52 | loss: 0.6618  | eval_custom_logloss: 0.72774 |  0:13:31s
epoch 53 | loss: 0.65396 | eval_custom_logloss: 0.96321 |  0:13:46s
epoch 54 | loss: 0.65939 | eval_custom_logloss: 0.9347  |  0:14:01s
epoch 55 | loss: 0.64872 | eval_custom_logloss: 0.97696 |  0:14:16s
epoch 56 | loss: 0.6383  | eval_custom_logloss: 0.8848  |  0:14:30s
epoch 57 | loss: 0.65559 | eval_custom_logloss: 0.70803 |  0:14:45s
epoch 58 | loss: 0.64466 | eval_custom_logloss: 0.81853 |  0:15:00s
epoch 59 | loss: 0.65924 | eval_custom_logloss: 0.88873 |  0:15:15s
epoch 60 | loss: 0.65171 | eval_custom_logloss: 0.64828 |  0:15:30s
epoch 61 | loss: 0.65014 | eval_custom_logloss: 0.72864 |  0:15:45s
epoch 62 | loss: 0.65833 | eval_custom_logloss: 1.35874 |  0:16:00s
epoch 63 | loss: 0.63261 | eval_custom_logloss: 0.86382 |  0:16:14s
epoch 64 | loss: 0.63152 | eval_custom_logloss: 1.04557 |  0:16:29s
epoch 65 | loss: 0.64262 | eval_custom_logloss: 0.80023 |  0:16:44s
epoch 66 | loss: 0.63339 | eval_custom_logloss: 1.04036 |  0:16:59s
epoch 67 | loss: 0.64382 | eval_custom_logloss: 0.85327 |  0:17:14s
epoch 68 | loss: 0.61824 | eval_custom_logloss: 0.85018 |  0:17:29s
epoch 69 | loss: 0.64378 | eval_custom_logloss: 0.89822 |  0:17:44s
epoch 70 | loss: 0.62473 | eval_custom_logloss: 0.8712  |  0:17:58s
epoch 71 | loss: 0.63576 | eval_custom_logloss: 0.64235 |  0:18:13s
epoch 72 | loss: 0.62799 | eval_custom_logloss: 0.8885  |  0:18:28s
epoch 73 | loss: 0.62815 | eval_custom_logloss: 0.8155  |  0:18:43s
epoch 74 | loss: 0.62598 | eval_custom_logloss: 1.40217 |  0:18:58s
epoch 75 | loss: 0.63178 | eval_custom_logloss: 1.65437 |  0:19:13s
epoch 76 | loss: 0.63258 | eval_custom_logloss: 0.76524 |  0:19:28s
epoch 77 | loss: 0.63106 | eval_custom_logloss: 0.94744 |  0:19:43s
epoch 78 | loss: 0.63082 | eval_custom_logloss: 0.66682 |  0:19:58s
epoch 79 | loss: 0.62696 | eval_custom_logloss: 1.14515 |  0:20:13s
epoch 80 | loss: 0.62473 | eval_custom_logloss: 0.78612 |  0:20:28s
epoch 81 | loss: 0.62739 | eval_custom_logloss: 2.04278 |  0:20:42s
epoch 82 | loss: 0.62978 | eval_custom_logloss: 0.63057 |  0:20:57s
epoch 83 | loss: 0.62432 | eval_custom_logloss: 0.91105 |  0:21:12s
epoch 84 | loss: 0.62806 | eval_custom_logloss: 0.70745 |  0:21:27s
epoch 85 | loss: 0.62614 | eval_custom_logloss: 1.5748  |  0:21:42s
epoch 86 | loss: 0.61763 | eval_custom_logloss: 0.94847 |  0:21:57s
epoch 87 | loss: 0.61718 | eval_custom_logloss: 1.10859 |  0:22:12s
epoch 88 | loss: 0.61353 | eval_custom_logloss: 0.6794  |  0:22:27s
epoch 89 | loss: 0.60727 | eval_custom_logloss: 1.10583 |  0:22:42s
epoch 90 | loss: 0.61295 | eval_custom_logloss: 0.93574 |  0:22:57s
epoch 91 | loss: 0.61677 | eval_custom_logloss: 0.80352 |  0:23:12s
epoch 92 | loss: 0.61136 | eval_custom_logloss: 0.7887  |  0:23:27s
epoch 93 | loss: 0.59757 | eval_custom_logloss: 0.81093 |  0:23:41s
epoch 94 | loss: 0.61825 | eval_custom_logloss: 0.99522 |  0:23:56s
epoch 95 | loss: 0.60558 | eval_custom_logloss: 0.78007 |  0:24:11s
epoch 96 | loss: 0.60724 | eval_custom_logloss: 0.89622 |  0:24:26s
epoch 97 | loss: 0.59991 | eval_custom_logloss: 0.63261 |  0:24:41s
epoch 98 | loss: 0.60606 | eval_custom_logloss: 0.72912 |  0:24:56s
epoch 99 | loss: 0.6223  | eval_custom_logloss: 0.89688 |  0:25:11s
Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_eval_custom_logloss = 0.63057
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.63505, 'Log Loss - std': 0.005349999999999966} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.51368 | eval_custom_logloss: 10.86796|  0:00:14s
epoch 1  | loss: 1.10991 | eval_custom_logloss: 10.55337|  0:00:30s
epoch 2  | loss: 1.01435 | eval_custom_logloss: 9.22781 |  0:00:44s
epoch 3  | loss: 0.9774  | eval_custom_logloss: 7.82678 |  0:00:59s
epoch 4  | loss: 0.92621 | eval_custom_logloss: 6.52738 |  0:01:14s
epoch 5  | loss: 0.90952 | eval_custom_logloss: 5.56269 |  0:01:29s
epoch 6  | loss: 0.91339 | eval_custom_logloss: 4.89106 |  0:01:44s
epoch 7  | loss: 0.87565 | eval_custom_logloss: 4.18326 |  0:01:59s
epoch 8  | loss: 0.845   | eval_custom_logloss: 3.64023 |  0:02:14s
epoch 9  | loss: 0.81867 | eval_custom_logloss: 1.58381 |  0:02:29s
epoch 10 | loss: 0.80226 | eval_custom_logloss: 1.45982 |  0:02:44s
epoch 11 | loss: 0.80695 | eval_custom_logloss: 1.97741 |  0:02:59s
epoch 12 | loss: 0.80836 | eval_custom_logloss: 1.70673 |  0:03:14s
epoch 13 | loss: 0.79205 | eval_custom_logloss: 1.78317 |  0:03:29s
epoch 14 | loss: 0.78893 | eval_custom_logloss: 1.69503 |  0:03:44s
epoch 15 | loss: 0.76624 | eval_custom_logloss: 1.28248 |  0:03:59s
epoch 16 | loss: 0.76475 | eval_custom_logloss: 2.36626 |  0:04:14s
epoch 17 | loss: 0.75236 | eval_custom_logloss: 1.10219 |  0:04:29s
epoch 18 | loss: 0.74934 | eval_custom_logloss: 1.06602 |  0:04:44s
epoch 19 | loss: 0.74598 | eval_custom_logloss: 1.22658 |  0:04:58s
epoch 20 | loss: 0.74001 | eval_custom_logloss: 1.3595  |  0:05:13s
epoch 21 | loss: 0.74753 | eval_custom_logloss: 1.05285 |  0:05:28s
epoch 22 | loss: 0.72202 | eval_custom_logloss: 1.0717  |  0:05:43s
epoch 23 | loss: 0.71668 | eval_custom_logloss: 1.10289 |  0:05:58s
epoch 24 | loss: 0.7177  | eval_custom_logloss: 1.06646 |  0:06:13s
epoch 25 | loss: 0.71831 | eval_custom_logloss: 1.74621 |  0:06:28s
epoch 26 | loss: 0.70143 | eval_custom_logloss: 1.39366 |  0:06:43s
epoch 27 | loss: 0.71053 | eval_custom_logloss: 2.3456  |  0:06:58s
epoch 28 | loss: 0.72172 | eval_custom_logloss: 2.0291  |  0:07:13s
epoch 29 | loss: 0.70382 | eval_custom_logloss: 1.96175 |  0:07:28s
epoch 30 | loss: 0.70681 | eval_custom_logloss: 0.918   |  0:07:43s
epoch 31 | loss: 0.68687 | eval_custom_logloss: 1.1909  |  0:07:57s
epoch 32 | loss: 0.70458 | eval_custom_logloss: 1.01413 |  0:08:12s
epoch 33 | loss: 0.68529 | eval_custom_logloss: 0.94782 |  0:08:27s
epoch 34 | loss: 0.68805 | eval_custom_logloss: 1.41092 |  0:08:42s
epoch 35 | loss: 0.68038 | eval_custom_logloss: 1.40501 |  0:08:57s
epoch 36 | loss: 0.67804 | eval_custom_logloss: 1.01434 |  0:09:12s
epoch 37 | loss: 0.6799  | eval_custom_logloss: 1.86545 |  0:09:27s
epoch 38 | loss: 0.68876 | eval_custom_logloss: 0.83886 |  0:09:42s
epoch 39 | loss: 0.68072 | eval_custom_logloss: 0.9016  |  0:09:57s
epoch 40 | loss: 0.67503 | eval_custom_logloss: 1.28362 |  0:10:12s
epoch 41 | loss: 0.66852 | eval_custom_logloss: 1.28108 |  0:10:27s
epoch 42 | loss: 0.67598 | eval_custom_logloss: 1.28734 |  0:10:42s
epoch 43 | loss: 0.66813 | eval_custom_logloss: 0.80621 |  0:10:57s
epoch 44 | loss: 0.65439 | eval_custom_logloss: 1.91354 |  0:11:11s
epoch 45 | loss: 0.65146 | eval_custom_logloss: 0.7658  |  0:11:26s
epoch 46 | loss: 0.66699 | eval_custom_logloss: 0.77573 |  0:11:41s
epoch 47 | loss: 0.66111 | eval_custom_logloss: 0.93386 |  0:11:56s
epoch 48 | loss: 0.65207 | eval_custom_logloss: 0.85333 |  0:12:10s
epoch 49 | loss: 0.66593 | eval_custom_logloss: 1.57311 |  0:12:25s
epoch 50 | loss: 0.65679 | eval_custom_logloss: 0.9038  |  0:12:40s
epoch 51 | loss: 0.663   | eval_custom_logloss: 0.98281 |  0:12:55s
epoch 52 | loss: 0.67157 | eval_custom_logloss: 0.81156 |  0:13:10s
epoch 53 | loss: 0.65911 | eval_custom_logloss: 1.16949 |  0:13:25s
epoch 54 | loss: 0.66151 | eval_custom_logloss: 1.3501  |  0:13:39s
epoch 55 | loss: 0.64941 | eval_custom_logloss: 1.01442 |  0:13:54s
epoch 56 | loss: 0.64382 | eval_custom_logloss: 0.98726 |  0:14:09s
epoch 57 | loss: 0.65405 | eval_custom_logloss: 1.24124 |  0:14:24s
epoch 58 | loss: 0.65749 | eval_custom_logloss: 0.96471 |  0:14:39s
epoch 59 | loss: 0.64162 | eval_custom_logloss: 1.69957 |  0:14:54s
epoch 60 | loss: 0.64513 | eval_custom_logloss: 0.88721 |  0:15:09s
epoch 61 | loss: 0.64529 | eval_custom_logloss: 1.63035 |  0:15:23s
epoch 62 | loss: 0.64115 | eval_custom_logloss: 0.82361 |  0:15:38s
epoch 63 | loss: 0.63615 | eval_custom_logloss: 0.9624  |  0:15:53s
epoch 64 | loss: 0.63575 | eval_custom_logloss: 0.77753 |  0:16:08s
epoch 65 | loss: 0.63886 | eval_custom_logloss: 1.05712 |  0:16:23s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 0.7658
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6775666666666668, 'Log Loss - std': 0.06028611411888771} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58786 | eval_custom_logloss: 10.36881|  0:00:14s
epoch 1  | loss: 1.17116 | eval_custom_logloss: 7.46827 |  0:00:29s
epoch 2  | loss: 1.07763 | eval_custom_logloss: 5.94308 |  0:00:44s
epoch 3  | loss: 1.05053 | eval_custom_logloss: 5.13882 |  0:00:59s
epoch 4  | loss: 1.01325 | eval_custom_logloss: 5.68414 |  0:01:14s
epoch 5  | loss: 0.9664  | eval_custom_logloss: 4.16193 |  0:01:28s
epoch 6  | loss: 0.99023 | eval_custom_logloss: 3.57138 |  0:01:43s
epoch 7  | loss: 0.92099 | eval_custom_logloss: 3.92496 |  0:01:58s
epoch 8  | loss: 0.91369 | eval_custom_logloss: 3.16045 |  0:02:13s
epoch 9  | loss: 0.86743 | eval_custom_logloss: 2.84662 |  0:02:28s
epoch 10 | loss: 0.85539 | eval_custom_logloss: 2.21199 |  0:02:43s
epoch 11 | loss: 0.86318 | eval_custom_logloss: 2.33976 |  0:02:57s
epoch 12 | loss: 0.84241 | eval_custom_logloss: 1.92453 |  0:03:12s
epoch 13 | loss: 0.84751 | eval_custom_logloss: 1.40722 |  0:03:27s
epoch 14 | loss: 0.82514 | eval_custom_logloss: 2.08444 |  0:03:42s
epoch 15 | loss: 0.82758 | eval_custom_logloss: 3.77504 |  0:03:57s
epoch 16 | loss: 0.82553 | eval_custom_logloss: 1.86498 |  0:04:12s
epoch 17 | loss: 0.80191 | eval_custom_logloss: 1.71576 |  0:04:26s
epoch 18 | loss: 0.8113  | eval_custom_logloss: 2.47458 |  0:04:41s
epoch 19 | loss: 0.79616 | eval_custom_logloss: 1.24604 |  0:04:56s
epoch 20 | loss: 0.80688 | eval_custom_logloss: 1.37716 |  0:05:11s
epoch 21 | loss: 0.82825 | eval_custom_logloss: 1.18289 |  0:05:26s
epoch 22 | loss: 0.84133 | eval_custom_logloss: 1.87427 |  0:05:41s
epoch 23 | loss: 0.81155 | eval_custom_logloss: 1.67215 |  0:05:56s
epoch 24 | loss: 0.79759 | eval_custom_logloss: 2.10724 |  0:06:11s
epoch 25 | loss: 0.80299 | eval_custom_logloss: 1.94431 |  0:06:26s
epoch 26 | loss: 0.78959 | eval_custom_logloss: 1.95564 |  0:06:40s
epoch 27 | loss: 0.78917 | eval_custom_logloss: 1.48247 |  0:06:55s
epoch 28 | loss: 0.78982 | eval_custom_logloss: 0.7251  |  0:07:10s
epoch 29 | loss: 0.78253 | eval_custom_logloss: 0.80639 |  0:07:25s
epoch 30 | loss: 0.77814 | eval_custom_logloss: 1.66667 |  0:07:40s
epoch 31 | loss: 0.78259 | eval_custom_logloss: 0.86359 |  0:07:55s
epoch 32 | loss: 0.78197 | eval_custom_logloss: 1.1046  |  0:08:10s
epoch 33 | loss: 0.77307 | eval_custom_logloss: 0.8778  |  0:08:24s
epoch 34 | loss: 0.73721 | eval_custom_logloss: 1.10172 |  0:08:39s
epoch 35 | loss: 0.75812 | eval_custom_logloss: 0.98142 |  0:08:54s
epoch 36 | loss: 0.73597 | eval_custom_logloss: 0.91397 |  0:09:09s
epoch 37 | loss: 0.7418  | eval_custom_logloss: 1.08809 |  0:09:24s
epoch 38 | loss: 0.729   | eval_custom_logloss: 0.81666 |  0:09:39s
epoch 39 | loss: 0.72351 | eval_custom_logloss: 0.91285 |  0:09:54s
epoch 40 | loss: 0.73709 | eval_custom_logloss: 0.91344 |  0:10:08s
epoch 41 | loss: 0.71714 | eval_custom_logloss: 0.72008 |  0:10:23s
epoch 42 | loss: 0.7148  | eval_custom_logloss: 0.74674 |  0:10:38s
epoch 43 | loss: 0.71782 | eval_custom_logloss: 0.89598 |  0:10:53s
epoch 44 | loss: 0.68432 | eval_custom_logloss: 0.75677 |  0:11:08s
epoch 45 | loss: 0.71946 | eval_custom_logloss: 0.76098 |  0:11:23s
epoch 46 | loss: 0.69618 | eval_custom_logloss: 0.75785 |  0:11:38s
epoch 47 | loss: 0.69928 | eval_custom_logloss: 0.8153  |  0:11:53s
epoch 48 | loss: 0.70292 | eval_custom_logloss: 0.94348 |  0:12:08s
epoch 49 | loss: 0.69688 | eval_custom_logloss: 1.06821 |  0:12:23s
epoch 50 | loss: 0.69354 | eval_custom_logloss: 1.02393 |  0:12:38s
epoch 51 | loss: 0.69761 | eval_custom_logloss: 0.91035 |  0:12:53s
epoch 52 | loss: 0.71089 | eval_custom_logloss: 0.87134 |  0:13:08s
epoch 53 | loss: 0.69565 | eval_custom_logloss: 0.88894 |  0:13:23s
epoch 54 | loss: 0.69447 | eval_custom_logloss: 0.84204 |  0:13:38s
epoch 55 | loss: 0.68796 | eval_custom_logloss: 0.67936 |  0:13:53s
epoch 56 | loss: 0.68418 | eval_custom_logloss: 0.86122 |  0:14:07s
epoch 57 | loss: 0.69244 | eval_custom_logloss: 0.68987 |  0:14:23s
epoch 58 | loss: 0.68017 | eval_custom_logloss: 0.88104 |  0:14:38s
epoch 59 | loss: 0.6837  | eval_custom_logloss: 0.83092 |  0:14:53s
epoch 60 | loss: 0.68081 | eval_custom_logloss: 0.81505 |  0:15:08s
epoch 61 | loss: 0.67965 | eval_custom_logloss: 0.77455 |  0:15:22s
epoch 62 | loss: 0.67272 | eval_custom_logloss: 0.93875 |  0:15:37s
epoch 63 | loss: 0.68536 | eval_custom_logloss: 0.8756  |  0:15:52s
epoch 64 | loss: 0.68314 | eval_custom_logloss: 0.70371 |  0:16:07s
epoch 65 | loss: 0.68605 | eval_custom_logloss: 0.95485 |  0:16:22s
epoch 66 | loss: 0.67839 | eval_custom_logloss: 0.9053  |  0:16:38s
epoch 67 | loss: 0.69    | eval_custom_logloss: 0.78391 |  0:16:53s
epoch 68 | loss: 0.66853 | eval_custom_logloss: 0.75646 |  0:17:08s
epoch 69 | loss: 0.67543 | eval_custom_logloss: 0.74013 |  0:17:23s
epoch 70 | loss: 0.68101 | eval_custom_logloss: 0.81673 |  0:17:38s
epoch 71 | loss: 0.6756  | eval_custom_logloss: 0.84765 |  0:17:53s
epoch 72 | loss: 0.6683  | eval_custom_logloss: 1.02717 |  0:18:08s
epoch 73 | loss: 0.6734  | eval_custom_logloss: 0.79778 |  0:18:23s
epoch 74 | loss: 0.66495 | eval_custom_logloss: 0.97909 |  0:18:38s
epoch 75 | loss: 0.66538 | eval_custom_logloss: 0.90985 |  0:18:53s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.67936
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.67805, 'Log Loss - std': 0.052216017657420004} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax', 'n_a': 41, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.60668 | eval_custom_logloss: 10.32234|  0:00:14s
epoch 1  | loss: 1.22549 | eval_custom_logloss: 8.92675 |  0:00:29s
epoch 2  | loss: 1.10818 | eval_custom_logloss: 8.38335 |  0:00:44s
epoch 3  | loss: 1.09404 | eval_custom_logloss: 8.70046 |  0:00:59s
epoch 4  | loss: 1.01955 | eval_custom_logloss: 8.69297 |  0:01:14s
epoch 5  | loss: 0.98775 | eval_custom_logloss: 5.66522 |  0:01:29s
epoch 6  | loss: 0.99409 | eval_custom_logloss: 7.1446  |  0:01:44s
epoch 7  | loss: 0.94958 | eval_custom_logloss: 5.48016 |  0:01:58s
epoch 8  | loss: 0.93852 | eval_custom_logloss: 3.91227 |  0:02:13s
epoch 9  | loss: 0.91284 | eval_custom_logloss: 4.16418 |  0:02:28s
epoch 10 | loss: 0.88389 | eval_custom_logloss: 2.56803 |  0:02:43s
epoch 11 | loss: 0.89994 | eval_custom_logloss: 3.09691 |  0:02:58s
epoch 12 | loss: 0.88931 | eval_custom_logloss: 2.51451 |  0:03:13s
epoch 13 | loss: 0.88885 | eval_custom_logloss: 2.24531 |  0:03:28s
epoch 14 | loss: 0.86445 | eval_custom_logloss: 1.98607 |  0:03:43s
epoch 15 | loss: 0.86443 | eval_custom_logloss: 2.49741 |  0:03:58s
epoch 16 | loss: 0.84803 | eval_custom_logloss: 1.51307 |  0:04:13s
epoch 17 | loss: 0.83107 | eval_custom_logloss: 1.45646 |  0:04:27s
epoch 18 | loss: 0.81638 | eval_custom_logloss: 1.35099 |  0:04:43s
epoch 19 | loss: 0.8012  | eval_custom_logloss: 1.30896 |  0:04:58s
epoch 20 | loss: 0.79491 | eval_custom_logloss: 1.11487 |  0:05:12s
epoch 21 | loss: 0.79712 | eval_custom_logloss: 1.12739 |  0:05:27s
epoch 22 | loss: 0.79752 | eval_custom_logloss: 1.32586 |  0:05:42s
epoch 23 | loss: 0.76709 | eval_custom_logloss: 1.21786 |  0:05:57s
epoch 24 | loss: 0.75508 | eval_custom_logloss: 1.46456 |  0:06:12s
epoch 25 | loss: 0.75803 | eval_custom_logloss: 1.90544 |  0:06:27s
epoch 26 | loss: 0.75312 | eval_custom_logloss: 1.4661  |  0:06:42s
epoch 27 | loss: 0.76171 | eval_custom_logloss: 1.02145 |  0:06:57s
epoch 28 | loss: 0.76641 | eval_custom_logloss: 0.95361 |  0:07:12s
epoch 29 | loss: 0.75599 | eval_custom_logloss: 0.99666 |  0:07:27s
epoch 30 | loss: 0.74316 | eval_custom_logloss: 0.91716 |  0:07:42s
epoch 31 | loss: 0.7435  | eval_custom_logloss: 0.8582  |  0:07:57s
epoch 32 | loss: 0.75994 | eval_custom_logloss: 1.08493 |  0:08:12s
epoch 33 | loss: 0.72874 | eval_custom_logloss: 1.09641 |  0:08:27s
epoch 34 | loss: 0.752   | eval_custom_logloss: 0.92688 |  0:08:42s
epoch 35 | loss: 0.73513 | eval_custom_logloss: 0.89593 |  0:08:57s
epoch 36 | loss: 0.72426 | eval_custom_logloss: 0.90091 |  0:09:11s
epoch 37 | loss: 0.73064 | eval_custom_logloss: 0.82238 |  0:09:26s
epoch 38 | loss: 0.74882 | eval_custom_logloss: 1.02539 |  0:09:41s
epoch 39 | loss: 0.71941 | eval_custom_logloss: 0.97671 |  0:09:56s
epoch 40 | loss: 0.74885 | eval_custom_logloss: 0.96513 |  0:10:11s
epoch 41 | loss: 0.72858 | eval_custom_logloss: 1.00225 |  0:10:25s
epoch 42 | loss: 0.70959 | eval_custom_logloss: 0.90148 |  0:10:40s
epoch 43 | loss: 0.71094 | eval_custom_logloss: 0.97917 |  0:10:55s
epoch 44 | loss: 0.71022 | eval_custom_logloss: 0.7746  |  0:11:10s
epoch 45 | loss: 0.70881 | eval_custom_logloss: 1.10682 |  0:11:25s
epoch 46 | loss: 0.71456 | eval_custom_logloss: 0.7796  |  0:11:40s
epoch 47 | loss: 0.71088 | eval_custom_logloss: 1.11487 |  0:11:54s
epoch 48 | loss: 0.70617 | eval_custom_logloss: 0.84313 |  0:12:09s
epoch 49 | loss: 0.70366 | eval_custom_logloss: 0.91574 |  0:12:24s
epoch 50 | loss: 0.73868 | eval_custom_logloss: 2.04919 |  0:12:39s
epoch 51 | loss: 0.76243 | eval_custom_logloss: 1.32008 |  0:12:54s
epoch 52 | loss: 0.7738  | eval_custom_logloss: 1.6548  |  0:13:09s
epoch 53 | loss: 0.74921 | eval_custom_logloss: 0.94399 |  0:13:24s
epoch 54 | loss: 0.73407 | eval_custom_logloss: 0.80908 |  0:13:39s
epoch 55 | loss: 0.71088 | eval_custom_logloss: 1.0502  |  0:13:54s
epoch 56 | loss: 0.72912 | eval_custom_logloss: 1.57563 |  0:14:09s
epoch 57 | loss: 0.71631 | eval_custom_logloss: 1.41043 |  0:14:24s
epoch 58 | loss: 0.71253 | eval_custom_logloss: 1.05455 |  0:14:39s
epoch 59 | loss: 0.70698 | eval_custom_logloss: 1.04189 |  0:14:54s
epoch 60 | loss: 0.69777 | eval_custom_logloss: 0.77302 |  0:15:09s
epoch 61 | loss: 0.69404 | eval_custom_logloss: 1.25286 |  0:15:24s
epoch 62 | loss: 0.71307 | eval_custom_logloss: 0.81279 |  0:15:39s
epoch 63 | loss: 0.69826 | eval_custom_logloss: 0.92926 |  0:15:53s
epoch 64 | loss: 0.69727 | eval_custom_logloss: 0.91289 |  0:16:08s
epoch 65 | loss: 0.71015 | eval_custom_logloss: 0.78985 |  0:16:23s
epoch 66 | loss: 0.69782 | eval_custom_logloss: 1.03856 |  0:16:38s
epoch 67 | loss: 0.68502 | eval_custom_logloss: 0.87298 |  0:16:53s
epoch 68 | loss: 0.68913 | eval_custom_logloss: 0.75914 |  0:17:08s
epoch 69 | loss: 0.69767 | eval_custom_logloss: 0.93362 |  0:17:23s
epoch 70 | loss: 0.69538 | eval_custom_logloss: 1.2041  |  0:17:38s
epoch 71 | loss: 0.69658 | eval_custom_logloss: 0.91218 |  0:17:53s
epoch 72 | loss: 0.70041 | eval_custom_logloss: 0.9386  |  0:18:08s
epoch 73 | loss: 0.70499 | eval_custom_logloss: 1.1773  |  0:18:23s
epoch 74 | loss: 0.68333 | eval_custom_logloss: 1.04769 |  0:18:37s
epoch 75 | loss: 0.67657 | eval_custom_logloss: 0.78588 |  0:18:52s
epoch 76 | loss: 0.68915 | eval_custom_logloss: 0.84812 |  0:19:07s
epoch 77 | loss: 0.68514 | eval_custom_logloss: 0.80581 |  0:19:22s
epoch 78 | loss: 0.69129 | eval_custom_logloss: 0.68798 |  0:19:37s
epoch 79 | loss: 0.6864  | eval_custom_logloss: 0.71573 |  0:19:52s
epoch 80 | loss: 0.67953 | eval_custom_logloss: 0.98763 |  0:20:07s
epoch 81 | loss: 0.6802  | eval_custom_logloss: 0.69663 |  0:20:22s
epoch 82 | loss: 0.66772 | eval_custom_logloss: 0.81949 |  0:20:37s
epoch 83 | loss: 0.68226 | eval_custom_logloss: 0.75762 |  0:20:52s
epoch 84 | loss: 0.67136 | eval_custom_logloss: 0.83995 |  0:21:07s
epoch 85 | loss: 0.67667 | eval_custom_logloss: 0.82833 |  0:21:21s
epoch 86 | loss: 0.66186 | eval_custom_logloss: 0.68466 |  0:21:36s
epoch 87 | loss: 0.6702  | eval_custom_logloss: 0.69021 |  0:21:52s
epoch 88 | loss: 0.67665 | eval_custom_logloss: 0.82274 |  0:22:07s
epoch 89 | loss: 0.67551 | eval_custom_logloss: 0.90109 |  0:22:22s
epoch 90 | loss: 0.66406 | eval_custom_logloss: 0.92148 |  0:22:36s
epoch 91 | loss: 0.66633 | eval_custom_logloss: 0.75972 |  0:22:51s
epoch 92 | loss: 0.66217 | eval_custom_logloss: 0.79982 |  0:23:07s
epoch 93 | loss: 0.65877 | eval_custom_logloss: 0.86764 |  0:23:22s
epoch 94 | loss: 0.68208 | eval_custom_logloss: 0.80151 |  0:23:36s
epoch 95 | loss: 0.66333 | eval_custom_logloss: 0.82909 |  0:23:51s
epoch 96 | loss: 0.66701 | eval_custom_logloss: 0.90488 |  0:24:06s
epoch 97 | loss: 0.666   | eval_custom_logloss: 0.70285 |  0:24:21s
epoch 98 | loss: 0.66345 | eval_custom_logloss: 0.91687 |  0:24:36s
epoch 99 | loss: 0.66327 | eval_custom_logloss: 0.7224  |  0:24:51s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.68466
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.67898, 'Log Loss - std': 0.04674044929180717} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 0.67898 and parameters: {'n_d': 41, 'n_steps': 7, 'gamma': 1.1265842440565628, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.0010556995878123812, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.85914 | eval_custom_logloss: 11.97084|  0:00:25s
epoch 1  | loss: 1.32682 | eval_custom_logloss: 9.4663  |  0:00:50s
epoch 2  | loss: 1.17606 | eval_custom_logloss: 8.19156 |  0:01:16s
epoch 3  | loss: 1.14311 | eval_custom_logloss: 8.69986 |  0:01:42s
epoch 4  | loss: 1.09348 | eval_custom_logloss: 5.37926 |  0:02:08s
epoch 5  | loss: 1.06552 | eval_custom_logloss: 6.9638  |  0:02:33s
epoch 6  | loss: 1.03969 | eval_custom_logloss: 6.04703 |  0:02:59s
epoch 7  | loss: 1.05327 | eval_custom_logloss: 5.02578 |  0:03:24s
epoch 8  | loss: 1.02804 | eval_custom_logloss: 4.68085 |  0:03:50s
epoch 9  | loss: 0.9934  | eval_custom_logloss: 4.1496  |  0:04:16s
epoch 10 | loss: 0.97871 | eval_custom_logloss: 5.05452 |  0:04:41s
epoch 11 | loss: 0.96809 | eval_custom_logloss: 4.02021 |  0:05:07s
epoch 12 | loss: 0.97837 | eval_custom_logloss: 2.28141 |  0:05:32s
epoch 13 | loss: 0.95656 | eval_custom_logloss: 3.58394 |  0:05:57s
epoch 14 | loss: 0.97527 | eval_custom_logloss: 3.76574 |  0:06:23s
epoch 15 | loss: 0.93274 | eval_custom_logloss: 3.01253 |  0:06:48s
epoch 16 | loss: 0.9581  | eval_custom_logloss: 2.57408 |  0:07:14s
epoch 17 | loss: 0.92794 | eval_custom_logloss: 3.35172 |  0:07:39s
epoch 18 | loss: 0.90991 | eval_custom_logloss: 3.12762 |  0:08:05s
epoch 19 | loss: 0.89195 | eval_custom_logloss: 1.08103 |  0:08:30s
epoch 20 | loss: 0.88907 | eval_custom_logloss: 1.23636 |  0:08:55s
epoch 21 | loss: 0.87233 | eval_custom_logloss: 1.05238 |  0:09:21s
epoch 22 | loss: 0.83948 | eval_custom_logloss: 1.20474 |  0:09:46s
epoch 23 | loss: 0.84242 | eval_custom_logloss: 1.36971 |  0:10:12s
epoch 24 | loss: 0.83198 | eval_custom_logloss: 0.85096 |  0:10:37s
epoch 25 | loss: 0.81349 | eval_custom_logloss: 1.222   |  0:11:03s
epoch 26 | loss: 0.79972 | eval_custom_logloss: 0.98975 |  0:11:28s
epoch 27 | loss: 0.76793 | eval_custom_logloss: 2.01026 |  0:11:54s
epoch 28 | loss: 0.74584 | eval_custom_logloss: 0.98227 |  0:12:20s
epoch 29 | loss: 0.75079 | eval_custom_logloss: 3.369   |  0:12:45s
epoch 30 | loss: 0.7493  | eval_custom_logloss: 3.51101 |  0:13:11s
epoch 31 | loss: 0.72686 | eval_custom_logloss: 2.49115 |  0:13:37s
epoch 32 | loss: 0.71387 | eval_custom_logloss: 1.62438 |  0:14:02s
epoch 33 | loss: 0.71407 | eval_custom_logloss: 1.34161 |  0:14:28s
epoch 34 | loss: 0.73411 | eval_custom_logloss: 0.98895 |  0:14:54s
epoch 35 | loss: 0.74036 | eval_custom_logloss: 1.12561 |  0:15:19s
epoch 36 | loss: 0.71091 | eval_custom_logloss: 0.98158 |  0:15:45s
epoch 37 | loss: 0.73113 | eval_custom_logloss: 2.31892 |  0:16:11s
epoch 38 | loss: 0.70853 | eval_custom_logloss: 1.90577 |  0:16:37s
epoch 39 | loss: 0.70636 | eval_custom_logloss: 1.27007 |  0:17:03s
epoch 40 | loss: 0.70182 | eval_custom_logloss: 4.26092 |  0:17:28s
epoch 41 | loss: 0.71664 | eval_custom_logloss: 0.81019 |  0:17:54s
epoch 42 | loss: 0.69241 | eval_custom_logloss: 1.1642  |  0:18:20s
epoch 43 | loss: 0.69496 | eval_custom_logloss: 1.14486 |  0:18:46s
epoch 44 | loss: 0.68674 | eval_custom_logloss: 1.08031 |  0:19:11s
epoch 45 | loss: 0.67613 | eval_custom_logloss: 0.75586 |  0:19:37s
epoch 46 | loss: 0.6746  | eval_custom_logloss: 0.91002 |  0:20:03s
epoch 47 | loss: 0.68063 | eval_custom_logloss: 0.94339 |  0:20:29s
epoch 48 | loss: 0.68934 | eval_custom_logloss: 1.2309  |  0:20:55s
epoch 49 | loss: 0.68204 | eval_custom_logloss: 2.03716 |  0:21:21s
epoch 50 | loss: 0.67257 | eval_custom_logloss: 1.57307 |  0:21:47s
epoch 51 | loss: 0.66839 | eval_custom_logloss: 0.84771 |  0:22:13s
epoch 52 | loss: 0.67924 | eval_custom_logloss: 0.72746 |  0:22:39s
epoch 53 | loss: 0.67073 | eval_custom_logloss: 2.12181 |  0:23:05s
epoch 54 | loss: 0.65394 | eval_custom_logloss: 1.31193 |  0:23:31s
epoch 55 | loss: 0.67498 | eval_custom_logloss: 1.18766 |  0:23:56s
epoch 56 | loss: 0.67407 | eval_custom_logloss: 0.71442 |  0:24:22s
epoch 57 | loss: 0.66368 | eval_custom_logloss: 0.83703 |  0:24:47s
epoch 58 | loss: 0.67175 | eval_custom_logloss: 1.30711 |  0:25:13s
epoch 59 | loss: 0.6547  | eval_custom_logloss: 1.73069 |  0:25:39s
epoch 60 | loss: 0.65801 | eval_custom_logloss: 1.26262 |  0:26:04s
epoch 61 | loss: 0.66147 | eval_custom_logloss: 0.71881 |  0:26:29s
epoch 62 | loss: 0.65431 | eval_custom_logloss: 1.04584 |  0:26:55s
epoch 63 | loss: 0.65928 | eval_custom_logloss: 2.30202 |  0:27:20s
epoch 64 | loss: 0.66015 | eval_custom_logloss: 0.92289 |  0:27:46s
epoch 65 | loss: 0.68128 | eval_custom_logloss: 4.50521 |  0:28:12s
epoch 66 | loss: 0.66956 | eval_custom_logloss: 2.1433  |  0:28:37s
epoch 67 | loss: 0.67997 | eval_custom_logloss: 1.31279 |  0:29:03s
epoch 68 | loss: 0.65441 | eval_custom_logloss: 0.85998 |  0:29:29s
epoch 69 | loss: 0.66635 | eval_custom_logloss: 0.77898 |  0:29:54s
epoch 70 | loss: 0.66509 | eval_custom_logloss: 1.47741 |  0:30:20s
epoch 71 | loss: 0.65052 | eval_custom_logloss: 0.95713 |  0:30:45s
epoch 72 | loss: 0.65342 | eval_custom_logloss: 0.84295 |  0:31:11s
epoch 73 | loss: 0.67753 | eval_custom_logloss: 1.53864 |  0:31:37s
epoch 74 | loss: 0.66766 | eval_custom_logloss: 1.33832 |  0:32:02s
epoch 75 | loss: 0.65058 | eval_custom_logloss: 0.71127 |  0:32:28s
epoch 76 | loss: 0.65781 | eval_custom_logloss: 1.33067 |  0:32:54s
epoch 77 | loss: 0.67118 | eval_custom_logloss: 0.85243 |  0:33:20s
epoch 78 | loss: 0.64407 | eval_custom_logloss: 0.86691 |  0:33:46s
epoch 79 | loss: 0.63601 | eval_custom_logloss: 0.9363  |  0:34:11s
epoch 80 | loss: 0.64934 | eval_custom_logloss: 0.74797 |  0:34:37s
epoch 81 | loss: 0.65027 | eval_custom_logloss: 0.98378 |  0:35:03s
epoch 82 | loss: 0.64121 | eval_custom_logloss: 1.00366 |  0:35:29s
epoch 83 | loss: 0.63627 | eval_custom_logloss: 0.82785 |  0:35:54s
epoch 84 | loss: 0.63871 | eval_custom_logloss: 0.7308  |  0:36:20s
epoch 85 | loss: 0.65281 | eval_custom_logloss: 1.34811 |  0:36:46s
epoch 86 | loss: 0.62907 | eval_custom_logloss: 0.74235 |  0:37:12s
epoch 87 | loss: 0.62604 | eval_custom_logloss: 2.13719 |  0:37:37s
epoch 88 | loss: 0.6434  | eval_custom_logloss: 1.72285 |  0:38:03s
epoch 89 | loss: 0.6275  | eval_custom_logloss: 1.10635 |  0:38:29s
epoch 90 | loss: 0.62391 | eval_custom_logloss: 0.82227 |  0:38:54s
epoch 91 | loss: 0.63006 | eval_custom_logloss: 1.17782 |  0:39:20s
epoch 92 | loss: 0.62866 | eval_custom_logloss: 0.99302 |  0:39:45s
epoch 93 | loss: 0.62986 | eval_custom_logloss: 1.51686 |  0:40:11s
epoch 94 | loss: 0.62313 | eval_custom_logloss: 1.33852 |  0:40:36s
epoch 95 | loss: 0.62126 | eval_custom_logloss: 0.89885 |  0:41:02s

Early stopping occurred at epoch 95 with best_epoch = 75 and best_eval_custom_logloss = 0.71127
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7104, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.89884 | eval_custom_logloss: 13.91651|  0:00:25s
epoch 1  | loss: 1.41568 | eval_custom_logloss: 8.76507 |  0:00:50s
epoch 2  | loss: 1.32151 | eval_custom_logloss: 6.72783 |  0:01:16s
epoch 3  | loss: 1.23983 | eval_custom_logloss: 5.05547 |  0:01:42s
epoch 4  | loss: 1.15543 | eval_custom_logloss: 7.26877 |  0:02:07s
epoch 5  | loss: 1.09769 | eval_custom_logloss: 8.10279 |  0:02:33s
epoch 6  | loss: 1.0684  | eval_custom_logloss: 2.79571 |  0:02:59s
epoch 7  | loss: 1.05813 | eval_custom_logloss: 1.67546 |  0:03:24s
epoch 8  | loss: 1.07864 | eval_custom_logloss: 2.70367 |  0:03:50s
epoch 9  | loss: 1.02469 | eval_custom_logloss: 1.76027 |  0:04:15s
epoch 10 | loss: 0.97373 | eval_custom_logloss: 1.65975 |  0:04:41s
epoch 11 | loss: 0.95077 | eval_custom_logloss: 1.53267 |  0:05:08s
epoch 12 | loss: 0.92424 | eval_custom_logloss: 1.35346 |  0:05:33s
epoch 13 | loss: 0.91666 | eval_custom_logloss: 1.66617 |  0:05:59s
epoch 14 | loss: 0.92886 | eval_custom_logloss: 1.76201 |  0:06:24s
epoch 15 | loss: 0.93475 | eval_custom_logloss: 0.95851 |  0:06:50s
epoch 16 | loss: 1.02215 | eval_custom_logloss: 1.90788 |  0:07:15s
epoch 17 | loss: 0.99724 | eval_custom_logloss: 2.55829 |  0:07:41s
epoch 18 | loss: 1.00044 | eval_custom_logloss: 1.98629 |  0:08:07s
epoch 19 | loss: 1.00761 | eval_custom_logloss: 1.49568 |  0:08:32s
epoch 20 | loss: 0.9666  | eval_custom_logloss: 1.8983  |  0:08:58s
epoch 21 | loss: 0.92823 | eval_custom_logloss: 1.54976 |  0:09:23s
epoch 22 | loss: 0.90364 | eval_custom_logloss: 1.74092 |  0:09:49s
epoch 23 | loss: 0.89099 | eval_custom_logloss: 1.49337 |  0:10:15s
epoch 24 | loss: 0.85843 | eval_custom_logloss: 1.17559 |  0:10:40s
epoch 25 | loss: 0.84898 | eval_custom_logloss: 0.96812 |  0:11:06s
epoch 26 | loss: 0.82447 | eval_custom_logloss: 1.9921  |  0:11:32s
epoch 27 | loss: 0.80312 | eval_custom_logloss: 0.91441 |  0:11:58s
epoch 28 | loss: 0.81235 | eval_custom_logloss: 3.40975 |  0:12:23s
epoch 29 | loss: 0.79993 | eval_custom_logloss: 3.79251 |  0:12:49s
epoch 30 | loss: 0.76865 | eval_custom_logloss: 1.00005 |  0:13:15s
epoch 31 | loss: 0.77758 | eval_custom_logloss: 3.98477 |  0:13:40s
epoch 32 | loss: 0.76285 | eval_custom_logloss: 1.56675 |  0:14:06s
epoch 33 | loss: 0.75727 | eval_custom_logloss: 0.90789 |  0:14:31s
epoch 34 | loss: 0.77591 | eval_custom_logloss: 0.84121 |  0:14:56s
epoch 35 | loss: 0.77702 | eval_custom_logloss: 1.57074 |  0:15:21s
epoch 36 | loss: 0.7666  | eval_custom_logloss: 1.97131 |  0:15:47s
epoch 37 | loss: 0.76981 | eval_custom_logloss: 1.79379 |  0:16:13s
epoch 38 | loss: 0.75291 | eval_custom_logloss: 0.80116 |  0:16:38s
epoch 39 | loss: 0.7506  | eval_custom_logloss: 0.72129 |  0:17:03s
epoch 40 | loss: 0.74425 | eval_custom_logloss: 0.98971 |  0:17:29s
epoch 41 | loss: 0.74342 | eval_custom_logloss: 1.29392 |  0:17:55s
epoch 42 | loss: 0.72825 | eval_custom_logloss: 0.91787 |  0:18:20s
epoch 43 | loss: 0.72359 | eval_custom_logloss: 2.0417  |  0:18:45s
epoch 44 | loss: 0.74015 | eval_custom_logloss: 0.98472 |  0:19:11s
epoch 45 | loss: 0.72961 | eval_custom_logloss: 4.93304 |  0:19:37s
epoch 46 | loss: 0.70877 | eval_custom_logloss: 1.79421 |  0:20:02s
epoch 47 | loss: 0.7152  | eval_custom_logloss: 0.84533 |  0:20:28s
epoch 48 | loss: 0.70284 | eval_custom_logloss: 1.25031 |  0:20:53s
epoch 49 | loss: 0.70943 | eval_custom_logloss: 1.48722 |  0:21:19s
epoch 50 | loss: 0.69896 | eval_custom_logloss: 0.9588  |  0:21:44s
epoch 51 | loss: 0.70254 | eval_custom_logloss: 1.06006 |  0:22:10s
epoch 52 | loss: 0.68773 | eval_custom_logloss: 2.32963 |  0:22:36s
epoch 53 | loss: 0.68429 | eval_custom_logloss: 1.16775 |  0:23:01s
epoch 54 | loss: 0.69171 | eval_custom_logloss: 1.02911 |  0:23:27s
epoch 55 | loss: 0.69419 | eval_custom_logloss: 1.24605 |  0:23:52s
epoch 56 | loss: 0.67288 | eval_custom_logloss: 1.72645 |  0:24:18s
epoch 57 | loss: 0.68267 | eval_custom_logloss: 1.49459 |  0:24:43s
epoch 58 | loss: 0.69598 | eval_custom_logloss: 5.55716 |  0:25:09s
epoch 59 | loss: 0.6908  | eval_custom_logloss: 0.89399 |  0:25:34s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.72129
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7157, 'Log Loss - std': 0.005299999999999971} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.86129 | eval_custom_logloss: 11.64204|  0:00:25s
epoch 1  | loss: 1.33279 | eval_custom_logloss: 9.46016 |  0:00:51s
epoch 2  | loss: 1.29049 | eval_custom_logloss: 7.22289 |  0:01:16s
epoch 3  | loss: 1.23844 | eval_custom_logloss: 5.10467 |  0:01:42s
epoch 4  | loss: 1.18426 | eval_custom_logloss: 2.81071 |  0:02:07s
epoch 5  | loss: 1.10679 | eval_custom_logloss: 2.02723 |  0:02:33s
epoch 6  | loss: 1.02397 | eval_custom_logloss: 1.91526 |  0:02:59s
epoch 7  | loss: 0.99946 | eval_custom_logloss: 1.87193 |  0:03:25s
epoch 8  | loss: 0.98991 | eval_custom_logloss: 1.7242  |  0:03:50s
epoch 9  | loss: 0.99167 | eval_custom_logloss: 2.39252 |  0:04:16s
epoch 10 | loss: 0.98232 | eval_custom_logloss: 1.78347 |  0:04:42s
epoch 11 | loss: 0.97097 | eval_custom_logloss: 1.70609 |  0:05:08s
epoch 12 | loss: 0.98247 | eval_custom_logloss: 2.3346  |  0:05:33s
epoch 13 | loss: 0.94752 | eval_custom_logloss: 3.14713 |  0:05:59s
epoch 14 | loss: 0.93419 | eval_custom_logloss: 1.41228 |  0:06:25s
epoch 15 | loss: 0.96326 | eval_custom_logloss: 2.11357 |  0:06:51s
epoch 16 | loss: 0.92216 | eval_custom_logloss: 2.20756 |  0:07:16s
epoch 17 | loss: 0.91418 | eval_custom_logloss: 2.73318 |  0:07:42s
epoch 18 | loss: 0.89644 | eval_custom_logloss: 1.85302 |  0:08:08s
epoch 19 | loss: 0.91804 | eval_custom_logloss: 1.03845 |  0:08:34s
epoch 20 | loss: 0.93874 | eval_custom_logloss: 1.26824 |  0:08:59s
epoch 21 | loss: 0.90195 | eval_custom_logloss: 1.09005 |  0:09:25s
epoch 22 | loss: 0.89597 | eval_custom_logloss: 1.10047 |  0:09:50s
epoch 23 | loss: 0.88357 | eval_custom_logloss: 1.84853 |  0:10:16s
epoch 24 | loss: 0.93056 | eval_custom_logloss: 1.17688 |  0:10:42s
epoch 25 | loss: 0.87643 | eval_custom_logloss: 0.85376 |  0:11:08s
epoch 26 | loss: 0.85926 | eval_custom_logloss: 0.75488 |  0:11:33s
epoch 27 | loss: 0.84431 | eval_custom_logloss: 1.60613 |  0:11:59s
epoch 28 | loss: 0.85861 | eval_custom_logloss: 1.34396 |  0:12:25s
epoch 29 | loss: 0.83756 | eval_custom_logloss: 1.00808 |  0:12:50s
epoch 30 | loss: 0.8436  | eval_custom_logloss: 0.85598 |  0:13:16s
epoch 31 | loss: 0.82725 | eval_custom_logloss: 1.01889 |  0:13:42s
epoch 32 | loss: 0.82845 | eval_custom_logloss: 1.14396 |  0:14:07s
epoch 33 | loss: 0.83063 | eval_custom_logloss: 1.07241 |  0:14:33s
epoch 34 | loss: 0.83884 | eval_custom_logloss: 1.71846 |  0:14:58s
epoch 35 | loss: 0.84184 | eval_custom_logloss: 1.58052 |  0:15:24s
epoch 36 | loss: 0.8258  | eval_custom_logloss: 1.08098 |  0:15:50s
epoch 37 | loss: 0.82609 | eval_custom_logloss: 0.96755 |  0:16:15s
epoch 38 | loss: 0.81994 | eval_custom_logloss: 1.25499 |  0:16:40s
epoch 39 | loss: 0.80402 | eval_custom_logloss: 1.1378  |  0:17:05s
epoch 40 | loss: 0.82146 | eval_custom_logloss: 0.93183 |  0:17:31s
epoch 41 | loss: 0.8281  | eval_custom_logloss: 0.82202 |  0:17:56s
epoch 42 | loss: 0.83584 | eval_custom_logloss: 0.93179 |  0:18:22s
epoch 43 | loss: 0.81586 | eval_custom_logloss: 0.91864 |  0:18:47s
epoch 44 | loss: 0.80778 | eval_custom_logloss: 2.36609 |  0:19:12s
epoch 45 | loss: 0.79609 | eval_custom_logloss: 2.36325 |  0:19:38s
epoch 46 | loss: 0.79128 | eval_custom_logloss: 1.278   |  0:20:03s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 0.75488
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7281333333333334, 'Log Loss - std': 0.018108070637763207} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.83658 | eval_custom_logloss: 12.49865|  0:00:25s
epoch 1  | loss: 1.28519 | eval_custom_logloss: 10.15719|  0:00:50s
epoch 2  | loss: 1.25264 | eval_custom_logloss: 10.48755|  0:01:16s
epoch 3  | loss: 1.19512 | eval_custom_logloss: 8.19934 |  0:01:41s
epoch 4  | loss: 1.20059 | eval_custom_logloss: 5.77683 |  0:02:06s
epoch 5  | loss: 1.15415 | eval_custom_logloss: 3.4089  |  0:02:32s
epoch 6  | loss: 1.12617 | eval_custom_logloss: 2.46439 |  0:02:57s
epoch 7  | loss: 1.09719 | eval_custom_logloss: 2.3862  |  0:03:22s
epoch 8  | loss: 1.08256 | eval_custom_logloss: 4.4116  |  0:03:48s
epoch 9  | loss: 1.11019 | eval_custom_logloss: 5.73528 |  0:04:13s
epoch 10 | loss: 1.04785 | eval_custom_logloss: 3.17077 |  0:04:38s
epoch 11 | loss: 1.01751 | eval_custom_logloss: 2.21862 |  0:05:04s
epoch 12 | loss: 0.98986 | eval_custom_logloss: 2.22393 |  0:05:29s
epoch 13 | loss: 0.94338 | eval_custom_logloss: 3.01179 |  0:05:54s
epoch 14 | loss: 0.986   | eval_custom_logloss: 1.68291 |  0:06:20s
epoch 15 | loss: 0.96865 | eval_custom_logloss: 1.67298 |  0:06:45s
epoch 16 | loss: 0.96465 | eval_custom_logloss: 3.19648 |  0:07:11s
epoch 17 | loss: 0.97798 | eval_custom_logloss: 2.19694 |  0:07:36s
epoch 18 | loss: 1.01437 | eval_custom_logloss: 1.79124 |  0:08:01s
epoch 19 | loss: 1.00614 | eval_custom_logloss: 1.48104 |  0:08:27s
epoch 20 | loss: 0.9194  | eval_custom_logloss: 1.15396 |  0:08:52s
epoch 21 | loss: 0.88345 | eval_custom_logloss: 1.05742 |  0:09:18s
epoch 22 | loss: 0.86703 | eval_custom_logloss: 2.61915 |  0:09:43s
epoch 23 | loss: 0.85209 | eval_custom_logloss: 2.18361 |  0:10:09s
epoch 24 | loss: 0.85552 | eval_custom_logloss: 2.13436 |  0:10:34s
epoch 25 | loss: 0.85111 | eval_custom_logloss: 1.01781 |  0:11:00s
epoch 26 | loss: 0.83752 | eval_custom_logloss: 0.94986 |  0:11:25s
epoch 27 | loss: 0.83065 | eval_custom_logloss: 1.00047 |  0:11:51s
epoch 28 | loss: 0.8233  | eval_custom_logloss: 1.59693 |  0:12:16s
epoch 29 | loss: 0.82265 | eval_custom_logloss: 1.28971 |  0:12:42s
epoch 30 | loss: 0.81632 | eval_custom_logloss: 0.96534 |  0:13:07s
epoch 31 | loss: 0.81527 | eval_custom_logloss: 0.93305 |  0:13:33s
epoch 32 | loss: 0.81461 | eval_custom_logloss: 1.08586 |  0:13:58s
epoch 33 | loss: 0.81499 | eval_custom_logloss: 1.52966 |  0:14:23s
epoch 34 | loss: 0.78775 | eval_custom_logloss: 1.29831 |  0:14:48s
epoch 35 | loss: 0.77552 | eval_custom_logloss: 2.66273 |  0:15:14s
epoch 36 | loss: 0.7563  | eval_custom_logloss: 1.98004 |  0:15:39s
epoch 37 | loss: 0.75017 | eval_custom_logloss: 1.91928 |  0:16:04s
epoch 38 | loss: 0.75204 | eval_custom_logloss: 1.67423 |  0:16:29s
epoch 39 | loss: 0.74551 | eval_custom_logloss: 1.7016  |  0:16:55s
epoch 40 | loss: 0.72941 | eval_custom_logloss: 1.75053 |  0:17:20s
epoch 41 | loss: 0.74339 | eval_custom_logloss: 2.06953 |  0:17:45s
epoch 42 | loss: 0.72486 | eval_custom_logloss: 1.44761 |  0:18:11s
epoch 43 | loss: 0.71726 | eval_custom_logloss: 1.56509 |  0:18:36s
epoch 44 | loss: 0.74733 | eval_custom_logloss: 1.49152 |  0:19:02s
epoch 45 | loss: 0.72277 | eval_custom_logloss: 1.89878 |  0:19:28s
epoch 46 | loss: 0.7296  | eval_custom_logloss: 1.56985 |  0:19:53s
epoch 47 | loss: 0.72356 | eval_custom_logloss: 1.81001 |  0:20:18s
epoch 48 | loss: 0.73601 | eval_custom_logloss: 2.35794 |  0:20:44s
epoch 49 | loss: 0.71938 | eval_custom_logloss: 1.09669 |  0:21:09s
epoch 50 | loss: 0.72513 | eval_custom_logloss: 1.13323 |  0:21:35s
epoch 51 | loss: 0.70793 | eval_custom_logloss: 0.92046 |  0:22:00s
epoch 52 | loss: 0.72376 | eval_custom_logloss: 0.78371 |  0:22:26s
epoch 53 | loss: 0.69868 | eval_custom_logloss: 1.26323 |  0:22:51s
epoch 54 | loss: 0.72995 | eval_custom_logloss: 0.84075 |  0:23:16s
epoch 55 | loss: 0.70913 | eval_custom_logloss: 3.37153 |  0:23:42s
epoch 56 | loss: 0.69471 | eval_custom_logloss: 0.75405 |  0:24:07s
epoch 57 | loss: 0.7105  | eval_custom_logloss: 1.76226 |  0:24:33s
epoch 58 | loss: 0.69789 | eval_custom_logloss: 1.20686 |  0:24:58s
epoch 59 | loss: 0.69288 | eval_custom_logloss: 1.6168  |  0:25:23s
epoch 60 | loss: 0.6861  | eval_custom_logloss: 1.14638 |  0:25:49s
epoch 61 | loss: 0.68652 | eval_custom_logloss: 0.82052 |  0:26:14s
epoch 62 | loss: 0.6784  | eval_custom_logloss: 1.50176 |  0:26:39s
epoch 63 | loss: 0.67042 | eval_custom_logloss: 1.13992 |  0:27:05s
epoch 64 | loss: 0.6888  | eval_custom_logloss: 1.23834 |  0:27:30s
epoch 65 | loss: 0.67599 | eval_custom_logloss: 0.78513 |  0:27:55s
epoch 66 | loss: 0.67264 | eval_custom_logloss: 0.85252 |  0:28:20s
epoch 67 | loss: 0.66815 | eval_custom_logloss: 1.19677 |  0:28:45s
epoch 68 | loss: 0.66817 | eval_custom_logloss: 0.74061 |  0:29:11s
epoch 69 | loss: 0.67304 | eval_custom_logloss: 1.08872 |  0:29:36s
epoch 70 | loss: 0.66153 | eval_custom_logloss: 2.53323 |  0:30:02s
epoch 71 | loss: 0.65638 | eval_custom_logloss: 0.7929  |  0:30:27s
epoch 72 | loss: 0.65742 | eval_custom_logloss: 1.02845 |  0:30:52s
epoch 73 | loss: 0.67833 | eval_custom_logloss: 0.95218 |  0:31:17s
epoch 74 | loss: 0.64852 | eval_custom_logloss: 0.92038 |  0:31:42s
epoch 75 | loss: 0.65529 | eval_custom_logloss: 0.87254 |  0:32:07s
epoch 76 | loss: 0.64535 | eval_custom_logloss: 1.03175 |  0:32:31s
epoch 77 | loss: 0.65476 | eval_custom_logloss: 1.27028 |  0:32:55s
epoch 78 | loss: 0.65172 | eval_custom_logloss: 0.90752 |  0:33:19s
epoch 79 | loss: 0.64568 | eval_custom_logloss: 0.78547 |  0:33:43s
epoch 80 | loss: 0.67085 | eval_custom_logloss: 1.77753 |  0:34:07s
epoch 81 | loss: 0.66515 | eval_custom_logloss: 0.89862 |  0:34:31s
epoch 82 | loss: 0.67824 | eval_custom_logloss: 2.72699 |  0:34:54s
epoch 83 | loss: 0.65243 | eval_custom_logloss: 0.67619 |  0:35:18s
epoch 84 | loss: 0.64509 | eval_custom_logloss: 1.41785 |  0:35:42s
epoch 85 | loss: 0.64364 | eval_custom_logloss: 0.87402 |  0:36:06s
epoch 86 | loss: 0.64114 | eval_custom_logloss: 1.00685 |  0:36:30s
epoch 87 | loss: 0.63457 | eval_custom_logloss: 0.92479 |  0:36:54s
epoch 88 | loss: 0.63947 | eval_custom_logloss: 0.91104 |  0:37:18s
epoch 89 | loss: 0.64529 | eval_custom_logloss: 0.99539 |  0:37:42s
epoch 90 | loss: 0.66612 | eval_custom_logloss: 0.68541 |  0:38:06s
epoch 91 | loss: 0.66892 | eval_custom_logloss: 1.45123 |  0:38:31s
epoch 92 | loss: 0.65823 | eval_custom_logloss: 1.71922 |  0:38:56s
epoch 93 | loss: 0.66912 | eval_custom_logloss: 1.99839 |  0:39:20s
epoch 94 | loss: 0.65913 | eval_custom_logloss: 1.72955 |  0:39:45s
epoch 95 | loss: 0.64706 | eval_custom_logloss: 0.68519 |  0:40:10s
epoch 96 | loss: 0.65807 | eval_custom_logloss: 0.81009 |  0:40:35s
epoch 97 | loss: 0.66689 | eval_custom_logloss: 0.82252 |  0:40:59s
epoch 98 | loss: 0.65318 | eval_custom_logloss: 0.70504 |  0:41:24s
epoch 99 | loss: 0.65603 | eval_custom_logloss: 1.49827 |  0:41:49s
Stop training because you reached max_epochs = 100 with best_epoch = 83 and best_eval_custom_logloss = 0.67619
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.71495, 'Log Loss - std': 0.02770067688703653} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.81287 | eval_custom_logloss: 12.22302|  0:00:25s
epoch 1  | loss: 1.30137 | eval_custom_logloss: 7.50385 |  0:00:50s
epoch 2  | loss: 1.22465 | eval_custom_logloss: 6.80254 |  0:01:15s
epoch 3  | loss: 1.17073 | eval_custom_logloss: 2.93906 |  0:01:40s
epoch 4  | loss: 1.14292 | eval_custom_logloss: 3.63132 |  0:02:05s
epoch 5  | loss: 1.07816 | eval_custom_logloss: 5.34224 |  0:02:30s
epoch 6  | loss: 1.03501 | eval_custom_logloss: 4.19227 |  0:02:55s
epoch 7  | loss: 1.03066 | eval_custom_logloss: 2.27505 |  0:03:20s
epoch 8  | loss: 1.02997 | eval_custom_logloss: 2.37707 |  0:03:45s
epoch 9  | loss: 1.02885 | eval_custom_logloss: 2.73511 |  0:04:10s
epoch 10 | loss: 0.97767 | eval_custom_logloss: 2.04309 |  0:04:35s
epoch 11 | loss: 0.95951 | eval_custom_logloss: 1.85512 |  0:05:00s
epoch 12 | loss: 0.93039 | eval_custom_logloss: 2.29359 |  0:05:25s
epoch 13 | loss: 0.92808 | eval_custom_logloss: 1.68408 |  0:05:50s
epoch 14 | loss: 0.90762 | eval_custom_logloss: 1.9761  |  0:06:15s
epoch 15 | loss: 0.87751 | eval_custom_logloss: 2.25905 |  0:06:40s
epoch 16 | loss: 0.881   | eval_custom_logloss: 2.14476 |  0:07:05s
epoch 17 | loss: 0.84207 | eval_custom_logloss: 1.09276 |  0:07:30s
epoch 18 | loss: 0.84403 | eval_custom_logloss: 1.43901 |  0:07:55s
epoch 19 | loss: 0.85596 | eval_custom_logloss: 1.46314 |  0:08:20s
epoch 20 | loss: 0.82945 | eval_custom_logloss: 1.10392 |  0:08:45s
epoch 21 | loss: 0.81455 | eval_custom_logloss: 1.05632 |  0:09:09s
epoch 22 | loss: 0.80673 | eval_custom_logloss: 0.89191 |  0:09:34s
epoch 23 | loss: 0.81542 | eval_custom_logloss: 0.89888 |  0:09:59s
epoch 24 | loss: 0.81801 | eval_custom_logloss: 1.02305 |  0:10:25s
epoch 25 | loss: 0.7989  | eval_custom_logloss: 0.93165 |  0:10:50s
epoch 26 | loss: 0.78519 | eval_custom_logloss: 0.7971  |  0:11:15s
epoch 27 | loss: 0.77543 | eval_custom_logloss: 0.80435 |  0:11:40s
epoch 28 | loss: 0.77803 | eval_custom_logloss: 0.81063 |  0:12:05s
epoch 29 | loss: 0.78582 | eval_custom_logloss: 1.74868 |  0:12:30s
epoch 30 | loss: 0.78158 | eval_custom_logloss: 1.16684 |  0:12:55s
epoch 31 | loss: 0.80224 | eval_custom_logloss: 1.09061 |  0:13:19s
epoch 32 | loss: 0.77724 | eval_custom_logloss: 1.10531 |  0:13:44s
epoch 33 | loss: 0.77716 | eval_custom_logloss: 1.30339 |  0:14:09s
epoch 34 | loss: 0.7695  | eval_custom_logloss: 0.89311 |  0:14:34s
epoch 35 | loss: 0.77593 | eval_custom_logloss: 0.9777  |  0:14:59s
epoch 36 | loss: 0.75604 | eval_custom_logloss: 0.72384 |  0:15:24s
epoch 37 | loss: 0.75306 | eval_custom_logloss: 1.06452 |  0:15:49s
epoch 38 | loss: 0.75829 | eval_custom_logloss: 1.1458  |  0:16:13s
epoch 39 | loss: 0.74738 | eval_custom_logloss: 0.94193 |  0:16:38s
epoch 40 | loss: 0.75705 | eval_custom_logloss: 1.90423 |  0:17:03s
epoch 41 | loss: 0.76925 | eval_custom_logloss: 0.99607 |  0:17:28s
epoch 42 | loss: 0.75518 | eval_custom_logloss: 1.09049 |  0:17:53s
epoch 43 | loss: 0.74228 | eval_custom_logloss: 1.10919 |  0:18:18s
epoch 44 | loss: 0.73262 | eval_custom_logloss: 1.03744 |  0:18:43s
epoch 45 | loss: 0.73197 | eval_custom_logloss: 1.10293 |  0:19:08s
epoch 46 | loss: 0.73173 | eval_custom_logloss: 0.99753 |  0:19:33s
epoch 47 | loss: 0.74844 | eval_custom_logloss: 1.10256 |  0:19:58s
epoch 48 | loss: 0.71633 | eval_custom_logloss: 0.7976  |  0:20:23s
epoch 49 | loss: 0.72787 | eval_custom_logloss: 1.47037 |  0:20:48s
epoch 50 | loss: 0.71532 | eval_custom_logloss: 1.00432 |  0:21:13s
epoch 51 | loss: 0.7198  | eval_custom_logloss: 0.85572 |  0:21:38s
epoch 52 | loss: 0.72445 | eval_custom_logloss: 0.68279 |  0:22:03s
epoch 53 | loss: 0.7139  | eval_custom_logloss: 0.67723 |  0:22:28s
epoch 54 | loss: 0.70217 | eval_custom_logloss: 0.67161 |  0:22:53s
epoch 55 | loss: 0.71696 | eval_custom_logloss: 0.9692  |  0:23:18s
epoch 56 | loss: 0.69739 | eval_custom_logloss: 0.82419 |  0:23:44s
epoch 57 | loss: 0.68813 | eval_custom_logloss: 0.9317  |  0:24:09s
epoch 58 | loss: 0.69525 | eval_custom_logloss: 0.68523 |  0:24:33s
epoch 59 | loss: 0.70523 | eval_custom_logloss: 1.98978 |  0:24:59s
epoch 60 | loss: 0.69883 | eval_custom_logloss: 0.99797 |  0:25:24s
epoch 61 | loss: 0.69897 | eval_custom_logloss: 0.73433 |  0:25:48s
epoch 62 | loss: 0.67892 | eval_custom_logloss: 1.12222 |  0:26:13s
epoch 63 | loss: 0.70084 | eval_custom_logloss: 0.86398 |  0:26:38s
epoch 64 | loss: 0.69106 | eval_custom_logloss: 0.91886 |  0:27:04s
epoch 65 | loss: 0.69241 | eval_custom_logloss: 0.85036 |  0:27:29s
epoch 66 | loss: 0.67915 | eval_custom_logloss: 1.38438 |  0:27:53s
epoch 67 | loss: 0.67923 | eval_custom_logloss: 0.77821 |  0:28:17s
epoch 68 | loss: 0.66275 | eval_custom_logloss: 0.68218 |  0:28:41s
epoch 69 | loss: 0.67712 | eval_custom_logloss: 0.96108 |  0:29:05s
epoch 70 | loss: 0.66584 | eval_custom_logloss: 1.09023 |  0:29:30s
epoch 71 | loss: 0.67138 | eval_custom_logloss: 1.23413 |  0:29:55s
epoch 72 | loss: 0.6704  | eval_custom_logloss: 0.71348 |  0:30:20s
epoch 73 | loss: 0.68735 | eval_custom_logloss: 2.80495 |  0:30:45s
epoch 74 | loss: 0.67109 | eval_custom_logloss: 0.84561 |  0:31:10s

Early stopping occurred at epoch 74 with best_epoch = 54 and best_eval_custom_logloss = 0.67161
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.70586, 'Log Loss - std': 0.03073067522850743} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 0.70586 and parameters: {'n_d': 51, 'n_steps': 9, 'gamma': 1.625630799277245, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0018668086779116187, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71789 | eval_custom_logloss: 12.10281|  0:00:25s
epoch 1  | loss: 1.20369 | eval_custom_logloss: 9.58464 |  0:00:50s
epoch 2  | loss: 1.08573 | eval_custom_logloss: 8.99882 |  0:01:15s
epoch 3  | loss: 1.06163 | eval_custom_logloss: 8.71977 |  0:01:40s
epoch 4  | loss: 1.04025 | eval_custom_logloss: 7.3333  |  0:02:05s
epoch 5  | loss: 1.01583 | eval_custom_logloss: 6.14004 |  0:02:30s
epoch 6  | loss: 0.96954 | eval_custom_logloss: 4.41897 |  0:02:55s
epoch 7  | loss: 0.97158 | eval_custom_logloss: 2.21521 |  0:03:20s
epoch 8  | loss: 0.97434 | eval_custom_logloss: 3.82037 |  0:03:45s
epoch 9  | loss: 0.95752 | eval_custom_logloss: 3.46506 |  0:04:11s
epoch 10 | loss: 0.93393 | eval_custom_logloss: 1.64832 |  0:04:35s
epoch 11 | loss: 0.91292 | eval_custom_logloss: 1.11798 |  0:05:01s
epoch 12 | loss: 0.89769 | eval_custom_logloss: 1.60695 |  0:05:26s
epoch 13 | loss: 0.8841  | eval_custom_logloss: 2.42543 |  0:05:51s
epoch 14 | loss: 0.8868  | eval_custom_logloss: 1.19419 |  0:06:16s
epoch 15 | loss: 0.86372 | eval_custom_logloss: 1.78973 |  0:06:41s
epoch 16 | loss: 0.85872 | eval_custom_logloss: 2.13305 |  0:07:06s
epoch 17 | loss: 0.84586 | eval_custom_logloss: 2.81463 |  0:07:31s
epoch 18 | loss: 0.86039 | eval_custom_logloss: 1.46686 |  0:07:56s
epoch 19 | loss: 0.84119 | eval_custom_logloss: 2.82002 |  0:08:21s
epoch 20 | loss: 0.79184 | eval_custom_logloss: 1.97093 |  0:08:46s
epoch 21 | loss: 0.79728 | eval_custom_logloss: 2.56303 |  0:09:11s
epoch 22 | loss: 0.76539 | eval_custom_logloss: 1.50409 |  0:09:36s
epoch 23 | loss: 0.77238 | eval_custom_logloss: 2.49254 |  0:10:02s
epoch 24 | loss: 0.7678  | eval_custom_logloss: 2.35784 |  0:10:27s
epoch 25 | loss: 0.76159 | eval_custom_logloss: 1.3141  |  0:10:52s
epoch 26 | loss: 0.74773 | eval_custom_logloss: 1.76896 |  0:11:17s
epoch 27 | loss: 0.74134 | eval_custom_logloss: 2.12661 |  0:11:42s
epoch 28 | loss: 0.73235 | eval_custom_logloss: 1.00258 |  0:12:07s
epoch 29 | loss: 0.74984 | eval_custom_logloss: 0.98571 |  0:12:32s
epoch 30 | loss: 0.7307  | eval_custom_logloss: 4.02621 |  0:12:57s
epoch 31 | loss: 0.72431 | eval_custom_logloss: 0.92798 |  0:13:22s
epoch 32 | loss: 0.71007 | eval_custom_logloss: 1.08032 |  0:13:47s
epoch 33 | loss: 0.71716 | eval_custom_logloss: 7.0464  |  0:14:12s
epoch 34 | loss: 0.72499 | eval_custom_logloss: 1.56054 |  0:14:37s
epoch 35 | loss: 0.7329  | eval_custom_logloss: 2.15083 |  0:15:02s
epoch 36 | loss: 0.7049  | eval_custom_logloss: 1.97292 |  0:15:27s
epoch 37 | loss: 0.72103 | eval_custom_logloss: 0.7245  |  0:15:52s
epoch 38 | loss: 0.70856 | eval_custom_logloss: 1.07081 |  0:16:17s
epoch 39 | loss: 0.70964 | eval_custom_logloss: 0.89608 |  0:16:42s
epoch 40 | loss: 0.72113 | eval_custom_logloss: 1.22195 |  0:17:07s
epoch 41 | loss: 0.72305 | eval_custom_logloss: 1.10119 |  0:17:32s
epoch 42 | loss: 0.69768 | eval_custom_logloss: 1.94469 |  0:17:57s
epoch 43 | loss: 0.70078 | eval_custom_logloss: 0.84655 |  0:18:22s
epoch 44 | loss: 0.71416 | eval_custom_logloss: 1.50496 |  0:18:48s
epoch 45 | loss: 0.71258 | eval_custom_logloss: 1.33798 |  0:19:13s
epoch 46 | loss: 0.69067 | eval_custom_logloss: 1.71573 |  0:19:38s
epoch 47 | loss: 0.70316 | eval_custom_logloss: 2.40049 |  0:20:03s
epoch 48 | loss: 0.71353 | eval_custom_logloss: 2.46459 |  0:20:28s
epoch 49 | loss: 0.69673 | eval_custom_logloss: 1.92689 |  0:20:53s
epoch 50 | loss: 0.71081 | eval_custom_logloss: 0.84776 |  0:21:19s
epoch 51 | loss: 0.69765 | eval_custom_logloss: 1.68179 |  0:21:44s
epoch 52 | loss: 0.69238 | eval_custom_logloss: 2.36728 |  0:22:09s
epoch 53 | loss: 0.68286 | eval_custom_logloss: 4.35104 |  0:22:35s
epoch 54 | loss: 0.67116 | eval_custom_logloss: 2.51324 |  0:23:00s
epoch 55 | loss: 0.68902 | eval_custom_logloss: 1.86511 |  0:23:25s
epoch 56 | loss: 0.69106 | eval_custom_logloss: 0.83532 |  0:23:50s
epoch 57 | loss: 0.68803 | eval_custom_logloss: 1.73183 |  0:24:15s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 0.7245
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7229, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.80737 | eval_custom_logloss: 11.43856|  0:00:25s
epoch 1  | loss: 1.253   | eval_custom_logloss: 11.02644|  0:00:50s
epoch 2  | loss: 1.12983 | eval_custom_logloss: 8.84094 |  0:01:15s
epoch 3  | loss: 1.05679 | eval_custom_logloss: 6.69266 |  0:01:40s
epoch 4  | loss: 1.03586 | eval_custom_logloss: 4.85317 |  0:02:06s
epoch 5  | loss: 1.00149 | eval_custom_logloss: 3.70613 |  0:02:31s
epoch 6  | loss: 1.00715 | eval_custom_logloss: 3.14525 |  0:02:56s
epoch 7  | loss: 0.96367 | eval_custom_logloss: 3.7398  |  0:03:22s
epoch 8  | loss: 1.00627 | eval_custom_logloss: 2.89869 |  0:03:47s
epoch 9  | loss: 0.92134 | eval_custom_logloss: 3.55591 |  0:04:12s
epoch 10 | loss: 0.88501 | eval_custom_logloss: 3.1806  |  0:04:37s
epoch 11 | loss: 0.87081 | eval_custom_logloss: 1.75463 |  0:05:03s
epoch 12 | loss: 0.83067 | eval_custom_logloss: 1.43455 |  0:05:28s
epoch 13 | loss: 0.80931 | eval_custom_logloss: 1.18149 |  0:05:53s
epoch 14 | loss: 0.81002 | eval_custom_logloss: 1.23709 |  0:06:18s
epoch 15 | loss: 0.78751 | eval_custom_logloss: 1.80336 |  0:06:44s
epoch 16 | loss: 0.80618 | eval_custom_logloss: 2.03215 |  0:07:09s
epoch 17 | loss: 0.7554  | eval_custom_logloss: 1.58179 |  0:07:34s
epoch 18 | loss: 0.76926 | eval_custom_logloss: 1.68471 |  0:07:59s
epoch 19 | loss: 0.76489 | eval_custom_logloss: 0.9855  |  0:08:25s
epoch 20 | loss: 0.74764 | eval_custom_logloss: 2.48046 |  0:08:50s
epoch 21 | loss: 0.74575 | eval_custom_logloss: 1.08398 |  0:09:15s
epoch 22 | loss: 0.72738 | eval_custom_logloss: 1.00339 |  0:09:41s
epoch 23 | loss: 0.74152 | eval_custom_logloss: 1.48057 |  0:10:06s
epoch 24 | loss: 0.71954 | eval_custom_logloss: 0.78461 |  0:10:31s
epoch 25 | loss: 0.7346  | eval_custom_logloss: 0.93031 |  0:10:57s
epoch 26 | loss: 0.71495 | eval_custom_logloss: 0.87304 |  0:11:22s
epoch 27 | loss: 0.70486 | eval_custom_logloss: 2.21577 |  0:11:47s
epoch 28 | loss: 0.71571 | eval_custom_logloss: 1.35818 |  0:12:13s
epoch 29 | loss: 0.71439 | eval_custom_logloss: 1.1067  |  0:12:38s
epoch 30 | loss: 0.71829 | eval_custom_logloss: 1.21704 |  0:13:04s
epoch 31 | loss: 0.70776 | eval_custom_logloss: 0.93984 |  0:13:29s
epoch 32 | loss: 0.73903 | eval_custom_logloss: 1.16571 |  0:13:54s
epoch 33 | loss: 0.7008  | eval_custom_logloss: 1.05754 |  0:14:19s
epoch 34 | loss: 0.68042 | eval_custom_logloss: 1.32927 |  0:14:45s
epoch 35 | loss: 0.69865 | eval_custom_logloss: 0.9427  |  0:15:10s
epoch 36 | loss: 0.68703 | eval_custom_logloss: 1.05318 |  0:15:35s
epoch 37 | loss: 0.69812 | eval_custom_logloss: 1.99617 |  0:16:00s
epoch 38 | loss: 0.67709 | eval_custom_logloss: 1.05537 |  0:16:26s
epoch 39 | loss: 0.66732 | eval_custom_logloss: 1.07041 |  0:16:51s
epoch 40 | loss: 0.67279 | eval_custom_logloss: 0.90434 |  0:17:16s
epoch 41 | loss: 0.67452 | eval_custom_logloss: 0.90077 |  0:17:42s
epoch 42 | loss: 0.65654 | eval_custom_logloss: 1.36458 |  0:18:07s
epoch 43 | loss: 0.65695 | eval_custom_logloss: 0.63423 |  0:18:33s
epoch 44 | loss: 0.66383 | eval_custom_logloss: 0.81117 |  0:18:58s
epoch 45 | loss: 0.65436 | eval_custom_logloss: 1.03648 |  0:19:23s
epoch 46 | loss: 0.65159 | eval_custom_logloss: 0.95633 |  0:19:49s
epoch 47 | loss: 0.65988 | eval_custom_logloss: 0.98453 |  0:20:14s
epoch 48 | loss: 0.64858 | eval_custom_logloss: 0.88522 |  0:20:39s
epoch 49 | loss: 0.65339 | eval_custom_logloss: 0.79857 |  0:21:05s
epoch 50 | loss: 0.65624 | eval_custom_logloss: 0.92823 |  0:21:30s
epoch 51 | loss: 0.65244 | eval_custom_logloss: 0.91115 |  0:21:55s
epoch 52 | loss: 0.64302 | eval_custom_logloss: 0.69694 |  0:22:20s
epoch 53 | loss: 0.64452 | eval_custom_logloss: 0.91527 |  0:22:46s
epoch 54 | loss: 0.64245 | eval_custom_logloss: 0.81099 |  0:23:11s
epoch 55 | loss: 0.65147 | eval_custom_logloss: 0.9054  |  0:23:36s
epoch 56 | loss: 0.63527 | eval_custom_logloss: 0.91378 |  0:24:01s
epoch 57 | loss: 0.63777 | eval_custom_logloss: 0.86881 |  0:24:27s
epoch 58 | loss: 0.65073 | eval_custom_logloss: 0.92228 |  0:24:53s
epoch 59 | loss: 0.64714 | eval_custom_logloss: 1.19685 |  0:25:18s
epoch 60 | loss: 0.6316  | eval_custom_logloss: 1.03646 |  0:25:43s
epoch 61 | loss: 0.63456 | eval_custom_logloss: 0.81655 |  0:26:09s
epoch 62 | loss: 0.63826 | eval_custom_logloss: 0.97308 |  0:26:34s
epoch 63 | loss: 0.63034 | eval_custom_logloss: 0.84937 |  0:26:59s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.63423
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6781999999999999, 'Log Loss - std': 0.04470000000000002} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.80128 | eval_custom_logloss: 13.20865|  0:00:25s
epoch 1  | loss: 1.21044 | eval_custom_logloss: 11.03708|  0:00:50s
epoch 2  | loss: 1.13776 | eval_custom_logloss: 9.11846 |  0:01:15s
epoch 3  | loss: 1.08663 | eval_custom_logloss: 6.39439 |  0:01:41s
epoch 4  | loss: 1.02016 | eval_custom_logloss: 5.90337 |  0:02:06s
epoch 5  | loss: 1.00182 | eval_custom_logloss: 3.38183 |  0:02:31s
epoch 6  | loss: 0.96454 | eval_custom_logloss: 4.78977 |  0:02:56s
epoch 7  | loss: 0.95837 | eval_custom_logloss: 4.6611  |  0:03:22s
epoch 8  | loss: 0.93411 | eval_custom_logloss: 5.88075 |  0:03:47s
epoch 9  | loss: 0.90025 | eval_custom_logloss: 4.94379 |  0:04:12s
epoch 10 | loss: 0.89241 | eval_custom_logloss: 4.21402 |  0:04:38s
epoch 11 | loss: 0.85709 | eval_custom_logloss: 2.06496 |  0:05:03s
epoch 12 | loss: 0.8529  | eval_custom_logloss: 2.87083 |  0:05:28s
epoch 13 | loss: 0.82352 | eval_custom_logloss: 1.70779 |  0:05:54s
epoch 14 | loss: 0.81508 | eval_custom_logloss: 1.58275 |  0:06:19s
epoch 15 | loss: 0.79562 | eval_custom_logloss: 1.65489 |  0:06:44s
epoch 16 | loss: 0.80042 | eval_custom_logloss: 3.51305 |  0:07:09s
epoch 17 | loss: 0.78717 | eval_custom_logloss: 2.55584 |  0:07:34s
epoch 18 | loss: 0.76832 | eval_custom_logloss: 1.26473 |  0:07:59s
epoch 19 | loss: 0.76651 | eval_custom_logloss: 3.45839 |  0:08:25s
epoch 20 | loss: 0.76319 | eval_custom_logloss: 1.1295  |  0:08:50s
epoch 21 | loss: 0.76816 | eval_custom_logloss: 2.54816 |  0:09:15s
epoch 22 | loss: 0.73794 | eval_custom_logloss: 1.25783 |  0:09:41s
epoch 23 | loss: 0.73974 | eval_custom_logloss: 1.6436  |  0:10:06s
epoch 24 | loss: 0.74667 | eval_custom_logloss: 1.17806 |  0:10:31s
epoch 25 | loss: 0.72217 | eval_custom_logloss: 0.88795 |  0:10:56s
epoch 26 | loss: 0.70533 | eval_custom_logloss: 1.06872 |  0:11:21s
epoch 27 | loss: 0.70145 | eval_custom_logloss: 1.41679 |  0:11:47s
epoch 28 | loss: 0.71166 | eval_custom_logloss: 1.26405 |  0:12:12s
epoch 29 | loss: 0.70817 | eval_custom_logloss: 1.32646 |  0:12:37s
epoch 30 | loss: 0.69683 | eval_custom_logloss: 1.27728 |  0:13:03s
epoch 31 | loss: 0.69685 | eval_custom_logloss: 2.72688 |  0:13:28s
epoch 32 | loss: 0.70659 | eval_custom_logloss: 1.85264 |  0:13:53s
epoch 33 | loss: 0.69974 | eval_custom_logloss: 1.98206 |  0:14:19s
epoch 34 | loss: 0.72177 | eval_custom_logloss: 4.44664 |  0:14:44s
epoch 35 | loss: 0.71024 | eval_custom_logloss: 1.33557 |  0:15:09s
epoch 36 | loss: 0.68663 | eval_custom_logloss: 0.97506 |  0:15:34s
epoch 37 | loss: 0.70171 | eval_custom_logloss: 0.97148 |  0:15:59s
epoch 38 | loss: 0.69223 | eval_custom_logloss: 3.18546 |  0:16:25s
epoch 39 | loss: 0.68806 | eval_custom_logloss: 1.90368 |  0:16:50s
epoch 40 | loss: 0.67132 | eval_custom_logloss: 0.87322 |  0:17:15s
epoch 41 | loss: 0.70177 | eval_custom_logloss: 1.29318 |  0:17:41s
epoch 42 | loss: 0.66031 | eval_custom_logloss: 1.32651 |  0:18:06s
epoch 43 | loss: 0.67083 | eval_custom_logloss: 1.83867 |  0:18:31s
epoch 44 | loss: 0.6772  | eval_custom_logloss: 1.47305 |  0:18:57s
epoch 45 | loss: 0.68105 | eval_custom_logloss: 0.94046 |  0:19:22s
epoch 46 | loss: 0.66396 | eval_custom_logloss: 1.21828 |  0:19:47s
epoch 47 | loss: 0.6654  | eval_custom_logloss: 3.23984 |  0:20:12s
epoch 48 | loss: 0.66429 | eval_custom_logloss: 1.21435 |  0:20:38s
epoch 49 | loss: 0.65836 | eval_custom_logloss: 2.20899 |  0:21:03s
epoch 50 | loss: 0.65194 | eval_custom_logloss: 1.1088  |  0:21:28s
epoch 51 | loss: 0.6745  | eval_custom_logloss: 2.4843  |  0:21:54s
epoch 52 | loss: 0.67549 | eval_custom_logloss: 0.93269 |  0:22:20s
epoch 53 | loss: 0.66173 | eval_custom_logloss: 1.61795 |  0:22:45s
epoch 54 | loss: 0.66153 | eval_custom_logloss: 0.99579 |  0:23:10s
epoch 55 | loss: 0.66231 | eval_custom_logloss: 3.43906 |  0:23:35s
epoch 56 | loss: 0.64429 | eval_custom_logloss: 1.30597 |  0:24:01s
epoch 57 | loss: 0.65125 | eval_custom_logloss: 1.81276 |  0:24:27s
epoch 58 | loss: 0.63967 | eval_custom_logloss: 2.40518 |  0:24:52s
epoch 59 | loss: 0.64183 | eval_custom_logloss: 1.54713 |  0:25:17s
epoch 60 | loss: 0.66243 | eval_custom_logloss: 1.18284 |  0:25:42s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.87322
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7423666666666667, 'Log Loss - std': 0.097809929057444} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.85044 | eval_custom_logloss: 11.90995|  0:00:25s
epoch 1  | loss: 1.33629 | eval_custom_logloss: 9.25925 |  0:00:50s
epoch 2  | loss: 1.16038 | eval_custom_logloss: 7.24657 |  0:01:15s
epoch 3  | loss: 1.0952  | eval_custom_logloss: 6.77428 |  0:01:40s
epoch 4  | loss: 1.03767 | eval_custom_logloss: 7.40502 |  0:02:06s
epoch 5  | loss: 0.98743 | eval_custom_logloss: 5.76419 |  0:02:31s
epoch 6  | loss: 0.95873 | eval_custom_logloss: 4.9407  |  0:02:56s
epoch 7  | loss: 0.92717 | eval_custom_logloss: 4.84519 |  0:03:21s
epoch 8  | loss: 0.91341 | eval_custom_logloss: 4.89012 |  0:03:47s
epoch 9  | loss: 0.86109 | eval_custom_logloss: 5.07127 |  0:04:12s
epoch 10 | loss: 0.8767  | eval_custom_logloss: 2.75018 |  0:04:37s
epoch 11 | loss: 0.82521 | eval_custom_logloss: 1.68788 |  0:05:02s
epoch 12 | loss: 0.82231 | eval_custom_logloss: 1.916   |  0:05:27s
epoch 13 | loss: 0.8462  | eval_custom_logloss: 4.97033 |  0:05:52s
epoch 14 | loss: 0.86425 | eval_custom_logloss: 2.63674 |  0:06:18s
epoch 15 | loss: 0.823   | eval_custom_logloss: 2.41345 |  0:06:44s
epoch 16 | loss: 0.81392 | eval_custom_logloss: 1.69097 |  0:07:10s
epoch 17 | loss: 0.80427 | eval_custom_logloss: 1.2681  |  0:07:35s
epoch 18 | loss: 0.80202 | eval_custom_logloss: 1.30326 |  0:08:00s
epoch 19 | loss: 0.78476 | eval_custom_logloss: 1.03326 |  0:08:26s
epoch 20 | loss: 0.77413 | eval_custom_logloss: 0.83396 |  0:08:51s
epoch 21 | loss: 0.77564 | eval_custom_logloss: 0.93318 |  0:09:16s
epoch 22 | loss: 0.75679 | eval_custom_logloss: 1.34269 |  0:09:42s
epoch 23 | loss: 0.73866 | eval_custom_logloss: 1.52    |  0:10:07s
epoch 24 | loss: 0.75785 | eval_custom_logloss: 1.45618 |  0:10:32s
epoch 25 | loss: 0.7588  | eval_custom_logloss: 0.97435 |  0:10:58s
epoch 26 | loss: 0.72887 | eval_custom_logloss: 0.91773 |  0:11:23s
epoch 27 | loss: 0.70683 | eval_custom_logloss: 1.17409 |  0:11:49s
epoch 28 | loss: 0.70722 | eval_custom_logloss: 0.91335 |  0:12:14s
epoch 29 | loss: 0.70915 | eval_custom_logloss: 1.09936 |  0:12:40s
epoch 30 | loss: 0.70792 | eval_custom_logloss: 0.82865 |  0:13:05s
epoch 31 | loss: 0.70809 | eval_custom_logloss: 0.96847 |  0:13:30s
epoch 32 | loss: 0.69365 | eval_custom_logloss: 0.83998 |  0:13:56s
epoch 33 | loss: 0.69062 | eval_custom_logloss: 0.9501  |  0:14:21s
epoch 34 | loss: 0.68144 | eval_custom_logloss: 1.07237 |  0:14:47s
epoch 35 | loss: 0.69466 | eval_custom_logloss: 0.9609  |  0:15:12s
epoch 36 | loss: 0.66878 | eval_custom_logloss: 0.87257 |  0:15:38s
epoch 37 | loss: 0.67628 | eval_custom_logloss: 1.0076  |  0:16:03s
epoch 38 | loss: 0.66966 | eval_custom_logloss: 0.97125 |  0:16:29s
epoch 39 | loss: 0.66748 | eval_custom_logloss: 1.15221 |  0:16:54s
epoch 40 | loss: 0.67097 | eval_custom_logloss: 1.18879 |  0:17:19s
epoch 41 | loss: 0.67724 | eval_custom_logloss: 0.86049 |  0:17:45s
epoch 42 | loss: 0.65648 | eval_custom_logloss: 0.86431 |  0:18:10s
epoch 43 | loss: 0.65408 | eval_custom_logloss: 2.32014 |  0:18:35s
epoch 44 | loss: 0.66375 | eval_custom_logloss: 1.4015  |  0:19:00s
epoch 45 | loss: 0.6507  | eval_custom_logloss: 0.96321 |  0:19:26s
epoch 46 | loss: 0.6494  | eval_custom_logloss: 1.59806 |  0:19:52s
epoch 47 | loss: 0.65691 | eval_custom_logloss: 0.91129 |  0:20:17s
epoch 48 | loss: 0.63746 | eval_custom_logloss: 1.10172 |  0:20:42s
epoch 49 | loss: 0.63336 | eval_custom_logloss: 0.90715 |  0:21:08s
epoch 50 | loss: 0.62824 | eval_custom_logloss: 0.75148 |  0:21:33s
epoch 51 | loss: 0.63055 | eval_custom_logloss: 1.11415 |  0:21:59s
epoch 52 | loss: 0.63477 | eval_custom_logloss: 1.09078 |  0:22:24s
epoch 53 | loss: 0.62603 | eval_custom_logloss: 0.8264  |  0:22:50s
epoch 54 | loss: 0.62406 | eval_custom_logloss: 0.99241 |  0:23:15s
epoch 55 | loss: 0.63867 | eval_custom_logloss: 1.62763 |  0:23:40s
epoch 56 | loss: 0.61658 | eval_custom_logloss: 1.06005 |  0:24:06s
epoch 57 | loss: 0.62174 | eval_custom_logloss: 1.20181 |  0:24:31s
epoch 58 | loss: 0.62334 | eval_custom_logloss: 0.95169 |  0:24:57s
epoch 59 | loss: 0.62472 | eval_custom_logloss: 1.26145 |  0:25:22s
epoch 60 | loss: 0.61451 | eval_custom_logloss: 1.55801 |  0:25:47s
epoch 61 | loss: 0.64891 | eval_custom_logloss: 1.22791 |  0:26:13s
epoch 62 | loss: 0.61523 | eval_custom_logloss: 0.79821 |  0:26:39s
epoch 63 | loss: 0.60457 | eval_custom_logloss: 1.49803 |  0:27:04s
epoch 64 | loss: 0.62918 | eval_custom_logloss: 0.80884 |  0:27:30s
epoch 65 | loss: 0.60998 | eval_custom_logloss: 0.92766 |  0:27:55s
epoch 66 | loss: 0.60926 | eval_custom_logloss: 1.64135 |  0:28:21s
epoch 67 | loss: 0.60182 | eval_custom_logloss: 0.60511 |  0:28:46s
epoch 68 | loss: 0.61855 | eval_custom_logloss: 1.08301 |  0:29:12s
epoch 69 | loss: 0.6069  | eval_custom_logloss: 0.66458 |  0:29:38s
epoch 70 | loss: 0.60041 | eval_custom_logloss: 0.67861 |  0:30:04s
epoch 71 | loss: 0.5983  | eval_custom_logloss: 1.20127 |  0:30:30s
epoch 72 | loss: 0.59539 | eval_custom_logloss: 1.3363  |  0:30:56s
epoch 73 | loss: 0.62067 | eval_custom_logloss: 1.06653 |  0:31:22s
epoch 74 | loss: 0.60125 | eval_custom_logloss: 2.88648 |  0:31:48s
epoch 75 | loss: 0.60241 | eval_custom_logloss: 0.91825 |  0:32:13s
epoch 76 | loss: 0.59046 | eval_custom_logloss: 1.0348  |  0:32:39s
epoch 77 | loss: 0.60123 | eval_custom_logloss: 0.85896 |  0:33:05s
epoch 78 | loss: 0.59275 | eval_custom_logloss: 0.81611 |  0:33:31s
epoch 79 | loss: 0.59841 | eval_custom_logloss: 1.02176 |  0:33:57s
epoch 80 | loss: 0.60273 | eval_custom_logloss: 0.82393 |  0:34:23s
epoch 81 | loss: 0.59261 | eval_custom_logloss: 0.6084  |  0:34:49s
epoch 82 | loss: 0.60261 | eval_custom_logloss: 0.70311 |  0:35:15s
epoch 83 | loss: 0.58653 | eval_custom_logloss: 0.71101 |  0:35:41s
epoch 84 | loss: 0.58685 | eval_custom_logloss: 0.89854 |  0:36:07s
epoch 85 | loss: 0.59381 | eval_custom_logloss: 1.28327 |  0:36:33s
epoch 86 | loss: 0.58109 | eval_custom_logloss: 1.15812 |  0:36:59s
epoch 87 | loss: 0.57522 | eval_custom_logloss: 0.80608 |  0:37:25s

Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_custom_logloss = 0.60511
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.70775, 'Log Loss - std': 0.10377874300645583} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax', 'n_a': 51, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.77801 | eval_custom_logloss: 9.76009 |  0:00:25s
epoch 1  | loss: 1.26473 | eval_custom_logloss: 8.21761 |  0:00:51s
epoch 2  | loss: 1.20841 | eval_custom_logloss: 8.69155 |  0:01:17s
epoch 3  | loss: 1.14762 | eval_custom_logloss: 5.31074 |  0:01:43s
epoch 4  | loss: 1.08649 | eval_custom_logloss: 4.84327 |  0:02:09s
epoch 5  | loss: 1.00524 | eval_custom_logloss: 3.61258 |  0:02:35s
epoch 6  | loss: 0.96316 | eval_custom_logloss: 1.87374 |  0:03:01s
epoch 7  | loss: 0.99167 | eval_custom_logloss: 5.03208 |  0:03:27s
epoch 8  | loss: 1.01444 | eval_custom_logloss: 3.83995 |  0:03:53s
epoch 9  | loss: 0.96842 | eval_custom_logloss: 3.69916 |  0:04:19s
epoch 10 | loss: 0.96126 | eval_custom_logloss: 1.89774 |  0:04:45s
epoch 11 | loss: 0.92943 | eval_custom_logloss: 1.54389 |  0:05:11s
epoch 12 | loss: 0.88077 | eval_custom_logloss: 1.59233 |  0:05:37s
epoch 13 | loss: 0.88807 | eval_custom_logloss: 1.50007 |  0:06:03s
epoch 14 | loss: 0.86335 | eval_custom_logloss: 2.1866  |  0:06:29s
epoch 15 | loss: 0.85397 | eval_custom_logloss: 1.38024 |  0:06:55s
epoch 16 | loss: 0.83648 | eval_custom_logloss: 1.69216 |  0:07:21s
epoch 17 | loss: 0.81297 | eval_custom_logloss: 1.50473 |  0:07:47s
epoch 18 | loss: 0.79487 | eval_custom_logloss: 0.92036 |  0:08:13s
epoch 19 | loss: 0.79673 | eval_custom_logloss: 1.04604 |  0:08:39s
epoch 20 | loss: 0.76753 | eval_custom_logloss: 1.83493 |  0:09:05s
epoch 21 | loss: 0.75785 | eval_custom_logloss: 1.26634 |  0:09:31s
epoch 22 | loss: 0.74992 | eval_custom_logloss: 1.38581 |  0:09:57s
epoch 23 | loss: 0.75269 | eval_custom_logloss: 2.33625 |  0:10:23s
epoch 24 | loss: 0.74745 | eval_custom_logloss: 1.01339 |  0:10:49s
epoch 25 | loss: 0.73581 | eval_custom_logloss: 1.14929 |  0:11:14s
epoch 26 | loss: 0.7262  | eval_custom_logloss: 0.97183 |  0:11:40s
epoch 27 | loss: 0.71182 | eval_custom_logloss: 0.8484  |  0:12:06s
epoch 28 | loss: 0.71528 | eval_custom_logloss: 0.99525 |  0:12:32s
epoch 29 | loss: 0.70581 | eval_custom_logloss: 0.98257 |  0:12:58s
epoch 30 | loss: 0.69606 | eval_custom_logloss: 1.24523 |  0:13:24s
epoch 31 | loss: 0.70285 | eval_custom_logloss: 1.37808 |  0:13:50s
epoch 32 | loss: 0.69285 | eval_custom_logloss: 1.63905 |  0:14:16s
epoch 33 | loss: 0.69955 | eval_custom_logloss: 1.38001 |  0:14:42s
epoch 34 | loss: 0.69172 | eval_custom_logloss: 1.41177 |  0:15:08s
epoch 35 | loss: 0.69121 | eval_custom_logloss: 0.89286 |  0:15:34s
epoch 36 | loss: 0.68278 | eval_custom_logloss: 0.9262  |  0:15:59s
epoch 37 | loss: 0.66935 | eval_custom_logloss: 1.10229 |  0:16:25s
epoch 38 | loss: 0.68352 | eval_custom_logloss: 0.69248 |  0:16:51s
epoch 39 | loss: 0.66533 | eval_custom_logloss: 2.4044  |  0:17:18s
epoch 40 | loss: 0.68358 | eval_custom_logloss: 1.04498 |  0:17:44s
epoch 41 | loss: 0.69195 | eval_custom_logloss: 0.74542 |  0:18:10s
epoch 42 | loss: 0.67544 | eval_custom_logloss: 0.92696 |  0:18:35s
epoch 43 | loss: 0.65257 | eval_custom_logloss: 0.78335 |  0:19:00s
epoch 44 | loss: 0.66079 | eval_custom_logloss: 1.34105 |  0:19:24s
epoch 45 | loss: 0.65539 | eval_custom_logloss: 1.14052 |  0:19:49s
epoch 46 | loss: 0.66489 | eval_custom_logloss: 2.20787 |  0:20:14s
epoch 47 | loss: 0.67432 | eval_custom_logloss: 0.96647 |  0:20:39s
epoch 48 | loss: 0.64849 | eval_custom_logloss: 0.8834  |  0:21:04s
epoch 49 | loss: 0.65177 | eval_custom_logloss: 1.05955 |  0:21:29s
epoch 50 | loss: 0.64281 | eval_custom_logloss: 1.16398 |  0:21:54s
epoch 51 | loss: 0.65307 | eval_custom_logloss: 0.91755 |  0:22:18s
epoch 52 | loss: 0.651   | eval_custom_logloss: 1.01843 |  0:22:43s
epoch 53 | loss: 0.6373  | eval_custom_logloss: 0.83355 |  0:23:09s
epoch 54 | loss: 0.63553 | eval_custom_logloss: 1.63881 |  0:23:33s
epoch 55 | loss: 0.65005 | eval_custom_logloss: 5.81346 |  0:23:58s
epoch 56 | loss: 0.63415 | eval_custom_logloss: 1.46396 |  0:24:23s
epoch 57 | loss: 0.63361 | eval_custom_logloss: 2.90956 |  0:24:48s
epoch 58 | loss: 0.63369 | eval_custom_logloss: 1.27566 |  0:25:13s

Early stopping occurred at epoch 58 with best_epoch = 38 and best_eval_custom_logloss = 0.69248
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.70438, 'Log Loss - std': 0.09306690926424926} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 0.70438 and parameters: {'n_d': 51, 'n_steps': 9, 'gamma': 1.6603685020671035, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017932371578793097, 'mask_type': 'entmax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.75156 | eval_custom_logloss: 1.4194  |  0:00:27s
epoch 1  | loss: 1.43709 | eval_custom_logloss: 1.31912 |  0:00:54s
epoch 2  | loss: 1.34238 | eval_custom_logloss: 1.3025  |  0:01:21s
epoch 3  | loss: 1.22166 | eval_custom_logloss: 1.09354 |  0:01:48s
epoch 4  | loss: 1.16739 | eval_custom_logloss: 1.13068 |  0:02:15s
epoch 5  | loss: 1.17253 | eval_custom_logloss: 1.13773 |  0:02:42s
epoch 6  | loss: 1.12333 | eval_custom_logloss: 1.09106 |  0:03:09s
epoch 7  | loss: 1.10682 | eval_custom_logloss: 1.01784 |  0:03:36s
epoch 8  | loss: 1.18697 | eval_custom_logloss: 1.12659 |  0:04:04s
epoch 9  | loss: 1.15361 | eval_custom_logloss: 1.06784 |  0:04:30s
epoch 10 | loss: 1.09174 | eval_custom_logloss: 1.00269 |  0:04:58s
epoch 11 | loss: 1.07912 | eval_custom_logloss: 1.07676 |  0:05:25s
epoch 12 | loss: 1.04641 | eval_custom_logloss: 0.96788 |  0:05:52s
epoch 13 | loss: 1.03271 | eval_custom_logloss: 0.98982 |  0:06:19s
epoch 14 | loss: 1.01541 | eval_custom_logloss: 0.93678 |  0:06:46s
epoch 15 | loss: 1.01011 | eval_custom_logloss: 1.02675 |  0:07:13s
epoch 16 | loss: 1.00732 | eval_custom_logloss: 0.93761 |  0:07:40s
epoch 17 | loss: 0.99766 | eval_custom_logloss: 1.10776 |  0:08:08s
epoch 18 | loss: 0.98366 | eval_custom_logloss: 0.92781 |  0:08:35s
epoch 19 | loss: 0.98407 | eval_custom_logloss: 0.95824 |  0:09:02s
epoch 20 | loss: 0.98134 | eval_custom_logloss: 0.94203 |  0:09:29s
epoch 21 | loss: 1.14625 | eval_custom_logloss: 1.01757 |  0:09:56s
epoch 22 | loss: 1.05471 | eval_custom_logloss: 1.0145  |  0:10:23s
epoch 23 | loss: 1.05566 | eval_custom_logloss: 0.99981 |  0:10:51s
epoch 24 | loss: 1.03676 | eval_custom_logloss: 0.98437 |  0:11:17s
epoch 25 | loss: 1.16628 | eval_custom_logloss: 1.13692 |  0:11:45s
epoch 26 | loss: 1.13748 | eval_custom_logloss: 1.12467 |  0:12:12s
epoch 27 | loss: 1.11374 | eval_custom_logloss: 1.0593  |  0:12:39s
epoch 28 | loss: 1.1154  | eval_custom_logloss: 1.09418 |  0:13:06s
epoch 29 | loss: 1.11331 | eval_custom_logloss: 1.08409 |  0:13:33s
epoch 30 | loss: 1.12037 | eval_custom_logloss: 1.06653 |  0:14:00s
epoch 31 | loss: 1.13265 | eval_custom_logloss: 1.16018 |  0:14:27s
epoch 32 | loss: 1.14531 | eval_custom_logloss: 1.11511 |  0:14:54s
epoch 33 | loss: 1.14188 | eval_custom_logloss: 1.11281 |  0:15:21s
epoch 34 | loss: 1.14208 | eval_custom_logloss: 1.13086 |  0:15:48s
epoch 35 | loss: 1.1569  | eval_custom_logloss: 1.13895 |  0:16:15s
epoch 36 | loss: 1.15703 | eval_custom_logloss: 1.12792 |  0:16:42s
epoch 37 | loss: 1.15146 | eval_custom_logloss: 1.11179 |  0:17:09s
epoch 38 | loss: 1.14186 | eval_custom_logloss: 1.12982 |  0:17:36s

Early stopping occurred at epoch 38 with best_epoch = 18 and best_eval_custom_logloss = 0.92781
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9262, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.76925 | eval_custom_logloss: 1.36381 |  0:00:26s
epoch 1  | loss: 1.33565 | eval_custom_logloss: 1.20536 |  0:00:53s
epoch 2  | loss: 1.30046 | eval_custom_logloss: 1.22708 |  0:01:21s
epoch 3  | loss: 1.26624 | eval_custom_logloss: 1.20822 |  0:01:48s
epoch 4  | loss: 1.21571 | eval_custom_logloss: 1.1354  |  0:02:15s
epoch 5  | loss: 1.18672 | eval_custom_logloss: 1.10099 |  0:02:42s
epoch 6  | loss: 1.17602 | eval_custom_logloss: 1.10541 |  0:03:09s
epoch 7  | loss: 1.155   | eval_custom_logloss: 1.04974 |  0:03:36s
epoch 8  | loss: 1.09988 | eval_custom_logloss: 1.00544 |  0:04:03s
epoch 9  | loss: 1.03959 | eval_custom_logloss: 0.91627 |  0:04:30s
epoch 10 | loss: 1.01833 | eval_custom_logloss: 0.8999  |  0:04:57s
epoch 11 | loss: 0.99333 | eval_custom_logloss: 0.98754 |  0:05:24s
epoch 12 | loss: 0.97021 | eval_custom_logloss: 0.84393 |  0:05:50s
epoch 13 | loss: 0.95311 | eval_custom_logloss: 0.89449 |  0:06:17s
epoch 14 | loss: 0.94192 | eval_custom_logloss: 0.8486  |  0:06:44s
epoch 15 | loss: 0.94927 | eval_custom_logloss: 0.94739 |  0:07:11s
epoch 16 | loss: 0.94147 | eval_custom_logloss: 0.8691  |  0:07:38s
epoch 17 | loss: 0.93766 | eval_custom_logloss: 0.84411 |  0:08:05s
epoch 18 | loss: 0.92923 | eval_custom_logloss: 0.94784 |  0:08:32s
epoch 19 | loss: 0.92442 | eval_custom_logloss: 0.86793 |  0:08:59s
epoch 20 | loss: 0.927   | eval_custom_logloss: 0.81206 |  0:09:26s
epoch 21 | loss: 0.92119 | eval_custom_logloss: 0.82756 |  0:09:53s
epoch 22 | loss: 0.90384 | eval_custom_logloss: 0.83166 |  0:10:20s
epoch 23 | loss: 0.91407 | eval_custom_logloss: 1.03275 |  0:10:47s
epoch 24 | loss: 0.89067 | eval_custom_logloss: 0.86589 |  0:11:14s
epoch 25 | loss: 0.90621 | eval_custom_logloss: 0.80071 |  0:11:41s
epoch 26 | loss: 0.92349 | eval_custom_logloss: 0.96891 |  0:12:08s
epoch 27 | loss: 0.90374 | eval_custom_logloss: 0.80457 |  0:12:34s
epoch 28 | loss: 0.87969 | eval_custom_logloss: 0.80174 |  0:13:01s
epoch 29 | loss: 0.87968 | eval_custom_logloss: 0.90903 |  0:13:28s
epoch 30 | loss: 0.90073 | eval_custom_logloss: 0.82816 |  0:13:55s
epoch 31 | loss: 0.88265 | eval_custom_logloss: 0.81477 |  0:14:22s
epoch 32 | loss: 0.88767 | eval_custom_logloss: 0.84274 |  0:14:49s
epoch 33 | loss: 0.8835  | eval_custom_logloss: 0.83984 |  0:15:16s
epoch 34 | loss: 0.88332 | eval_custom_logloss: 0.89012 |  0:15:43s
epoch 35 | loss: 0.87947 | eval_custom_logloss: 0.79454 |  0:16:10s
epoch 36 | loss: 0.86841 | eval_custom_logloss: 0.80669 |  0:16:37s
epoch 37 | loss: 0.87503 | eval_custom_logloss: 0.86314 |  0:17:04s
epoch 38 | loss: 0.85605 | eval_custom_logloss: 0.82918 |  0:17:31s
epoch 39 | loss: 0.86402 | eval_custom_logloss: 0.78368 |  0:17:58s
epoch 40 | loss: 0.8586  | eval_custom_logloss: 0.84651 |  0:18:26s
epoch 41 | loss: 0.86956 | eval_custom_logloss: 0.8539  |  0:18:53s
epoch 42 | loss: 0.86772 | eval_custom_logloss: 0.93244 |  0:19:20s
epoch 43 | loss: 0.86991 | eval_custom_logloss: 0.90706 |  0:19:47s
epoch 44 | loss: 0.85439 | eval_custom_logloss: 0.82382 |  0:20:14s
epoch 45 | loss: 0.84816 | eval_custom_logloss: 0.83358 |  0:20:41s
epoch 46 | loss: 0.86352 | eval_custom_logloss: 0.79622 |  0:21:09s
epoch 47 | loss: 0.84543 | eval_custom_logloss: 0.92017 |  0:21:36s
epoch 48 | loss: 0.84304 | eval_custom_logloss: 0.79138 |  0:22:04s
epoch 49 | loss: 0.84998 | eval_custom_logloss: 0.79877 |  0:22:31s
epoch 50 | loss: 0.84419 | eval_custom_logloss: 0.78397 |  0:22:58s
epoch 51 | loss: 0.83499 | eval_custom_logloss: 0.79493 |  0:23:26s
epoch 52 | loss: 0.83844 | eval_custom_logloss: 0.8247  |  0:23:53s
epoch 53 | loss: 0.83299 | eval_custom_logloss: 0.78714 |  0:24:20s
epoch 54 | loss: 0.83737 | eval_custom_logloss: 0.82952 |  0:24:47s
epoch 55 | loss: 0.8466  | eval_custom_logloss: 0.79464 |  0:25:15s
epoch 56 | loss: 0.8176  | eval_custom_logloss: 0.79489 |  0:25:42s
epoch 57 | loss: 0.81775 | eval_custom_logloss: 0.80424 |  0:26:09s
epoch 58 | loss: 0.83015 | eval_custom_logloss: 0.8191  |  0:26:37s
epoch 59 | loss: 0.82944 | eval_custom_logloss: 0.77466 |  0:27:04s
epoch 60 | loss: 0.83893 | eval_custom_logloss: 0.85149 |  0:27:31s
epoch 61 | loss: 0.82939 | eval_custom_logloss: 0.8537  |  0:27:59s
epoch 62 | loss: 0.82584 | eval_custom_logloss: 0.81216 |  0:28:26s
epoch 63 | loss: 0.84171 | eval_custom_logloss: 0.84858 |  0:28:53s
epoch 64 | loss: 0.83343 | eval_custom_logloss: 0.85918 |  0:29:21s
epoch 65 | loss: 0.83246 | eval_custom_logloss: 0.79058 |  0:29:48s
epoch 66 | loss: 0.83589 | eval_custom_logloss: 0.82344 |  0:30:15s
epoch 67 | loss: 0.83032 | eval_custom_logloss: 0.80656 |  0:30:42s
epoch 68 | loss: 0.82811 | eval_custom_logloss: 0.8686  |  0:31:10s
epoch 69 | loss: 0.82586 | eval_custom_logloss: 0.81335 |  0:31:38s
epoch 70 | loss: 0.81862 | eval_custom_logloss: 0.76456 |  0:32:06s
epoch 71 | loss: 0.82776 | eval_custom_logloss: 0.78882 |  0:32:34s
epoch 72 | loss: 0.82693 | eval_custom_logloss: 0.83648 |  0:33:01s
epoch 73 | loss: 0.82302 | eval_custom_logloss: 0.79552 |  0:33:29s
epoch 74 | loss: 0.83143 | eval_custom_logloss: 0.805   |  0:33:57s
epoch 75 | loss: 0.82225 | eval_custom_logloss: 0.80599 |  0:34:25s
epoch 76 | loss: 0.8261  | eval_custom_logloss: 0.81858 |  0:34:53s
epoch 77 | loss: 0.82473 | eval_custom_logloss: 0.77489 |  0:35:20s
epoch 78 | loss: 0.81066 | eval_custom_logloss: 0.82242 |  0:35:48s
epoch 79 | loss: 0.82533 | eval_custom_logloss: 0.82833 |  0:36:16s
epoch 80 | loss: 0.83513 | eval_custom_logloss: 0.76642 |  0:36:44s
epoch 81 | loss: 0.82828 | eval_custom_logloss: 0.88781 |  0:37:12s
epoch 82 | loss: 0.80908 | eval_custom_logloss: 0.776   |  0:37:40s
epoch 83 | loss: 0.80967 | eval_custom_logloss: 0.81025 |  0:38:08s
epoch 84 | loss: 0.81085 | eval_custom_logloss: 0.79984 |  0:38:35s
epoch 85 | loss: 0.81624 | eval_custom_logloss: 0.79769 |  0:39:03s
epoch 86 | loss: 0.82483 | eval_custom_logloss: 1.17618 |  0:39:31s
epoch 87 | loss: 0.81085 | eval_custom_logloss: 0.86403 |  0:39:58s
epoch 88 | loss: 0.81722 | eval_custom_logloss: 0.7633  |  0:40:26s
epoch 89 | loss: 0.8055  | eval_custom_logloss: 0.82299 |  0:40:54s
epoch 90 | loss: 0.79888 | eval_custom_logloss: 0.9424  |  0:41:22s
epoch 91 | loss: 0.81191 | eval_custom_logloss: 0.79899 |  0:41:49s
epoch 92 | loss: 0.81301 | eval_custom_logloss: 0.78052 |  0:42:18s
epoch 93 | loss: 0.81004 | eval_custom_logloss: 0.84045 |  0:42:46s
epoch 94 | loss: 0.79792 | eval_custom_logloss: 0.83525 |  0:43:13s
epoch 95 | loss: 0.80432 | eval_custom_logloss: 0.79422 |  0:43:41s
epoch 96 | loss: 0.81259 | eval_custom_logloss: 0.79611 |  0:44:09s
epoch 97 | loss: 0.80982 | eval_custom_logloss: 0.9806  |  0:44:37s
epoch 98 | loss: 0.80781 | eval_custom_logloss: 0.85489 |  0:45:05s
epoch 99 | loss: 0.81164 | eval_custom_logloss: 0.80655 |  0:45:32s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 0.7633
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8441000000000001, 'Log Loss - std': 0.0821} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.75885 | eval_custom_logloss: 1.27018 |  0:00:27s
epoch 1  | loss: 1.30709 | eval_custom_logloss: 1.17209 |  0:00:55s
epoch 2  | loss: 1.2407  | eval_custom_logloss: 1.26796 |  0:01:23s
epoch 3  | loss: 1.19875 | eval_custom_logloss: 1.02766 |  0:01:50s
epoch 4  | loss: 1.11684 | eval_custom_logloss: 0.98114 |  0:02:18s
epoch 5  | loss: 1.07118 | eval_custom_logloss: 1.0878  |  0:02:46s
epoch 6  | loss: 1.0375  | eval_custom_logloss: 0.89252 |  0:03:14s
epoch 7  | loss: 1.00053 | eval_custom_logloss: 0.9135  |  0:03:42s
epoch 8  | loss: 0.99075 | eval_custom_logloss: 0.8754  |  0:04:09s
epoch 9  | loss: 0.98047 | eval_custom_logloss: 0.85768 |  0:04:37s
epoch 10 | loss: 0.96478 | eval_custom_logloss: 0.86344 |  0:05:05s
epoch 11 | loss: 0.96414 | eval_custom_logloss: 0.85888 |  0:05:33s
epoch 12 | loss: 0.92105 | eval_custom_logloss: 0.82297 |  0:06:01s
epoch 13 | loss: 0.92352 | eval_custom_logloss: 0.86487 |  0:06:28s
epoch 14 | loss: 0.90984 | eval_custom_logloss: 0.8057  |  0:06:56s
epoch 15 | loss: 0.90429 | eval_custom_logloss: 0.9773  |  0:07:24s
epoch 16 | loss: 0.88677 | eval_custom_logloss: 0.79327 |  0:07:51s
epoch 17 | loss: 0.89709 | eval_custom_logloss: 0.88753 |  0:08:19s
epoch 18 | loss: 0.91429 | eval_custom_logloss: 0.81013 |  0:08:46s
epoch 19 | loss: 0.90689 | eval_custom_logloss: 0.82887 |  0:09:14s
epoch 20 | loss: 1.00213 | eval_custom_logloss: 0.97594 |  0:09:42s
epoch 21 | loss: 0.96829 | eval_custom_logloss: 0.89207 |  0:10:11s
epoch 22 | loss: 0.88062 | eval_custom_logloss: 0.82761 |  0:10:38s
epoch 23 | loss: 0.85884 | eval_custom_logloss: 1.12043 |  0:11:06s
epoch 24 | loss: 0.82882 | eval_custom_logloss: 0.92149 |  0:11:33s
epoch 25 | loss: 0.83722 | eval_custom_logloss: 0.81208 |  0:12:01s
epoch 26 | loss: 0.85065 | eval_custom_logloss: 0.85463 |  0:12:29s
epoch 27 | loss: 0.84511 | eval_custom_logloss: 0.87647 |  0:12:56s
epoch 28 | loss: 0.81412 | eval_custom_logloss: 0.77167 |  0:13:24s
epoch 29 | loss: 0.80127 | eval_custom_logloss: 0.79681 |  0:13:52s
epoch 30 | loss: 0.80133 | eval_custom_logloss: 0.80649 |  0:14:20s
epoch 31 | loss: 0.79113 | eval_custom_logloss: 0.7488  |  0:14:47s
epoch 32 | loss: 0.77379 | eval_custom_logloss: 0.79515 |  0:15:15s
epoch 33 | loss: 0.78246 | eval_custom_logloss: 0.7874  |  0:15:43s
epoch 34 | loss: 0.7816  | eval_custom_logloss: 0.83668 |  0:16:10s
epoch 35 | loss: 0.77791 | eval_custom_logloss: 0.75789 |  0:16:38s
epoch 36 | loss: 0.76606 | eval_custom_logloss: 0.80862 |  0:17:05s
epoch 37 | loss: 0.77889 | eval_custom_logloss: 0.81518 |  0:17:33s
epoch 38 | loss: 0.76828 | eval_custom_logloss: 0.72351 |  0:18:01s
epoch 39 | loss: 0.75571 | eval_custom_logloss: 0.74657 |  0:18:28s
epoch 40 | loss: 0.74852 | eval_custom_logloss: 0.83036 |  0:18:56s
epoch 41 | loss: 0.75002 | eval_custom_logloss: 0.76452 |  0:19:24s
epoch 42 | loss: 0.75413 | eval_custom_logloss: 0.74857 |  0:19:52s
epoch 43 | loss: 0.73449 | eval_custom_logloss: 0.95874 |  0:20:20s
epoch 44 | loss: 0.74178 | eval_custom_logloss: 0.68511 |  0:20:48s
epoch 45 | loss: 0.73226 | eval_custom_logloss: 0.76577 |  0:21:15s
epoch 46 | loss: 0.73468 | eval_custom_logloss: 0.92739 |  0:21:43s
epoch 47 | loss: 0.72865 | eval_custom_logloss: 0.81023 |  0:22:10s
epoch 48 | loss: 0.73325 | eval_custom_logloss: 0.7651  |  0:22:38s
epoch 49 | loss: 0.7288  | eval_custom_logloss: 0.7928  |  0:23:06s
epoch 50 | loss: 0.7294  | eval_custom_logloss: 0.8037  |  0:23:34s
epoch 51 | loss: 0.71978 | eval_custom_logloss: 0.77085 |  0:24:01s
epoch 52 | loss: 0.71419 | eval_custom_logloss: 0.82056 |  0:24:29s
epoch 53 | loss: 0.71747 | eval_custom_logloss: 0.86091 |  0:24:57s
epoch 54 | loss: 0.71485 | eval_custom_logloss: 0.68332 |  0:25:24s
epoch 55 | loss: 0.71352 | eval_custom_logloss: 1.02327 |  0:25:52s
epoch 56 | loss: 0.71217 | eval_custom_logloss: 0.76039 |  0:26:20s
epoch 57 | loss: 0.70933 | eval_custom_logloss: 0.74947 |  0:26:47s
epoch 58 | loss: 0.70969 | eval_custom_logloss: 0.69508 |  0:27:15s
epoch 59 | loss: 0.70588 | eval_custom_logloss: 0.6889  |  0:27:43s
epoch 60 | loss: 0.70148 | eval_custom_logloss: 0.83982 |  0:28:10s
epoch 61 | loss: 0.7011  | eval_custom_logloss: 0.80346 |  0:28:38s
epoch 62 | loss: 0.69325 | eval_custom_logloss: 0.80323 |  0:29:05s
epoch 63 | loss: 0.70579 | eval_custom_logloss: 0.91174 |  0:29:33s
epoch 64 | loss: 0.69569 | eval_custom_logloss: 1.69609 |  0:30:00s
epoch 65 | loss: 0.68759 | eval_custom_logloss: 0.78193 |  0:30:28s
epoch 66 | loss: 0.70372 | eval_custom_logloss: 0.67757 |  0:30:56s
epoch 67 | loss: 0.6947  | eval_custom_logloss: 0.70764 |  0:31:23s
epoch 68 | loss: 0.67611 | eval_custom_logloss: 0.77601 |  0:31:51s
epoch 69 | loss: 0.68988 | eval_custom_logloss: 7.01487 |  0:32:18s
epoch 70 | loss: 0.68937 | eval_custom_logloss: 0.73008 |  0:32:46s
epoch 71 | loss: 0.69955 | eval_custom_logloss: 0.67268 |  0:33:14s
epoch 72 | loss: 0.69698 | eval_custom_logloss: 0.75669 |  0:33:41s
epoch 73 | loss: 0.69519 | eval_custom_logloss: 0.87188 |  0:34:09s
epoch 74 | loss: 0.67929 | eval_custom_logloss: 0.80516 |  0:34:36s
epoch 75 | loss: 0.68573 | eval_custom_logloss: 0.8574  |  0:35:04s
epoch 76 | loss: 0.70189 | eval_custom_logloss: 0.90141 |  0:35:31s
epoch 77 | loss: 0.68199 | eval_custom_logloss: 0.87029 |  0:35:59s
epoch 78 | loss: 0.71625 | eval_custom_logloss: 0.84898 |  0:36:27s
epoch 79 | loss: 0.70908 | eval_custom_logloss: 0.67225 |  0:36:54s
epoch 80 | loss: 0.68524 | eval_custom_logloss: 7.89508 |  0:37:22s
epoch 81 | loss: 0.68546 | eval_custom_logloss: 1.15993 |  0:37:50s
epoch 82 | loss: 0.68306 | eval_custom_logloss: 0.93956 |  0:38:18s
epoch 83 | loss: 0.68461 | eval_custom_logloss: 1.25182 |  0:38:45s
epoch 84 | loss: 0.67281 | eval_custom_logloss: 1.31568 |  0:39:13s
epoch 85 | loss: 0.67665 | eval_custom_logloss: 1.80336 |  0:39:41s
epoch 86 | loss: 0.68413 | eval_custom_logloss: 6.47058 |  0:40:08s
epoch 87 | loss: 0.69306 | eval_custom_logloss: 0.91679 |  0:40:36s
epoch 88 | loss: 0.67929 | eval_custom_logloss: 0.69187 |  0:41:04s
epoch 89 | loss: 0.66324 | eval_custom_logloss: 0.6855  |  0:41:31s
epoch 90 | loss: 0.65503 | eval_custom_logloss: 0.65597 |  0:41:59s
epoch 91 | loss: 0.6816  | eval_custom_logloss: 9.05342 |  0:42:27s
epoch 92 | loss: 0.67724 | eval_custom_logloss: 0.74102 |  0:42:54s
epoch 93 | loss: 0.66816 | eval_custom_logloss: 0.70309 |  0:43:21s
epoch 94 | loss: 0.67009 | eval_custom_logloss: 0.81574 |  0:43:49s
epoch 95 | loss: 0.66689 | eval_custom_logloss: 0.67145 |  0:44:16s
epoch 96 | loss: 0.67043 | eval_custom_logloss: 0.66751 |  0:44:44s
epoch 97 | loss: 0.66618 | eval_custom_logloss: 1.41978 |  0:45:12s
epoch 98 | loss: 0.66538 | eval_custom_logloss: 0.79433 |  0:45:39s
epoch 99 | loss: 0.66397 | eval_custom_logloss: 8.08777 |  0:46:07s
Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_eval_custom_logloss = 0.65597
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7804333333333334, 'Log Loss - std': 0.11225192896140163} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.74205 | eval_custom_logloss: 1.32375 |  0:00:27s
epoch 1  | loss: 1.28951 | eval_custom_logloss: 1.1682  |  0:00:55s
epoch 2  | loss: 1.25251 | eval_custom_logloss: 1.13343 |  0:01:23s
epoch 3  | loss: 1.20141 | eval_custom_logloss: 1.02934 |  0:01:51s
epoch 4  | loss: 1.13102 | eval_custom_logloss: 1.04972 |  0:02:18s
epoch 5  | loss: 1.11498 | eval_custom_logloss: 1.02242 |  0:02:46s
epoch 6  | loss: 1.07204 | eval_custom_logloss: 0.98793 |  0:03:14s
epoch 7  | loss: 1.05402 | eval_custom_logloss: 0.90757 |  0:03:41s
epoch 8  | loss: 1.03046 | eval_custom_logloss: 0.88362 |  0:04:09s
epoch 9  | loss: 1.01035 | eval_custom_logloss: 0.86083 |  0:04:37s
epoch 10 | loss: 1.00403 | eval_custom_logloss: 0.87718 |  0:05:04s
epoch 11 | loss: 1.00656 | eval_custom_logloss: 0.91087 |  0:05:32s
epoch 12 | loss: 0.97886 | eval_custom_logloss: 0.86321 |  0:05:59s
epoch 13 | loss: 0.99378 | eval_custom_logloss: 0.89537 |  0:06:27s
epoch 14 | loss: 0.96491 | eval_custom_logloss: 0.90589 |  0:06:55s
epoch 15 | loss: 0.9668  | eval_custom_logloss: 0.89825 |  0:07:22s
epoch 16 | loss: 0.95267 | eval_custom_logloss: 0.85193 |  0:07:50s
epoch 17 | loss: 0.9575  | eval_custom_logloss: 0.92061 |  0:08:17s
epoch 18 | loss: 0.97494 | eval_custom_logloss: 0.87812 |  0:08:45s
epoch 19 | loss: 0.93811 | eval_custom_logloss: 0.88231 |  0:09:13s
epoch 20 | loss: 0.9342  | eval_custom_logloss: 0.84352 |  0:09:41s
epoch 21 | loss: 0.95129 | eval_custom_logloss: 0.86822 |  0:10:08s
epoch 22 | loss: 0.95171 | eval_custom_logloss: 0.8401  |  0:10:36s
epoch 23 | loss: 0.94752 | eval_custom_logloss: 0.89929 |  0:11:04s
epoch 24 | loss: 0.92807 | eval_custom_logloss: 0.84441 |  0:11:31s
epoch 25 | loss: 0.94136 | eval_custom_logloss: 0.89609 |  0:11:59s
epoch 26 | loss: 0.93785 | eval_custom_logloss: 0.87383 |  0:12:26s
epoch 27 | loss: 0.92194 | eval_custom_logloss: 0.87553 |  0:12:54s
epoch 28 | loss: 0.92242 | eval_custom_logloss: 0.83651 |  0:13:22s
epoch 29 | loss: 0.91671 | eval_custom_logloss: 0.81835 |  0:13:49s
epoch 30 | loss: 0.90923 | eval_custom_logloss: 0.82818 |  0:14:17s
epoch 31 | loss: 0.92422 | eval_custom_logloss: 0.87532 |  0:14:45s
epoch 32 | loss: 0.90806 | eval_custom_logloss: 0.89095 |  0:15:12s
epoch 33 | loss: 0.90474 | eval_custom_logloss: 0.81153 |  0:15:40s
epoch 34 | loss: 0.92795 | eval_custom_logloss: 0.87162 |  0:16:07s
epoch 35 | loss: 0.89429 | eval_custom_logloss: 0.82274 |  0:16:35s
epoch 36 | loss: 0.89611 | eval_custom_logloss: 0.91497 |  0:17:02s
epoch 37 | loss: 0.90063 | eval_custom_logloss: 0.88949 |  0:17:30s
epoch 38 | loss: 0.8948  | eval_custom_logloss: 0.80192 |  0:17:57s
epoch 39 | loss: 0.88404 | eval_custom_logloss: 0.80873 |  0:18:25s
epoch 40 | loss: 0.89267 | eval_custom_logloss: 0.86383 |  0:18:53s
epoch 41 | loss: 0.88155 | eval_custom_logloss: 0.89216 |  0:19:20s
epoch 42 | loss: 0.89388 | eval_custom_logloss: 0.93263 |  0:19:48s
epoch 43 | loss: 0.87534 | eval_custom_logloss: 0.86165 |  0:20:15s
epoch 44 | loss: 0.87502 | eval_custom_logloss: 0.79982 |  0:20:43s
epoch 45 | loss: 0.88036 | eval_custom_logloss: 0.87036 |  0:21:10s
epoch 46 | loss: 0.8676  | eval_custom_logloss: 0.80467 |  0:21:38s
epoch 47 | loss: 0.87229 | eval_custom_logloss: 0.83943 |  0:22:05s
epoch 48 | loss: 0.87117 | eval_custom_logloss: 0.83504 |  0:22:33s
epoch 49 | loss: 0.87642 | eval_custom_logloss: 0.84408 |  0:23:01s
epoch 50 | loss: 0.85661 | eval_custom_logloss: 0.87266 |  0:23:28s
epoch 51 | loss: 0.86115 | eval_custom_logloss: 0.90852 |  0:23:56s
epoch 52 | loss: 0.88394 | eval_custom_logloss: 0.86132 |  0:24:24s
epoch 53 | loss: 0.87032 | eval_custom_logloss: 0.86576 |  0:24:52s
epoch 54 | loss: 0.86734 | eval_custom_logloss: 0.81507 |  0:25:19s
epoch 55 | loss: 0.86712 | eval_custom_logloss: 0.83226 |  0:25:47s
epoch 56 | loss: 0.85823 | eval_custom_logloss: 0.82795 |  0:26:15s
epoch 57 | loss: 0.86741 | eval_custom_logloss: 0.94151 |  0:26:42s
epoch 58 | loss: 0.86124 | eval_custom_logloss: 0.82595 |  0:27:10s
epoch 59 | loss: 0.85132 | eval_custom_logloss: 0.81537 |  0:27:38s
epoch 60 | loss: 0.86227 | eval_custom_logloss: 0.8189  |  0:28:06s
epoch 61 | loss: 0.86109 | eval_custom_logloss: 0.82039 |  0:28:33s
epoch 62 | loss: 0.86357 | eval_custom_logloss: 0.8175  |  0:29:01s
epoch 63 | loss: 0.8659  | eval_custom_logloss: 0.86459 |  0:29:29s
epoch 64 | loss: 0.86313 | eval_custom_logloss: 0.87088 |  0:29:57s

Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_custom_logloss = 0.79982
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7850250000000001, 'Log Loss - std': 0.09753779716089553} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.77724 | eval_custom_logloss: 1.38063 |  0:00:27s
epoch 1  | loss: 1.32119 | eval_custom_logloss: 1.1517  |  0:00:55s
epoch 2  | loss: 1.18638 | eval_custom_logloss: 1.22901 |  0:01:22s
epoch 3  | loss: 1.10683 | eval_custom_logloss: 1.05234 |  0:01:50s
epoch 4  | loss: 1.0927  | eval_custom_logloss: 1.07876 |  0:02:17s
epoch 5  | loss: 1.06534 | eval_custom_logloss: 0.99072 |  0:02:45s
epoch 6  | loss: 1.02679 | eval_custom_logloss: 0.94499 |  0:03:13s
epoch 7  | loss: 1.02083 | eval_custom_logloss: 0.94304 |  0:03:41s
epoch 8  | loss: 0.9999  | eval_custom_logloss: 0.89768 |  0:04:09s
epoch 9  | loss: 0.9804  | eval_custom_logloss: 0.97212 |  0:04:36s
epoch 10 | loss: 0.99204 | eval_custom_logloss: 0.88667 |  0:05:04s
epoch 11 | loss: 0.9567  | eval_custom_logloss: 0.90658 |  0:05:32s
epoch 12 | loss: 0.91663 | eval_custom_logloss: 0.81069 |  0:06:00s
epoch 13 | loss: 0.92213 | eval_custom_logloss: 0.86436 |  0:06:28s
epoch 14 | loss: 0.91994 | eval_custom_logloss: 0.92412 |  0:06:56s
epoch 15 | loss: 0.91753 | eval_custom_logloss: 0.87714 |  0:07:23s
epoch 16 | loss: 0.90566 | eval_custom_logloss: 0.83202 |  0:07:51s
epoch 17 | loss: 0.90933 | eval_custom_logloss: 0.83378 |  0:08:19s
epoch 18 | loss: 0.9011  | eval_custom_logloss: 0.88041 |  0:08:47s
epoch 19 | loss: 0.89177 | eval_custom_logloss: 0.91777 |  0:09:15s
epoch 20 | loss: 0.88694 | eval_custom_logloss: 0.80643 |  0:09:43s
epoch 21 | loss: 0.88357 | eval_custom_logloss: 0.83651 |  0:10:11s
epoch 22 | loss: 0.87657 | eval_custom_logloss: 0.82021 |  0:10:39s
epoch 23 | loss: 0.87295 | eval_custom_logloss: 0.86647 |  0:11:06s
epoch 24 | loss: 0.86034 | eval_custom_logloss: 0.84954 |  0:11:34s
epoch 25 | loss: 0.85986 | eval_custom_logloss: 0.80702 |  0:12:02s
epoch 26 | loss: 0.86192 | eval_custom_logloss: 0.84786 |  0:12:30s
epoch 27 | loss: 0.85347 | eval_custom_logloss: 0.78814 |  0:12:58s
epoch 28 | loss: 0.84138 | eval_custom_logloss: 0.95336 |  0:13:26s
epoch 29 | loss: 0.82697 | eval_custom_logloss: 0.78261 |  0:13:54s
epoch 30 | loss: 0.8488  | eval_custom_logloss: 0.77591 |  0:14:22s
epoch 31 | loss: 0.84291 | eval_custom_logloss: 0.81303 |  0:14:50s
epoch 32 | loss: 0.83502 | eval_custom_logloss: 0.82531 |  0:15:18s
epoch 33 | loss: 0.83884 | eval_custom_logloss: 0.81481 |  0:15:46s
epoch 34 | loss: 0.84759 | eval_custom_logloss: 0.83468 |  0:16:14s
epoch 35 | loss: 0.82696 | eval_custom_logloss: 0.77766 |  0:16:41s
epoch 36 | loss: 0.83178 | eval_custom_logloss: 0.79042 |  0:17:09s
epoch 37 | loss: 0.83365 | eval_custom_logloss: 0.90262 |  0:17:37s
epoch 38 | loss: 0.81808 | eval_custom_logloss: 0.80637 |  0:18:05s
epoch 39 | loss: 0.81866 | eval_custom_logloss: 0.79153 |  0:18:33s
epoch 40 | loss: 0.81257 | eval_custom_logloss: 0.84276 |  0:19:01s
epoch 41 | loss: 0.82561 | eval_custom_logloss: 0.85119 |  0:19:29s
epoch 42 | loss: 0.80917 | eval_custom_logloss: 0.85122 |  0:19:57s
epoch 43 | loss: 0.82577 | eval_custom_logloss: 0.82714 |  0:20:25s
epoch 44 | loss: 0.80931 | eval_custom_logloss: 0.90418 |  0:20:53s
epoch 45 | loss: 0.79894 | eval_custom_logloss: 0.76251 |  0:21:20s
epoch 46 | loss: 0.80147 | eval_custom_logloss: 0.74673 |  0:21:48s
epoch 47 | loss: 0.79964 | eval_custom_logloss: 0.79086 |  0:22:16s
epoch 48 | loss: 0.8179  | eval_custom_logloss: 0.99766 |  0:22:44s
epoch 49 | loss: 0.79946 | eval_custom_logloss: 0.80713 |  0:23:12s
epoch 50 | loss: 0.80594 | eval_custom_logloss: 0.80216 |  0:23:40s
epoch 51 | loss: 0.79849 | eval_custom_logloss: 0.8433  |  0:24:08s
epoch 52 | loss: 0.79881 | eval_custom_logloss: 0.99994 |  0:24:36s
epoch 53 | loss: 0.80005 | eval_custom_logloss: 0.78403 |  0:25:04s
epoch 54 | loss: 0.79872 | eval_custom_logloss: 0.87856 |  0:25:31s
epoch 55 | loss: 0.80325 | eval_custom_logloss: 0.75054 |  0:26:00s
epoch 56 | loss: 0.80044 | eval_custom_logloss: 0.88927 |  0:26:28s
epoch 57 | loss: 0.80265 | eval_custom_logloss: 0.81885 |  0:26:56s
epoch 58 | loss: 0.8055  | eval_custom_logloss: 0.78233 |  0:27:23s
epoch 59 | loss: 0.79072 | eval_custom_logloss: 0.89099 |  0:27:51s
epoch 60 | loss: 0.93711 | eval_custom_logloss: 2.41035 |  0:28:19s
epoch 61 | loss: 0.81822 | eval_custom_logloss: 0.83346 |  0:28:47s
epoch 62 | loss: 0.82667 | eval_custom_logloss: 0.84748 |  0:29:15s
epoch 63 | loss: 0.83044 | eval_custom_logloss: 0.87481 |  0:29:43s
epoch 64 | loss: 0.81423 | eval_custom_logloss: 0.83358 |  0:30:11s
epoch 65 | loss: 0.82327 | eval_custom_logloss: 0.96595 |  0:30:39s
epoch 66 | loss: 0.8339  | eval_custom_logloss: 0.99666 |  0:31:07s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 0.74673
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7771000000000001, 'Log Loss - std': 0.08866859647022728} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 0.7771000000000001 and parameters: {'n_d': 8, 'n_steps': 9, 'gamma': 1.4779082785768605, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.37339297702791013, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.82809 | eval_custom_logloss: 1.44214 |  0:00:27s
epoch 1  | loss: 1.32668 | eval_custom_logloss: 1.11851 |  0:00:55s
epoch 2  | loss: 1.17598 | eval_custom_logloss: 1.02535 |  0:01:23s
epoch 3  | loss: 1.07398 | eval_custom_logloss: 1.02529 |  0:01:51s
epoch 4  | loss: 1.06303 | eval_custom_logloss: 0.94007 |  0:02:19s
epoch 5  | loss: 1.02747 | eval_custom_logloss: 0.89691 |  0:02:46s
epoch 6  | loss: 1.02043 | eval_custom_logloss: 0.86265 |  0:03:14s
epoch 7  | loss: 0.9771  | eval_custom_logloss: 0.93353 |  0:03:42s
epoch 8  | loss: 0.98334 | eval_custom_logloss: 0.86417 |  0:04:09s
epoch 9  | loss: 0.96119 | eval_custom_logloss: 0.93234 |  0:04:37s
epoch 10 | loss: 0.97003 | eval_custom_logloss: 0.8227  |  0:05:05s
epoch 11 | loss: 0.96752 | eval_custom_logloss: 0.91014 |  0:05:33s
epoch 12 | loss: 0.97077 | eval_custom_logloss: 0.86429 |  0:06:01s
epoch 13 | loss: 0.93695 | eval_custom_logloss: 0.87142 |  0:06:29s
epoch 14 | loss: 0.95898 | eval_custom_logloss: 0.87437 |  0:06:57s
epoch 15 | loss: 0.92338 | eval_custom_logloss: 0.93507 |  0:07:25s
epoch 16 | loss: 0.92569 | eval_custom_logloss: 0.82836 |  0:07:53s
epoch 17 | loss: 0.9167  | eval_custom_logloss: 0.97103 |  0:08:21s
epoch 18 | loss: 0.90896 | eval_custom_logloss: 0.85253 |  0:08:49s
epoch 19 | loss: 0.91941 | eval_custom_logloss: 0.86969 |  0:09:17s
epoch 20 | loss: 0.93499 | eval_custom_logloss: 0.8302  |  0:09:44s
epoch 21 | loss: 0.90276 | eval_custom_logloss: 0.77822 |  0:10:12s
epoch 22 | loss: 0.88311 | eval_custom_logloss: 0.79648 |  0:10:40s
epoch 23 | loss: 0.88253 | eval_custom_logloss: 0.85749 |  0:11:08s
epoch 24 | loss: 0.87108 | eval_custom_logloss: 0.81327 |  0:11:36s
epoch 25 | loss: 0.87434 | eval_custom_logloss: 0.82678 |  0:12:04s
epoch 26 | loss: 0.86135 | eval_custom_logloss: 1.13086 |  0:12:32s
epoch 27 | loss: 0.87288 | eval_custom_logloss: 0.8287  |  0:12:59s
epoch 28 | loss: 0.87082 | eval_custom_logloss: 0.79295 |  0:13:28s
epoch 29 | loss: 0.85906 | eval_custom_logloss: 0.7909  |  0:13:55s
epoch 30 | loss: 0.85966 | eval_custom_logloss: 0.77466 |  0:14:23s
epoch 31 | loss: 0.83319 | eval_custom_logloss: 0.72173 |  0:14:51s
epoch 32 | loss: 0.81831 | eval_custom_logloss: 0.77481 |  0:15:19s
epoch 33 | loss: 0.81619 | eval_custom_logloss: 0.74239 |  0:15:47s
epoch 34 | loss: 0.80343 | eval_custom_logloss: 0.77249 |  0:16:15s
epoch 35 | loss: 0.80501 | eval_custom_logloss: 0.71404 |  0:16:44s
epoch 36 | loss: 0.81095 | eval_custom_logloss: 0.80075 |  0:17:12s
epoch 37 | loss: 0.78204 | eval_custom_logloss: 0.68214 |  0:17:39s
epoch 38 | loss: 0.76148 | eval_custom_logloss: 0.71742 |  0:18:07s
epoch 39 | loss: 0.78008 | eval_custom_logloss: 0.90396 |  0:18:36s
epoch 40 | loss: 0.81648 | eval_custom_logloss: 0.74228 |  0:19:03s
epoch 41 | loss: 0.7704  | eval_custom_logloss: 0.68781 |  0:19:31s
epoch 42 | loss: 0.77569 | eval_custom_logloss: 0.76981 |  0:19:59s
epoch 43 | loss: 0.75112 | eval_custom_logloss: 0.77711 |  0:20:27s
epoch 44 | loss: 0.76707 | eval_custom_logloss: 0.78751 |  0:20:54s
epoch 45 | loss: 0.75825 | eval_custom_logloss: 0.64954 |  0:21:22s
epoch 46 | loss: 0.7524  | eval_custom_logloss: 0.69502 |  0:21:50s
epoch 47 | loss: 0.74519 | eval_custom_logloss: 0.69085 |  0:22:18s
epoch 48 | loss: 0.74565 | eval_custom_logloss: 0.66106 |  0:22:46s
epoch 49 | loss: 0.74066 | eval_custom_logloss: 0.76836 |  0:23:13s
epoch 50 | loss: 0.74238 | eval_custom_logloss: 0.76868 |  0:23:41s
epoch 51 | loss: 0.74427 | eval_custom_logloss: 0.86931 |  0:24:09s
epoch 52 | loss: 0.73799 | eval_custom_logloss: 0.78321 |  0:24:36s
epoch 53 | loss: 0.74136 | eval_custom_logloss: 0.64248 |  0:25:04s
epoch 54 | loss: 0.7269  | eval_custom_logloss: 0.85043 |  0:25:32s
epoch 55 | loss: 0.72054 | eval_custom_logloss: 0.82868 |  0:26:00s
epoch 56 | loss: 0.72134 | eval_custom_logloss: 0.66424 |  0:26:27s
epoch 57 | loss: 0.71947 | eval_custom_logloss: 0.66181 |  0:26:55s
epoch 58 | loss: 0.72603 | eval_custom_logloss: 0.70301 |  0:27:23s
epoch 59 | loss: 0.72802 | eval_custom_logloss: 0.65223 |  0:27:51s
epoch 60 | loss: 0.71656 | eval_custom_logloss: 0.76222 |  0:28:19s
epoch 61 | loss: 0.71168 | eval_custom_logloss: 0.87656 |  0:28:47s
epoch 62 | loss: 0.69846 | eval_custom_logloss: 0.79126 |  0:29:14s
epoch 63 | loss: 0.71275 | eval_custom_logloss: 0.66053 |  0:29:42s
epoch 64 | loss: 0.71035 | eval_custom_logloss: 0.67353 |  0:30:10s
epoch 65 | loss: 0.71262 | eval_custom_logloss: 0.76888 |  0:30:37s
epoch 66 | loss: 0.71294 | eval_custom_logloss: 0.71019 |  0:31:05s
epoch 67 | loss: 0.71427 | eval_custom_logloss: 0.6528  |  0:31:33s
epoch 68 | loss: 0.70881 | eval_custom_logloss: 0.65166 |  0:32:01s
epoch 69 | loss: 0.72521 | eval_custom_logloss: 0.64856 |  0:32:29s
epoch 70 | loss: 0.69878 | eval_custom_logloss: 0.86741 |  0:32:56s
epoch 71 | loss: 0.69293 | eval_custom_logloss: 0.6966  |  0:33:24s
epoch 72 | loss: 0.69863 | eval_custom_logloss: 0.96166 |  0:33:52s
epoch 73 | loss: 0.68977 | eval_custom_logloss: 0.90434 |  0:34:20s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.64248
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6419, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.81983 | eval_custom_logloss: 1.38917 |  0:00:28s
epoch 1  | loss: 1.32641 | eval_custom_logloss: 1.1265  |  0:00:55s
epoch 2  | loss: 1.06341 | eval_custom_logloss: 0.89276 |  0:01:23s
epoch 3  | loss: 0.97578 | eval_custom_logloss: 0.92946 |  0:01:51s
epoch 4  | loss: 0.96832 | eval_custom_logloss: 0.83393 |  0:02:19s
epoch 5  | loss: 0.90546 | eval_custom_logloss: 0.78179 |  0:02:47s
epoch 6  | loss: 0.90934 | eval_custom_logloss: 0.8604  |  0:03:14s
epoch 7  | loss: 0.89358 | eval_custom_logloss: 0.77033 |  0:03:42s
epoch 8  | loss: 0.91968 | eval_custom_logloss: 0.83868 |  0:04:10s
epoch 9  | loss: 0.8648  | eval_custom_logloss: 0.76601 |  0:04:38s
epoch 10 | loss: 0.83574 | eval_custom_logloss: 0.78658 |  0:05:06s
epoch 11 | loss: 0.83478 | eval_custom_logloss: 0.85109 |  0:05:34s
epoch 12 | loss: 0.83954 | eval_custom_logloss: 0.8773  |  0:06:02s
epoch 13 | loss: 0.84937 | eval_custom_logloss: 0.76113 |  0:06:29s
epoch 14 | loss: 0.84401 | eval_custom_logloss: 0.72604 |  0:06:57s
epoch 15 | loss: 0.82144 | eval_custom_logloss: 0.85232 |  0:07:25s
epoch 16 | loss: 0.81167 | eval_custom_logloss: 0.74168 |  0:07:53s
epoch 17 | loss: 0.80579 | eval_custom_logloss: 0.90124 |  0:08:21s
epoch 18 | loss: 0.83637 | eval_custom_logloss: 1.18453 |  0:08:49s
epoch 19 | loss: 0.82443 | eval_custom_logloss: 0.75954 |  0:09:17s
epoch 20 | loss: 0.83351 | eval_custom_logloss: 0.7295  |  0:09:44s
epoch 21 | loss: 0.80659 | eval_custom_logloss: 0.75561 |  0:10:12s
epoch 22 | loss: 0.80926 | eval_custom_logloss: 0.90825 |  0:10:40s
epoch 23 | loss: 0.80612 | eval_custom_logloss: 0.79213 |  0:11:08s
epoch 24 | loss: 0.79631 | eval_custom_logloss: 0.9266  |  0:11:36s
epoch 25 | loss: 0.80609 | eval_custom_logloss: 0.66362 |  0:12:04s
epoch 26 | loss: 0.76284 | eval_custom_logloss: 1.08763 |  0:12:31s
epoch 27 | loss: 0.78027 | eval_custom_logloss: 0.73211 |  0:12:59s
epoch 28 | loss: 0.76293 | eval_custom_logloss: 0.76364 |  0:13:27s
epoch 29 | loss: 0.7854  | eval_custom_logloss: 0.96905 |  0:13:55s
epoch 30 | loss: 0.79673 | eval_custom_logloss: 0.8444  |  0:14:23s
epoch 31 | loss: 0.77912 | eval_custom_logloss: 0.86996 |  0:14:51s
epoch 32 | loss: 0.76804 | eval_custom_logloss: 0.74038 |  0:15:19s
epoch 33 | loss: 0.773   | eval_custom_logloss: 0.75074 |  0:15:47s
epoch 34 | loss: 0.75759 | eval_custom_logloss: 0.78862 |  0:16:14s
epoch 35 | loss: 0.74997 | eval_custom_logloss: 1.03319 |  0:16:42s
epoch 36 | loss: 0.7526  | eval_custom_logloss: 0.77984 |  0:17:10s
epoch 37 | loss: 0.74637 | eval_custom_logloss: 0.6735  |  0:17:38s
epoch 38 | loss: 0.72648 | eval_custom_logloss: 0.82585 |  0:18:06s
epoch 39 | loss: 0.76235 | eval_custom_logloss: 0.74051 |  0:18:33s
epoch 40 | loss: 0.7329  | eval_custom_logloss: 0.8978  |  0:19:01s
epoch 41 | loss: 0.72129 | eval_custom_logloss: 0.76692 |  0:19:29s
epoch 42 | loss: 0.74695 | eval_custom_logloss: 0.72631 |  0:19:57s
epoch 43 | loss: 0.73384 | eval_custom_logloss: 0.92425 |  0:20:24s
epoch 44 | loss: 0.73282 | eval_custom_logloss: 0.69325 |  0:20:51s
epoch 45 | loss: 0.73423 | eval_custom_logloss: 0.72695 |  0:21:18s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 0.66362
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6524000000000001, 'Log Loss - std': 0.01050000000000001} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.70483 | eval_custom_logloss: 1.32557 |  0:00:27s
epoch 1  | loss: 1.31472 | eval_custom_logloss: 1.19084 |  0:00:54s
epoch 2  | loss: 1.29419 | eval_custom_logloss: 1.1938  |  0:01:21s
epoch 3  | loss: 1.20742 | eval_custom_logloss: 1.13346 |  0:01:48s
epoch 4  | loss: 1.10768 | eval_custom_logloss: 0.96735 |  0:02:14s
epoch 5  | loss: 1.0326  | eval_custom_logloss: 0.91419 |  0:02:42s
epoch 6  | loss: 1.00878 | eval_custom_logloss: 0.92521 |  0:03:09s
epoch 7  | loss: 0.98384 | eval_custom_logloss: 0.92632 |  0:03:36s
epoch 8  | loss: 0.9769  | eval_custom_logloss: 0.90082 |  0:04:03s
epoch 9  | loss: 0.93015 | eval_custom_logloss: 0.8241  |  0:04:30s
epoch 10 | loss: 0.92546 | eval_custom_logloss: 0.85803 |  0:04:58s
epoch 11 | loss: 0.93551 | eval_custom_logloss: 0.84702 |  0:05:25s
epoch 12 | loss: 0.92778 | eval_custom_logloss: 0.87921 |  0:05:52s
epoch 13 | loss: 0.93065 | eval_custom_logloss: 0.86961 |  0:06:19s
epoch 14 | loss: 0.92031 | eval_custom_logloss: 0.81178 |  0:06:46s
epoch 15 | loss: 0.86182 | eval_custom_logloss: 0.85256 |  0:07:13s
epoch 16 | loss: 0.86503 | eval_custom_logloss: 0.82907 |  0:07:40s
epoch 17 | loss: 0.86696 | eval_custom_logloss: 0.81834 |  0:08:08s
epoch 18 | loss: 0.84329 | eval_custom_logloss: 0.86845 |  0:08:35s
epoch 19 | loss: 0.80582 | eval_custom_logloss: 0.88612 |  0:09:02s
epoch 20 | loss: 0.7988  | eval_custom_logloss: 0.8725  |  0:09:28s
epoch 21 | loss: 0.77803 | eval_custom_logloss: 0.75939 |  0:09:55s
epoch 22 | loss: 0.7475  | eval_custom_logloss: 0.73689 |  0:10:23s
epoch 23 | loss: 0.74381 | eval_custom_logloss: 0.79684 |  0:10:50s
epoch 24 | loss: 0.74882 | eval_custom_logloss: 0.70848 |  0:11:17s
epoch 25 | loss: 0.74207 | eval_custom_logloss: 0.67792 |  0:11:44s
epoch 26 | loss: 0.74074 | eval_custom_logloss: 1.30986 |  0:12:11s
epoch 27 | loss: 0.72472 | eval_custom_logloss: 0.82992 |  0:12:38s
epoch 28 | loss: 0.73428 | eval_custom_logloss: 0.75688 |  0:13:05s
epoch 29 | loss: 0.7185  | eval_custom_logloss: 0.74091 |  0:13:33s
epoch 30 | loss: 0.71361 | eval_custom_logloss: 0.99541 |  0:14:00s
epoch 31 | loss: 0.71773 | eval_custom_logloss: 0.83481 |  0:14:27s
epoch 32 | loss: 0.72204 | eval_custom_logloss: 0.71564 |  0:14:54s
epoch 33 | loss: 0.7108  | eval_custom_logloss: 0.67758 |  0:15:22s
epoch 34 | loss: 0.71516 | eval_custom_logloss: 0.68016 |  0:15:49s
epoch 35 | loss: 0.70033 | eval_custom_logloss: 0.83212 |  0:16:16s
epoch 36 | loss: 0.69671 | eval_custom_logloss: 0.80022 |  0:16:43s
epoch 37 | loss: 0.70666 | eval_custom_logloss: 0.8441  |  0:17:10s
epoch 38 | loss: 0.69555 | eval_custom_logloss: 0.74319 |  0:17:37s
epoch 39 | loss: 0.69563 | eval_custom_logloss: 0.7428  |  0:18:04s
epoch 40 | loss: 0.6888  | eval_custom_logloss: 0.75278 |  0:18:31s
epoch 41 | loss: 0.69547 | eval_custom_logloss: 0.66948 |  0:18:58s
epoch 42 | loss: 0.69044 | eval_custom_logloss: 0.79229 |  0:19:25s
epoch 43 | loss: 0.69297 | eval_custom_logloss: 0.75495 |  0:19:52s
epoch 44 | loss: 0.70023 | eval_custom_logloss: 0.81495 |  0:20:19s
epoch 45 | loss: 0.6819  | eval_custom_logloss: 0.76133 |  0:20:47s
epoch 46 | loss: 0.67604 | eval_custom_logloss: 0.84975 |  0:21:14s
epoch 47 | loss: 0.67779 | eval_custom_logloss: 0.70798 |  0:21:41s
epoch 48 | loss: 0.68815 | eval_custom_logloss: 0.66468 |  0:22:08s
epoch 49 | loss: 0.67458 | eval_custom_logloss: 0.79038 |  0:22:35s
epoch 50 | loss: 0.70176 | eval_custom_logloss: 0.79391 |  0:23:02s
epoch 51 | loss: 0.67141 | eval_custom_logloss: 0.84149 |  0:23:29s
epoch 52 | loss: 0.70271 | eval_custom_logloss: 0.84558 |  0:23:57s
epoch 53 | loss: 0.6784  | eval_custom_logloss: 0.67845 |  0:24:24s
epoch 54 | loss: 0.67079 | eval_custom_logloss: 0.69529 |  0:24:51s
epoch 55 | loss: 0.66733 | eval_custom_logloss: 0.84164 |  0:25:18s
epoch 56 | loss: 0.67068 | eval_custom_logloss: 0.67652 |  0:25:45s
epoch 57 | loss: 0.66467 | eval_custom_logloss: 0.81427 |  0:26:12s
epoch 58 | loss: 0.6772  | eval_custom_logloss: 0.78101 |  0:26:40s
epoch 59 | loss: 0.66629 | eval_custom_logloss: 0.86972 |  0:27:07s
epoch 60 | loss: 0.66681 | eval_custom_logloss: 1.46122 |  0:27:34s
epoch 61 | loss: 0.65993 | eval_custom_logloss: 0.79668 |  0:28:02s
epoch 62 | loss: 0.67122 | eval_custom_logloss: 0.88609 |  0:28:29s
epoch 63 | loss: 0.67256 | eval_custom_logloss: 0.78349 |  0:28:56s
epoch 64 | loss: 0.65445 | eval_custom_logloss: 0.71869 |  0:29:22s
epoch 65 | loss: 0.6631  | eval_custom_logloss: 0.89449 |  0:29:50s
epoch 66 | loss: 0.66766 | eval_custom_logloss: 0.75644 |  0:30:17s
epoch 67 | loss: 0.66375 | eval_custom_logloss: 0.86114 |  0:30:44s
epoch 68 | loss: 0.65257 | eval_custom_logloss: 0.82947 |  0:31:12s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 0.66468
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6558666666666667, 'Log Loss - std': 0.009876009090495779} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.74601 | eval_custom_logloss: 1.36952 |  0:00:27s
epoch 1  | loss: 1.37787 | eval_custom_logloss: 1.28925 |  0:00:54s
epoch 2  | loss: 1.30761 | eval_custom_logloss: 1.17046 |  0:01:22s
epoch 3  | loss: 1.14116 | eval_custom_logloss: 0.98199 |  0:01:49s
epoch 4  | loss: 1.07546 | eval_custom_logloss: 0.98605 |  0:02:16s
epoch 5  | loss: 1.04309 | eval_custom_logloss: 0.95317 |  0:02:44s
epoch 6  | loss: 1.01588 | eval_custom_logloss: 0.90587 |  0:03:11s
epoch 7  | loss: 0.97413 | eval_custom_logloss: 0.90002 |  0:03:38s
epoch 8  | loss: 0.99111 | eval_custom_logloss: 0.96185 |  0:04:05s
epoch 9  | loss: 0.9642  | eval_custom_logloss: 0.95839 |  0:04:32s
epoch 10 | loss: 0.93674 | eval_custom_logloss: 0.85843 |  0:04:59s
epoch 11 | loss: 0.97751 | eval_custom_logloss: 0.88553 |  0:05:26s
epoch 12 | loss: 0.94722 | eval_custom_logloss: 0.89905 |  0:05:53s
epoch 13 | loss: 0.91776 | eval_custom_logloss: 0.95635 |  0:06:20s
epoch 14 | loss: 0.89427 | eval_custom_logloss: 0.77525 |  0:06:47s
epoch 15 | loss: 0.85589 | eval_custom_logloss: 0.82606 |  0:07:14s
epoch 16 | loss: 0.85004 | eval_custom_logloss: 0.84025 |  0:07:41s
epoch 17 | loss: 0.84484 | eval_custom_logloss: 1.15866 |  0:08:08s
epoch 18 | loss: 0.82022 | eval_custom_logloss: 0.8099  |  0:08:35s
epoch 19 | loss: 0.85726 | eval_custom_logloss: 0.87798 |  0:09:02s
epoch 20 | loss: 0.82807 | eval_custom_logloss: 0.7638  |  0:09:30s
epoch 21 | loss: 0.83483 | eval_custom_logloss: 1.01924 |  0:09:57s
epoch 22 | loss: 0.81087 | eval_custom_logloss: 0.83634 |  0:10:24s
epoch 23 | loss: 0.79816 | eval_custom_logloss: 0.75678 |  0:10:51s
epoch 24 | loss: 0.78028 | eval_custom_logloss: 0.88048 |  0:11:18s
epoch 25 | loss: 0.7869  | eval_custom_logloss: 0.81389 |  0:11:45s
epoch 26 | loss: 0.76714 | eval_custom_logloss: 0.90232 |  0:12:12s
epoch 27 | loss: 0.74892 | eval_custom_logloss: 0.7865  |  0:12:39s
epoch 28 | loss: 0.76366 | eval_custom_logloss: 0.71235 |  0:13:06s
epoch 29 | loss: 0.75325 | eval_custom_logloss: 0.71786 |  0:13:33s
epoch 30 | loss: 0.73873 | eval_custom_logloss: 0.79603 |  0:14:01s
epoch 31 | loss: 0.74634 | eval_custom_logloss: 0.74095 |  0:14:27s
epoch 32 | loss: 0.74264 | eval_custom_logloss: 0.75933 |  0:14:54s
epoch 33 | loss: 0.74358 | eval_custom_logloss: 0.85773 |  0:15:22s
epoch 34 | loss: 0.74159 | eval_custom_logloss: 0.79779 |  0:15:49s
epoch 35 | loss: 0.7342  | eval_custom_logloss: 0.73658 |  0:16:16s
epoch 36 | loss: 0.73176 | eval_custom_logloss: 0.83479 |  0:16:43s
epoch 37 | loss: 0.73658 | eval_custom_logloss: 0.80069 |  0:17:10s
epoch 38 | loss: 0.7309  | eval_custom_logloss: 0.84885 |  0:17:37s
epoch 39 | loss: 0.73781 | eval_custom_logloss: 0.68471 |  0:18:04s
epoch 40 | loss: 0.72184 | eval_custom_logloss: 0.86524 |  0:18:31s
epoch 41 | loss: 0.71958 | eval_custom_logloss: 0.70694 |  0:18:58s
epoch 42 | loss: 0.71755 | eval_custom_logloss: 0.81154 |  0:19:25s
epoch 43 | loss: 0.72487 | eval_custom_logloss: 0.77503 |  0:19:53s
epoch 44 | loss: 0.7081  | eval_custom_logloss: 0.74842 |  0:20:20s
epoch 45 | loss: 0.71892 | eval_custom_logloss: 0.65772 |  0:20:47s
epoch 46 | loss: 0.71815 | eval_custom_logloss: 0.81017 |  0:21:14s
epoch 47 | loss: 0.70277 | eval_custom_logloss: 0.71638 |  0:21:41s
epoch 48 | loss: 0.69838 | eval_custom_logloss: 0.64948 |  0:22:08s
epoch 49 | loss: 0.69226 | eval_custom_logloss: 0.85194 |  0:22:36s
epoch 50 | loss: 0.71488 | eval_custom_logloss: 0.89437 |  0:23:03s
epoch 51 | loss: 0.70372 | eval_custom_logloss: 0.70682 |  0:23:30s
epoch 52 | loss: 0.70221 | eval_custom_logloss: 0.86162 |  0:23:57s
epoch 53 | loss: 0.70569 | eval_custom_logloss: 0.81084 |  0:24:24s
epoch 54 | loss: 0.69715 | eval_custom_logloss: 0.73302 |  0:24:51s
epoch 55 | loss: 0.70099 | eval_custom_logloss: 0.66684 |  0:25:18s
epoch 56 | loss: 0.69785 | eval_custom_logloss: 0.81174 |  0:25:45s
epoch 57 | loss: 0.70289 | eval_custom_logloss: 0.71039 |  0:26:12s
epoch 58 | loss: 0.68755 | eval_custom_logloss: 0.65976 |  0:26:39s
epoch 59 | loss: 0.68818 | eval_custom_logloss: 0.77555 |  0:27:07s
epoch 60 | loss: 0.70021 | eval_custom_logloss: 0.88739 |  0:27:34s
epoch 61 | loss: 0.7084  | eval_custom_logloss: 0.88553 |  0:28:01s
epoch 62 | loss: 0.69509 | eval_custom_logloss: 0.70105 |  0:28:28s
epoch 63 | loss: 0.7005  | eval_custom_logloss: 0.81547 |  0:28:56s
epoch 64 | loss: 0.68038 | eval_custom_logloss: 0.62525 |  0:29:23s
epoch 65 | loss: 0.68292 | eval_custom_logloss: 0.8461  |  0:29:50s
epoch 66 | loss: 0.68468 | eval_custom_logloss: 0.77897 |  0:30:17s
epoch 67 | loss: 0.69898 | eval_custom_logloss: 0.70621 |  0:30:44s
epoch 68 | loss: 0.68751 | eval_custom_logloss: 0.76936 |  0:31:11s
epoch 69 | loss: 0.69035 | eval_custom_logloss: 0.61127 |  0:31:38s
epoch 70 | loss: 0.68649 | eval_custom_logloss: 0.61204 |  0:32:06s
epoch 71 | loss: 0.68261 | eval_custom_logloss: 0.775   |  0:32:33s
epoch 72 | loss: 0.68267 | eval_custom_logloss: 0.64666 |  0:33:00s
epoch 73 | loss: 0.68031 | eval_custom_logloss: 0.79083 |  0:33:27s
epoch 74 | loss: 0.67735 | eval_custom_logloss: 0.66742 |  0:33:54s
epoch 75 | loss: 0.68077 | eval_custom_logloss: 0.88138 |  0:34:22s
epoch 76 | loss: 0.67451 | eval_custom_logloss: 0.68531 |  0:34:49s
epoch 77 | loss: 0.67254 | eval_custom_logloss: 0.74831 |  0:35:16s
epoch 78 | loss: 0.6832  | eval_custom_logloss: 0.63563 |  0:35:43s
epoch 79 | loss: 0.67803 | eval_custom_logloss: 0.82533 |  0:36:11s
epoch 80 | loss: 0.68691 | eval_custom_logloss: 0.75107 |  0:36:38s
epoch 81 | loss: 0.67888 | eval_custom_logloss: 0.84573 |  0:37:05s
epoch 82 | loss: 0.67899 | eval_custom_logloss: 0.99863 |  0:37:32s
epoch 83 | loss: 0.66822 | eval_custom_logloss: 0.60262 |  0:37:59s
epoch 84 | loss: 0.67206 | eval_custom_logloss: 0.69614 |  0:38:27s
epoch 85 | loss: 0.6738  | eval_custom_logloss: 0.75167 |  0:38:54s
epoch 86 | loss: 0.66809 | eval_custom_logloss: 0.79226 |  0:39:21s
epoch 87 | loss: 0.65628 | eval_custom_logloss: 0.75297 |  0:39:48s
epoch 88 | loss: 0.68229 | eval_custom_logloss: 0.64309 |  0:40:15s
epoch 89 | loss: 0.66466 | eval_custom_logloss: 0.87054 |  0:40:42s
epoch 90 | loss: 0.68279 | eval_custom_logloss: 0.77002 |  0:41:09s
epoch 91 | loss: 0.6628  | eval_custom_logloss: 0.63653 |  0:41:36s
epoch 92 | loss: 0.66506 | eval_custom_logloss: 0.88955 |  0:42:04s
epoch 93 | loss: 0.6659  | eval_custom_logloss: 0.67089 |  0:42:31s
epoch 94 | loss: 0.6688  | eval_custom_logloss: 0.89277 |  0:42:58s
epoch 95 | loss: 0.67393 | eval_custom_logloss: 0.74202 |  0:43:25s
epoch 96 | loss: 0.66626 | eval_custom_logloss: 0.60441 |  0:43:52s
epoch 97 | loss: 0.65738 | eval_custom_logloss: 0.66514 |  0:44:19s
epoch 98 | loss: 0.6595  | eval_custom_logloss: 0.75556 |  0:44:46s
epoch 99 | loss: 0.68061 | eval_custom_logloss: 0.78252 |  0:45:14s
Stop training because you reached max_epochs = 100 with best_epoch = 83 and best_eval_custom_logloss = 0.60262
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.642475, 'Log Loss - std': 0.024721688352537758} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73732 | eval_custom_logloss: 1.37552 |  0:00:27s
epoch 1  | loss: 1.2198  | eval_custom_logloss: 0.94755 |  0:00:54s
epoch 2  | loss: 1.05136 | eval_custom_logloss: 0.94302 |  0:01:21s
epoch 3  | loss: 1.00463 | eval_custom_logloss: 0.99412 |  0:01:49s
epoch 4  | loss: 1.00457 | eval_custom_logloss: 0.93803 |  0:02:16s
epoch 5  | loss: 0.97311 | eval_custom_logloss: 0.84973 |  0:02:43s
epoch 6  | loss: 0.95438 | eval_custom_logloss: 0.80933 |  0:03:10s
epoch 7  | loss: 0.93758 | eval_custom_logloss: 0.80944 |  0:03:37s
epoch 8  | loss: 0.91189 | eval_custom_logloss: 0.77005 |  0:04:04s
epoch 9  | loss: 0.90547 | eval_custom_logloss: 0.80016 |  0:04:32s
epoch 10 | loss: 0.90912 | eval_custom_logloss: 0.81888 |  0:04:59s
epoch 11 | loss: 0.88703 | eval_custom_logloss: 0.80972 |  0:05:26s
epoch 12 | loss: 0.8926  | eval_custom_logloss: 0.80653 |  0:05:53s
epoch 13 | loss: 0.86073 | eval_custom_logloss: 0.75179 |  0:06:20s
epoch 14 | loss: 0.87063 | eval_custom_logloss: 0.82939 |  0:06:48s
epoch 15 | loss: 0.83681 | eval_custom_logloss: 0.7426  |  0:07:15s
epoch 16 | loss: 0.82747 | eval_custom_logloss: 0.80521 |  0:07:42s
epoch 17 | loss: 0.81892 | eval_custom_logloss: 0.76639 |  0:08:09s
epoch 18 | loss: 0.82063 | eval_custom_logloss: 0.75745 |  0:08:36s
epoch 19 | loss: 0.79113 | eval_custom_logloss: 0.72704 |  0:09:03s
epoch 20 | loss: 0.79839 | eval_custom_logloss: 0.9245  |  0:09:31s
epoch 21 | loss: 0.75996 | eval_custom_logloss: 0.79511 |  0:09:58s
epoch 22 | loss: 0.76845 | eval_custom_logloss: 0.81873 |  0:10:25s
epoch 23 | loss: 0.76794 | eval_custom_logloss: 0.72733 |  0:10:52s
epoch 24 | loss: 0.77027 | eval_custom_logloss: 0.85776 |  0:11:20s
epoch 25 | loss: 0.73668 | eval_custom_logloss: 0.8058  |  0:11:47s
epoch 26 | loss: 0.7444  | eval_custom_logloss: 0.79132 |  0:12:14s
epoch 27 | loss: 0.74066 | eval_custom_logloss: 0.67547 |  0:12:41s
epoch 28 | loss: 0.72094 | eval_custom_logloss: 0.7906  |  0:13:08s
epoch 29 | loss: 0.75499 | eval_custom_logloss: 0.67296 |  0:13:36s
epoch 30 | loss: 0.72362 | eval_custom_logloss: 0.87677 |  0:14:03s
epoch 31 | loss: 0.71849 | eval_custom_logloss: 0.66828 |  0:14:30s
epoch 32 | loss: 0.71218 | eval_custom_logloss: 0.71662 |  0:14:57s
epoch 33 | loss: 0.71316 | eval_custom_logloss: 0.72761 |  0:15:24s
epoch 34 | loss: 0.69967 | eval_custom_logloss: 0.69624 |  0:15:51s
epoch 35 | loss: 0.7288  | eval_custom_logloss: 0.86397 |  0:16:18s
epoch 36 | loss: 0.732   | eval_custom_logloss: 1.24374 |  0:16:45s
epoch 37 | loss: 0.73205 | eval_custom_logloss: 0.86987 |  0:17:13s
epoch 38 | loss: 0.71669 | eval_custom_logloss: 0.73458 |  0:17:40s
epoch 39 | loss: 0.71554 | eval_custom_logloss: 0.92232 |  0:18:07s
epoch 40 | loss: 0.70322 | eval_custom_logloss: 0.79541 |  0:18:34s
epoch 41 | loss: 0.71022 | eval_custom_logloss: 0.6793  |  0:19:01s
epoch 42 | loss: 0.71092 | eval_custom_logloss: 0.75617 |  0:19:28s
epoch 43 | loss: 0.69779 | eval_custom_logloss: 0.81721 |  0:19:56s
epoch 44 | loss: 0.70148 | eval_custom_logloss: 0.65785 |  0:20:23s
epoch 45 | loss: 0.68249 | eval_custom_logloss: 0.66284 |  0:20:50s
epoch 46 | loss: 0.7047  | eval_custom_logloss: 0.81915 |  0:21:17s
epoch 47 | loss: 0.68338 | eval_custom_logloss: 0.72989 |  0:21:44s
epoch 48 | loss: 0.68638 | eval_custom_logloss: 0.69612 |  0:22:10s
epoch 49 | loss: 0.68222 | eval_custom_logloss: 0.87807 |  0:22:38s
epoch 50 | loss: 0.71566 | eval_custom_logloss: 0.9395  |  0:23:05s
epoch 51 | loss: 0.68668 | eval_custom_logloss: 0.71577 |  0:23:32s
epoch 52 | loss: 0.70386 | eval_custom_logloss: 0.65916 |  0:23:59s
epoch 53 | loss: 0.74605 | eval_custom_logloss: 0.6329  |  0:24:26s
epoch 54 | loss: 0.68993 | eval_custom_logloss: 0.63021 |  0:24:53s
epoch 55 | loss: 0.66962 | eval_custom_logloss: 0.73314 |  0:25:20s
epoch 56 | loss: 0.69384 | eval_custom_logloss: 0.75792 |  0:25:47s
epoch 57 | loss: 0.69673 | eval_custom_logloss: 0.60503 |  0:26:14s
epoch 58 | loss: 0.68814 | eval_custom_logloss: 0.66818 |  0:26:42s
epoch 59 | loss: 0.69828 | eval_custom_logloss: 0.83904 |  0:27:09s
epoch 60 | loss: 0.68313 | eval_custom_logloss: 0.75461 |  0:27:36s
epoch 61 | loss: 0.67428 | eval_custom_logloss: 0.73231 |  0:28:03s
epoch 62 | loss: 0.67357 | eval_custom_logloss: 0.88244 |  0:28:30s
epoch 63 | loss: 0.66984 | eval_custom_logloss: 0.84541 |  0:28:58s
epoch 64 | loss: 0.67138 | eval_custom_logloss: 0.61452 |  0:29:25s
epoch 65 | loss: 0.67257 | eval_custom_logloss: 0.84078 |  0:29:51s
epoch 66 | loss: 0.67733 | eval_custom_logloss: 0.71715 |  0:30:18s
epoch 67 | loss: 0.71599 | eval_custom_logloss: 0.79529 |  0:30:45s
epoch 68 | loss: 0.69878 | eval_custom_logloss: 0.76165 |  0:31:13s
epoch 69 | loss: 0.68612 | eval_custom_logloss: 0.70395 |  0:31:40s
epoch 70 | loss: 0.66541 | eval_custom_logloss: 0.63637 |  0:32:07s
epoch 71 | loss: 0.67023 | eval_custom_logloss: 0.66032 |  0:32:34s
epoch 72 | loss: 0.69463 | eval_custom_logloss: 0.6443  |  0:33:01s
epoch 73 | loss: 0.67866 | eval_custom_logloss: 0.65188 |  0:33:28s
epoch 74 | loss: 0.69705 | eval_custom_logloss: 0.7987  |  0:33:55s
epoch 75 | loss: 0.66466 | eval_custom_logloss: 0.75788 |  0:34:22s
epoch 76 | loss: 0.66675 | eval_custom_logloss: 0.85787 |  0:34:50s
epoch 77 | loss: 0.66463 | eval_custom_logloss: 0.75577 |  0:35:17s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.60503
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6346, 'Log Loss - std': 0.027147596578702894} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 0.6346 and parameters: {'n_d': 8, 'n_steps': 9, 'gamma': 1.990873796851433, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.3859999130900889, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax', 'n_a': 15, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.78957 | eval_custom_logloss: 1.44309 |  0:00:27s
epoch 1  | loss: 1.41545 | eval_custom_logloss: 1.43076 |  0:00:54s
epoch 2  | loss: 1.29654 | eval_custom_logloss: 1.17985 |  0:01:21s
epoch 3  | loss: 1.21711 | eval_custom_logloss: 1.13825 |  0:01:49s
epoch 4  | loss: 1.12951 | eval_custom_logloss: 1.07216 |  0:02:16s
epoch 5  | loss: 1.05397 | eval_custom_logloss: 0.96132 |  0:02:43s
epoch 6  | loss: 1.02072 | eval_custom_logloss: 0.98406 |  0:03:10s
epoch 7  | loss: 0.98496 | eval_custom_logloss: 0.89506 |  0:03:37s
epoch 8  | loss: 0.95846 | eval_custom_logloss: 0.90846 |  0:04:04s
epoch 9  | loss: 0.94416 | eval_custom_logloss: 1.27059 |  0:04:31s
epoch 10 | loss: 0.92775 | eval_custom_logloss: 0.80168 |  0:04:58s
epoch 11 | loss: 0.9187  | eval_custom_logloss: 0.903   |  0:05:25s
epoch 12 | loss: 0.90748 | eval_custom_logloss: 1.04113 |  0:05:53s
epoch 13 | loss: 0.90162 | eval_custom_logloss: 0.84857 |  0:06:20s
epoch 14 | loss: 0.8605  | eval_custom_logloss: 0.77473 |  0:06:47s
epoch 15 | loss: 0.85284 | eval_custom_logloss: 0.99202 |  0:07:14s
epoch 16 | loss: 0.80751 | eval_custom_logloss: 0.77182 |  0:07:42s
epoch 17 | loss: 0.80874 | eval_custom_logloss: 0.84272 |  0:08:09s
epoch 18 | loss: 0.85522 | eval_custom_logloss: 0.9269  |  0:08:36s
epoch 19 | loss: 0.80142 | eval_custom_logloss: 0.82182 |  0:09:03s
epoch 20 | loss: 0.80307 | eval_custom_logloss: 0.77665 |  0:09:30s
epoch 21 | loss: 0.78563 | eval_custom_logloss: 0.71399 |  0:09:57s
epoch 22 | loss: 0.76565 | eval_custom_logloss: 0.88427 |  0:10:24s
epoch 23 | loss: 0.75534 | eval_custom_logloss: 0.6479  |  0:10:52s
epoch 24 | loss: 0.76702 | eval_custom_logloss: 0.82908 |  0:11:19s
epoch 25 | loss: 0.75147 | eval_custom_logloss: 0.70539 |  0:11:46s
epoch 26 | loss: 0.74955 | eval_custom_logloss: 0.82019 |  0:12:12s
epoch 27 | loss: 0.74229 | eval_custom_logloss: 0.94207 |  0:12:40s
epoch 28 | loss: 0.76805 | eval_custom_logloss: 0.8042  |  0:13:06s
epoch 29 | loss: 0.74562 | eval_custom_logloss: 0.65898 |  0:13:34s
epoch 30 | loss: 0.72287 | eval_custom_logloss: 0.75417 |  0:14:01s
epoch 31 | loss: 0.73961 | eval_custom_logloss: 0.91926 |  0:14:28s
epoch 32 | loss: 0.74228 | eval_custom_logloss: 0.69237 |  0:14:55s
epoch 33 | loss: 0.7304  | eval_custom_logloss: 0.79408 |  0:15:22s
epoch 34 | loss: 0.74379 | eval_custom_logloss: 0.76943 |  0:15:49s
epoch 35 | loss: 0.76617 | eval_custom_logloss: 0.75436 |  0:16:16s
epoch 36 | loss: 0.72745 | eval_custom_logloss: 0.85125 |  0:16:43s
epoch 37 | loss: 0.70687 | eval_custom_logloss: 0.65181 |  0:17:10s
epoch 38 | loss: 0.70944 | eval_custom_logloss: 0.79073 |  0:17:37s
epoch 39 | loss: 0.69716 | eval_custom_logloss: 0.86991 |  0:18:04s
epoch 40 | loss: 0.72056 | eval_custom_logloss: 0.80874 |  0:18:31s
epoch 41 | loss: 0.70294 | eval_custom_logloss: 0.67259 |  0:18:59s
epoch 42 | loss: 0.7047  | eval_custom_logloss: 0.67692 |  0:19:25s
epoch 43 | loss: 0.69692 | eval_custom_logloss: 0.85369 |  0:19:53s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.6479
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6465, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax', 'n_a': 15, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.83367 | eval_custom_logloss: 1.42589 |  0:00:27s
epoch 1  | loss: 1.32528 | eval_custom_logloss: 1.20911 |  0:00:54s
epoch 2  | loss: 1.23669 | eval_custom_logloss: 1.11114 |  0:01:21s
epoch 3  | loss: 1.1476  | eval_custom_logloss: 1.13455 |  0:01:48s
epoch 4  | loss: 1.09418 | eval_custom_logloss: 0.98063 |  0:02:15s
epoch 5  | loss: 1.04871 | eval_custom_logloss: 0.95855 |  0:02:43s
epoch 6  | loss: 1.08071 | eval_custom_logloss: 0.92217 |  0:03:10s
epoch 7  | loss: 1.02667 | eval_custom_logloss: 1.11886 |  0:03:37s
epoch 8  | loss: 0.9955  | eval_custom_logloss: 0.91486 |  0:04:04s
epoch 9  | loss: 1.02036 | eval_custom_logloss: 0.96353 |  0:04:31s
epoch 10 | loss: 0.98871 | eval_custom_logloss: 0.88823 |  0:04:58s
epoch 11 | loss: 1.01288 | eval_custom_logloss: 0.89016 |  0:05:26s
epoch 12 | loss: 0.94526 | eval_custom_logloss: 0.85506 |  0:05:53s
epoch 13 | loss: 0.95939 | eval_custom_logloss: 1.0735  |  0:06:20s
epoch 14 | loss: 0.96866 | eval_custom_logloss: 0.85477 |  0:06:47s
epoch 15 | loss: 0.95118 | eval_custom_logloss: 0.83672 |  0:07:15s
epoch 16 | loss: 0.90308 | eval_custom_logloss: 0.81658 |  0:07:42s
epoch 17 | loss: 0.92128 | eval_custom_logloss: 0.94714 |  0:08:09s
epoch 18 | loss: 0.87569 | eval_custom_logloss: 1.10708 |  0:08:36s
epoch 19 | loss: 0.8571  | eval_custom_logloss: 1.02441 |  0:09:03s
epoch 20 | loss: 0.87994 | eval_custom_logloss: 0.89286 |  0:09:31s
epoch 21 | loss: 0.84659 | eval_custom_logloss: 0.75983 |  0:09:58s
epoch 22 | loss: 0.83601 | eval_custom_logloss: 0.81834 |  0:10:25s
epoch 23 | loss: 0.82471 | eval_custom_logloss: 0.9613  |  0:10:53s
epoch 24 | loss: 0.80962 | eval_custom_logloss: 0.78079 |  0:11:20s
epoch 25 | loss: 0.81331 | eval_custom_logloss: 0.78066 |  0:11:47s
epoch 26 | loss: 0.80072 | eval_custom_logloss: 0.79814 |  0:12:14s
epoch 27 | loss: 0.78858 | eval_custom_logloss: 1.1655  |  0:12:41s
epoch 28 | loss: 0.80345 | eval_custom_logloss: 0.83614 |  0:13:08s
epoch 29 | loss: 0.79554 | eval_custom_logloss: 0.94718 |  0:13:36s
epoch 30 | loss: 0.79723 | eval_custom_logloss: 0.75144 |  0:14:03s
epoch 31 | loss: 0.78755 | eval_custom_logloss: 0.76963 |  0:14:30s
epoch 32 | loss: 0.79059 | eval_custom_logloss: 0.81921 |  0:14:57s
epoch 33 | loss: 0.78031 | eval_custom_logloss: 0.82692 |  0:15:24s
epoch 34 | loss: 0.77693 | eval_custom_logloss: 1.05776 |  0:15:51s
epoch 35 | loss: 0.80687 | eval_custom_logloss: 0.69871 |  0:16:18s
epoch 36 | loss: 0.79151 | eval_custom_logloss: 0.84391 |  0:16:46s
epoch 37 | loss: 0.7772  | eval_custom_logloss: 0.72103 |  0:17:13s
epoch 38 | loss: 0.76273 | eval_custom_logloss: 0.72895 |  0:17:40s
epoch 39 | loss: 0.75182 | eval_custom_logloss: 0.91087 |  0:18:06s
epoch 40 | loss: 0.75701 | eval_custom_logloss: 0.84684 |  0:18:33s
epoch 41 | loss: 0.74317 | eval_custom_logloss: 1.10112 |  0:18:59s
epoch 42 | loss: 0.7317  | eval_custom_logloss: 0.76549 |  0:19:26s
epoch 43 | loss: 0.74469 | eval_custom_logloss: 1.08096 |  0:19:52s
epoch 44 | loss: 0.74082 | eval_custom_logloss: 0.76161 |  0:20:19s
epoch 45 | loss: 0.73865 | eval_custom_logloss: 0.81674 |  0:20:45s
epoch 46 | loss: 0.72401 | eval_custom_logloss: 0.64734 |  0:21:12s
epoch 47 | loss: 0.73122 | eval_custom_logloss: 0.78156 |  0:21:38s
epoch 48 | loss: 0.7343  | eval_custom_logloss: 0.75401 |  0:22:05s
epoch 49 | loss: 0.70545 | eval_custom_logloss: 0.68675 |  0:22:31s
epoch 50 | loss: 0.71164 | eval_custom_logloss: 0.68563 |  0:22:58s
epoch 51 | loss: 0.72416 | eval_custom_logloss: 0.6776  |  0:23:24s
epoch 52 | loss: 0.72349 | eval_custom_logloss: 0.76267 |  0:23:50s
epoch 53 | loss: 0.72143 | eval_custom_logloss: 0.6454  |  0:24:17s
epoch 54 | loss: 0.71867 | eval_custom_logloss: 0.69461 |  0:24:43s
epoch 55 | loss: 0.7102  | eval_custom_logloss: 0.71736 |  0:25:10s
epoch 56 | loss: 0.72095 | eval_custom_logloss: 0.73484 |  0:25:37s
epoch 57 | loss: 0.7027  | eval_custom_logloss: 0.7734  |  0:26:03s
epoch 58 | loss: 0.70821 | eval_custom_logloss: 0.70113 |  0:26:30s
epoch 59 | loss: 0.71284 | eval_custom_logloss: 0.86359 |  0:26:56s
epoch 60 | loss: 0.69436 | eval_custom_logloss: 0.69655 |  0:27:22s
epoch 61 | loss: 0.73842 | eval_custom_logloss: 0.70617 |  0:27:49s
epoch 62 | loss: 0.70882 | eval_custom_logloss: 0.7026  |  0:28:16s
epoch 63 | loss: 0.70859 | eval_custom_logloss: 1.13212 |  0:28:43s
epoch 64 | loss: 0.70048 | eval_custom_logloss: 0.70531 |  0:29:09s
epoch 65 | loss: 0.70318 | eval_custom_logloss: 0.71513 |  0:29:36s
epoch 66 | loss: 0.68937 | eval_custom_logloss: 0.82087 |  0:30:02s
epoch 67 | loss: 0.68766 | eval_custom_logloss: 0.64693 |  0:30:29s
epoch 68 | loss: 0.69852 | eval_custom_logloss: 0.77101 |  0:30:55s
epoch 69 | loss: 0.68072 | eval_custom_logloss: 0.69956 |  0:31:22s
epoch 70 | loss: 0.70243 | eval_custom_logloss: 0.77451 |  0:31:49s
epoch 71 | loss: 0.699   | eval_custom_logloss: 0.87238 |  0:32:15s
epoch 72 | loss: 0.69434 | eval_custom_logloss: 0.8889  |  0:32:42s
epoch 73 | loss: 0.69607 | eval_custom_logloss: 0.83995 |  0:33:08s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.6454
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6454, 'Log Loss - std': 0.0010999999999999899} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax', 'n_a': 15, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.74525 | eval_custom_logloss: 1.29253 |  0:00:26s
epoch 1  | loss: 1.29205 | eval_custom_logloss: 1.12623 |  0:00:53s
epoch 2  | loss: 1.20455 | eval_custom_logloss: 1.11629 |  0:01:19s
epoch 3  | loss: 1.14579 | eval_custom_logloss: 1.05653 |  0:01:46s
epoch 4  | loss: 1.11437 | eval_custom_logloss: 1.02138 |  0:02:12s
epoch 5  | loss: 1.09692 | eval_custom_logloss: 1.04211 |  0:02:39s
epoch 6  | loss: 1.06854 | eval_custom_logloss: 0.96999 |  0:03:06s
epoch 7  | loss: 1.02815 | eval_custom_logloss: 0.97751 |  0:03:32s
epoch 8  | loss: 1.01387 | eval_custom_logloss: 0.91723 |  0:03:59s
epoch 9  | loss: 1.00348 | eval_custom_logloss: 1.00964 |  0:04:25s
epoch 10 | loss: 0.98498 | eval_custom_logloss: 1.03383 |  0:04:52s
epoch 11 | loss: 0.98813 | eval_custom_logloss: 0.90134 |  0:05:19s
epoch 12 | loss: 0.94961 | eval_custom_logloss: 0.85745 |  0:05:45s
epoch 13 | loss: 0.94712 | eval_custom_logloss: 0.84672 |  0:06:11s
epoch 14 | loss: 0.94712 | eval_custom_logloss: 0.84447 |  0:06:38s
epoch 15 | loss: 0.93561 | eval_custom_logloss: 0.83408 |  0:07:05s
epoch 16 | loss: 0.92644 | eval_custom_logloss: 0.8502  |  0:07:31s
epoch 17 | loss: 0.929   | eval_custom_logloss: 0.86407 |  0:07:57s
epoch 18 | loss: 0.93125 | eval_custom_logloss: 0.90406 |  0:08:24s
epoch 19 | loss: 0.92504 | eval_custom_logloss: 0.89169 |  0:08:50s
epoch 20 | loss: 0.92313 | eval_custom_logloss: 0.82083 |  0:09:16s
epoch 21 | loss: 0.88738 | eval_custom_logloss: 0.85382 |  0:09:43s
epoch 22 | loss: 0.88743 | eval_custom_logloss: 0.90145 |  0:10:09s
epoch 23 | loss: 0.86646 | eval_custom_logloss: 0.81498 |  0:10:36s
epoch 24 | loss: 0.86981 | eval_custom_logloss: 0.84099 |  0:11:02s
epoch 25 | loss: 0.86912 | eval_custom_logloss: 0.8103  |  0:11:29s
epoch 26 | loss: 0.86901 | eval_custom_logloss: 0.75815 |  0:11:55s
epoch 27 | loss: 0.84367 | eval_custom_logloss: 0.81885 |  0:12:22s
epoch 28 | loss: 0.84057 | eval_custom_logloss: 0.80834 |  0:12:47s
epoch 29 | loss: 0.84313 | eval_custom_logloss: 0.77394 |  0:13:13s
epoch 30 | loss: 0.81922 | eval_custom_logloss: 1.13481 |  0:13:39s
epoch 31 | loss: 0.8253  | eval_custom_logloss: 0.96152 |  0:14:04s
epoch 32 | loss: 0.81666 | eval_custom_logloss: 0.8441  |  0:14:30s
epoch 33 | loss: 0.81894 | eval_custom_logloss: 0.79781 |  0:14:56s
epoch 34 | loss: 0.80364 | eval_custom_logloss: 0.75238 |  0:15:21s
epoch 35 | loss: 0.82336 | eval_custom_logloss: 0.76273 |  0:15:47s
epoch 36 | loss: 0.79892 | eval_custom_logloss: 0.73484 |  0:16:13s
epoch 37 | loss: 0.80283 | eval_custom_logloss: 0.72622 |  0:16:38s
epoch 38 | loss: 0.79407 | eval_custom_logloss: 0.75444 |  0:17:04s
epoch 39 | loss: 0.78752 | eval_custom_logloss: 0.74106 |  0:17:30s
epoch 40 | loss: 0.81507 | eval_custom_logloss: 0.74666 |  0:17:55s
epoch 41 | loss: 0.78536 | eval_custom_logloss: 0.72714 |  0:18:20s
epoch 42 | loss: 0.78484 | eval_custom_logloss: 0.8752  |  0:18:46s
epoch 43 | loss: 0.78832 | eval_custom_logloss: 0.74655 |  0:19:12s
epoch 44 | loss: 0.79248 | eval_custom_logloss: 0.76478 |  0:19:37s
epoch 45 | loss: 0.79213 | eval_custom_logloss: 0.79931 |  0:20:03s
epoch 46 | loss: 0.78917 | eval_custom_logloss: 0.75118 |  0:20:29s
epoch 47 | loss: 0.79698 | eval_custom_logloss: 0.79216 |  0:20:54s
epoch 48 | loss: 0.8162  | eval_custom_logloss: 0.81855 |  0:21:20s
epoch 49 | loss: 0.80436 | eval_custom_logloss: 0.737   |  0:21:45s
epoch 50 | loss: 0.78861 | eval_custom_logloss: 0.77453 |  0:22:11s
epoch 51 | loss: 0.78972 | eval_custom_logloss: 0.98212 |  0:22:37s
epoch 52 | loss: 0.78478 | eval_custom_logloss: 0.89152 |  0:23:03s
epoch 53 | loss: 0.78527 | eval_custom_logloss: 0.81565 |  0:23:29s
epoch 54 | loss: 0.76812 | eval_custom_logloss: 0.79169 |  0:23:55s
epoch 55 | loss: 0.7772  | eval_custom_logloss: 0.87083 |  0:24:20s
epoch 56 | loss: 0.78724 | eval_custom_logloss: 0.72002 |  0:24:46s
epoch 57 | loss: 0.76595 | eval_custom_logloss: 0.73485 |  0:25:12s
epoch 58 | loss: 0.77502 | eval_custom_logloss: 0.74274 |  0:25:37s
epoch 59 | loss: 0.77839 | eval_custom_logloss: 0.74664 |  0:26:03s
epoch 60 | loss: 0.78515 | eval_custom_logloss: 0.75815 |  0:26:29s
epoch 61 | loss: 0.77514 | eval_custom_logloss: 1.66871 |  0:26:55s
epoch 62 | loss: 0.78856 | eval_custom_logloss: 0.75898 |  0:27:21s
epoch 63 | loss: 0.76726 | eval_custom_logloss: 0.76304 |  0:27:46s
epoch 64 | loss: 0.77498 | eval_custom_logloss: 0.85518 |  0:28:12s
epoch 65 | loss: 0.76813 | eval_custom_logloss: 0.73282 |  0:28:38s
epoch 66 | loss: 0.76364 | eval_custom_logloss: 0.77578 |  0:29:03s
epoch 67 | loss: 0.75955 | eval_custom_logloss: 0.74939 |  0:29:29s
epoch 68 | loss: 0.7658  | eval_custom_logloss: 0.72111 |  0:29:55s
epoch 69 | loss: 0.76027 | eval_custom_logloss: 0.73712 |  0:30:20s
epoch 70 | loss: 0.75973 | eval_custom_logloss: 0.77997 |  0:30:46s
epoch 71 | loss: 0.75458 | eval_custom_logloss: 1.02212 |  0:31:12s
epoch 72 | loss: 0.75274 | eval_custom_logloss: 0.78307 |  0:31:38s
epoch 73 | loss: 0.7537  | eval_custom_logloss: 0.87554 |  0:32:03s
epoch 74 | loss: 0.75122 | eval_custom_logloss: 0.74431 |  0:32:29s
epoch 75 | loss: 0.75222 | eval_custom_logloss: 0.7925  |  0:32:55s
epoch 76 | loss: 0.75406 | eval_custom_logloss: 0.80651 |  0:33:20s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.72002
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6698666666666666, 'Log Loss - std': 0.0346127465281538} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax', 'n_a': 15, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.75747 | eval_custom_logloss: 1.26781 |  0:00:25s
epoch 1  | loss: 1.26311 | eval_custom_logloss: 1.12118 |  0:00:51s
epoch 2  | loss: 1.21633 | eval_custom_logloss: 1.08284 |  0:01:17s
epoch 3  | loss: 1.17005 | eval_custom_logloss: 1.09941 |  0:01:42s
epoch 4  | loss: 1.13701 | eval_custom_logloss: 1.10579 |  0:02:08s
epoch 5  | loss: 1.11662 | eval_custom_logloss: 1.06185 |  0:02:34s
epoch 6  | loss: 1.10426 | eval_custom_logloss: 0.99976 |  0:02:59s
epoch 7  | loss: 1.08992 | eval_custom_logloss: 1.12169 |  0:03:25s
epoch 8  | loss: 1.05329 | eval_custom_logloss: 0.92729 |  0:03:51s
epoch 9  | loss: 1.02249 | eval_custom_logloss: 0.95733 |  0:04:17s
epoch 10 | loss: 0.98686 | eval_custom_logloss: 0.93116 |  0:04:42s
epoch 11 | loss: 0.98466 | eval_custom_logloss: 0.85212 |  0:05:08s
epoch 12 | loss: 0.9874  | eval_custom_logloss: 0.95039 |  0:05:34s
epoch 13 | loss: 0.9919  | eval_custom_logloss: 0.86771 |  0:06:00s
epoch 14 | loss: 0.95476 | eval_custom_logloss: 0.86579 |  0:06:26s
epoch 15 | loss: 0.93643 | eval_custom_logloss: 0.86342 |  0:06:51s
epoch 16 | loss: 0.91695 | eval_custom_logloss: 1.06984 |  0:07:17s
epoch 17 | loss: 0.91994 | eval_custom_logloss: 0.92269 |  0:07:43s
epoch 18 | loss: 0.90984 | eval_custom_logloss: 1.02544 |  0:08:08s
epoch 19 | loss: 0.89951 | eval_custom_logloss: 0.81261 |  0:08:34s
epoch 20 | loss: 0.89749 | eval_custom_logloss: 0.87873 |  0:09:00s
epoch 21 | loss: 0.87931 | eval_custom_logloss: 0.8792  |  0:09:26s
epoch 22 | loss: 0.88298 | eval_custom_logloss: 0.79803 |  0:09:52s
epoch 23 | loss: 0.88004 | eval_custom_logloss: 0.77462 |  0:10:17s
epoch 24 | loss: 0.89271 | eval_custom_logloss: 0.93939 |  0:10:43s
epoch 25 | loss: 0.87792 | eval_custom_logloss: 0.83906 |  0:11:09s
epoch 26 | loss: 0.9381  | eval_custom_logloss: 0.81019 |  0:11:35s
epoch 27 | loss: 0.88163 | eval_custom_logloss: 0.9877  |  0:12:01s
epoch 28 | loss: 0.87984 | eval_custom_logloss: 0.90071 |  0:12:27s
epoch 29 | loss: 0.86724 | eval_custom_logloss: 0.88745 |  0:12:52s
epoch 30 | loss: 0.84392 | eval_custom_logloss: 0.77878 |  0:13:18s
epoch 31 | loss: 0.82902 | eval_custom_logloss: 0.76479 |  0:13:43s
epoch 32 | loss: 0.82342 | eval_custom_logloss: 0.83093 |  0:14:09s
epoch 33 | loss: 0.8897  | eval_custom_logloss: 0.99861 |  0:14:35s
epoch 34 | loss: 0.8398  | eval_custom_logloss: 1.02591 |  0:15:00s
epoch 35 | loss: 0.84946 | eval_custom_logloss: 0.8786  |  0:15:26s
epoch 36 | loss: 0.80024 | eval_custom_logloss: 0.91276 |  0:15:51s
epoch 37 | loss: 0.80214 | eval_custom_logloss: 0.97585 |  0:16:17s
epoch 38 | loss: 0.78907 | eval_custom_logloss: 0.91276 |  0:16:43s
epoch 39 | loss: 0.78582 | eval_custom_logloss: 0.7834  |  0:17:09s
epoch 40 | loss: 0.79625 | eval_custom_logloss: 0.85647 |  0:17:34s
epoch 41 | loss: 0.83327 | eval_custom_logloss: 1.05045 |  0:18:00s
epoch 42 | loss: 0.79798 | eval_custom_logloss: 0.92607 |  0:18:26s
epoch 43 | loss: 0.83679 | eval_custom_logloss: 0.80474 |  0:18:52s
epoch 44 | loss: 0.7863  | eval_custom_logloss: 0.85709 |  0:19:17s
epoch 45 | loss: 0.7806  | eval_custom_logloss: 1.09155 |  0:19:43s
epoch 46 | loss: 0.78278 | eval_custom_logloss: 0.86556 |  0:20:09s
epoch 47 | loss: 0.78719 | eval_custom_logloss: 1.01359 |  0:20:35s
epoch 48 | loss: 0.78058 | eval_custom_logloss: 0.88781 |  0:21:00s
epoch 49 | loss: 0.76846 | eval_custom_logloss: 0.96092 |  0:21:26s
epoch 50 | loss: 0.77794 | eval_custom_logloss: 0.90597 |  0:21:52s
epoch 51 | loss: 0.78749 | eval_custom_logloss: 0.78701 |  0:22:18s

Early stopping occurred at epoch 51 with best_epoch = 31 and best_eval_custom_logloss = 0.76479
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6935499999999999, 'Log Loss - std': 0.05080583135822107} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax', 'n_a': 15, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71912 | eval_custom_logloss: 1.36083 |  0:00:25s
epoch 1  | loss: 1.18442 | eval_custom_logloss: 1.04733 |  0:00:51s
epoch 2  | loss: 1.11434 | eval_custom_logloss: 0.976   |  0:01:17s
epoch 3  | loss: 1.03603 | eval_custom_logloss: 0.94872 |  0:01:43s
epoch 4  | loss: 0.99188 | eval_custom_logloss: 0.97566 |  0:02:09s
epoch 5  | loss: 1.03027 | eval_custom_logloss: 0.89665 |  0:02:34s
epoch 6  | loss: 0.97976 | eval_custom_logloss: 1.11156 |  0:03:00s
epoch 7  | loss: 0.94388 | eval_custom_logloss: 0.89059 |  0:03:26s
epoch 8  | loss: 0.92913 | eval_custom_logloss: 0.8904  |  0:03:52s
epoch 9  | loss: 0.89684 | eval_custom_logloss: 0.81462 |  0:04:18s
epoch 10 | loss: 0.89    | eval_custom_logloss: 0.82658 |  0:04:43s
epoch 11 | loss: 0.88554 | eval_custom_logloss: 1.06483 |  0:05:09s
epoch 12 | loss: 0.8682  | eval_custom_logloss: 1.13213 |  0:05:34s
epoch 13 | loss: 0.89217 | eval_custom_logloss: 1.33113 |  0:06:00s
epoch 14 | loss: 0.85864 | eval_custom_logloss: 0.92497 |  0:06:26s
epoch 15 | loss: 0.85451 | eval_custom_logloss: 0.75204 |  0:06:52s
epoch 16 | loss: 0.82079 | eval_custom_logloss: 0.77274 |  0:07:17s
epoch 17 | loss: 0.82431 | eval_custom_logloss: 0.76718 |  0:07:43s
epoch 18 | loss: 0.81769 | eval_custom_logloss: 0.90956 |  0:08:08s
epoch 19 | loss: 0.79688 | eval_custom_logloss: 0.83212 |  0:08:34s
epoch 20 | loss: 0.79616 | eval_custom_logloss: 1.03446 |  0:09:00s
epoch 21 | loss: 0.78358 | eval_custom_logloss: 0.84688 |  0:09:25s
epoch 22 | loss: 0.76746 | eval_custom_logloss: 0.76669 |  0:09:51s
epoch 23 | loss: 0.76661 | eval_custom_logloss: 0.68044 |  0:10:17s
epoch 24 | loss: 0.76823 | eval_custom_logloss: 0.93505 |  0:10:43s
epoch 25 | loss: 0.7591  | eval_custom_logloss: 0.67797 |  0:11:08s
epoch 26 | loss: 0.75617 | eval_custom_logloss: 0.69868 |  0:11:34s
epoch 27 | loss: 0.75319 | eval_custom_logloss: 0.79695 |  0:12:00s
epoch 28 | loss: 0.74576 | eval_custom_logloss: 0.73442 |  0:12:25s
epoch 29 | loss: 0.74098 | eval_custom_logloss: 0.67757 |  0:12:51s
epoch 30 | loss: 0.74305 | eval_custom_logloss: 0.90796 |  0:13:17s
epoch 31 | loss: 0.73471 | eval_custom_logloss: 0.9106  |  0:13:43s
epoch 32 | loss: 0.73205 | eval_custom_logloss: 0.69035 |  0:14:09s
epoch 33 | loss: 0.76798 | eval_custom_logloss: 1.13012 |  0:14:34s
epoch 34 | loss: 0.78024 | eval_custom_logloss: 0.84352 |  0:15:00s
epoch 35 | loss: 0.74422 | eval_custom_logloss: 0.97318 |  0:15:26s
epoch 36 | loss: 0.75602 | eval_custom_logloss: 0.7716  |  0:15:52s
epoch 37 | loss: 0.72684 | eval_custom_logloss: 0.66143 |  0:16:18s
epoch 38 | loss: 0.73532 | eval_custom_logloss: 0.9095  |  0:16:43s
epoch 39 | loss: 0.72695 | eval_custom_logloss: 0.80788 |  0:17:09s
epoch 40 | loss: 0.78332 | eval_custom_logloss: 0.80151 |  0:17:35s
epoch 41 | loss: 0.72788 | eval_custom_logloss: 0.69163 |  0:18:00s
epoch 42 | loss: 0.72019 | eval_custom_logloss: 0.86145 |  0:18:25s
epoch 43 | loss: 0.71183 | eval_custom_logloss: 0.81816 |  0:18:51s
epoch 44 | loss: 0.73264 | eval_custom_logloss: 0.85402 |  0:19:16s
epoch 45 | loss: 0.71709 | eval_custom_logloss: 0.66325 |  0:19:42s
epoch 46 | loss: 0.71187 | eval_custom_logloss: 0.82132 |  0:20:07s
epoch 47 | loss: 0.71941 | eval_custom_logloss: 0.84865 |  0:20:33s
epoch 48 | loss: 0.71727 | eval_custom_logloss: 0.72552 |  0:20:59s
epoch 49 | loss: 0.69983 | eval_custom_logloss: 0.6781  |  0:21:24s
epoch 50 | loss: 0.70073 | eval_custom_logloss: 0.72423 |  0:21:50s
epoch 51 | loss: 0.69928 | eval_custom_logloss: 0.68566 |  0:22:16s
epoch 52 | loss: 0.69931 | eval_custom_logloss: 0.7855  |  0:22:41s
epoch 53 | loss: 0.70822 | eval_custom_logloss: 0.87198 |  0:23:07s
epoch 54 | loss: 0.7012  | eval_custom_logloss: 1.32452 |  0:23:33s
epoch 55 | loss: 0.70135 | eval_custom_logloss: 0.65767 |  0:23:58s
epoch 56 | loss: 0.69333 | eval_custom_logloss: 0.63712 |  0:24:24s
epoch 57 | loss: 0.69025 | eval_custom_logloss: 0.89076 |  0:24:49s
epoch 58 | loss: 0.68705 | eval_custom_logloss: 1.06283 |  0:25:15s
epoch 59 | loss: 0.68078 | eval_custom_logloss: 0.8078  |  0:25:41s
epoch 60 | loss: 0.68973 | eval_custom_logloss: 0.78066 |  0:26:07s
epoch 61 | loss: 0.67778 | eval_custom_logloss: 0.75539 |  0:26:32s
epoch 62 | loss: 0.69028 | eval_custom_logloss: 0.66419 |  0:26:58s
epoch 63 | loss: 0.69126 | eval_custom_logloss: 0.73242 |  0:27:24s
epoch 64 | loss: 0.68511 | eval_custom_logloss: 0.76043 |  0:27:50s
epoch 65 | loss: 0.69043 | eval_custom_logloss: 0.95182 |  0:28:15s
epoch 66 | loss: 0.67986 | eval_custom_logloss: 0.93321 |  0:28:41s
epoch 67 | loss: 0.67824 | eval_custom_logloss: 1.03856 |  0:29:07s
epoch 68 | loss: 0.67092 | eval_custom_logloss: 0.81109 |  0:29:32s
epoch 69 | loss: 0.67962 | eval_custom_logloss: 0.65447 |  0:29:58s
epoch 70 | loss: 0.67354 | eval_custom_logloss: 0.74003 |  0:30:24s
epoch 71 | loss: 0.7042  | eval_custom_logloss: 0.79595 |  0:30:49s
epoch 72 | loss: 0.69609 | eval_custom_logloss: 0.6591  |  0:31:15s
epoch 73 | loss: 0.68603 | eval_custom_logloss: 0.71325 |  0:31:41s
epoch 74 | loss: 0.67674 | eval_custom_logloss: 0.73179 |  0:32:06s
epoch 75 | loss: 0.6783  | eval_custom_logloss: 0.73824 |  0:32:32s
epoch 76 | loss: 0.67377 | eval_custom_logloss: 0.73113 |  0:32:58s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.63712
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6819399999999999, 'Log Loss - std': 0.051030916119544634} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 0.6819399999999999 and parameters: {'n_d': 15, 'n_steps': 9, 'gamma': 1.4915681364774445, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.06863525065603433, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.79878 | eval_custom_logloss: 13.53815|  0:00:25s
epoch 1  | loss: 1.35855 | eval_custom_logloss: 13.38287|  0:00:51s
epoch 2  | loss: 1.22389 | eval_custom_logloss: 11.21843|  0:01:17s
epoch 3  | loss: 1.16652 | eval_custom_logloss: 10.05006|  0:01:43s
epoch 4  | loss: 1.11281 | eval_custom_logloss: 9.8634  |  0:02:09s
epoch 5  | loss: 1.05773 | eval_custom_logloss: 9.32894 |  0:02:34s
epoch 6  | loss: 1.05532 | eval_custom_logloss: 7.79779 |  0:03:00s
epoch 7  | loss: 1.00258 | eval_custom_logloss: 7.34255 |  0:03:26s
epoch 8  | loss: 1.03217 | eval_custom_logloss: 6.75302 |  0:03:52s
epoch 9  | loss: 1.03786 | eval_custom_logloss: 6.33577 |  0:04:18s
epoch 10 | loss: 1.00395 | eval_custom_logloss: 4.95886 |  0:04:44s
epoch 11 | loss: 0.98892 | eval_custom_logloss: 5.38428 |  0:05:10s
epoch 12 | loss: 0.94763 | eval_custom_logloss: 7.05732 |  0:05:36s
epoch 13 | loss: 0.94766 | eval_custom_logloss: 4.11242 |  0:06:01s
epoch 14 | loss: 0.93135 | eval_custom_logloss: 5.29877 |  0:06:27s
epoch 15 | loss: 0.91767 | eval_custom_logloss: 4.93411 |  0:06:53s
epoch 16 | loss: 0.90466 | eval_custom_logloss: 5.39822 |  0:07:19s
epoch 17 | loss: 0.88231 | eval_custom_logloss: 4.43457 |  0:07:45s
epoch 18 | loss: 0.89671 | eval_custom_logloss: 4.69951 |  0:08:11s
epoch 19 | loss: 0.87291 | eval_custom_logloss: 5.02265 |  0:08:36s
epoch 20 | loss: 0.84933 | eval_custom_logloss: 2.9966  |  0:09:02s
epoch 21 | loss: 0.85879 | eval_custom_logloss: 4.3798  |  0:09:28s
epoch 22 | loss: 0.84597 | eval_custom_logloss: 5.15486 |  0:09:54s
epoch 23 | loss: 0.85554 | eval_custom_logloss: 2.84888 |  0:10:20s
epoch 24 | loss: 0.8424  | eval_custom_logloss: 2.66426 |  0:10:46s
epoch 25 | loss: 0.86178 | eval_custom_logloss: 3.47253 |  0:11:11s
epoch 26 | loss: 0.90289 | eval_custom_logloss: 3.59593 |  0:11:37s
epoch 27 | loss: 0.86938 | eval_custom_logloss: 1.69669 |  0:12:03s
epoch 28 | loss: 0.88066 | eval_custom_logloss: 2.5486  |  0:12:29s
epoch 29 | loss: 0.83927 | eval_custom_logloss: 2.72596 |  0:12:55s
epoch 30 | loss: 0.82733 | eval_custom_logloss: 1.90695 |  0:13:21s
epoch 31 | loss: 0.83114 | eval_custom_logloss: 1.72075 |  0:13:46s
epoch 32 | loss: 0.82982 | eval_custom_logloss: 1.85607 |  0:14:12s
epoch 33 | loss: 0.8336  | eval_custom_logloss: 1.84831 |  0:14:38s
epoch 34 | loss: 0.81635 | eval_custom_logloss: 1.6365  |  0:15:03s
epoch 35 | loss: 0.83795 | eval_custom_logloss: 1.58186 |  0:15:29s
epoch 36 | loss: 0.82213 | eval_custom_logloss: 1.43962 |  0:15:55s
epoch 37 | loss: 0.81151 | eval_custom_logloss: 2.27514 |  0:16:21s
epoch 38 | loss: 0.80487 | eval_custom_logloss: 1.52547 |  0:16:46s
epoch 39 | loss: 0.82414 | eval_custom_logloss: 3.00072 |  0:17:12s
epoch 40 | loss: 0.79528 | eval_custom_logloss: 3.56738 |  0:17:38s
epoch 41 | loss: 0.81725 | eval_custom_logloss: 3.61714 |  0:18:04s
epoch 42 | loss: 0.81035 | eval_custom_logloss: 1.05856 |  0:18:30s
epoch 43 | loss: 0.82114 | eval_custom_logloss: 1.18666 |  0:18:56s
epoch 44 | loss: 0.81064 | eval_custom_logloss: 1.36752 |  0:19:21s
epoch 45 | loss: 0.8021  | eval_custom_logloss: 3.62069 |  0:19:47s
epoch 46 | loss: 0.79676 | eval_custom_logloss: 0.88048 |  0:20:13s
epoch 47 | loss: 0.77938 | eval_custom_logloss: 1.36328 |  0:20:39s
epoch 48 | loss: 0.77736 | eval_custom_logloss: 1.03302 |  0:21:05s
epoch 49 | loss: 0.79228 | eval_custom_logloss: 0.99227 |  0:21:30s
epoch 50 | loss: 0.77939 | eval_custom_logloss: 1.46607 |  0:21:56s
epoch 51 | loss: 0.77594 | eval_custom_logloss: 1.9454  |  0:22:22s
epoch 52 | loss: 0.77354 | eval_custom_logloss: 3.85779 |  0:22:48s
epoch 53 | loss: 0.76206 | eval_custom_logloss: 0.85793 |  0:23:14s
epoch 54 | loss: 0.76536 | eval_custom_logloss: 2.44987 |  0:23:39s
epoch 55 | loss: 0.74941 | eval_custom_logloss: 4.62778 |  0:24:05s
epoch 56 | loss: 0.75766 | eval_custom_logloss: 2.67853 |  0:24:31s
epoch 57 | loss: 0.75058 | eval_custom_logloss: 2.00128 |  0:24:57s
epoch 58 | loss: 0.75179 | eval_custom_logloss: 0.95669 |  0:25:23s
epoch 59 | loss: 0.75509 | eval_custom_logloss: 1.45997 |  0:25:49s
epoch 60 | loss: 0.76258 | eval_custom_logloss: 3.94884 |  0:26:15s
epoch 61 | loss: 0.7522  | eval_custom_logloss: 3.62218 |  0:26:40s
epoch 62 | loss: 0.75631 | eval_custom_logloss: 4.02176 |  0:27:06s
epoch 63 | loss: 0.73936 | eval_custom_logloss: 3.80007 |  0:27:32s
epoch 64 | loss: 0.73913 | eval_custom_logloss: 1.02325 |  0:27:58s
epoch 65 | loss: 0.74949 | eval_custom_logloss: 4.19598 |  0:28:24s
epoch 66 | loss: 0.75126 | eval_custom_logloss: 0.9507  |  0:28:50s
epoch 67 | loss: 0.74691 | eval_custom_logloss: 2.90216 |  0:29:16s
epoch 68 | loss: 0.74582 | eval_custom_logloss: 3.08401 |  0:29:42s
epoch 69 | loss: 0.73081 | eval_custom_logloss: 2.2508  |  0:30:08s
epoch 70 | loss: 0.72639 | eval_custom_logloss: 1.36898 |  0:30:34s
epoch 71 | loss: 0.72555 | eval_custom_logloss: 2.53309 |  0:31:00s
epoch 72 | loss: 0.7355  | eval_custom_logloss: 2.64763 |  0:31:25s
epoch 73 | loss: 0.73486 | eval_custom_logloss: 2.77085 |  0:31:51s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.85793
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8575, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.81395 | eval_custom_logloss: 12.87677|  0:00:25s
epoch 1  | loss: 1.30758 | eval_custom_logloss: 10.75651|  0:00:51s
epoch 2  | loss: 1.21052 | eval_custom_logloss: 10.55306|  0:01:18s
epoch 3  | loss: 1.09541 | eval_custom_logloss: 9.82295 |  0:01:43s
epoch 4  | loss: 1.04957 | eval_custom_logloss: 8.61501 |  0:02:10s
epoch 5  | loss: 1.01384 | eval_custom_logloss: 9.3224  |  0:02:36s
epoch 6  | loss: 0.99403 | eval_custom_logloss: 7.10749 |  0:03:02s
epoch 7  | loss: 0.95959 | eval_custom_logloss: 6.56193 |  0:03:28s
epoch 8  | loss: 0.95406 | eval_custom_logloss: 6.58485 |  0:03:54s
epoch 9  | loss: 0.9302  | eval_custom_logloss: 6.72521 |  0:04:20s
epoch 10 | loss: 0.9198  | eval_custom_logloss: 5.74055 |  0:04:46s
epoch 11 | loss: 0.92336 | eval_custom_logloss: 5.49181 |  0:05:12s
epoch 12 | loss: 0.90016 | eval_custom_logloss: 4.74123 |  0:05:38s
epoch 13 | loss: 0.87868 | eval_custom_logloss: 6.29946 |  0:06:03s
epoch 14 | loss: 0.89005 | eval_custom_logloss: 4.65347 |  0:06:29s
epoch 15 | loss: 0.88399 | eval_custom_logloss: 4.98892 |  0:06:55s
epoch 16 | loss: 0.89109 | eval_custom_logloss: 3.44836 |  0:07:21s
epoch 17 | loss: 0.882   | eval_custom_logloss: 3.93224 |  0:07:47s
epoch 18 | loss: 0.89957 | eval_custom_logloss: 5.10805 |  0:08:13s
epoch 19 | loss: 0.86325 | eval_custom_logloss: 3.57909 |  0:08:39s
epoch 20 | loss: 0.88037 | eval_custom_logloss: 4.01535 |  0:09:05s
epoch 21 | loss: 0.87554 | eval_custom_logloss: 3.83835 |  0:09:31s
epoch 22 | loss: 0.86512 | eval_custom_logloss: 4.55378 |  0:09:57s
epoch 23 | loss: 0.88778 | eval_custom_logloss: 2.5148  |  0:10:23s
epoch 24 | loss: 0.87457 | eval_custom_logloss: 2.93571 |  0:10:49s
epoch 25 | loss: 0.85673 | eval_custom_logloss: 1.70144 |  0:11:15s
epoch 26 | loss: 0.90488 | eval_custom_logloss: 2.6435  |  0:11:41s
epoch 27 | loss: 0.87931 | eval_custom_logloss: 2.97036 |  0:12:07s
epoch 28 | loss: 0.97637 | eval_custom_logloss: 2.33072 |  0:12:33s
epoch 29 | loss: 0.97939 | eval_custom_logloss: 1.63243 |  0:12:59s
epoch 30 | loss: 0.98578 | eval_custom_logloss: 2.51817 |  0:13:25s
epoch 31 | loss: 0.98196 | eval_custom_logloss: 1.87839 |  0:13:51s
epoch 32 | loss: 0.98677 | eval_custom_logloss: 1.55579 |  0:14:17s
epoch 33 | loss: 0.98004 | eval_custom_logloss: 2.54621 |  0:14:43s
epoch 34 | loss: 0.98942 | eval_custom_logloss: 1.78162 |  0:15:09s
epoch 35 | loss: 0.87639 | eval_custom_logloss: 6.77565 |  0:15:35s
epoch 36 | loss: 0.86567 | eval_custom_logloss: 1.26262 |  0:16:01s
epoch 37 | loss: 0.84916 | eval_custom_logloss: 2.23689 |  0:16:27s
epoch 38 | loss: 0.81779 | eval_custom_logloss: 0.99185 |  0:16:53s
epoch 39 | loss: 0.82834 | eval_custom_logloss: 1.66224 |  0:17:19s
epoch 40 | loss: 0.8245  | eval_custom_logloss: 2.11944 |  0:17:45s
epoch 41 | loss: 0.80806 | eval_custom_logloss: 1.02261 |  0:18:11s
epoch 42 | loss: 0.8196  | eval_custom_logloss: 1.32992 |  0:18:37s
epoch 43 | loss: 0.82113 | eval_custom_logloss: 1.06628 |  0:19:02s
epoch 44 | loss: 0.81124 | eval_custom_logloss: 1.45252 |  0:19:28s
epoch 45 | loss: 0.81722 | eval_custom_logloss: 1.28166 |  0:19:55s
epoch 46 | loss: 0.82572 | eval_custom_logloss: 0.98952 |  0:20:21s
epoch 47 | loss: 0.79608 | eval_custom_logloss: 0.96354 |  0:20:47s
epoch 48 | loss: 0.80601 | eval_custom_logloss: 1.27254 |  0:21:12s
epoch 49 | loss: 0.7945  | eval_custom_logloss: 1.52372 |  0:21:38s
epoch 50 | loss: 0.81228 | eval_custom_logloss: 1.71051 |  0:22:04s
epoch 51 | loss: 0.80407 | eval_custom_logloss: 1.05478 |  0:22:31s
epoch 52 | loss: 0.80479 | eval_custom_logloss: 1.25328 |  0:22:57s
epoch 53 | loss: 0.79074 | eval_custom_logloss: 1.16581 |  0:23:23s
epoch 54 | loss: 0.78935 | eval_custom_logloss: 2.26005 |  0:23:49s
epoch 55 | loss: 0.80236 | eval_custom_logloss: 1.2516  |  0:24:15s
epoch 56 | loss: 0.78582 | eval_custom_logloss: 0.86911 |  0:24:41s
epoch 57 | loss: 0.78951 | eval_custom_logloss: 0.9906  |  0:25:06s
epoch 58 | loss: 0.80406 | eval_custom_logloss: 1.25273 |  0:25:32s
epoch 59 | loss: 0.79281 | eval_custom_logloss: 2.5388  |  0:25:58s
epoch 60 | loss: 0.78232 | eval_custom_logloss: 1.70691 |  0:26:24s
epoch 61 | loss: 0.78113 | eval_custom_logloss: 1.85341 |  0:26:50s
epoch 62 | loss: 0.77578 | eval_custom_logloss: 1.17506 |  0:27:16s
epoch 63 | loss: 0.79095 | eval_custom_logloss: 1.76003 |  0:27:42s
epoch 64 | loss: 0.77194 | eval_custom_logloss: 1.26174 |  0:28:08s
epoch 65 | loss: 0.77828 | eval_custom_logloss: 1.08422 |  0:28:34s
epoch 66 | loss: 0.77447 | eval_custom_logloss: 2.36603 |  0:29:00s
epoch 67 | loss: 0.77399 | eval_custom_logloss: 0.94845 |  0:29:26s
epoch 68 | loss: 0.78246 | eval_custom_logloss: 1.72395 |  0:29:52s
epoch 69 | loss: 0.77434 | eval_custom_logloss: 1.30614 |  0:30:18s
epoch 70 | loss: 0.76923 | eval_custom_logloss: 0.85013 |  0:30:44s
epoch 71 | loss: 0.77565 | eval_custom_logloss: 1.32699 |  0:31:10s
epoch 72 | loss: 0.77615 | eval_custom_logloss: 1.85013 |  0:31:36s
epoch 73 | loss: 0.77337 | eval_custom_logloss: 0.82059 |  0:32:02s
epoch 74 | loss: 0.77436 | eval_custom_logloss: 0.82326 |  0:32:27s
epoch 75 | loss: 0.78702 | eval_custom_logloss: 0.79595 |  0:32:52s
epoch 76 | loss: 0.76699 | eval_custom_logloss: 0.9433  |  0:33:18s
epoch 77 | loss: 0.76103 | eval_custom_logloss: 0.8077  |  0:33:43s
epoch 78 | loss: 0.78337 | eval_custom_logloss: 0.76169 |  0:34:08s
epoch 79 | loss: 0.77648 | eval_custom_logloss: 0.8862  |  0:34:34s
epoch 80 | loss: 0.77096 | eval_custom_logloss: 0.81947 |  0:34:59s
epoch 81 | loss: 0.75886 | eval_custom_logloss: 0.87844 |  0:35:24s
epoch 82 | loss: 0.76877 | eval_custom_logloss: 0.81406 |  0:35:50s
epoch 83 | loss: 0.7692  | eval_custom_logloss: 0.7938  |  0:36:15s
epoch 84 | loss: 0.75918 | eval_custom_logloss: 0.82805 |  0:36:40s
epoch 85 | loss: 0.77619 | eval_custom_logloss: 2.18286 |  0:37:05s
epoch 86 | loss: 0.75935 | eval_custom_logloss: 1.10618 |  0:37:31s
epoch 87 | loss: 0.76303 | eval_custom_logloss: 0.98329 |  0:37:56s
epoch 88 | loss: 0.76237 | eval_custom_logloss: 0.88236 |  0:38:21s
epoch 89 | loss: 0.76746 | eval_custom_logloss: 0.86145 |  0:38:46s
epoch 90 | loss: 0.76902 | eval_custom_logloss: 1.44188 |  0:39:12s
epoch 91 | loss: 0.76164 | eval_custom_logloss: 1.08441 |  0:39:37s
epoch 92 | loss: 0.77378 | eval_custom_logloss: 1.40024 |  0:40:03s
epoch 93 | loss: 0.76569 | eval_custom_logloss: 0.90749 |  0:40:28s
epoch 94 | loss: 0.75301 | eval_custom_logloss: 1.3288  |  0:40:53s
epoch 95 | loss: 0.76093 | eval_custom_logloss: 1.04023 |  0:41:19s
epoch 96 | loss: 0.76215 | eval_custom_logloss: 1.15812 |  0:41:44s
epoch 97 | loss: 0.78708 | eval_custom_logloss: 3.459   |  0:42:09s
epoch 98 | loss: 0.75681 | eval_custom_logloss: 1.9311  |  0:42:34s

Early stopping occurred at epoch 98 with best_epoch = 78 and best_eval_custom_logloss = 0.76169
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8089500000000001, 'Log Loss - std': 0.04855000000000004} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.81442 | eval_custom_logloss: 13.9626 |  0:00:25s
epoch 1  | loss: 1.27302 | eval_custom_logloss: 12.58026|  0:00:50s
epoch 2  | loss: 1.1793  | eval_custom_logloss: 7.96739 |  0:01:16s
epoch 3  | loss: 1.15162 | eval_custom_logloss: 8.1163  |  0:01:41s
epoch 4  | loss: 1.14052 | eval_custom_logloss: 7.69494 |  0:02:06s
epoch 5  | loss: 1.09818 | eval_custom_logloss: 7.47417 |  0:02:31s
epoch 6  | loss: 1.06073 | eval_custom_logloss: 7.70461 |  0:02:57s
epoch 7  | loss: 1.01562 | eval_custom_logloss: 5.13929 |  0:03:22s
epoch 8  | loss: 1.02924 | eval_custom_logloss: 5.88986 |  0:03:47s
epoch 9  | loss: 1.03731 | eval_custom_logloss: 4.7294  |  0:04:13s
epoch 10 | loss: 0.9739  | eval_custom_logloss: 3.71449 |  0:04:38s
epoch 11 | loss: 0.94709 | eval_custom_logloss: 4.27013 |  0:05:03s
epoch 12 | loss: 0.96583 | eval_custom_logloss: 5.05559 |  0:05:28s
epoch 13 | loss: 0.92793 | eval_custom_logloss: 4.19959 |  0:05:54s
epoch 14 | loss: 0.93376 | eval_custom_logloss: 4.91893 |  0:06:19s
epoch 15 | loss: 0.93667 | eval_custom_logloss: 3.11069 |  0:06:44s
epoch 16 | loss: 0.89566 | eval_custom_logloss: 1.80549 |  0:07:09s
epoch 17 | loss: 0.88531 | eval_custom_logloss: 2.16973 |  0:07:34s
epoch 18 | loss: 0.89632 | eval_custom_logloss: 3.80264 |  0:08:00s
epoch 19 | loss: 0.87696 | eval_custom_logloss: 3.05888 |  0:08:25s
epoch 20 | loss: 0.84876 | eval_custom_logloss: 2.15594 |  0:08:51s
epoch 21 | loss: 0.86888 | eval_custom_logloss: 2.38182 |  0:09:16s
epoch 22 | loss: 0.86183 | eval_custom_logloss: 2.8168  |  0:09:41s
epoch 23 | loss: 0.89879 | eval_custom_logloss: 2.64984 |  0:10:06s
epoch 24 | loss: 0.89184 | eval_custom_logloss: 2.40536 |  0:10:31s
epoch 25 | loss: 0.84757 | eval_custom_logloss: 4.50031 |  0:10:57s
epoch 26 | loss: 0.82718 | eval_custom_logloss: 4.12685 |  0:11:22s
epoch 27 | loss: 0.82121 | eval_custom_logloss: 1.89368 |  0:11:47s
epoch 28 | loss: 0.82901 | eval_custom_logloss: 1.67822 |  0:12:12s
epoch 29 | loss: 0.81513 | eval_custom_logloss: 2.45316 |  0:12:38s
epoch 30 | loss: 0.80904 | eval_custom_logloss: 3.3556  |  0:13:03s
epoch 31 | loss: 0.81769 | eval_custom_logloss: 3.81497 |  0:13:28s
epoch 32 | loss: 0.80903 | eval_custom_logloss: 4.12098 |  0:13:54s
epoch 33 | loss: 0.79313 | eval_custom_logloss: 2.30132 |  0:14:19s
epoch 34 | loss: 0.7998  | eval_custom_logloss: 1.56878 |  0:14:44s
epoch 35 | loss: 0.78293 | eval_custom_logloss: 1.24402 |  0:15:10s
epoch 36 | loss: 0.79383 | eval_custom_logloss: 4.55421 |  0:15:35s
epoch 37 | loss: 0.81194 | eval_custom_logloss: 1.86638 |  0:16:00s
epoch 38 | loss: 0.77309 | eval_custom_logloss: 1.11612 |  0:16:25s
epoch 39 | loss: 0.77931 | eval_custom_logloss: 3.21726 |  0:16:50s
epoch 40 | loss: 0.79855 | eval_custom_logloss: 3.52312 |  0:17:16s
epoch 41 | loss: 0.80312 | eval_custom_logloss: 3.22823 |  0:17:41s
epoch 42 | loss: 0.77601 | eval_custom_logloss: 2.48809 |  0:18:06s
epoch 43 | loss: 0.76372 | eval_custom_logloss: 0.89961 |  0:18:31s
epoch 44 | loss: 0.74491 | eval_custom_logloss: 1.24274 |  0:18:56s
epoch 45 | loss: 0.75122 | eval_custom_logloss: 1.31577 |  0:19:21s
epoch 46 | loss: 0.89168 | eval_custom_logloss: 1.70202 |  0:19:47s
epoch 47 | loss: 0.7611  | eval_custom_logloss: 1.32951 |  0:20:12s
epoch 48 | loss: 0.75314 | eval_custom_logloss: 0.93888 |  0:20:37s
epoch 49 | loss: 0.7456  | eval_custom_logloss: 1.02519 |  0:21:02s
epoch 50 | loss: 0.7412  | eval_custom_logloss: 1.50208 |  0:21:28s
epoch 51 | loss: 0.73488 | eval_custom_logloss: 0.91952 |  0:21:53s
epoch 52 | loss: 0.74262 | eval_custom_logloss: 1.01146 |  0:22:19s
epoch 53 | loss: 0.72329 | eval_custom_logloss: 1.65524 |  0:22:44s
epoch 54 | loss: 0.77251 | eval_custom_logloss: 1.18975 |  0:23:09s
epoch 55 | loss: 0.74058 | eval_custom_logloss: 3.8957  |  0:23:34s
epoch 56 | loss: 0.89385 | eval_custom_logloss: 1.85719 |  0:23:59s
epoch 57 | loss: 0.88501 | eval_custom_logloss: 1.49976 |  0:24:25s
epoch 58 | loss: 0.87642 | eval_custom_logloss: 3.44582 |  0:24:50s
epoch 59 | loss: 0.84563 | eval_custom_logloss: 1.421   |  0:25:15s
epoch 60 | loss: 0.82778 | eval_custom_logloss: 1.34238 |  0:25:41s
epoch 61 | loss: 0.81809 | eval_custom_logloss: 1.75773 |  0:26:06s
epoch 62 | loss: 0.7896  | eval_custom_logloss: 2.46255 |  0:26:31s
epoch 63 | loss: 0.77667 | eval_custom_logloss: 3.18851 |  0:26:56s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.89961
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8384666666666667, 'Log Loss - std': 0.057566213084489844} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.88241 | eval_custom_logloss: 12.11621|  0:00:24s
epoch 1  | loss: 1.29291 | eval_custom_logloss: 12.08732|  0:00:50s
epoch 2  | loss: 1.14167 | eval_custom_logloss: 10.22827|  0:01:15s
epoch 3  | loss: 1.12738 | eval_custom_logloss: 9.73709 |  0:01:40s
epoch 4  | loss: 1.16713 | eval_custom_logloss: 7.05132 |  0:02:05s
epoch 5  | loss: 1.09145 | eval_custom_logloss: 6.03166 |  0:02:30s
epoch 6  | loss: 1.03338 | eval_custom_logloss: 5.57496 |  0:02:56s
epoch 7  | loss: 1.02128 | eval_custom_logloss: 3.98047 |  0:03:21s
epoch 8  | loss: 1.03488 | eval_custom_logloss: 5.13732 |  0:03:47s
epoch 9  | loss: 1.00894 | eval_custom_logloss: 5.04641 |  0:04:12s
epoch 10 | loss: 1.02304 | eval_custom_logloss: 5.58256 |  0:04:36s
epoch 11 | loss: 0.9577  | eval_custom_logloss: 5.65715 |  0:05:01s
epoch 12 | loss: 0.92107 | eval_custom_logloss: 3.50905 |  0:05:27s
epoch 13 | loss: 0.90395 | eval_custom_logloss: 4.44956 |  0:05:52s
epoch 14 | loss: 0.90429 | eval_custom_logloss: 4.07029 |  0:06:17s
epoch 15 | loss: 0.91704 | eval_custom_logloss: 4.74575 |  0:06:42s
epoch 16 | loss: 0.87681 | eval_custom_logloss: 3.04966 |  0:07:08s
epoch 17 | loss: 0.8575  | eval_custom_logloss: 1.43548 |  0:07:33s
epoch 18 | loss: 0.8776  | eval_custom_logloss: 1.60787 |  0:07:58s
epoch 19 | loss: 0.83942 | eval_custom_logloss: 1.31993 |  0:08:23s
epoch 20 | loss: 0.83355 | eval_custom_logloss: 2.00959 |  0:08:48s
epoch 21 | loss: 0.83203 | eval_custom_logloss: 1.04361 |  0:09:13s
epoch 22 | loss: 0.82391 | eval_custom_logloss: 2.03259 |  0:09:39s
epoch 23 | loss: 0.82411 | eval_custom_logloss: 1.35258 |  0:10:04s
epoch 24 | loss: 0.79633 | eval_custom_logloss: 2.16526 |  0:10:29s
epoch 25 | loss: 0.79251 | eval_custom_logloss: 1.79385 |  0:10:55s
epoch 26 | loss: 0.79705 | eval_custom_logloss: 1.19205 |  0:11:20s
epoch 27 | loss: 0.77384 | eval_custom_logloss: 1.89673 |  0:11:45s
epoch 28 | loss: 0.79563 | eval_custom_logloss: 3.08843 |  0:12:10s
epoch 29 | loss: 0.77096 | eval_custom_logloss: 1.01279 |  0:12:36s
epoch 30 | loss: 0.76248 | eval_custom_logloss: 2.2316  |  0:13:01s
epoch 31 | loss: 0.80082 | eval_custom_logloss: 1.24916 |  0:13:26s
epoch 32 | loss: 0.7741  | eval_custom_logloss: 1.20101 |  0:13:51s
epoch 33 | loss: 0.75747 | eval_custom_logloss: 1.14025 |  0:14:17s
epoch 34 | loss: 0.75475 | eval_custom_logloss: 1.19708 |  0:14:41s
epoch 35 | loss: 0.74007 | eval_custom_logloss: 1.24006 |  0:15:07s
epoch 36 | loss: 0.74043 | eval_custom_logloss: 1.13037 |  0:15:32s
epoch 37 | loss: 0.74465 | eval_custom_logloss: 1.2677  |  0:15:57s
epoch 38 | loss: 0.73106 | eval_custom_logloss: 1.13456 |  0:16:23s
epoch 39 | loss: 0.72679 | eval_custom_logloss: 1.93813 |  0:16:48s
epoch 40 | loss: 0.73684 | eval_custom_logloss: 1.03565 |  0:17:13s
epoch 41 | loss: 0.7351  | eval_custom_logloss: 1.57484 |  0:17:38s
epoch 42 | loss: 0.72064 | eval_custom_logloss: 0.85344 |  0:18:04s
epoch 43 | loss: 0.72179 | eval_custom_logloss: 1.05576 |  0:18:29s
epoch 44 | loss: 0.71656 | eval_custom_logloss: 1.54541 |  0:18:54s
epoch 45 | loss: 0.77226 | eval_custom_logloss: 1.12102 |  0:19:19s
epoch 46 | loss: 0.75535 | eval_custom_logloss: 2.38397 |  0:19:45s
epoch 47 | loss: 0.73132 | eval_custom_logloss: 3.71234 |  0:20:10s
epoch 48 | loss: 0.74844 | eval_custom_logloss: 2.16227 |  0:20:35s
epoch 49 | loss: 0.72328 | eval_custom_logloss: 2.82874 |  0:21:01s
epoch 50 | loss: 0.73153 | eval_custom_logloss: 1.12404 |  0:21:26s
epoch 51 | loss: 0.72431 | eval_custom_logloss: 1.21191 |  0:21:51s
epoch 52 | loss: 0.73395 | eval_custom_logloss: 1.51769 |  0:22:16s
epoch 53 | loss: 0.71728 | eval_custom_logloss: 2.69939 |  0:22:42s
epoch 54 | loss: 0.70115 | eval_custom_logloss: 2.57558 |  0:23:07s
epoch 55 | loss: 0.70174 | eval_custom_logloss: 2.17097 |  0:23:33s
epoch 56 | loss: 0.70437 | eval_custom_logloss: 4.58431 |  0:23:58s
epoch 57 | loss: 0.69276 | eval_custom_logloss: 2.01918 |  0:24:23s
epoch 58 | loss: 0.71237 | eval_custom_logloss: 3.18075 |  0:24:49s
epoch 59 | loss: 0.70155 | eval_custom_logloss: 2.58976 |  0:25:14s
epoch 60 | loss: 0.70653 | eval_custom_logloss: 1.3462  |  0:25:40s
epoch 61 | loss: 0.68646 | eval_custom_logloss: 1.94933 |  0:26:05s
epoch 62 | loss: 0.68007 | eval_custom_logloss: 3.49897 |  0:26:30s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.85344
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.841975, 'Log Loss - std': 0.05022277247424719} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.84151 | eval_custom_logloss: 14.00304|  0:00:24s
epoch 1  | loss: 1.34648 | eval_custom_logloss: 13.66402|  0:00:50s
epoch 2  | loss: 1.23634 | eval_custom_logloss: 12.46153|  0:01:15s
epoch 3  | loss: 1.17956 | eval_custom_logloss: 12.86601|  0:01:40s
epoch 4  | loss: 1.12385 | eval_custom_logloss: 10.95973|  0:02:05s
epoch 5  | loss: 1.09062 | eval_custom_logloss: 10.71561|  0:02:30s
epoch 6  | loss: 1.07755 | eval_custom_logloss: 8.8415  |  0:02:56s
epoch 7  | loss: 1.06184 | eval_custom_logloss: 7.86492 |  0:03:21s
epoch 8  | loss: 1.07126 | eval_custom_logloss: 6.2555  |  0:03:47s
epoch 9  | loss: 1.03993 | eval_custom_logloss: 5.91648 |  0:04:12s
epoch 10 | loss: 1.01044 | eval_custom_logloss: 4.42177 |  0:04:37s
epoch 11 | loss: 1.00169 | eval_custom_logloss: 4.24273 |  0:05:02s
epoch 12 | loss: 1.01806 | eval_custom_logloss: 5.66132 |  0:05:28s
epoch 13 | loss: 0.99639 | eval_custom_logloss: 5.59958 |  0:05:53s
epoch 14 | loss: 0.98055 | eval_custom_logloss: 4.78466 |  0:06:18s
epoch 15 | loss: 0.97406 | eval_custom_logloss: 6.09788 |  0:06:43s
epoch 16 | loss: 0.95921 | eval_custom_logloss: 4.56143 |  0:07:08s
epoch 17 | loss: 0.9388  | eval_custom_logloss: 5.87957 |  0:07:34s
epoch 18 | loss: 0.95626 | eval_custom_logloss: 4.97709 |  0:07:59s
epoch 19 | loss: 0.90881 | eval_custom_logloss: 6.33175 |  0:08:25s
epoch 20 | loss: 0.90908 | eval_custom_logloss: 5.32684 |  0:08:50s
epoch 21 | loss: 0.90718 | eval_custom_logloss: 4.90953 |  0:09:15s
epoch 22 | loss: 0.90833 | eval_custom_logloss: 3.69791 |  0:09:41s
epoch 23 | loss: 0.8791  | eval_custom_logloss: 3.40427 |  0:10:06s
epoch 24 | loss: 0.86145 | eval_custom_logloss: 2.5384  |  0:10:31s
epoch 25 | loss: 0.86804 | eval_custom_logloss: 2.09466 |  0:10:57s
epoch 26 | loss: 0.86539 | eval_custom_logloss: 3.03171 |  0:11:23s
epoch 27 | loss: 0.85558 | eval_custom_logloss: 2.51067 |  0:11:48s
epoch 28 | loss: 0.83501 | eval_custom_logloss: 1.91351 |  0:12:13s
epoch 29 | loss: 0.82009 | eval_custom_logloss: 1.83767 |  0:12:38s
epoch 30 | loss: 0.82481 | eval_custom_logloss: 3.03621 |  0:13:04s
epoch 31 | loss: 0.84196 | eval_custom_logloss: 2.92986 |  0:13:29s
epoch 32 | loss: 0.81128 | eval_custom_logloss: 1.93846 |  0:13:54s
epoch 33 | loss: 0.82345 | eval_custom_logloss: 2.16503 |  0:14:20s
epoch 34 | loss: 0.82871 | eval_custom_logloss: 2.54992 |  0:14:45s
epoch 35 | loss: 0.81604 | eval_custom_logloss: 3.5421  |  0:15:10s
epoch 36 | loss: 0.81278 | eval_custom_logloss: 3.17413 |  0:15:35s
epoch 37 | loss: 0.81215 | eval_custom_logloss: 1.87142 |  0:16:01s
epoch 38 | loss: 0.79608 | eval_custom_logloss: 2.09198 |  0:16:26s
epoch 39 | loss: 0.82077 | eval_custom_logloss: 1.83761 |  0:16:51s
epoch 40 | loss: 0.8054  | eval_custom_logloss: 1.96249 |  0:17:16s
epoch 41 | loss: 0.79814 | eval_custom_logloss: 1.75914 |  0:17:41s
epoch 42 | loss: 0.79238 | eval_custom_logloss: 2.07786 |  0:18:07s
epoch 43 | loss: 0.78848 | eval_custom_logloss: 2.39746 |  0:18:32s
epoch 44 | loss: 0.79441 | eval_custom_logloss: 2.10215 |  0:18:58s
epoch 45 | loss: 0.7827  | eval_custom_logloss: 1.7234  |  0:19:23s
epoch 46 | loss: 0.78058 | eval_custom_logloss: 1.06403 |  0:19:48s
epoch 47 | loss: 0.77639 | eval_custom_logloss: 0.85839 |  0:20:14s
epoch 48 | loss: 0.79026 | eval_custom_logloss: 0.97533 |  0:20:39s
epoch 49 | loss: 0.7732  | eval_custom_logloss: 2.13598 |  0:21:04s
epoch 50 | loss: 0.77904 | eval_custom_logloss: 1.31411 |  0:21:29s
epoch 51 | loss: 0.78078 | eval_custom_logloss: 0.9787  |  0:21:55s
epoch 52 | loss: 0.76671 | eval_custom_logloss: 0.95828 |  0:22:20s
epoch 53 | loss: 0.77644 | eval_custom_logloss: 1.25162 |  0:22:45s
epoch 54 | loss: 0.78646 | eval_custom_logloss: 1.47223 |  0:23:11s
epoch 55 | loss: 0.78034 | eval_custom_logloss: 1.41639 |  0:23:36s
epoch 56 | loss: 0.78615 | eval_custom_logloss: 3.15405 |  0:24:01s
epoch 57 | loss: 0.77666 | eval_custom_logloss: 2.45693 |  0:24:26s
epoch 58 | loss: 0.79692 | eval_custom_logloss: 1.63722 |  0:24:51s
epoch 59 | loss: 0.77507 | eval_custom_logloss: 1.28571 |  0:25:17s
epoch 60 | loss: 0.77391 | eval_custom_logloss: 1.31406 |  0:25:42s
epoch 61 | loss: 0.76933 | eval_custom_logloss: 1.12863 |  0:26:07s
epoch 62 | loss: 0.76095 | eval_custom_logloss: 1.15179 |  0:26:32s
epoch 63 | loss: 0.74764 | eval_custom_logloss: 0.82587 |  0:26:57s
epoch 64 | loss: 0.76164 | eval_custom_logloss: 1.18214 |  0:27:23s
epoch 65 | loss: 0.75621 | eval_custom_logloss: 1.36007 |  0:27:48s
epoch 66 | loss: 0.75115 | eval_custom_logloss: 1.01717 |  0:28:13s
epoch 67 | loss: 0.76629 | eval_custom_logloss: 0.89683 |  0:28:38s
epoch 68 | loss: 0.7612  | eval_custom_logloss: 0.86242 |  0:29:04s
epoch 69 | loss: 0.75069 | eval_custom_logloss: 1.1094  |  0:29:30s
epoch 70 | loss: 0.77078 | eval_custom_logloss: 1.26673 |  0:29:55s
epoch 71 | loss: 0.76031 | eval_custom_logloss: 1.28427 |  0:30:20s
epoch 72 | loss: 0.77291 | eval_custom_logloss: 1.85614 |  0:30:45s
epoch 73 | loss: 0.78067 | eval_custom_logloss: 1.66276 |  0:31:10s
epoch 74 | loss: 0.75193 | eval_custom_logloss: 1.22455 |  0:31:35s
epoch 75 | loss: 0.75412 | eval_custom_logloss: 1.06861 |  0:32:01s
epoch 76 | loss: 0.77634 | eval_custom_logloss: 1.87138 |  0:32:26s
epoch 77 | loss: 0.77136 | eval_custom_logloss: 1.56634 |  0:32:51s
epoch 78 | loss: 0.75101 | eval_custom_logloss: 1.03509 |  0:33:16s
epoch 79 | loss: 0.74775 | eval_custom_logloss: 0.97551 |  0:33:41s
epoch 80 | loss: 0.7501  | eval_custom_logloss: 1.09162 |  0:34:07s
epoch 81 | loss: 0.75777 | eval_custom_logloss: 1.20546 |  0:34:32s
epoch 82 | loss: 0.74571 | eval_custom_logloss: 1.09844 |  0:34:57s
epoch 83 | loss: 0.73911 | eval_custom_logloss: 0.82749 |  0:35:22s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 0.82587
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.83844, 'Log Loss - std': 0.04547357914217882} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 0.83844 and parameters: {'n_d': 26, 'n_steps': 10, 'gamma': 1.682235388746962, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0010390937576289076, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.90128 | eval_custom_logloss: 1.32537 |  0:00:25s
epoch 1  | loss: 1.3049  | eval_custom_logloss: 1.11277 |  0:00:50s
epoch 2  | loss: 1.20013 | eval_custom_logloss: 1.03549 |  0:01:15s
epoch 3  | loss: 1.09627 | eval_custom_logloss: 1.07268 |  0:01:41s
epoch 4  | loss: 1.0739  | eval_custom_logloss: 0.88744 |  0:02:06s
epoch 5  | loss: 1.02861 | eval_custom_logloss: 0.88802 |  0:02:31s
epoch 6  | loss: 1.01235 | eval_custom_logloss: 0.8771  |  0:02:56s
epoch 7  | loss: 0.99016 | eval_custom_logloss: 0.89575 |  0:03:21s
epoch 8  | loss: 0.98445 | eval_custom_logloss: 0.90579 |  0:03:47s
epoch 9  | loss: 0.95251 | eval_custom_logloss: 0.91568 |  0:04:12s
epoch 10 | loss: 0.94267 | eval_custom_logloss: 0.85541 |  0:04:37s
epoch 11 | loss: 0.94418 | eval_custom_logloss: 0.83072 |  0:05:02s
epoch 12 | loss: 0.9348  | eval_custom_logloss: 0.8701  |  0:05:28s
epoch 13 | loss: 0.92966 | eval_custom_logloss: 1.05513 |  0:05:53s
epoch 14 | loss: 0.90544 | eval_custom_logloss: 0.9161  |  0:06:18s
epoch 15 | loss: 0.8952  | eval_custom_logloss: 0.81532 |  0:06:43s
epoch 16 | loss: 0.91482 | eval_custom_logloss: 0.88165 |  0:07:08s
epoch 17 | loss: 0.91254 | eval_custom_logloss: 0.7752  |  0:07:33s
epoch 18 | loss: 0.89727 | eval_custom_logloss: 0.90998 |  0:07:59s
epoch 19 | loss: 0.86864 | eval_custom_logloss: 0.81693 |  0:08:24s
epoch 20 | loss: 0.85703 | eval_custom_logloss: 0.76125 |  0:08:49s
epoch 21 | loss: 0.84315 | eval_custom_logloss: 0.77119 |  0:09:14s
epoch 22 | loss: 0.83223 | eval_custom_logloss: 0.82653 |  0:09:39s
epoch 23 | loss: 0.81693 | eval_custom_logloss: 0.74141 |  0:10:04s
epoch 24 | loss: 0.81989 | eval_custom_logloss: 1.08404 |  0:10:30s
epoch 25 | loss: 0.82706 | eval_custom_logloss: 0.75452 |  0:10:55s
epoch 26 | loss: 0.80507 | eval_custom_logloss: 0.71556 |  0:11:20s
epoch 27 | loss: 0.81694 | eval_custom_logloss: 0.72501 |  0:11:45s
epoch 28 | loss: 0.79969 | eval_custom_logloss: 0.7393  |  0:12:10s
epoch 29 | loss: 0.80116 | eval_custom_logloss: 0.84061 |  0:12:35s
epoch 30 | loss: 0.79809 | eval_custom_logloss: 0.79347 |  0:13:01s
epoch 31 | loss: 0.79289 | eval_custom_logloss: 1.46597 |  0:13:26s
epoch 32 | loss: 0.77606 | eval_custom_logloss: 0.91016 |  0:13:51s
epoch 33 | loss: 0.79235 | eval_custom_logloss: 0.80673 |  0:14:16s
epoch 34 | loss: 0.77318 | eval_custom_logloss: 1.01435 |  0:14:42s
epoch 35 | loss: 0.77746 | eval_custom_logloss: 0.77441 |  0:15:07s
epoch 36 | loss: 0.767   | eval_custom_logloss: 0.70153 |  0:15:33s
epoch 37 | loss: 0.7768  | eval_custom_logloss: 0.76284 |  0:15:58s
epoch 38 | loss: 0.75841 | eval_custom_logloss: 0.81096 |  0:16:23s
epoch 39 | loss: 0.75732 | eval_custom_logloss: 0.82542 |  0:16:49s
epoch 40 | loss: 0.75937 | eval_custom_logloss: 0.73103 |  0:17:14s
epoch 41 | loss: 0.77212 | eval_custom_logloss: 0.84242 |  0:17:39s
epoch 42 | loss: 0.75847 | eval_custom_logloss: 0.71488 |  0:18:04s
epoch 43 | loss: 0.74514 | eval_custom_logloss: 0.73729 |  0:18:30s
epoch 44 | loss: 0.75003 | eval_custom_logloss: 0.7099  |  0:18:55s
epoch 45 | loss: 0.75125 | eval_custom_logloss: 0.80779 |  0:19:20s
epoch 46 | loss: 0.7434  | eval_custom_logloss: 0.77166 |  0:19:46s
epoch 47 | loss: 0.7423  | eval_custom_logloss: 0.76479 |  0:20:11s
epoch 48 | loss: 0.74826 | eval_custom_logloss: 0.67082 |  0:20:38s
epoch 49 | loss: 0.74254 | eval_custom_logloss: 0.95715 |  0:21:09s
epoch 50 | loss: 0.73997 | eval_custom_logloss: 0.70525 |  0:21:42s
epoch 51 | loss: 0.75407 | eval_custom_logloss: 0.91982 |  0:22:16s
epoch 52 | loss: 0.71795 | eval_custom_logloss: 0.68301 |  0:22:50s
epoch 53 | loss: 0.72637 | eval_custom_logloss: 0.80734 |  0:23:23s
epoch 54 | loss: 0.72895 | eval_custom_logloss: 0.83792 |  0:23:56s
epoch 55 | loss: 0.73874 | eval_custom_logloss: 0.74207 |  0:24:29s
epoch 56 | loss: 0.72923 | eval_custom_logloss: 0.79006 |  0:25:02s
epoch 57 | loss: 0.73312 | eval_custom_logloss: 0.68529 |  0:25:35s
epoch 58 | loss: 0.72454 | eval_custom_logloss: 0.79891 |  0:26:08s
epoch 59 | loss: 0.72511 | eval_custom_logloss: 0.81077 |  0:26:41s
epoch 60 | loss: 0.71541 | eval_custom_logloss: 0.95166 |  0:27:15s
epoch 61 | loss: 0.70331 | eval_custom_logloss: 0.89    |  0:27:48s
epoch 62 | loss: 0.72243 | eval_custom_logloss: 0.7527  |  0:28:21s
epoch 63 | loss: 0.71467 | eval_custom_logloss: 0.74424 |  0:28:55s
epoch 64 | loss: 0.72202 | eval_custom_logloss: 0.75782 |  0:29:28s
epoch 65 | loss: 0.71343 | eval_custom_logloss: 0.6656  |  0:30:01s
epoch 66 | loss: 0.70851 | eval_custom_logloss: 0.75006 |  0:30:34s
epoch 67 | loss: 0.70785 | eval_custom_logloss: 0.7582  |  0:31:07s
epoch 68 | loss: 0.70393 | eval_custom_logloss: 0.88056 |  0:31:41s
epoch 69 | loss: 0.70858 | eval_custom_logloss: 0.69648 |  0:32:14s
epoch 70 | loss: 0.7146  | eval_custom_logloss: 0.71433 |  0:32:47s
epoch 71 | loss: 0.7157  | eval_custom_logloss: 0.67331 |  0:33:20s
epoch 72 | loss: 0.71633 | eval_custom_logloss: 0.68452 |  0:33:54s
epoch 73 | loss: 0.70547 | eval_custom_logloss: 0.7328  |  0:34:26s
epoch 74 | loss: 0.70717 | eval_custom_logloss: 0.777   |  0:35:00s
epoch 75 | loss: 0.69559 | eval_custom_logloss: 0.70973 |  0:35:33s
epoch 76 | loss: 0.7008  | eval_custom_logloss: 0.94103 |  0:36:06s
epoch 77 | loss: 0.70017 | eval_custom_logloss: 0.75414 |  0:36:39s
epoch 78 | loss: 0.69194 | eval_custom_logloss: 0.64801 |  0:37:12s
epoch 79 | loss: 0.69972 | eval_custom_logloss: 0.67255 |  0:37:45s
epoch 80 | loss: 0.70089 | eval_custom_logloss: 0.68625 |  0:38:18s
epoch 81 | loss: 0.68599 | eval_custom_logloss: 0.78377 |  0:38:51s
epoch 82 | loss: 0.69798 | eval_custom_logloss: 0.72051 |  0:39:24s
epoch 83 | loss: 0.69312 | eval_custom_logloss: 0.88794 |  0:39:58s
epoch 84 | loss: 0.69744 | eval_custom_logloss: 0.75257 |  0:40:31s
epoch 85 | loss: 0.6949  | eval_custom_logloss: 0.66736 |  0:41:04s
epoch 86 | loss: 0.6925  | eval_custom_logloss: 0.70404 |  0:41:36s
epoch 87 | loss: 0.6911  | eval_custom_logloss: 0.67826 |  0:42:10s
epoch 88 | loss: 0.68668 | eval_custom_logloss: 1.04833 |  0:42:43s
epoch 89 | loss: 0.68251 | eval_custom_logloss: 0.72768 |  0:43:15s
epoch 90 | loss: 0.68704 | eval_custom_logloss: 1.10823 |  0:43:49s
epoch 91 | loss: 0.69045 | eval_custom_logloss: 0.73049 |  0:44:22s
epoch 92 | loss: 0.69949 | eval_custom_logloss: 0.76284 |  0:44:55s
epoch 93 | loss: 0.69037 | eval_custom_logloss: 0.68936 |  0:45:28s
epoch 94 | loss: 0.689   | eval_custom_logloss: 0.69001 |  0:46:01s
epoch 95 | loss: 0.68724 | eval_custom_logloss: 0.82588 |  0:46:34s
epoch 96 | loss: 0.69587 | eval_custom_logloss: 0.77213 |  0:47:07s
epoch 97 | loss: 0.6906  | eval_custom_logloss: 0.69794 |  0:47:41s
epoch 98 | loss: 0.68928 | eval_custom_logloss: 1.04803 |  0:48:14s

Early stopping occurred at epoch 98 with best_epoch = 78 and best_eval_custom_logloss = 0.64801
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6467, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.78656 | eval_custom_logloss: 1.33348 |  0:00:33s
epoch 1  | loss: 1.35012 | eval_custom_logloss: 1.21679 |  0:01:06s
epoch 2  | loss: 1.21316 | eval_custom_logloss: 1.06913 |  0:01:39s
epoch 3  | loss: 1.11576 | eval_custom_logloss: 1.01719 |  0:02:13s
epoch 4  | loss: 1.0884  | eval_custom_logloss: 0.94415 |  0:02:46s
epoch 5  | loss: 1.06648 | eval_custom_logloss: 0.99313 |  0:03:19s
epoch 6  | loss: 1.03948 | eval_custom_logloss: 0.90353 |  0:03:52s
epoch 7  | loss: 1.01647 | eval_custom_logloss: 0.92721 |  0:04:24s
epoch 8  | loss: 1.07821 | eval_custom_logloss: 0.97946 |  0:04:57s
epoch 9  | loss: 1.07234 | eval_custom_logloss: 1.00947 |  0:05:30s
epoch 10 | loss: 1.04336 | eval_custom_logloss: 0.98685 |  0:06:04s
epoch 11 | loss: 1.02376 | eval_custom_logloss: 0.9749  |  0:06:37s
epoch 12 | loss: 0.99312 | eval_custom_logloss: 0.90221 |  0:07:10s
epoch 13 | loss: 0.96945 | eval_custom_logloss: 1.09688 |  0:07:43s
epoch 14 | loss: 0.94659 | eval_custom_logloss: 0.94208 |  0:08:16s
epoch 15 | loss: 0.92771 | eval_custom_logloss: 0.91247 |  0:08:49s
epoch 16 | loss: 0.94248 | eval_custom_logloss: 0.87154 |  0:09:22s
epoch 17 | loss: 0.91894 | eval_custom_logloss: 0.812   |  0:09:55s
epoch 18 | loss: 0.88703 | eval_custom_logloss: 0.81213 |  0:10:28s
epoch 19 | loss: 0.87675 | eval_custom_logloss: 0.84904 |  0:11:01s
epoch 20 | loss: 0.85937 | eval_custom_logloss: 0.98184 |  0:11:33s
epoch 21 | loss: 0.86563 | eval_custom_logloss: 0.80153 |  0:12:06s
epoch 22 | loss: 0.83594 | eval_custom_logloss: 0.83345 |  0:12:39s
epoch 23 | loss: 0.82511 | eval_custom_logloss: 0.78822 |  0:13:12s
epoch 24 | loss: 0.84267 | eval_custom_logloss: 0.84011 |  0:13:45s
epoch 25 | loss: 0.82925 | eval_custom_logloss: 0.75822 |  0:14:18s
epoch 26 | loss: 0.83009 | eval_custom_logloss: 0.81428 |  0:14:51s
epoch 27 | loss: 0.81336 | eval_custom_logloss: 0.81925 |  0:15:24s
epoch 28 | loss: 0.82197 | eval_custom_logloss: 0.7726  |  0:15:56s
epoch 29 | loss: 0.81988 | eval_custom_logloss: 1.23093 |  0:16:29s
epoch 30 | loss: 0.8221  | eval_custom_logloss: 1.45549 |  0:17:02s
epoch 31 | loss: 0.83978 | eval_custom_logloss: 0.85697 |  0:17:36s
epoch 32 | loss: 0.81781 | eval_custom_logloss: 0.79209 |  0:18:09s
epoch 33 | loss: 0.82008 | eval_custom_logloss: 0.74808 |  0:18:41s
epoch 34 | loss: 0.80502 | eval_custom_logloss: 1.07053 |  0:19:14s
epoch 35 | loss: 0.80541 | eval_custom_logloss: 0.75046 |  0:19:47s
epoch 36 | loss: 0.80124 | eval_custom_logloss: 0.85513 |  0:20:20s
epoch 37 | loss: 0.78889 | eval_custom_logloss: 0.75776 |  0:20:53s
epoch 38 | loss: 0.7839  | eval_custom_logloss: 0.83929 |  0:21:25s
epoch 39 | loss: 0.77773 | eval_custom_logloss: 1.23641 |  0:21:58s
epoch 40 | loss: 0.77941 | eval_custom_logloss: 0.78804 |  0:22:31s
epoch 41 | loss: 0.78061 | eval_custom_logloss: 0.78239 |  0:23:04s
epoch 42 | loss: 0.77487 | eval_custom_logloss: 0.77552 |  0:23:36s
epoch 43 | loss: 0.76922 | eval_custom_logloss: 0.91252 |  0:24:09s
epoch 44 | loss: 0.77518 | eval_custom_logloss: 0.84338 |  0:24:42s
epoch 45 | loss: 0.7807  | eval_custom_logloss: 1.05066 |  0:25:15s
epoch 46 | loss: 0.77803 | eval_custom_logloss: 0.96846 |  0:25:49s
epoch 47 | loss: 0.78244 | eval_custom_logloss: 0.7386  |  0:26:21s
epoch 48 | loss: 0.77797 | eval_custom_logloss: 1.23103 |  0:26:54s
epoch 49 | loss: 0.78007 | eval_custom_logloss: 0.87578 |  0:27:27s
epoch 50 | loss: 0.76835 | eval_custom_logloss: 0.81914 |  0:28:00s
epoch 51 | loss: 0.78672 | eval_custom_logloss: 0.74678 |  0:28:33s
epoch 52 | loss: 0.77907 | eval_custom_logloss: 0.7442  |  0:29:06s
epoch 53 | loss: 0.77283 | eval_custom_logloss: 0.73001 |  0:29:38s
epoch 54 | loss: 0.77098 | eval_custom_logloss: 0.84986 |  0:30:11s
epoch 55 | loss: 0.81268 | eval_custom_logloss: 0.86277 |  0:30:44s
epoch 56 | loss: 0.7791  | eval_custom_logloss: 0.84509 |  0:31:17s
epoch 57 | loss: 0.7755  | eval_custom_logloss: 0.88465 |  0:31:49s
epoch 58 | loss: 0.82375 | eval_custom_logloss: 0.8529  |  0:32:22s
epoch 59 | loss: 0.83123 | eval_custom_logloss: 0.75553 |  0:32:55s
epoch 60 | loss: 0.80785 | eval_custom_logloss: 0.76511 |  0:33:28s
epoch 61 | loss: 0.82089 | eval_custom_logloss: 0.83818 |  0:34:00s
epoch 62 | loss: 0.83582 | eval_custom_logloss: 0.87108 |  0:34:32s
epoch 63 | loss: 0.82793 | eval_custom_logloss: 0.84062 |  0:35:05s
epoch 64 | loss: 0.81528 | eval_custom_logloss: 1.09373 |  0:35:39s
epoch 65 | loss: 0.78616 | eval_custom_logloss: 0.79622 |  0:36:11s
epoch 66 | loss: 0.78241 | eval_custom_logloss: 0.94377 |  0:36:45s
epoch 67 | loss: 0.80498 | eval_custom_logloss: 0.94865 |  0:37:18s
epoch 68 | loss: 0.83729 | eval_custom_logloss: 0.76819 |  0:37:51s
epoch 69 | loss: 0.82559 | eval_custom_logloss: 0.7439  |  0:38:24s
epoch 70 | loss: 0.80771 | eval_custom_logloss: 0.78573 |  0:38:57s
epoch 71 | loss: 0.81556 | eval_custom_logloss: 0.81727 |  0:39:30s
epoch 72 | loss: 0.85966 | eval_custom_logloss: 0.82867 |  0:40:04s
epoch 73 | loss: 0.83451 | eval_custom_logloss: 0.78682 |  0:40:37s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.73001
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.68805, 'Log Loss - std': 0.04135} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.83556 | eval_custom_logloss: 1.32095 |  0:00:33s
epoch 1  | loss: 1.28806 | eval_custom_logloss: 1.07739 |  0:01:06s
epoch 2  | loss: 1.14128 | eval_custom_logloss: 1.03077 |  0:01:38s
epoch 3  | loss: 1.09726 | eval_custom_logloss: 1.00777 |  0:02:12s
epoch 4  | loss: 1.11064 | eval_custom_logloss: 0.96762 |  0:02:45s
epoch 5  | loss: 1.12259 | eval_custom_logloss: 0.98828 |  0:03:17s
epoch 6  | loss: 1.21323 | eval_custom_logloss: 1.17274 |  0:03:50s
epoch 7  | loss: 1.09554 | eval_custom_logloss: 0.9264  |  0:04:23s
epoch 8  | loss: 1.03476 | eval_custom_logloss: 0.8693  |  0:04:56s
epoch 9  | loss: 1.02533 | eval_custom_logloss: 0.92484 |  0:05:29s
epoch 10 | loss: 1.03085 | eval_custom_logloss: 0.93933 |  0:06:02s
epoch 11 | loss: 0.97426 | eval_custom_logloss: 0.92755 |  0:06:35s
epoch 12 | loss: 0.94844 | eval_custom_logloss: 0.80193 |  0:07:08s
epoch 13 | loss: 0.9402  | eval_custom_logloss: 0.79015 |  0:07:41s
epoch 14 | loss: 0.90039 | eval_custom_logloss: 0.86163 |  0:08:14s
epoch 15 | loss: 0.8825  | eval_custom_logloss: 0.81216 |  0:08:47s
epoch 16 | loss: 0.86762 | eval_custom_logloss: 0.78586 |  0:09:20s
epoch 17 | loss: 0.85717 | eval_custom_logloss: 0.82673 |  0:09:53s
epoch 18 | loss: 0.84041 | eval_custom_logloss: 0.82398 |  0:10:26s
epoch 19 | loss: 0.83562 | eval_custom_logloss: 0.88461 |  0:10:59s
epoch 20 | loss: 0.81134 | eval_custom_logloss: 0.93285 |  0:11:31s
epoch 21 | loss: 0.82547 | eval_custom_logloss: 0.85495 |  0:12:04s
epoch 22 | loss: 0.81824 | eval_custom_logloss: 0.75074 |  0:12:37s
epoch 23 | loss: 0.79912 | eval_custom_logloss: 0.71741 |  0:13:10s
epoch 24 | loss: 0.78852 | eval_custom_logloss: 0.81799 |  0:13:42s
epoch 25 | loss: 0.78162 | eval_custom_logloss: 0.77825 |  0:14:15s
epoch 26 | loss: 0.77202 | eval_custom_logloss: 0.75633 |  0:14:48s
epoch 27 | loss: 0.77164 | eval_custom_logloss: 0.82391 |  0:15:21s
epoch 28 | loss: 0.75768 | eval_custom_logloss: 0.90638 |  0:15:54s
epoch 29 | loss: 0.75755 | eval_custom_logloss: 0.76713 |  0:16:27s
epoch 30 | loss: 0.7507  | eval_custom_logloss: 0.76322 |  0:17:00s
epoch 31 | loss: 0.74999 | eval_custom_logloss: 0.97225 |  0:17:32s
epoch 32 | loss: 0.74942 | eval_custom_logloss: 0.8922  |  0:18:05s
epoch 33 | loss: 0.78502 | eval_custom_logloss: 0.81103 |  0:18:38s
epoch 34 | loss: 0.77489 | eval_custom_logloss: 0.84073 |  0:19:11s
epoch 35 | loss: 0.76397 | eval_custom_logloss: 0.81562 |  0:19:44s
epoch 36 | loss: 0.78697 | eval_custom_logloss: 0.79894 |  0:20:17s
epoch 37 | loss: 0.75272 | eval_custom_logloss: 0.80311 |  0:20:50s
epoch 38 | loss: 0.76782 | eval_custom_logloss: 0.82491 |  0:21:23s
epoch 39 | loss: 0.75158 | eval_custom_logloss: 0.68323 |  0:21:56s
epoch 40 | loss: 0.74485 | eval_custom_logloss: 0.71035 |  0:22:29s
epoch 41 | loss: 0.74605 | eval_custom_logloss: 0.69743 |  0:23:02s
epoch 42 | loss: 0.72586 | eval_custom_logloss: 0.68281 |  0:23:35s
epoch 43 | loss: 0.72372 | eval_custom_logloss: 0.83748 |  0:24:07s
epoch 44 | loss: 0.73282 | eval_custom_logloss: 0.77603 |  0:24:40s
epoch 45 | loss: 0.72386 | eval_custom_logloss: 0.66472 |  0:25:13s
epoch 46 | loss: 0.77208 | eval_custom_logloss: 0.8536  |  0:25:46s
epoch 47 | loss: 0.74902 | eval_custom_logloss: 0.74363 |  0:26:19s
epoch 48 | loss: 0.7541  | eval_custom_logloss: 0.76016 |  0:26:52s
epoch 49 | loss: 0.73413 | eval_custom_logloss: 0.82772 |  0:27:25s
epoch 50 | loss: 0.71955 | eval_custom_logloss: 0.73158 |  0:27:58s
epoch 51 | loss: 0.72048 | eval_custom_logloss: 0.87156 |  0:28:31s
epoch 52 | loss: 0.72195 | eval_custom_logloss: 0.82637 |  0:29:04s
epoch 53 | loss: 0.7224  | eval_custom_logloss: 0.75519 |  0:29:37s
epoch 54 | loss: 0.70067 | eval_custom_logloss: 1.0112  |  0:30:10s
epoch 55 | loss: 0.71797 | eval_custom_logloss: 0.69098 |  0:30:43s
epoch 56 | loss: 0.70919 | eval_custom_logloss: 0.83931 |  0:31:17s
epoch 57 | loss: 0.7097  | eval_custom_logloss: 0.64942 |  0:31:50s
epoch 58 | loss: 0.70835 | eval_custom_logloss: 0.71117 |  0:32:23s
epoch 59 | loss: 0.71485 | eval_custom_logloss: 0.80337 |  0:32:56s
epoch 60 | loss: 0.70382 | eval_custom_logloss: 0.88062 |  0:33:29s
epoch 61 | loss: 0.70136 | eval_custom_logloss: 1.30993 |  0:34:03s
epoch 62 | loss: 0.68885 | eval_custom_logloss: 0.74573 |  0:34:36s
epoch 63 | loss: 0.68449 | eval_custom_logloss: 0.81673 |  0:35:09s
epoch 64 | loss: 0.6913  | eval_custom_logloss: 0.92018 |  0:35:42s
epoch 65 | loss: 0.69619 | eval_custom_logloss: 1.1466  |  0:36:15s
epoch 66 | loss: 0.68734 | eval_custom_logloss: 0.74942 |  0:36:48s
epoch 67 | loss: 0.70182 | eval_custom_logloss: 0.80145 |  0:37:21s
epoch 68 | loss: 0.68783 | eval_custom_logloss: 0.77474 |  0:37:54s
epoch 69 | loss: 0.67804 | eval_custom_logloss: 0.66947 |  0:38:26s
epoch 70 | loss: 0.69168 | eval_custom_logloss: 0.72774 |  0:38:59s
epoch 71 | loss: 0.69592 | eval_custom_logloss: 0.65019 |  0:39:32s
epoch 72 | loss: 0.69074 | eval_custom_logloss: 0.68152 |  0:40:05s
epoch 73 | loss: 0.66453 | eval_custom_logloss: 0.65245 |  0:40:38s
epoch 74 | loss: 0.68083 | eval_custom_logloss: 0.86787 |  0:41:11s
epoch 75 | loss: 0.67866 | eval_custom_logloss: 0.93137 |  0:41:44s
epoch 76 | loss: 0.69218 | eval_custom_logloss: 0.87868 |  0:42:19s
epoch 77 | loss: 0.69943 | eval_custom_logloss: 1.06625 |  0:42:51s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.64942
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6746, 'Log Loss - std': 0.038751602117417884} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.77882 | eval_custom_logloss: 1.18505 |  0:00:32s
epoch 1  | loss: 1.29932 | eval_custom_logloss: 1.11645 |  0:01:05s
epoch 2  | loss: 1.16281 | eval_custom_logloss: 1.05561 |  0:01:38s
epoch 3  | loss: 1.09506 | eval_custom_logloss: 0.97693 |  0:02:11s
epoch 4  | loss: 1.062   | eval_custom_logloss: 0.91874 |  0:02:44s
epoch 5  | loss: 1.04428 | eval_custom_logloss: 0.93899 |  0:03:18s
epoch 6  | loss: 1.06641 | eval_custom_logloss: 0.94957 |  0:03:51s
epoch 7  | loss: 1.02677 | eval_custom_logloss: 0.90106 |  0:04:25s
epoch 8  | loss: 1.00219 | eval_custom_logloss: 0.89255 |  0:04:58s
epoch 9  | loss: 0.98752 | eval_custom_logloss: 0.85271 |  0:05:31s
epoch 10 | loss: 0.96203 | eval_custom_logloss: 0.99402 |  0:06:04s
epoch 11 | loss: 0.94649 | eval_custom_logloss: 0.84882 |  0:06:37s
epoch 12 | loss: 0.94097 | eval_custom_logloss: 0.84936 |  0:07:10s
epoch 13 | loss: 0.93209 | eval_custom_logloss: 0.90241 |  0:07:43s
epoch 14 | loss: 0.90726 | eval_custom_logloss: 0.87262 |  0:08:16s
epoch 15 | loss: 0.90397 | eval_custom_logloss: 0.95326 |  0:08:48s
epoch 16 | loss: 0.94431 | eval_custom_logloss: 0.82439 |  0:09:22s
epoch 17 | loss: 0.91243 | eval_custom_logloss: 0.81252 |  0:09:54s
epoch 18 | loss: 0.90342 | eval_custom_logloss: 0.81755 |  0:10:27s
epoch 19 | loss: 0.87785 | eval_custom_logloss: 0.78884 |  0:11:00s
epoch 20 | loss: 0.87499 | eval_custom_logloss: 0.89359 |  0:11:33s
epoch 21 | loss: 0.88034 | eval_custom_logloss: 0.76712 |  0:12:06s
epoch 22 | loss: 0.88395 | eval_custom_logloss: 0.77503 |  0:12:38s
epoch 23 | loss: 0.86505 | eval_custom_logloss: 0.822   |  0:13:11s
epoch 24 | loss: 0.86358 | eval_custom_logloss: 0.84493 |  0:13:44s
epoch 25 | loss: 0.8616  | eval_custom_logloss: 0.7641  |  0:14:17s
epoch 26 | loss: 0.84669 | eval_custom_logloss: 0.76567 |  0:14:51s
epoch 27 | loss: 0.84974 | eval_custom_logloss: 0.88252 |  0:15:24s
epoch 28 | loss: 0.83365 | eval_custom_logloss: 1.01807 |  0:15:57s
epoch 29 | loss: 0.85234 | eval_custom_logloss: 0.86806 |  0:16:30s
epoch 30 | loss: 0.83653 | eval_custom_logloss: 0.7996  |  0:17:02s
epoch 31 | loss: 0.83948 | eval_custom_logloss: 0.77045 |  0:17:35s
epoch 32 | loss: 0.82924 | eval_custom_logloss: 0.78412 |  0:18:08s
epoch 33 | loss: 0.8443  | eval_custom_logloss: 0.76259 |  0:18:41s
epoch 34 | loss: 0.82786 | eval_custom_logloss: 0.87477 |  0:19:14s
epoch 35 | loss: 0.83494 | eval_custom_logloss: 0.96965 |  0:19:46s
epoch 36 | loss: 0.83423 | eval_custom_logloss: 0.86235 |  0:20:19s
epoch 37 | loss: 0.82189 | eval_custom_logloss: 0.8767  |  0:20:52s
epoch 38 | loss: 0.82057 | eval_custom_logloss: 0.81809 |  0:21:25s
epoch 39 | loss: 0.80709 | eval_custom_logloss: 0.7237  |  0:21:58s
epoch 40 | loss: 0.82428 | eval_custom_logloss: 0.9032  |  0:22:29s
epoch 41 | loss: 0.81023 | eval_custom_logloss: 0.77503 |  0:23:00s
epoch 42 | loss: 0.79437 | eval_custom_logloss: 0.8363  |  0:23:33s
epoch 43 | loss: 0.79748 | eval_custom_logloss: 0.79898 |  0:24:06s
epoch 44 | loss: 0.80851 | eval_custom_logloss: 0.74253 |  0:24:38s
epoch 45 | loss: 0.80299 | eval_custom_logloss: 0.87501 |  0:25:11s
epoch 46 | loss: 0.80398 | eval_custom_logloss: 0.79443 |  0:25:45s
epoch 47 | loss: 0.79943 | eval_custom_logloss: 0.76299 |  0:26:18s
epoch 48 | loss: 0.81152 | eval_custom_logloss: 0.78254 |  0:26:50s
epoch 49 | loss: 0.79212 | eval_custom_logloss: 0.80383 |  0:27:23s
epoch 50 | loss: 0.78355 | eval_custom_logloss: 0.80032 |  0:27:57s
epoch 51 | loss: 0.8022  | eval_custom_logloss: 0.73435 |  0:28:29s
epoch 52 | loss: 0.78761 | eval_custom_logloss: 0.81214 |  0:29:02s
epoch 53 | loss: 0.77348 | eval_custom_logloss: 0.74256 |  0:29:35s
epoch 54 | loss: 0.78707 | eval_custom_logloss: 0.79689 |  0:30:08s
epoch 55 | loss: 0.79484 | eval_custom_logloss: 0.76807 |  0:30:42s
epoch 56 | loss: 0.77891 | eval_custom_logloss: 0.73995 |  0:31:15s
epoch 57 | loss: 0.79057 | eval_custom_logloss: 0.75391 |  0:31:49s
epoch 58 | loss: 0.78816 | eval_custom_logloss: 0.9073  |  0:32:22s
epoch 59 | loss: 0.79978 | eval_custom_logloss: 0.75688 |  0:32:55s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.7237
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.686775, 'Log Loss - std': 0.03963529834629733} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.88516 | eval_custom_logloss: 1.39909 |  0:00:33s
epoch 1  | loss: 1.36546 | eval_custom_logloss: 1.20583 |  0:01:06s
epoch 2  | loss: 1.2175  | eval_custom_logloss: 1.08021 |  0:01:32s
epoch 3  | loss: 1.16973 | eval_custom_logloss: 1.13306 |  0:02:00s
epoch 4  | loss: 1.16035 | eval_custom_logloss: 1.06551 |  0:02:27s
epoch 5  | loss: 1.13477 | eval_custom_logloss: 1.06574 |  0:02:55s
epoch 6  | loss: 1.13561 | eval_custom_logloss: 1.05598 |  0:03:23s
epoch 7  | loss: 1.09978 | eval_custom_logloss: 1.00763 |  0:03:50s
epoch 8  | loss: 1.10389 | eval_custom_logloss: 1.09559 |  0:04:18s
epoch 9  | loss: 1.15886 | eval_custom_logloss: 1.08819 |  0:04:46s
epoch 10 | loss: 1.14406 | eval_custom_logloss: 1.07649 |  0:05:14s
epoch 11 | loss: 1.07006 | eval_custom_logloss: 0.9482  |  0:05:41s
epoch 12 | loss: 1.036   | eval_custom_logloss: 0.91233 |  0:06:09s
epoch 13 | loss: 0.99918 | eval_custom_logloss: 0.96068 |  0:06:37s
epoch 14 | loss: 0.97858 | eval_custom_logloss: 0.8806  |  0:07:05s
epoch 15 | loss: 0.96085 | eval_custom_logloss: 0.92029 |  0:07:32s
epoch 16 | loss: 0.94747 | eval_custom_logloss: 0.84613 |  0:08:00s
epoch 17 | loss: 0.94855 | eval_custom_logloss: 0.83715 |  0:08:28s
epoch 18 | loss: 0.94124 | eval_custom_logloss: 0.81417 |  0:08:56s
epoch 19 | loss: 0.91852 | eval_custom_logloss: 0.85957 |  0:09:24s
epoch 20 | loss: 0.89375 | eval_custom_logloss: 0.83114 |  0:09:51s
epoch 21 | loss: 0.89235 | eval_custom_logloss: 0.81049 |  0:10:19s
epoch 22 | loss: 0.89762 | eval_custom_logloss: 0.81166 |  0:10:47s
epoch 23 | loss: 0.86768 | eval_custom_logloss: 0.90508 |  0:11:15s
epoch 24 | loss: 0.87284 | eval_custom_logloss: 0.84258 |  0:11:42s
epoch 25 | loss: 0.85365 | eval_custom_logloss: 0.81226 |  0:12:10s
epoch 26 | loss: 0.85559 | eval_custom_logloss: 0.80142 |  0:12:38s
epoch 27 | loss: 0.84462 | eval_custom_logloss: 0.8937  |  0:13:05s
epoch 28 | loss: 0.83903 | eval_custom_logloss: 0.87326 |  0:13:33s
epoch 29 | loss: 0.84425 | eval_custom_logloss: 0.77486 |  0:14:01s
epoch 30 | loss: 0.81719 | eval_custom_logloss: 0.76572 |  0:14:29s
epoch 31 | loss: 0.81051 | eval_custom_logloss: 0.77567 |  0:14:57s
epoch 32 | loss: 0.80041 | eval_custom_logloss: 1.02766 |  0:15:24s
epoch 33 | loss: 0.80188 | eval_custom_logloss: 0.74357 |  0:15:52s
epoch 34 | loss: 0.80464 | eval_custom_logloss: 0.78294 |  0:16:20s
epoch 35 | loss: 0.79699 | eval_custom_logloss: 0.95307 |  0:16:47s
epoch 36 | loss: 0.81117 | eval_custom_logloss: 0.76891 |  0:17:16s
epoch 37 | loss: 0.80172 | eval_custom_logloss: 0.81112 |  0:17:43s
epoch 38 | loss: 0.77797 | eval_custom_logloss: 0.77593 |  0:18:11s
epoch 39 | loss: 0.78464 | eval_custom_logloss: 0.90873 |  0:18:38s
epoch 40 | loss: 0.81648 | eval_custom_logloss: 0.84459 |  0:19:06s
epoch 41 | loss: 0.80143 | eval_custom_logloss: 0.78993 |  0:19:34s
epoch 42 | loss: 0.76461 | eval_custom_logloss: 0.81784 |  0:20:02s
epoch 43 | loss: 0.76688 | eval_custom_logloss: 0.768   |  0:20:29s
epoch 44 | loss: 0.77506 | eval_custom_logloss: 0.95801 |  0:20:57s
epoch 45 | loss: 0.76283 | eval_custom_logloss: 0.87888 |  0:21:25s
epoch 46 | loss: 0.75647 | eval_custom_logloss: 0.72827 |  0:21:52s
epoch 47 | loss: 0.75521 | eval_custom_logloss: 0.76172 |  0:22:20s
epoch 48 | loss: 0.77268 | eval_custom_logloss: 0.88209 |  0:22:48s
epoch 49 | loss: 0.77636 | eval_custom_logloss: 0.87298 |  0:23:16s
epoch 50 | loss: 0.77231 | eval_custom_logloss: 0.8331  |  0:23:44s
epoch 51 | loss: 0.75212 | eval_custom_logloss: 0.83333 |  0:24:11s
epoch 52 | loss: 0.77193 | eval_custom_logloss: 0.81322 |  0:24:39s
epoch 53 | loss: 0.76891 | eval_custom_logloss: 0.77415 |  0:25:07s
epoch 54 | loss: 0.77476 | eval_custom_logloss: 0.79685 |  0:25:34s
epoch 55 | loss: 0.78366 | eval_custom_logloss: 0.74128 |  0:26:02s
epoch 56 | loss: 0.76713 | eval_custom_logloss: 0.869   |  0:26:29s
epoch 57 | loss: 0.75889 | eval_custom_logloss: 0.86471 |  0:26:57s
epoch 58 | loss: 0.75714 | eval_custom_logloss: 0.80554 |  0:27:24s
epoch 59 | loss: 0.76725 | eval_custom_logloss: 0.6882  |  0:27:52s
epoch 60 | loss: 0.74307 | eval_custom_logloss: 0.90476 |  0:28:20s
epoch 61 | loss: 0.73555 | eval_custom_logloss: 0.725   |  0:28:48s
epoch 62 | loss: 0.74319 | eval_custom_logloss: 0.88503 |  0:29:16s
epoch 63 | loss: 0.7476  | eval_custom_logloss: 0.7502  |  0:29:43s
epoch 64 | loss: 0.74859 | eval_custom_logloss: 0.78051 |  0:30:11s
epoch 65 | loss: 0.74074 | eval_custom_logloss: 0.80984 |  0:30:39s
epoch 66 | loss: 0.73005 | eval_custom_logloss: 0.81886 |  0:31:07s
epoch 67 | loss: 0.72549 | eval_custom_logloss: 0.70809 |  0:31:34s
epoch 68 | loss: 0.73104 | eval_custom_logloss: 0.67318 |  0:32:02s
epoch 69 | loss: 0.71129 | eval_custom_logloss: 0.76796 |  0:32:30s
epoch 70 | loss: 0.72722 | eval_custom_logloss: 1.07613 |  0:32:58s
epoch 71 | loss: 0.74353 | eval_custom_logloss: 0.68747 |  0:33:25s
epoch 72 | loss: 0.72576 | eval_custom_logloss: 0.72732 |  0:33:53s
epoch 73 | loss: 0.71201 | eval_custom_logloss: 0.86975 |  0:34:21s
epoch 74 | loss: 0.71772 | eval_custom_logloss: 0.84628 |  0:34:48s
epoch 75 | loss: 0.70851 | eval_custom_logloss: 0.87239 |  0:35:16s
epoch 76 | loss: 0.74669 | eval_custom_logloss: 0.68517 |  0:35:44s
epoch 77 | loss: 0.73649 | eval_custom_logloss: 0.8677  |  0:36:12s
epoch 78 | loss: 0.72117 | eval_custom_logloss: 0.85978 |  0:36:40s
epoch 79 | loss: 0.7312  | eval_custom_logloss: 0.80671 |  0:37:07s
epoch 80 | loss: 0.71027 | eval_custom_logloss: 0.63637 |  0:37:35s
epoch 81 | loss: 0.72465 | eval_custom_logloss: 0.98434 |  0:38:03s
epoch 82 | loss: 0.7171  | eval_custom_logloss: 0.64525 |  0:38:31s
epoch 83 | loss: 0.71907 | eval_custom_logloss: 0.69854 |  0:38:58s
epoch 84 | loss: 0.70824 | eval_custom_logloss: 0.82193 |  0:39:26s
epoch 85 | loss: 0.71421 | eval_custom_logloss: 0.78369 |  0:39:53s
epoch 86 | loss: 0.70946 | eval_custom_logloss: 0.92833 |  0:40:21s
epoch 87 | loss: 0.71092 | eval_custom_logloss: 0.73787 |  0:40:49s
epoch 88 | loss: 0.6964  | eval_custom_logloss: 0.84838 |  0:41:17s
epoch 89 | loss: 0.7012  | eval_custom_logloss: 0.79387 |  0:41:45s
epoch 90 | loss: 0.7035  | eval_custom_logloss: 0.76123 |  0:42:12s
epoch 91 | loss: 0.70009 | eval_custom_logloss: 0.64363 |  0:42:40s
epoch 92 | loss: 0.70085 | eval_custom_logloss: 0.79967 |  0:43:08s
epoch 93 | loss: 0.69887 | eval_custom_logloss: 0.83788 |  0:43:35s
epoch 94 | loss: 0.70584 | eval_custom_logloss: 0.71653 |  0:44:03s
epoch 95 | loss: 0.69377 | eval_custom_logloss: 0.70336 |  0:44:31s
epoch 96 | loss: 0.70681 | eval_custom_logloss: 0.6731  |  0:44:58s
epoch 97 | loss: 0.71035 | eval_custom_logloss: 0.78136 |  0:45:26s
epoch 98 | loss: 0.69629 | eval_custom_logloss: 0.7189  |  0:45:54s
epoch 99 | loss: 0.68834 | eval_custom_logloss: 0.67913 |  0:46:21s
Stop training because you reached max_epochs = 100 with best_epoch = 80 and best_eval_custom_logloss = 0.63637
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.67638, 'Log Loss - std': 0.04109731864732783} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 0.67638 and parameters: {'n_d': 24, 'n_steps': 10, 'gamma': 1.70476578639084, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.3613034920083372, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.83659 | eval_custom_logloss: 1.46743 |  0:00:30s
epoch 1  | loss: 1.44586 | eval_custom_logloss: 1.40083 |  0:01:01s
epoch 2  | loss: 1.38135 | eval_custom_logloss: 1.16072 |  0:01:32s
epoch 3  | loss: 1.2107  | eval_custom_logloss: 1.08781 |  0:02:02s
epoch 4  | loss: 1.16855 | eval_custom_logloss: 1.08342 |  0:02:33s
epoch 5  | loss: 1.18849 | eval_custom_logloss: 1.07497 |  0:03:04s
epoch 6  | loss: 1.17824 | eval_custom_logloss: 1.06001 |  0:03:34s
epoch 7  | loss: 1.21299 | eval_custom_logloss: 1.11377 |  0:04:05s
epoch 8  | loss: 1.22339 | eval_custom_logloss: 1.08692 |  0:04:35s
epoch 9  | loss: 1.16618 | eval_custom_logloss: 1.07722 |  0:05:06s
epoch 10 | loss: 1.08243 | eval_custom_logloss: 0.99931 |  0:05:36s
epoch 11 | loss: 1.02984 | eval_custom_logloss: 0.91959 |  0:06:07s
epoch 12 | loss: 1.02407 | eval_custom_logloss: 0.96329 |  0:06:37s
epoch 13 | loss: 1.00235 | eval_custom_logloss: 0.8895  |  0:07:08s
epoch 14 | loss: 1.02911 | eval_custom_logloss: 0.92889 |  0:07:39s
epoch 15 | loss: 0.98891 | eval_custom_logloss: 0.90391 |  0:08:09s
epoch 16 | loss: 0.96931 | eval_custom_logloss: 0.91103 |  0:08:39s
epoch 17 | loss: 0.97882 | eval_custom_logloss: 0.94617 |  0:09:09s
epoch 18 | loss: 0.96471 | eval_custom_logloss: 0.89289 |  0:09:40s
epoch 19 | loss: 0.94715 | eval_custom_logloss: 0.87527 |  0:10:10s
epoch 20 | loss: 0.93456 | eval_custom_logloss: 0.89728 |  0:10:41s
epoch 21 | loss: 0.936   | eval_custom_logloss: 0.87191 |  0:11:12s
epoch 22 | loss: 0.909   | eval_custom_logloss: 0.9216  |  0:11:42s
epoch 23 | loss: 0.91438 | eval_custom_logloss: 0.89495 |  0:12:13s
epoch 24 | loss: 0.92195 | eval_custom_logloss: 0.83682 |  0:12:43s
epoch 25 | loss: 0.91818 | eval_custom_logloss: 0.85287 |  0:13:13s
epoch 26 | loss: 0.94752 | eval_custom_logloss: 0.90933 |  0:13:44s
epoch 27 | loss: 0.9299  | eval_custom_logloss: 0.83323 |  0:14:15s
epoch 28 | loss: 0.90592 | eval_custom_logloss: 0.83964 |  0:14:45s
epoch 29 | loss: 0.9119  | eval_custom_logloss: 0.89304 |  0:15:16s
epoch 30 | loss: 0.99433 | eval_custom_logloss: 0.91685 |  0:15:47s
epoch 31 | loss: 0.96956 | eval_custom_logloss: 0.91995 |  0:16:18s
epoch 32 | loss: 0.99419 | eval_custom_logloss: 0.97112 |  0:16:49s
epoch 33 | loss: 1.04014 | eval_custom_logloss: 0.96229 |  0:17:19s
epoch 34 | loss: 1.00008 | eval_custom_logloss: 0.92887 |  0:17:50s
epoch 35 | loss: 0.94657 | eval_custom_logloss: 0.90412 |  0:18:20s
epoch 36 | loss: 0.93743 | eval_custom_logloss: 0.85101 |  0:18:51s
epoch 37 | loss: 0.92106 | eval_custom_logloss: 0.8523  |  0:19:22s
epoch 38 | loss: 0.91215 | eval_custom_logloss: 0.89578 |  0:19:52s
epoch 39 | loss: 0.92082 | eval_custom_logloss: 0.83433 |  0:20:23s
epoch 40 | loss: 0.90638 | eval_custom_logloss: 0.87469 |  0:20:54s
epoch 41 | loss: 0.90471 | eval_custom_logloss: 0.87649 |  0:21:25s
epoch 42 | loss: 0.92359 | eval_custom_logloss: 0.89479 |  0:21:55s
epoch 43 | loss: 0.90078 | eval_custom_logloss: 0.82973 |  0:22:26s
epoch 44 | loss: 0.9052  | eval_custom_logloss: 0.85623 |  0:22:56s
epoch 45 | loss: 0.87321 | eval_custom_logloss: 0.86813 |  0:23:27s
epoch 46 | loss: 0.86793 | eval_custom_logloss: 0.84028 |  0:23:57s
epoch 47 | loss: 0.85765 | eval_custom_logloss: 0.79087 |  0:24:28s
epoch 48 | loss: 0.86396 | eval_custom_logloss: 0.84055 |  0:24:58s
epoch 49 | loss: 0.84216 | eval_custom_logloss: 0.77991 |  0:25:29s
epoch 50 | loss: 0.85395 | eval_custom_logloss: 0.77285 |  0:25:59s
epoch 51 | loss: 0.85432 | eval_custom_logloss: 0.7869  |  0:26:30s
epoch 52 | loss: 0.85186 | eval_custom_logloss: 0.77535 |  0:27:01s
epoch 53 | loss: 0.83484 | eval_custom_logloss: 0.82074 |  0:27:31s
epoch 54 | loss: 0.82113 | eval_custom_logloss: 0.76836 |  0:28:02s
epoch 55 | loss: 0.83509 | eval_custom_logloss: 0.78097 |  0:28:32s
epoch 56 | loss: 0.84359 | eval_custom_logloss: 0.79768 |  0:29:03s
epoch 57 | loss: 0.86769 | eval_custom_logloss: 0.86645 |  0:29:32s
epoch 58 | loss: 0.83453 | eval_custom_logloss: 0.83306 |  0:30:02s
epoch 59 | loss: 0.85303 | eval_custom_logloss: 0.85406 |  0:30:33s
epoch 60 | loss: 0.82622 | eval_custom_logloss: 0.77973 |  0:31:03s
epoch 61 | loss: 0.82655 | eval_custom_logloss: 0.8024  |  0:31:35s
epoch 62 | loss: 0.82349 | eval_custom_logloss: 0.75886 |  0:32:09s
epoch 63 | loss: 0.81881 | eval_custom_logloss: 0.81995 |  0:32:43s
epoch 64 | loss: 0.81125 | eval_custom_logloss: 0.92076 |  0:33:16s
epoch 65 | loss: 0.82046 | eval_custom_logloss: 0.78886 |  0:33:50s
epoch 66 | loss: 0.80962 | eval_custom_logloss: 0.84224 |  0:34:23s
epoch 67 | loss: 0.81976 | eval_custom_logloss: 0.78368 |  0:34:57s
epoch 68 | loss: 0.81594 | eval_custom_logloss: 0.75577 |  0:35:30s
epoch 69 | loss: 0.81503 | eval_custom_logloss: 0.78744 |  0:36:04s
epoch 70 | loss: 0.80709 | eval_custom_logloss: 0.80467 |  0:36:38s
epoch 71 | loss: 0.79989 | eval_custom_logloss: 0.84174 |  0:37:11s
epoch 72 | loss: 0.79263 | eval_custom_logloss: 0.77661 |  0:37:44s
epoch 73 | loss: 0.7867  | eval_custom_logloss: 0.77086 |  0:38:18s
epoch 74 | loss: 0.81186 | eval_custom_logloss: 0.78943 |  0:38:51s
epoch 75 | loss: 0.80837 | eval_custom_logloss: 0.7598  |  0:39:24s
epoch 76 | loss: 0.81895 | eval_custom_logloss: 0.77908 |  0:39:58s
epoch 77 | loss: 0.80703 | eval_custom_logloss: 0.76804 |  0:40:31s
epoch 78 | loss: 0.80221 | eval_custom_logloss: 0.75572 |  0:41:04s
epoch 79 | loss: 0.81064 | eval_custom_logloss: 0.737   |  0:41:38s
epoch 80 | loss: 0.79288 | eval_custom_logloss: 0.80072 |  0:42:10s
epoch 81 | loss: 0.79809 | eval_custom_logloss: 0.75356 |  0:42:44s
epoch 82 | loss: 0.80143 | eval_custom_logloss: 0.8073  |  0:43:17s
epoch 83 | loss: 0.80222 | eval_custom_logloss: 0.76818 |  0:43:50s
epoch 84 | loss: 0.80136 | eval_custom_logloss: 0.80145 |  0:44:23s
epoch 85 | loss: 0.80651 | eval_custom_logloss: 0.78976 |  0:44:56s
epoch 86 | loss: 0.79982 | eval_custom_logloss: 0.76159 |  0:45:29s
epoch 87 | loss: 0.84755 | eval_custom_logloss: 0.81788 |  0:46:02s
epoch 88 | loss: 0.85494 | eval_custom_logloss: 0.7566  |  0:46:35s
epoch 89 | loss: 0.83173 | eval_custom_logloss: 0.79439 |  0:47:07s
epoch 90 | loss: 0.82253 | eval_custom_logloss: 0.78605 |  0:47:41s
epoch 91 | loss: 0.8264  | eval_custom_logloss: 0.85099 |  0:48:14s
epoch 92 | loss: 0.8353  | eval_custom_logloss: 0.7607  |  0:48:47s
epoch 93 | loss: 0.82974 | eval_custom_logloss: 0.75957 |  0:49:20s
epoch 94 | loss: 0.81369 | eval_custom_logloss: 0.85354 |  0:49:53s
epoch 95 | loss: 0.81096 | eval_custom_logloss: 0.78834 |  0:50:26s
epoch 96 | loss: 0.82013 | eval_custom_logloss: 0.96337 |  0:50:59s
epoch 97 | loss: 0.81315 | eval_custom_logloss: 0.86687 |  0:51:32s
epoch 98 | loss: 0.81453 | eval_custom_logloss: 0.92996 |  0:52:05s
epoch 99 | loss: 0.80961 | eval_custom_logloss: 0.75963 |  0:52:38s

Early stopping occurred at epoch 99 with best_epoch = 79 and best_eval_custom_logloss = 0.737
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7349, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.79128 | eval_custom_logloss: 1.32661 |  0:00:33s
epoch 1  | loss: 1.37611 | eval_custom_logloss: 1.24961 |  0:01:06s
epoch 2  | loss: 1.25948 | eval_custom_logloss: 1.26134 |  0:01:38s
epoch 3  | loss: 1.25708 | eval_custom_logloss: 1.12351 |  0:02:11s
epoch 4  | loss: 1.19846 | eval_custom_logloss: 1.1608  |  0:02:44s
epoch 5  | loss: 1.13532 | eval_custom_logloss: 1.04623 |  0:03:18s
epoch 6  | loss: 1.09302 | eval_custom_logloss: 0.95392 |  0:03:50s
epoch 7  | loss: 1.05416 | eval_custom_logloss: 0.95458 |  0:04:24s
epoch 8  | loss: 1.0567  | eval_custom_logloss: 0.90646 |  0:04:57s
epoch 9  | loss: 1.0253  | eval_custom_logloss: 0.93805 |  0:05:30s
epoch 10 | loss: 1.01341 | eval_custom_logloss: 0.98371 |  0:06:03s
epoch 11 | loss: 1.01314 | eval_custom_logloss: 0.92276 |  0:06:36s
epoch 12 | loss: 1.00203 | eval_custom_logloss: 1.39477 |  0:07:09s
epoch 13 | loss: 0.98372 | eval_custom_logloss: 0.86643 |  0:07:42s
epoch 14 | loss: 0.98559 | eval_custom_logloss: 0.977   |  0:08:15s
epoch 15 | loss: 0.97666 | eval_custom_logloss: 0.93137 |  0:08:48s
epoch 16 | loss: 0.9658  | eval_custom_logloss: 1.00178 |  0:09:22s
epoch 17 | loss: 0.96306 | eval_custom_logloss: 0.98967 |  0:09:55s
epoch 18 | loss: 0.94868 | eval_custom_logloss: 0.88052 |  0:10:28s
epoch 19 | loss: 0.94058 | eval_custom_logloss: 0.88286 |  0:11:02s
epoch 20 | loss: 0.94451 | eval_custom_logloss: 0.8589  |  0:11:35s
epoch 21 | loss: 0.96518 | eval_custom_logloss: 1.00994 |  0:12:08s
epoch 22 | loss: 1.03164 | eval_custom_logloss: 1.16817 |  0:12:41s
epoch 23 | loss: 0.99909 | eval_custom_logloss: 1.07764 |  0:13:15s
epoch 24 | loss: 0.94382 | eval_custom_logloss: 0.86379 |  0:13:48s
epoch 25 | loss: 0.94247 | eval_custom_logloss: 0.94998 |  0:14:22s
epoch 26 | loss: 0.95533 | eval_custom_logloss: 2.63522 |  0:14:55s
epoch 27 | loss: 0.97353 | eval_custom_logloss: 1.08713 |  0:15:28s
epoch 28 | loss: 0.99431 | eval_custom_logloss: 1.00183 |  0:16:02s
epoch 29 | loss: 0.99675 | eval_custom_logloss: 0.90056 |  0:16:35s
epoch 30 | loss: 1.14932 | eval_custom_logloss: 1.30215 |  0:17:08s
epoch 31 | loss: 1.06433 | eval_custom_logloss: 1.01563 |  0:17:42s
epoch 32 | loss: 1.04384 | eval_custom_logloss: 0.98236 |  0:18:15s
epoch 33 | loss: 1.03808 | eval_custom_logloss: 1.08616 |  0:18:48s
epoch 34 | loss: 1.02369 | eval_custom_logloss: 1.00292 |  0:19:22s
epoch 35 | loss: 1.0172  | eval_custom_logloss: 1.09845 |  0:19:55s
epoch 36 | loss: 1.00612 | eval_custom_logloss: 1.01574 |  0:20:28s
epoch 37 | loss: 1.00079 | eval_custom_logloss: 0.94986 |  0:21:01s
epoch 38 | loss: 1.0208  | eval_custom_logloss: 1.09278 |  0:21:34s
epoch 39 | loss: 1.03458 | eval_custom_logloss: 1.04154 |  0:22:07s
epoch 40 | loss: 1.02976 | eval_custom_logloss: 1.00658 |  0:22:41s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 0.8589
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.79645, 'Log Loss - std': 0.061549999999999994} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.75452 | eval_custom_logloss: 1.33403 |  0:00:33s
epoch 1  | loss: 1.32871 | eval_custom_logloss: 1.26389 |  0:01:06s
epoch 2  | loss: 1.23636 | eval_custom_logloss: 1.1519  |  0:01:40s
epoch 3  | loss: 1.11903 | eval_custom_logloss: 0.98121 |  0:02:13s
epoch 4  | loss: 1.07023 | eval_custom_logloss: 0.96143 |  0:02:46s
epoch 5  | loss: 1.00233 | eval_custom_logloss: 0.88402 |  0:03:19s
epoch 6  | loss: 0.9918  | eval_custom_logloss: 0.87863 |  0:03:52s
epoch 7  | loss: 0.97303 | eval_custom_logloss: 0.87416 |  0:04:25s
epoch 8  | loss: 0.95662 | eval_custom_logloss: 0.84198 |  0:04:58s
epoch 9  | loss: 0.9681  | eval_custom_logloss: 0.83884 |  0:05:32s
epoch 10 | loss: 0.93091 | eval_custom_logloss: 0.88336 |  0:06:05s
epoch 11 | loss: 0.95045 | eval_custom_logloss: 0.82872 |  0:06:38s
epoch 12 | loss: 0.94751 | eval_custom_logloss: 0.78855 |  0:07:10s
epoch 13 | loss: 0.91187 | eval_custom_logloss: 0.83692 |  0:07:43s
epoch 14 | loss: 0.91829 | eval_custom_logloss: 0.80424 |  0:08:16s
epoch 15 | loss: 0.88507 | eval_custom_logloss: 0.82045 |  0:08:49s
epoch 16 | loss: 0.89142 | eval_custom_logloss: 0.83866 |  0:09:23s
epoch 17 | loss: 0.87834 | eval_custom_logloss: 0.83565 |  0:09:56s
epoch 18 | loss: 0.87198 | eval_custom_logloss: 0.80736 |  0:10:30s
epoch 19 | loss: 0.85544 | eval_custom_logloss: 0.74338 |  0:11:03s
epoch 20 | loss: 0.85187 | eval_custom_logloss: 0.76777 |  0:11:37s
epoch 21 | loss: 0.84453 | eval_custom_logloss: 0.84317 |  0:12:10s
epoch 22 | loss: 0.8508  | eval_custom_logloss: 0.85503 |  0:12:44s
epoch 23 | loss: 0.83939 | eval_custom_logloss: 0.79076 |  0:13:17s
epoch 24 | loss: 0.8343  | eval_custom_logloss: 0.91892 |  0:13:50s
epoch 25 | loss: 0.8445  | eval_custom_logloss: 0.81651 |  0:14:23s
epoch 26 | loss: 0.82613 | eval_custom_logloss: 0.78758 |  0:14:56s
epoch 27 | loss: 0.80569 | eval_custom_logloss: 0.84272 |  0:15:30s
epoch 28 | loss: 0.82154 | eval_custom_logloss: 0.71669 |  0:16:03s
epoch 29 | loss: 0.83477 | eval_custom_logloss: 0.82245 |  0:16:36s
epoch 30 | loss: 0.83789 | eval_custom_logloss: 0.77625 |  0:17:09s
epoch 31 | loss: 0.81715 | eval_custom_logloss: 0.84873 |  0:17:42s
epoch 32 | loss: 0.82917 | eval_custom_logloss: 0.81316 |  0:18:15s
epoch 33 | loss: 0.92833 | eval_custom_logloss: 0.99623 |  0:18:48s
epoch 34 | loss: 0.90778 | eval_custom_logloss: 0.87118 |  0:19:21s
epoch 35 | loss: 0.87167 | eval_custom_logloss: 0.96317 |  0:19:54s
epoch 36 | loss: 0.86359 | eval_custom_logloss: 0.94855 |  0:20:27s
epoch 37 | loss: 0.82932 | eval_custom_logloss: 1.05997 |  0:21:00s
epoch 38 | loss: 0.82239 | eval_custom_logloss: 0.78835 |  0:21:33s
epoch 39 | loss: 0.81209 | eval_custom_logloss: 0.97417 |  0:22:06s
epoch 40 | loss: 0.80544 | eval_custom_logloss: 0.80309 |  0:22:40s
epoch 41 | loss: 0.78948 | eval_custom_logloss: 0.77333 |  0:23:13s
epoch 42 | loss: 0.80197 | eval_custom_logloss: 0.76723 |  0:23:46s
epoch 43 | loss: 0.79993 | eval_custom_logloss: 0.85957 |  0:24:19s
epoch 44 | loss: 0.7907  | eval_custom_logloss: 0.78983 |  0:24:53s
epoch 45 | loss: 0.78501 | eval_custom_logloss: 0.81107 |  0:25:25s
epoch 46 | loss: 0.79275 | eval_custom_logloss: 0.73215 |  0:25:59s
epoch 47 | loss: 0.77365 | eval_custom_logloss: 0.69879 |  0:26:32s
epoch 48 | loss: 0.77237 | eval_custom_logloss: 0.95556 |  0:27:05s
epoch 49 | loss: 0.76547 | eval_custom_logloss: 0.8564  |  0:27:39s
epoch 50 | loss: 0.76311 | eval_custom_logloss: 0.8144  |  0:28:12s
epoch 51 | loss: 0.76509 | eval_custom_logloss: 0.93745 |  0:28:45s
epoch 52 | loss: 0.76529 | eval_custom_logloss: 1.04084 |  0:29:19s
epoch 53 | loss: 0.76588 | eval_custom_logloss: 0.76251 |  0:29:52s
epoch 54 | loss: 0.76924 | eval_custom_logloss: 0.91122 |  0:30:26s
epoch 55 | loss: 0.76084 | eval_custom_logloss: 0.80125 |  0:30:59s
epoch 56 | loss: 0.75091 | eval_custom_logloss: 0.78885 |  0:31:32s
epoch 57 | loss: 0.77253 | eval_custom_logloss: 0.78812 |  0:32:05s
epoch 58 | loss: 0.75504 | eval_custom_logloss: 0.86153 |  0:32:39s
epoch 59 | loss: 0.7572  | eval_custom_logloss: 0.78917 |  0:33:12s
epoch 60 | loss: 0.74084 | eval_custom_logloss: 0.74123 |  0:33:45s
epoch 61 | loss: 0.74216 | eval_custom_logloss: 0.85458 |  0:34:18s
epoch 62 | loss: 0.7431  | eval_custom_logloss: 0.79666 |  0:34:52s
epoch 63 | loss: 0.7539  | eval_custom_logloss: 0.73802 |  0:35:24s
epoch 64 | loss: 0.73457 | eval_custom_logloss: 1.07762 |  0:35:58s
epoch 65 | loss: 0.74063 | eval_custom_logloss: 0.77949 |  0:36:31s
epoch 66 | loss: 0.74464 | eval_custom_logloss: 0.82283 |  0:37:04s
epoch 67 | loss: 0.74091 | eval_custom_logloss: 0.79146 |  0:37:38s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.69879
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7633666666666666, 'Log Loss - std': 0.06866305815761159} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.77792 | eval_custom_logloss: 1.37603 |  0:00:33s
epoch 1  | loss: 1.35752 | eval_custom_logloss: 1.1717  |  0:01:06s
epoch 2  | loss: 1.24035 | eval_custom_logloss: 1.16467 |  0:01:40s
epoch 3  | loss: 1.19034 | eval_custom_logloss: 1.12286 |  0:02:13s
epoch 4  | loss: 1.15042 | eval_custom_logloss: 1.11673 |  0:02:46s
epoch 5  | loss: 1.12779 | eval_custom_logloss: 1.01173 |  0:03:20s
epoch 6  | loss: 1.06151 | eval_custom_logloss: 0.94512 |  0:03:53s
epoch 7  | loss: 1.03296 | eval_custom_logloss: 0.89336 |  0:04:27s
epoch 8  | loss: 0.99312 | eval_custom_logloss: 0.85348 |  0:05:00s
epoch 9  | loss: 0.98333 | eval_custom_logloss: 0.99308 |  0:05:34s
epoch 10 | loss: 1.00035 | eval_custom_logloss: 0.95428 |  0:06:07s
epoch 11 | loss: 0.94436 | eval_custom_logloss: 0.81756 |  0:06:41s
epoch 12 | loss: 0.9269  | eval_custom_logloss: 0.86715 |  0:07:15s
epoch 13 | loss: 0.93436 | eval_custom_logloss: 0.8493  |  0:07:48s
epoch 14 | loss: 0.95206 | eval_custom_logloss: 0.8487  |  0:08:22s
epoch 15 | loss: 0.91918 | eval_custom_logloss: 0.859   |  0:08:55s
epoch 16 | loss: 0.8973  | eval_custom_logloss: 0.82906 |  0:09:28s
epoch 17 | loss: 0.94912 | eval_custom_logloss: 0.88068 |  0:10:02s
epoch 18 | loss: 0.91741 | eval_custom_logloss: 0.7939  |  0:10:35s
epoch 19 | loss: 0.89108 | eval_custom_logloss: 0.78688 |  0:11:08s
epoch 20 | loss: 0.89756 | eval_custom_logloss: 0.79909 |  0:11:42s
epoch 21 | loss: 0.88423 | eval_custom_logloss: 0.81069 |  0:12:15s
epoch 22 | loss: 0.88416 | eval_custom_logloss: 0.94794 |  0:12:48s
epoch 23 | loss: 0.88448 | eval_custom_logloss: 0.80548 |  0:13:22s
epoch 24 | loss: 0.89864 | eval_custom_logloss: 0.80727 |  0:13:55s
epoch 25 | loss: 0.89278 | eval_custom_logloss: 0.86894 |  0:14:30s
epoch 26 | loss: 0.89    | eval_custom_logloss: 0.79527 |  0:15:04s
epoch 27 | loss: 0.89618 | eval_custom_logloss: 0.80366 |  0:15:37s
epoch 28 | loss: 0.89201 | eval_custom_logloss: 0.81181 |  0:16:11s
epoch 29 | loss: 0.88804 | eval_custom_logloss: 0.84056 |  0:16:45s
epoch 30 | loss: 0.89426 | eval_custom_logloss: 0.88117 |  0:17:18s
epoch 31 | loss: 0.88015 | eval_custom_logloss: 0.80721 |  0:17:52s
epoch 32 | loss: 0.87905 | eval_custom_logloss: 0.8212  |  0:18:25s
epoch 33 | loss: 0.86248 | eval_custom_logloss: 0.81984 |  0:18:59s
epoch 34 | loss: 0.85536 | eval_custom_logloss: 0.80521 |  0:19:32s
epoch 35 | loss: 0.8543  | eval_custom_logloss: 0.87421 |  0:20:06s
epoch 36 | loss: 0.86137 | eval_custom_logloss: 0.87917 |  0:20:39s
epoch 37 | loss: 0.87499 | eval_custom_logloss: 0.83812 |  0:21:13s
epoch 38 | loss: 0.84207 | eval_custom_logloss: 0.8252  |  0:21:46s
epoch 39 | loss: 0.83616 | eval_custom_logloss: 0.78181 |  0:22:20s
epoch 40 | loss: 0.8539  | eval_custom_logloss: 0.82293 |  0:22:54s
epoch 41 | loss: 0.83526 | eval_custom_logloss: 1.03566 |  0:23:28s
epoch 42 | loss: 0.83901 | eval_custom_logloss: 0.7975  |  0:24:01s
epoch 43 | loss: 0.83763 | eval_custom_logloss: 0.80018 |  0:24:35s
epoch 44 | loss: 0.84752 | eval_custom_logloss: 1.17782 |  0:25:09s
epoch 45 | loss: 0.80595 | eval_custom_logloss: 0.73859 |  0:25:43s
epoch 46 | loss: 0.79008 | eval_custom_logloss: 0.80833 |  0:26:17s
epoch 47 | loss: 0.78165 | eval_custom_logloss: 0.75384 |  0:26:50s
epoch 48 | loss: 0.7876  | eval_custom_logloss: 1.04129 |  0:27:23s
epoch 49 | loss: 0.77138 | eval_custom_logloss: 0.80258 |  0:27:56s
epoch 50 | loss: 0.76366 | eval_custom_logloss: 0.72421 |  0:28:30s
epoch 51 | loss: 0.76633 | eval_custom_logloss: 0.75692 |  0:29:04s
epoch 52 | loss: 0.76533 | eval_custom_logloss: 0.8087  |  0:29:37s
epoch 53 | loss: 0.75871 | eval_custom_logloss: 0.70058 |  0:30:11s
epoch 54 | loss: 0.76044 | eval_custom_logloss: 0.91527 |  0:30:44s
epoch 55 | loss: 0.75041 | eval_custom_logloss: 0.71298 |  0:31:18s
epoch 56 | loss: 0.75368 | eval_custom_logloss: 0.7232  |  0:31:51s
epoch 57 | loss: 0.75667 | eval_custom_logloss: 0.74149 |  0:32:25s
epoch 58 | loss: 0.74218 | eval_custom_logloss: 0.66132 |  0:32:59s
epoch 59 | loss: 0.7519  | eval_custom_logloss: 0.72847 |  0:33:33s
epoch 60 | loss: 0.75105 | eval_custom_logloss: 0.68844 |  0:34:06s
epoch 61 | loss: 0.7517  | eval_custom_logloss: 0.70597 |  0:34:39s
epoch 62 | loss: 0.74509 | eval_custom_logloss: 0.76202 |  0:35:13s
epoch 63 | loss: 0.73514 | eval_custom_logloss: 0.70164 |  0:35:46s
epoch 64 | loss: 0.72892 | eval_custom_logloss: 0.85841 |  0:36:20s
epoch 65 | loss: 0.74923 | eval_custom_logloss: 0.78361 |  0:36:53s
epoch 66 | loss: 0.74905 | eval_custom_logloss: 0.72086 |  0:37:27s
epoch 67 | loss: 0.73981 | eval_custom_logloss: 0.94152 |  0:38:00s
epoch 68 | loss: 0.743   | eval_custom_logloss: 0.68259 |  0:38:34s
epoch 69 | loss: 0.7628  | eval_custom_logloss: 0.80714 |  0:39:07s
epoch 70 | loss: 0.74298 | eval_custom_logloss: 0.71233 |  0:39:41s
epoch 71 | loss: 0.72886 | eval_custom_logloss: 0.76538 |  0:40:14s
epoch 72 | loss: 0.73491 | eval_custom_logloss: 0.73796 |  0:40:48s
epoch 73 | loss: 0.71827 | eval_custom_logloss: 0.69539 |  0:41:21s
epoch 74 | loss: 0.74616 | eval_custom_logloss: 0.68927 |  0:41:55s
epoch 75 | loss: 0.73561 | eval_custom_logloss: 0.6746  |  0:42:28s
epoch 76 | loss: 0.72981 | eval_custom_logloss: 0.78632 |  0:43:02s
epoch 77 | loss: 0.74514 | eval_custom_logloss: 0.84704 |  0:43:35s
epoch 78 | loss: 0.73272 | eval_custom_logloss: 0.82248 |  0:44:08s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.66132
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.737825, 'Log Loss - std': 0.07411539566783677} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax', 'n_a': 8, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.84736 | eval_custom_logloss: 1.39767 |  0:00:33s
epoch 1  | loss: 1.36358 | eval_custom_logloss: 1.16124 |  0:01:06s
epoch 2  | loss: 1.22999 | eval_custom_logloss: 1.04742 |  0:01:40s
epoch 3  | loss: 1.14614 | eval_custom_logloss: 1.00234 |  0:02:13s
epoch 4  | loss: 1.09652 | eval_custom_logloss: 1.12743 |  0:02:47s
epoch 5  | loss: 1.1478  | eval_custom_logloss: 0.93929 |  0:03:20s
epoch 6  | loss: 1.06902 | eval_custom_logloss: 0.93156 |  0:03:53s
epoch 7  | loss: 1.04259 | eval_custom_logloss: 0.93569 |  0:04:27s
epoch 8  | loss: 1.05075 | eval_custom_logloss: 0.96625 |  0:05:00s
epoch 9  | loss: 1.09506 | eval_custom_logloss: 1.09119 |  0:05:33s
epoch 10 | loss: 1.14757 | eval_custom_logloss: 0.98301 |  0:06:07s
epoch 11 | loss: 1.07515 | eval_custom_logloss: 0.9308  |  0:06:40s
epoch 12 | loss: 1.08227 | eval_custom_logloss: 1.00482 |  0:07:14s
epoch 13 | loss: 1.02198 | eval_custom_logloss: 1.00997 |  0:07:47s
epoch 14 | loss: 1.06329 | eval_custom_logloss: 1.01632 |  0:08:20s
epoch 15 | loss: 1.02934 | eval_custom_logloss: 0.93947 |  0:08:53s
epoch 16 | loss: 1.08682 | eval_custom_logloss: 1.0713  |  0:09:27s
epoch 17 | loss: 1.11334 | eval_custom_logloss: 1.11649 |  0:10:01s
epoch 18 | loss: 1.05654 | eval_custom_logloss: 0.90463 |  0:10:34s
epoch 19 | loss: 0.98257 | eval_custom_logloss: 0.91117 |  0:11:08s
epoch 20 | loss: 0.95836 | eval_custom_logloss: 0.85052 |  0:11:42s
epoch 21 | loss: 0.963   | eval_custom_logloss: 0.86301 |  0:12:16s
epoch 22 | loss: 0.9448  | eval_custom_logloss: 0.88072 |  0:12:50s
epoch 23 | loss: 0.9311  | eval_custom_logloss: 0.87498 |  0:13:23s
epoch 24 | loss: 0.93754 | eval_custom_logloss: 0.95474 |  0:13:57s
epoch 25 | loss: 0.92828 | eval_custom_logloss: 0.86805 |  0:14:30s
epoch 26 | loss: 0.97835 | eval_custom_logloss: 0.99015 |  0:15:03s
epoch 27 | loss: 0.93491 | eval_custom_logloss: 0.86984 |  0:15:36s
epoch 28 | loss: 0.95958 | eval_custom_logloss: 0.89033 |  0:16:09s
epoch 29 | loss: 0.92567 | eval_custom_logloss: 0.86531 |  0:16:42s
epoch 30 | loss: 0.9321  | eval_custom_logloss: 0.86334 |  0:17:16s
epoch 31 | loss: 0.92294 | eval_custom_logloss: 0.8996  |  0:17:49s
epoch 32 | loss: 0.90541 | eval_custom_logloss: 0.91381 |  0:18:22s
epoch 33 | loss: 0.89301 | eval_custom_logloss: 0.97199 |  0:18:55s
epoch 34 | loss: 0.88339 | eval_custom_logloss: 0.835   |  0:19:28s
epoch 35 | loss: 0.87762 | eval_custom_logloss: 0.84386 |  0:20:01s
epoch 36 | loss: 0.8759  | eval_custom_logloss: 0.80731 |  0:20:35s
epoch 37 | loss: 0.87455 | eval_custom_logloss: 0.81066 |  0:21:09s
epoch 38 | loss: 0.85936 | eval_custom_logloss: 0.84552 |  0:21:44s
epoch 39 | loss: 0.86471 | eval_custom_logloss: 0.87389 |  0:22:17s
epoch 40 | loss: 0.85133 | eval_custom_logloss: 0.82913 |  0:22:50s
epoch 41 | loss: 0.8616  | eval_custom_logloss: 0.81987 |  0:23:23s
epoch 42 | loss: 0.84578 | eval_custom_logloss: 0.8773  |  0:23:57s
epoch 43 | loss: 0.84725 | eval_custom_logloss: 0.80691 |  0:24:30s
epoch 44 | loss: 0.84858 | eval_custom_logloss: 0.939   |  0:25:04s
epoch 45 | loss: 0.86022 | eval_custom_logloss: 0.78836 |  0:25:37s
epoch 46 | loss: 0.86005 | eval_custom_logloss: 0.82406 |  0:26:11s
epoch 47 | loss: 0.84764 | eval_custom_logloss: 0.82394 |  0:26:45s
epoch 48 | loss: 0.84338 | eval_custom_logloss: 0.77817 |  0:27:18s
epoch 49 | loss: 0.83325 | eval_custom_logloss: 0.80222 |  0:27:51s
epoch 50 | loss: 0.84123 | eval_custom_logloss: 0.83132 |  0:28:24s
epoch 51 | loss: 0.82971 | eval_custom_logloss: 0.80076 |  0:28:58s
epoch 52 | loss: 0.82475 | eval_custom_logloss: 0.81231 |  0:29:31s
epoch 53 | loss: 0.82234 | eval_custom_logloss: 0.81551 |  0:30:04s
epoch 54 | loss: 0.83446 | eval_custom_logloss: 0.83736 |  0:30:37s
epoch 55 | loss: 0.82298 | eval_custom_logloss: 0.8261  |  0:31:10s
epoch 56 | loss: 0.81577 | eval_custom_logloss: 0.86863 |  0:31:44s
epoch 57 | loss: 1.03489 | eval_custom_logloss: 1.1132  |  0:32:17s
epoch 58 | loss: 1.0642  | eval_custom_logloss: 1.06984 |  0:32:50s
epoch 59 | loss: 1.03682 | eval_custom_logloss: 0.99343 |  0:33:23s
epoch 60 | loss: 0.94222 | eval_custom_logloss: 0.89334 |  0:33:57s
epoch 61 | loss: 0.94587 | eval_custom_logloss: 0.94681 |  0:34:30s
epoch 62 | loss: 0.99214 | eval_custom_logloss: 1.00717 |  0:35:03s
epoch 63 | loss: 1.0061  | eval_custom_logloss: 1.00474 |  0:35:36s
epoch 64 | loss: 0.98359 | eval_custom_logloss: 0.95902 |  0:36:09s
epoch 65 | loss: 0.95071 | eval_custom_logloss: 0.93544 |  0:36:42s
epoch 66 | loss: 0.98425 | eval_custom_logloss: 0.98987 |  0:37:15s
epoch 67 | loss: 0.98256 | eval_custom_logloss: 0.99479 |  0:37:48s
epoch 68 | loss: 0.99134 | eval_custom_logloss: 0.98023 |  0:38:21s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 0.77817
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.74566, 'Log Loss - std': 0.06811770988516862} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 0.74566 and parameters: {'n_d': 8, 'n_steps': 10, 'gamma': 1.4069334146035226, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.14339273105437386, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax', 'n_a': 16, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73804 | eval_custom_logloss: 1.81633 |  0:00:24s
epoch 1  | loss: 1.45727 | eval_custom_logloss: 1.2982  |  0:00:49s
epoch 2  | loss: 1.26331 | eval_custom_logloss: 1.0948  |  0:01:14s
epoch 3  | loss: 1.16411 | eval_custom_logloss: 1.21149 |  0:01:39s
epoch 4  | loss: 1.07388 | eval_custom_logloss: 1.151   |  0:02:04s
epoch 5  | loss: 1.03217 | eval_custom_logloss: 0.94899 |  0:02:29s
epoch 6  | loss: 1.00034 | eval_custom_logloss: 1.03759 |  0:02:54s
epoch 7  | loss: 0.96999 | eval_custom_logloss: 0.85103 |  0:03:19s
epoch 8  | loss: 0.97373 | eval_custom_logloss: 0.89045 |  0:03:44s
epoch 9  | loss: 0.96837 | eval_custom_logloss: 0.9294  |  0:04:09s
epoch 10 | loss: 0.96725 | eval_custom_logloss: 0.86992 |  0:04:34s
epoch 11 | loss: 0.94029 | eval_custom_logloss: 0.84199 |  0:04:58s
epoch 12 | loss: 0.93078 | eval_custom_logloss: 0.80409 |  0:05:23s
epoch 13 | loss: 0.93382 | eval_custom_logloss: 0.82498 |  0:05:49s
epoch 14 | loss: 0.90276 | eval_custom_logloss: 0.78673 |  0:06:13s
epoch 15 | loss: 0.89557 | eval_custom_logloss: 0.89297 |  0:06:38s
epoch 16 | loss: 0.86799 | eval_custom_logloss: 1.2987  |  0:07:03s
epoch 17 | loss: 0.86699 | eval_custom_logloss: 0.96637 |  0:07:28s
epoch 18 | loss: 0.84738 | eval_custom_logloss: 0.886   |  0:07:53s
epoch 19 | loss: 0.83803 | eval_custom_logloss: 1.24593 |  0:08:18s
epoch 20 | loss: 0.84992 | eval_custom_logloss: 0.79443 |  0:08:44s
epoch 21 | loss: 0.82753 | eval_custom_logloss: 0.90393 |  0:09:08s
epoch 22 | loss: 0.82297 | eval_custom_logloss: 0.82208 |  0:09:33s
epoch 23 | loss: 0.81772 | eval_custom_logloss: 0.80327 |  0:09:59s
epoch 24 | loss: 0.8116  | eval_custom_logloss: 0.76919 |  0:10:24s
epoch 25 | loss: 0.81619 | eval_custom_logloss: 0.84956 |  0:10:49s
epoch 26 | loss: 0.80544 | eval_custom_logloss: 0.99507 |  0:11:14s
epoch 27 | loss: 0.82199 | eval_custom_logloss: 0.86439 |  0:11:39s
epoch 28 | loss: 0.7955  | eval_custom_logloss: 0.78513 |  0:12:04s
epoch 29 | loss: 0.80574 | eval_custom_logloss: 0.99396 |  0:12:29s
epoch 30 | loss: 0.7898  | eval_custom_logloss: 0.87179 |  0:12:54s
epoch 31 | loss: 0.79922 | eval_custom_logloss: 0.87281 |  0:13:19s
epoch 32 | loss: 0.78256 | eval_custom_logloss: 0.79675 |  0:13:44s
epoch 33 | loss: 0.76845 | eval_custom_logloss: 0.91162 |  0:14:09s
epoch 34 | loss: 0.77024 | eval_custom_logloss: 0.75854 |  0:14:34s
epoch 35 | loss: 0.7759  | eval_custom_logloss: 0.78303 |  0:14:58s
epoch 36 | loss: 0.78624 | eval_custom_logloss: 0.92741 |  0:15:24s
epoch 37 | loss: 0.77142 | eval_custom_logloss: 0.96806 |  0:15:48s
epoch 38 | loss: 0.7583  | eval_custom_logloss: 0.84376 |  0:16:13s
epoch 39 | loss: 0.75232 | eval_custom_logloss: 1.43276 |  0:16:38s
epoch 40 | loss: 0.75612 | eval_custom_logloss: 0.78446 |  0:17:03s
epoch 41 | loss: 0.74763 | eval_custom_logloss: 0.80079 |  0:17:28s
epoch 42 | loss: 0.73951 | eval_custom_logloss: 1.22007 |  0:17:53s
epoch 43 | loss: 0.74499 | eval_custom_logloss: 0.71119 |  0:18:18s
epoch 44 | loss: 0.73458 | eval_custom_logloss: 0.69516 |  0:18:43s
epoch 45 | loss: 0.74052 | eval_custom_logloss: 0.74867 |  0:19:08s
epoch 46 | loss: 0.7326  | eval_custom_logloss: 0.77039 |  0:19:33s
epoch 47 | loss: 0.729   | eval_custom_logloss: 0.87887 |  0:19:59s
epoch 48 | loss: 0.72946 | eval_custom_logloss: 0.87176 |  0:20:23s
epoch 49 | loss: 0.71691 | eval_custom_logloss: 0.73757 |  0:20:48s
epoch 50 | loss: 0.72244 | eval_custom_logloss: 0.79857 |  0:21:13s
epoch 51 | loss: 0.72228 | eval_custom_logloss: 0.71252 |  0:21:38s
epoch 52 | loss: 0.72285 | eval_custom_logloss: 0.72796 |  0:22:03s
epoch 53 | loss: 0.71659 | eval_custom_logloss: 0.83416 |  0:22:28s
epoch 54 | loss: 0.73297 | eval_custom_logloss: 1.23277 |  0:22:53s
epoch 55 | loss: 0.71923 | eval_custom_logloss: 0.85231 |  0:23:18s
epoch 56 | loss: 0.72298 | eval_custom_logloss: 0.75584 |  0:23:43s
epoch 57 | loss: 0.71731 | eval_custom_logloss: 0.85381 |  0:24:08s
epoch 58 | loss: 0.71841 | eval_custom_logloss: 1.13465 |  0:24:33s
epoch 59 | loss: 0.71657 | eval_custom_logloss: 0.71084 |  0:24:58s
epoch 60 | loss: 0.70621 | eval_custom_logloss: 1.50559 |  0:25:23s
epoch 61 | loss: 0.69539 | eval_custom_logloss: 0.68026 |  0:25:48s
epoch 62 | loss: 0.7082  | eval_custom_logloss: 1.06133 |  0:26:12s
epoch 63 | loss: 0.71933 | eval_custom_logloss: 0.72604 |  0:26:37s
epoch 64 | loss: 0.69935 | eval_custom_logloss: 0.8047  |  0:27:02s
epoch 65 | loss: 0.70228 | eval_custom_logloss: 0.82713 |  0:27:27s
epoch 66 | loss: 0.69923 | eval_custom_logloss: 0.7736  |  0:27:52s
epoch 67 | loss: 0.69541 | eval_custom_logloss: 0.81748 |  0:28:17s
epoch 68 | loss: 0.69407 | eval_custom_logloss: 0.8183  |  0:28:42s
epoch 69 | loss: 0.70218 | eval_custom_logloss: 0.7473  |  0:29:07s
epoch 70 | loss: 0.69867 | eval_custom_logloss: 0.8251  |  0:29:32s
epoch 71 | loss: 0.69402 | eval_custom_logloss: 0.85818 |  0:29:56s
epoch 72 | loss: 0.70768 | eval_custom_logloss: 1.39531 |  0:30:21s
epoch 73 | loss: 0.68669 | eval_custom_logloss: 0.6476  |  0:30:46s
epoch 74 | loss: 0.6955  | eval_custom_logloss: 0.83527 |  0:31:11s
epoch 75 | loss: 0.69843 | eval_custom_logloss: 0.92997 |  0:31:36s
epoch 76 | loss: 0.68145 | eval_custom_logloss: 0.7839  |  0:32:01s
epoch 77 | loss: 0.69307 | eval_custom_logloss: 1.02862 |  0:32:26s
epoch 78 | loss: 0.68202 | eval_custom_logloss: 0.93921 |  0:32:51s
epoch 79 | loss: 0.68971 | eval_custom_logloss: 0.88392 |  0:33:16s
epoch 80 | loss: 0.69021 | eval_custom_logloss: 0.74108 |  0:33:42s
epoch 81 | loss: 0.67802 | eval_custom_logloss: 0.85534 |  0:34:07s
epoch 82 | loss: 0.67722 | eval_custom_logloss: 0.85521 |  0:34:32s
epoch 83 | loss: 0.68246 | eval_custom_logloss: 1.21762 |  0:34:58s
epoch 84 | loss: 0.68163 | eval_custom_logloss: 0.87648 |  0:35:23s
epoch 85 | loss: 0.68507 | eval_custom_logloss: 0.8175  |  0:35:48s
epoch 86 | loss: 0.67973 | eval_custom_logloss: 0.76419 |  0:36:13s
epoch 87 | loss: 0.68748 | eval_custom_logloss: 0.74419 |  0:36:38s
epoch 88 | loss: 0.67219 | eval_custom_logloss: 1.32923 |  0:37:03s
epoch 89 | loss: 0.68165 | eval_custom_logloss: 0.76154 |  0:37:28s
epoch 90 | loss: 0.68807 | eval_custom_logloss: 0.80614 |  0:37:53s
epoch 91 | loss: 0.66523 | eval_custom_logloss: 0.8914  |  0:38:18s
epoch 92 | loss: 0.69095 | eval_custom_logloss: 0.84639 |  0:38:43s
epoch 93 | loss: 0.67362 | eval_custom_logloss: 0.77601 |  0:39:08s

Early stopping occurred at epoch 93 with best_epoch = 73 and best_eval_custom_logloss = 0.6476
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6446, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax', 'n_a': 16, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71995 | eval_custom_logloss: 1.57455 |  0:00:24s
epoch 1  | loss: 1.32288 | eval_custom_logloss: 1.22872 |  0:00:49s
epoch 2  | loss: 1.18227 | eval_custom_logloss: 1.06799 |  0:01:14s
epoch 3  | loss: 1.09276 | eval_custom_logloss: 1.19488 |  0:01:40s
epoch 4  | loss: 1.11659 | eval_custom_logloss: 1.12203 |  0:02:05s
epoch 5  | loss: 1.0935  | eval_custom_logloss: 1.01829 |  0:02:30s
epoch 6  | loss: 1.08124 | eval_custom_logloss: 1.10807 |  0:02:55s
epoch 7  | loss: 1.02684 | eval_custom_logloss: 0.92972 |  0:03:20s
epoch 8  | loss: 1.0313  | eval_custom_logloss: 1.0763  |  0:03:45s
epoch 9  | loss: 1.01135 | eval_custom_logloss: 1.02015 |  0:04:10s
epoch 10 | loss: 1.03056 | eval_custom_logloss: 0.93575 |  0:04:35s
epoch 11 | loss: 0.98477 | eval_custom_logloss: 0.90379 |  0:05:00s
epoch 12 | loss: 0.9705  | eval_custom_logloss: 0.88732 |  0:05:25s
epoch 13 | loss: 0.97778 | eval_custom_logloss: 0.91897 |  0:05:49s
epoch 14 | loss: 0.95773 | eval_custom_logloss: 0.89402 |  0:06:14s
epoch 15 | loss: 0.94282 | eval_custom_logloss: 0.84144 |  0:06:39s
epoch 16 | loss: 0.95477 | eval_custom_logloss: 1.02434 |  0:07:04s
epoch 17 | loss: 0.96733 | eval_custom_logloss: 1.02888 |  0:07:29s
epoch 18 | loss: 0.95327 | eval_custom_logloss: 0.88866 |  0:07:53s
epoch 19 | loss: 0.9703  | eval_custom_logloss: 1.9968  |  0:08:18s
epoch 20 | loss: 1.04007 | eval_custom_logloss: 1.2145  |  0:08:44s
epoch 21 | loss: 1.10778 | eval_custom_logloss: 1.0638  |  0:09:08s
epoch 22 | loss: 1.07408 | eval_custom_logloss: 1.0634  |  0:09:33s
epoch 23 | loss: 1.05716 | eval_custom_logloss: 1.02063 |  0:09:58s
epoch 24 | loss: 1.04831 | eval_custom_logloss: 0.9629  |  0:10:23s
epoch 25 | loss: 1.02058 | eval_custom_logloss: 0.96928 |  0:10:48s
epoch 26 | loss: 1.05132 | eval_custom_logloss: 1.19316 |  0:11:13s
epoch 27 | loss: 1.10105 | eval_custom_logloss: 1.11238 |  0:11:37s
epoch 28 | loss: 1.07181 | eval_custom_logloss: 0.97182 |  0:12:02s
epoch 29 | loss: 0.978   | eval_custom_logloss: 0.84771 |  0:12:27s
epoch 30 | loss: 0.89067 | eval_custom_logloss: 0.86684 |  0:12:51s
epoch 31 | loss: 0.89966 | eval_custom_logloss: 1.0988  |  0:13:16s
epoch 32 | loss: 0.86937 | eval_custom_logloss: 0.8545  |  0:13:41s
epoch 33 | loss: 0.86843 | eval_custom_logloss: 0.84132 |  0:14:06s
epoch 34 | loss: 0.86609 | eval_custom_logloss: 0.80674 |  0:14:31s
epoch 35 | loss: 0.86768 | eval_custom_logloss: 0.80389 |  0:14:55s
epoch 36 | loss: 0.86463 | eval_custom_logloss: 0.8568  |  0:15:20s
epoch 37 | loss: 0.83816 | eval_custom_logloss: 0.8295  |  0:15:45s
epoch 38 | loss: 0.83761 | eval_custom_logloss: 0.86374 |  0:16:10s
epoch 39 | loss: 0.84307 | eval_custom_logloss: 0.89799 |  0:16:35s
epoch 40 | loss: 0.83279 | eval_custom_logloss: 0.94204 |  0:17:00s
epoch 41 | loss: 0.83486 | eval_custom_logloss: 0.93283 |  0:17:24s
epoch 42 | loss: 0.81754 | eval_custom_logloss: 0.84135 |  0:17:49s
epoch 43 | loss: 0.81403 | eval_custom_logloss: 0.84188 |  0:18:14s
epoch 44 | loss: 0.79729 | eval_custom_logloss: 0.77911 |  0:18:39s
epoch 45 | loss: 0.79914 | eval_custom_logloss: 1.2645  |  0:19:04s
epoch 46 | loss: 0.78471 | eval_custom_logloss: 0.74687 |  0:19:29s
epoch 47 | loss: 0.78255 | eval_custom_logloss: 0.79524 |  0:19:53s
epoch 48 | loss: 0.78666 | eval_custom_logloss: 0.79712 |  0:20:18s
epoch 49 | loss: 0.78556 | eval_custom_logloss: 0.82099 |  0:20:43s
epoch 50 | loss: 0.784   | eval_custom_logloss: 0.85766 |  0:21:08s
epoch 51 | loss: 0.77655 | eval_custom_logloss: 0.81081 |  0:21:33s
epoch 52 | loss: 0.76645 | eval_custom_logloss: 0.74124 |  0:21:58s
epoch 53 | loss: 0.77622 | eval_custom_logloss: 0.71438 |  0:22:23s
epoch 54 | loss: 0.76597 | eval_custom_logloss: 0.81603 |  0:22:48s
epoch 55 | loss: 0.76196 | eval_custom_logloss: 0.73606 |  0:23:13s
epoch 56 | loss: 0.75004 | eval_custom_logloss: 0.75364 |  0:23:38s
epoch 57 | loss: 0.75478 | eval_custom_logloss: 1.13481 |  0:24:03s
epoch 58 | loss: 0.75881 | eval_custom_logloss: 0.80678 |  0:24:28s
epoch 59 | loss: 0.75474 | eval_custom_logloss: 1.21824 |  0:24:53s
epoch 60 | loss: 0.74179 | eval_custom_logloss: 1.0511  |  0:25:18s
epoch 61 | loss: 0.74138 | eval_custom_logloss: 2.24163 |  0:25:43s
epoch 62 | loss: 0.74843 | eval_custom_logloss: 1.21832 |  0:26:07s
epoch 63 | loss: 0.75358 | eval_custom_logloss: 0.69355 |  0:26:32s
epoch 64 | loss: 0.74815 | eval_custom_logloss: 0.74353 |  0:26:57s
epoch 65 | loss: 0.74865 | eval_custom_logloss: 0.82564 |  0:27:22s
epoch 66 | loss: 0.74229 | eval_custom_logloss: 0.93975 |  0:27:47s
epoch 67 | loss: 0.74418 | eval_custom_logloss: 0.93614 |  0:28:12s
epoch 68 | loss: 0.73695 | eval_custom_logloss: 0.7222  |  0:28:37s
epoch 69 | loss: 0.73749 | eval_custom_logloss: 0.74475 |  0:29:02s
epoch 70 | loss: 0.72497 | eval_custom_logloss: 0.71811 |  0:29:27s
epoch 71 | loss: 0.73346 | eval_custom_logloss: 0.74966 |  0:29:52s
epoch 72 | loss: 0.74352 | eval_custom_logloss: 0.92269 |  0:30:18s
epoch 73 | loss: 0.72721 | eval_custom_logloss: 0.7085  |  0:30:43s
epoch 74 | loss: 0.73676 | eval_custom_logloss: 1.11733 |  0:31:08s
epoch 75 | loss: 0.7365  | eval_custom_logloss: 1.85581 |  0:31:33s
epoch 76 | loss: 0.72313 | eval_custom_logloss: 0.86588 |  0:31:58s
epoch 77 | loss: 0.72645 | eval_custom_logloss: 1.70976 |  0:32:24s
epoch 78 | loss: 0.72613 | eval_custom_logloss: 0.92469 |  0:32:49s
epoch 79 | loss: 0.72639 | eval_custom_logloss: 0.88139 |  0:33:14s
epoch 80 | loss: 0.72036 | eval_custom_logloss: 0.78516 |  0:33:38s
epoch 81 | loss: 0.72242 | eval_custom_logloss: 0.78438 |  0:34:03s
epoch 82 | loss: 0.71759 | eval_custom_logloss: 0.88803 |  0:34:28s
epoch 83 | loss: 0.72842 | eval_custom_logloss: 0.7288  |  0:34:53s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 0.69355
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.66885, 'Log Loss - std': 0.02425000000000005} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax', 'n_a': 16, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.66409 | eval_custom_logloss: 1.27793 |  0:00:24s
epoch 1  | loss: 1.27139 | eval_custom_logloss: 1.1024  |  0:00:48s
epoch 2  | loss: 1.09704 | eval_custom_logloss: 0.99624 |  0:01:12s
epoch 3  | loss: 1.0121  | eval_custom_logloss: 0.8703  |  0:01:37s
epoch 4  | loss: 0.97039 | eval_custom_logloss: 1.19452 |  0:02:02s
epoch 5  | loss: 0.9616  | eval_custom_logloss: 0.86059 |  0:02:27s
epoch 6  | loss: 0.9553  | eval_custom_logloss: 1.03396 |  0:02:52s
epoch 7  | loss: 0.91015 | eval_custom_logloss: 1.02505 |  0:03:17s
epoch 8  | loss: 0.91465 | eval_custom_logloss: 1.73119 |  0:03:41s
epoch 9  | loss: 0.91005 | eval_custom_logloss: 1.14479 |  0:04:06s
epoch 10 | loss: 0.88718 | eval_custom_logloss: 0.8679  |  0:04:31s
epoch 11 | loss: 0.87269 | eval_custom_logloss: 1.55046 |  0:04:56s
epoch 12 | loss: 0.84718 | eval_custom_logloss: 1.06797 |  0:05:21s
epoch 13 | loss: 0.89037 | eval_custom_logloss: 1.12994 |  0:05:46s
epoch 14 | loss: 0.84272 | eval_custom_logloss: 1.73484 |  0:06:10s
epoch 15 | loss: 0.84029 | eval_custom_logloss: 1.08203 |  0:06:35s
epoch 16 | loss: 0.83012 | eval_custom_logloss: 0.93296 |  0:07:00s
epoch 17 | loss: 0.82484 | eval_custom_logloss: 0.91913 |  0:07:25s
epoch 18 | loss: 0.8694  | eval_custom_logloss: 1.01436 |  0:07:50s
epoch 19 | loss: 0.88778 | eval_custom_logloss: 2.09666 |  0:08:15s
epoch 20 | loss: 0.85438 | eval_custom_logloss: 0.97899 |  0:08:40s
epoch 21 | loss: 0.83859 | eval_custom_logloss: 0.87042 |  0:09:05s
epoch 22 | loss: 0.83026 | eval_custom_logloss: 1.05099 |  0:09:30s
epoch 23 | loss: 0.82692 | eval_custom_logloss: 3.30343 |  0:09:54s
epoch 24 | loss: 0.85404 | eval_custom_logloss: 0.98543 |  0:10:19s
epoch 25 | loss: 0.82453 | eval_custom_logloss: 1.46224 |  0:10:44s

Early stopping occurred at epoch 25 with best_epoch = 5 and best_eval_custom_logloss = 0.86059
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7323666666666666, 'Log Loss - std': 0.0919824741760927} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax', 'n_a': 16, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73879 | eval_custom_logloss: 1.44995 |  0:00:24s
epoch 1  | loss: 1.3771  | eval_custom_logloss: 1.55546 |  0:00:49s
epoch 2  | loss: 1.26354 | eval_custom_logloss: 1.23896 |  0:01:14s
epoch 3  | loss: 1.17661 | eval_custom_logloss: 1.05244 |  0:01:39s
epoch 4  | loss: 1.10642 | eval_custom_logloss: 1.10642 |  0:02:04s
epoch 5  | loss: 1.08415 | eval_custom_logloss: 0.99216 |  0:02:29s
epoch 6  | loss: 1.04215 | eval_custom_logloss: 1.05292 |  0:02:54s
epoch 7  | loss: 1.00677 | eval_custom_logloss: 0.99641 |  0:03:18s
epoch 8  | loss: 0.99455 | eval_custom_logloss: 0.87704 |  0:03:43s
epoch 9  | loss: 0.97889 | eval_custom_logloss: 0.90661 |  0:04:08s
epoch 10 | loss: 0.97734 | eval_custom_logloss: 0.93478 |  0:04:33s
epoch 11 | loss: 0.96494 | eval_custom_logloss: 0.86453 |  0:04:58s
epoch 12 | loss: 0.95892 | eval_custom_logloss: 1.22675 |  0:05:23s
epoch 13 | loss: 1.01714 | eval_custom_logloss: 0.93913 |  0:05:48s
epoch 14 | loss: 0.98535 | eval_custom_logloss: 0.92324 |  0:06:13s
epoch 15 | loss: 0.95281 | eval_custom_logloss: 0.94243 |  0:06:38s
epoch 16 | loss: 0.94257 | eval_custom_logloss: 1.0261  |  0:07:03s
epoch 17 | loss: 0.89856 | eval_custom_logloss: 1.93873 |  0:07:28s
epoch 18 | loss: 0.89    | eval_custom_logloss: 1.09274 |  0:07:53s
epoch 19 | loss: 0.85197 | eval_custom_logloss: 1.01754 |  0:08:18s
epoch 20 | loss: 0.85486 | eval_custom_logloss: 0.92596 |  0:08:43s
epoch 21 | loss: 0.8358  | eval_custom_logloss: 0.90267 |  0:09:08s
epoch 22 | loss: 0.82322 | eval_custom_logloss: 1.10164 |  0:09:33s
epoch 23 | loss: 0.82685 | eval_custom_logloss: 0.84527 |  0:09:58s
epoch 24 | loss: 0.80584 | eval_custom_logloss: 1.03352 |  0:10:23s
epoch 25 | loss: 0.81108 | eval_custom_logloss: 0.89024 |  0:10:47s
epoch 26 | loss: 0.80241 | eval_custom_logloss: 0.90082 |  0:11:12s
epoch 27 | loss: 0.80187 | eval_custom_logloss: 0.84748 |  0:11:37s
epoch 28 | loss: 0.80211 | eval_custom_logloss: 0.75211 |  0:12:01s
epoch 29 | loss: 0.78429 | eval_custom_logloss: 0.82359 |  0:12:26s
epoch 30 | loss: 0.77706 | eval_custom_logloss: 0.81397 |  0:12:51s
epoch 31 | loss: 0.78129 | eval_custom_logloss: 0.93259 |  0:13:16s
epoch 32 | loss: 0.76833 | eval_custom_logloss: 0.77475 |  0:13:40s
epoch 33 | loss: 0.76732 | eval_custom_logloss: 1.01047 |  0:14:05s
epoch 34 | loss: 0.76347 | eval_custom_logloss: 2.14656 |  0:14:30s
epoch 35 | loss: 0.77509 | eval_custom_logloss: 0.74235 |  0:14:54s
epoch 36 | loss: 0.75251 | eval_custom_logloss: 0.91357 |  0:15:19s
epoch 37 | loss: 0.75038 | eval_custom_logloss: 0.78004 |  0:15:44s
epoch 38 | loss: 0.74629 | eval_custom_logloss: 0.77185 |  0:16:08s
epoch 39 | loss: 0.75096 | eval_custom_logloss: 0.87218 |  0:16:33s
epoch 40 | loss: 0.75219 | eval_custom_logloss: 0.70205 |  0:16:58s
epoch 41 | loss: 0.75213 | eval_custom_logloss: 0.76856 |  0:17:22s
epoch 42 | loss: 0.77432 | eval_custom_logloss: 1.13203 |  0:17:47s
epoch 43 | loss: 0.745   | eval_custom_logloss: 0.96649 |  0:18:12s
epoch 44 | loss: 0.74171 | eval_custom_logloss: 0.84809 |  0:18:37s
epoch 45 | loss: 0.73128 | eval_custom_logloss: 2.24617 |  0:19:01s
epoch 46 | loss: 0.73019 | eval_custom_logloss: 2.07117 |  0:19:26s
epoch 47 | loss: 0.72163 | eval_custom_logloss: 0.89404 |  0:19:51s
epoch 48 | loss: 0.71064 | eval_custom_logloss: 1.107   |  0:20:15s
epoch 49 | loss: 0.72577 | eval_custom_logloss: 0.66303 |  0:20:42s
epoch 50 | loss: 0.712   | eval_custom_logloss: 0.82769 |  0:21:09s
epoch 51 | loss: 0.70956 | eval_custom_logloss: 0.70455 |  0:21:36s
epoch 52 | loss: 0.70955 | eval_custom_logloss: 0.91858 |  0:22:04s
epoch 53 | loss: 0.72068 | eval_custom_logloss: 0.86467 |  0:22:31s
epoch 54 | loss: 0.7166  | eval_custom_logloss: 0.8209  |  0:22:58s
epoch 55 | loss: 0.70285 | eval_custom_logloss: 1.2294  |  0:23:25s
epoch 56 | loss: 0.70178 | eval_custom_logloss: 1.03692 |  0:23:52s
epoch 57 | loss: 0.71591 | eval_custom_logloss: 0.75804 |  0:24:20s
epoch 58 | loss: 0.68909 | eval_custom_logloss: 0.97935 |  0:24:48s
epoch 59 | loss: 0.69059 | eval_custom_logloss: 0.7734  |  0:25:16s
epoch 60 | loss: 0.68564 | eval_custom_logloss: 0.76808 |  0:25:43s
epoch 61 | loss: 0.69395 | eval_custom_logloss: 0.84692 |  0:26:11s
epoch 62 | loss: 0.68985 | eval_custom_logloss: 2.39704 |  0:26:38s
epoch 63 | loss: 0.69629 | eval_custom_logloss: 0.85614 |  0:27:05s
epoch 64 | loss: 0.68005 | eval_custom_logloss: 0.8784  |  0:27:32s
epoch 65 | loss: 0.67524 | eval_custom_logloss: 0.6865  |  0:27:59s
epoch 66 | loss: 0.67864 | eval_custom_logloss: 1.0744  |  0:28:26s
epoch 67 | loss: 0.70087 | eval_custom_logloss: 0.79417 |  0:28:54s
epoch 68 | loss: 0.6842  | eval_custom_logloss: 0.73732 |  0:29:21s
epoch 69 | loss: 0.67998 | eval_custom_logloss: 0.73078 |  0:29:48s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 0.66303
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.71505, 'Log Loss - std': 0.08511863779455123} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax', 'n_a': 16, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.74325 | eval_custom_logloss: 1.52364 |  0:00:27s
epoch 1  | loss: 1.24791 | eval_custom_logloss: 1.34618 |  0:00:54s
epoch 2  | loss: 1.15198 | eval_custom_logloss: 1.02058 |  0:01:20s
epoch 3  | loss: 1.07797 | eval_custom_logloss: 0.97773 |  0:01:47s
epoch 4  | loss: 1.04408 | eval_custom_logloss: 1.00046 |  0:02:13s
epoch 5  | loss: 1.0562  | eval_custom_logloss: 1.17468 |  0:02:40s
epoch 6  | loss: 1.04041 | eval_custom_logloss: 0.95814 |  0:03:07s
epoch 7  | loss: 1.00219 | eval_custom_logloss: 0.9727  |  0:03:34s
epoch 8  | loss: 1.03635 | eval_custom_logloss: 0.99163 |  0:04:01s
epoch 9  | loss: 1.02009 | eval_custom_logloss: 0.9154  |  0:04:28s
epoch 10 | loss: 1.00411 | eval_custom_logloss: 0.9337  |  0:04:55s
epoch 11 | loss: 0.96245 | eval_custom_logloss: 0.92587 |  0:05:21s
epoch 12 | loss: 0.97253 | eval_custom_logloss: 0.91151 |  0:05:49s
epoch 13 | loss: 0.97156 | eval_custom_logloss: 1.03201 |  0:06:16s
epoch 14 | loss: 0.94437 | eval_custom_logloss: 0.92162 |  0:06:44s
epoch 15 | loss: 0.95029 | eval_custom_logloss: 0.94978 |  0:07:11s
epoch 16 | loss: 0.9397  | eval_custom_logloss: 1.10261 |  0:07:39s
epoch 17 | loss: 0.93688 | eval_custom_logloss: 0.85017 |  0:08:06s
epoch 18 | loss: 0.93985 | eval_custom_logloss: 0.86781 |  0:08:34s
epoch 19 | loss: 0.92889 | eval_custom_logloss: 0.91646 |  0:09:01s
epoch 20 | loss: 0.93521 | eval_custom_logloss: 0.8892  |  0:09:28s
epoch 21 | loss: 0.91104 | eval_custom_logloss: 0.99011 |  0:09:56s
epoch 22 | loss: 0.92362 | eval_custom_logloss: 0.88887 |  0:10:23s
epoch 23 | loss: 0.92321 | eval_custom_logloss: 0.92234 |  0:10:51s
epoch 24 | loss: 0.9219  | eval_custom_logloss: 0.95556 |  0:11:18s
epoch 25 | loss: 0.90883 | eval_custom_logloss: 1.10148 |  0:11:45s
epoch 26 | loss: 0.9105  | eval_custom_logloss: 0.98867 |  0:12:12s
epoch 27 | loss: 0.90815 | eval_custom_logloss: 1.03331 |  0:12:39s
epoch 28 | loss: 0.90718 | eval_custom_logloss: 0.8469  |  0:13:06s
epoch 29 | loss: 0.8945  | eval_custom_logloss: 0.8643  |  0:13:33s
epoch 30 | loss: 0.91419 | eval_custom_logloss: 0.92777 |  0:14:01s
epoch 31 | loss: 0.94216 | eval_custom_logloss: 0.94289 |  0:14:28s
epoch 32 | loss: 0.92932 | eval_custom_logloss: 0.94647 |  0:14:55s
epoch 33 | loss: 0.91854 | eval_custom_logloss: 0.93691 |  0:15:22s
epoch 34 | loss: 0.91363 | eval_custom_logloss: 1.12219 |  0:15:50s
epoch 35 | loss: 0.93952 | eval_custom_logloss: 0.9652  |  0:16:17s
epoch 36 | loss: 0.94638 | eval_custom_logloss: 1.10812 |  0:16:44s
epoch 37 | loss: 0.92949 | eval_custom_logloss: 0.90477 |  0:17:11s
epoch 38 | loss: 0.91274 | eval_custom_logloss: 0.86326 |  0:17:38s
epoch 39 | loss: 0.91095 | eval_custom_logloss: 1.18288 |  0:18:06s
epoch 40 | loss: 0.96351 | eval_custom_logloss: 0.89047 |  0:18:33s
epoch 41 | loss: 0.99197 | eval_custom_logloss: 1.03465 |  0:19:00s
epoch 42 | loss: 0.926   | eval_custom_logloss: 0.9351  |  0:19:27s
epoch 43 | loss: 0.93092 | eval_custom_logloss: 1.0933  |  0:19:54s
epoch 44 | loss: 0.92744 | eval_custom_logloss: 0.95885 |  0:20:22s
epoch 45 | loss: 0.91856 | eval_custom_logloss: 0.88117 |  0:20:49s
epoch 46 | loss: 0.93567 | eval_custom_logloss: 0.94248 |  0:21:16s
epoch 47 | loss: 0.90755 | eval_custom_logloss: 0.85179 |  0:21:43s
epoch 48 | loss: 0.90792 | eval_custom_logloss: 0.94532 |  0:22:10s

Early stopping occurred at epoch 48 with best_epoch = 28 and best_eval_custom_logloss = 0.8469
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7410599999999999, 'Log Loss - std': 0.0922075181316578} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 0.7410599999999999 and parameters: {'n_d': 16, 'n_steps': 8, 'gamma': 1.5288896670620353, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.018137229261795007, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.60522 | eval_custom_logloss: 11.28094|  0:00:30s
epoch 1  | loss: 1.09658 | eval_custom_logloss: 4.54384 |  0:01:00s
epoch 2  | loss: 1.01341 | eval_custom_logloss: 2.83237 |  0:01:30s
epoch 3  | loss: 0.95509 | eval_custom_logloss: 2.40458 |  0:02:00s
epoch 4  | loss: 0.91014 | eval_custom_logloss: 2.12816 |  0:02:31s
epoch 5  | loss: 0.90451 | eval_custom_logloss: 1.36263 |  0:03:01s
epoch 6  | loss: 0.87304 | eval_custom_logloss: 3.90996 |  0:03:31s
epoch 7  | loss: 0.85023 | eval_custom_logloss: 1.02647 |  0:04:01s
epoch 8  | loss: 0.83235 | eval_custom_logloss: 1.35636 |  0:04:31s
epoch 9  | loss: 0.81923 | eval_custom_logloss: 1.51996 |  0:05:02s
epoch 10 | loss: 0.82528 | eval_custom_logloss: 1.50809 |  0:05:32s
epoch 11 | loss: 0.82922 | eval_custom_logloss: 1.26835 |  0:06:01s
epoch 12 | loss: 0.80297 | eval_custom_logloss: 1.99819 |  0:06:32s
epoch 13 | loss: 0.77993 | eval_custom_logloss: 1.04705 |  0:07:02s
epoch 14 | loss: 0.76603 | eval_custom_logloss: 1.10561 |  0:07:32s
epoch 15 | loss: 0.77757 | eval_custom_logloss: 1.00656 |  0:08:02s
epoch 16 | loss: 0.74878 | eval_custom_logloss: 0.95295 |  0:08:33s
epoch 17 | loss: 0.75919 | eval_custom_logloss: 1.11553 |  0:09:03s
epoch 18 | loss: 0.74016 | eval_custom_logloss: 1.11278 |  0:09:33s
epoch 19 | loss: 0.75301 | eval_custom_logloss: 1.24395 |  0:10:03s
epoch 20 | loss: 0.75077 | eval_custom_logloss: 1.55312 |  0:10:33s
epoch 21 | loss: 0.77527 | eval_custom_logloss: 1.14905 |  0:11:03s
epoch 22 | loss: 0.77092 | eval_custom_logloss: 1.37125 |  0:11:33s
epoch 23 | loss: 0.74249 | eval_custom_logloss: 0.88877 |  0:12:03s
epoch 24 | loss: 0.75778 | eval_custom_logloss: 0.92339 |  0:12:33s
epoch 25 | loss: 0.73227 | eval_custom_logloss: 0.85149 |  0:13:03s
epoch 26 | loss: 0.7397  | eval_custom_logloss: 0.66588 |  0:13:33s
epoch 27 | loss: 0.71817 | eval_custom_logloss: 0.70326 |  0:14:03s
epoch 28 | loss: 0.7187  | eval_custom_logloss: 0.89977 |  0:14:33s
epoch 29 | loss: 0.70998 | eval_custom_logloss: 1.11515 |  0:15:03s
epoch 30 | loss: 0.70817 | eval_custom_logloss: 0.68188 |  0:15:33s
epoch 31 | loss: 0.70933 | eval_custom_logloss: 0.87533 |  0:16:03s
epoch 32 | loss: 0.7719  | eval_custom_logloss: 1.5612  |  0:16:34s
epoch 33 | loss: 0.75119 | eval_custom_logloss: 0.80827 |  0:17:04s
epoch 34 | loss: 0.74627 | eval_custom_logloss: 0.94439 |  0:17:34s
epoch 35 | loss: 0.71816 | eval_custom_logloss: 0.82705 |  0:18:05s
epoch 36 | loss: 0.70807 | eval_custom_logloss: 0.8021  |  0:18:35s
epoch 37 | loss: 0.6965  | eval_custom_logloss: 0.97318 |  0:19:04s
epoch 38 | loss: 0.70692 | eval_custom_logloss: 0.92001 |  0:19:34s
epoch 39 | loss: 0.69595 | eval_custom_logloss: 0.74851 |  0:20:05s
epoch 40 | loss: 0.66733 | eval_custom_logloss: 0.9286  |  0:20:35s
epoch 41 | loss: 0.68108 | eval_custom_logloss: 0.84866 |  0:21:05s
epoch 42 | loss: 0.68199 | eval_custom_logloss: 0.79085 |  0:21:35s
epoch 43 | loss: 0.66522 | eval_custom_logloss: 1.0662  |  0:22:05s
epoch 44 | loss: 0.67006 | eval_custom_logloss: 0.94429 |  0:22:35s
epoch 45 | loss: 0.67506 | eval_custom_logloss: 0.68921 |  0:23:07s
epoch 46 | loss: 0.66629 | eval_custom_logloss: 1.37412 |  0:23:36s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 0.66588
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6648, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58421 | eval_custom_logloss: 12.30923|  0:00:30s
epoch 1  | loss: 1.08822 | eval_custom_logloss: 6.11691 |  0:01:00s
epoch 2  | loss: 1.01795 | eval_custom_logloss: 4.6482  |  0:01:29s
epoch 3  | loss: 0.98504 | eval_custom_logloss: 4.08632 |  0:01:58s
epoch 4  | loss: 0.948   | eval_custom_logloss: 1.87804 |  0:02:27s
epoch 5  | loss: 0.94884 | eval_custom_logloss: 3.40042 |  0:02:56s
epoch 6  | loss: 0.93646 | eval_custom_logloss: 1.58871 |  0:03:25s
epoch 7  | loss: 0.91483 | eval_custom_logloss: 1.38692 |  0:03:55s
epoch 8  | loss: 0.9111  | eval_custom_logloss: 1.55012 |  0:04:24s
epoch 9  | loss: 0.89775 | eval_custom_logloss: 1.00927 |  0:04:53s
epoch 10 | loss: 0.88035 | eval_custom_logloss: 1.29234 |  0:05:22s
epoch 11 | loss: 0.8819  | eval_custom_logloss: 1.4577  |  0:05:51s
epoch 12 | loss: 0.8671  | eval_custom_logloss: 1.4314  |  0:06:21s
epoch 13 | loss: 0.82858 | eval_custom_logloss: 1.1846  |  0:06:51s
epoch 14 | loss: 0.83742 | eval_custom_logloss: 0.98177 |  0:07:21s
epoch 15 | loss: 0.83296 | eval_custom_logloss: 1.51714 |  0:07:51s
epoch 16 | loss: 0.81029 | eval_custom_logloss: 1.47314 |  0:08:21s
epoch 17 | loss: 0.80718 | eval_custom_logloss: 1.70324 |  0:08:51s
epoch 18 | loss: 0.75414 | eval_custom_logloss: 0.95616 |  0:09:21s
epoch 19 | loss: 0.74441 | eval_custom_logloss: 1.16278 |  0:09:51s
epoch 20 | loss: 0.74866 | eval_custom_logloss: 0.88511 |  0:10:21s
epoch 21 | loss: 0.74588 | eval_custom_logloss: 0.8319  |  0:10:51s
epoch 22 | loss: 0.74644 | eval_custom_logloss: 0.73759 |  0:11:22s
epoch 23 | loss: 0.73365 | eval_custom_logloss: 1.26192 |  0:11:51s
epoch 24 | loss: 0.72577 | eval_custom_logloss: 1.09929 |  0:12:21s
epoch 25 | loss: 0.72613 | eval_custom_logloss: 1.00807 |  0:12:51s
epoch 26 | loss: 0.70522 | eval_custom_logloss: 0.85993 |  0:13:21s
epoch 27 | loss: 0.70981 | eval_custom_logloss: 0.91438 |  0:13:51s
epoch 28 | loss: 0.69485 | eval_custom_logloss: 0.94793 |  0:14:21s
epoch 29 | loss: 0.70289 | eval_custom_logloss: 1.03701 |  0:14:52s
epoch 30 | loss: 0.6992  | eval_custom_logloss: 1.38228 |  0:15:21s
epoch 31 | loss: 0.69506 | eval_custom_logloss: 0.69698 |  0:15:51s
epoch 32 | loss: 0.66818 | eval_custom_logloss: 1.55165 |  0:16:21s
epoch 33 | loss: 0.6872  | eval_custom_logloss: 0.90942 |  0:16:51s
epoch 34 | loss: 0.67996 | eval_custom_logloss: 0.85103 |  0:17:21s
epoch 35 | loss: 0.67736 | eval_custom_logloss: 0.96366 |  0:17:50s
epoch 36 | loss: 0.68949 | eval_custom_logloss: 1.51103 |  0:18:20s
epoch 37 | loss: 0.67687 | eval_custom_logloss: 0.94358 |  0:18:50s
epoch 38 | loss: 0.66755 | eval_custom_logloss: 0.7552  |  0:19:20s
epoch 39 | loss: 0.66119 | eval_custom_logloss: 0.82586 |  0:19:50s
epoch 40 | loss: 0.66503 | eval_custom_logloss: 0.85313 |  0:20:19s
epoch 41 | loss: 0.67157 | eval_custom_logloss: 0.68342 |  0:20:49s
epoch 42 | loss: 0.67557 | eval_custom_logloss: 0.71227 |  0:21:19s
epoch 43 | loss: 0.67085 | eval_custom_logloss: 0.86037 |  0:21:49s
epoch 44 | loss: 0.65565 | eval_custom_logloss: 0.87029 |  0:22:19s
epoch 45 | loss: 0.65555 | eval_custom_logloss: 1.15153 |  0:22:48s
epoch 46 | loss: 0.6493  | eval_custom_logloss: 1.00734 |  0:23:18s
epoch 47 | loss: 0.65954 | eval_custom_logloss: 1.50162 |  0:23:48s
epoch 48 | loss: 0.65323 | eval_custom_logloss: 0.78551 |  0:24:18s
epoch 49 | loss: 0.65217 | eval_custom_logloss: 0.86945 |  0:24:48s
epoch 50 | loss: 0.65076 | eval_custom_logloss: 0.85748 |  0:25:18s
epoch 51 | loss: 0.64206 | eval_custom_logloss: 0.7138  |  0:25:48s
epoch 52 | loss: 0.64762 | eval_custom_logloss: 1.09133 |  0:26:18s
epoch 53 | loss: 0.63632 | eval_custom_logloss: 0.74371 |  0:26:47s
epoch 54 | loss: 0.64107 | eval_custom_logloss: 0.98327 |  0:27:17s
epoch 55 | loss: 0.66066 | eval_custom_logloss: 0.64224 |  0:27:47s
epoch 56 | loss: 0.65251 | eval_custom_logloss: 0.69688 |  0:28:17s
epoch 57 | loss: 0.6414  | eval_custom_logloss: 0.81834 |  0:28:47s
epoch 58 | loss: 0.63592 | eval_custom_logloss: 0.99205 |  0:29:17s
epoch 59 | loss: 0.63401 | eval_custom_logloss: 1.45582 |  0:29:47s
epoch 60 | loss: 0.63268 | eval_custom_logloss: 0.75791 |  0:30:17s
epoch 61 | loss: 0.63639 | eval_custom_logloss: 0.8992  |  0:30:47s
epoch 62 | loss: 0.64235 | eval_custom_logloss: 0.74368 |  0:31:17s
epoch 63 | loss: 0.63507 | eval_custom_logloss: 0.7358  |  0:31:48s
epoch 64 | loss: 0.62462 | eval_custom_logloss: 0.61152 |  0:32:17s
epoch 65 | loss: 0.62114 | eval_custom_logloss: 0.92829 |  0:32:47s
epoch 66 | loss: 0.62277 | eval_custom_logloss: 1.02698 |  0:33:17s
epoch 67 | loss: 0.6305  | eval_custom_logloss: 0.77116 |  0:33:47s
epoch 68 | loss: 0.62817 | eval_custom_logloss: 0.84899 |  0:34:17s
epoch 69 | loss: 0.63839 | eval_custom_logloss: 0.84566 |  0:34:46s
epoch 70 | loss: 0.65292 | eval_custom_logloss: 0.70631 |  0:35:16s
epoch 71 | loss: 0.62978 | eval_custom_logloss: 0.79346 |  0:35:46s
epoch 72 | loss: 0.64057 | eval_custom_logloss: 0.85607 |  0:36:16s
epoch 73 | loss: 0.6344  | eval_custom_logloss: 0.71975 |  0:36:45s
epoch 74 | loss: 0.62213 | eval_custom_logloss: 0.62842 |  0:37:15s
epoch 75 | loss: 0.62701 | eval_custom_logloss: 0.663   |  0:37:45s
epoch 76 | loss: 0.61119 | eval_custom_logloss: 0.72027 |  0:38:15s
epoch 77 | loss: 0.6196  | eval_custom_logloss: 0.86966 |  0:38:44s
epoch 78 | loss: 0.62301 | eval_custom_logloss: 1.22603 |  0:39:14s
epoch 79 | loss: 0.62165 | eval_custom_logloss: 0.76697 |  0:39:44s
epoch 80 | loss: 0.61518 | eval_custom_logloss: 0.82941 |  0:40:13s
epoch 81 | loss: 0.61207 | eval_custom_logloss: 0.85315 |  0:40:43s
epoch 82 | loss: 0.62529 | eval_custom_logloss: 0.91258 |  0:41:13s
epoch 83 | loss: 0.6285  | eval_custom_logloss: 0.73115 |  0:41:43s
epoch 84 | loss: 0.60542 | eval_custom_logloss: 0.7552  |  0:42:12s

Early stopping occurred at epoch 84 with best_epoch = 64 and best_eval_custom_logloss = 0.61152
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.63775, 'Log Loss - std': 0.027049999999999963} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.60945 | eval_custom_logloss: 11.32882|  0:00:29s
epoch 1  | loss: 1.15215 | eval_custom_logloss: 6.0218  |  0:00:59s
epoch 2  | loss: 1.01268 | eval_custom_logloss: 3.70655 |  0:01:29s
epoch 3  | loss: 0.97673 | eval_custom_logloss: 2.28974 |  0:01:58s
epoch 4  | loss: 0.96752 | eval_custom_logloss: 2.0728  |  0:02:28s
epoch 5  | loss: 0.9482  | eval_custom_logloss: 1.76213 |  0:02:58s
epoch 6  | loss: 0.91589 | eval_custom_logloss: 1.11656 |  0:03:27s
epoch 7  | loss: 0.90584 | eval_custom_logloss: 1.25405 |  0:03:57s
epoch 8  | loss: 0.87831 | eval_custom_logloss: 1.29812 |  0:04:27s
epoch 9  | loss: 0.88327 | eval_custom_logloss: 1.36273 |  0:04:56s
epoch 10 | loss: 0.84999 | eval_custom_logloss: 1.27793 |  0:05:25s
epoch 11 | loss: 0.86242 | eval_custom_logloss: 0.95931 |  0:05:55s
epoch 12 | loss: 0.86351 | eval_custom_logloss: 0.89149 |  0:06:24s
epoch 13 | loss: 0.83004 | eval_custom_logloss: 0.99321 |  0:06:53s
epoch 14 | loss: 0.81004 | eval_custom_logloss: 1.28342 |  0:07:22s
epoch 15 | loss: 0.81951 | eval_custom_logloss: 1.58741 |  0:07:51s
epoch 16 | loss: 0.79658 | eval_custom_logloss: 0.91387 |  0:08:20s
epoch 17 | loss: 0.78802 | eval_custom_logloss: 0.77557 |  0:08:49s
epoch 18 | loss: 0.75916 | eval_custom_logloss: 0.83325 |  0:09:18s
epoch 19 | loss: 0.73709 | eval_custom_logloss: 1.23718 |  0:09:48s
epoch 20 | loss: 0.75123 | eval_custom_logloss: 1.06584 |  0:10:18s
epoch 21 | loss: 0.73307 | eval_custom_logloss: 1.04879 |  0:10:48s
epoch 22 | loss: 0.72309 | eval_custom_logloss: 0.81359 |  0:11:17s
epoch 23 | loss: 0.73705 | eval_custom_logloss: 1.60765 |  0:11:47s
epoch 24 | loss: 0.72929 | eval_custom_logloss: 0.86401 |  0:12:17s
epoch 25 | loss: 0.71509 | eval_custom_logloss: 0.84181 |  0:12:47s
epoch 26 | loss: 0.68808 | eval_custom_logloss: 1.0767  |  0:13:17s
epoch 27 | loss: 0.70813 | eval_custom_logloss: 0.93773 |  0:13:47s
epoch 28 | loss: 0.70094 | eval_custom_logloss: 1.15732 |  0:14:17s
epoch 29 | loss: 0.69307 | eval_custom_logloss: 1.0036  |  0:14:46s
epoch 30 | loss: 0.71227 | eval_custom_logloss: 0.94524 |  0:15:16s
epoch 31 | loss: 0.71645 | eval_custom_logloss: 1.75752 |  0:15:46s
epoch 32 | loss: 0.71393 | eval_custom_logloss: 1.1245  |  0:16:16s
epoch 33 | loss: 0.70753 | eval_custom_logloss: 1.81148 |  0:16:45s
epoch 34 | loss: 0.70962 | eval_custom_logloss: 0.80836 |  0:17:15s
epoch 35 | loss: 0.70768 | eval_custom_logloss: 0.83104 |  0:17:45s
epoch 36 | loss: 0.70148 | eval_custom_logloss: 0.91474 |  0:18:15s
epoch 37 | loss: 0.70674 | eval_custom_logloss: 0.7411  |  0:18:45s
epoch 38 | loss: 0.69795 | eval_custom_logloss: 0.80109 |  0:19:15s
epoch 39 | loss: 0.67728 | eval_custom_logloss: 0.88069 |  0:19:45s
epoch 40 | loss: 0.67403 | eval_custom_logloss: 0.94039 |  0:20:15s
epoch 41 | loss: 0.68668 | eval_custom_logloss: 0.95351 |  0:20:45s
epoch 42 | loss: 0.68563 | eval_custom_logloss: 1.16259 |  0:21:14s
epoch 43 | loss: 0.6848  | eval_custom_logloss: 1.01807 |  0:21:44s
epoch 44 | loss: 0.68465 | eval_custom_logloss: 0.8256  |  0:22:14s
epoch 45 | loss: 0.68781 | eval_custom_logloss: 0.96988 |  0:22:44s
epoch 46 | loss: 0.67616 | eval_custom_logloss: 0.98419 |  0:23:14s
epoch 47 | loss: 0.68803 | eval_custom_logloss: 0.89444 |  0:23:44s
epoch 48 | loss: 0.70033 | eval_custom_logloss: 1.08023 |  0:24:14s
epoch 49 | loss: 0.70643 | eval_custom_logloss: 2.32464 |  0:24:44s
epoch 50 | loss: 0.69291 | eval_custom_logloss: 0.77356 |  0:25:13s
epoch 51 | loss: 0.68378 | eval_custom_logloss: 1.64116 |  0:25:43s
epoch 52 | loss: 0.67018 | eval_custom_logloss: 0.79055 |  0:26:13s
epoch 53 | loss: 0.67113 | eval_custom_logloss: 0.87266 |  0:26:43s
epoch 54 | loss: 0.67839 | eval_custom_logloss: 1.0508  |  0:27:12s
epoch 55 | loss: 0.67596 | eval_custom_logloss: 0.79815 |  0:27:42s
epoch 56 | loss: 0.67747 | eval_custom_logloss: 0.914   |  0:28:11s
epoch 57 | loss: 0.66406 | eval_custom_logloss: 0.63807 |  0:28:40s
epoch 58 | loss: 0.66116 | eval_custom_logloss: 1.04253 |  0:29:09s
epoch 59 | loss: 0.65627 | eval_custom_logloss: 0.97694 |  0:29:38s
epoch 60 | loss: 0.65692 | eval_custom_logloss: 0.89553 |  0:30:07s
epoch 61 | loss: 0.65198 | eval_custom_logloss: 1.21146 |  0:30:36s
epoch 62 | loss: 0.65039 | eval_custom_logloss: 1.06775 |  0:31:05s
epoch 63 | loss: 0.64851 | eval_custom_logloss: 1.09453 |  0:31:35s
epoch 64 | loss: 0.65863 | eval_custom_logloss: 1.33542 |  0:32:03s
epoch 65 | loss: 0.65655 | eval_custom_logloss: 1.06456 |  0:32:32s
epoch 66 | loss: 0.68234 | eval_custom_logloss: 0.93198 |  0:33:02s
epoch 67 | loss: 0.68194 | eval_custom_logloss: 1.05115 |  0:33:30s
epoch 68 | loss: 0.65643 | eval_custom_logloss: 0.71404 |  0:34:00s
epoch 69 | loss: 0.65514 | eval_custom_logloss: 1.23836 |  0:34:30s
epoch 70 | loss: 0.66077 | eval_custom_logloss: 0.70905 |  0:34:59s
epoch 71 | loss: 0.67377 | eval_custom_logloss: 1.03496 |  0:35:29s
epoch 72 | loss: 0.65795 | eval_custom_logloss: 1.35754 |  0:35:59s
epoch 73 | loss: 0.65901 | eval_custom_logloss: 0.82237 |  0:36:28s
epoch 74 | loss: 0.66223 | eval_custom_logloss: 0.89877 |  0:36:58s
epoch 75 | loss: 0.66297 | eval_custom_logloss: 2.66493 |  0:37:28s
epoch 76 | loss: 0.6825  | eval_custom_logloss: 0.95103 |  0:37:57s
epoch 77 | loss: 0.67983 | eval_custom_logloss: 1.02193 |  0:38:27s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.63807
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6374000000000001, 'Log Loss - std': 0.022091778259494307} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56195 | eval_custom_logloss: 9.02466 |  0:00:29s
epoch 1  | loss: 1.12307 | eval_custom_logloss: 5.1816  |  0:00:58s
epoch 2  | loss: 1.00986 | eval_custom_logloss: 2.66323 |  0:01:27s
epoch 3  | loss: 0.96402 | eval_custom_logloss: 1.91315 |  0:01:56s
epoch 4  | loss: 0.95281 | eval_custom_logloss: 4.38181 |  0:02:25s
epoch 5  | loss: 0.92506 | eval_custom_logloss: 1.86357 |  0:02:54s
epoch 6  | loss: 0.90594 | eval_custom_logloss: 2.14624 |  0:03:23s
epoch 7  | loss: 0.88438 | eval_custom_logloss: 1.96646 |  0:03:52s
epoch 8  | loss: 0.87375 | eval_custom_logloss: 1.63082 |  0:04:21s
epoch 9  | loss: 0.86079 | eval_custom_logloss: 1.20479 |  0:04:51s
epoch 10 | loss: 0.83515 | eval_custom_logloss: 2.68162 |  0:05:20s
epoch 11 | loss: 0.84264 | eval_custom_logloss: 2.10555 |  0:05:50s
epoch 12 | loss: 0.81695 | eval_custom_logloss: 2.18014 |  0:06:18s
epoch 13 | loss: 0.81195 | eval_custom_logloss: 1.78038 |  0:06:47s
epoch 14 | loss: 0.80805 | eval_custom_logloss: 1.56624 |  0:07:16s
epoch 15 | loss: 0.78871 | eval_custom_logloss: 1.37082 |  0:07:45s
epoch 16 | loss: 0.77721 | eval_custom_logloss: 1.60643 |  0:08:13s
epoch 17 | loss: 0.80533 | eval_custom_logloss: 1.0445  |  0:08:43s
epoch 18 | loss: 0.77761 | eval_custom_logloss: 1.69634 |  0:09:11s
epoch 19 | loss: 0.78692 | eval_custom_logloss: 1.12151 |  0:09:40s
epoch 20 | loss: 0.81267 | eval_custom_logloss: 1.25489 |  0:10:09s
epoch 21 | loss: 0.79206 | eval_custom_logloss: 1.27792 |  0:10:38s
epoch 22 | loss: 0.76555 | eval_custom_logloss: 1.25717 |  0:11:06s
epoch 23 | loss: 0.75837 | eval_custom_logloss: 0.92824 |  0:11:35s
epoch 24 | loss: 0.76449 | eval_custom_logloss: 0.8978  |  0:12:05s
epoch 25 | loss: 0.75708 | eval_custom_logloss: 1.12711 |  0:12:33s
epoch 26 | loss: 0.74189 | eval_custom_logloss: 1.01384 |  0:13:02s
epoch 27 | loss: 0.73869 | eval_custom_logloss: 0.81616 |  0:13:30s
epoch 28 | loss: 0.73077 | eval_custom_logloss: 0.87407 |  0:13:58s
epoch 29 | loss: 0.72216 | eval_custom_logloss: 0.9479  |  0:14:26s
epoch 30 | loss: 0.7463  | eval_custom_logloss: 0.92023 |  0:14:54s
epoch 31 | loss: 0.73313 | eval_custom_logloss: 1.45532 |  0:15:22s
epoch 32 | loss: 0.72651 | eval_custom_logloss: 1.30205 |  0:15:50s
epoch 33 | loss: 0.72418 | eval_custom_logloss: 1.23962 |  0:16:18s
epoch 34 | loss: 0.72095 | eval_custom_logloss: 1.4211  |  0:16:46s
epoch 35 | loss: 0.71416 | eval_custom_logloss: 1.07614 |  0:17:15s
epoch 36 | loss: 0.72576 | eval_custom_logloss: 0.85043 |  0:17:44s
epoch 37 | loss: 0.71766 | eval_custom_logloss: 1.27362 |  0:18:12s
epoch 38 | loss: 0.7042  | eval_custom_logloss: 0.86139 |  0:18:41s
epoch 39 | loss: 0.70687 | eval_custom_logloss: 0.94007 |  0:19:10s
epoch 40 | loss: 0.70166 | eval_custom_logloss: 0.9147  |  0:19:38s
epoch 41 | loss: 0.70171 | eval_custom_logloss: 0.95627 |  0:20:07s
epoch 42 | loss: 0.70527 | eval_custom_logloss: 1.09206 |  0:20:36s
epoch 43 | loss: 0.69893 | eval_custom_logloss: 0.79887 |  0:21:05s
epoch 44 | loss: 0.68078 | eval_custom_logloss: 1.04912 |  0:21:33s
epoch 45 | loss: 0.69513 | eval_custom_logloss: 0.96718 |  0:22:02s
epoch 46 | loss: 0.69569 | eval_custom_logloss: 1.25076 |  0:22:31s
epoch 47 | loss: 0.68239 | eval_custom_logloss: 0.98156 |  0:23:00s
epoch 48 | loss: 0.67867 | eval_custom_logloss: 0.703   |  0:23:29s
epoch 49 | loss: 0.68594 | eval_custom_logloss: 1.10382 |  0:23:58s
epoch 50 | loss: 0.68067 | eval_custom_logloss: 0.81269 |  0:24:27s
epoch 51 | loss: 0.68499 | eval_custom_logloss: 0.83523 |  0:24:55s
epoch 52 | loss: 0.66763 | eval_custom_logloss: 0.81696 |  0:25:24s
epoch 53 | loss: 0.66691 | eval_custom_logloss: 0.86506 |  0:25:53s
epoch 54 | loss: 0.67322 | eval_custom_logloss: 0.99294 |  0:26:22s
epoch 55 | loss: 0.67267 | eval_custom_logloss: 0.78419 |  0:26:51s
epoch 56 | loss: 0.66912 | eval_custom_logloss: 0.68551 |  0:27:21s
epoch 57 | loss: 0.65911 | eval_custom_logloss: 1.14496 |  0:27:50s
epoch 58 | loss: 0.67028 | eval_custom_logloss: 0.94456 |  0:28:20s
epoch 59 | loss: 0.67242 | eval_custom_logloss: 0.76404 |  0:28:50s
epoch 60 | loss: 0.67527 | eval_custom_logloss: 0.69175 |  0:29:19s
epoch 61 | loss: 0.66132 | eval_custom_logloss: 1.04684 |  0:29:49s
epoch 62 | loss: 0.67744 | eval_custom_logloss: 0.73637 |  0:30:18s
epoch 63 | loss: 0.65899 | eval_custom_logloss: 1.09022 |  0:30:47s
epoch 64 | loss: 0.65308 | eval_custom_logloss: 0.85919 |  0:31:16s
epoch 65 | loss: 0.65486 | eval_custom_logloss: 0.90806 |  0:31:46s
epoch 66 | loss: 0.68244 | eval_custom_logloss: 0.79573 |  0:32:16s
epoch 67 | loss: 0.65769 | eval_custom_logloss: 0.72809 |  0:32:45s
epoch 68 | loss: 0.66041 | eval_custom_logloss: 0.92177 |  0:33:15s
epoch 69 | loss: 0.65571 | eval_custom_logloss: 1.25981 |  0:33:44s
epoch 70 | loss: 0.66382 | eval_custom_logloss: 0.77354 |  0:34:14s
epoch 71 | loss: 0.65786 | eval_custom_logloss: 1.87792 |  0:34:44s
epoch 72 | loss: 0.65162 | eval_custom_logloss: 1.2804  |  0:35:13s
epoch 73 | loss: 0.66475 | eval_custom_logloss: 0.85619 |  0:35:43s
epoch 74 | loss: 0.66197 | eval_custom_logloss: 0.76991 |  0:36:13s
epoch 75 | loss: 0.65949 | eval_custom_logloss: 0.80717 |  0:36:43s
epoch 76 | loss: 0.66817 | eval_custom_logloss: 0.85514 |  0:37:12s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.68551
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6493500000000001, 'Log Loss - std': 0.02818585638223539} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax', 'n_a': 26, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58421 | eval_custom_logloss: 11.88433|  0:00:29s
epoch 1  | loss: 1.13756 | eval_custom_logloss: 6.44357 |  0:00:59s
epoch 2  | loss: 1.026   | eval_custom_logloss: 7.08611 |  0:01:29s
epoch 3  | loss: 0.99802 | eval_custom_logloss: 4.32968 |  0:01:58s
epoch 4  | loss: 0.97177 | eval_custom_logloss: 3.17256 |  0:02:28s
epoch 5  | loss: 0.94321 | eval_custom_logloss: 2.50154 |  0:02:58s
epoch 6  | loss: 0.90956 | eval_custom_logloss: 2.6798  |  0:03:27s
epoch 7  | loss: 0.88928 | eval_custom_logloss: 4.00399 |  0:03:57s
epoch 8  | loss: 0.85975 | eval_custom_logloss: 1.55013 |  0:04:27s
epoch 9  | loss: 0.8665  | eval_custom_logloss: 1.8465  |  0:04:57s
epoch 10 | loss: 0.83127 | eval_custom_logloss: 1.19347 |  0:05:26s
epoch 11 | loss: 0.84206 | eval_custom_logloss: 1.2408  |  0:05:55s
epoch 12 | loss: 0.81197 | eval_custom_logloss: 0.82785 |  0:06:25s
epoch 13 | loss: 0.78881 | eval_custom_logloss: 1.63844 |  0:06:55s
epoch 14 | loss: 0.78557 | eval_custom_logloss: 1.25378 |  0:07:25s
epoch 15 | loss: 0.77309 | eval_custom_logloss: 2.74245 |  0:07:55s
epoch 16 | loss: 0.76182 | eval_custom_logloss: 1.22705 |  0:08:24s
epoch 17 | loss: 0.77584 | eval_custom_logloss: 0.99271 |  0:08:54s
epoch 18 | loss: 0.75551 | eval_custom_logloss: 2.18434 |  0:09:24s
epoch 19 | loss: 0.73855 | eval_custom_logloss: 1.29305 |  0:09:54s
epoch 20 | loss: 0.72585 | eval_custom_logloss: 1.55169 |  0:10:24s
epoch 21 | loss: 0.73287 | eval_custom_logloss: 1.03747 |  0:10:53s
epoch 22 | loss: 0.73563 | eval_custom_logloss: 1.07246 |  0:11:23s
epoch 23 | loss: 0.73353 | eval_custom_logloss: 1.56777 |  0:11:53s
epoch 24 | loss: 0.73242 | eval_custom_logloss: 2.07668 |  0:12:23s
epoch 25 | loss: 0.71991 | eval_custom_logloss: 0.99246 |  0:12:52s
epoch 26 | loss: 0.69679 | eval_custom_logloss: 0.97607 |  0:13:23s
epoch 27 | loss: 0.69887 | eval_custom_logloss: 1.05471 |  0:13:53s
epoch 28 | loss: 0.72353 | eval_custom_logloss: 1.12142 |  0:14:23s
epoch 29 | loss: 0.71152 | eval_custom_logloss: 1.01139 |  0:14:52s
epoch 30 | loss: 0.69405 | eval_custom_logloss: 1.76206 |  0:15:22s
epoch 31 | loss: 0.68568 | eval_custom_logloss: 1.05083 |  0:15:52s
epoch 32 | loss: 0.67916 | eval_custom_logloss: 0.94892 |  0:16:22s

Early stopping occurred at epoch 32 with best_epoch = 12 and best_eval_custom_logloss = 0.82785
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6846800000000001, 'Log Loss - std': 0.07502259393009547} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 0.6846800000000001 and parameters: {'n_d': 26, 'n_steps': 9, 'gamma': 1.1163142829780783, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.0029029074267611643, 'mask_type': 'entmax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.85874 | eval_custom_logloss: 12.35476|  0:00:32s
epoch 1  | loss: 1.33694 | eval_custom_logloss: 10.75286|  0:01:05s
epoch 2  | loss: 1.23873 | eval_custom_logloss: 9.19399 |  0:01:37s
epoch 3  | loss: 1.14893 | eval_custom_logloss: 9.21725 |  0:02:10s
epoch 4  | loss: 1.06724 | eval_custom_logloss: 6.89693 |  0:02:43s
epoch 5  | loss: 1.05076 | eval_custom_logloss: 6.57014 |  0:03:15s
epoch 6  | loss: 1.02827 | eval_custom_logloss: 4.33906 |  0:03:48s
epoch 7  | loss: 0.98838 | eval_custom_logloss: 4.35565 |  0:04:21s
epoch 8  | loss: 0.9972  | eval_custom_logloss: 2.49339 |  0:04:54s
epoch 9  | loss: 0.98652 | eval_custom_logloss: 1.74104 |  0:05:26s
epoch 10 | loss: 0.9614  | eval_custom_logloss: 1.64544 |  0:05:58s
epoch 11 | loss: 0.94317 | eval_custom_logloss: 1.37248 |  0:06:30s
epoch 12 | loss: 0.96909 | eval_custom_logloss: 1.97448 |  0:07:02s
epoch 13 | loss: 0.93854 | eval_custom_logloss: 1.72363 |  0:07:34s
epoch 14 | loss: 0.91371 | eval_custom_logloss: 1.97172 |  0:08:07s
epoch 15 | loss: 0.91787 | eval_custom_logloss: 4.86204 |  0:08:39s
epoch 16 | loss: 0.89392 | eval_custom_logloss: 1.80594 |  0:09:11s
epoch 17 | loss: 0.89555 | eval_custom_logloss: 1.76031 |  0:09:43s
epoch 18 | loss: 0.88878 | eval_custom_logloss: 1.57451 |  0:10:16s
epoch 19 | loss: 0.88015 | eval_custom_logloss: 1.81124 |  0:10:49s
epoch 20 | loss: 0.87306 | eval_custom_logloss: 2.23625 |  0:11:21s
epoch 21 | loss: 0.85763 | eval_custom_logloss: 2.06984 |  0:11:54s
epoch 22 | loss: 0.83176 | eval_custom_logloss: 1.96443 |  0:12:26s
epoch 23 | loss: 0.84208 | eval_custom_logloss: 1.77155 |  0:12:59s
epoch 24 | loss: 0.82547 | eval_custom_logloss: 1.13244 |  0:13:31s
epoch 25 | loss: 0.81878 | eval_custom_logloss: 1.11585 |  0:14:04s
epoch 26 | loss: 0.81442 | eval_custom_logloss: 1.1571  |  0:14:37s
epoch 27 | loss: 0.80637 | eval_custom_logloss: 1.04481 |  0:15:10s
epoch 28 | loss: 0.77362 | eval_custom_logloss: 1.0065  |  0:15:43s
epoch 29 | loss: 0.77788 | eval_custom_logloss: 1.40901 |  0:16:16s
epoch 30 | loss: 0.7675  | eval_custom_logloss: 2.20745 |  0:16:48s
epoch 31 | loss: 0.77914 | eval_custom_logloss: 1.14026 |  0:17:21s
epoch 32 | loss: 0.75589 | eval_custom_logloss: 1.17185 |  0:17:54s
epoch 33 | loss: 0.77164 | eval_custom_logloss: 1.37512 |  0:18:27s
epoch 34 | loss: 0.7846  | eval_custom_logloss: 2.9163  |  0:19:00s
epoch 35 | loss: 0.7879  | eval_custom_logloss: 1.57921 |  0:19:33s
epoch 36 | loss: 0.74724 | eval_custom_logloss: 1.36173 |  0:20:06s
epoch 37 | loss: 0.75299 | eval_custom_logloss: 1.92599 |  0:20:38s
epoch 38 | loss: 0.7401  | eval_custom_logloss: 1.99301 |  0:21:11s
epoch 39 | loss: 0.71689 | eval_custom_logloss: 0.97871 |  0:21:43s
epoch 40 | loss: 0.72156 | eval_custom_logloss: 0.83825 |  0:22:15s
epoch 41 | loss: 0.72986 | eval_custom_logloss: 1.10814 |  0:22:48s
epoch 42 | loss: 0.71802 | eval_custom_logloss: 1.25219 |  0:23:21s
epoch 43 | loss: 0.72858 | eval_custom_logloss: 1.43351 |  0:23:53s
epoch 44 | loss: 0.71967 | eval_custom_logloss: 1.34723 |  0:24:26s
epoch 45 | loss: 0.71054 | eval_custom_logloss: 1.26698 |  0:24:59s
epoch 46 | loss: 0.76837 | eval_custom_logloss: 2.59125 |  0:25:32s
epoch 47 | loss: 0.72961 | eval_custom_logloss: 1.05413 |  0:26:04s
epoch 48 | loss: 0.73266 | eval_custom_logloss: 1.06142 |  0:26:37s
epoch 49 | loss: 0.73432 | eval_custom_logloss: 0.887   |  0:27:10s
epoch 50 | loss: 0.72051 | eval_custom_logloss: 1.13706 |  0:27:42s
epoch 51 | loss: 0.71264 | eval_custom_logloss: 1.02389 |  0:28:15s
epoch 52 | loss: 0.70831 | eval_custom_logloss: 1.0212  |  0:28:47s
epoch 53 | loss: 0.69913 | eval_custom_logloss: 1.55174 |  0:29:20s
epoch 54 | loss: 0.70638 | eval_custom_logloss: 1.00949 |  0:29:52s
epoch 55 | loss: 0.72576 | eval_custom_logloss: 0.96125 |  0:30:25s
epoch 56 | loss: 0.70867 | eval_custom_logloss: 0.86701 |  0:30:58s
epoch 57 | loss: 0.73752 | eval_custom_logloss: 0.95796 |  0:31:30s
epoch 58 | loss: 0.74135 | eval_custom_logloss: 1.12643 |  0:32:03s
epoch 59 | loss: 0.73821 | eval_custom_logloss: 2.04057 |  0:32:35s
epoch 60 | loss: 0.72399 | eval_custom_logloss: 0.77312 |  0:33:08s
epoch 61 | loss: 0.72025 | eval_custom_logloss: 2.20983 |  0:33:40s
epoch 62 | loss: 0.71123 | eval_custom_logloss: 1.55614 |  0:34:13s
epoch 63 | loss: 0.70399 | eval_custom_logloss: 1.24518 |  0:34:45s
epoch 64 | loss: 0.70476 | eval_custom_logloss: 1.05834 |  0:35:18s
epoch 65 | loss: 0.7047  | eval_custom_logloss: 0.75089 |  0:35:51s
epoch 66 | loss: 0.69151 | eval_custom_logloss: 0.71503 |  0:36:24s
epoch 67 | loss: 0.69541 | eval_custom_logloss: 1.5136  |  0:36:56s
epoch 68 | loss: 0.68673 | eval_custom_logloss: 1.15654 |  0:37:30s
epoch 69 | loss: 0.69737 | eval_custom_logloss: 0.99571 |  0:38:02s
epoch 70 | loss: 0.68057 | eval_custom_logloss: 0.98448 |  0:38:36s
epoch 71 | loss: 0.67721 | eval_custom_logloss: 0.99705 |  0:39:09s
epoch 72 | loss: 0.69727 | eval_custom_logloss: 0.8859  |  0:39:42s
epoch 73 | loss: 0.69663 | eval_custom_logloss: 1.15027 |  0:40:16s
epoch 74 | loss: 0.68914 | eval_custom_logloss: 0.86439 |  0:40:49s
epoch 75 | loss: 0.67785 | eval_custom_logloss: 0.90961 |  0:41:23s
epoch 76 | loss: 0.6881  | eval_custom_logloss: 1.14516 |  0:41:56s
epoch 77 | loss: 0.68314 | eval_custom_logloss: 0.77543 |  0:42:29s
epoch 78 | loss: 0.68039 | eval_custom_logloss: 0.71087 |  0:43:02s
epoch 79 | loss: 0.6812  | eval_custom_logloss: 1.1078  |  0:43:35s
epoch 80 | loss: 0.67678 | eval_custom_logloss: 1.53758 |  0:44:08s
epoch 81 | loss: 0.69428 | eval_custom_logloss: 0.92486 |  0:44:41s
epoch 82 | loss: 0.68043 | eval_custom_logloss: 0.828   |  0:45:14s
epoch 83 | loss: 0.66718 | eval_custom_logloss: 1.27282 |  0:45:47s
epoch 84 | loss: 0.6928  | eval_custom_logloss: 0.80416 |  0:46:20s
epoch 85 | loss: 0.67021 | eval_custom_logloss: 0.95237 |  0:46:53s
epoch 86 | loss: 0.68064 | eval_custom_logloss: 1.01969 |  0:47:26s
epoch 87 | loss: 0.66737 | eval_custom_logloss: 0.87698 |  0:47:59s
epoch 88 | loss: 0.66082 | eval_custom_logloss: 0.76827 |  0:48:32s
epoch 89 | loss: 0.66284 | eval_custom_logloss: 1.16873 |  0:49:05s
epoch 90 | loss: 0.68046 | eval_custom_logloss: 1.38918 |  0:49:37s
epoch 91 | loss: 0.67579 | eval_custom_logloss: 2.30467 |  0:50:10s
epoch 92 | loss: 0.66582 | eval_custom_logloss: 1.00582 |  0:50:43s
epoch 93 | loss: 0.66426 | eval_custom_logloss: 2.70431 |  0:51:16s
epoch 94 | loss: 0.66466 | eval_custom_logloss: 0.88652 |  0:51:49s
epoch 95 | loss: 0.68267 | eval_custom_logloss: 2.32853 |  0:52:21s
epoch 96 | loss: 0.65319 | eval_custom_logloss: 1.46431 |  0:52:54s
epoch 97 | loss: 0.65654 | eval_custom_logloss: 3.18108 |  0:53:26s
epoch 98 | loss: 0.65669 | eval_custom_logloss: 1.09331 |  0:53:58s

Early stopping occurred at epoch 98 with best_epoch = 78 and best_eval_custom_logloss = 0.71087
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7087, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.8062  | eval_custom_logloss: 12.98782|  0:00:32s
epoch 1  | loss: 1.33787 | eval_custom_logloss: 9.83956 |  0:01:04s
epoch 2  | loss: 1.24905 | eval_custom_logloss: 9.01852 |  0:01:36s
epoch 3  | loss: 1.12358 | eval_custom_logloss: 7.48932 |  0:02:08s
epoch 4  | loss: 1.03632 | eval_custom_logloss: 5.31461 |  0:02:40s
epoch 5  | loss: 0.99718 | eval_custom_logloss: 6.5292  |  0:03:13s
epoch 6  | loss: 0.97844 | eval_custom_logloss: 3.56691 |  0:03:46s
epoch 7  | loss: 0.919   | eval_custom_logloss: 5.7583  |  0:04:19s
epoch 8  | loss: 0.93539 | eval_custom_logloss: 3.90646 |  0:04:51s
epoch 9  | loss: 0.91336 | eval_custom_logloss: 3.70287 |  0:05:24s
epoch 10 | loss: 0.89613 | eval_custom_logloss: 3.2317  |  0:05:57s
epoch 11 | loss: 0.89335 | eval_custom_logloss: 4.08006 |  0:06:29s
epoch 12 | loss: 0.89741 | eval_custom_logloss: 5.30394 |  0:07:02s
epoch 13 | loss: 0.89365 | eval_custom_logloss: 4.93676 |  0:07:35s
epoch 14 | loss: 0.87159 | eval_custom_logloss: 4.96511 |  0:08:07s
epoch 15 | loss: 0.87046 | eval_custom_logloss: 4.25634 |  0:08:40s
epoch 16 | loss: 0.83468 | eval_custom_logloss: 4.76529 |  0:09:13s
epoch 17 | loss: 0.80592 | eval_custom_logloss: 4.08765 |  0:09:45s
epoch 18 | loss: 0.8079  | eval_custom_logloss: 4.71284 |  0:10:18s
epoch 19 | loss: 0.79532 | eval_custom_logloss: 4.07864 |  0:10:51s
epoch 20 | loss: 0.77591 | eval_custom_logloss: 2.40668 |  0:11:23s
epoch 21 | loss: 0.77157 | eval_custom_logloss: 3.18176 |  0:11:56s
epoch 22 | loss: 0.7884  | eval_custom_logloss: 2.33501 |  0:12:29s
epoch 23 | loss: 0.76936 | eval_custom_logloss: 1.91198 |  0:13:02s
epoch 24 | loss: 0.76873 | eval_custom_logloss: 2.48183 |  0:13:33s
epoch 25 | loss: 0.74922 | eval_custom_logloss: 1.47573 |  0:14:06s
epoch 26 | loss: 0.75257 | eval_custom_logloss: 1.56608 |  0:14:38s
epoch 27 | loss: 0.75261 | eval_custom_logloss: 2.73891 |  0:15:11s
epoch 28 | loss: 0.74213 | eval_custom_logloss: 1.80808 |  0:15:43s
epoch 29 | loss: 0.74386 | eval_custom_logloss: 1.94633 |  0:16:15s
epoch 30 | loss: 0.7521  | eval_custom_logloss: 2.19105 |  0:16:48s
epoch 31 | loss: 0.71362 | eval_custom_logloss: 3.3804  |  0:17:20s
epoch 32 | loss: 0.7248  | eval_custom_logloss: 2.02361 |  0:17:52s
epoch 33 | loss: 0.71653 | eval_custom_logloss: 1.97634 |  0:18:24s
epoch 34 | loss: 0.72486 | eval_custom_logloss: 1.92387 |  0:18:57s
epoch 35 | loss: 0.7169  | eval_custom_logloss: 3.28064 |  0:19:29s
epoch 36 | loss: 0.72844 | eval_custom_logloss: 1.26176 |  0:20:01s
epoch 37 | loss: 0.71345 | eval_custom_logloss: 1.20059 |  0:20:34s
epoch 38 | loss: 0.71    | eval_custom_logloss: 1.98357 |  0:21:06s
epoch 39 | loss: 0.70085 | eval_custom_logloss: 3.41404 |  0:21:39s
epoch 40 | loss: 0.7057  | eval_custom_logloss: 4.04285 |  0:22:11s
epoch 41 | loss: 0.70749 | eval_custom_logloss: 2.87976 |  0:22:44s
epoch 42 | loss: 0.69074 | eval_custom_logloss: 1.53483 |  0:23:16s
epoch 43 | loss: 0.70745 | eval_custom_logloss: 1.63108 |  0:23:49s
epoch 44 | loss: 0.69851 | eval_custom_logloss: 1.83079 |  0:24:21s
epoch 45 | loss: 0.69294 | eval_custom_logloss: 1.71497 |  0:24:53s
epoch 46 | loss: 0.70032 | eval_custom_logloss: 1.09845 |  0:25:25s
epoch 47 | loss: 0.68353 | eval_custom_logloss: 2.65447 |  0:25:58s
epoch 48 | loss: 0.69458 | eval_custom_logloss: 1.73918 |  0:26:30s
epoch 49 | loss: 0.68019 | eval_custom_logloss: 1.50351 |  0:27:03s
epoch 50 | loss: 0.69018 | eval_custom_logloss: 1.30467 |  0:27:35s
epoch 51 | loss: 0.68073 | eval_custom_logloss: 1.86636 |  0:28:07s
epoch 52 | loss: 0.68741 | eval_custom_logloss: 1.35712 |  0:28:40s
epoch 53 | loss: 0.66999 | eval_custom_logloss: 1.21034 |  0:29:13s
epoch 54 | loss: 0.67375 | eval_custom_logloss: 1.1985  |  0:29:45s
epoch 55 | loss: 0.69424 | eval_custom_logloss: 1.12829 |  0:30:17s
epoch 56 | loss: 0.67095 | eval_custom_logloss: 2.13886 |  0:30:50s
epoch 57 | loss: 0.6671  | eval_custom_logloss: 2.66511 |  0:31:22s
epoch 58 | loss: 0.66483 | eval_custom_logloss: 1.93824 |  0:31:55s
epoch 59 | loss: 0.66957 | eval_custom_logloss: 2.08601 |  0:32:27s
epoch 60 | loss: 0.66627 | eval_custom_logloss: 1.22768 |  0:33:00s
epoch 61 | loss: 0.66007 | eval_custom_logloss: 1.38601 |  0:33:32s
epoch 62 | loss: 0.65981 | eval_custom_logloss: 1.47251 |  0:34:05s
epoch 63 | loss: 0.65418 | eval_custom_logloss: 1.10355 |  0:34:37s
epoch 64 | loss: 0.66867 | eval_custom_logloss: 2.10896 |  0:35:09s
epoch 65 | loss: 0.66095 | eval_custom_logloss: 1.61226 |  0:35:42s
epoch 66 | loss: 0.65416 | eval_custom_logloss: 1.19903 |  0:36:14s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 1.09845
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.90225, 'Log Loss - std': 0.19355000000000006} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.79764 | eval_custom_logloss: 12.20609|  0:00:32s
epoch 1  | loss: 1.32016 | eval_custom_logloss: 10.60669|  0:01:04s
epoch 2  | loss: 1.10973 | eval_custom_logloss: 9.82371 |  0:01:37s
epoch 3  | loss: 1.02673 | eval_custom_logloss: 7.14702 |  0:02:09s
epoch 4  | loss: 0.98954 | eval_custom_logloss: 3.36928 |  0:02:42s
epoch 5  | loss: 0.95851 | eval_custom_logloss: 5.14143 |  0:03:14s
epoch 6  | loss: 0.93269 | eval_custom_logloss: 5.03902 |  0:03:47s
epoch 7  | loss: 0.92012 | eval_custom_logloss: 3.66317 |  0:04:19s
epoch 8  | loss: 0.90038 | eval_custom_logloss: 2.2887  |  0:04:52s
epoch 9  | loss: 0.87644 | eval_custom_logloss: 4.10929 |  0:05:24s
epoch 10 | loss: 0.86177 | eval_custom_logloss: 3.6354  |  0:05:56s
epoch 11 | loss: 0.84122 | eval_custom_logloss: 3.20324 |  0:06:29s
epoch 12 | loss: 0.82972 | eval_custom_logloss: 3.67422 |  0:07:02s
epoch 13 | loss: 0.81506 | eval_custom_logloss: 2.9883  |  0:07:34s
epoch 14 | loss: 0.8016  | eval_custom_logloss: 3.16109 |  0:08:07s
epoch 15 | loss: 0.7802  | eval_custom_logloss: 1.99804 |  0:08:39s
epoch 16 | loss: 0.78619 | eval_custom_logloss: 2.98996 |  0:09:11s
epoch 17 | loss: 0.76852 | eval_custom_logloss: 2.55182 |  0:09:43s
epoch 18 | loss: 0.76885 | eval_custom_logloss: 2.23979 |  0:10:14s
epoch 19 | loss: 0.74743 | eval_custom_logloss: 1.90869 |  0:10:46s
epoch 20 | loss: 0.75476 | eval_custom_logloss: 1.68131 |  0:11:18s
epoch 21 | loss: 0.75355 | eval_custom_logloss: 1.67026 |  0:11:50s
epoch 22 | loss: 0.76134 | eval_custom_logloss: 1.41192 |  0:12:22s
epoch 23 | loss: 0.73827 | eval_custom_logloss: 1.72126 |  0:12:54s
epoch 24 | loss: 0.75559 | eval_custom_logloss: 2.2151  |  0:13:27s
epoch 25 | loss: 0.74475 | eval_custom_logloss: 1.06528 |  0:14:00s
epoch 26 | loss: 0.7507  | eval_custom_logloss: 1.69545 |  0:14:32s
epoch 27 | loss: 0.7357  | eval_custom_logloss: 1.35631 |  0:15:04s
epoch 28 | loss: 0.72849 | eval_custom_logloss: 1.35597 |  0:15:37s
epoch 29 | loss: 0.72747 | eval_custom_logloss: 1.18904 |  0:16:09s
epoch 30 | loss: 0.71464 | eval_custom_logloss: 1.44265 |  0:16:42s
epoch 31 | loss: 0.71634 | eval_custom_logloss: 1.65568 |  0:17:14s
epoch 32 | loss: 0.7318  | eval_custom_logloss: 1.21999 |  0:17:47s
epoch 33 | loss: 0.71587 | eval_custom_logloss: 1.15174 |  0:18:19s
epoch 34 | loss: 0.71038 | eval_custom_logloss: 1.41105 |  0:18:52s
epoch 35 | loss: 0.71238 | eval_custom_logloss: 1.16101 |  0:19:24s
epoch 36 | loss: 0.71584 | eval_custom_logloss: 0.93477 |  0:19:56s
epoch 37 | loss: 0.70777 | eval_custom_logloss: 1.28392 |  0:20:29s
epoch 38 | loss: 0.70086 | eval_custom_logloss: 1.08219 |  0:21:01s
epoch 39 | loss: 0.6935  | eval_custom_logloss: 1.38404 |  0:21:34s
epoch 40 | loss: 0.69126 | eval_custom_logloss: 0.99621 |  0:22:07s
epoch 41 | loss: 0.69035 | eval_custom_logloss: 1.22366 |  0:22:39s
epoch 42 | loss: 0.69238 | eval_custom_logloss: 2.03211 |  0:23:12s
epoch 43 | loss: 0.69785 | eval_custom_logloss: 1.04609 |  0:23:45s
epoch 44 | loss: 0.68299 | eval_custom_logloss: 0.9741  |  0:24:17s
epoch 45 | loss: 0.69419 | eval_custom_logloss: 0.96513 |  0:24:49s
epoch 46 | loss: 0.69098 | eval_custom_logloss: 1.08545 |  0:25:22s
epoch 47 | loss: 0.68676 | eval_custom_logloss: 1.13451 |  0:25:55s
epoch 48 | loss: 0.69915 | eval_custom_logloss: 0.91699 |  0:26:27s
epoch 49 | loss: 0.69349 | eval_custom_logloss: 1.40006 |  0:26:59s
epoch 50 | loss: 0.68739 | eval_custom_logloss: 1.41337 |  0:27:32s
epoch 51 | loss: 0.69649 | eval_custom_logloss: 1.15813 |  0:28:05s
epoch 52 | loss: 0.68142 | eval_custom_logloss: 1.15402 |  0:28:38s
epoch 53 | loss: 0.68074 | eval_custom_logloss: 0.74821 |  0:29:10s
epoch 54 | loss: 0.68799 | eval_custom_logloss: 0.85656 |  0:29:43s
epoch 55 | loss: 0.69082 | eval_custom_logloss: 0.77909 |  0:30:15s
epoch 56 | loss: 0.67897 | eval_custom_logloss: 0.82766 |  0:30:47s
epoch 57 | loss: 0.68449 | eval_custom_logloss: 0.68749 |  0:31:20s
epoch 58 | loss: 0.67721 | eval_custom_logloss: 1.10517 |  0:31:53s
epoch 59 | loss: 0.67995 | eval_custom_logloss: 1.67874 |  0:32:26s
epoch 60 | loss: 0.66809 | eval_custom_logloss: 1.31956 |  0:32:59s
epoch 61 | loss: 0.67078 | eval_custom_logloss: 0.82722 |  0:33:31s
epoch 62 | loss: 0.66213 | eval_custom_logloss: 0.71717 |  0:34:03s
epoch 63 | loss: 0.66604 | eval_custom_logloss: 0.65189 |  0:34:36s
epoch 64 | loss: 0.6732  | eval_custom_logloss: 0.81756 |  0:35:09s
epoch 65 | loss: 0.66402 | eval_custom_logloss: 0.97955 |  0:35:41s
epoch 66 | loss: 0.65308 | eval_custom_logloss: 1.17578 |  0:36:12s
epoch 67 | loss: 0.67594 | eval_custom_logloss: 0.86366 |  0:36:44s
epoch 68 | loss: 0.65941 | eval_custom_logloss: 1.17262 |  0:37:16s
epoch 69 | loss: 0.65995 | eval_custom_logloss: 0.69792 |  0:37:47s
epoch 70 | loss: 0.66248 | eval_custom_logloss: 1.22263 |  0:38:19s
epoch 71 | loss: 0.65503 | eval_custom_logloss: 1.32883 |  0:38:51s
epoch 72 | loss: 0.66003 | eval_custom_logloss: 0.81822 |  0:39:22s
epoch 73 | loss: 0.66091 | eval_custom_logloss: 0.90384 |  0:39:54s
epoch 74 | loss: 0.64672 | eval_custom_logloss: 0.78528 |  0:40:26s
epoch 75 | loss: 0.65183 | eval_custom_logloss: 1.16269 |  0:40:58s
epoch 76 | loss: 0.65084 | eval_custom_logloss: 1.19314 |  0:41:29s
epoch 77 | loss: 0.64716 | eval_custom_logloss: 0.90421 |  0:42:00s
epoch 78 | loss: 0.64891 | eval_custom_logloss: 1.00759 |  0:42:32s
epoch 79 | loss: 0.65593 | eval_custom_logloss: 0.75498 |  0:43:03s
epoch 80 | loss: 0.64837 | eval_custom_logloss: 0.90034 |  0:43:35s
epoch 81 | loss: 0.65143 | eval_custom_logloss: 0.93503 |  0:44:06s
epoch 82 | loss: 0.65216 | eval_custom_logloss: 0.71632 |  0:44:38s
epoch 83 | loss: 0.64866 | eval_custom_logloss: 0.71248 |  0:45:10s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 0.65189
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8180666666666667, 'Log Loss - std': 0.19785870940873163} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73411 | eval_custom_logloss: 12.35425|  0:00:31s
epoch 1  | loss: 1.20366 | eval_custom_logloss: 12.67657|  0:01:02s
epoch 2  | loss: 1.06336 | eval_custom_logloss: 12.52796|  0:01:34s
epoch 3  | loss: 1.02608 | eval_custom_logloss: 9.17483 |  0:02:05s
epoch 4  | loss: 0.97547 | eval_custom_logloss: 8.53432 |  0:02:37s
epoch 5  | loss: 0.9284  | eval_custom_logloss: 7.02021 |  0:03:09s
epoch 6  | loss: 0.92136 | eval_custom_logloss: 2.69086 |  0:03:41s
epoch 7  | loss: 0.90731 | eval_custom_logloss: 4.16413 |  0:04:12s
epoch 8  | loss: 0.91841 | eval_custom_logloss: 3.14441 |  0:04:44s
epoch 9  | loss: 0.88714 | eval_custom_logloss: 3.02557 |  0:05:15s
epoch 10 | loss: 0.88035 | eval_custom_logloss: 1.46454 |  0:05:47s
epoch 11 | loss: 0.88787 | eval_custom_logloss: 4.65886 |  0:06:19s
epoch 12 | loss: 0.881   | eval_custom_logloss: 3.55686 |  0:06:50s
epoch 13 | loss: 0.8603  | eval_custom_logloss: 3.67826 |  0:07:22s
epoch 14 | loss: 0.85837 | eval_custom_logloss: 2.18104 |  0:07:54s
epoch 15 | loss: 0.84253 | eval_custom_logloss: 1.53    |  0:08:25s
epoch 16 | loss: 0.8317  | eval_custom_logloss: 1.70592 |  0:08:57s
epoch 17 | loss: 0.81334 | eval_custom_logloss: 1.73484 |  0:09:27s
epoch 18 | loss: 0.79939 | eval_custom_logloss: 1.64252 |  0:09:58s
epoch 19 | loss: 0.7751  | eval_custom_logloss: 1.40174 |  0:10:29s
epoch 20 | loss: 0.78241 | eval_custom_logloss: 1.82205 |  0:11:00s
epoch 21 | loss: 0.76128 | eval_custom_logloss: 1.27184 |  0:11:31s
epoch 22 | loss: 0.76216 | eval_custom_logloss: 1.09478 |  0:12:02s
epoch 23 | loss: 0.76955 | eval_custom_logloss: 1.31785 |  0:12:30s
epoch 24 | loss: 0.76563 | eval_custom_logloss: 1.04375 |  0:12:59s
epoch 25 | loss: 0.75063 | eval_custom_logloss: 1.09546 |  0:13:28s
epoch 26 | loss: 0.75619 | eval_custom_logloss: 1.20316 |  0:13:57s
epoch 27 | loss: 0.73558 | eval_custom_logloss: 1.18583 |  0:14:26s
epoch 28 | loss: 0.74032 | eval_custom_logloss: 0.87596 |  0:14:54s
epoch 29 | loss: 0.73544 | eval_custom_logloss: 1.17278 |  0:15:22s
epoch 30 | loss: 0.72622 | eval_custom_logloss: 0.98071 |  0:15:50s
epoch 31 | loss: 0.70441 | eval_custom_logloss: 1.06911 |  0:16:18s
epoch 32 | loss: 0.71837 | eval_custom_logloss: 1.17129 |  0:16:46s
epoch 33 | loss: 0.71111 | eval_custom_logloss: 1.075   |  0:17:14s
epoch 34 | loss: 0.71297 | eval_custom_logloss: 0.98815 |  0:17:42s
epoch 35 | loss: 0.71713 | eval_custom_logloss: 0.72884 |  0:18:10s
epoch 36 | loss: 0.71746 | eval_custom_logloss: 1.07747 |  0:18:38s
epoch 37 | loss: 0.70135 | eval_custom_logloss: 0.83877 |  0:19:07s
epoch 38 | loss: 0.7166  | eval_custom_logloss: 1.10106 |  0:19:34s
epoch 39 | loss: 0.71051 | eval_custom_logloss: 1.73672 |  0:20:03s
epoch 40 | loss: 0.70112 | eval_custom_logloss: 1.04237 |  0:20:31s
epoch 41 | loss: 0.71021 | eval_custom_logloss: 0.85572 |  0:20:59s
epoch 42 | loss: 0.70069 | eval_custom_logloss: 0.73704 |  0:21:27s
epoch 43 | loss: 0.6942  | eval_custom_logloss: 0.93291 |  0:21:56s
epoch 44 | loss: 0.69253 | eval_custom_logloss: 1.13078 |  0:22:24s
epoch 45 | loss: 0.69501 | eval_custom_logloss: 0.87965 |  0:22:52s
epoch 46 | loss: 0.69618 | eval_custom_logloss: 0.82739 |  0:23:21s
epoch 47 | loss: 0.68721 | eval_custom_logloss: 0.88914 |  0:23:49s
epoch 48 | loss: 0.69389 | eval_custom_logloss: 0.67091 |  0:24:17s
epoch 49 | loss: 0.68478 | eval_custom_logloss: 1.09993 |  0:24:46s
epoch 50 | loss: 0.67721 | eval_custom_logloss: 1.12606 |  0:25:14s
epoch 51 | loss: 0.69801 | eval_custom_logloss: 0.78795 |  0:25:42s
epoch 52 | loss: 0.6832  | eval_custom_logloss: 0.92474 |  0:26:10s
epoch 53 | loss: 0.68176 | eval_custom_logloss: 0.82764 |  0:26:38s
epoch 54 | loss: 0.67552 | eval_custom_logloss: 0.88967 |  0:27:06s
epoch 55 | loss: 0.68719 | eval_custom_logloss: 0.84536 |  0:27:34s
epoch 56 | loss: 0.67957 | eval_custom_logloss: 0.8     |  0:28:02s
epoch 57 | loss: 0.67793 | eval_custom_logloss: 0.8058  |  0:28:31s
epoch 58 | loss: 0.67387 | eval_custom_logloss: 0.91165 |  0:28:59s
epoch 59 | loss: 0.67853 | eval_custom_logloss: 0.85149 |  0:29:27s
epoch 60 | loss: 0.67536 | eval_custom_logloss: 0.76197 |  0:29:55s
epoch 61 | loss: 0.67342 | eval_custom_logloss: 0.72967 |  0:30:23s
epoch 62 | loss: 0.69041 | eval_custom_logloss: 1.14971 |  0:30:51s
epoch 63 | loss: 0.67563 | eval_custom_logloss: 0.69756 |  0:31:20s
epoch 64 | loss: 0.68144 | eval_custom_logloss: 0.78203 |  0:31:48s
epoch 65 | loss: 0.6734  | eval_custom_logloss: 0.66514 |  0:32:16s
epoch 66 | loss: 0.66513 | eval_custom_logloss: 0.86264 |  0:32:44s
epoch 67 | loss: 0.68288 | eval_custom_logloss: 0.90649 |  0:33:13s
epoch 68 | loss: 0.6707  | eval_custom_logloss: 0.83569 |  0:33:41s
epoch 69 | loss: 0.67678 | eval_custom_logloss: 0.94289 |  0:34:09s
epoch 70 | loss: 0.67232 | eval_custom_logloss: 0.71617 |  0:34:38s
epoch 71 | loss: 0.66581 | eval_custom_logloss: 0.71996 |  0:35:06s
epoch 72 | loss: 0.67045 | eval_custom_logloss: 0.78534 |  0:35:35s
epoch 73 | loss: 0.67507 | eval_custom_logloss: 0.91437 |  0:36:03s
epoch 74 | loss: 0.67314 | eval_custom_logloss: 0.7797  |  0:36:31s
epoch 75 | loss: 0.66415 | eval_custom_logloss: 0.67116 |  0:36:59s
epoch 76 | loss: 0.6616  | eval_custom_logloss: 0.67305 |  0:37:27s
epoch 77 | loss: 0.66442 | eval_custom_logloss: 0.98052 |  0:37:56s
epoch 78 | loss: 0.65401 | eval_custom_logloss: 0.94603 |  0:38:24s
epoch 79 | loss: 0.66104 | eval_custom_logloss: 0.62222 |  0:38:53s
epoch 80 | loss: 0.66724 | eval_custom_logloss: 0.90448 |  0:39:20s
epoch 81 | loss: 0.65454 | eval_custom_logloss: 0.66283 |  0:39:49s
epoch 82 | loss: 0.6485  | eval_custom_logloss: 0.68336 |  0:40:17s
epoch 83 | loss: 0.65254 | eval_custom_logloss: 0.87059 |  0:40:45s
epoch 84 | loss: 0.6523  | eval_custom_logloss: 0.9055  |  0:41:12s
epoch 85 | loss: 0.64969 | eval_custom_logloss: 0.92842 |  0:41:40s
epoch 86 | loss: 0.66141 | eval_custom_logloss: 0.95573 |  0:42:09s
epoch 87 | loss: 0.65188 | eval_custom_logloss: 0.66463 |  0:42:36s
epoch 88 | loss: 0.64935 | eval_custom_logloss: 0.66401 |  0:43:05s
epoch 89 | loss: 0.6602  | eval_custom_logloss: 0.83934 |  0:43:33s
epoch 90 | loss: 0.6616  | eval_custom_logloss: 0.7078  |  0:44:01s
epoch 91 | loss: 0.66317 | eval_custom_logloss: 1.06383 |  0:44:29s
epoch 92 | loss: 0.66155 | eval_custom_logloss: 0.84044 |  0:44:58s
epoch 93 | loss: 0.64596 | eval_custom_logloss: 0.63468 |  0:45:26s
epoch 94 | loss: 0.64206 | eval_custom_logloss: 0.70883 |  0:45:54s
epoch 95 | loss: 0.65294 | eval_custom_logloss: 0.9205  |  0:46:22s
epoch 96 | loss: 0.63826 | eval_custom_logloss: 0.66093 |  0:46:50s
epoch 97 | loss: 0.64864 | eval_custom_logloss: 0.99912 |  0:47:18s
epoch 98 | loss: 0.64534 | eval_custom_logloss: 0.62008 |  0:47:46s
epoch 99 | loss: 0.64761 | eval_custom_logloss: 0.6952  |  0:48:14s
Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_eval_custom_logloss = 0.62008
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.768425, 'Log Loss - std': 0.1917131630196529} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71769 | eval_custom_logloss: 12.27438|  0:00:27s
epoch 1  | loss: 1.25499 | eval_custom_logloss: 11.33008|  0:00:55s
epoch 2  | loss: 1.13027 | eval_custom_logloss: 9.11839 |  0:01:22s
epoch 3  | loss: 1.05688 | eval_custom_logloss: 5.11698 |  0:01:50s
epoch 4  | loss: 1.01462 | eval_custom_logloss: 6.00274 |  0:02:18s
epoch 5  | loss: 0.99351 | eval_custom_logloss: 6.27606 |  0:02:46s
epoch 6  | loss: 1.01841 | eval_custom_logloss: 4.16386 |  0:03:13s
epoch 7  | loss: 1.00882 | eval_custom_logloss: 3.29469 |  0:03:41s
epoch 8  | loss: 0.97412 | eval_custom_logloss: 3.1792  |  0:04:08s
epoch 9  | loss: 0.95934 | eval_custom_logloss: 3.05764 |  0:04:36s
epoch 10 | loss: 0.92653 | eval_custom_logloss: 2.59346 |  0:05:04s
epoch 11 | loss: 0.91302 | eval_custom_logloss: 1.66368 |  0:05:32s
epoch 12 | loss: 0.89921 | eval_custom_logloss: 2.27935 |  0:06:00s
epoch 13 | loss: 0.9013  | eval_custom_logloss: 1.91778 |  0:06:29s
epoch 14 | loss: 0.88171 | eval_custom_logloss: 2.35147 |  0:06:57s
epoch 15 | loss: 0.8705  | eval_custom_logloss: 1.8756  |  0:07:25s
epoch 16 | loss: 0.84933 | eval_custom_logloss: 1.34692 |  0:07:53s
epoch 17 | loss: 0.84622 | eval_custom_logloss: 1.31636 |  0:08:21s
epoch 18 | loss: 0.85194 | eval_custom_logloss: 2.43871 |  0:08:49s
epoch 19 | loss: 0.82878 | eval_custom_logloss: 2.13221 |  0:09:17s
epoch 20 | loss: 0.83521 | eval_custom_logloss: 1.11694 |  0:09:45s
epoch 21 | loss: 0.79272 | eval_custom_logloss: 1.36066 |  0:10:13s
epoch 22 | loss: 0.79001 | eval_custom_logloss: 1.68901 |  0:10:41s
epoch 23 | loss: 0.77372 | eval_custom_logloss: 1.42524 |  0:11:10s
epoch 24 | loss: 0.76375 | eval_custom_logloss: 1.36706 |  0:11:38s
epoch 25 | loss: 0.75329 | eval_custom_logloss: 1.40129 |  0:12:06s
epoch 26 | loss: 0.76338 | eval_custom_logloss: 2.38132 |  0:12:34s
epoch 27 | loss: 0.76797 | eval_custom_logloss: 2.42256 |  0:13:02s
epoch 28 | loss: 0.74972 | eval_custom_logloss: 1.30122 |  0:13:31s
epoch 29 | loss: 0.74944 | eval_custom_logloss: 1.88724 |  0:13:58s
epoch 30 | loss: 0.73965 | eval_custom_logloss: 1.09302 |  0:14:27s
epoch 31 | loss: 0.72339 | eval_custom_logloss: 1.7864  |  0:14:56s
epoch 32 | loss: 0.73768 | eval_custom_logloss: 0.88215 |  0:15:25s
epoch 33 | loss: 0.72795 | eval_custom_logloss: 1.32843 |  0:15:53s
epoch 34 | loss: 0.71233 | eval_custom_logloss: 1.66081 |  0:16:21s
epoch 35 | loss: 0.72235 | eval_custom_logloss: 1.37427 |  0:16:49s
epoch 36 | loss: 0.71575 | eval_custom_logloss: 3.19701 |  0:17:17s
epoch 37 | loss: 0.71866 | eval_custom_logloss: 1.31163 |  0:17:44s
epoch 38 | loss: 0.7135  | eval_custom_logloss: 1.06582 |  0:18:12s
epoch 39 | loss: 0.69222 | eval_custom_logloss: 0.87225 |  0:18:40s
epoch 40 | loss: 0.70541 | eval_custom_logloss: 1.34138 |  0:19:08s
epoch 41 | loss: 0.69336 | eval_custom_logloss: 0.86279 |  0:19:36s
epoch 42 | loss: 0.69445 | eval_custom_logloss: 0.91573 |  0:20:04s
epoch 43 | loss: 0.68891 | eval_custom_logloss: 1.60301 |  0:20:32s
epoch 44 | loss: 0.69337 | eval_custom_logloss: 1.34421 |  0:21:00s
epoch 45 | loss: 0.68739 | eval_custom_logloss: 1.41151 |  0:21:28s
epoch 46 | loss: 0.6849  | eval_custom_logloss: 0.94898 |  0:21:56s
epoch 47 | loss: 0.68223 | eval_custom_logloss: 1.11515 |  0:22:24s
epoch 48 | loss: 0.68329 | eval_custom_logloss: 0.91906 |  0:22:52s
epoch 49 | loss: 0.66207 | eval_custom_logloss: 1.36821 |  0:23:20s
epoch 50 | loss: 0.67489 | eval_custom_logloss: 1.32417 |  0:23:48s
epoch 51 | loss: 0.66778 | eval_custom_logloss: 0.95166 |  0:24:15s
epoch 52 | loss: 0.67184 | eval_custom_logloss: 2.08246 |  0:24:43s
epoch 53 | loss: 0.66546 | eval_custom_logloss: 0.71879 |  0:25:11s
epoch 54 | loss: 0.65275 | eval_custom_logloss: 1.608   |  0:25:39s
epoch 55 | loss: 0.67049 | eval_custom_logloss: 1.01148 |  0:26:07s
epoch 56 | loss: 0.65335 | eval_custom_logloss: 1.33154 |  0:26:35s
epoch 57 | loss: 0.68607 | eval_custom_logloss: 1.11317 |  0:27:02s
epoch 58 | loss: 0.71233 | eval_custom_logloss: 1.61491 |  0:27:30s
epoch 59 | loss: 0.69707 | eval_custom_logloss: 1.25744 |  0:27:58s
epoch 60 | loss: 0.6786  | eval_custom_logloss: 2.09118 |  0:28:26s
epoch 61 | loss: 0.68676 | eval_custom_logloss: 0.99756 |  0:28:54s
epoch 62 | loss: 0.6673  | eval_custom_logloss: 1.22937 |  0:29:22s
epoch 63 | loss: 0.66072 | eval_custom_logloss: 0.88642 |  0:29:50s
epoch 64 | loss: 0.66888 | eval_custom_logloss: 1.84687 |  0:30:17s
epoch 65 | loss: 0.67534 | eval_custom_logloss: 1.2651  |  0:30:45s
epoch 66 | loss: 0.65159 | eval_custom_logloss: 1.1445  |  0:31:13s
epoch 67 | loss: 0.67073 | eval_custom_logloss: 1.67881 |  0:31:40s
epoch 68 | loss: 0.65151 | eval_custom_logloss: 0.90665 |  0:32:08s
epoch 69 | loss: 0.65348 | eval_custom_logloss: 0.93416 |  0:32:37s
epoch 70 | loss: 0.63705 | eval_custom_logloss: 0.79652 |  0:33:05s
epoch 71 | loss: 0.63818 | eval_custom_logloss: 0.97466 |  0:33:33s
epoch 72 | loss: 0.65229 | eval_custom_logloss: 2.11645 |  0:34:01s
epoch 73 | loss: 0.65581 | eval_custom_logloss: 0.80647 |  0:34:29s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.71879
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7581800000000001, 'Log Loss - std': 0.17269333976734602} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 0.7581800000000001 and parameters: {'n_d': 19, 'n_steps': 10, 'gamma': 1.3964610725585291, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0014337999108576462, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.76371 | eval_custom_logloss: 13.68268|  0:00:27s
epoch 1  | loss: 1.29666 | eval_custom_logloss: 11.42755|  0:00:54s
epoch 2  | loss: 1.16968 | eval_custom_logloss: 11.01078|  0:01:21s
epoch 3  | loss: 1.074   | eval_custom_logloss: 11.6167 |  0:01:49s
epoch 4  | loss: 1.02897 | eval_custom_logloss: 12.056  |  0:02:16s
epoch 5  | loss: 1.00021 | eval_custom_logloss: 9.56791 |  0:02:43s
epoch 6  | loss: 0.98265 | eval_custom_logloss: 8.73281 |  0:03:10s
epoch 7  | loss: 0.98521 | eval_custom_logloss: 8.67627 |  0:03:37s
epoch 8  | loss: 0.92115 | eval_custom_logloss: 6.63771 |  0:04:05s
epoch 9  | loss: 0.92157 | eval_custom_logloss: 8.17185 |  0:04:32s
epoch 10 | loss: 0.9495  | eval_custom_logloss: 2.99345 |  0:04:59s
epoch 11 | loss: 0.89311 | eval_custom_logloss: 4.64151 |  0:05:26s
epoch 12 | loss: 0.88229 | eval_custom_logloss: 4.46409 |  0:05:53s
epoch 13 | loss: 0.84853 | eval_custom_logloss: 4.74991 |  0:06:20s
epoch 14 | loss: 0.8618  | eval_custom_logloss: 4.46696 |  0:06:47s
epoch 15 | loss: 0.81706 | eval_custom_logloss: 6.77547 |  0:07:14s
epoch 16 | loss: 0.79887 | eval_custom_logloss: 4.55955 |  0:07:41s
epoch 17 | loss: 0.79531 | eval_custom_logloss: 3.41072 |  0:08:08s
epoch 18 | loss: 0.78996 | eval_custom_logloss: 4.07237 |  0:08:35s
epoch 19 | loss: 0.77878 | eval_custom_logloss: 5.3088  |  0:09:02s
epoch 20 | loss: 0.77829 | eval_custom_logloss: 3.5614  |  0:09:28s
epoch 21 | loss: 0.76645 | eval_custom_logloss: 2.14425 |  0:09:56s
epoch 22 | loss: 0.75104 | eval_custom_logloss: 3.13973 |  0:10:23s
epoch 23 | loss: 0.77953 | eval_custom_logloss: 3.39167 |  0:10:51s
epoch 24 | loss: 0.77546 | eval_custom_logloss: 5.46226 |  0:11:18s
epoch 25 | loss: 0.7572  | eval_custom_logloss: 1.66069 |  0:11:45s
epoch 26 | loss: 0.75712 | eval_custom_logloss: 1.8357  |  0:12:13s
epoch 27 | loss: 0.75334 | eval_custom_logloss: 3.75052 |  0:12:40s
epoch 28 | loss: 0.76663 | eval_custom_logloss: 6.8649  |  0:13:07s
epoch 29 | loss: 0.73289 | eval_custom_logloss: 2.41597 |  0:13:34s
epoch 30 | loss: 0.73259 | eval_custom_logloss: 3.67169 |  0:14:01s
epoch 31 | loss: 0.72631 | eval_custom_logloss: 1.9689  |  0:14:29s
epoch 32 | loss: 0.72767 | eval_custom_logloss: 3.79087 |  0:14:56s
epoch 33 | loss: 0.72541 | eval_custom_logloss: 3.23644 |  0:15:23s
epoch 34 | loss: 0.71037 | eval_custom_logloss: 1.72624 |  0:15:50s
epoch 35 | loss: 0.7157  | eval_custom_logloss: 4.97417 |  0:16:18s
epoch 36 | loss: 0.70602 | eval_custom_logloss: 2.44932 |  0:16:45s
epoch 37 | loss: 0.71075 | eval_custom_logloss: 2.93335 |  0:17:12s
epoch 38 | loss: 0.71187 | eval_custom_logloss: 3.92735 |  0:17:39s
epoch 39 | loss: 0.70535 | eval_custom_logloss: 1.71574 |  0:18:06s
epoch 40 | loss: 0.7044  | eval_custom_logloss: 2.06701 |  0:18:34s
epoch 41 | loss: 0.69338 | eval_custom_logloss: 3.50408 |  0:19:01s
epoch 42 | loss: 0.68715 | eval_custom_logloss: 1.53127 |  0:19:28s
epoch 43 | loss: 0.69651 | eval_custom_logloss: 1.89161 |  0:19:56s
epoch 44 | loss: 0.69565 | eval_custom_logloss: 1.81618 |  0:20:23s
epoch 45 | loss: 0.68854 | eval_custom_logloss: 1.41548 |  0:20:51s
epoch 46 | loss: 0.7039  | eval_custom_logloss: 1.79071 |  0:21:19s
epoch 47 | loss: 0.69395 | eval_custom_logloss: 2.20278 |  0:21:47s
epoch 48 | loss: 0.67787 | eval_custom_logloss: 1.438   |  0:22:14s
epoch 49 | loss: 0.68341 | eval_custom_logloss: 1.08068 |  0:22:42s
epoch 50 | loss: 0.68308 | eval_custom_logloss: 2.96791 |  0:23:10s
epoch 51 | loss: 0.68276 | eval_custom_logloss: 1.93907 |  0:23:37s
epoch 52 | loss: 0.67304 | eval_custom_logloss: 2.18337 |  0:24:05s
epoch 53 | loss: 0.68199 | eval_custom_logloss: 1.35254 |  0:24:33s
epoch 54 | loss: 0.671   | eval_custom_logloss: 3.44345 |  0:25:00s
epoch 55 | loss: 0.66167 | eval_custom_logloss: 3.67028 |  0:25:27s
epoch 56 | loss: 0.66615 | eval_custom_logloss: 1.92578 |  0:25:55s
epoch 57 | loss: 0.67233 | eval_custom_logloss: 1.07537 |  0:26:22s
epoch 58 | loss: 0.66303 | eval_custom_logloss: 1.24212 |  0:26:49s
epoch 59 | loss: 0.66706 | eval_custom_logloss: 0.90251 |  0:27:16s
epoch 60 | loss: 0.66786 | eval_custom_logloss: 1.89422 |  0:27:43s
epoch 61 | loss: 0.6554  | eval_custom_logloss: 1.00931 |  0:28:11s
epoch 62 | loss: 0.67622 | eval_custom_logloss: 1.46962 |  0:28:38s
epoch 63 | loss: 0.66298 | eval_custom_logloss: 1.36216 |  0:29:05s
epoch 64 | loss: 0.67409 | eval_custom_logloss: 0.91472 |  0:29:32s
epoch 65 | loss: 0.66082 | eval_custom_logloss: 0.90237 |  0:29:59s
epoch 66 | loss: 0.65761 | eval_custom_logloss: 0.85101 |  0:30:26s
epoch 67 | loss: 0.66944 | eval_custom_logloss: 5.60174 |  0:30:54s
epoch 68 | loss: 0.65301 | eval_custom_logloss: 2.60046 |  0:31:21s
epoch 69 | loss: 0.6543  | eval_custom_logloss: 5.32308 |  0:31:48s
epoch 70 | loss: 0.64848 | eval_custom_logloss: 3.33996 |  0:32:15s
epoch 71 | loss: 0.66262 | eval_custom_logloss: 2.71538 |  0:32:42s
epoch 72 | loss: 0.65547 | eval_custom_logloss: 1.88957 |  0:33:10s
epoch 73 | loss: 0.65172 | eval_custom_logloss: 2.95541 |  0:33:37s
epoch 74 | loss: 0.651   | eval_custom_logloss: 2.64628 |  0:34:04s
epoch 75 | loss: 0.64688 | eval_custom_logloss: 1.30436 |  0:34:31s
epoch 76 | loss: 0.64941 | eval_custom_logloss: 4.05395 |  0:34:58s
epoch 77 | loss: 0.65322 | eval_custom_logloss: 3.31495 |  0:35:25s
epoch 78 | loss: 0.64475 | eval_custom_logloss: 2.44254 |  0:35:53s
epoch 79 | loss: 0.6456  | eval_custom_logloss: 4.55084 |  0:36:20s
epoch 80 | loss: 0.65708 | eval_custom_logloss: 4.50056 |  0:36:47s
epoch 81 | loss: 0.66331 | eval_custom_logloss: 0.84697 |  0:37:15s
epoch 82 | loss: 0.6372  | eval_custom_logloss: 1.38741 |  0:37:42s
epoch 83 | loss: 0.63822 | eval_custom_logloss: 1.78024 |  0:38:09s
epoch 84 | loss: 0.64287 | eval_custom_logloss: 2.9701  |  0:38:37s
epoch 85 | loss: 0.65245 | eval_custom_logloss: 1.42359 |  0:39:04s
epoch 86 | loss: 0.6391  | eval_custom_logloss: 0.7998  |  0:39:31s
epoch 87 | loss: 0.65183 | eval_custom_logloss: 1.27039 |  0:39:58s
epoch 88 | loss: 0.63636 | eval_custom_logloss: 2.01119 |  0:40:25s
epoch 89 | loss: 0.65602 | eval_custom_logloss: 1.0115  |  0:40:53s
epoch 90 | loss: 0.6529  | eval_custom_logloss: 3.06725 |  0:41:20s
epoch 91 | loss: 0.64223 | eval_custom_logloss: 0.85689 |  0:41:48s
epoch 92 | loss: 0.6326  | eval_custom_logloss: 4.46549 |  0:42:15s
epoch 93 | loss: 0.64264 | eval_custom_logloss: 2.36499 |  0:42:42s
epoch 94 | loss: 0.63904 | eval_custom_logloss: 1.04015 |  0:43:10s
epoch 95 | loss: 0.63465 | eval_custom_logloss: 1.32472 |  0:43:37s
epoch 96 | loss: 0.67572 | eval_custom_logloss: 8.60458 |  0:44:04s
epoch 97 | loss: 0.64252 | eval_custom_logloss: 6.9274  |  0:44:32s
epoch 98 | loss: 0.64987 | eval_custom_logloss: 8.30701 |  0:44:59s
epoch 99 | loss: 0.65753 | eval_custom_logloss: 10.09501|  0:45:26s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.7998
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7982, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.80125 | eval_custom_logloss: 13.83855|  0:00:26s
epoch 1  | loss: 1.25761 | eval_custom_logloss: 12.74751|  0:00:53s
epoch 2  | loss: 1.14848 | eval_custom_logloss: 13.18903|  0:01:20s
epoch 3  | loss: 1.13242 | eval_custom_logloss: 10.84578|  0:01:47s
epoch 4  | loss: 1.06909 | eval_custom_logloss: 9.17499 |  0:02:13s
epoch 5  | loss: 1.04649 | eval_custom_logloss: 9.41179 |  0:02:41s
epoch 6  | loss: 0.96567 | eval_custom_logloss: 8.57026 |  0:03:08s
epoch 7  | loss: 0.95714 | eval_custom_logloss: 7.2587  |  0:03:34s
epoch 8  | loss: 0.90051 | eval_custom_logloss: 6.271   |  0:04:02s
epoch 9  | loss: 0.91021 | eval_custom_logloss: 6.08818 |  0:04:29s
epoch 10 | loss: 0.90092 | eval_custom_logloss: 5.14679 |  0:04:56s
epoch 11 | loss: 0.90284 | eval_custom_logloss: 6.29483 |  0:05:24s
epoch 12 | loss: 0.85591 | eval_custom_logloss: 5.5105  |  0:05:51s
epoch 13 | loss: 0.84819 | eval_custom_logloss: 5.6006  |  0:06:18s
epoch 14 | loss: 0.85295 | eval_custom_logloss: 5.65134 |  0:06:45s
epoch 15 | loss: 0.85072 | eval_custom_logloss: 3.78148 |  0:07:13s
epoch 16 | loss: 0.82366 | eval_custom_logloss: 3.0829  |  0:07:41s
epoch 17 | loss: 0.81108 | eval_custom_logloss: 2.74767 |  0:08:08s
epoch 18 | loss: 0.81661 | eval_custom_logloss: 2.88909 |  0:08:36s
epoch 19 | loss: 0.79841 | eval_custom_logloss: 2.59891 |  0:09:04s
epoch 20 | loss: 0.79558 | eval_custom_logloss: 2.41463 |  0:09:31s
epoch 21 | loss: 0.78425 | eval_custom_logloss: 2.62114 |  0:09:58s
epoch 22 | loss: 0.78636 | eval_custom_logloss: 2.92006 |  0:10:25s
epoch 23 | loss: 0.78741 | eval_custom_logloss: 2.00852 |  0:10:52s
epoch 24 | loss: 0.77683 | eval_custom_logloss: 1.70352 |  0:11:20s
epoch 25 | loss: 0.77323 | eval_custom_logloss: 2.27327 |  0:11:47s
epoch 26 | loss: 0.76986 | eval_custom_logloss: 2.60742 |  0:12:14s
epoch 27 | loss: 0.76705 | eval_custom_logloss: 1.70209 |  0:12:41s
epoch 28 | loss: 0.76712 | eval_custom_logloss: 1.51244 |  0:13:08s
epoch 29 | loss: 0.74771 | eval_custom_logloss: 1.57584 |  0:13:36s
epoch 30 | loss: 0.74236 | eval_custom_logloss: 1.43039 |  0:14:03s
epoch 31 | loss: 0.75624 | eval_custom_logloss: 1.17229 |  0:14:30s
epoch 32 | loss: 0.74639 | eval_custom_logloss: 1.50939 |  0:14:57s
epoch 33 | loss: 0.73835 | eval_custom_logloss: 1.82861 |  0:15:24s
epoch 34 | loss: 0.74272 | eval_custom_logloss: 1.05898 |  0:15:52s
epoch 35 | loss: 0.75706 | eval_custom_logloss: 1.21462 |  0:16:19s
epoch 36 | loss: 0.73696 | eval_custom_logloss: 0.98792 |  0:16:46s
epoch 37 | loss: 0.73878 | eval_custom_logloss: 1.04437 |  0:17:13s
epoch 38 | loss: 0.73801 | eval_custom_logloss: 1.06118 |  0:17:40s
epoch 39 | loss: 0.73229 | eval_custom_logloss: 2.11468 |  0:18:07s
epoch 40 | loss: 0.73593 | eval_custom_logloss: 1.02236 |  0:18:34s
epoch 41 | loss: 0.7356  | eval_custom_logloss: 1.14315 |  0:19:01s
epoch 42 | loss: 0.72344 | eval_custom_logloss: 1.12244 |  0:19:28s
epoch 43 | loss: 0.7258  | eval_custom_logloss: 0.78548 |  0:19:56s
epoch 44 | loss: 0.72987 | eval_custom_logloss: 1.08723 |  0:20:23s
epoch 45 | loss: 0.71514 | eval_custom_logloss: 0.79026 |  0:20:50s
epoch 46 | loss: 0.71028 | eval_custom_logloss: 1.06693 |  0:21:17s
epoch 47 | loss: 0.72351 | eval_custom_logloss: 0.9404  |  0:21:45s
epoch 48 | loss: 0.7027  | eval_custom_logloss: 1.10615 |  0:22:12s
epoch 49 | loss: 0.70231 | eval_custom_logloss: 1.20008 |  0:22:39s
epoch 50 | loss: 0.71681 | eval_custom_logloss: 1.12553 |  0:23:07s
epoch 51 | loss: 0.71522 | eval_custom_logloss: 0.96051 |  0:23:34s
epoch 52 | loss: 0.69863 | eval_custom_logloss: 1.97067 |  0:24:01s
epoch 53 | loss: 0.70977 | eval_custom_logloss: 0.93393 |  0:24:29s
epoch 54 | loss: 0.70206 | eval_custom_logloss: 1.15575 |  0:24:56s
epoch 55 | loss: 0.69749 | eval_custom_logloss: 1.44508 |  0:25:23s
epoch 56 | loss: 0.68012 | eval_custom_logloss: 0.97592 |  0:25:50s
epoch 57 | loss: 0.69319 | eval_custom_logloss: 1.1659  |  0:26:18s
epoch 58 | loss: 0.69    | eval_custom_logloss: 0.81113 |  0:26:45s
epoch 59 | loss: 0.70234 | eval_custom_logloss: 0.95041 |  0:27:12s
epoch 60 | loss: 0.69558 | eval_custom_logloss: 1.14981 |  0:27:39s
epoch 61 | loss: 0.6969  | eval_custom_logloss: 0.88669 |  0:28:07s
epoch 62 | loss: 0.7029  | eval_custom_logloss: 1.50853 |  0:28:34s
epoch 63 | loss: 0.70364 | eval_custom_logloss: 0.82157 |  0:29:02s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.78548
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.79105, 'Log Loss - std': 0.00714999999999999} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.86205 | eval_custom_logloss: 12.95425|  0:00:27s
epoch 1  | loss: 1.41046 | eval_custom_logloss: 9.77869 |  0:00:54s
epoch 2  | loss: 1.28479 | eval_custom_logloss: 8.96272 |  0:01:21s
epoch 3  | loss: 1.1493  | eval_custom_logloss: 10.27311|  0:01:49s
epoch 4  | loss: 1.01508 | eval_custom_logloss: 9.43759 |  0:02:16s
epoch 5  | loss: 1.00245 | eval_custom_logloss: 9.57756 |  0:02:43s
epoch 6  | loss: 0.95874 | eval_custom_logloss: 7.20387 |  0:03:10s
epoch 7  | loss: 0.94946 | eval_custom_logloss: 9.64851 |  0:03:38s
epoch 8  | loss: 0.91289 | eval_custom_logloss: 8.48988 |  0:04:05s
epoch 9  | loss: 0.88299 | eval_custom_logloss: 6.5766  |  0:04:32s
epoch 10 | loss: 0.88474 | eval_custom_logloss: 6.92318 |  0:04:59s
epoch 11 | loss: 0.87513 | eval_custom_logloss: 4.97867 |  0:05:27s
epoch 12 | loss: 0.85361 | eval_custom_logloss: 5.33303 |  0:05:54s
epoch 13 | loss: 0.83535 | eval_custom_logloss: 3.71659 |  0:06:21s
epoch 14 | loss: 0.85612 | eval_custom_logloss: 4.16574 |  0:06:48s
epoch 15 | loss: 0.83956 | eval_custom_logloss: 4.01156 |  0:07:16s
epoch 16 | loss: 0.8108  | eval_custom_logloss: 2.66935 |  0:07:43s
epoch 17 | loss: 0.80891 | eval_custom_logloss: 4.83439 |  0:08:10s
epoch 18 | loss: 0.81388 | eval_custom_logloss: 2.41078 |  0:08:37s
epoch 19 | loss: 0.79843 | eval_custom_logloss: 2.4686  |  0:09:05s
epoch 20 | loss: 0.79006 | eval_custom_logloss: 3.3456  |  0:09:32s
epoch 21 | loss: 0.78285 | eval_custom_logloss: 1.90135 |  0:09:59s
epoch 22 | loss: 0.79273 | eval_custom_logloss: 3.44478 |  0:10:26s
epoch 23 | loss: 0.77338 | eval_custom_logloss: 2.14267 |  0:10:52s
epoch 24 | loss: 0.78419 | eval_custom_logloss: 2.56431 |  0:11:19s
epoch 25 | loss: 0.7821  | eval_custom_logloss: 2.47275 |  0:11:46s
epoch 26 | loss: 0.75832 | eval_custom_logloss: 1.55098 |  0:12:13s
epoch 27 | loss: 0.76218 | eval_custom_logloss: 2.86881 |  0:12:40s
epoch 28 | loss: 0.75159 | eval_custom_logloss: 2.6135  |  0:13:06s
epoch 29 | loss: 0.75346 | eval_custom_logloss: 2.2093  |  0:13:33s
epoch 30 | loss: 0.74105 | eval_custom_logloss: 2.44287 |  0:14:00s
epoch 31 | loss: 0.72766 | eval_custom_logloss: 2.31334 |  0:14:27s
epoch 32 | loss: 0.73621 | eval_custom_logloss: 2.08183 |  0:14:55s
epoch 33 | loss: 0.73486 | eval_custom_logloss: 2.55644 |  0:15:22s
epoch 34 | loss: 0.72847 | eval_custom_logloss: 1.8855  |  0:15:49s
epoch 35 | loss: 0.72488 | eval_custom_logloss: 2.6565  |  0:16:16s
epoch 36 | loss: 0.71817 | eval_custom_logloss: 2.47359 |  0:16:43s
epoch 37 | loss: 0.7153  | eval_custom_logloss: 2.09552 |  0:17:11s
epoch 38 | loss: 0.71289 | eval_custom_logloss: 1.62536 |  0:17:38s
epoch 39 | loss: 0.71193 | eval_custom_logloss: 1.05121 |  0:18:05s
epoch 40 | loss: 0.7241  | eval_custom_logloss: 1.29041 |  0:18:32s
epoch 41 | loss: 0.71688 | eval_custom_logloss: 1.75097 |  0:18:59s
epoch 42 | loss: 0.70399 | eval_custom_logloss: 1.29141 |  0:19:27s
epoch 43 | loss: 0.72583 | eval_custom_logloss: 0.9065  |  0:19:54s
epoch 44 | loss: 0.71268 | eval_custom_logloss: 1.80197 |  0:20:21s
epoch 45 | loss: 0.70264 | eval_custom_logloss: 1.47825 |  0:20:48s
epoch 46 | loss: 0.69301 | eval_custom_logloss: 1.44002 |  0:21:16s
epoch 47 | loss: 0.7145  | eval_custom_logloss: 2.44048 |  0:21:43s
epoch 48 | loss: 0.68057 | eval_custom_logloss: 1.91437 |  0:22:10s
epoch 49 | loss: 0.70602 | eval_custom_logloss: 1.54644 |  0:22:37s
epoch 50 | loss: 0.69429 | eval_custom_logloss: 1.81799 |  0:23:05s
epoch 51 | loss: 0.68831 | eval_custom_logloss: 1.48673 |  0:23:32s
epoch 52 | loss: 0.69248 | eval_custom_logloss: 0.78438 |  0:23:59s
epoch 53 | loss: 0.69415 | eval_custom_logloss: 1.10132 |  0:24:26s
epoch 54 | loss: 0.67908 | eval_custom_logloss: 1.57607 |  0:24:54s
epoch 55 | loss: 0.68717 | eval_custom_logloss: 2.06278 |  0:25:21s
epoch 56 | loss: 0.66965 | eval_custom_logloss: 1.58679 |  0:25:48s
epoch 57 | loss: 0.68639 | eval_custom_logloss: 1.5698  |  0:26:15s
epoch 58 | loss: 0.66962 | eval_custom_logloss: 1.27111 |  0:26:43s
epoch 59 | loss: 0.6749  | eval_custom_logloss: 1.09343 |  0:27:10s
epoch 60 | loss: 0.68744 | eval_custom_logloss: 1.49989 |  0:27:37s
epoch 61 | loss: 0.74059 | eval_custom_logloss: 4.19207 |  0:28:04s
epoch 62 | loss: 0.75169 | eval_custom_logloss: 2.18283 |  0:28:31s
epoch 63 | loss: 0.72869 | eval_custom_logloss: 1.49044 |  0:28:59s
epoch 64 | loss: 0.75056 | eval_custom_logloss: 2.10099 |  0:29:26s
epoch 65 | loss: 0.71819 | eval_custom_logloss: 2.24131 |  0:29:53s
epoch 66 | loss: 0.70554 | eval_custom_logloss: 2.29824 |  0:30:20s
epoch 67 | loss: 0.75218 | eval_custom_logloss: 2.24297 |  0:30:47s
epoch 68 | loss: 0.75534 | eval_custom_logloss: 1.5589  |  0:31:15s
epoch 69 | loss: 0.71505 | eval_custom_logloss: 1.79783 |  0:31:42s
epoch 70 | loss: 0.7072  | eval_custom_logloss: 2.42785 |  0:32:09s
epoch 71 | loss: 0.70395 | eval_custom_logloss: 1.92574 |  0:32:36s
epoch 72 | loss: 0.67995 | eval_custom_logloss: 1.16105 |  0:33:04s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 0.78438
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7879, 'Log Loss - std': 0.007343477831836007} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.80309 | eval_custom_logloss: 11.31199|  0:00:27s
epoch 1  | loss: 1.37823 | eval_custom_logloss: 10.24359|  0:00:54s
epoch 2  | loss: 1.20492 | eval_custom_logloss: 11.40219|  0:01:21s
epoch 3  | loss: 1.12053 | eval_custom_logloss: 9.03566 |  0:01:49s
epoch 4  | loss: 1.04055 | eval_custom_logloss: 8.59496 |  0:02:16s
epoch 5  | loss: 1.02944 | eval_custom_logloss: 9.98578 |  0:02:43s
epoch 6  | loss: 0.98739 | eval_custom_logloss: 9.87884 |  0:03:10s
epoch 7  | loss: 0.93014 | eval_custom_logloss: 8.79321 |  0:03:37s
epoch 8  | loss: 0.90718 | eval_custom_logloss: 8.74527 |  0:04:04s
epoch 9  | loss: 0.90942 | eval_custom_logloss: 9.03949 |  0:04:32s
epoch 10 | loss: 0.88403 | eval_custom_logloss: 9.32963 |  0:04:59s
epoch 11 | loss: 0.88349 | eval_custom_logloss: 9.6573  |  0:05:26s
epoch 12 | loss: 0.84517 | eval_custom_logloss: 7.63466 |  0:05:53s
epoch 13 | loss: 0.85426 | eval_custom_logloss: 7.48453 |  0:06:21s
epoch 14 | loss: 0.86319 | eval_custom_logloss: 7.37915 |  0:06:48s
epoch 15 | loss: 0.83242 | eval_custom_logloss: 5.17068 |  0:07:15s
epoch 16 | loss: 0.82083 | eval_custom_logloss: 5.08253 |  0:07:43s
epoch 17 | loss: 0.81809 | eval_custom_logloss: 3.56442 |  0:08:10s
epoch 18 | loss: 0.80437 | eval_custom_logloss: 3.52236 |  0:08:37s
epoch 19 | loss: 0.79987 | eval_custom_logloss: 3.9105  |  0:09:05s
epoch 20 | loss: 0.80183 | eval_custom_logloss: 7.77201 |  0:09:32s
epoch 21 | loss: 0.80195 | eval_custom_logloss: 5.47429 |  0:09:59s
epoch 22 | loss: 0.79071 | eval_custom_logloss: 5.92191 |  0:10:27s
epoch 23 | loss: 0.79214 | eval_custom_logloss: 5.73213 |  0:10:54s
epoch 24 | loss: 0.78535 | eval_custom_logloss: 4.95237 |  0:11:21s
epoch 25 | loss: 0.78151 | eval_custom_logloss: 4.47676 |  0:11:49s
epoch 26 | loss: 0.78388 | eval_custom_logloss: 4.18804 |  0:12:16s
epoch 27 | loss: 0.77935 | eval_custom_logloss: 1.23319 |  0:12:44s
epoch 28 | loss: 0.77566 | eval_custom_logloss: 2.98263 |  0:13:11s
epoch 29 | loss: 0.78536 | eval_custom_logloss: 2.91192 |  0:13:39s
epoch 30 | loss: 0.77032 | eval_custom_logloss: 3.78354 |  0:14:06s
epoch 31 | loss: 0.76567 | eval_custom_logloss: 5.93035 |  0:14:33s
epoch 32 | loss: 0.75871 | eval_custom_logloss: 7.59929 |  0:15:00s
epoch 33 | loss: 0.75585 | eval_custom_logloss: 4.88405 |  0:15:28s
epoch 34 | loss: 0.76305 | eval_custom_logloss: 2.3608  |  0:15:55s
epoch 35 | loss: 0.75256 | eval_custom_logloss: 3.27688 |  0:16:22s
epoch 36 | loss: 0.74772 | eval_custom_logloss: 3.68435 |  0:16:49s
epoch 37 | loss: 0.76389 | eval_custom_logloss: 3.02141 |  0:17:16s
epoch 38 | loss: 0.74173 | eval_custom_logloss: 6.90655 |  0:17:43s
epoch 39 | loss: 0.75491 | eval_custom_logloss: 4.28015 |  0:18:10s
epoch 40 | loss: 0.76761 | eval_custom_logloss: 4.11108 |  0:18:36s
epoch 41 | loss: 0.76014 | eval_custom_logloss: 2.08586 |  0:19:03s
epoch 42 | loss: 0.74348 | eval_custom_logloss: 1.82901 |  0:19:30s
epoch 43 | loss: 0.74397 | eval_custom_logloss: 1.15984 |  0:19:57s
epoch 44 | loss: 0.75701 | eval_custom_logloss: 6.26559 |  0:20:24s
epoch 45 | loss: 0.77746 | eval_custom_logloss: 6.78629 |  0:20:51s
epoch 46 | loss: 0.75044 | eval_custom_logloss: 5.10249 |  0:21:19s
epoch 47 | loss: 0.75418 | eval_custom_logloss: 4.68642 |  0:21:46s
epoch 48 | loss: 0.75978 | eval_custom_logloss: 2.72175 |  0:22:13s
epoch 49 | loss: 0.74357 | eval_custom_logloss: 4.3139  |  0:22:41s
epoch 50 | loss: 0.74241 | eval_custom_logloss: 3.04411 |  0:23:08s
epoch 51 | loss: 0.74216 | eval_custom_logloss: 2.30857 |  0:23:35s
epoch 52 | loss: 0.74762 | eval_custom_logloss: 1.36575 |  0:24:02s
epoch 53 | loss: 0.74144 | eval_custom_logloss: 1.62122 |  0:24:29s
epoch 54 | loss: 0.73354 | eval_custom_logloss: 1.37883 |  0:24:56s
epoch 55 | loss: 0.73825 | eval_custom_logloss: 1.55772 |  0:25:23s
epoch 56 | loss: 0.72469 | eval_custom_logloss: 1.72252 |  0:25:50s
epoch 57 | loss: 0.73246 | eval_custom_logloss: 2.37737 |  0:26:18s
epoch 58 | loss: 0.72656 | eval_custom_logloss: 2.06129 |  0:26:45s
epoch 59 | loss: 0.72168 | eval_custom_logloss: 1.447   |  0:27:12s
epoch 60 | loss: 0.72671 | eval_custom_logloss: 1.57269 |  0:27:39s
epoch 61 | loss: 0.71884 | eval_custom_logloss: 1.37914 |  0:28:06s
epoch 62 | loss: 0.74457 | eval_custom_logloss: 2.54324 |  0:28:34s
epoch 63 | loss: 0.72542 | eval_custom_logloss: 1.63344 |  0:29:01s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 1.15984
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8772500000000001, 'Log Loss - std': 0.15488935567042686} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.76107 | eval_custom_logloss: 13.53956|  0:00:27s
epoch 1  | loss: 1.2812  | eval_custom_logloss: 12.88486|  0:00:55s
epoch 2  | loss: 1.18665 | eval_custom_logloss: 13.3424 |  0:01:22s
epoch 3  | loss: 1.11117 | eval_custom_logloss: 12.28917|  0:01:49s
epoch 4  | loss: 1.07503 | eval_custom_logloss: 11.06314|  0:02:16s
epoch 5  | loss: 1.06814 | eval_custom_logloss: 11.45674|  0:02:44s
epoch 6  | loss: 1.02471 | eval_custom_logloss: 8.70751 |  0:03:11s
epoch 7  | loss: 1.01785 | eval_custom_logloss: 9.00609 |  0:03:38s
epoch 8  | loss: 1.00629 | eval_custom_logloss: 6.44657 |  0:04:05s
epoch 9  | loss: 1.03222 | eval_custom_logloss: 8.78622 |  0:04:32s
epoch 10 | loss: 1.01103 | eval_custom_logloss: 8.77473 |  0:04:59s
epoch 11 | loss: 1.01314 | eval_custom_logloss: 8.41589 |  0:05:27s
epoch 12 | loss: 0.95297 | eval_custom_logloss: 7.90785 |  0:05:54s
epoch 13 | loss: 0.95762 | eval_custom_logloss: 7.72605 |  0:06:21s
epoch 14 | loss: 0.95459 | eval_custom_logloss: 6.85582 |  0:06:48s
epoch 15 | loss: 0.95272 | eval_custom_logloss: 6.96108 |  0:07:15s
epoch 16 | loss: 0.92489 | eval_custom_logloss: 6.50943 |  0:07:43s
epoch 17 | loss: 0.919   | eval_custom_logloss: 4.37155 |  0:08:10s
epoch 18 | loss: 0.91175 | eval_custom_logloss: 4.02573 |  0:08:37s
epoch 19 | loss: 0.91229 | eval_custom_logloss: 5.32026 |  0:09:04s
epoch 20 | loss: 0.8954  | eval_custom_logloss: 4.33307 |  0:09:31s
epoch 21 | loss: 0.88966 | eval_custom_logloss: 4.53108 |  0:09:58s
epoch 22 | loss: 0.88932 | eval_custom_logloss: 5.7935  |  0:10:26s
epoch 23 | loss: 0.869   | eval_custom_logloss: 5.82196 |  0:10:53s
epoch 24 | loss: 0.86819 | eval_custom_logloss: 5.38472 |  0:11:20s
epoch 25 | loss: 0.87415 | eval_custom_logloss: 6.01245 |  0:11:47s
epoch 26 | loss: 0.86302 | eval_custom_logloss: 5.84142 |  0:12:14s
epoch 27 | loss: 0.85832 | eval_custom_logloss: 3.89231 |  0:12:42s
epoch 28 | loss: 0.84331 | eval_custom_logloss: 2.51015 |  0:13:09s
epoch 29 | loss: 0.85846 | eval_custom_logloss: 2.98925 |  0:13:36s
epoch 30 | loss: 0.84143 | eval_custom_logloss: 3.64121 |  0:14:03s
epoch 31 | loss: 0.8368  | eval_custom_logloss: 3.14154 |  0:14:30s
epoch 32 | loss: 0.83396 | eval_custom_logloss: 2.39415 |  0:14:58s
epoch 33 | loss: 0.82075 | eval_custom_logloss: 3.62223 |  0:15:25s
epoch 34 | loss: 0.82545 | eval_custom_logloss: 4.2532  |  0:15:52s
epoch 35 | loss: 0.81731 | eval_custom_logloss: 4.30601 |  0:16:19s
epoch 36 | loss: 0.80802 | eval_custom_logloss: 4.54415 |  0:16:46s
epoch 37 | loss: 0.81077 | eval_custom_logloss: 4.09083 |  0:17:13s
epoch 38 | loss: 0.80552 | eval_custom_logloss: 3.77662 |  0:17:40s
epoch 39 | loss: 0.81303 | eval_custom_logloss: 2.33941 |  0:18:08s
epoch 40 | loss: 0.8129  | eval_custom_logloss: 1.75264 |  0:18:35s
epoch 41 | loss: 0.81333 | eval_custom_logloss: 1.45598 |  0:19:02s
epoch 42 | loss: 0.81    | eval_custom_logloss: 2.63961 |  0:19:29s
epoch 43 | loss: 0.79846 | eval_custom_logloss: 2.25131 |  0:19:56s
epoch 44 | loss: 0.79877 | eval_custom_logloss: 1.77631 |  0:20:24s
epoch 45 | loss: 0.7912  | eval_custom_logloss: 2.42537 |  0:20:51s
epoch 46 | loss: 0.79014 | eval_custom_logloss: 0.99873 |  0:21:18s
epoch 47 | loss: 0.805   | eval_custom_logloss: 2.6307  |  0:21:45s
epoch 48 | loss: 0.78851 | eval_custom_logloss: 1.69832 |  0:22:12s
epoch 49 | loss: 0.80096 | eval_custom_logloss: 2.17171 |  0:22:40s
epoch 50 | loss: 0.78662 | eval_custom_logloss: 1.19187 |  0:23:07s
epoch 51 | loss: 0.80241 | eval_custom_logloss: 1.70479 |  0:23:34s
epoch 52 | loss: 0.80304 | eval_custom_logloss: 1.84152 |  0:24:01s
epoch 53 | loss: 0.81251 | eval_custom_logloss: 1.40399 |  0:24:29s
epoch 54 | loss: 0.79728 | eval_custom_logloss: 1.63798 |  0:24:56s
epoch 55 | loss: 0.79461 | eval_custom_logloss: 2.04116 |  0:25:23s
epoch 56 | loss: 0.79447 | eval_custom_logloss: 1.28029 |  0:25:50s
epoch 57 | loss: 0.79946 | eval_custom_logloss: 1.19457 |  0:26:17s
epoch 58 | loss: 0.79334 | eval_custom_logloss: 1.90947 |  0:26:44s
epoch 59 | loss: 0.77008 | eval_custom_logloss: 2.44845 |  0:27:11s
epoch 60 | loss: 0.7641  | eval_custom_logloss: 0.92806 |  0:27:38s
epoch 61 | loss: 0.77025 | eval_custom_logloss: 2.97948 |  0:28:04s
epoch 62 | loss: 0.75068 | eval_custom_logloss: 1.29546 |  0:28:31s
epoch 63 | loss: 0.71567 | eval_custom_logloss: 1.05506 |  0:28:58s
epoch 64 | loss: 0.72024 | eval_custom_logloss: 1.39102 |  0:29:25s
epoch 65 | loss: 0.74169 | eval_custom_logloss: 1.9344  |  0:29:52s
epoch 66 | loss: 0.71262 | eval_custom_logloss: 1.2769  |  0:30:19s
epoch 67 | loss: 0.70588 | eval_custom_logloss: 2.00016 |  0:30:46s
epoch 68 | loss: 0.70306 | eval_custom_logloss: 0.93785 |  0:31:13s
epoch 69 | loss: 0.70927 | eval_custom_logloss: 1.80151 |  0:31:40s
epoch 70 | loss: 0.69571 | eval_custom_logloss: 3.35269 |  0:32:08s
epoch 71 | loss: 0.70317 | eval_custom_logloss: 1.08292 |  0:32:35s
epoch 72 | loss: 0.68861 | eval_custom_logloss: 1.92662 |  0:33:02s
epoch 73 | loss: 0.68284 | eval_custom_logloss: 0.76896 |  0:33:29s
epoch 74 | loss: 0.68895 | eval_custom_logloss: 1.82601 |  0:33:56s
epoch 75 | loss: 0.68724 | eval_custom_logloss: 1.59222 |  0:34:23s
epoch 76 | loss: 0.69074 | eval_custom_logloss: 1.62835 |  0:34:51s
epoch 77 | loss: 0.68869 | eval_custom_logloss: 2.21987 |  0:35:18s
epoch 78 | loss: 0.69041 | eval_custom_logloss: 2.26626 |  0:35:45s
epoch 79 | loss: 0.68038 | eval_custom_logloss: 0.74196 |  0:36:12s
epoch 80 | loss: 0.67734 | eval_custom_logloss: 0.79955 |  0:36:40s
epoch 81 | loss: 0.67918 | eval_custom_logloss: 0.7368  |  0:37:07s
epoch 82 | loss: 0.67248 | eval_custom_logloss: 1.01676 |  0:37:35s
epoch 83 | loss: 0.67395 | eval_custom_logloss: 1.12619 |  0:38:02s
epoch 84 | loss: 0.67691 | eval_custom_logloss: 0.90785 |  0:38:29s
epoch 85 | loss: 0.66999 | eval_custom_logloss: 0.76932 |  0:38:57s
epoch 86 | loss: 0.67453 | eval_custom_logloss: 1.91384 |  0:39:24s
epoch 87 | loss: 0.67592 | eval_custom_logloss: 1.05952 |  0:39:51s
epoch 88 | loss: 0.66428 | eval_custom_logloss: 0.95769 |  0:40:18s
epoch 89 | loss: 0.68328 | eval_custom_logloss: 0.98184 |  0:40:46s
epoch 90 | loss: 0.67447 | eval_custom_logloss: 1.21103 |  0:41:13s
epoch 91 | loss: 0.67476 | eval_custom_logloss: 0.75176 |  0:41:40s
epoch 92 | loss: 0.6612  | eval_custom_logloss: 1.1611  |  0:42:07s
epoch 93 | loss: 0.67525 | eval_custom_logloss: 0.91283 |  0:42:34s
epoch 94 | loss: 0.66276 | eval_custom_logloss: 1.01121 |  0:43:01s
epoch 95 | loss: 0.66885 | eval_custom_logloss: 0.85102 |  0:43:29s
epoch 96 | loss: 0.65959 | eval_custom_logloss: 0.69452 |  0:43:56s
epoch 97 | loss: 0.66502 | eval_custom_logloss: 0.76558 |  0:44:23s
epoch 98 | loss: 0.66121 | eval_custom_logloss: 0.95291 |  0:44:50s
epoch 99 | loss: 0.66818 | eval_custom_logloss: 1.79178 |  0:45:17s
Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_eval_custom_logloss = 0.69452
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8404200000000002, 'Log Loss - std': 0.15690240788464654} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 0.8404200000000002 and parameters: {'n_d': 13, 'n_steps': 8, 'gamma': 1.968150761157319, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0010617019128179748, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax', 'n_a': 35, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.63415 | eval_custom_logloss: 13.64429|  0:00:18s
epoch 1  | loss: 1.18144 | eval_custom_logloss: 8.80247 |  0:00:36s
epoch 2  | loss: 1.09495 | eval_custom_logloss: 11.65342|  0:00:55s
epoch 3  | loss: 1.02493 | eval_custom_logloss: 13.22319|  0:01:13s
epoch 4  | loss: 0.96849 | eval_custom_logloss: 11.44785|  0:01:31s
epoch 5  | loss: 0.92559 | eval_custom_logloss: 8.90964 |  0:01:49s
epoch 6  | loss: 0.92103 | eval_custom_logloss: 8.12721 |  0:02:07s
epoch 7  | loss: 0.88503 | eval_custom_logloss: 7.04534 |  0:02:25s
epoch 8  | loss: 0.85668 | eval_custom_logloss: 6.88162 |  0:02:43s
epoch 9  | loss: 0.84989 | eval_custom_logloss: 5.78407 |  0:03:01s
epoch 10 | loss: 0.81008 | eval_custom_logloss: 3.70565 |  0:03:19s
epoch 11 | loss: 0.7992  | eval_custom_logloss: 3.62076 |  0:03:37s
epoch 12 | loss: 0.78881 | eval_custom_logloss: 4.40746 |  0:03:55s
epoch 13 | loss: 0.7706  | eval_custom_logloss: 3.13049 |  0:04:14s
epoch 14 | loss: 0.75648 | eval_custom_logloss: 3.32844 |  0:04:32s
epoch 15 | loss: 0.75054 | eval_custom_logloss: 2.42129 |  0:04:50s
epoch 16 | loss: 0.74535 | eval_custom_logloss: 1.73145 |  0:05:08s
epoch 17 | loss: 0.74248 | eval_custom_logloss: 3.54407 |  0:05:26s
epoch 18 | loss: 0.72265 | eval_custom_logloss: 2.56091 |  0:05:44s
epoch 19 | loss: 0.71981 | eval_custom_logloss: 3.15167 |  0:06:02s
epoch 20 | loss: 0.72872 | eval_custom_logloss: 3.08612 |  0:06:20s
epoch 21 | loss: 0.70514 | eval_custom_logloss: 2.51993 |  0:06:38s
epoch 22 | loss: 0.70783 | eval_custom_logloss: 3.19729 |  0:06:56s
epoch 23 | loss: 0.72116 | eval_custom_logloss: 2.37121 |  0:07:14s
epoch 24 | loss: 0.7052  | eval_custom_logloss: 1.73432 |  0:07:32s
epoch 25 | loss: 0.71601 | eval_custom_logloss: 1.43774 |  0:07:50s
epoch 26 | loss: 0.71166 | eval_custom_logloss: 1.31023 |  0:08:08s
epoch 27 | loss: 0.71319 | eval_custom_logloss: 1.25214 |  0:08:26s
epoch 28 | loss: 0.70489 | eval_custom_logloss: 1.11475 |  0:08:44s
epoch 29 | loss: 0.68948 | eval_custom_logloss: 1.16197 |  0:09:02s
epoch 30 | loss: 0.69255 | eval_custom_logloss: 2.85941 |  0:09:21s
epoch 31 | loss: 0.68298 | eval_custom_logloss: 0.98406 |  0:09:39s
epoch 32 | loss: 0.66771 | eval_custom_logloss: 1.00528 |  0:09:57s
epoch 33 | loss: 0.67391 | eval_custom_logloss: 1.95024 |  0:10:15s
epoch 34 | loss: 0.67161 | eval_custom_logloss: 1.02635 |  0:10:33s
epoch 35 | loss: 0.68217 | eval_custom_logloss: 1.6935  |  0:10:51s
epoch 36 | loss: 0.66676 | eval_custom_logloss: 2.46684 |  0:11:09s
epoch 37 | loss: 0.68405 | eval_custom_logloss: 1.30447 |  0:11:27s
epoch 38 | loss: 0.67388 | eval_custom_logloss: 1.14204 |  0:11:45s
epoch 39 | loss: 0.66227 | eval_custom_logloss: 1.16457 |  0:12:04s
epoch 40 | loss: 0.65331 | eval_custom_logloss: 1.21903 |  0:12:22s
epoch 41 | loss: 0.66008 | eval_custom_logloss: 1.49377 |  0:12:40s
epoch 42 | loss: 0.67996 | eval_custom_logloss: 0.97736 |  0:12:58s
epoch 43 | loss: 0.64822 | eval_custom_logloss: 0.80989 |  0:13:16s
epoch 44 | loss: 0.65305 | eval_custom_logloss: 0.96    |  0:13:34s
epoch 45 | loss: 0.6565  | eval_custom_logloss: 0.99064 |  0:13:53s
epoch 46 | loss: 0.65463 | eval_custom_logloss: 1.10349 |  0:14:11s
epoch 47 | loss: 0.6409  | eval_custom_logloss: 0.96682 |  0:14:29s
epoch 48 | loss: 0.64751 | eval_custom_logloss: 1.24882 |  0:14:47s
epoch 49 | loss: 0.64868 | eval_custom_logloss: 1.12012 |  0:15:05s
epoch 50 | loss: 0.63441 | eval_custom_logloss: 0.96798 |  0:15:23s
epoch 51 | loss: 0.64853 | eval_custom_logloss: 1.27543 |  0:15:42s
epoch 52 | loss: 0.63542 | eval_custom_logloss: 0.96024 |  0:16:00s
epoch 53 | loss: 0.6385  | eval_custom_logloss: 1.19651 |  0:16:18s
epoch 54 | loss: 0.64029 | eval_custom_logloss: 0.78089 |  0:16:36s
epoch 55 | loss: 0.62439 | eval_custom_logloss: 0.94354 |  0:16:55s
epoch 56 | loss: 0.63659 | eval_custom_logloss: 1.06946 |  0:17:13s
epoch 57 | loss: 0.63118 | eval_custom_logloss: 1.09326 |  0:17:31s
epoch 58 | loss: 0.6209  | eval_custom_logloss: 1.16671 |  0:17:49s
epoch 59 | loss: 0.61745 | eval_custom_logloss: 0.75235 |  0:18:07s
epoch 60 | loss: 0.62942 | eval_custom_logloss: 1.14185 |  0:18:25s
epoch 61 | loss: 0.62686 | eval_custom_logloss: 1.24013 |  0:18:43s
epoch 62 | loss: 0.65014 | eval_custom_logloss: 1.05066 |  0:19:02s
epoch 63 | loss: 0.61781 | eval_custom_logloss: 0.69814 |  0:19:20s
epoch 64 | loss: 0.62298 | eval_custom_logloss: 0.78967 |  0:19:38s
epoch 65 | loss: 0.62982 | eval_custom_logloss: 0.93939 |  0:19:56s
epoch 66 | loss: 0.62655 | eval_custom_logloss: 1.26895 |  0:20:14s
epoch 67 | loss: 0.62348 | eval_custom_logloss: 0.82501 |  0:20:32s
epoch 68 | loss: 0.61067 | eval_custom_logloss: 1.13897 |  0:20:50s
epoch 69 | loss: 0.61662 | eval_custom_logloss: 1.04938 |  0:21:08s
epoch 70 | loss: 0.61763 | eval_custom_logloss: 1.94481 |  0:21:26s
epoch 71 | loss: 0.61349 | eval_custom_logloss: 1.87346 |  0:21:43s
epoch 72 | loss: 0.62565 | eval_custom_logloss: 0.77514 |  0:22:01s
epoch 73 | loss: 0.61645 | eval_custom_logloss: 0.7961  |  0:22:19s
epoch 74 | loss: 0.60982 | eval_custom_logloss: 1.8288  |  0:22:37s
epoch 75 | loss: 0.61307 | eval_custom_logloss: 1.22333 |  0:22:55s
epoch 76 | loss: 0.62365 | eval_custom_logloss: 0.79077 |  0:23:13s
epoch 77 | loss: 0.62045 | eval_custom_logloss: 0.76116 |  0:23:31s
epoch 78 | loss: 0.61278 | eval_custom_logloss: 0.76014 |  0:23:49s
epoch 79 | loss: 0.60907 | eval_custom_logloss: 0.79261 |  0:24:06s
epoch 80 | loss: 0.60036 | eval_custom_logloss: 0.80186 |  0:24:24s
epoch 81 | loss: 0.59427 | eval_custom_logloss: 0.67533 |  0:24:42s
epoch 82 | loss: 0.59653 | eval_custom_logloss: 0.6814  |  0:25:00s
epoch 83 | loss: 0.59733 | eval_custom_logloss: 0.63554 |  0:25:19s
epoch 84 | loss: 0.59906 | eval_custom_logloss: 0.68785 |  0:25:37s
epoch 85 | loss: 0.59999 | eval_custom_logloss: 1.61935 |  0:25:55s
epoch 86 | loss: 0.60045 | eval_custom_logloss: 1.03036 |  0:26:13s
epoch 87 | loss: 0.58875 | eval_custom_logloss: 1.28971 |  0:26:32s
epoch 88 | loss: 0.59977 | eval_custom_logloss: 0.68532 |  0:26:50s
epoch 89 | loss: 0.60514 | eval_custom_logloss: 0.78644 |  0:27:08s
epoch 90 | loss: 0.59328 | eval_custom_logloss: 0.73418 |  0:27:26s
epoch 91 | loss: 0.60372 | eval_custom_logloss: 1.71546 |  0:27:44s
epoch 92 | loss: 0.59102 | eval_custom_logloss: 0.89279 |  0:28:02s
epoch 93 | loss: 0.5791  | eval_custom_logloss: 0.68424 |  0:28:20s
epoch 94 | loss: 0.60268 | eval_custom_logloss: 1.24037 |  0:28:38s
epoch 95 | loss: 0.59018 | eval_custom_logloss: 0.73776 |  0:28:56s
epoch 96 | loss: 0.58663 | eval_custom_logloss: 0.96824 |  0:29:14s
epoch 97 | loss: 0.59138 | eval_custom_logloss: 0.9293  |  0:29:32s
epoch 98 | loss: 0.59135 | eval_custom_logloss: 1.4848  |  0:29:50s
epoch 99 | loss: 0.5873  | eval_custom_logloss: 0.88906 |  0:30:09s
Stop training because you reached max_epochs = 100 with best_epoch = 83 and best_eval_custom_logloss = 0.63554
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6334, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax', 'n_a': 35, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.57757 | eval_custom_logloss: 12.98318|  0:00:18s
epoch 1  | loss: 1.22202 | eval_custom_logloss: 12.36515|  0:00:36s
epoch 2  | loss: 1.14518 | eval_custom_logloss: 11.12769|  0:00:54s
epoch 3  | loss: 1.11833 | eval_custom_logloss: 10.35264|  0:01:12s
epoch 4  | loss: 1.08406 | eval_custom_logloss: 9.33047 |  0:01:30s
epoch 5  | loss: 1.01059 | eval_custom_logloss: 10.59384|  0:01:48s
epoch 6  | loss: 0.99734 | eval_custom_logloss: 8.61721 |  0:02:06s
epoch 7  | loss: 0.96753 | eval_custom_logloss: 5.81187 |  0:02:25s
epoch 8  | loss: 0.92663 | eval_custom_logloss: 7.76386 |  0:02:43s
epoch 9  | loss: 0.93923 | eval_custom_logloss: 6.2166  |  0:03:01s
epoch 10 | loss: 0.91364 | eval_custom_logloss: 6.13812 |  0:03:19s
epoch 11 | loss: 0.90702 | eval_custom_logloss: 4.42424 |  0:03:37s
epoch 12 | loss: 0.90043 | eval_custom_logloss: 4.02127 |  0:03:55s
epoch 13 | loss: 0.88323 | eval_custom_logloss: 5.30372 |  0:04:13s
epoch 14 | loss: 0.85532 | eval_custom_logloss: 3.8101  |  0:04:31s
epoch 15 | loss: 0.84146 | eval_custom_logloss: 4.47761 |  0:04:49s
epoch 16 | loss: 0.83383 | eval_custom_logloss: 3.79276 |  0:05:08s
epoch 17 | loss: 0.83725 | eval_custom_logloss: 5.46571 |  0:05:26s
epoch 18 | loss: 0.80157 | eval_custom_logloss: 3.15428 |  0:05:44s
epoch 19 | loss: 0.77786 | eval_custom_logloss: 2.31652 |  0:06:02s
epoch 20 | loss: 0.76591 | eval_custom_logloss: 2.11557 |  0:06:20s
epoch 21 | loss: 0.7676  | eval_custom_logloss: 1.87501 |  0:06:38s
epoch 22 | loss: 0.76435 | eval_custom_logloss: 1.9666  |  0:06:56s
epoch 23 | loss: 0.7495  | eval_custom_logloss: 1.99419 |  0:07:14s
epoch 24 | loss: 0.74118 | eval_custom_logloss: 1.85317 |  0:07:32s
epoch 25 | loss: 0.7248  | eval_custom_logloss: 1.29645 |  0:07:50s
epoch 26 | loss: 0.72476 | eval_custom_logloss: 1.75546 |  0:08:09s
epoch 27 | loss: 0.72367 | eval_custom_logloss: 1.40475 |  0:08:27s
epoch 28 | loss: 0.71286 | eval_custom_logloss: 1.13137 |  0:08:45s
epoch 29 | loss: 0.72658 | eval_custom_logloss: 3.9359  |  0:09:03s
epoch 30 | loss: 0.70679 | eval_custom_logloss: 2.28285 |  0:09:21s
epoch 31 | loss: 0.69761 | eval_custom_logloss: 1.64901 |  0:09:39s
epoch 32 | loss: 0.69643 | eval_custom_logloss: 1.42672 |  0:09:57s
epoch 33 | loss: 0.69808 | eval_custom_logloss: 2.21146 |  0:10:15s
epoch 34 | loss: 0.68237 | eval_custom_logloss: 1.07034 |  0:10:33s
epoch 35 | loss: 0.69316 | eval_custom_logloss: 1.0646  |  0:10:51s
epoch 36 | loss: 0.68027 | eval_custom_logloss: 1.71766 |  0:11:09s
epoch 37 | loss: 0.6868  | eval_custom_logloss: 1.21577 |  0:11:28s
epoch 38 | loss: 0.68798 | eval_custom_logloss: 1.60031 |  0:11:46s
epoch 39 | loss: 0.67133 | eval_custom_logloss: 1.18968 |  0:12:04s
epoch 40 | loss: 0.66259 | eval_custom_logloss: 1.64355 |  0:12:22s
epoch 41 | loss: 0.66824 | eval_custom_logloss: 1.29033 |  0:12:41s
epoch 42 | loss: 0.66586 | eval_custom_logloss: 1.18348 |  0:13:00s
epoch 43 | loss: 0.66321 | eval_custom_logloss: 1.12393 |  0:13:18s
epoch 44 | loss: 0.67069 | eval_custom_logloss: 2.15951 |  0:13:36s
epoch 45 | loss: 0.66312 | eval_custom_logloss: 1.00018 |  0:13:54s
epoch 46 | loss: 0.66293 | eval_custom_logloss: 1.00557 |  0:14:12s
epoch 47 | loss: 0.65682 | eval_custom_logloss: 1.16693 |  0:14:31s
epoch 48 | loss: 0.64966 | eval_custom_logloss: 0.90262 |  0:14:49s
epoch 49 | loss: 0.65482 | eval_custom_logloss: 1.09    |  0:15:07s
epoch 50 | loss: 0.6564  | eval_custom_logloss: 1.30942 |  0:15:25s
epoch 51 | loss: 0.66515 | eval_custom_logloss: 1.1722  |  0:15:43s
epoch 52 | loss: 0.65753 | eval_custom_logloss: 1.36991 |  0:16:01s
epoch 53 | loss: 0.64566 | eval_custom_logloss: 1.2534  |  0:16:19s
epoch 54 | loss: 0.64762 | eval_custom_logloss: 0.83161 |  0:16:37s
epoch 55 | loss: 0.64952 | eval_custom_logloss: 0.90887 |  0:16:56s
epoch 56 | loss: 0.64739 | eval_custom_logloss: 1.56693 |  0:17:14s
epoch 57 | loss: 0.64204 | eval_custom_logloss: 1.22311 |  0:17:31s
epoch 58 | loss: 0.64028 | eval_custom_logloss: 0.99835 |  0:17:49s
epoch 59 | loss: 0.64142 | eval_custom_logloss: 0.85677 |  0:18:07s
epoch 60 | loss: 0.63805 | eval_custom_logloss: 0.78847 |  0:18:26s
epoch 61 | loss: 0.63305 | eval_custom_logloss: 1.01072 |  0:18:44s
epoch 62 | loss: 0.65027 | eval_custom_logloss: 0.81428 |  0:19:02s
epoch 63 | loss: 0.62952 | eval_custom_logloss: 1.00734 |  0:19:21s
epoch 64 | loss: 0.63377 | eval_custom_logloss: 1.4073  |  0:19:39s
epoch 65 | loss: 0.62402 | eval_custom_logloss: 0.97345 |  0:19:57s
epoch 66 | loss: 0.63441 | eval_custom_logloss: 1.04215 |  0:20:15s
epoch 67 | loss: 0.64136 | eval_custom_logloss: 1.40596 |  0:20:33s
epoch 68 | loss: 0.63156 | eval_custom_logloss: 1.0273  |  0:20:52s
epoch 69 | loss: 0.63087 | eval_custom_logloss: 0.87587 |  0:21:11s
epoch 70 | loss: 0.62321 | eval_custom_logloss: 1.21915 |  0:21:29s
epoch 71 | loss: 0.62481 | eval_custom_logloss: 0.84577 |  0:21:47s
epoch 72 | loss: 0.61686 | eval_custom_logloss: 1.65044 |  0:22:05s
epoch 73 | loss: 0.62746 | eval_custom_logloss: 1.11526 |  0:22:24s
epoch 74 | loss: 0.62514 | eval_custom_logloss: 1.01845 |  0:22:42s
epoch 75 | loss: 0.62627 | eval_custom_logloss: 0.97217 |  0:23:00s
epoch 76 | loss: 0.6275  | eval_custom_logloss: 0.87586 |  0:23:18s
epoch 77 | loss: 0.62516 | eval_custom_logloss: 1.34118 |  0:23:37s
epoch 78 | loss: 0.63043 | eval_custom_logloss: 0.97438 |  0:23:56s
epoch 79 | loss: 0.61203 | eval_custom_logloss: 1.66835 |  0:24:14s
epoch 80 | loss: 0.62336 | eval_custom_logloss: 2.46215 |  0:24:32s

Early stopping occurred at epoch 80 with best_epoch = 60 and best_eval_custom_logloss = 0.78847
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7078, 'Log Loss - std': 0.07440000000000002} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax', 'n_a': 35, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.56911 | eval_custom_logloss: 13.5317 |  0:00:17s
epoch 1  | loss: 1.23533 | eval_custom_logloss: 12.20742|  0:00:36s
epoch 2  | loss: 1.0672  | eval_custom_logloss: 10.39192|  0:00:54s
epoch 3  | loss: 1.00976 | eval_custom_logloss: 10.37556|  0:01:12s
epoch 4  | loss: 0.98094 | eval_custom_logloss: 11.32674|  0:01:30s
epoch 5  | loss: 0.92045 | eval_custom_logloss: 7.35348 |  0:01:48s
epoch 6  | loss: 0.90475 | eval_custom_logloss: 5.12186 |  0:02:06s
epoch 7  | loss: 0.88885 | eval_custom_logloss: 4.24604 |  0:02:24s
epoch 8  | loss: 0.88887 | eval_custom_logloss: 4.79585 |  0:02:43s
epoch 9  | loss: 0.86979 | eval_custom_logloss: 5.59177 |  0:03:01s
epoch 10 | loss: 0.8456  | eval_custom_logloss: 4.62761 |  0:03:19s
epoch 11 | loss: 0.82393 | eval_custom_logloss: 3.75808 |  0:03:37s
epoch 12 | loss: 0.81755 | eval_custom_logloss: 3.53903 |  0:03:55s
epoch 13 | loss: 0.79195 | eval_custom_logloss: 2.32778 |  0:04:13s
epoch 14 | loss: 0.79114 | eval_custom_logloss: 2.53006 |  0:04:31s
epoch 15 | loss: 0.78316 | eval_custom_logloss: 2.56868 |  0:04:49s
epoch 16 | loss: 0.76463 | eval_custom_logloss: 1.75989 |  0:05:08s
epoch 17 | loss: 0.74644 | eval_custom_logloss: 1.87141 |  0:05:25s
epoch 18 | loss: 0.75232 | eval_custom_logloss: 1.90978 |  0:05:43s
epoch 19 | loss: 0.74522 | eval_custom_logloss: 1.56642 |  0:06:01s
epoch 20 | loss: 0.74941 | eval_custom_logloss: 1.75184 |  0:06:19s
epoch 21 | loss: 0.72363 | eval_custom_logloss: 1.41437 |  0:06:36s
epoch 22 | loss: 0.73063 | eval_custom_logloss: 1.43652 |  0:06:54s
epoch 23 | loss: 0.72876 | eval_custom_logloss: 1.45922 |  0:07:12s
epoch 24 | loss: 0.72004 | eval_custom_logloss: 1.64898 |  0:07:30s
epoch 25 | loss: 0.71924 | eval_custom_logloss: 1.4509  |  0:07:48s
epoch 26 | loss: 0.71295 | eval_custom_logloss: 1.70507 |  0:08:07s
epoch 27 | loss: 0.7035  | eval_custom_logloss: 1.31771 |  0:08:25s
epoch 28 | loss: 0.69896 | eval_custom_logloss: 1.65203 |  0:08:42s
epoch 29 | loss: 0.69824 | eval_custom_logloss: 2.86712 |  0:09:00s
epoch 30 | loss: 0.68966 | eval_custom_logloss: 1.65746 |  0:09:18s
epoch 31 | loss: 0.68249 | eval_custom_logloss: 1.42218 |  0:09:36s
epoch 32 | loss: 0.66899 | eval_custom_logloss: 1.37815 |  0:09:54s
epoch 33 | loss: 0.68504 | eval_custom_logloss: 1.17125 |  0:10:12s
epoch 34 | loss: 0.67814 | eval_custom_logloss: 1.1108  |  0:10:30s
epoch 35 | loss: 0.66436 | eval_custom_logloss: 1.12341 |  0:10:48s
epoch 36 | loss: 0.66751 | eval_custom_logloss: 1.12253 |  0:11:06s
epoch 37 | loss: 0.66861 | eval_custom_logloss: 1.12506 |  0:11:24s
epoch 38 | loss: 0.66549 | eval_custom_logloss: 0.92449 |  0:11:43s
epoch 39 | loss: 0.6563  | eval_custom_logloss: 1.08817 |  0:12:01s
epoch 40 | loss: 0.65361 | eval_custom_logloss: 1.12636 |  0:12:18s
epoch 41 | loss: 0.64749 | eval_custom_logloss: 1.03025 |  0:12:36s
epoch 42 | loss: 0.6508  | eval_custom_logloss: 1.01355 |  0:12:54s
epoch 43 | loss: 0.64285 | eval_custom_logloss: 1.22849 |  0:13:13s
epoch 44 | loss: 0.64205 | eval_custom_logloss: 1.32251 |  0:13:31s
epoch 45 | loss: 0.65545 | eval_custom_logloss: 1.06055 |  0:13:49s
epoch 46 | loss: 0.64104 | eval_custom_logloss: 0.89049 |  0:14:07s
epoch 47 | loss: 0.63804 | eval_custom_logloss: 0.87212 |  0:14:25s
epoch 48 | loss: 0.63504 | eval_custom_logloss: 1.02084 |  0:14:43s
epoch 49 | loss: 0.63738 | eval_custom_logloss: 0.98894 |  0:15:01s
epoch 50 | loss: 0.63529 | eval_custom_logloss: 0.84789 |  0:15:19s
epoch 51 | loss: 0.64149 | eval_custom_logloss: 7.15818 |  0:15:37s
epoch 52 | loss: 0.62788 | eval_custom_logloss: 1.47885 |  0:15:55s
epoch 53 | loss: 0.62724 | eval_custom_logloss: 0.892   |  0:16:13s
epoch 54 | loss: 0.62447 | eval_custom_logloss: 0.74888 |  0:16:31s
epoch 55 | loss: 0.62689 | eval_custom_logloss: 1.06197 |  0:16:49s
epoch 56 | loss: 0.62478 | eval_custom_logloss: 1.03065 |  0:17:07s
epoch 57 | loss: 0.61842 | eval_custom_logloss: 0.67317 |  0:17:25s
epoch 58 | loss: 0.6136  | eval_custom_logloss: 0.75617 |  0:17:44s
epoch 59 | loss: 0.62052 | eval_custom_logloss: 1.69747 |  0:18:02s
epoch 60 | loss: 0.61642 | eval_custom_logloss: 1.17303 |  0:18:20s
epoch 61 | loss: 0.61965 | eval_custom_logloss: 1.26898 |  0:18:38s
epoch 62 | loss: 0.60617 | eval_custom_logloss: 1.27183 |  0:18:56s
epoch 63 | loss: 0.6034  | eval_custom_logloss: 0.91223 |  0:19:14s
epoch 64 | loss: 0.605   | eval_custom_logloss: 1.14543 |  0:19:32s
epoch 65 | loss: 0.62112 | eval_custom_logloss: 1.12965 |  0:19:51s
epoch 66 | loss: 0.6159  | eval_custom_logloss: 0.74479 |  0:20:09s
epoch 67 | loss: 0.61802 | eval_custom_logloss: 0.95123 |  0:20:27s
epoch 68 | loss: 0.60197 | eval_custom_logloss: 0.9475  |  0:20:45s
epoch 69 | loss: 0.61863 | eval_custom_logloss: 1.1281  |  0:21:03s
epoch 70 | loss: 0.59729 | eval_custom_logloss: 0.99531 |  0:21:21s
epoch 71 | loss: 0.61068 | eval_custom_logloss: 1.05946 |  0:21:39s
epoch 72 | loss: 0.60255 | eval_custom_logloss: 1.0113  |  0:21:57s
epoch 73 | loss: 0.60053 | eval_custom_logloss: 0.99137 |  0:22:15s
epoch 74 | loss: 0.60411 | eval_custom_logloss: 0.91505 |  0:22:33s
epoch 75 | loss: 0.60977 | eval_custom_logloss: 0.80177 |  0:22:51s
epoch 76 | loss: 0.60682 | eval_custom_logloss: 0.82543 |  0:23:09s
epoch 77 | loss: 0.60321 | eval_custom_logloss: 1.1396  |  0:23:27s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.67317
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6959, 'Log Loss - std': 0.0630353868870494} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax', 'n_a': 35, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.52222 | eval_custom_logloss: 13.04302|  0:00:18s
epoch 1  | loss: 1.10198 | eval_custom_logloss: 11.15797|  0:00:36s
epoch 2  | loss: 1.06883 | eval_custom_logloss: 11.00341|  0:00:54s
epoch 3  | loss: 1.0197  | eval_custom_logloss: 11.03448|  0:01:12s
epoch 4  | loss: 1.02251 | eval_custom_logloss: 10.64762|  0:01:30s
epoch 5  | loss: 0.97169 | eval_custom_logloss: 9.23768 |  0:01:49s
epoch 6  | loss: 0.95801 | eval_custom_logloss: 8.51263 |  0:02:07s
epoch 7  | loss: 0.94599 | eval_custom_logloss: 8.07785 |  0:02:25s
epoch 8  | loss: 0.91838 | eval_custom_logloss: 6.21165 |  0:02:43s
epoch 9  | loss: 0.90488 | eval_custom_logloss: 5.4163  |  0:03:01s
epoch 10 | loss: 0.88799 | eval_custom_logloss: 4.88575 |  0:03:19s
epoch 11 | loss: 0.89694 | eval_custom_logloss: 3.87433 |  0:03:37s
epoch 12 | loss: 0.88355 | eval_custom_logloss: 4.30448 |  0:03:55s
epoch 13 | loss: 0.85783 | eval_custom_logloss: 2.1192  |  0:04:14s
epoch 14 | loss: 0.83936 | eval_custom_logloss: 1.50994 |  0:04:32s
epoch 15 | loss: 0.8388  | eval_custom_logloss: 2.77451 |  0:04:50s
epoch 16 | loss: 0.81719 | eval_custom_logloss: 1.75327 |  0:05:08s
epoch 17 | loss: 0.80124 | eval_custom_logloss: 1.72697 |  0:05:26s
epoch 18 | loss: 0.80192 | eval_custom_logloss: 1.85116 |  0:05:45s
epoch 19 | loss: 0.78996 | eval_custom_logloss: 1.90804 |  0:06:03s
epoch 20 | loss: 0.78025 | eval_custom_logloss: 1.14    |  0:06:21s
epoch 21 | loss: 0.75707 | eval_custom_logloss: 1.67269 |  0:06:39s
epoch 22 | loss: 0.76271 | eval_custom_logloss: 2.68875 |  0:06:57s
epoch 23 | loss: 0.75877 | eval_custom_logloss: 1.64306 |  0:07:15s
epoch 24 | loss: 0.7608  | eval_custom_logloss: 2.54198 |  0:07:33s
epoch 25 | loss: 0.75886 | eval_custom_logloss: 1.4914  |  0:07:51s
epoch 26 | loss: 0.74315 | eval_custom_logloss: 1.15091 |  0:08:09s
epoch 27 | loss: 0.74418 | eval_custom_logloss: 1.64992 |  0:08:28s
epoch 28 | loss: 0.7257  | eval_custom_logloss: 1.13463 |  0:08:46s
epoch 29 | loss: 0.7285  | eval_custom_logloss: 1.05646 |  0:09:04s
epoch 30 | loss: 0.71462 | eval_custom_logloss: 1.3767  |  0:09:22s
epoch 31 | loss: 0.70291 | eval_custom_logloss: 1.31074 |  0:09:40s
epoch 32 | loss: 0.6896  | eval_custom_logloss: 1.13202 |  0:09:58s
epoch 33 | loss: 0.70531 | eval_custom_logloss: 0.79265 |  0:10:16s
epoch 34 | loss: 0.69314 | eval_custom_logloss: 0.85207 |  0:10:34s
epoch 35 | loss: 0.70333 | eval_custom_logloss: 1.23912 |  0:10:52s
epoch 36 | loss: 0.6856  | eval_custom_logloss: 1.18108 |  0:11:10s
epoch 37 | loss: 0.68713 | eval_custom_logloss: 0.99628 |  0:11:29s
epoch 38 | loss: 0.69515 | eval_custom_logloss: 1.35141 |  0:11:47s
epoch 39 | loss: 0.67884 | eval_custom_logloss: 1.56461 |  0:12:05s
epoch 40 | loss: 0.67163 | eval_custom_logloss: 1.29845 |  0:12:23s
epoch 41 | loss: 0.68367 | eval_custom_logloss: 1.08159 |  0:12:41s
epoch 42 | loss: 0.66805 | eval_custom_logloss: 1.72515 |  0:12:59s
epoch 43 | loss: 0.66716 | eval_custom_logloss: 1.17521 |  0:13:17s
epoch 44 | loss: 0.6858  | eval_custom_logloss: 1.41424 |  0:13:35s
epoch 45 | loss: 0.67122 | eval_custom_logloss: 1.01668 |  0:13:53s
epoch 46 | loss: 0.6666  | eval_custom_logloss: 1.01325 |  0:14:11s
epoch 47 | loss: 0.65886 | eval_custom_logloss: 1.20242 |  0:14:29s
epoch 48 | loss: 0.65609 | eval_custom_logloss: 1.40112 |  0:14:47s
epoch 49 | loss: 0.66098 | eval_custom_logloss: 1.30627 |  0:15:06s
epoch 50 | loss: 0.65732 | eval_custom_logloss: 1.15047 |  0:15:24s
epoch 51 | loss: 0.6513  | eval_custom_logloss: 1.72753 |  0:15:42s
epoch 52 | loss: 0.64939 | eval_custom_logloss: 1.32745 |  0:16:00s
epoch 53 | loss: 0.64362 | eval_custom_logloss: 0.88683 |  0:16:18s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.79265
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.71995, 'Log Loss - std': 0.06866806026093937} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax', 'n_a': 35, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.60947 | eval_custom_logloss: 12.474  |  0:00:17s
epoch 1  | loss: 1.27932 | eval_custom_logloss: 13.75728|  0:00:36s
epoch 2  | loss: 1.1208  | eval_custom_logloss: 12.18244|  0:00:54s
epoch 3  | loss: 1.0411  | eval_custom_logloss: 11.17246|  0:01:12s
epoch 4  | loss: 1.03105 | eval_custom_logloss: 9.0303  |  0:01:30s
epoch 5  | loss: 0.98063 | eval_custom_logloss: 7.3955  |  0:01:47s
epoch 6  | loss: 0.97355 | eval_custom_logloss: 9.02967 |  0:02:06s
epoch 7  | loss: 0.97811 | eval_custom_logloss: 7.34372 |  0:02:24s
epoch 8  | loss: 0.95634 | eval_custom_logloss: 8.10112 |  0:02:42s
epoch 9  | loss: 0.94244 | eval_custom_logloss: 5.99787 |  0:03:00s
epoch 10 | loss: 0.91891 | eval_custom_logloss: 5.77352 |  0:03:18s
epoch 11 | loss: 0.90255 | eval_custom_logloss: 5.56367 |  0:03:36s
epoch 12 | loss: 0.90207 | eval_custom_logloss: 4.49114 |  0:03:54s
epoch 13 | loss: 0.87509 | eval_custom_logloss: 2.73625 |  0:04:12s
epoch 14 | loss: 0.85549 | eval_custom_logloss: 3.18895 |  0:04:30s
epoch 15 | loss: 0.83514 | eval_custom_logloss: 3.07926 |  0:04:48s
epoch 16 | loss: 0.83031 | eval_custom_logloss: 4.40714 |  0:05:06s
epoch 17 | loss: 0.8377  | eval_custom_logloss: 2.45211 |  0:05:24s
epoch 18 | loss: 0.80574 | eval_custom_logloss: 2.58608 |  0:05:42s
epoch 19 | loss: 0.81619 | eval_custom_logloss: 1.20169 |  0:05:59s
epoch 20 | loss: 0.79106 | eval_custom_logloss: 1.71059 |  0:06:17s
epoch 21 | loss: 0.79832 | eval_custom_logloss: 2.65713 |  0:06:35s
epoch 22 | loss: 0.78455 | eval_custom_logloss: 2.37911 |  0:06:53s
epoch 23 | loss: 0.7767  | eval_custom_logloss: 2.61702 |  0:07:10s
epoch 24 | loss: 0.77118 | eval_custom_logloss: 3.1415  |  0:07:28s
epoch 25 | loss: 0.75838 | eval_custom_logloss: 2.5976  |  0:07:46s
epoch 26 | loss: 0.76825 | eval_custom_logloss: 1.34418 |  0:08:03s
epoch 27 | loss: 0.74912 | eval_custom_logloss: 1.53628 |  0:08:21s
epoch 28 | loss: 0.73371 | eval_custom_logloss: 2.16483 |  0:08:39s
epoch 29 | loss: 0.73802 | eval_custom_logloss: 1.32735 |  0:08:57s
epoch 30 | loss: 0.72337 | eval_custom_logloss: 1.48883 |  0:09:15s
epoch 31 | loss: 0.72937 | eval_custom_logloss: 1.29294 |  0:09:33s
epoch 32 | loss: 0.72851 | eval_custom_logloss: 0.83429 |  0:09:51s
epoch 33 | loss: 0.71647 | eval_custom_logloss: 0.82018 |  0:10:09s
epoch 34 | loss: 0.68734 | eval_custom_logloss: 0.95937 |  0:10:27s
epoch 35 | loss: 0.69016 | eval_custom_logloss: 1.01212 |  0:10:45s
epoch 36 | loss: 0.68546 | eval_custom_logloss: 1.04672 |  0:11:03s
epoch 37 | loss: 0.71173 | eval_custom_logloss: 1.21946 |  0:11:21s
epoch 38 | loss: 0.68272 | eval_custom_logloss: 1.12413 |  0:11:39s
epoch 39 | loss: 0.7017  | eval_custom_logloss: 1.51142 |  0:11:57s
epoch 40 | loss: 0.68435 | eval_custom_logloss: 1.27767 |  0:12:15s
epoch 41 | loss: 0.6815  | eval_custom_logloss: 1.65658 |  0:12:33s
epoch 42 | loss: 0.67031 | eval_custom_logloss: 1.57181 |  0:12:51s
epoch 43 | loss: 0.66704 | eval_custom_logloss: 1.26716 |  0:13:09s
epoch 44 | loss: 0.66565 | eval_custom_logloss: 1.12458 |  0:13:27s
epoch 45 | loss: 0.67888 | eval_custom_logloss: 1.16195 |  0:13:45s
epoch 46 | loss: 0.66297 | eval_custom_logloss: 0.92831 |  0:14:03s
epoch 47 | loss: 0.66667 | eval_custom_logloss: 1.42026 |  0:14:21s
epoch 48 | loss: 0.67278 | eval_custom_logloss: 1.52081 |  0:14:39s
epoch 49 | loss: 0.67232 | eval_custom_logloss: 2.0487  |  0:14:58s
epoch 50 | loss: 0.65709 | eval_custom_logloss: 1.50621 |  0:15:16s
epoch 51 | loss: 0.66774 | eval_custom_logloss: 1.63336 |  0:15:34s
epoch 52 | loss: 0.66218 | eval_custom_logloss: 1.78599 |  0:15:52s
epoch 53 | loss: 0.69442 | eval_custom_logloss: 1.28177 |  0:16:10s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.82018
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7395799999999999, 'Log Loss - std': 0.0728943728966784} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 32 finished with value: 0.7395799999999999 and parameters: {'n_d': 35, 'n_steps': 6, 'gamma': 1.9537803548511197, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0011736335526755324, 'mask_type': 'entmax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71142 | eval_custom_logloss: 9.16913 |  0:00:20s
epoch 1  | loss: 1.25926 | eval_custom_logloss: 6.00305 |  0:00:41s
epoch 2  | loss: 1.06691 | eval_custom_logloss: 5.17467 |  0:01:02s
epoch 3  | loss: 1.00823 | eval_custom_logloss: 5.56011 |  0:01:23s
epoch 4  | loss: 1.0002  | eval_custom_logloss: 2.09402 |  0:01:43s
epoch 5  | loss: 0.95935 | eval_custom_logloss: 2.45828 |  0:02:04s
epoch 6  | loss: 0.93261 | eval_custom_logloss: 4.39787 |  0:02:25s
epoch 7  | loss: 0.88326 | eval_custom_logloss: 2.27149 |  0:02:46s
epoch 8  | loss: 0.87441 | eval_custom_logloss: 1.07937 |  0:03:07s
epoch 9  | loss: 0.85847 | eval_custom_logloss: 1.72251 |  0:03:28s
epoch 10 | loss: 0.86079 | eval_custom_logloss: 2.09272 |  0:03:49s
epoch 11 | loss: 0.85309 | eval_custom_logloss: 2.28932 |  0:04:11s
epoch 12 | loss: 0.83073 | eval_custom_logloss: 0.97645 |  0:04:32s
epoch 13 | loss: 0.82144 | eval_custom_logloss: 2.80642 |  0:04:52s
epoch 14 | loss: 0.80652 | eval_custom_logloss: 2.75198 |  0:05:13s
epoch 15 | loss: 0.79386 | eval_custom_logloss: 1.18795 |  0:05:34s
epoch 16 | loss: 0.79596 | eval_custom_logloss: 1.18063 |  0:05:54s
epoch 17 | loss: 0.79254 | eval_custom_logloss: 6.18391 |  0:06:15s
epoch 18 | loss: 0.77918 | eval_custom_logloss: 1.46541 |  0:06:36s
epoch 19 | loss: 0.78748 | eval_custom_logloss: 0.87708 |  0:06:57s
epoch 20 | loss: 0.76983 | eval_custom_logloss: 0.85956 |  0:07:17s
epoch 21 | loss: 0.76495 | eval_custom_logloss: 1.49504 |  0:07:38s
epoch 22 | loss: 0.76192 | eval_custom_logloss: 1.05593 |  0:07:59s
epoch 23 | loss: 0.76175 | eval_custom_logloss: 1.36791 |  0:08:20s
epoch 24 | loss: 0.73678 | eval_custom_logloss: 1.1434  |  0:08:41s
epoch 25 | loss: 0.74528 | eval_custom_logloss: 0.94414 |  0:09:01s
epoch 26 | loss: 0.75047 | eval_custom_logloss: 0.91819 |  0:09:22s
epoch 27 | loss: 0.73988 | eval_custom_logloss: 1.10061 |  0:09:43s
epoch 28 | loss: 0.72949 | eval_custom_logloss: 0.98026 |  0:10:04s
epoch 29 | loss: 0.73474 | eval_custom_logloss: 1.04115 |  0:10:24s
epoch 30 | loss: 0.72186 | eval_custom_logloss: 0.96985 |  0:10:45s
epoch 31 | loss: 0.72107 | eval_custom_logloss: 0.71599 |  0:11:06s
epoch 32 | loss: 0.71779 | eval_custom_logloss: 1.04774 |  0:11:26s
epoch 33 | loss: 0.71563 | eval_custom_logloss: 1.98733 |  0:11:47s
epoch 34 | loss: 0.70727 | eval_custom_logloss: 0.73044 |  0:12:08s
epoch 35 | loss: 0.70467 | eval_custom_logloss: 1.00504 |  0:12:28s
epoch 36 | loss: 0.72082 | eval_custom_logloss: 0.82786 |  0:12:49s
epoch 37 | loss: 0.71224 | eval_custom_logloss: 2.30475 |  0:13:10s
epoch 38 | loss: 0.72781 | eval_custom_logloss: 1.5875  |  0:13:31s
epoch 39 | loss: 0.71874 | eval_custom_logloss: 1.60139 |  0:13:52s
epoch 40 | loss: 0.73355 | eval_custom_logloss: 1.39063 |  0:14:12s
epoch 41 | loss: 0.72831 | eval_custom_logloss: 1.51344 |  0:14:33s
epoch 42 | loss: 0.70859 | eval_custom_logloss: 0.77694 |  0:14:54s
epoch 43 | loss: 0.72539 | eval_custom_logloss: 0.83988 |  0:15:14s
epoch 44 | loss: 0.70957 | eval_custom_logloss: 1.06233 |  0:15:35s
epoch 45 | loss: 0.71145 | eval_custom_logloss: 0.78454 |  0:15:56s
epoch 46 | loss: 0.69721 | eval_custom_logloss: 1.0702  |  0:16:17s
epoch 47 | loss: 0.6826  | eval_custom_logloss: 1.09049 |  0:16:38s
epoch 48 | loss: 0.70572 | eval_custom_logloss: 1.04374 |  0:16:58s
epoch 49 | loss: 0.68301 | eval_custom_logloss: 0.77457 |  0:17:19s
epoch 50 | loss: 0.68753 | eval_custom_logloss: 0.81774 |  0:17:40s
epoch 51 | loss: 0.67524 | eval_custom_logloss: 1.01789 |  0:18:01s

Early stopping occurred at epoch 51 with best_epoch = 31 and best_eval_custom_logloss = 0.71599
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7136, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68592 | eval_custom_logloss: 6.34239 |  0:00:20s
epoch 1  | loss: 1.30144 | eval_custom_logloss: 6.27824 |  0:00:41s
epoch 2  | loss: 1.07977 | eval_custom_logloss: 4.39662 |  0:01:02s
epoch 3  | loss: 1.00801 | eval_custom_logloss: 2.13048 |  0:01:23s
epoch 4  | loss: 0.9918  | eval_custom_logloss: 2.22286 |  0:01:44s
epoch 5  | loss: 0.94395 | eval_custom_logloss: 1.97753 |  0:02:05s
epoch 6  | loss: 0.9391  | eval_custom_logloss: 2.32779 |  0:02:26s
epoch 7  | loss: 0.92244 | eval_custom_logloss: 1.75037 |  0:02:47s
epoch 8  | loss: 0.90476 | eval_custom_logloss: 1.79002 |  0:03:08s
epoch 9  | loss: 0.88928 | eval_custom_logloss: 0.95968 |  0:03:29s
epoch 10 | loss: 0.88135 | eval_custom_logloss: 2.26892 |  0:03:50s
epoch 11 | loss: 0.85603 | eval_custom_logloss: 1.34525 |  0:04:11s
epoch 12 | loss: 0.84085 | eval_custom_logloss: 2.00923 |  0:04:32s
epoch 13 | loss: 0.82127 | eval_custom_logloss: 0.78453 |  0:04:53s
epoch 14 | loss: 0.80419 | eval_custom_logloss: 0.92096 |  0:05:14s
epoch 15 | loss: 0.7972  | eval_custom_logloss: 1.91355 |  0:05:35s
epoch 16 | loss: 0.80569 | eval_custom_logloss: 1.26931 |  0:05:56s
epoch 17 | loss: 0.78955 | eval_custom_logloss: 0.93264 |  0:06:17s
epoch 18 | loss: 0.77247 | eval_custom_logloss: 1.17778 |  0:06:38s
epoch 19 | loss: 0.77247 | eval_custom_logloss: 1.26248 |  0:06:59s
epoch 20 | loss: 0.77415 | eval_custom_logloss: 1.07232 |  0:07:20s
epoch 21 | loss: 0.75931 | eval_custom_logloss: 1.16583 |  0:07:41s
epoch 22 | loss: 0.75589 | eval_custom_logloss: 0.91427 |  0:08:02s
epoch 23 | loss: 0.75868 | eval_custom_logloss: 0.88901 |  0:08:23s
epoch 24 | loss: 0.75086 | eval_custom_logloss: 0.88064 |  0:08:44s
epoch 25 | loss: 0.74441 | eval_custom_logloss: 0.72668 |  0:09:05s
epoch 26 | loss: 0.75299 | eval_custom_logloss: 0.83001 |  0:09:26s
epoch 27 | loss: 0.7532  | eval_custom_logloss: 1.2244  |  0:09:47s
epoch 28 | loss: 0.74537 | eval_custom_logloss: 0.76736 |  0:10:08s
epoch 29 | loss: 0.73011 | eval_custom_logloss: 0.81674 |  0:10:29s
epoch 30 | loss: 0.73013 | eval_custom_logloss: 1.11673 |  0:10:50s
epoch 31 | loss: 0.7375  | eval_custom_logloss: 0.89648 |  0:11:10s
epoch 32 | loss: 0.74372 | eval_custom_logloss: 0.73293 |  0:11:31s
epoch 33 | loss: 0.74912 | eval_custom_logloss: 1.06209 |  0:11:52s
epoch 34 | loss: 0.74428 | eval_custom_logloss: 0.94091 |  0:12:12s
epoch 35 | loss: 0.73035 | eval_custom_logloss: 0.88573 |  0:12:33s
epoch 36 | loss: 0.75051 | eval_custom_logloss: 0.98762 |  0:12:54s
epoch 37 | loss: 0.73109 | eval_custom_logloss: 0.64503 |  0:13:14s
epoch 38 | loss: 0.72695 | eval_custom_logloss: 0.7203  |  0:13:35s
epoch 39 | loss: 0.71307 | eval_custom_logloss: 0.78214 |  0:13:56s
epoch 40 | loss: 0.74039 | eval_custom_logloss: 0.99471 |  0:14:16s
epoch 41 | loss: 0.72201 | eval_custom_logloss: 0.887   |  0:14:38s
epoch 42 | loss: 0.71753 | eval_custom_logloss: 0.75506 |  0:14:58s
epoch 43 | loss: 0.71131 | eval_custom_logloss: 0.66029 |  0:15:19s
epoch 44 | loss: 0.73    | eval_custom_logloss: 0.67773 |  0:15:40s
epoch 45 | loss: 0.71927 | eval_custom_logloss: 1.00884 |  0:16:01s
epoch 46 | loss: 0.70957 | eval_custom_logloss: 0.93125 |  0:16:22s
epoch 47 | loss: 0.70733 | eval_custom_logloss: 0.90554 |  0:16:43s
epoch 48 | loss: 0.70381 | eval_custom_logloss: 0.95982 |  0:17:04s
epoch 49 | loss: 0.70426 | eval_custom_logloss: 0.76084 |  0:17:25s
epoch 50 | loss: 0.71103 | eval_custom_logloss: 0.92319 |  0:17:46s
epoch 51 | loss: 0.69617 | eval_custom_logloss: 0.95846 |  0:18:07s
epoch 52 | loss: 0.69601 | eval_custom_logloss: 0.79768 |  0:18:28s
epoch 53 | loss: 0.69259 | eval_custom_logloss: 0.67521 |  0:18:49s
epoch 54 | loss: 0.69137 | eval_custom_logloss: 1.0204  |  0:19:10s
epoch 55 | loss: 0.70798 | eval_custom_logloss: 0.80532 |  0:19:31s
epoch 56 | loss: 0.68481 | eval_custom_logloss: 0.66492 |  0:19:53s
epoch 57 | loss: 0.70171 | eval_custom_logloss: 0.71463 |  0:20:14s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 0.64503
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6787000000000001, 'Log Loss - std': 0.03489999999999999} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68846 | eval_custom_logloss: 11.03585|  0:00:20s
epoch 1  | loss: 1.17388 | eval_custom_logloss: 7.36607 |  0:00:41s
epoch 2  | loss: 1.03401 | eval_custom_logloss: 3.50423 |  0:01:02s
epoch 3  | loss: 0.99789 | eval_custom_logloss: 4.42422 |  0:01:23s
epoch 4  | loss: 0.97894 | eval_custom_logloss: 1.90709 |  0:01:44s
epoch 5  | loss: 0.91193 | eval_custom_logloss: 2.5791  |  0:02:04s
epoch 6  | loss: 0.90287 | eval_custom_logloss: 2.23356 |  0:02:25s
epoch 7  | loss: 0.86666 | eval_custom_logloss: 1.86968 |  0:02:46s
epoch 8  | loss: 0.86492 | eval_custom_logloss: 1.72727 |  0:03:07s
epoch 9  | loss: 0.84476 | eval_custom_logloss: 1.04954 |  0:03:28s
epoch 10 | loss: 0.83625 | eval_custom_logloss: 0.87022 |  0:03:48s
epoch 11 | loss: 0.85227 | eval_custom_logloss: 1.14495 |  0:04:09s
epoch 12 | loss: 0.83003 | eval_custom_logloss: 1.04692 |  0:04:30s
epoch 13 | loss: 0.81909 | eval_custom_logloss: 0.89514 |  0:04:51s
epoch 14 | loss: 0.81052 | eval_custom_logloss: 1.47298 |  0:05:12s
epoch 15 | loss: 0.8201  | eval_custom_logloss: 1.01932 |  0:05:33s
epoch 16 | loss: 0.80493 | eval_custom_logloss: 0.81024 |  0:05:53s
epoch 17 | loss: 0.80431 | eval_custom_logloss: 1.31227 |  0:06:14s
epoch 18 | loss: 0.79933 | eval_custom_logloss: 1.05101 |  0:06:35s
epoch 19 | loss: 0.80987 | eval_custom_logloss: 1.24274 |  0:06:56s
epoch 20 | loss: 0.79664 | eval_custom_logloss: 0.99849 |  0:07:17s
epoch 21 | loss: 0.78521 | eval_custom_logloss: 0.90466 |  0:07:37s
epoch 22 | loss: 0.79141 | eval_custom_logloss: 1.35744 |  0:07:58s
epoch 23 | loss: 0.79589 | eval_custom_logloss: 1.10485 |  0:08:19s
epoch 24 | loss: 0.75722 | eval_custom_logloss: 1.02546 |  0:08:40s
epoch 25 | loss: 0.76239 | eval_custom_logloss: 0.94599 |  0:09:01s
epoch 26 | loss: 0.75946 | eval_custom_logloss: 0.83238 |  0:09:22s
epoch 27 | loss: 0.76266 | eval_custom_logloss: 0.81803 |  0:09:43s
epoch 28 | loss: 0.75531 | eval_custom_logloss: 0.73003 |  0:10:04s
epoch 29 | loss: 0.75529 | eval_custom_logloss: 1.21151 |  0:10:25s
epoch 30 | loss: 0.75978 | eval_custom_logloss: 1.62534 |  0:10:46s
epoch 31 | loss: 0.73681 | eval_custom_logloss: 1.31942 |  0:11:07s
epoch 32 | loss: 0.74782 | eval_custom_logloss: 1.01777 |  0:11:28s
epoch 33 | loss: 0.73887 | eval_custom_logloss: 0.92904 |  0:11:49s
epoch 34 | loss: 0.74337 | eval_custom_logloss: 0.98376 |  0:12:10s
epoch 35 | loss: 0.73034 | eval_custom_logloss: 0.83728 |  0:12:31s
epoch 36 | loss: 0.74791 | eval_custom_logloss: 0.89534 |  0:12:52s
epoch 37 | loss: 0.73716 | eval_custom_logloss: 0.97585 |  0:13:13s
epoch 38 | loss: 0.73748 | eval_custom_logloss: 0.89296 |  0:13:33s
epoch 39 | loss: 0.72503 | eval_custom_logloss: 0.72345 |  0:13:54s
epoch 40 | loss: 0.73662 | eval_custom_logloss: 0.82644 |  0:14:15s
epoch 41 | loss: 0.73571 | eval_custom_logloss: 0.90903 |  0:14:36s
epoch 42 | loss: 0.72045 | eval_custom_logloss: 0.89996 |  0:14:57s
epoch 43 | loss: 0.72646 | eval_custom_logloss: 0.66591 |  0:15:17s
epoch 44 | loss: 0.72371 | eval_custom_logloss: 1.06074 |  0:15:38s
epoch 45 | loss: 0.72716 | eval_custom_logloss: 0.99473 |  0:15:59s
epoch 46 | loss: 0.70365 | eval_custom_logloss: 0.81222 |  0:16:20s
epoch 47 | loss: 0.71436 | eval_custom_logloss: 0.81584 |  0:16:41s
epoch 48 | loss: 0.71813 | eval_custom_logloss: 0.88341 |  0:17:02s
epoch 49 | loss: 0.70692 | eval_custom_logloss: 1.12173 |  0:17:22s
epoch 50 | loss: 0.71569 | eval_custom_logloss: 0.90325 |  0:17:43s
epoch 51 | loss: 0.69235 | eval_custom_logloss: 0.94173 |  0:18:04s
epoch 52 | loss: 0.70115 | eval_custom_logloss: 1.08996 |  0:18:25s
epoch 53 | loss: 0.71285 | eval_custom_logloss: 0.72915 |  0:18:46s
epoch 54 | loss: 0.68659 | eval_custom_logloss: 0.79793 |  0:19:06s
epoch 55 | loss: 0.70467 | eval_custom_logloss: 0.88419 |  0:19:27s
epoch 56 | loss: 0.6988  | eval_custom_logloss: 0.90653 |  0:19:48s
epoch 57 | loss: 0.69899 | eval_custom_logloss: 0.69779 |  0:20:09s
epoch 58 | loss: 0.70249 | eval_custom_logloss: 0.86975 |  0:20:30s
epoch 59 | loss: 0.7022  | eval_custom_logloss: 0.95865 |  0:20:51s
epoch 60 | loss: 0.69074 | eval_custom_logloss: 0.74251 |  0:21:12s
epoch 61 | loss: 0.69378 | eval_custom_logloss: 0.70154 |  0:21:33s
epoch 62 | loss: 0.68749 | eval_custom_logloss: 1.12919 |  0:21:53s
epoch 63 | loss: 0.69971 | eval_custom_logloss: 0.7258  |  0:22:14s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.66591
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6739333333333334, 'Log Loss - std': 0.029282228209084236} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.64319 | eval_custom_logloss: 6.68295 |  0:00:20s
epoch 1  | loss: 1.14881 | eval_custom_logloss: 6.10386 |  0:00:41s
epoch 2  | loss: 1.0444  | eval_custom_logloss: 2.92831 |  0:01:02s
epoch 3  | loss: 1.00708 | eval_custom_logloss: 2.80875 |  0:01:23s
epoch 4  | loss: 0.98198 | eval_custom_logloss: 2.12982 |  0:01:45s
epoch 5  | loss: 0.9392  | eval_custom_logloss: 2.45193 |  0:02:06s
epoch 6  | loss: 0.94199 | eval_custom_logloss: 2.08025 |  0:02:27s
epoch 7  | loss: 0.93086 | eval_custom_logloss: 1.23816 |  0:02:48s
epoch 8  | loss: 0.91581 | eval_custom_logloss: 1.20231 |  0:03:09s
epoch 9  | loss: 0.8763  | eval_custom_logloss: 1.08631 |  0:03:30s
epoch 10 | loss: 0.87716 | eval_custom_logloss: 0.97007 |  0:03:50s
epoch 11 | loss: 0.86982 | eval_custom_logloss: 1.20871 |  0:04:11s
epoch 12 | loss: 0.84623 | eval_custom_logloss: 0.85814 |  0:04:32s
epoch 13 | loss: 0.84481 | eval_custom_logloss: 1.27344 |  0:04:53s
epoch 14 | loss: 0.86347 | eval_custom_logloss: 1.41296 |  0:05:14s
epoch 15 | loss: 0.82353 | eval_custom_logloss: 1.6222  |  0:05:34s
epoch 16 | loss: 0.81367 | eval_custom_logloss: 1.16739 |  0:05:55s
epoch 17 | loss: 0.81927 | eval_custom_logloss: 1.10267 |  0:06:16s
epoch 18 | loss: 0.79771 | eval_custom_logloss: 0.73346 |  0:06:36s
epoch 19 | loss: 0.81733 | eval_custom_logloss: 1.18026 |  0:06:57s
epoch 20 | loss: 0.81061 | eval_custom_logloss: 0.81094 |  0:07:18s
epoch 21 | loss: 0.79636 | eval_custom_logloss: 0.7857  |  0:07:38s
epoch 22 | loss: 0.77725 | eval_custom_logloss: 0.88928 |  0:07:59s
epoch 23 | loss: 0.79046 | eval_custom_logloss: 0.91115 |  0:08:20s
epoch 24 | loss: 0.75758 | eval_custom_logloss: 0.67362 |  0:08:41s
epoch 25 | loss: 0.78328 | eval_custom_logloss: 1.1591  |  0:09:01s
epoch 26 | loss: 0.78791 | eval_custom_logloss: 0.89832 |  0:09:22s
epoch 27 | loss: 0.78527 | eval_custom_logloss: 1.52176 |  0:09:42s
epoch 28 | loss: 0.80436 | eval_custom_logloss: 1.13812 |  0:10:03s
epoch 29 | loss: 0.78653 | eval_custom_logloss: 1.05194 |  0:10:23s
epoch 30 | loss: 0.76143 | eval_custom_logloss: 1.07942 |  0:10:43s
epoch 31 | loss: 0.75744 | eval_custom_logloss: 0.70364 |  0:11:04s
epoch 32 | loss: 0.75609 | eval_custom_logloss: 0.7706  |  0:11:24s
epoch 33 | loss: 0.75228 | eval_custom_logloss: 1.01127 |  0:11:44s
epoch 34 | loss: 0.76395 | eval_custom_logloss: 0.78792 |  0:12:05s
epoch 35 | loss: 0.74664 | eval_custom_logloss: 0.73902 |  0:12:26s
epoch 36 | loss: 0.75657 | eval_custom_logloss: 1.01941 |  0:12:46s
epoch 37 | loss: 0.74288 | eval_custom_logloss: 0.91025 |  0:13:07s
epoch 38 | loss: 0.73062 | eval_custom_logloss: 1.03357 |  0:13:28s
epoch 39 | loss: 0.73023 | eval_custom_logloss: 1.18658 |  0:13:49s
epoch 40 | loss: 0.72376 | eval_custom_logloss: 0.74271 |  0:14:09s
epoch 41 | loss: 0.73896 | eval_custom_logloss: 0.79294 |  0:14:30s
epoch 42 | loss: 0.72115 | eval_custom_logloss: 0.807   |  0:14:51s
epoch 43 | loss: 0.72101 | eval_custom_logloss: 0.65541 |  0:15:12s
epoch 44 | loss: 0.72206 | eval_custom_logloss: 1.04651 |  0:15:33s
epoch 45 | loss: 0.72498 | eval_custom_logloss: 0.83781 |  0:15:54s
epoch 46 | loss: 0.71211 | eval_custom_logloss: 0.98378 |  0:16:15s
epoch 47 | loss: 0.7085  | eval_custom_logloss: 1.03035 |  0:16:36s
epoch 48 | loss: 0.71382 | eval_custom_logloss: 0.65779 |  0:16:57s
epoch 49 | loss: 0.71405 | eval_custom_logloss: 0.74529 |  0:17:18s
epoch 50 | loss: 0.71842 | eval_custom_logloss: 0.84779 |  0:17:39s
epoch 51 | loss: 0.70152 | eval_custom_logloss: 0.84951 |  0:18:00s
epoch 52 | loss: 0.71042 | eval_custom_logloss: 0.84328 |  0:18:21s
epoch 53 | loss: 0.7009  | eval_custom_logloss: 0.62816 |  0:18:42s
epoch 54 | loss: 0.69932 | eval_custom_logloss: 0.92232 |  0:19:04s
epoch 55 | loss: 0.70325 | eval_custom_logloss: 1.10766 |  0:19:25s
epoch 56 | loss: 0.69829 | eval_custom_logloss: 0.77669 |  0:19:46s
epoch 57 | loss: 0.70021 | eval_custom_logloss: 0.73148 |  0:20:07s
epoch 58 | loss: 0.69153 | eval_custom_logloss: 0.67957 |  0:20:28s
epoch 59 | loss: 0.69412 | eval_custom_logloss: 0.8823  |  0:20:49s
epoch 60 | loss: 0.69347 | eval_custom_logloss: 0.77584 |  0:21:10s
epoch 61 | loss: 0.70459 | eval_custom_logloss: 0.9248  |  0:21:31s
epoch 62 | loss: 0.68858 | eval_custom_logloss: 1.09497 |  0:21:52s
epoch 63 | loss: 0.69875 | eval_custom_logloss: 0.80385 |  0:22:13s
epoch 64 | loss: 0.68227 | eval_custom_logloss: 0.8523  |  0:22:34s
epoch 65 | loss: 0.67241 | eval_custom_logloss: 0.92212 |  0:22:55s
epoch 66 | loss: 0.68121 | eval_custom_logloss: 0.69562 |  0:23:16s
epoch 67 | loss: 0.69543 | eval_custom_logloss: 0.92517 |  0:23:37s
epoch 68 | loss: 0.68669 | eval_custom_logloss: 0.64337 |  0:23:58s
epoch 69 | loss: 0.69241 | eval_custom_logloss: 0.74522 |  0:24:19s
epoch 70 | loss: 0.68612 | eval_custom_logloss: 0.67918 |  0:24:40s
epoch 71 | loss: 0.68905 | eval_custom_logloss: 0.74546 |  0:25:01s
epoch 72 | loss: 0.66609 | eval_custom_logloss: 0.74359 |  0:25:22s
epoch 73 | loss: 0.67936 | eval_custom_logloss: 0.65753 |  0:25:44s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.62816
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6622750000000001, 'Log Loss - std': 0.0324166141816199} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax', 'n_a': 13, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.69811 | eval_custom_logloss: 9.95671 |  0:00:21s
epoch 1  | loss: 1.30333 | eval_custom_logloss: 4.68744 |  0:00:42s
epoch 2  | loss: 1.20057 | eval_custom_logloss: 3.29042 |  0:01:03s
epoch 3  | loss: 1.13648 | eval_custom_logloss: 3.61407 |  0:01:24s
epoch 4  | loss: 1.1064  | eval_custom_logloss: 2.39434 |  0:01:45s
epoch 5  | loss: 1.02477 | eval_custom_logloss: 2.67146 |  0:02:07s
epoch 6  | loss: 1.02282 | eval_custom_logloss: 1.7952  |  0:02:28s
epoch 7  | loss: 1.00939 | eval_custom_logloss: 1.89915 |  0:02:49s
epoch 8  | loss: 0.98329 | eval_custom_logloss: 1.14973 |  0:03:10s
epoch 9  | loss: 0.99759 | eval_custom_logloss: 3.32143 |  0:03:31s
epoch 10 | loss: 0.98449 | eval_custom_logloss: 2.73305 |  0:03:53s
epoch 11 | loss: 0.97784 | eval_custom_logloss: 3.53806 |  0:04:14s
epoch 12 | loss: 0.96614 | eval_custom_logloss: 1.23631 |  0:04:35s
epoch 13 | loss: 0.93823 | eval_custom_logloss: 1.79319 |  0:04:56s
epoch 14 | loss: 0.91748 | eval_custom_logloss: 2.35995 |  0:05:17s
epoch 15 | loss: 0.9039  | eval_custom_logloss: 1.39755 |  0:05:38s
epoch 16 | loss: 0.87508 | eval_custom_logloss: 0.86844 |  0:05:59s
epoch 17 | loss: 0.87705 | eval_custom_logloss: 1.02195 |  0:06:20s
epoch 18 | loss: 0.85252 | eval_custom_logloss: 0.88617 |  0:06:42s
epoch 19 | loss: 0.86121 | eval_custom_logloss: 1.47016 |  0:07:02s
epoch 20 | loss: 0.8443  | eval_custom_logloss: 1.15772 |  0:07:24s
epoch 21 | loss: 0.83885 | eval_custom_logloss: 1.37274 |  0:07:45s
epoch 22 | loss: 0.82126 | eval_custom_logloss: 1.22228 |  0:08:07s
epoch 23 | loss: 0.80793 | eval_custom_logloss: 1.00103 |  0:08:28s
epoch 24 | loss: 0.78994 | eval_custom_logloss: 1.0973  |  0:08:49s
epoch 25 | loss: 0.76848 | eval_custom_logloss: 0.85831 |  0:09:11s
epoch 26 | loss: 0.7659  | eval_custom_logloss: 0.7919  |  0:09:32s
epoch 27 | loss: 0.75852 | eval_custom_logloss: 0.82302 |  0:09:53s
epoch 28 | loss: 0.74802 | eval_custom_logloss: 1.03912 |  0:10:14s
epoch 29 | loss: 0.74497 | eval_custom_logloss: 0.88178 |  0:10:36s
epoch 30 | loss: 0.73153 | eval_custom_logloss: 1.32164 |  0:10:57s
epoch 31 | loss: 0.73832 | eval_custom_logloss: 0.91688 |  0:11:18s
epoch 32 | loss: 0.72938 | eval_custom_logloss: 1.05777 |  0:11:39s
epoch 33 | loss: 0.73671 | eval_custom_logloss: 1.47991 |  0:12:01s
epoch 34 | loss: 0.71625 | eval_custom_logloss: 1.43861 |  0:12:22s
epoch 35 | loss: 0.70884 | eval_custom_logloss: 0.76946 |  0:12:43s
epoch 36 | loss: 0.72227 | eval_custom_logloss: 1.0591  |  0:13:04s
epoch 37 | loss: 0.70457 | eval_custom_logloss: 1.70212 |  0:13:25s
epoch 38 | loss: 0.68872 | eval_custom_logloss: 0.90248 |  0:13:47s
epoch 39 | loss: 0.69648 | eval_custom_logloss: 0.78898 |  0:14:08s
epoch 40 | loss: 0.70642 | eval_custom_logloss: 0.7712  |  0:14:29s
epoch 41 | loss: 0.70153 | eval_custom_logloss: 1.13878 |  0:14:50s
epoch 42 | loss: 0.68279 | eval_custom_logloss: 1.06817 |  0:15:11s
epoch 43 | loss: 0.69596 | eval_custom_logloss: 1.02536 |  0:15:33s
epoch 44 | loss: 0.68978 | eval_custom_logloss: 0.84195 |  0:15:54s
epoch 45 | loss: 0.69719 | eval_custom_logloss: 0.69189 |  0:16:14s
epoch 46 | loss: 0.68037 | eval_custom_logloss: 0.84626 |  0:16:35s
epoch 47 | loss: 0.67606 | eval_custom_logloss: 0.86274 |  0:16:57s
epoch 48 | loss: 0.67683 | eval_custom_logloss: 0.84546 |  0:17:17s
epoch 49 | loss: 0.68544 | eval_custom_logloss: 1.2817  |  0:17:38s
epoch 50 | loss: 0.67619 | eval_custom_logloss: 0.89554 |  0:17:59s
epoch 51 | loss: 0.66421 | eval_custom_logloss: 1.01527 |  0:18:20s
epoch 52 | loss: 0.67913 | eval_custom_logloss: 0.76839 |  0:18:41s
epoch 53 | loss: 0.67509 | eval_custom_logloss: 0.71646 |  0:19:02s
epoch 54 | loss: 0.66764 | eval_custom_logloss: 0.93154 |  0:19:23s
epoch 55 | loss: 0.67924 | eval_custom_logloss: 0.92536 |  0:19:44s
epoch 56 | loss: 0.66257 | eval_custom_logloss: 0.94041 |  0:20:05s
epoch 57 | loss: 0.66839 | eval_custom_logloss: 1.11523 |  0:20:26s
epoch 58 | loss: 0.67097 | eval_custom_logloss: 0.80472 |  0:20:47s
epoch 59 | loss: 0.65778 | eval_custom_logloss: 0.73029 |  0:21:08s
epoch 60 | loss: 0.65506 | eval_custom_logloss: 0.80964 |  0:21:29s
epoch 61 | loss: 0.66715 | eval_custom_logloss: 0.8245  |  0:21:49s
epoch 62 | loss: 0.65923 | eval_custom_logloss: 0.77296 |  0:22:10s
epoch 63 | loss: 0.67291 | eval_custom_logloss: 0.69919 |  0:22:31s
epoch 64 | loss: 0.66282 | eval_custom_logloss: 0.65062 |  0:22:52s
epoch 65 | loss: 0.64237 | eval_custom_logloss: 0.90164 |  0:23:13s
epoch 66 | loss: 0.66053 | eval_custom_logloss: 0.72174 |  0:23:34s
epoch 67 | loss: 0.64796 | eval_custom_logloss: 0.73015 |  0:23:55s
epoch 68 | loss: 0.66056 | eval_custom_logloss: 0.73249 |  0:24:15s
epoch 69 | loss: 0.66825 | eval_custom_logloss: 1.4104  |  0:24:36s
epoch 70 | loss: 0.65409 | eval_custom_logloss: 1.35641 |  0:24:56s
epoch 71 | loss: 0.63936 | eval_custom_logloss: 0.61874 |  0:25:17s
epoch 72 | loss: 0.64914 | eval_custom_logloss: 1.44628 |  0:25:38s
epoch 73 | loss: 0.64854 | eval_custom_logloss: 0.73329 |  0:25:58s
epoch 74 | loss: 0.65445 | eval_custom_logloss: 0.99018 |  0:26:19s
epoch 75 | loss: 0.66093 | eval_custom_logloss: 0.73099 |  0:26:39s
epoch 76 | loss: 0.65911 | eval_custom_logloss: 0.93967 |  0:27:00s
epoch 77 | loss: 0.65966 | eval_custom_logloss: 1.38816 |  0:27:21s
epoch 78 | loss: 0.6556  | eval_custom_logloss: 1.20238 |  0:27:41s
epoch 79 | loss: 0.64055 | eval_custom_logloss: 0.72314 |  0:28:02s
epoch 80 | loss: 0.65842 | eval_custom_logloss: 0.62768 |  0:28:22s
epoch 81 | loss: 0.64352 | eval_custom_logloss: 1.11193 |  0:28:43s
epoch 82 | loss: 0.63619 | eval_custom_logloss: 0.82885 |  0:29:04s
epoch 83 | loss: 0.65278 | eval_custom_logloss: 0.9887  |  0:29:25s
epoch 84 | loss: 0.64424 | eval_custom_logloss: 0.76879 |  0:29:46s
epoch 85 | loss: 0.63789 | eval_custom_logloss: 0.96126 |  0:30:07s
epoch 86 | loss: 0.63783 | eval_custom_logloss: 0.61259 |  0:30:28s
epoch 87 | loss: 0.63228 | eval_custom_logloss: 0.69993 |  0:30:49s
epoch 88 | loss: 0.64019 | eval_custom_logloss: 0.67179 |  0:31:10s
epoch 89 | loss: 0.63812 | eval_custom_logloss: 0.86031 |  0:31:31s
epoch 90 | loss: 0.63973 | eval_custom_logloss: 0.9648  |  0:31:52s
epoch 91 | loss: 0.63411 | eval_custom_logloss: 0.77911 |  0:32:13s
epoch 92 | loss: 0.63981 | eval_custom_logloss: 0.85859 |  0:32:34s
epoch 93 | loss: 0.64347 | eval_custom_logloss: 0.74424 |  0:32:54s
epoch 94 | loss: 0.6573  | eval_custom_logloss: 1.48952 |  0:33:15s
epoch 95 | loss: 0.65319 | eval_custom_logloss: 1.02435 |  0:33:36s
epoch 96 | loss: 0.64483 | eval_custom_logloss: 0.92792 |  0:33:57s
epoch 97 | loss: 0.63647 | eval_custom_logloss: 1.29289 |  0:34:18s
epoch 98 | loss: 0.6468  | eval_custom_logloss: 0.79749 |  0:34:39s
epoch 99 | loss: 0.62506 | eval_custom_logloss: 0.64848 |  0:35:01s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.61259
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.65202, 'Log Loss - std': 0.035515202378699755} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 33 finished with value: 0.65202 and parameters: {'n_d': 13, 'n_steps': 7, 'gamma': 1.926090515974195, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.002781023040405986, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax', 'n_a': 9, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.82632 | eval_custom_logloss: 11.93655|  0:00:25s
epoch 1  | loss: 1.42162 | eval_custom_logloss: 7.83607 |  0:00:51s
epoch 2  | loss: 1.2562  | eval_custom_logloss: 6.31186 |  0:01:16s
epoch 3  | loss: 1.17105 | eval_custom_logloss: 6.76773 |  0:01:41s
epoch 4  | loss: 1.08995 | eval_custom_logloss: 6.80493 |  0:02:07s
epoch 5  | loss: 1.13884 | eval_custom_logloss: 2.94152 |  0:02:33s
epoch 6  | loss: 1.09358 | eval_custom_logloss: 2.66512 |  0:02:58s
epoch 7  | loss: 1.04327 | eval_custom_logloss: 5.03975 |  0:03:24s
epoch 8  | loss: 1.02941 | eval_custom_logloss: 3.87866 |  0:03:50s
epoch 9  | loss: 1.01475 | eval_custom_logloss: 3.43488 |  0:04:16s
epoch 10 | loss: 0.98481 | eval_custom_logloss: 2.78682 |  0:04:41s
epoch 11 | loss: 0.96607 | eval_custom_logloss: 1.79166 |  0:05:07s
epoch 12 | loss: 0.95922 | eval_custom_logloss: 1.9154  |  0:05:33s
epoch 13 | loss: 0.96195 | eval_custom_logloss: 3.1715  |  0:05:59s
epoch 14 | loss: 0.94854 | eval_custom_logloss: 1.1022  |  0:06:25s
epoch 15 | loss: 0.95054 | eval_custom_logloss: 1.04656 |  0:06:51s
epoch 16 | loss: 0.9316  | eval_custom_logloss: 2.98197 |  0:07:16s
epoch 17 | loss: 0.92798 | eval_custom_logloss: 2.14114 |  0:07:42s
epoch 18 | loss: 0.92225 | eval_custom_logloss: 1.61941 |  0:08:08s
epoch 19 | loss: 0.93136 | eval_custom_logloss: 1.38753 |  0:08:33s
epoch 20 | loss: 0.91206 | eval_custom_logloss: 0.9495  |  0:08:59s
epoch 21 | loss: 0.92473 | eval_custom_logloss: 3.00039 |  0:09:25s
epoch 22 | loss: 0.92714 | eval_custom_logloss: 2.00506 |  0:09:51s
epoch 23 | loss: 0.89911 | eval_custom_logloss: 1.17651 |  0:10:16s
epoch 24 | loss: 0.88696 | eval_custom_logloss: 0.96051 |  0:10:42s
epoch 25 | loss: 0.88858 | eval_custom_logloss: 1.13644 |  0:11:08s
epoch 26 | loss: 0.87234 | eval_custom_logloss: 1.15064 |  0:11:34s
epoch 27 | loss: 0.88346 | eval_custom_logloss: 1.50411 |  0:11:59s
epoch 28 | loss: 0.89306 | eval_custom_logloss: 2.58922 |  0:12:25s
epoch 29 | loss: 0.86073 | eval_custom_logloss: 1.05307 |  0:12:51s
epoch 30 | loss: 0.86356 | eval_custom_logloss: 0.9436  |  0:13:16s
epoch 31 | loss: 0.85803 | eval_custom_logloss: 1.89738 |  0:13:42s
epoch 32 | loss: 0.86349 | eval_custom_logloss: 1.32323 |  0:14:08s
epoch 33 | loss: 0.86025 | eval_custom_logloss: 1.88049 |  0:14:34s
epoch 34 | loss: 0.85621 | eval_custom_logloss: 1.02529 |  0:14:59s
epoch 35 | loss: 0.84944 | eval_custom_logloss: 0.83657 |  0:15:25s
epoch 36 | loss: 0.85848 | eval_custom_logloss: 1.08501 |  0:15:51s
epoch 37 | loss: 0.84204 | eval_custom_logloss: 0.91632 |  0:16:17s
epoch 38 | loss: 0.86065 | eval_custom_logloss: 0.8542  |  0:16:43s
epoch 39 | loss: 0.85106 | eval_custom_logloss: 2.38739 |  0:17:08s
epoch 40 | loss: 0.86433 | eval_custom_logloss: 1.29625 |  0:17:34s
epoch 41 | loss: 0.85887 | eval_custom_logloss: 2.73709 |  0:18:00s
epoch 42 | loss: 0.8606  | eval_custom_logloss: 1.58925 |  0:18:26s
epoch 43 | loss: 0.83594 | eval_custom_logloss: 0.86401 |  0:18:52s
epoch 44 | loss: 0.85926 | eval_custom_logloss: 2.77556 |  0:19:17s
epoch 45 | loss: 0.85399 | eval_custom_logloss: 2.79334 |  0:19:43s
epoch 46 | loss: 0.84124 | eval_custom_logloss: 0.91611 |  0:20:09s
epoch 47 | loss: 0.83201 | eval_custom_logloss: 1.25074 |  0:20:35s
epoch 48 | loss: 0.8215  | eval_custom_logloss: 1.27685 |  0:21:00s
epoch 49 | loss: 0.81245 | eval_custom_logloss: 1.02768 |  0:21:26s
epoch 50 | loss: 0.81884 | eval_custom_logloss: 1.07874 |  0:21:52s
epoch 51 | loss: 0.8215  | eval_custom_logloss: 1.98795 |  0:22:18s
epoch 52 | loss: 0.80821 | eval_custom_logloss: 3.52108 |  0:22:43s
epoch 53 | loss: 0.80625 | eval_custom_logloss: 3.30428 |  0:23:09s
epoch 54 | loss: 0.80385 | eval_custom_logloss: 3.0599  |  0:23:35s
epoch 55 | loss: 0.83735 | eval_custom_logloss: 2.9749  |  0:24:00s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 0.83657
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8357, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax', 'n_a': 9, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.84188 | eval_custom_logloss: 10.89285|  0:00:25s
epoch 1  | loss: 1.35809 | eval_custom_logloss: 12.47655|  0:00:51s
epoch 2  | loss: 1.1792  | eval_custom_logloss: 11.63182|  0:01:17s
epoch 3  | loss: 1.12969 | eval_custom_logloss: 10.43246|  0:01:42s
epoch 4  | loss: 1.08918 | eval_custom_logloss: 8.49983 |  0:02:08s
epoch 5  | loss: 1.07148 | eval_custom_logloss: 7.40829 |  0:02:34s
epoch 6  | loss: 1.08311 | eval_custom_logloss: 5.11806 |  0:02:59s
epoch 7  | loss: 1.05097 | eval_custom_logloss: 5.14762 |  0:03:25s
epoch 8  | loss: 1.03261 | eval_custom_logloss: 3.98709 |  0:03:51s
epoch 9  | loss: 0.9982  | eval_custom_logloss: 2.97238 |  0:04:17s
epoch 10 | loss: 1.05203 | eval_custom_logloss: 5.28154 |  0:04:42s
epoch 11 | loss: 1.0661  | eval_custom_logloss: 3.83789 |  0:05:08s
epoch 12 | loss: 1.04401 | eval_custom_logloss: 4.7103  |  0:05:34s
epoch 13 | loss: 1.02742 | eval_custom_logloss: 3.72607 |  0:05:59s
epoch 14 | loss: 1.00549 | eval_custom_logloss: 1.93156 |  0:06:24s
epoch 15 | loss: 1.03562 | eval_custom_logloss: 1.60413 |  0:06:50s
epoch 16 | loss: 1.00017 | eval_custom_logloss: 1.83838 |  0:07:15s
epoch 17 | loss: 1.01169 | eval_custom_logloss: 2.80016 |  0:07:40s
epoch 18 | loss: 0.9919  | eval_custom_logloss: 2.16398 |  0:08:06s
epoch 19 | loss: 0.98766 | eval_custom_logloss: 1.63807 |  0:08:31s
epoch 20 | loss: 0.97725 | eval_custom_logloss: 1.05373 |  0:08:56s
epoch 21 | loss: 0.97091 | eval_custom_logloss: 1.23393 |  0:09:21s
epoch 22 | loss: 0.97511 | eval_custom_logloss: 1.69785 |  0:09:47s
epoch 23 | loss: 0.95627 | eval_custom_logloss: 1.18979 |  0:10:13s
epoch 24 | loss: 0.94898 | eval_custom_logloss: 1.11922 |  0:10:38s
epoch 25 | loss: 0.95617 | eval_custom_logloss: 1.42357 |  0:11:04s
epoch 26 | loss: 0.93909 | eval_custom_logloss: 1.96975 |  0:11:30s
epoch 27 | loss: 0.94531 | eval_custom_logloss: 1.67753 |  0:11:56s
epoch 28 | loss: 0.93124 | eval_custom_logloss: 1.75815 |  0:12:21s
epoch 29 | loss: 0.90187 | eval_custom_logloss: 1.81626 |  0:12:47s
epoch 30 | loss: 0.88798 | eval_custom_logloss: 1.90353 |  0:13:13s
epoch 31 | loss: 0.88231 | eval_custom_logloss: 3.15452 |  0:13:38s
epoch 32 | loss: 0.87701 | eval_custom_logloss: 1.32538 |  0:14:04s
epoch 33 | loss: 0.86294 | eval_custom_logloss: 1.11653 |  0:14:30s
epoch 34 | loss: 0.84774 | eval_custom_logloss: 2.73759 |  0:14:55s
epoch 35 | loss: 0.86145 | eval_custom_logloss: 4.03072 |  0:15:21s
epoch 36 | loss: 0.87105 | eval_custom_logloss: 3.40239 |  0:15:46s
epoch 37 | loss: 0.84552 | eval_custom_logloss: 2.95254 |  0:16:12s
epoch 38 | loss: 0.84763 | eval_custom_logloss: 3.37264 |  0:16:38s
epoch 39 | loss: 0.83223 | eval_custom_logloss: 1.41569 |  0:17:04s
epoch 40 | loss: 0.82007 | eval_custom_logloss: 1.4782  |  0:17:29s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 1.05373
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9439500000000001, 'Log Loss - std': 0.10825000000000001} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax', 'n_a': 9, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.84097 | eval_custom_logloss: 10.85851|  0:00:25s
epoch 1  | loss: 1.41437 | eval_custom_logloss: 10.38998|  0:00:51s
epoch 2  | loss: 1.28617 | eval_custom_logloss: 5.0428  |  0:01:16s
epoch 3  | loss: 1.22159 | eval_custom_logloss: 6.7809  |  0:01:42s
epoch 4  | loss: 1.19162 | eval_custom_logloss: 4.81099 |  0:02:07s
epoch 5  | loss: 1.13545 | eval_custom_logloss: 4.16641 |  0:02:33s
epoch 6  | loss: 1.09168 | eval_custom_logloss: 3.21414 |  0:02:58s
epoch 7  | loss: 1.09472 | eval_custom_logloss: 3.97534 |  0:03:24s
epoch 8  | loss: 1.0745  | eval_custom_logloss: 3.68819 |  0:03:50s
epoch 9  | loss: 1.03791 | eval_custom_logloss: 3.67068 |  0:04:15s
epoch 10 | loss: 1.00793 | eval_custom_logloss: 2.18628 |  0:04:41s
epoch 11 | loss: 0.99199 | eval_custom_logloss: 3.17561 |  0:05:06s
epoch 12 | loss: 0.98151 | eval_custom_logloss: 2.12193 |  0:05:32s
epoch 13 | loss: 0.97086 | eval_custom_logloss: 2.1403  |  0:05:57s
epoch 14 | loss: 0.96043 | eval_custom_logloss: 1.93324 |  0:06:23s
epoch 15 | loss: 0.941   | eval_custom_logloss: 1.25464 |  0:06:49s
epoch 16 | loss: 0.91648 | eval_custom_logloss: 1.22744 |  0:07:14s
epoch 17 | loss: 0.92397 | eval_custom_logloss: 1.41193 |  0:07:40s
epoch 18 | loss: 0.89414 | eval_custom_logloss: 2.22892 |  0:08:06s
epoch 19 | loss: 0.93091 | eval_custom_logloss: 3.85395 |  0:08:31s
epoch 20 | loss: 0.89337 | eval_custom_logloss: 1.9294  |  0:08:57s
epoch 21 | loss: 0.88472 | eval_custom_logloss: 1.1334  |  0:09:22s
epoch 22 | loss: 0.85241 | eval_custom_logloss: 1.25522 |  0:09:48s
epoch 23 | loss: 0.8425  | eval_custom_logloss: 1.15344 |  0:10:14s
epoch 24 | loss: 0.82503 | eval_custom_logloss: 1.68048 |  0:10:39s
epoch 25 | loss: 0.81143 | eval_custom_logloss: 3.17709 |  0:11:05s
epoch 26 | loss: 0.80126 | eval_custom_logloss: 3.69114 |  0:11:30s
epoch 27 | loss: 0.78799 | eval_custom_logloss: 3.75503 |  0:11:56s
epoch 28 | loss: 0.7715  | eval_custom_logloss: 1.99888 |  0:12:21s
epoch 29 | loss: 0.75543 | eval_custom_logloss: 1.47695 |  0:12:47s
epoch 30 | loss: 0.75724 | eval_custom_logloss: 1.47276 |  0:13:13s
epoch 31 | loss: 0.76166 | eval_custom_logloss: 2.22011 |  0:13:39s
epoch 32 | loss: 0.7478  | eval_custom_logloss: 1.58087 |  0:14:04s
epoch 33 | loss: 0.75051 | eval_custom_logloss: 2.02332 |  0:14:30s
epoch 34 | loss: 0.73971 | eval_custom_logloss: 1.6072  |  0:14:55s
epoch 35 | loss: 0.74399 | eval_custom_logloss: 0.94373 |  0:15:21s
epoch 36 | loss: 0.73864 | eval_custom_logloss: 2.51167 |  0:15:47s
epoch 37 | loss: 0.73517 | eval_custom_logloss: 1.41354 |  0:16:12s
epoch 38 | loss: 0.74455 | eval_custom_logloss: 0.95053 |  0:16:38s
epoch 39 | loss: 0.74884 | eval_custom_logloss: 2.00619 |  0:17:03s
epoch 40 | loss: 0.73258 | eval_custom_logloss: 6.47293 |  0:17:29s
epoch 41 | loss: 0.73513 | eval_custom_logloss: 1.56988 |  0:17:55s
epoch 42 | loss: 0.72775 | eval_custom_logloss: 0.86957 |  0:18:20s
epoch 43 | loss: 0.70853 | eval_custom_logloss: 2.69821 |  0:18:46s
epoch 44 | loss: 0.71679 | eval_custom_logloss: 1.54137 |  0:19:12s
epoch 45 | loss: 0.70802 | eval_custom_logloss: 1.15517 |  0:19:38s
epoch 46 | loss: 0.7143  | eval_custom_logloss: 1.5843  |  0:20:03s
epoch 47 | loss: 0.70794 | eval_custom_logloss: 1.37811 |  0:20:29s
epoch 48 | loss: 0.71596 | eval_custom_logloss: 2.78239 |  0:20:54s
epoch 49 | loss: 0.70461 | eval_custom_logloss: 1.07738 |  0:21:20s
epoch 50 | loss: 0.69805 | eval_custom_logloss: 1.19904 |  0:21:45s
epoch 51 | loss: 0.7054  | eval_custom_logloss: 1.23077 |  0:22:11s
epoch 52 | loss: 0.70166 | eval_custom_logloss: 1.3325  |  0:22:37s
epoch 53 | loss: 0.70256 | eval_custom_logloss: 0.82323 |  0:23:02s
epoch 54 | loss: 0.68704 | eval_custom_logloss: 1.09737 |  0:23:28s
epoch 55 | loss: 0.69368 | eval_custom_logloss: 1.02622 |  0:23:53s
epoch 56 | loss: 0.68519 | eval_custom_logloss: 2.01028 |  0:24:19s
epoch 57 | loss: 0.70208 | eval_custom_logloss: 1.12428 |  0:24:44s
epoch 58 | loss: 0.68737 | eval_custom_logloss: 1.41621 |  0:25:10s
epoch 59 | loss: 0.68901 | eval_custom_logloss: 2.38127 |  0:25:36s
epoch 60 | loss: 0.68639 | eval_custom_logloss: 0.88899 |  0:26:01s
epoch 61 | loss: 0.69198 | eval_custom_logloss: 2.582   |  0:26:27s
epoch 62 | loss: 0.67688 | eval_custom_logloss: 1.06679 |  0:26:52s
epoch 63 | loss: 0.68196 | eval_custom_logloss: 2.42611 |  0:27:18s
epoch 64 | loss: 0.68228 | eval_custom_logloss: 1.47132 |  0:27:43s
epoch 65 | loss: 0.67921 | eval_custom_logloss: 1.00023 |  0:28:09s
epoch 66 | loss: 0.69135 | eval_custom_logloss: 1.37417 |  0:28:34s
epoch 67 | loss: 0.69443 | eval_custom_logloss: 1.33329 |  0:28:59s
epoch 68 | loss: 0.67346 | eval_custom_logloss: 0.76266 |  0:29:25s
epoch 69 | loss: 0.67438 | eval_custom_logloss: 0.95516 |  0:29:50s
epoch 70 | loss: 0.67324 | eval_custom_logloss: 1.33619 |  0:30:15s
epoch 71 | loss: 0.67122 | eval_custom_logloss: 0.99396 |  0:30:40s
epoch 72 | loss: 0.67193 | eval_custom_logloss: 0.80999 |  0:31:05s
epoch 73 | loss: 0.66664 | eval_custom_logloss: 0.79356 |  0:31:30s
epoch 74 | loss: 0.66819 | eval_custom_logloss: 0.71665 |  0:31:56s
epoch 75 | loss: 0.69542 | eval_custom_logloss: 1.15575 |  0:32:21s
epoch 76 | loss: 0.82123 | eval_custom_logloss: 2.15924 |  0:32:46s
epoch 77 | loss: 0.76275 | eval_custom_logloss: 1.79447 |  0:33:12s
epoch 78 | loss: 0.71598 | eval_custom_logloss: 1.54573 |  0:33:37s
epoch 79 | loss: 0.71323 | eval_custom_logloss: 1.21102 |  0:34:03s
epoch 80 | loss: 0.69213 | eval_custom_logloss: 0.73518 |  0:34:28s
epoch 81 | loss: 0.68108 | eval_custom_logloss: 1.13373 |  0:34:54s
epoch 82 | loss: 0.69511 | eval_custom_logloss: 0.78652 |  0:35:20s
epoch 83 | loss: 0.65685 | eval_custom_logloss: 0.78796 |  0:35:45s
epoch 84 | loss: 0.65798 | eval_custom_logloss: 0.81808 |  0:36:11s
epoch 85 | loss: 0.66471 | eval_custom_logloss: 0.95711 |  0:36:36s
epoch 86 | loss: 0.67443 | eval_custom_logloss: 0.88476 |  0:37:02s
epoch 87 | loss: 0.68105 | eval_custom_logloss: 1.31327 |  0:37:28s
epoch 88 | loss: 0.65645 | eval_custom_logloss: 1.50186 |  0:37:53s
epoch 89 | loss: 0.64944 | eval_custom_logloss: 0.99427 |  0:38:19s
epoch 90 | loss: 0.6578  | eval_custom_logloss: 0.84944 |  0:38:44s
epoch 91 | loss: 0.66458 | eval_custom_logloss: 1.38073 |  0:39:10s
epoch 92 | loss: 0.65304 | eval_custom_logloss: 0.89712 |  0:39:36s
epoch 93 | loss: 0.6536  | eval_custom_logloss: 1.52194 |  0:40:01s
epoch 94 | loss: 0.65951 | eval_custom_logloss: 1.36692 |  0:40:27s

Early stopping occurred at epoch 94 with best_epoch = 74 and best_eval_custom_logloss = 0.71665
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8674333333333334, 'Log Loss - std': 0.13971987053466023} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax', 'n_a': 9, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.85812 | eval_custom_logloss: 11.82341|  0:00:25s
epoch 1  | loss: 1.4634  | eval_custom_logloss: 9.68826 |  0:00:51s
epoch 2  | loss: 1.4085  | eval_custom_logloss: 8.94315 |  0:01:16s
epoch 3  | loss: 1.35443 | eval_custom_logloss: 4.61587 |  0:01:42s
epoch 4  | loss: 1.30184 | eval_custom_logloss: 4.56892 |  0:02:08s
epoch 5  | loss: 1.18716 | eval_custom_logloss: 7.0875  |  0:02:33s
epoch 6  | loss: 1.0861  | eval_custom_logloss: 5.59145 |  0:02:59s
epoch 7  | loss: 1.08803 | eval_custom_logloss: 5.60937 |  0:03:25s
epoch 8  | loss: 1.07032 | eval_custom_logloss: 4.06663 |  0:03:51s
epoch 9  | loss: 1.02968 | eval_custom_logloss: 2.95017 |  0:04:16s
epoch 10 | loss: 1.01709 | eval_custom_logloss: 2.32302 |  0:04:42s
epoch 11 | loss: 0.99918 | eval_custom_logloss: 1.33688 |  0:05:08s
epoch 12 | loss: 0.99129 | eval_custom_logloss: 1.58417 |  0:05:34s
epoch 13 | loss: 0.99377 | eval_custom_logloss: 2.55087 |  0:06:00s
epoch 14 | loss: 0.96542 | eval_custom_logloss: 2.62241 |  0:06:26s
epoch 15 | loss: 0.98923 | eval_custom_logloss: 1.3782  |  0:06:52s
epoch 16 | loss: 0.9611  | eval_custom_logloss: 2.33683 |  0:07:17s
epoch 17 | loss: 0.96308 | eval_custom_logloss: 1.52918 |  0:07:43s
epoch 18 | loss: 0.96135 | eval_custom_logloss: 1.5889  |  0:08:09s
epoch 19 | loss: 0.96827 | eval_custom_logloss: 2.05652 |  0:08:35s
epoch 20 | loss: 0.93911 | eval_custom_logloss: 1.92723 |  0:09:01s
epoch 21 | loss: 0.95048 | eval_custom_logloss: 2.3867  |  0:09:27s
epoch 22 | loss: 0.92574 | eval_custom_logloss: 1.61281 |  0:09:52s
epoch 23 | loss: 0.91887 | eval_custom_logloss: 1.55178 |  0:10:18s
epoch 24 | loss: 0.90414 | eval_custom_logloss: 1.43787 |  0:10:43s
epoch 25 | loss: 0.92004 | eval_custom_logloss: 1.53775 |  0:11:09s
epoch 26 | loss: 0.89513 | eval_custom_logloss: 1.47518 |  0:11:34s
epoch 27 | loss: 0.90433 | eval_custom_logloss: 1.21746 |  0:12:00s
epoch 28 | loss: 0.91209 | eval_custom_logloss: 1.38939 |  0:12:26s
epoch 29 | loss: 0.91278 | eval_custom_logloss: 1.58205 |  0:12:51s
epoch 30 | loss: 0.93467 | eval_custom_logloss: 1.85498 |  0:13:17s
epoch 31 | loss: 0.92588 | eval_custom_logloss: 1.32537 |  0:13:43s
epoch 32 | loss: 0.919   | eval_custom_logloss: 1.30764 |  0:14:08s
epoch 33 | loss: 0.90579 | eval_custom_logloss: 1.25515 |  0:14:34s
epoch 34 | loss: 0.89868 | eval_custom_logloss: 1.66831 |  0:15:00s
epoch 35 | loss: 0.88409 | eval_custom_logloss: 1.08581 |  0:15:25s
epoch 36 | loss: 0.86676 | eval_custom_logloss: 1.23503 |  0:15:51s
epoch 37 | loss: 0.85628 | eval_custom_logloss: 1.22212 |  0:16:17s
epoch 38 | loss: 0.8662  | eval_custom_logloss: 0.85757 |  0:16:43s
epoch 39 | loss: 0.85683 | eval_custom_logloss: 2.05777 |  0:17:09s
epoch 40 | loss: 0.84779 | eval_custom_logloss: 1.31614 |  0:17:35s
epoch 41 | loss: 0.82472 | eval_custom_logloss: 3.32054 |  0:18:00s
epoch 42 | loss: 0.81758 | eval_custom_logloss: 2.53015 |  0:18:26s
epoch 43 | loss: 0.82324 | eval_custom_logloss: 2.27726 |  0:18:52s
epoch 44 | loss: 0.79708 | eval_custom_logloss: 3.39135 |  0:19:17s
epoch 45 | loss: 0.80428 | eval_custom_logloss: 4.00432 |  0:19:43s
epoch 46 | loss: 0.80156 | eval_custom_logloss: 2.76231 |  0:20:08s
epoch 47 | loss: 0.78681 | eval_custom_logloss: 1.79078 |  0:20:34s
epoch 48 | loss: 0.77668 | eval_custom_logloss: 3.89251 |  0:21:00s
epoch 49 | loss: 0.76775 | eval_custom_logloss: 2.14839 |  0:21:25s
epoch 50 | loss: 0.76959 | eval_custom_logloss: 1.82073 |  0:21:50s
epoch 51 | loss: 0.77161 | eval_custom_logloss: 1.70363 |  0:22:16s
epoch 52 | loss: 0.76055 | eval_custom_logloss: 3.22123 |  0:22:42s
epoch 53 | loss: 0.74998 | eval_custom_logloss: 2.05937 |  0:23:07s
epoch 54 | loss: 0.73809 | eval_custom_logloss: 1.36709 |  0:23:33s
epoch 55 | loss: 0.73561 | eval_custom_logloss: 1.39371 |  0:23:59s
epoch 56 | loss: 0.75395 | eval_custom_logloss: 2.0721  |  0:24:24s
epoch 57 | loss: 0.74575 | eval_custom_logloss: 1.48062 |  0:24:50s
epoch 58 | loss: 0.74459 | eval_custom_logloss: 1.20596 |  0:25:16s

Early stopping occurred at epoch 58 with best_epoch = 38 and best_eval_custom_logloss = 0.85757
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.86485, 'Log Loss - std': 0.12108365909568475} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax', 'n_a': 9, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.7679  | eval_custom_logloss: 10.72046|  0:00:25s
epoch 1  | loss: 1.24276 | eval_custom_logloss: 10.08562|  0:00:50s
epoch 2  | loss: 1.16387 | eval_custom_logloss: 8.47512 |  0:01:16s
epoch 3  | loss: 1.16808 | eval_custom_logloss: 4.76144 |  0:01:42s
epoch 4  | loss: 1.15713 | eval_custom_logloss: 5.86478 |  0:02:07s
epoch 5  | loss: 1.08986 | eval_custom_logloss: 4.53991 |  0:02:32s
epoch 6  | loss: 1.07583 | eval_custom_logloss: 7.02196 |  0:02:58s
epoch 7  | loss: 1.04413 | eval_custom_logloss: 8.41557 |  0:03:23s
epoch 8  | loss: 1.03328 | eval_custom_logloss: 4.33301 |  0:03:49s
epoch 9  | loss: 1.02621 | eval_custom_logloss: 2.12235 |  0:04:14s
epoch 10 | loss: 1.04472 | eval_custom_logloss: 1.22257 |  0:04:39s
epoch 11 | loss: 1.0009  | eval_custom_logloss: 1.26076 |  0:05:04s
epoch 12 | loss: 0.98612 | eval_custom_logloss: 3.64573 |  0:05:29s
epoch 13 | loss: 0.98276 | eval_custom_logloss: 4.45472 |  0:05:54s
epoch 14 | loss: 0.95816 | eval_custom_logloss: 2.45202 |  0:06:19s
epoch 15 | loss: 0.997   | eval_custom_logloss: 1.65593 |  0:06:44s
epoch 16 | loss: 0.96511 | eval_custom_logloss: 2.8001  |  0:07:09s
epoch 17 | loss: 0.96448 | eval_custom_logloss: 1.29004 |  0:07:35s
epoch 18 | loss: 0.95067 | eval_custom_logloss: 0.92843 |  0:08:00s
epoch 19 | loss: 0.94754 | eval_custom_logloss: 1.38453 |  0:08:26s
epoch 20 | loss: 0.91971 | eval_custom_logloss: 0.97758 |  0:08:52s
epoch 21 | loss: 0.93362 | eval_custom_logloss: 1.85694 |  0:09:17s
epoch 22 | loss: 0.9448  | eval_custom_logloss: 1.55082 |  0:09:43s
epoch 23 | loss: 0.90835 | eval_custom_logloss: 0.97536 |  0:10:08s
epoch 24 | loss: 0.92605 | eval_custom_logloss: 1.29047 |  0:10:34s
epoch 25 | loss: 0.95176 | eval_custom_logloss: 2.08806 |  0:11:00s
epoch 26 | loss: 0.9085  | eval_custom_logloss: 1.55413 |  0:11:25s
epoch 27 | loss: 0.91037 | eval_custom_logloss: 1.0089  |  0:11:51s
epoch 28 | loss: 0.88511 | eval_custom_logloss: 0.90896 |  0:12:16s
epoch 29 | loss: 0.89069 | eval_custom_logloss: 0.91287 |  0:12:42s
epoch 30 | loss: 0.88501 | eval_custom_logloss: 1.11654 |  0:13:08s
epoch 31 | loss: 0.86139 | eval_custom_logloss: 2.07256 |  0:13:33s
epoch 32 | loss: 0.884   | eval_custom_logloss: 2.85386 |  0:13:59s
epoch 33 | loss: 0.8482  | eval_custom_logloss: 2.4314  |  0:14:24s
epoch 34 | loss: 0.84905 | eval_custom_logloss: 2.647   |  0:14:50s
epoch 35 | loss: 0.8557  | eval_custom_logloss: 1.95256 |  0:15:15s
epoch 36 | loss: 0.8888  | eval_custom_logloss: 1.84847 |  0:15:41s
epoch 37 | loss: 0.85809 | eval_custom_logloss: 1.02498 |  0:16:07s
epoch 38 | loss: 0.85429 | eval_custom_logloss: 1.02371 |  0:16:32s
epoch 39 | loss: 0.85509 | eval_custom_logloss: 1.54698 |  0:16:58s
epoch 40 | loss: 0.84674 | eval_custom_logloss: 1.47924 |  0:17:24s
epoch 41 | loss: 0.82504 | eval_custom_logloss: 1.73455 |  0:17:49s
epoch 42 | loss: 0.83548 | eval_custom_logloss: 0.80092 |  0:18:15s
epoch 43 | loss: 0.83375 | eval_custom_logloss: 0.84882 |  0:18:41s
epoch 44 | loss: 0.81965 | eval_custom_logloss: 1.09529 |  0:19:07s
epoch 45 | loss: 0.84855 | eval_custom_logloss: 1.06579 |  0:19:32s
epoch 46 | loss: 0.83691 | eval_custom_logloss: 1.22185 |  0:19:58s
epoch 47 | loss: 0.83695 | eval_custom_logloss: 1.34127 |  0:20:23s
epoch 48 | loss: 0.8214  | eval_custom_logloss: 0.84205 |  0:20:49s
epoch 49 | loss: 0.80716 | eval_custom_logloss: 0.92063 |  0:21:15s
epoch 50 | loss: 0.8244  | eval_custom_logloss: 0.84795 |  0:21:40s
epoch 51 | loss: 0.81782 | eval_custom_logloss: 0.98188 |  0:22:06s
epoch 52 | loss: 0.81348 | eval_custom_logloss: 0.92489 |  0:22:32s
epoch 53 | loss: 0.80544 | eval_custom_logloss: 0.85867 |  0:22:58s
epoch 54 | loss: 0.80937 | eval_custom_logloss: 0.96614 |  0:23:23s
epoch 55 | loss: 0.82506 | eval_custom_logloss: 0.84914 |  0:23:49s
epoch 56 | loss: 0.82134 | eval_custom_logloss: 0.92288 |  0:24:14s
epoch 57 | loss: 0.80924 | eval_custom_logloss: 1.05092 |  0:24:40s
epoch 58 | loss: 0.814   | eval_custom_logloss: 1.65947 |  0:25:06s
epoch 59 | loss: 0.81781 | eval_custom_logloss: 1.3088  |  0:25:31s
epoch 60 | loss: 0.80599 | eval_custom_logloss: 0.80124 |  0:25:57s
epoch 61 | loss: 0.79816 | eval_custom_logloss: 0.85936 |  0:26:23s
epoch 62 | loss: 0.80449 | eval_custom_logloss: 0.81965 |  0:26:48s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.80092
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8518399999999999, 'Log Loss - std': 0.11138241512913967} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 34 finished with value: 0.8518399999999999 and parameters: {'n_d': 9, 'n_steps': 8, 'gamma': 1.7434722882409945, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.0017787527377500561, 'mask_type': 'sparsemax'}. Best is trial 18 with value: 0.8880799999999999.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.71634 | eval_custom_logloss: 13.77572|  0:00:27s
epoch 1  | loss: 1.31559 | eval_custom_logloss: 12.99323|  0:00:55s
epoch 2  | loss: 1.2083  | eval_custom_logloss: 12.01843|  0:01:23s
epoch 3  | loss: 1.17358 | eval_custom_logloss: 11.1964 |  0:01:50s
epoch 4  | loss: 1.155   | eval_custom_logloss: 9.28375 |  0:02:18s
epoch 5  | loss: 1.06469 | eval_custom_logloss: 9.7712  |  0:02:46s
epoch 6  | loss: 1.02498 | eval_custom_logloss: 8.57917 |  0:03:14s
epoch 7  | loss: 0.9899  | eval_custom_logloss: 9.4213  |  0:03:42s
epoch 8  | loss: 1.03933 | eval_custom_logloss: 9.20009 |  0:04:10s
epoch 9  | loss: 0.95885 | eval_custom_logloss: 9.58406 |  0:04:38s
epoch 10 | loss: 0.9359  | eval_custom_logloss: 9.57969 |  0:05:05s
epoch 11 | loss: 0.90168 | eval_custom_logloss: 11.20276|  0:05:33s
epoch 12 | loss: 0.90945 | eval_custom_logloss: 7.81984 |  0:06:01s
epoch 13 | loss: 0.83935 | eval_custom_logloss: 8.51784 |  0:06:29s
epoch 14 | loss: 0.85057 | eval_custom_logloss: 7.25794 |  0:06:56s
epoch 15 | loss: 0.85554 | eval_custom_logloss: 6.66054 |  0:07:24s
epoch 16 | loss: 0.8437  | eval_custom_logloss: 5.00133 |  0:07:52s
epoch 17 | loss: 0.8291  | eval_custom_logloss: 5.82717 |  0:08:19s
epoch 18 | loss: 0.84186 | eval_custom_logloss: 6.52961 |  0:08:47s
epoch 19 | loss: 0.8081  | eval_custom_logloss: 3.5189  |  0:09:15s
epoch 20 | loss: 0.80725 | eval_custom_logloss: 4.45833 |  0:09:43s
epoch 21 | loss: 0.80528 | eval_custom_logloss: 3.32485 |  0:10:10s
epoch 22 | loss: 0.79056 | eval_custom_logloss: 4.12147 |  0:10:38s
epoch 23 | loss: 0.77667 | eval_custom_logloss: 4.17325 |  0:11:06s
epoch 24 | loss: 0.79346 | eval_custom_logloss: 4.4952  |  0:11:33s
epoch 25 | loss: 0.78539 | eval_custom_logloss: 2.94615 |  0:12:01s
epoch 26 | loss: 0.76764 | eval_custom_logloss: 3.21969 |  0:12:30s
epoch 27 | loss: 0.7668  | eval_custom_logloss: 3.38045 |  0:12:57s
epoch 28 | loss: 0.77935 | eval_custom_logloss: 2.71734 |  0:13:25s
epoch 29 | loss: 0.76843 | eval_custom_logloss: 1.78674 |  0:13:53s
epoch 30 | loss: 0.75347 | eval_custom_logloss: 1.58475 |  0:14:21s
epoch 31 | loss: 0.75685 | eval_custom_logloss: 3.03899 |  0:14:49s
epoch 32 | loss: 0.73738 | eval_custom_logloss: 2.12171 |  0:15:17s
epoch 33 | loss: 0.74086 | eval_custom_logloss: 2.15388 |  0:15:45s
epoch 34 | loss: 0.74144 | eval_custom_logloss: 2.62762 |  0:16:13s
epoch 35 | loss: 0.7384  | eval_custom_logloss: 1.80445 |  0:16:41s
epoch 36 | loss: 0.72641 | eval_custom_logloss: 1.32907 |  0:17:09s
epoch 37 | loss: 0.71468 | eval_custom_logloss: 2.28487 |  0:17:37s
epoch 38 | loss: 0.72449 | eval_custom_logloss: 3.17903 |  0:18:04s
epoch 39 | loss: 0.72413 | eval_custom_logloss: 2.07543 |  0:18:32s
epoch 40 | loss: 0.73244 | eval_custom_logloss: 2.59468 |  0:19:00s
epoch 41 | loss: 0.70342 | eval_custom_logloss: 3.59573 |  0:19:28s
epoch 42 | loss: 0.71445 | eval_custom_logloss: 2.35495 |  0:19:56s
epoch 43 | loss: 0.70478 | eval_custom_logloss: 2.85648 |  0:20:24s
epoch 44 | loss: 0.70013 | eval_custom_logloss: 2.41462 |  0:20:52s
epoch 45 | loss: 0.70527 | eval_custom_logloss: 1.97633 |  0:21:19s
epoch 46 | loss: 0.69109 | eval_custom_logloss: 1.029   |  0:21:47s
epoch 47 | loss: 0.69208 | eval_custom_logloss: 2.14293 |  0:22:15s
epoch 48 | loss: 0.68767 | eval_custom_logloss: 1.42165 |  0:22:43s
epoch 49 | loss: 0.69892 | eval_custom_logloss: 1.48806 |  0:23:11s
epoch 50 | loss: 0.69536 | eval_custom_logloss: 1.92666 |  0:23:39s
epoch 51 | loss: 0.68656 | eval_custom_logloss: 1.60684 |  0:24:08s
epoch 52 | loss: 0.6915  | eval_custom_logloss: 1.04735 |  0:24:36s
epoch 53 | loss: 0.68604 | eval_custom_logloss: 0.74099 |  0:25:04s
epoch 54 | loss: 0.69275 | eval_custom_logloss: 1.61559 |  0:25:32s
epoch 55 | loss: 0.69222 | eval_custom_logloss: 2.36955 |  0:26:00s
epoch 56 | loss: 0.68268 | eval_custom_logloss: 0.93672 |  0:26:29s
epoch 57 | loss: 0.68007 | eval_custom_logloss: 2.12157 |  0:26:57s
epoch 58 | loss: 0.68379 | eval_custom_logloss: 1.34933 |  0:27:25s
epoch 59 | loss: 0.67737 | eval_custom_logloss: 1.19023 |  0:27:53s
epoch 60 | loss: 0.67656 | eval_custom_logloss: 0.83547 |  0:28:21s
epoch 61 | loss: 0.67714 | eval_custom_logloss: 1.63745 |  0:28:49s
epoch 62 | loss: 0.68239 | eval_custom_logloss: 1.55233 |  0:29:17s
epoch 63 | loss: 0.67692 | eval_custom_logloss: 0.8119  |  0:29:45s
epoch 64 | loss: 0.68021 | eval_custom_logloss: 1.02206 |  0:30:13s
epoch 65 | loss: 0.67733 | eval_custom_logloss: 3.38182 |  0:30:41s
epoch 66 | loss: 0.6807  | eval_custom_logloss: 2.36774 |  0:31:09s
epoch 67 | loss: 0.66307 | eval_custom_logloss: 0.87854 |  0:31:37s
epoch 68 | loss: 0.67662 | eval_custom_logloss: 1.00103 |  0:32:05s
epoch 69 | loss: 0.66295 | eval_custom_logloss: 1.1699  |  0:32:33s
epoch 70 | loss: 0.6677  | eval_custom_logloss: 1.55123 |  0:33:01s
epoch 71 | loss: 0.66826 | eval_custom_logloss: 1.36    |  0:33:30s
epoch 72 | loss: 0.65999 | eval_custom_logloss: 1.22613 |  0:33:58s
epoch 73 | loss: 0.65896 | eval_custom_logloss: 1.58846 |  0:34:26s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.74099
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7405, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 'Premium' 'E' 'SI1' -1.3617245236890496]
 [-1.1991826034964022 'Good' 'E' 'VS1' -3.3871902928108906]
 [-1.0725306248271413 'Premium' 'I' 'VS2' 0.4542103037994991]
 [-1.0303132986040544 'Good' 'J' 'SI2' 1.08280312869938]
 [-1.1780739403848586 'Very Good' 'J' 'VVS2' 0.73358489264389]
 [-1.1780739403848586 'Very Good' 'I' 'VVS1' 0.38436665658840014]
 [-1.1358566141617716 'Very Good' 'H' 'SI1' 0.10499206774400914]
 [-1.2202912666079455 'Fair' 'E' 'VS2' 2.339988778499142]
 [-1.1991826034964022 'Very Good' 'H' 'VS1' -1.6410991125334407]
 [-1.051421961715598 'Good' 'J' 'SI1' 1.571708659177068]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.241399929719489 4 6 3 -1.3617245236890496 1.5949518123380095
  -1.6437879201752599 -1.6514473425787888 -1.7685832176092422]
 [-1.1991826034964022 2 6 5 -3.3871902928108906 3.3941876961159525
  -1.5010019420077028 -1.4510608771835314 -1.7685832176092422]
 [-1.0725306248271413 4 2 4 0.4542103037994991 0.24552489950455236
  -1.3671400874756174 -1.3116615969085696 -1.307955447116008]
 [-1.0303132986040544 2 1 2 1.08280312869938 0.24552489950455236
  -1.242202356579005 -1.2071121367023494 -1.135220033181045]
 [-1.1780739403848586 3 1 6 0.73358489264389 -0.20428407143993338
  -1.5991673019978982 -1.5468978823725676 -1.5238747145347116]
 [-1.1780739403848586 3 2 7 0.38436665658840014 -0.20428407143993338
  -1.5902431783624258 -1.5294729723381975 -1.5382693323626249]
 [-1.1358566141617716 3 3 3 0.10499206774400914 -1.1039020133289048
  -1.4831536947367576 -1.416211057114791 -1.4519016253951438]
 [-1.2202912666079455 1 6 4 2.339988778499142 1.5949518123380095
  -1.6616361674462046 -1.7037220726818993 -1.5094800967067976]
 [-1.1991826034964022 3 3 5 -1.6410991125334407 1.5949518123380095
  -1.5456225601850642 -1.4684857872179018 -1.6534262749859334]
 [-1.051421961715598 2 1 3 1.571708659177068 -1.1039020133289048
  -1.3225194692982558 -1.2680993218226444 -1.164009268836872]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.79232 | eval_custom_logloss: 13.58526|  0:00:28s
epoch 1  | loss: 1.32908 | eval_custom_logloss: 11.75843|  0:00:56s
epoch 2  | loss: 1.23149 | eval_custom_logloss: 11.71505|  0:01:24s
epoch 3  | loss: 1.14146 | eval_custom_logloss: 11.89395|  0:01:52s
epoch 4  | loss: 1.12038 | eval_custom_logloss: 12.68829|  0:02:20s
epoch 5  | loss: 1.04363 | eval_custom_logloss: 12.33112|  0:02:48s
epoch 6  | loss: 1.0083  | eval_custom_logloss: 12.61139|  0:03:16s
epoch 7  | loss: 0.9827  | eval_custom_logloss: 12.27257|  0:03:44s
epoch 8  | loss: 0.97371 | eval_custom_logloss: 12.74594|  0:04:13s
epoch 9  | loss: 0.92872 | eval_custom_logloss: 11.00282|  0:04:41s
epoch 10 | loss: 0.90042 | eval_custom_logloss: 10.70781|  0:05:09s
epoch 11 | loss: 0.8817  | eval_custom_logloss: 10.22296|  0:05:37s
epoch 12 | loss: 0.88649 | eval_custom_logloss: 10.49706|  0:06:05s
epoch 13 | loss: 0.85293 | eval_custom_logloss: 9.004   |  0:06:33s
epoch 14 | loss: 0.85648 | eval_custom_logloss: 10.39953|  0:07:01s
epoch 15 | loss: 0.85088 | eval_custom_logloss: 6.60962 |  0:07:29s
epoch 16 | loss: 0.83047 | eval_custom_logloss: 4.99452 |  0:07:57s
epoch 17 | loss: 0.82045 | eval_custom_logloss: 7.86661 |  0:08:25s
epoch 18 | loss: 0.79915 | eval_custom_logloss: 6.16996 |  0:08:53s
epoch 19 | loss: 0.79245 | eval_custom_logloss: 4.16168 |  0:09:21s
epoch 20 | loss: 0.78521 | eval_custom_logloss: 5.64064 |  0:09:49s
epoch 21 | loss: 0.78724 | eval_custom_logloss: 5.39628 |  0:10:17s
epoch 22 | loss: 0.79405 | eval_custom_logloss: 8.05403 |  0:10:45s
epoch 23 | loss: 0.77236 | eval_custom_logloss: 7.76785 |  0:11:13s
epoch 24 | loss: 0.76762 | eval_custom_logloss: 5.57899 |  0:11:41s
epoch 25 | loss: 0.76301 | eval_custom_logloss: 4.43283 |  0:12:09s
epoch 26 | loss: 0.77033 | eval_custom_logloss: 7.49675 |  0:12:37s
epoch 27 | loss: 0.76532 | eval_custom_logloss: 5.36187 |  0:13:05s
epoch 28 | loss: 0.75464 | eval_custom_logloss: 5.17547 |  0:13:33s
epoch 29 | loss: 0.75682 | eval_custom_logloss: 4.56656 |  0:14:01s
epoch 30 | loss: 0.7493  | eval_custom_logloss: 5.64819 |  0:14:29s
epoch 31 | loss: 0.74271 | eval_custom_logloss: 4.88104 |  0:14:57s
epoch 32 | loss: 0.74267 | eval_custom_logloss: 5.05247 |  0:15:25s
epoch 33 | loss: 0.74107 | eval_custom_logloss: 5.99296 |  0:15:53s
epoch 34 | loss: 0.74197 | eval_custom_logloss: 9.41059 |  0:16:21s
epoch 35 | loss: 0.74625 | eval_custom_logloss: 10.21898|  0:16:49s
epoch 36 | loss: 0.74152 | eval_custom_logloss: 8.35012 |  0:17:17s
epoch 37 | loss: 0.7362  | eval_custom_logloss: 7.41229 |  0:17:45s
epoch 38 | loss: 0.73662 | eval_custom_logloss: 10.04709|  0:18:13s
epoch 39 | loss: 0.733   | eval_custom_logloss: 7.01703 |  0:18:41s

Early stopping occurred at epoch 39 with best_epoch = 19 and best_eval_custom_logloss = 4.16168
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2952500000000002, 'Log Loss - std': 1.55475} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 'Ideal' 'E' 'SI2' -0.17299106375200937]
 [-1.2431374791872112 'Premium' 'E' 'SI1' -1.358384470113667]
 [-1.2007760315615899 'Good' 'E' 'VS1' -3.380526163318843]
 [-1.0736916886847254 'Premium' 'I' 'VS2' 0.45457015138063084]
 [-1.031330241059104 'Good' 'J' 'SI2' 1.082131366513271]
 [-1.179595307748779 'Very Good' 'J' 'VVS2' 0.733486246995137]
 [-1.179595307748779 'Very Good' 'I' 'VVS1' 0.38484112747700305]
 [-1.2219567553744004 'Fair' 'E' 'VS2' 2.3372537967785516]
 [-1.2007760315615899 'Very Good' 'H' 'VS1' -1.6373005657281732]
 [-1.0525109648719149 'Good' 'J' 'SI1' 1.5702345338386605]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007760315615899 5 6 2 -0.17299106375200937 -1.0991811176282293
  -1.5887751661204468 -1.52741996295235 -1.5654913782973074]
 [-1.2431374791872112 4 6 3 -1.358384470113667 1.580885495554982
  -1.642349354189251 -1.6494018572095064 -1.7351080246385664]
 [-1.2007760315615899 2 6 5 -3.380526163318843 3.367596571010456
  -1.4994848526724398 -1.449003030929892 -1.7351080246385664]
 [-1.0736916886847254 4 2 4 0.45457015138063084 0.2408521889633763
  -1.3655493825004286 -1.309595151778856 -1.282796967728543]
 [-1.031330241059104 2 1 2 1.082131366513271 0.2408521889633763
  -1.2405429436732185 -1.2050392424155796 -1.113180321387284]
 [-1.179595307748779 3 1 6 0.733486246995137 -0.20582557990049227
  -1.5977041974652477 -1.5448459478462293 -1.4948177756551164]
 [-1.179595307748779 3 2 7 0.38484112747700305 -0.20582557990049227
  -1.5887751661204468 -1.52741996295235 -1.5089524961835543]
 [-1.2219567553744004 1 6 4 2.3372537967785516 1.580885495554982
  -1.6602074168788525 -1.701679811891145 -1.4806830551266779]
 [-1.2007760315615899 3 3 5 -1.6373005657281732 1.580885495554982
  -1.5441300093964432 -1.4664290158237718 -1.6220302604110604]
 [-1.0525109648719149 2 1 3 1.5702345338386605 -1.0991811176282293
  -1.320904225776425 -1.2660301895441572 -1.1414497624441604]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.80415 | eval_custom_logloss: 13.25852|  0:00:27s
epoch 1  | loss: 1.45744 | eval_custom_logloss: 12.34645|  0:00:55s
epoch 2  | loss: 1.30897 | eval_custom_logloss: 10.31546|  0:01:24s
epoch 3  | loss: 1.25807 | eval_custom_logloss: 10.27211|  0:01:52s
epoch 4  | loss: 1.23074 | eval_custom_logloss: 10.84021|  0:02:20s
epoch 5  | loss: 1.2292  | eval_custom_logloss: 9.97749 |  0:02:48s
epoch 6  | loss: 1.20784 | eval_custom_logloss: 7.89726 |  0:03:16s
epoch 7  | loss: 1.17598 | eval_custom_logloss: 7.42836 |  0:03:44s
epoch 8  | loss: 1.16887 | eval_custom_logloss: 6.75549 |  0:04:12s
epoch 9  | loss: 1.0821  | eval_custom_logloss: 4.74032 |  0:04:40s
epoch 10 | loss: 1.06169 | eval_custom_logloss: 3.71177 |  0:05:08s
epoch 11 | loss: 1.05857 | eval_custom_logloss: 6.06355 |  0:05:36s
epoch 12 | loss: 1.03773 | eval_custom_logloss: 4.56135 |  0:06:03s
epoch 13 | loss: 0.96769 | eval_custom_logloss: 5.50464 |  0:06:31s
epoch 14 | loss: 0.96361 | eval_custom_logloss: 2.99919 |  0:06:58s
epoch 15 | loss: 1.00959 | eval_custom_logloss: 5.74028 |  0:07:26s
epoch 16 | loss: 1.08999 | eval_custom_logloss: 4.28565 |  0:07:54s
epoch 17 | loss: 1.06281 | eval_custom_logloss: 4.29307 |  0:08:21s
epoch 18 | loss: 1.06244 | eval_custom_logloss: 3.15524 |  0:08:49s
epoch 19 | loss: 1.05266 | eval_custom_logloss: 4.6359  |  0:09:16s
epoch 20 | loss: 1.04711 | eval_custom_logloss: 3.27864 |  0:09:44s
epoch 21 | loss: 1.04024 | eval_custom_logloss: 2.05786 |  0:10:13s
epoch 22 | loss: 1.02372 | eval_custom_logloss: 2.13162 |  0:10:41s
epoch 23 | loss: 1.05676 | eval_custom_logloss: 1.87881 |  0:11:09s
epoch 24 | loss: 1.06536 | eval_custom_logloss: 3.24063 |  0:11:37s
epoch 25 | loss: 1.09272 | eval_custom_logloss: 5.74868 |  0:12:05s
epoch 26 | loss: 1.0523  | eval_custom_logloss: 2.88793 |  0:12:33s
epoch 27 | loss: 1.05456 | eval_custom_logloss: 3.24559 |  0:13:01s
epoch 28 | loss: 1.04994 | eval_custom_logloss: 3.30848 |  0:13:29s
epoch 29 | loss: 1.05491 | eval_custom_logloss: 2.09832 |  0:13:57s
epoch 30 | loss: 1.02832 | eval_custom_logloss: 1.78316 |  0:14:25s
epoch 31 | loss: 1.02838 | eval_custom_logloss: 1.33829 |  0:14:53s
epoch 32 | loss: 1.02692 | eval_custom_logloss: 1.28037 |  0:15:21s
epoch 33 | loss: 1.04223 | eval_custom_logloss: 1.27329 |  0:15:49s
epoch 34 | loss: 1.07528 | eval_custom_logloss: 1.84536 |  0:16:17s
epoch 35 | loss: 1.03939 | eval_custom_logloss: 1.55898 |  0:16:45s
epoch 36 | loss: 1.03565 | eval_custom_logloss: 1.56653 |  0:17:13s
epoch 37 | loss: 1.04149 | eval_custom_logloss: 1.25512 |  0:17:41s
epoch 38 | loss: 1.03194 | eval_custom_logloss: 1.32555 |  0:18:09s
epoch 39 | loss: 1.02784 | eval_custom_logloss: 1.28994 |  0:18:37s
epoch 40 | loss: 1.05997 | eval_custom_logloss: 3.47    |  0:19:05s
epoch 41 | loss: 1.06036 | eval_custom_logloss: 1.78547 |  0:19:33s
epoch 42 | loss: 1.0448  | eval_custom_logloss: 1.38854 |  0:20:01s
epoch 43 | loss: 1.04189 | eval_custom_logloss: 1.26577 |  0:20:29s
epoch 44 | loss: 1.0251  | eval_custom_logloss: 1.88675 |  0:20:57s
epoch 45 | loss: 0.9994  | eval_custom_logloss: 1.01799 |  0:21:25s
epoch 46 | loss: 1.001   | eval_custom_logloss: 1.05571 |  0:21:53s
epoch 47 | loss: 1.001   | eval_custom_logloss: 1.18341 |  0:22:21s
epoch 48 | loss: 1.0149  | eval_custom_logloss: 1.53152 |  0:22:49s
epoch 49 | loss: 1.00413 | eval_custom_logloss: 1.34473 |  0:23:17s
epoch 50 | loss: 1.01957 | eval_custom_logloss: 1.07431 |  0:23:45s
epoch 51 | loss: 1.00571 | eval_custom_logloss: 1.15592 |  0:24:13s
epoch 52 | loss: 1.0209  | eval_custom_logloss: 1.26603 |  0:24:42s
epoch 53 | loss: 1.01576 | eval_custom_logloss: 1.67525 |  0:25:10s
epoch 54 | loss: 1.00265 | eval_custom_logloss: 1.61278 |  0:25:37s
epoch 55 | loss: 1.01246 | eval_custom_logloss: 1.20116 |  0:26:05s
epoch 56 | loss: 1.0177  | eval_custom_logloss: 1.03876 |  0:26:33s
epoch 57 | loss: 1.01636 | eval_custom_logloss: 1.58888 |  0:27:01s
epoch 58 | loss: 1.00701 | eval_custom_logloss: 1.11866 |  0:27:29s
epoch 59 | loss: 1.00041 | eval_custom_logloss: 1.68716 |  0:27:57s
epoch 60 | loss: 0.99202 | eval_custom_logloss: 1.0748  |  0:28:25s
epoch 61 | loss: 1.00006 | eval_custom_logloss: 0.97076 |  0:28:53s
epoch 62 | loss: 1.0246  | eval_custom_logloss: 2.11964 |  0:29:21s
epoch 63 | loss: 1.02754 | eval_custom_logloss: 1.34332 |  0:29:49s
epoch 64 | loss: 1.01634 | eval_custom_logloss: 1.22987 |  0:30:18s
epoch 65 | loss: 1.02607 | eval_custom_logloss: 1.41744 |  0:30:46s
epoch 66 | loss: 1.07054 | eval_custom_logloss: 1.08823 |  0:31:15s
epoch 67 | loss: 1.06448 | eval_custom_logloss: 1.10623 |  0:31:43s
epoch 68 | loss: 1.04785 | eval_custom_logloss: 1.05993 |  0:32:12s
epoch 69 | loss: 1.08387 | eval_custom_logloss: 1.07813 |  0:32:41s
epoch 70 | loss: 1.06558 | eval_custom_logloss: 1.1246  |  0:33:10s
epoch 71 | loss: 1.0312  | eval_custom_logloss: 1.05669 |  0:33:39s
epoch 72 | loss: 1.0378  | eval_custom_logloss: 1.02375 |  0:34:07s
epoch 73 | loss: 1.07371 | eval_custom_logloss: 1.06029 |  0:34:35s
epoch 74 | loss: 1.06414 | eval_custom_logloss: 1.10278 |  0:35:03s
epoch 75 | loss: 1.04698 | eval_custom_logloss: 1.12284 |  0:35:31s
epoch 76 | loss: 1.02934 | eval_custom_logloss: 1.16306 |  0:35:59s
epoch 77 | loss: 1.0212  | eval_custom_logloss: 1.26029 |  0:36:27s
epoch 78 | loss: 1.04331 | eval_custom_logloss: 1.23679 |  0:36:56s
epoch 79 | loss: 1.04881 | eval_custom_logloss: 1.4228  |  0:37:24s
epoch 80 | loss: 1.04539 | eval_custom_logloss: 1.09157 |  0:37:52s
epoch 81 | loss: 1.02191 | eval_custom_logloss: 1.03564 |  0:38:20s

Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_custom_logloss = 0.97076
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8530333333333335, 'Log Loss - std': 1.4151358717648124} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 'Ideal' 'E' 'SI2' -0.17623676016550302]
 [-1.2007693433003284 'Good' 'E' 'VS1' -3.3952118794129094]
 [-1.0318275851408616 'Good' 'J' 'SI2' 1.0833621995400016]
 [-1.1796516235303949 'Very Good' 'J' 'VVS2' 0.7334735996218053]
 [-1.1374161839905281 'Very Good' 'H' 'SI1' 0.10367411976905298]
 [-1.2007693433003284 'Very Good' 'H' 'VS1' -1.6457688798219283]
 [-1.0529453049107949 'Good' 'J' 'SI1' 1.5732062394254782]
 [-1.2007693433003284 'Ideal' 'J' 'VS1' 0.7334735996218053]
 [-1.2218870630702616 'Premium' 'F' 'SI1' -0.9459916799855358]
 [-1.2641225026101284 'Premium' 'E' 'SI2' -1.0859471199528112]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.2007693433003284 5 6 2 -0.17623676016550302 -1.1001014896248056
  -1.5915444854651115 -1.5301186577850108 -1.5696115327877844]
 [-1.2007693433003284 2 6 5 -3.3952118794129094 3.3772680777522917
  -1.5023022126050385 -1.4517413942698225 -1.739235168570439]
 [-1.0318275851408616 2 1 2 1.0833621995400016 0.2431093805883235
  -1.2434996213108258 -1.2079010188892378 -1.1172818373673723]
 [-1.1796516235303949 3 1 6 0.7334735996218053 -0.20462757614938623
  -1.600468712751119 -1.5475358274550528 -1.4989350178783452]
 [-1.1374161839905281 3 3 3 0.10367411976905298 -1.1001014896248056
  -1.4844537580330235 -1.416907054929739 -1.428258502968906]
 [-1.2007693433003284 3 3 5 -1.6457688798219283 1.5863202508014527
  -1.5469233490350751 -1.4691585639398648 -1.6261527447153359]
 [-1.0529453049107949 2 1 3 1.5732062394254782 -1.1001014896248056
  -1.3238176668848918 -1.2688611127343836 -1.1455524433311481]
 [-1.2007693433003284 5 1 5 0.7334735996218053 -0.652364532887096
  -1.6093929400371263 -1.5997873364651782 -1.527205623842121]
 [-1.2218870630702616 4 5 3 -0.9459916799855358 1.5863202508014527
  -1.6540140764671631 -1.6520388454753037 -1.7109645626066634]
 [-1.2641225026101284 4 6 2 -1.0859471199528112 2.0340572075391625
  -1.7343321220412289 -1.7304161089904915 -1.7957763804979907]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73944 | eval_custom_logloss: 13.71962|  0:00:27s
epoch 1  | loss: 1.39224 | eval_custom_logloss: 10.44407|  0:00:55s
epoch 2  | loss: 1.24079 | eval_custom_logloss: 11.33962|  0:01:24s
epoch 3  | loss: 1.16175 | eval_custom_logloss: 12.37722|  0:01:52s
epoch 4  | loss: 1.08801 | eval_custom_logloss: 11.58433|  0:02:20s
epoch 5  | loss: 1.06218 | eval_custom_logloss: 10.75952|  0:02:48s
epoch 6  | loss: 1.04381 | eval_custom_logloss: 11.17633|  0:03:16s
epoch 7  | loss: 1.01261 | eval_custom_logloss: 9.16301 |  0:03:45s
epoch 8  | loss: 1.00137 | eval_custom_logloss: 9.41798 |  0:04:13s
epoch 9  | loss: 0.98441 | eval_custom_logloss: 6.56373 |  0:04:42s
epoch 10 | loss: 0.97483 | eval_custom_logloss: 12.19708|  0:05:10s
epoch 11 | loss: 0.96866 | eval_custom_logloss: 7.24261 |  0:05:39s
epoch 12 | loss: 0.98581 | eval_custom_logloss: 6.57704 |  0:06:07s
epoch 13 | loss: 0.94767 | eval_custom_logloss: 3.57173 |  0:06:35s
epoch 14 | loss: 0.93093 | eval_custom_logloss: 5.17735 |  0:07:04s
epoch 15 | loss: 0.92071 | eval_custom_logloss: 2.68258 |  0:07:32s
epoch 16 | loss: 0.90943 | eval_custom_logloss: 4.97141 |  0:08:00s
epoch 17 | loss: 0.88493 | eval_custom_logloss: 4.30319 |  0:08:29s
epoch 18 | loss: 0.86436 | eval_custom_logloss: 8.7483  |  0:08:57s
epoch 19 | loss: 0.84249 | eval_custom_logloss: 8.55679 |  0:09:25s
epoch 20 | loss: 0.84743 | eval_custom_logloss: 8.07673 |  0:09:53s
epoch 21 | loss: 0.84063 | eval_custom_logloss: 5.20734 |  0:10:22s
epoch 22 | loss: 0.83093 | eval_custom_logloss: 4.73687 |  0:10:49s
epoch 23 | loss: 0.81266 | eval_custom_logloss: 3.5624  |  0:11:17s
epoch 24 | loss: 0.8198  | eval_custom_logloss: 5.38356 |  0:11:46s
epoch 25 | loss: 0.79515 | eval_custom_logloss: 3.8551  |  0:12:14s
epoch 26 | loss: 0.80228 | eval_custom_logloss: 5.07273 |  0:12:42s
epoch 27 | loss: 0.80775 | eval_custom_logloss: 5.07942 |  0:13:10s
epoch 28 | loss: 0.80255 | eval_custom_logloss: 3.3009  |  0:13:38s
epoch 29 | loss: 0.80191 | eval_custom_logloss: 3.20101 |  0:14:06s
epoch 30 | loss: 0.81644 | eval_custom_logloss: 5.17638 |  0:14:34s
epoch 31 | loss: 0.81367 | eval_custom_logloss: 3.15435 |  0:15:02s
epoch 32 | loss: 0.78906 | eval_custom_logloss: 3.39461 |  0:15:31s
epoch 33 | loss: 0.78582 | eval_custom_logloss: 1.13479 |  0:15:59s
epoch 34 | loss: 0.80865 | eval_custom_logloss: 4.01525 |  0:16:27s
epoch 35 | loss: 0.77498 | eval_custom_logloss: 1.53511 |  0:16:55s
epoch 36 | loss: 0.77538 | eval_custom_logloss: 2.96847 |  0:17:23s
epoch 37 | loss: 0.78533 | eval_custom_logloss: 1.57846 |  0:17:51s
epoch 38 | loss: 0.76961 | eval_custom_logloss: 1.53048 |  0:18:19s
epoch 39 | loss: 0.77277 | eval_custom_logloss: 1.82825 |  0:18:48s
epoch 40 | loss: 0.77815 | eval_custom_logloss: 2.92925 |  0:19:16s
epoch 41 | loss: 0.76486 | eval_custom_logloss: 3.04945 |  0:19:44s
epoch 42 | loss: 0.78313 | eval_custom_logloss: 7.78007 |  0:20:12s
epoch 43 | loss: 0.78922 | eval_custom_logloss: 5.92215 |  0:20:40s
epoch 44 | loss: 0.78822 | eval_custom_logloss: 6.42938 |  0:21:09s
epoch 45 | loss: 0.78784 | eval_custom_logloss: 4.31946 |  0:21:37s
epoch 46 | loss: 0.79351 | eval_custom_logloss: 3.97709 |  0:22:05s
epoch 47 | loss: 0.76162 | eval_custom_logloss: 4.7997  |  0:22:33s
epoch 48 | loss: 0.77508 | eval_custom_logloss: 3.78865 |  0:23:01s
epoch 49 | loss: 0.77552 | eval_custom_logloss: 1.66006 |  0:23:29s
epoch 50 | loss: 0.76661 | eval_custom_logloss: 2.44505 |  0:23:57s
epoch 51 | loss: 0.76302 | eval_custom_logloss: 1.4361  |  0:24:25s
epoch 52 | loss: 0.75921 | eval_custom_logloss: 2.05784 |  0:24:53s
epoch 53 | loss: 0.75083 | eval_custom_logloss: 6.06476 |  0:25:22s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 1.13479
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.6726500000000002, 'Log Loss - std': 1.2647417097969056} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 'Ideal' 'E' 'SI2' -0.16918805719128635]
 [-1.2373782800036142 'Premium' 'E' 'SI1' -1.354043921120755]
 [-1.1951587568816937 'Good' 'E' 'VS1' -3.375268630176903]
 [-1.0685001875159332 'Premium' 'I' 'VS2' 0.4580885766537244]
 [-1.0262806643940128 'Good' 'J' 'SI2' 1.085365210498735]
 [-1.1740489953207336 'Very Good' 'I' 'VVS1' 0.3883911728931665]
 [-1.1318294721988134 'Very Good' 'H' 'SI1' 0.10960155785094007]
 [-1.2162685184426538 'Fair' 'E' 'VS2' 2.3399184781887565]
 [-1.1951587568816937 'Ideal' 'J' 'VS1' 0.7368781916959508]
 [-1.2162685184426538 'Premium' 'F' 'SI1' -0.9358594985574128]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1951587568816937 5 6 2 -0.16918805719128635 -1.095468028782346
  -1.5839349366235063 -1.5322976251522233 -1.56187701865597]
 [-1.2373782800036142 4 6 3 -1.354043921120755 1.5812091849357435
  -1.6374236122792667 -1.6548711768767947 -1.731354397531372]
 [-1.1951587568816937 2 6 5 -3.375268630176903 3.3656606607478032
  -1.4947871438639058 -1.4535003419007129 -1.731354397531372]
 [-1.0685001875159332 4 2 4 0.4580885766537244 0.24287057807669882
  -1.3610654547245045 -1.313416282786917 -1.2794147205303008]
 [-1.0262806643940128 2 1 2 1.085365210498735 0.24287057807669882
  -1.236258544861064 -1.2083532384515707 -1.1099373416548988]
 [-1.1740489953207336 3 2 7 0.3883911728931665 -0.2032422908763161
  -1.5839349366235063 -1.5322976251522233 -1.5053845590308361]
 [-1.1318294721988134 3 3 3 0.10960155785094007 -1.095468028782346
  -1.4769575853119854 -1.418479327122264 -1.4206458695931359]
 [-1.2162685184426538 1 6 4 2.3399184781887565 1.5812091849357435
  -1.6552531708311868 -1.7074026990444682 -1.4771383292182692]
 [-1.1951587568816937 5 1 5 0.7368781916959508 -0.649355159829331
  -1.6017644951754264 -1.602339654709121 -1.51950767393712]
 [-1.2162685184426538 4 5 3 -0.9358594985574128 1.5812091849357435
  -1.646338391555227 -1.6548711768767947 -1.7031081677188051]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.75074 | eval_custom_logloss: 13.71358|  0:00:28s
epoch 1  | loss: 1.31413 | eval_custom_logloss: 13.44794|  0:00:57s
epoch 2  | loss: 1.12572 | eval_custom_logloss: 14.1184 |  0:01:25s
epoch 3  | loss: 1.02439 | eval_custom_logloss: 12.45214|  0:01:53s
epoch 4  | loss: 0.99184 | eval_custom_logloss: 11.83098|  0:02:22s
epoch 5  | loss: 0.9668  | eval_custom_logloss: 11.19839|  0:02:50s
epoch 6  | loss: 0.94246 | eval_custom_logloss: 10.1079 |  0:03:18s
epoch 7  | loss: 0.95313 | eval_custom_logloss: 8.30059 |  0:03:47s
epoch 8  | loss: 0.93077 | eval_custom_logloss: 6.5602  |  0:04:15s
epoch 9  | loss: 0.91393 | eval_custom_logloss: 6.85464 |  0:04:44s
epoch 10 | loss: 0.88885 | eval_custom_logloss: 7.09902 |  0:05:12s
epoch 11 | loss: 0.89015 | eval_custom_logloss: 6.52444 |  0:05:41s
epoch 12 | loss: 0.89838 | eval_custom_logloss: 6.29813 |  0:06:09s
epoch 13 | loss: 0.88285 | eval_custom_logloss: 5.89363 |  0:06:38s
epoch 14 | loss: 0.87386 | eval_custom_logloss: 3.69163 |  0:07:06s
epoch 15 | loss: 0.89421 | eval_custom_logloss: 3.3343  |  0:07:35s
epoch 16 | loss: 0.85251 | eval_custom_logloss: 3.94969 |  0:08:03s
epoch 17 | loss: 0.85647 | eval_custom_logloss: 3.84387 |  0:08:32s
epoch 18 | loss: 0.8499  | eval_custom_logloss: 3.28637 |  0:09:00s
epoch 19 | loss: 0.83313 | eval_custom_logloss: 3.68436 |  0:09:29s
epoch 20 | loss: 0.83386 | eval_custom_logloss: 2.28518 |  0:09:57s
epoch 21 | loss: 0.84413 | eval_custom_logloss: 2.20253 |  0:10:26s
epoch 22 | loss: 0.82581 | eval_custom_logloss: 1.5349  |  0:10:54s
epoch 23 | loss: 0.8271  | eval_custom_logloss: 3.39518 |  0:11:23s
epoch 24 | loss: 0.83124 | eval_custom_logloss: 2.63742 |  0:11:51s
epoch 25 | loss: 0.83433 | eval_custom_logloss: 2.41003 |  0:12:19s
epoch 26 | loss: 0.82629 | eval_custom_logloss: 2.60885 |  0:12:48s
epoch 27 | loss: 0.8128  | eval_custom_logloss: 3.2829  |  0:13:16s
epoch 28 | loss: 0.8194  | eval_custom_logloss: 6.2385  |  0:13:45s
epoch 29 | loss: 0.78927 | eval_custom_logloss: 3.72941 |  0:14:13s
epoch 30 | loss: 0.78159 | eval_custom_logloss: 3.73209 |  0:14:42s
epoch 31 | loss: 0.78591 | eval_custom_logloss: 3.90949 |  0:15:10s
epoch 32 | loss: 0.78223 | eval_custom_logloss: 2.07097 |  0:15:39s
epoch 33 | loss: 0.7839  | eval_custom_logloss: 1.71663 |  0:16:08s
epoch 34 | loss: 0.78914 | eval_custom_logloss: 2.24878 |  0:16:36s
epoch 35 | loss: 0.79465 | eval_custom_logloss: 1.40254 |  0:17:05s
epoch 36 | loss: 0.78065 | eval_custom_logloss: 1.49985 |  0:17:33s
epoch 37 | loss: 0.7692  | eval_custom_logloss: 0.92321 |  0:18:02s
epoch 38 | loss: 0.77254 | eval_custom_logloss: 1.36995 |  0:18:30s
epoch 39 | loss: 0.77485 | eval_custom_logloss: 1.08341 |  0:18:59s
epoch 40 | loss: 0.7738  | eval_custom_logloss: 1.16566 |  0:19:28s
epoch 41 | loss: 0.75527 | eval_custom_logloss: 1.12876 |  0:19:56s
epoch 42 | loss: 0.75042 | eval_custom_logloss: 1.3682  |  0:20:25s
epoch 43 | loss: 0.74849 | eval_custom_logloss: 1.05371 |  0:20:53s
epoch 44 | loss: 0.74488 | eval_custom_logloss: 1.06845 |  0:21:22s
epoch 45 | loss: 0.7515  | eval_custom_logloss: 1.08771 |  0:21:50s
epoch 46 | loss: 0.75368 | eval_custom_logloss: 1.19615 |  0:22:19s
epoch 47 | loss: 0.72808 | eval_custom_logloss: 1.44841 |  0:22:47s
epoch 48 | loss: 0.75149 | eval_custom_logloss: 1.28207 |  0:23:16s
epoch 49 | loss: 0.74988 | eval_custom_logloss: 1.37888 |  0:23:45s
epoch 50 | loss: 0.75528 | eval_custom_logloss: 1.04829 |  0:24:13s
epoch 51 | loss: 0.73076 | eval_custom_logloss: 1.41657 |  0:24:41s
epoch 52 | loss: 0.72874 | eval_custom_logloss: 0.8161  |  0:25:10s
epoch 53 | loss: 0.72576 | eval_custom_logloss: 0.86739 |  0:25:38s
epoch 54 | loss: 0.74271 | eval_custom_logloss: 1.73059 |  0:26:07s
epoch 55 | loss: 0.73671 | eval_custom_logloss: 1.24228 |  0:26:36s
epoch 56 | loss: 0.73057 | eval_custom_logloss: 0.79861 |  0:27:04s
epoch 57 | loss: 0.72299 | eval_custom_logloss: 1.10881 |  0:27:32s
epoch 58 | loss: 0.7241  | eval_custom_logloss: 0.97295 |  0:28:00s
epoch 59 | loss: 0.71232 | eval_custom_logloss: 1.3474  |  0:28:28s
epoch 60 | loss: 0.70867 | eval_custom_logloss: 0.91001 |  0:28:56s
epoch 61 | loss: 0.71567 | eval_custom_logloss: 0.82062 |  0:29:24s
epoch 62 | loss: 0.73775 | eval_custom_logloss: 0.8442  |  0:29:52s
epoch 63 | loss: 0.72388 | eval_custom_logloss: 1.16733 |  0:30:20s
epoch 64 | loss: 0.70564 | eval_custom_logloss: 1.08472 |  0:30:49s
epoch 65 | loss: 0.72386 | eval_custom_logloss: 1.57165 |  0:31:17s
epoch 66 | loss: 0.72237 | eval_custom_logloss: 0.92086 |  0:31:45s
epoch 67 | loss: 0.7104  | eval_custom_logloss: 0.93052 |  0:32:14s
epoch 68 | loss: 0.70314 | eval_custom_logloss: 1.28099 |  0:32:42s
epoch 69 | loss: 0.69966 | eval_custom_logloss: 0.96059 |  0:33:11s
epoch 70 | loss: 0.7029  | eval_custom_logloss: 1.05879 |  0:33:39s
epoch 71 | loss: 0.7252  | eval_custom_logloss: 0.9471  |  0:34:08s
epoch 72 | loss: 0.69851 | eval_custom_logloss: 0.88216 |  0:34:37s
epoch 73 | loss: 0.69694 | eval_custom_logloss: 0.72823 |  0:35:05s
epoch 74 | loss: 0.71866 | eval_custom_logloss: 0.91588 |  0:35:34s
epoch 75 | loss: 0.70771 | eval_custom_logloss: 1.35726 |  0:36:02s
epoch 76 | loss: 0.69996 | eval_custom_logloss: 1.27105 |  0:36:31s
epoch 77 | loss: 0.69953 | eval_custom_logloss: 0.8813  |  0:36:59s
epoch 78 | loss: 0.69948 | eval_custom_logloss: 0.73708 |  0:37:28s
epoch 79 | loss: 0.70441 | eval_custom_logloss: 0.9307  |  0:37:56s
epoch 80 | loss: 0.71341 | eval_custom_logloss: 1.06608 |  0:38:25s
epoch 81 | loss: 0.70368 | eval_custom_logloss: 1.22608 |  0:38:53s
epoch 82 | loss: 0.69637 | eval_custom_logloss: 1.22324 |  0:39:21s
epoch 83 | loss: 0.69836 | eval_custom_logloss: 0.70278 |  0:39:50s
epoch 84 | loss: 0.69985 | eval_custom_logloss: 1.85162 |  0:40:19s
epoch 85 | loss: 0.70701 | eval_custom_logloss: 1.66051 |  0:40:47s
epoch 86 | loss: 0.69522 | eval_custom_logloss: 0.95193 |  0:41:16s
epoch 87 | loss: 0.70321 | eval_custom_logloss: 1.1321  |  0:41:44s
epoch 88 | loss: 0.6986  | eval_custom_logloss: 1.02039 |  0:42:13s
epoch 89 | loss: 0.70312 | eval_custom_logloss: 1.15705 |  0:42:41s
epoch 90 | loss: 0.71146 | eval_custom_logloss: 1.18195 |  0:43:10s
epoch 91 | loss: 0.70551 | eval_custom_logloss: 2.76441 |  0:43:39s
epoch 92 | loss: 0.69073 | eval_custom_logloss: 1.33871 |  0:44:07s
epoch 93 | loss: 0.69312 | eval_custom_logloss: 1.90816 |  0:44:35s
epoch 94 | loss: 0.69256 | eval_custom_logloss: 1.00251 |  0:45:03s
epoch 95 | loss: 0.68809 | eval_custom_logloss: 0.67448 |  0:45:32s
epoch 96 | loss: 0.68689 | eval_custom_logloss: 1.81001 |  0:46:01s
epoch 97 | loss: 0.697   | eval_custom_logloss: 1.3741  |  0:46:29s
epoch 98 | loss: 0.6734  | eval_custom_logloss: 0.88271 |  0:46:58s
epoch 99 | loss: 0.69098 | eval_custom_logloss: 0.90365 |  0:47:26s
Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_eval_custom_logloss = 0.67448
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 14
Class label len :14
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] 

Prediction shape : (10788,)
Probabilities shape : (10788, 14) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.47268, 'Log Loss - std': 1.199837187955099} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 35 finished with value: 1.47268 and parameters: {'n_d': 20, 'n_steps': 8, 'gamma': 1.7326743898919266, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001058109003799555, 'mask_type': 'sparsemax'}. Best is trial 35 with value: 1.47268.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : None
ordinal_idx : [1, 2, 3]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (43152, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 4, 5, 6, 7, 8]
Cat Dims V1 : [6, 8, 9]
Cat Idx V1 : [1, 2, 3] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 'Ideal' 'E' 'SI2' -0.17767576091934348]
 [-1.236976378485391 'Premium' 'E' 'SI1' -1.3637394355688461]
 [-1.069224675734413 'Premium' 'I' 'VS2' 0.45024030213039123]
 [-1.1740694899537742 'Very Good' 'J' 'VVS2' 0.7293141079302727]
 [-1.1740694899537742 'Very Good' 'I' 'VVS1' 0.3804718506804196]
 [-1.1321315642660297 'Very Good' 'H' 'SI1' 0.10139804488053805]
 [-1.2160074156415188 'Fair' 'E' 'VS2' 2.3339884912795954]
 [-1.1950384527976465 'Very Good' 'H' 'VS1' -1.6428132413687278]
 [-1.0482557128905408 'Good' 'J' 'SI1' 1.5665355253299222]
 [-1.1950384527976465 'Ideal' 'J' 'VS1' 0.7293141079302727]] 
 
 
Val : (43152, 9) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 4, 5, 6, 7, 8] 


OHE Idx : None


Ordinal Idx V2: [1, 2, 3]


Cat Dims V2 : [6, 8, 9]
Cat Idx V2 : [1, 2, 3] 
 

Train: [[-1.1950384527976465 5 6 2 -0.17767576091934348 -1.0997679290240274
  -1.5847483029459433 -1.5624470539339794 -1.5635817068251905]
 [-1.236976378485391 4 6 3 -1.3637394355688461 1.5843835902987453
  -1.6380368496537203 -1.6869504169773575 -1.7324417222907198]
 [-1.069224675734413 4 2 4 0.45024030213039123 0.24230783063735892
  -1.3627126916635395 -1.3401196199279468 -1.2821483477159754]
 [-1.1740694899537742 3 1 6 0.7293141079302727 -0.20505075591643654
  -1.5936297273972397 -1.580233248654462 -1.493223367047887]
 [-1.1740694899537742 3 2 7 0.3804718506804196 -0.20505075591643654
  -1.5847483029459433 -1.5624470539339794 -1.5072950350033474]
 [-1.1321315642660297 3 3 3 0.10139804488053805 -1.0997679290240274
  -1.4781712095303894 -1.4468367882508424 -1.4228650272705832]
 [-1.2160074156415188 1 6 4 2.3339884912795954 1.5843835902987453
  -1.6557996985563126 -1.7403090011388052 -1.4791516990924258]
 [-1.1950384527976465 3 3 5 -1.6428132413687278 1.5843835902987453
  -1.5403411806894627 -1.5001953724122905 -1.6198683786470336]
 [-1.0482557128905408 2 1 3 1.5665355253299222 -1.0997679290240274
  -1.318305569407059 -1.2956541331267404 -1.1414316681613677]
 [-1.1950384527976465 5 1 5 0.7293141079302727 -0.652409342470232
  -1.6025111518485355 -1.6335918328159098 -1.5213667029588085]] 
 
 
Val : (43152, 9) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 14
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13.]), 14)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13], Length : 14
Number of Classes After Bin Verifier: 14
In get_device
{'n_d': 19, 'n_steps': 8, 'gamma': 1.7315209243910317, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0022081681133978414, 'mask_type': 'sparsemax', 'n_a': 19, 'cat_idxs': [1, 2, 3], 'cat_dims': [6, 8, 9], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.73886 | eval_custom_logloss: 10.89833|  0:00:28s
epoch 1  | loss: 1.27343 | eval_custom_logloss: 9.4756  |  0:00:56s
epoch 2  | loss: 1.1755  | eval_custom_logloss: 8.63847 |  0:01:25s
epoch 3  | loss: 1.08685 | eval_custom_logloss: 6.15943 |  0:01:53s
epoch 4  | loss: 1.07773 | eval_custom_logloss: 4.62857 |  0:02:22s
epoch 5  | loss: 1.09462 | eval_custom_logloss: 4.73927 |  0:02:50s
epoch 6  | loss: 1.07905 | eval_custom_logloss: 4.05097 |  0:03:19s
epoch 7  | loss: 1.02834 | eval_custom_logloss: 3.07526 |  0:03:47s
epoch 8  | loss: 1.00156 | eval_custom_logloss: 1.83317 |  0:04:16s
epoch 9  | loss: 0.98403 | eval_custom_logloss: 2.06294 |  0:04:45s
epoch 10 | loss: 0.9714  | eval_custom_logloss: 2.6265  |  0:05:14s
epoch 11 | loss: 0.96362 | eval_custom_logloss: 2.4826  |  0:05:42s
epoch 12 | loss: 0.96343 | eval_custom_logloss: 2.67669 |  0:06:10s
epoch 13 | loss: 0.93029 | eval_custom_logloss: 2.75325 |  0:06:39s
epoch 14 | loss: 0.92088 | eval_custom_logloss: 1.681   |  0:07:07s
epoch 15 | loss: 0.95262 | eval_custom_logloss: 2.29828 |  0:07:36s
epoch 16 | loss: 0.93305 | eval_custom_logloss: 1.46373 |  0:08:04s
epoch 17 | loss: 0.9165  | eval_custom_logloss: 1.30093 |  0:08:33s
epoch 18 | loss: 0.91621 | eval_custom_logloss: 1.41105 |  0:09:01s
epoch 19 | loss: 0.90082 | eval_custom_logloss: 1.0891  |  0:09:30s
epoch 20 | loss: 0.91717 | eval_custom_logloss: 1.04727 |  0:09:59s
epoch 21 | loss: 0.8821  | eval_custom_logloss: 1.01202 |  0:10:27s
epoch 22 | loss: 0.9554  | eval_custom_logloss: 2.57666 |  0:10:56s
epoch 23 | loss: 0.9763  | eval_custom_logloss: 1.92343 |  0:11:24s
epoch 24 | loss: 0.93241 | eval_custom_logloss: 1.11699 |  0:11:53s
epoch 25 | loss: 0.91616 | eval_custom_logloss: 1.2548  |  0:12:22s
epoch 26 | loss: 0.90399 | eval_custom_logloss: 1.26499 |  0:12:50s
epoch 27 | loss: 0.90905 | eval_custom_logloss: 1.08492 |  0:13:19s
epoch 28 | loss: 0.89915 | eval_custom_logloss: 1.33164 |  0:13:47s
epoch 29 | loss: 0.89055 | eval_custom_logloss: 0.91223 |  0:14:16s
epoch 30 | loss: 0.88852 | eval_custom_logloss: 0.84791 |  0:14:45s
epoch 31 | loss: 0.90521 | eval_custom_logloss: 0.95815 |  0:15:13s
epoch 32 | loss: 0.88013 | eval_custom_logloss: 0.9364  |  0:15:42s
epoch 33 | loss: 0.87251 | eval_custom_logloss: 0.81983 |  0:16:10s
epoch 34 | loss: 0.90141 | eval_custom_logloss: 1.0995  |  0:16:39s
epoch 35 | loss: 0.87684 | eval_custom_logloss: 0.90624 |  0:17:07s
epoch 36 | loss: 0.87228 | eval_custom_logloss: 0.97689 |  0:17:36s
epoch 37 | loss: 0.8692  | eval_custom_logloss: 0.93863 |  0:18:04s
epoch 38 | loss: 0.86743 | eval_custom_logloss: 1.46599 |  0:18:33s
epoch 39 | loss: 0.87858 | eval_custom_logloss: 1.69363 |  0:19:02s
epoch 40 | loss: 0.86529 | eval_custom_logloss: 0.88286 |  0:19:30s
epoch 41 | loss: 0.86909 | eval_custom_logloss: 0.84087 |  0:19:59s
epoch 42 | loss: 0.86241 | eval_custom_logloss: 1.70674 |  0:20:27s
epoch 43 | loss: 0.86779 | eval_custom_logloss: 0.89103 |  0:20:56s
epoch 44 | loss: 0.84883 | eval_custom_logloss: 0.88412 |  0:21:24s
epoch 45 | loss: 0.8622  | eval_custom_logloss: 1.13851 |  0:21:53s
epoch 46 | loss: 0.87528 | eval_custom_logloss: 0.88327 |  0:22:21s
epoch 47 | loss: 0.86492 | eval_custom_logloss: 1.36934 |  0:22:49s
epoch 48 | loss: 0.85002 | eval_custom_logloss: 0.84137 |  0:23:17s
epoch 49 | loss: 0.86436 | eval_custom_logloss: 1.01579 |  0:23:45s
epoch 50 | loss: 0.83073 | eval_custom_logloss: 1.22627 |  0:24:14s
epoch 51 | loss: 0.81628 | eval_custom_logloss: 0.91214 |  0:24:42s
epoch 52 | loss: 0.83603 | eval_custom_logloss: 0.84163 |  0:25:10s
epoch 53 | loss: 0.81229 | eval_custom_logloss: 0.80643 |  0:25:37s
epoch 54 | loss: 0.82833 | eval_custom_logloss: 1.14237 |  0:26:06s
epoch 55 | loss: 0.81452 | eval_custom_logloss: 1.01544 |  0:26:33s
epoch 56 | loss: 0.82572 | eval_custom_logloss: 1.23819 |  0:27:02s
epoch 57 | loss: 0.81666 | eval_custom_logloss: 1.0288  |  0:27:30s
