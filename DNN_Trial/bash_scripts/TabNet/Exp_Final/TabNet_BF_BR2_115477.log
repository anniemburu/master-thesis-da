Namespace(batch_size=64, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/black_friday.yml', data_parallel=False, dataset='Black_Friday', direction='maximize', dropna_idx=None, early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabNet', n_trials=18, nominal_idx=[0, 2, 3, 4, 5, 6, 7, 8], num_bins=10, num_classes=1, num_features=9, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=True, ordinal_idx=[1], scale=False, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=128, y_distribution='bimodial')
Start hyperparameter optimization
Loading dataset Black_Friday...
Dataset loaded! 

(166821, 9)
Using an existing study with name 'TabNet_Black_Friday' instead of creating a new one.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133456, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133456, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]] 
 
 
Val : (133456, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.41129 | eval_custom_logloss: 2.24814 |  0:01:15s
epoch 1  | loss: 2.23186 | eval_custom_logloss: 2.23723 |  0:02:18s
epoch 2  | loss: 2.2291  | eval_custom_logloss: 2.22492 |  0:03:22s
epoch 3  | loss: 2.22586 | eval_custom_logloss: 2.22799 |  0:04:25s
epoch 4  | loss: 2.22139 | eval_custom_logloss: 2.23313 |  0:05:29s
epoch 5  | loss: 2.21821 | eval_custom_logloss: 2.22584 |  0:06:32s
epoch 6  | loss: 2.21798 | eval_custom_logloss: 2.53084 |  0:07:38s
epoch 7  | loss: 2.21416 | eval_custom_logloss: 2.21824 |  0:08:44s
epoch 8  | loss: 2.21302 | eval_custom_logloss: 2.22349 |  0:09:57s
epoch 9  | loss: 2.20838 | eval_custom_logloss: 2.22107 |  0:11:12s
epoch 10 | loss: 2.20902 | eval_custom_logloss: 2.21039 |  0:12:20s
epoch 11 | loss: 2.20925 | eval_custom_logloss: 2.21374 |  0:13:34s
epoch 12 | loss: 2.20816 | eval_custom_logloss: 2.21247 |  0:14:48s
epoch 13 | loss: 2.20656 | eval_custom_logloss: 2.21444 |  0:16:02s
epoch 14 | loss: 2.20823 | eval_custom_logloss: 2.22437 |  0:17:09s
epoch 15 | loss: 2.20562 | eval_custom_logloss: 2.21189 |  0:18:17s
epoch 16 | loss: 2.20582 | eval_custom_logloss: 2.20763 |  0:19:22s
epoch 17 | loss: 2.20531 | eval_custom_logloss: 2.21751 |  0:20:27s
epoch 18 | loss: 2.20713 | eval_custom_logloss: 2.2065  |  0:21:40s
epoch 19 | loss: 2.20471 | eval_custom_logloss: 2.21573 |  0:22:45s
epoch 20 | loss: 2.20468 | eval_custom_logloss: 2.21499 |  0:23:58s
epoch 21 | loss: 2.20459 | eval_custom_logloss: 2.21127 |  0:25:12s
epoch 22 | loss: 2.20323 | eval_custom_logloss: 2.20975 |  0:26:21s
epoch 23 | loss: 2.25321 | eval_custom_logloss: 2.2756  |  0:27:28s
epoch 24 | loss: 2.26753 | eval_custom_logloss: 2.27403 |  0:28:35s
epoch 25 | loss: 2.26855 | eval_custom_logloss: 2.27404 |  0:29:43s
epoch 26 | loss: 2.25239 | eval_custom_logloss: 2.24441 |  0:30:49s
epoch 27 | loss: 2.23324 | eval_custom_logloss: 2.23228 |  0:31:57s
epoch 28 | loss: 2.21272 | eval_custom_logloss: 2.20178 |  0:33:03s
epoch 29 | loss: 2.18411 | eval_custom_logloss: 2.19799 |  0:34:10s
epoch 30 | loss: 2.18478 | eval_custom_logloss: 2.70823 |  0:35:18s
epoch 31 | loss: 2.18195 | eval_custom_logloss: 2.1965  |  0:36:23s
epoch 32 | loss: 2.18321 | eval_custom_logloss: 2.199   |  0:37:29s
epoch 33 | loss: 2.18279 | eval_custom_logloss: 2.20037 |  0:38:33s
epoch 34 | loss: 2.18186 | eval_custom_logloss: 2.18826 |  0:39:37s
epoch 35 | loss: 2.18292 | eval_custom_logloss: 2.19195 |  0:40:48s
epoch 36 | loss: 2.22074 | eval_custom_logloss: 2.23664 |  0:42:03s
epoch 37 | loss: 2.193   | eval_custom_logloss: 2.80845 |  0:43:18s
epoch 38 | loss: 2.20615 | eval_custom_logloss: 2.88675 |  0:44:32s
epoch 39 | loss: 2.21998 | eval_custom_logloss: 3.30864 |  0:45:45s
epoch 40 | loss: 2.21971 | eval_custom_logloss: 3.39091 |  0:46:59s
epoch 41 | loss: 2.21803 | eval_custom_logloss: 3.26545 |  0:48:14s
epoch 42 | loss: 2.21886 | eval_custom_logloss: 2.24375 |  0:49:24s
epoch 43 | loss: 2.22008 | eval_custom_logloss: 2.2284  |  0:50:36s
epoch 44 | loss: 2.21765 | eval_custom_logloss: 2.92958 |  0:51:51s
epoch 45 | loss: 2.21612 | eval_custom_logloss: 2.28301 |  0:53:06s
epoch 46 | loss: 2.21661 | eval_custom_logloss: 2.95046 |  0:54:09s
epoch 47 | loss: 2.21759 | eval_custom_logloss: 2.2335  |  0:55:13s
epoch 48 | loss: 2.21648 | eval_custom_logloss: 2.2646  |  0:56:19s
epoch 49 | loss: 2.21628 | eval_custom_logloss: 2.88531 |  0:57:26s
epoch 50 | loss: 2.22038 | eval_custom_logloss: 2.22936 |  0:58:32s
epoch 51 | loss: 2.21966 | eval_custom_logloss: 2.25015 |  0:59:38s
epoch 52 | loss: 2.21767 | eval_custom_logloss: 2.22844 |  1:00:44s
epoch 53 | loss: 2.21897 | eval_custom_logloss: 2.95778 |  1:01:47s
epoch 54 | loss: 2.23511 | eval_custom_logloss: 2.25376 |  1:02:51s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 2.18826
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33365,)
Probabilities shape : (33365, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1866, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.48901 | eval_custom_logloss: 2.3423  |  0:01:13s
epoch 1  | loss: 2.30159 | eval_custom_logloss: 2.25433 |  0:02:19s
epoch 2  | loss: 2.23024 | eval_custom_logloss: 2.21149 |  0:03:25s
epoch 3  | loss: 2.22938 | eval_custom_logloss: 2.21346 |  0:04:40s
epoch 4  | loss: 2.21227 | eval_custom_logloss: 2.20588 |  0:05:48s
epoch 5  | loss: 2.21012 | eval_custom_logloss: 2.47971 |  0:06:59s
epoch 6  | loss: 2.20635 | eval_custom_logloss: 2.2152  |  0:08:07s
epoch 7  | loss: 2.20836 | eval_custom_logloss: 2.20132 |  0:09:14s
epoch 8  | loss: 2.20354 | eval_custom_logloss: 2.19943 |  0:10:18s
epoch 9  | loss: 2.20218 | eval_custom_logloss: 2.20093 |  0:11:22s
epoch 10 | loss: 2.20329 | eval_custom_logloss: 2.1989  |  0:12:26s
epoch 11 | loss: 2.20081 | eval_custom_logloss: 2.20204 |  0:13:29s
epoch 12 | loss: 2.20221 | eval_custom_logloss: 2.19647 |  0:14:33s
epoch 13 | loss: 2.19997 | eval_custom_logloss: 2.19952 |  0:15:37s
epoch 14 | loss: 2.19954 | eval_custom_logloss: 2.20137 |  0:16:40s
epoch 15 | loss: 2.19846 | eval_custom_logloss: 2.19355 |  0:17:47s
epoch 16 | loss: 2.19903 | eval_custom_logloss: 2.19641 |  0:18:51s
epoch 17 | loss: 2.20461 | eval_custom_logloss: 2.20163 |  0:19:54s
epoch 18 | loss: 2.19956 | eval_custom_logloss: 2.19694 |  0:20:58s
epoch 19 | loss: 2.19798 | eval_custom_logloss: 2.19974 |  0:22:01s
epoch 20 | loss: 2.19925 | eval_custom_logloss: 2.19635 |  0:23:06s
epoch 21 | loss: 2.19668 | eval_custom_logloss: 2.19526 |  0:24:14s
epoch 22 | loss: 2.19766 | eval_custom_logloss: 2.27929 |  0:25:22s
epoch 23 | loss: 2.19407 | eval_custom_logloss: 2.18748 |  0:26:27s
epoch 24 | loss: 2.19671 | eval_custom_logloss: 2.20169 |  0:27:35s
epoch 25 | loss: 2.19191 | eval_custom_logloss: 2.20057 |  0:28:43s
epoch 26 | loss: 2.19434 | eval_custom_logloss: 2.25909 |  0:29:51s
epoch 27 | loss: 2.19722 | eval_custom_logloss: 2.2005  |  0:30:54s
epoch 28 | loss: 2.20253 | eval_custom_logloss: 2.19293 |  0:31:58s
epoch 29 | loss: 2.19617 | eval_custom_logloss: 2.19075 |  0:33:01s
epoch 30 | loss: 2.1928  | eval_custom_logloss: 2.19012 |  0:34:13s
epoch 31 | loss: 2.19433 | eval_custom_logloss: 2.20931 |  0:35:16s
epoch 32 | loss: 2.20403 | eval_custom_logloss: 2.19857 |  0:36:19s
epoch 33 | loss: 2.19732 | eval_custom_logloss: 2.19537 |  0:37:24s
epoch 34 | loss: 2.19838 | eval_custom_logloss: 2.19284 |  0:38:27s
epoch 35 | loss: 2.19286 | eval_custom_logloss: 2.19214 |  0:39:34s
epoch 36 | loss: 2.19529 | eval_custom_logloss: 2.19315 |  0:40:41s
epoch 37 | loss: 2.2     | eval_custom_logloss: 2.20513 |  0:41:45s
epoch 38 | loss: 2.19328 | eval_custom_logloss: 2.19164 |  0:42:49s
epoch 39 | loss: 2.19602 | eval_custom_logloss: 2.20073 |  0:43:52s
epoch 40 | loss: 2.19364 | eval_custom_logloss: 2.19325 |  0:44:55s
epoch 41 | loss: 2.1911  | eval_custom_logloss: 2.18841 |  0:46:00s
epoch 42 | loss: 2.19534 | eval_custom_logloss: 2.19523 |  0:47:07s
epoch 43 | loss: 2.19101 | eval_custom_logloss: 2.19162 |  0:48:13s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 2.18748
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1871, 'Log Loss - std': 0.000500000000000167} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.41579 | eval_custom_logloss: 2.20852 |  0:01:02s
epoch 1  | loss: 2.21489 | eval_custom_logloss: 2.21657 |  0:02:05s
epoch 2  | loss: 2.20696 | eval_custom_logloss: 2.19836 |  0:03:08s
epoch 3  | loss: 2.1915  | eval_custom_logloss: 2.19243 |  0:04:11s
epoch 4  | loss: 2.18879 | eval_custom_logloss: 2.17492 |  0:05:14s
epoch 5  | loss: 2.19484 | eval_custom_logloss: 2.20022 |  0:06:17s
epoch 6  | loss: 2.20671 | eval_custom_logloss: 2.37746 |  0:07:21s
epoch 7  | loss: 2.24662 | eval_custom_logloss: 2.28823 |  0:08:24s
epoch 8  | loss: 2.23665 | eval_custom_logloss: 2.33293 |  0:09:30s
epoch 9  | loss: 2.23371 | eval_custom_logloss: 2.29684 |  0:10:37s
epoch 10 | loss: 2.23337 | eval_custom_logloss: 2.22549 |  0:11:41s
epoch 11 | loss: 2.23362 | eval_custom_logloss: 2.24037 |  0:12:44s
epoch 12 | loss: 2.23273 | eval_custom_logloss: 2.23009 |  0:13:48s
epoch 13 | loss: 2.23094 | eval_custom_logloss: 2.2271  |  0:14:51s
epoch 14 | loss: 2.22936 | eval_custom_logloss: 2.22635 |  0:15:54s
epoch 15 | loss: 2.22919 | eval_custom_logloss: 2.24367 |  0:16:59s
epoch 16 | loss: 2.22906 | eval_custom_logloss: 2.26167 |  0:18:05s
epoch 17 | loss: 2.22653 | eval_custom_logloss: 2.24786 |  0:19:17s
epoch 18 | loss: 2.2265  | eval_custom_logloss: 2.33689 |  0:20:30s
epoch 19 | loss: 2.22834 | eval_custom_logloss: 2.22646 |  0:21:42s
epoch 20 | loss: 2.2273  | eval_custom_logloss: 2.2679  |  0:22:48s
epoch 21 | loss: 2.2236  | eval_custom_logloss: 2.22771 |  0:23:52s
epoch 22 | loss: 2.22348 | eval_custom_logloss: 2.22333 |  0:24:59s
epoch 23 | loss: 2.22423 | eval_custom_logloss: 2.23826 |  0:26:06s
epoch 24 | loss: 2.22249 | eval_custom_logloss: 2.21861 |  0:27:13s

Early stopping occurred at epoch 24 with best_epoch = 4 and best_eval_custom_logloss = 2.17492
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1831333333333336, 'Log Loss - std': 0.005624549364665311} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.38959 | eval_custom_logloss: 2.20859 |  0:01:03s
epoch 1  | loss: 2.20456 | eval_custom_logloss: 2.19525 |  0:02:07s
epoch 2  | loss: 2.19218 | eval_custom_logloss: 2.18974 |  0:03:10s
epoch 3  | loss: 2.18801 | eval_custom_logloss: 2.18459 |  0:04:14s
epoch 4  | loss: 2.18439 | eval_custom_logloss: 2.18736 |  0:05:20s
epoch 5  | loss: 2.17962 | eval_custom_logloss: 2.18076 |  0:06:26s
epoch 6  | loss: 2.17803 | eval_custom_logloss: 2.18313 |  0:07:34s
epoch 7  | loss: 2.17687 | eval_custom_logloss: 2.19368 |  0:08:44s
epoch 8  | loss: 2.17703 | eval_custom_logloss: 2.17971 |  0:09:55s
epoch 9  | loss: 2.18317 | eval_custom_logloss: 2.18675 |  0:11:09s
epoch 10 | loss: 2.18257 | eval_custom_logloss: 2.22596 |  0:12:23s
epoch 11 | loss: 2.18152 | eval_custom_logloss: 2.18684 |  0:13:29s
epoch 12 | loss: 2.17821 | eval_custom_logloss: 2.18507 |  0:14:33s
epoch 13 | loss: 2.1883  | eval_custom_logloss: 2.31889 |  0:15:40s
epoch 14 | loss: 2.18038 | eval_custom_logloss: 2.18656 |  0:16:55s
epoch 15 | loss: 2.18188 | eval_custom_logloss: 2.1991  |  0:18:00s
epoch 16 | loss: 2.17721 | eval_custom_logloss: 2.21142 |  0:19:04s
epoch 17 | loss: 2.17849 | eval_custom_logloss: 2.19268 |  0:20:07s
epoch 18 | loss: 2.17579 | eval_custom_logloss: 2.17564 |  0:21:11s
epoch 19 | loss: 2.17446 | eval_custom_logloss: 2.18039 |  0:22:15s
epoch 20 | loss: 2.1748  | eval_custom_logloss: 2.19042 |  0:23:19s
epoch 21 | loss: 2.17419 | eval_custom_logloss: 2.1852  |  0:24:22s
epoch 22 | loss: 2.17287 | eval_custom_logloss: 2.18193 |  0:25:38s
epoch 23 | loss: 2.17376 | eval_custom_logloss: 2.25455 |  0:26:54s
epoch 24 | loss: 2.17257 | eval_custom_logloss: 2.17851 |  0:28:06s
epoch 25 | loss: 2.17291 | eval_custom_logloss: 2.19093 |  0:29:10s
epoch 26 | loss: 2.17192 | eval_custom_logloss: 2.17373 |  0:30:17s
epoch 27 | loss: 2.17096 | eval_custom_logloss: 2.17776 |  0:31:20s
epoch 28 | loss: 2.17178 | eval_custom_logloss: 2.19232 |  0:32:24s
epoch 29 | loss: 2.16957 | eval_custom_logloss: 2.18878 |  0:33:27s
epoch 30 | loss: 2.16975 | eval_custom_logloss: 2.17695 |  0:34:32s
epoch 31 | loss: 2.17026 | eval_custom_logloss: 2.1921  |  0:35:39s
epoch 32 | loss: 2.1674  | eval_custom_logloss: 2.18211 |  0:36:45s
epoch 33 | loss: 2.16891 | eval_custom_logloss: 2.17807 |  0:37:52s
epoch 34 | loss: 2.17628 | eval_custom_logloss: 2.17367 |  0:39:04s
epoch 35 | loss: 2.16982 | eval_custom_logloss: 2.18015 |  0:40:18s
epoch 36 | loss: 2.17043 | eval_custom_logloss: 2.17095 |  0:41:23s
epoch 37 | loss: 2.16607 | eval_custom_logloss: 2.18631 |  0:42:35s
epoch 38 | loss: 2.16818 | eval_custom_logloss: 2.18569 |  0:43:39s
epoch 39 | loss: 2.1657  | eval_custom_logloss: 2.18047 |  0:44:42s
epoch 40 | loss: 2.16632 | eval_custom_logloss: 2.17546 |  0:45:46s
epoch 41 | loss: 2.16725 | eval_custom_logloss: 2.17509 |  0:46:50s
epoch 42 | loss: 2.16743 | eval_custom_logloss: 2.17459 |  0:47:53s
epoch 43 | loss: 2.17141 | eval_custom_logloss: 2.17172 |  0:48:58s
epoch 44 | loss: 2.16416 | eval_custom_logloss: 2.20154 |  0:50:01s
epoch 45 | loss: 2.16442 | eval_custom_logloss: 2.17332 |  0:51:08s
epoch 46 | loss: 2.16477 | eval_custom_logloss: 2.2175  |  0:52:23s
epoch 47 | loss: 2.16414 | eval_custom_logloss: 2.1724  |  0:53:31s
epoch 48 | loss: 2.1648  | eval_custom_logloss: 2.17449 |  0:54:35s
epoch 49 | loss: 2.16445 | eval_custom_logloss: 2.17695 |  0:55:41s
epoch 50 | loss: 2.16815 | eval_custom_logloss: 2.17325 |  0:56:47s
epoch 51 | loss: 2.16578 | eval_custom_logloss: 2.18386 |  0:57:53s
epoch 52 | loss: 2.16883 | eval_custom_logloss: 2.17354 |  0:59:00s
epoch 53 | loss: 2.16405 | eval_custom_logloss: 2.19021 |  1:00:07s
epoch 54 | loss: 2.16723 | eval_custom_logloss: 2.21213 |  1:01:14s
epoch 55 | loss: 2.16656 | eval_custom_logloss: 2.18554 |  1:02:18s
epoch 56 | loss: 2.16273 | eval_custom_logloss: 2.21531 |  1:03:23s

Early stopping occurred at epoch 56 with best_epoch = 36 and best_eval_custom_logloss = 2.17095
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.180175, 'Log Loss - std': 0.007069786064655765} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax', 'n_a': 12, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.38506 | eval_custom_logloss: 2.21793 |  0:01:07s
epoch 1  | loss: 2.20796 | eval_custom_logloss: 2.19981 |  0:02:15s
epoch 2  | loss: 2.20744 | eval_custom_logloss: 2.21311 |  0:03:22s
epoch 3  | loss: 2.20765 | eval_custom_logloss: 2.22098 |  0:04:29s
epoch 4  | loss: 2.19638 | eval_custom_logloss: 2.23261 |  0:05:35s
epoch 5  | loss: 2.18711 | eval_custom_logloss: 2.18752 |  0:06:49s
epoch 6  | loss: 2.17798 | eval_custom_logloss: 2.20058 |  0:07:59s
epoch 7  | loss: 2.17685 | eval_custom_logloss: 2.17332 |  0:09:14s
epoch 8  | loss: 2.17346 | eval_custom_logloss: 2.19765 |  0:10:28s
epoch 9  | loss: 2.17393 | eval_custom_logloss: 2.17352 |  0:11:33s
epoch 10 | loss: 2.17123 | eval_custom_logloss: 2.1874  |  0:12:44s
epoch 11 | loss: 2.16989 | eval_custom_logloss: 2.17554 |  0:13:58s
epoch 12 | loss: 2.16995 | eval_custom_logloss: 2.17665 |  0:15:07s
epoch 13 | loss: 2.16917 | eval_custom_logloss: 2.16826 |  0:16:22s
epoch 14 | loss: 2.16976 | eval_custom_logloss: 2.16865 |  0:17:36s
epoch 15 | loss: 2.16769 | eval_custom_logloss: 2.26811 |  0:18:49s
epoch 16 | loss: 2.16934 | eval_custom_logloss: 2.17011 |  0:19:53s
epoch 17 | loss: 2.16563 | eval_custom_logloss: 2.17615 |  0:20:56s
epoch 18 | loss: 2.16561 | eval_custom_logloss: 2.1767  |  0:22:00s
epoch 19 | loss: 2.16389 | eval_custom_logloss: 2.16941 |  0:23:03s
epoch 20 | loss: 2.16637 | eval_custom_logloss: 2.1836  |  0:24:07s
epoch 21 | loss: 2.16432 | eval_custom_logloss: 2.17921 |  0:25:11s
epoch 22 | loss: 2.1623  | eval_custom_logloss: 2.17442 |  0:26:16s
epoch 23 | loss: 2.16318 | eval_custom_logloss: 2.18058 |  0:27:21s
epoch 24 | loss: 2.17209 | eval_custom_logloss: 2.19585 |  0:28:25s
epoch 25 | loss: 2.16371 | eval_custom_logloss: 2.18547 |  0:29:29s
epoch 26 | loss: 2.16994 | eval_custom_logloss: 2.17314 |  0:30:34s
epoch 27 | loss: 2.16098 | eval_custom_logloss: 2.18073 |  0:31:40s
epoch 28 | loss: 2.16241 | eval_custom_logloss: 2.1642  |  0:32:43s
epoch 29 | loss: 2.16042 | eval_custom_logloss: 2.18277 |  0:33:47s
epoch 30 | loss: 2.16127 | eval_custom_logloss: 2.17544 |  0:34:51s
epoch 31 | loss: 2.16412 | eval_custom_logloss: 2.16921 |  0:35:56s
epoch 32 | loss: 2.16074 | eval_custom_logloss: 2.17355 |  0:37:02s
epoch 33 | loss: 2.15736 | eval_custom_logloss: 2.179   |  0:38:12s
epoch 34 | loss: 2.15889 | eval_custom_logloss: 2.17086 |  0:39:26s
epoch 35 | loss: 2.15749 | eval_custom_logloss: 2.17386 |  0:40:33s
epoch 36 | loss: 2.16344 | eval_custom_logloss: 2.18574 |  0:41:45s
epoch 37 | loss: 2.16044 | eval_custom_logloss: 2.17924 |  0:42:49s
epoch 38 | loss: 2.1587  | eval_custom_logloss: 2.17291 |  0:43:55s
epoch 39 | loss: 2.16102 | eval_custom_logloss: 2.17459 |  0:44:58s
epoch 40 | loss: 2.1567  | eval_custom_logloss: 2.18273 |  0:46:01s
epoch 41 | loss: 2.15703 | eval_custom_logloss: 2.17379 |  0:47:05s
epoch 42 | loss: 2.16329 | eval_custom_logloss: 2.1903  |  0:48:08s
epoch 43 | loss: 2.16092 | eval_custom_logloss: 2.16346 |  0:49:12s
epoch 44 | loss: 2.15776 | eval_custom_logloss: 2.16691 |  0:50:16s
epoch 45 | loss: 2.15827 | eval_custom_logloss: 2.18876 |  0:51:19s
epoch 46 | loss: 2.15755 | eval_custom_logloss: 2.17198 |  0:52:23s
epoch 47 | loss: 2.16204 | eval_custom_logloss: 2.17147 |  0:53:28s
epoch 48 | loss: 2.15883 | eval_custom_logloss: 2.17327 |  0:54:35s
epoch 49 | loss: 2.158   | eval_custom_logloss: 2.17902 |  0:55:42s
epoch 50 | loss: 2.15675 | eval_custom_logloss: 2.20295 |  0:56:48s
epoch 51 | loss: 2.15442 | eval_custom_logloss: 2.17288 |  0:57:55s
epoch 52 | loss: 2.15614 | eval_custom_logloss: 2.16732 |  0:58:58s
epoch 53 | loss: 2.15424 | eval_custom_logloss: 2.17897 |  1:00:03s
epoch 54 | loss: 2.15401 | eval_custom_logloss: 2.17192 |  1:01:14s
epoch 55 | loss: 2.15448 | eval_custom_logloss: 2.17336 |  1:02:28s
epoch 56 | loss: 2.15392 | eval_custom_logloss: 2.17065 |  1:03:36s
epoch 57 | loss: 2.15558 | eval_custom_logloss: 2.16592 |  1:04:42s
epoch 58 | loss: 2.15475 | eval_custom_logloss: 2.15785 |  1:05:52s
epoch 59 | loss: 2.15333 | eval_custom_logloss: 2.15622 |  1:07:00s
epoch 60 | loss: 2.1544  | eval_custom_logloss: 2.15688 |  1:08:06s
epoch 61 | loss: 2.15609 | eval_custom_logloss: 2.18281 |  1:09:11s
epoch 62 | loss: 2.15246 | eval_custom_logloss: 2.17478 |  1:10:18s
epoch 63 | loss: 2.15217 | eval_custom_logloss: 2.17604 |  1:11:23s
epoch 64 | loss: 2.1539  | eval_custom_logloss: 2.17049 |  1:12:30s
epoch 65 | loss: 2.15314 | eval_custom_logloss: 2.18138 |  1:13:36s
epoch 66 | loss: 2.1556  | eval_custom_logloss: 2.17464 |  1:14:41s
epoch 67 | loss: 2.15252 | eval_custom_logloss: 2.19541 |  1:15:46s
epoch 68 | loss: 2.15291 | eval_custom_logloss: 2.17459 |  1:16:50s
epoch 69 | loss: 2.15335 | eval_custom_logloss: 2.1991  |  1:17:53s
epoch 70 | loss: 2.1509  | eval_custom_logloss: 2.17209 |  1:18:57s
epoch 71 | loss: 2.19988 | eval_custom_logloss: 2.2739  |  1:20:07s
epoch 72 | loss: 2.23774 | eval_custom_logloss: 2.25494 |  1:21:20s
epoch 73 | loss: 2.23166 | eval_custom_logloss: 2.25652 |  1:22:24s
epoch 74 | loss: 2.23358 | eval_custom_logloss: 2.27121 |  1:23:36s
epoch 75 | loss: 2.23116 | eval_custom_logloss: 2.25369 |  1:24:45s
epoch 76 | loss: 2.23178 | eval_custom_logloss: 2.26336 |  1:25:55s
epoch 77 | loss: 2.23155 | eval_custom_logloss: 2.25574 |  1:27:01s
epoch 78 | loss: 2.2372  | eval_custom_logloss: 2.25686 |  1:28:12s
epoch 79 | loss: 2.2188  | eval_custom_logloss: 2.24301 |  1:29:23s

Early stopping occurred at epoch 79 with best_epoch = 59 and best_eval_custom_logloss = 2.15622
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.17544, 'Log Loss - std': 0.0113871155258916} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 2.17544 and parameters: {'n_d': 12, 'n_steps': 5, 'gamma': 1.4567028272308393, 'cat_emb_dim': 1, 'n_independent': 1, 'n_shared': 1, 'momentum': 0.002379047115709727, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 2.17544.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133456, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133456, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]] 
 
 
Val : (133456, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax', 'n_a': 49, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.4241  | eval_custom_logloss: 2.2519  |  0:01:36s
epoch 1  | loss: 2.19641 | eval_custom_logloss: 2.21959 |  0:03:13s
epoch 2  | loss: 2.18313 | eval_custom_logloss: 2.28976 |  0:04:49s
epoch 3  | loss: 2.1877  | eval_custom_logloss: 2.27778 |  0:06:33s
epoch 4  | loss: 2.18667 | eval_custom_logloss: 2.17223 |  0:08:21s
epoch 5  | loss: 2.1796  | eval_custom_logloss: 2.17944 |  0:10:19s
epoch 6  | loss: 2.17661 | eval_custom_logloss: 2.16637 |  0:12:04s
epoch 7  | loss: 2.1714  | eval_custom_logloss: 2.16815 |  0:13:49s
epoch 8  | loss: 2.16919 | eval_custom_logloss: 2.20114 |  0:15:32s
epoch 9  | loss: 2.16831 | eval_custom_logloss: 2.17174 |  0:17:18s
epoch 10 | loss: 2.16847 | eval_custom_logloss: 2.1663  |  0:18:58s
epoch 11 | loss: 2.16928 | eval_custom_logloss: 2.17198 |  0:20:34s
epoch 12 | loss: 2.16729 | eval_custom_logloss: 2.17046 |  0:22:12s
epoch 13 | loss: 2.16341 | eval_custom_logloss: 2.1678  |  0:23:48s
epoch 14 | loss: 2.16307 | eval_custom_logloss: 2.16139 |  0:25:27s
epoch 15 | loss: 2.16098 | eval_custom_logloss: 2.17641 |  0:27:04s
epoch 16 | loss: 2.16516 | eval_custom_logloss: 2.18158 |  0:28:40s
epoch 17 | loss: 2.15827 | eval_custom_logloss: 2.17199 |  0:30:14s
epoch 18 | loss: 2.15807 | eval_custom_logloss: 2.17858 |  0:31:47s
epoch 19 | loss: 2.16002 | eval_custom_logloss: 2.16335 |  0:33:21s
epoch 20 | loss: 2.15723 | eval_custom_logloss: 2.16639 |  0:35:03s
epoch 21 | loss: 2.16951 | eval_custom_logloss: 2.19743 |  0:36:37s
epoch 22 | loss: 2.15803 | eval_custom_logloss: 2.16143 |  0:38:22s
epoch 23 | loss: 2.15485 | eval_custom_logloss: 2.16241 |  0:40:05s
epoch 24 | loss: 2.15258 | eval_custom_logloss: 2.16597 |  0:41:49s
epoch 25 | loss: 2.16555 | eval_custom_logloss: 2.16157 |  0:43:26s
epoch 26 | loss: 2.15768 | eval_custom_logloss: 2.16007 |  0:45:03s
epoch 27 | loss: 2.15202 | eval_custom_logloss: 2.16287 |  0:46:39s
epoch 28 | loss: 2.15727 | eval_custom_logloss: 2.16978 |  0:48:16s
epoch 29 | loss: 2.14846 | eval_custom_logloss: 2.16346 |  0:50:02s
epoch 30 | loss: 2.15162 | eval_custom_logloss: 2.16265 |  0:51:37s
epoch 31 | loss: 2.15575 | eval_custom_logloss: 2.17642 |  0:53:16s
epoch 32 | loss: 2.14705 | eval_custom_logloss: 2.17374 |  0:54:53s
epoch 33 | loss: 2.14718 | eval_custom_logloss: 2.16315 |  0:56:29s
epoch 34 | loss: 2.14943 | eval_custom_logloss: 2.16211 |  0:58:06s
epoch 35 | loss: 2.1473  | eval_custom_logloss: 2.15855 |  0:59:43s
epoch 36 | loss: 2.14575 | eval_custom_logloss: 2.19142 |  1:01:21s
epoch 37 | loss: 2.14477 | eval_custom_logloss: 2.19309 |  1:02:58s
epoch 38 | loss: 2.14346 | eval_custom_logloss: 2.19783 |  1:04:35s
epoch 39 | loss: 2.14466 | eval_custom_logloss: 3.12835 |  1:06:14s
epoch 40 | loss: 2.14369 | eval_custom_logloss: 2.19557 |  1:07:57s
epoch 41 | loss: 2.14093 | eval_custom_logloss: 2.17259 |  1:09:53s
epoch 42 | loss: 2.14035 | eval_custom_logloss: 2.16447 |  1:11:48s
epoch 43 | loss: 2.13945 | eval_custom_logloss: 2.17266 |  1:13:53s
epoch 44 | loss: 2.14178 | eval_custom_logloss: 2.17267 |  1:15:56s
epoch 45 | loss: 2.14081 | eval_custom_logloss: 2.17093 |  1:17:40s
epoch 46 | loss: 2.13763 | eval_custom_logloss: 2.18633 |  1:19:33s
epoch 47 | loss: 2.13935 | eval_custom_logloss: 2.1649  |  1:21:28s
epoch 48 | loss: 2.13678 | eval_custom_logloss: 2.16686 |  1:23:03s
epoch 49 | loss: 2.1393  | eval_custom_logloss: 2.16533 |  1:24:50s
epoch 50 | loss: 2.13634 | eval_custom_logloss: 2.16754 |  1:26:24s
epoch 51 | loss: 2.13575 | eval_custom_logloss: 2.18795 |  1:28:04s
epoch 52 | loss: 2.13571 | eval_custom_logloss: 2.17594 |  1:29:55s
epoch 53 | loss: 2.13994 | eval_custom_logloss: 2.17326 |  1:32:03s
epoch 54 | loss: 2.14367 | eval_custom_logloss: 2.18649 |  1:34:11s
epoch 55 | loss: 2.13558 | eval_custom_logloss: 2.22085 |  1:36:12s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 2.15855
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33365,)
Probabilities shape : (33365, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1589, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax', 'n_a': 49, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.41519 | eval_custom_logloss: 2.1962  |  0:01:54s
epoch 1  | loss: 2.19117 | eval_custom_logloss: 2.18995 |  0:03:42s
epoch 2  | loss: 2.18082 | eval_custom_logloss: 2.21996 |  0:05:31s
epoch 3  | loss: 2.17939 | eval_custom_logloss: 2.16904 |  0:07:15s
epoch 4  | loss: 2.17307 | eval_custom_logloss: 2.16927 |  0:08:56s
epoch 5  | loss: 2.17252 | eval_custom_logloss: 2.16622 |  0:10:44s
epoch 6  | loss: 2.17044 | eval_custom_logloss: 2.24084 |  0:12:32s
epoch 7  | loss: 2.16752 | eval_custom_logloss: 2.16619 |  0:14:14s
epoch 8  | loss: 2.18418 | eval_custom_logloss: 2.58783 |  0:16:00s
epoch 9  | loss: 2.16913 | eval_custom_logloss: 2.17141 |  0:17:47s
epoch 10 | loss: 2.16598 | eval_custom_logloss: 2.16333 |  0:19:32s
epoch 11 | loss: 2.16394 | eval_custom_logloss: 2.16858 |  0:21:16s
epoch 12 | loss: 2.15985 | eval_custom_logloss: 2.17878 |  0:23:03s
epoch 13 | loss: 2.15759 | eval_custom_logloss: 2.16438 |  0:24:39s
epoch 14 | loss: 2.15825 | eval_custom_logloss: 2.16827 |  0:26:27s
epoch 15 | loss: 2.161   | eval_custom_logloss: 2.1647  |  0:28:00s
epoch 16 | loss: 2.1551  | eval_custom_logloss: 2.22074 |  0:29:45s
epoch 17 | loss: 2.15607 | eval_custom_logloss: 2.18609 |  0:31:25s
epoch 18 | loss: 2.15271 | eval_custom_logloss: 2.88122 |  0:33:11s
epoch 19 | loss: 2.15055 | eval_custom_logloss: 2.16246 |  0:34:47s
epoch 20 | loss: 2.15196 | eval_custom_logloss: 2.16988 |  0:36:21s
epoch 21 | loss: 2.15049 | eval_custom_logloss: 2.16313 |  0:37:58s
epoch 22 | loss: 2.149   | eval_custom_logloss: 2.15229 |  0:39:37s
epoch 23 | loss: 2.14878 | eval_custom_logloss: 2.15259 |  0:41:16s
epoch 24 | loss: 2.14802 | eval_custom_logloss: 2.15988 |  0:42:51s
epoch 25 | loss: 2.14392 | eval_custom_logloss: 2.1491  |  0:44:27s
epoch 26 | loss: 2.14589 | eval_custom_logloss: 2.15532 |  0:46:13s
epoch 27 | loss: 2.14422 | eval_custom_logloss: 2.14982 |  0:47:56s
epoch 28 | loss: 2.15022 | eval_custom_logloss: 2.16195 |  0:49:32s
epoch 29 | loss: 2.14535 | eval_custom_logloss: 2.15201 |  0:51:20s
epoch 30 | loss: 2.14654 | eval_custom_logloss: 2.15803 |  0:53:20s
epoch 31 | loss: 2.14171 | eval_custom_logloss: 2.16006 |  0:55:24s
epoch 32 | loss: 2.14177 | eval_custom_logloss: 2.16073 |  0:57:16s
epoch 33 | loss: 2.13866 | eval_custom_logloss: 2.15876 |  0:59:00s
epoch 34 | loss: 2.13622 | eval_custom_logloss: 2.15797 |  1:00:39s
epoch 35 | loss: 2.13805 | eval_custom_logloss: 2.15707 |  1:02:22s
epoch 36 | loss: 2.13837 | eval_custom_logloss: 2.15034 |  1:04:08s
epoch 37 | loss: 2.1356  | eval_custom_logloss: 2.15608 |  1:05:53s
epoch 38 | loss: 2.13997 | eval_custom_logloss: 2.1612  |  1:07:39s
epoch 39 | loss: 2.14589 | eval_custom_logloss: 2.15681 |  1:09:14s
epoch 40 | loss: 2.13644 | eval_custom_logloss: 2.18069 |  1:10:52s
epoch 41 | loss: 2.13278 | eval_custom_logloss: 2.17158 |  1:12:28s
epoch 42 | loss: 2.13337 | eval_custom_logloss: 2.17352 |  1:14:06s
epoch 43 | loss: 2.14062 | eval_custom_logloss: 2.18238 |  1:15:43s
epoch 44 | loss: 2.1297  | eval_custom_logloss: 2.16513 |  1:17:30s
epoch 45 | loss: 2.1307  | eval_custom_logloss: 2.17021 |  1:19:03s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 2.1491
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.15415, 'Log Loss - std': 0.004750000000000032} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax', 'n_a': 49, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.51442 | eval_custom_logloss: 2.22073 |  0:01:46s
epoch 1  | loss: 2.21405 | eval_custom_logloss: 2.21927 |  0:03:23s
epoch 2  | loss: 2.18817 | eval_custom_logloss: 2.17641 |  0:05:06s
epoch 3  | loss: 2.1888  | eval_custom_logloss: 2.17531 |  0:06:44s
epoch 4  | loss: 2.18081 | eval_custom_logloss: 2.18579 |  0:08:21s
epoch 5  | loss: 2.17382 | eval_custom_logloss: 2.22692 |  0:09:59s
epoch 6  | loss: 2.17849 | eval_custom_logloss: 2.1774  |  0:11:35s
epoch 7  | loss: 2.16803 | eval_custom_logloss: 2.17304 |  0:13:39s
epoch 8  | loss: 2.1636  | eval_custom_logloss: 2.16176 |  0:15:27s
epoch 9  | loss: 2.16735 | eval_custom_logloss: 2.26494 |  0:17:12s
epoch 10 | loss: 2.16195 | eval_custom_logloss: 2.17331 |  0:19:01s
epoch 11 | loss: 2.16059 | eval_custom_logloss: 2.16479 |  0:20:48s
epoch 12 | loss: 2.16521 | eval_custom_logloss: 2.15862 |  0:22:35s
epoch 13 | loss: 2.15714 | eval_custom_logloss: 2.15898 |  0:24:13s
epoch 14 | loss: 2.15526 | eval_custom_logloss: 2.16549 |  0:26:01s
epoch 15 | loss: 2.1603  | eval_custom_logloss: 2.19342 |  0:27:52s
epoch 16 | loss: 2.15868 | eval_custom_logloss: 2.16042 |  0:29:36s
epoch 17 | loss: 2.15606 | eval_custom_logloss: 2.15211 |  0:31:13s
epoch 18 | loss: 2.15539 | eval_custom_logloss: 2.15719 |  0:32:49s
epoch 19 | loss: 2.1521  | eval_custom_logloss: 2.15537 |  0:34:27s
epoch 20 | loss: 2.15189 | eval_custom_logloss: 2.31729 |  0:36:04s
epoch 21 | loss: 2.15078 | eval_custom_logloss: 2.15109 |  0:37:41s
epoch 22 | loss: 2.14949 | eval_custom_logloss: 2.15312 |  0:39:22s
epoch 23 | loss: 2.14829 | eval_custom_logloss: 2.18593 |  0:41:04s
epoch 24 | loss: 2.1474  | eval_custom_logloss: 2.18889 |  0:42:48s
epoch 25 | loss: 2.14576 | eval_custom_logloss: 2.18759 |  0:44:40s
epoch 26 | loss: 2.1472  | eval_custom_logloss: 2.1613  |  0:46:46s
epoch 27 | loss: 2.14516 | eval_custom_logloss: 2.18262 |  0:48:51s
epoch 28 | loss: 2.15011 | eval_custom_logloss: 2.18159 |  0:50:46s
epoch 29 | loss: 2.1431  | eval_custom_logloss: 2.18828 |  0:52:38s
epoch 30 | loss: 2.14148 | eval_custom_logloss: 2.15138 |  0:54:13s
epoch 31 | loss: 2.14135 | eval_custom_logloss: 2.16219 |  0:56:02s
epoch 32 | loss: 2.14076 | eval_custom_logloss: 2.15805 |  0:57:40s
epoch 33 | loss: 2.14066 | eval_custom_logloss: 2.16168 |  0:59:28s
epoch 34 | loss: 2.14224 | eval_custom_logloss: 2.16875 |  1:01:09s
epoch 35 | loss: 2.13782 | eval_custom_logloss: 2.19023 |  1:03:01s
epoch 36 | loss: 2.14248 | eval_custom_logloss: 2.19594 |  1:04:51s
epoch 37 | loss: 2.13907 | eval_custom_logloss: 2.14995 |  1:06:34s
epoch 38 | loss: 2.13924 | eval_custom_logloss: 2.18794 |  1:08:20s
epoch 39 | loss: 2.14999 | eval_custom_logloss: 2.23686 |  1:09:56s
epoch 40 | loss: 2.13715 | eval_custom_logloss: 2.22777 |  1:11:41s
epoch 41 | loss: 2.13579 | eval_custom_logloss: 2.22488 |  1:13:36s
epoch 42 | loss: 2.13593 | eval_custom_logloss: 2.2424  |  1:15:12s
epoch 43 | loss: 2.13695 | eval_custom_logloss: 2.16958 |  1:16:58s
epoch 44 | loss: 2.13643 | eval_custom_logloss: 2.15802 |  1:18:48s
epoch 45 | loss: 2.15961 | eval_custom_logloss: 2.19535 |  1:20:27s
epoch 46 | loss: 2.14563 | eval_custom_logloss: 2.17803 |  1:22:05s
epoch 47 | loss: 2.1386  | eval_custom_logloss: 2.16497 |  1:23:41s
epoch 48 | loss: 2.14299 | eval_custom_logloss: 2.16751 |  1:25:30s
epoch 49 | loss: 2.13417 | eval_custom_logloss: 2.15606 |  1:27:19s
epoch 50 | loss: 2.13315 | eval_custom_logloss: 2.17693 |  1:29:05s
epoch 51 | loss: 2.13761 | eval_custom_logloss: 2.17159 |  1:30:51s
epoch 52 | loss: 2.13274 | eval_custom_logloss: 2.16207 |  1:32:28s
epoch 53 | loss: 2.13422 | eval_custom_logloss: 2.17742 |  1:34:27s
epoch 54 | loss: 2.13743 | eval_custom_logloss: 2.28801 |  1:36:24s
epoch 55 | loss: 2.13457 | eval_custom_logloss: 2.18051 |  1:38:13s
epoch 56 | loss: 2.13039 | eval_custom_logloss: 2.17355 |  1:39:53s
epoch 57 | loss: 2.13427 | eval_custom_logloss: 2.18272 |  1:41:31s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 2.14995
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1528666666666667, 'Log Loss - std': 0.004282003684673286} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax', 'n_a': 49, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.47416 | eval_custom_logloss: 2.25125 |  0:01:37s
epoch 1  | loss: 2.22049 | eval_custom_logloss: 2.2191  |  0:03:15s
epoch 2  | loss: 2.22817 | eval_custom_logloss: 2.21467 |  0:05:08s
epoch 3  | loss: 2.24455 | eval_custom_logloss: 2.24555 |  0:07:14s
epoch 4  | loss: 2.22263 | eval_custom_logloss: 2.20951 |  0:09:11s
epoch 5  | loss: 2.19691 | eval_custom_logloss: 2.21131 |  0:11:16s
epoch 6  | loss: 2.18699 | eval_custom_logloss: 2.1939  |  0:13:08s
epoch 7  | loss: 2.18626 | eval_custom_logloss: 2.19007 |  0:15:03s
epoch 8  | loss: 2.18166 | eval_custom_logloss: 2.18511 |  0:17:10s
epoch 9  | loss: 2.18333 | eval_custom_logloss: 2.19592 |  0:19:14s
epoch 10 | loss: 2.17949 | eval_custom_logloss: 2.17909 |  0:21:24s
epoch 11 | loss: 2.17526 | eval_custom_logloss: 2.17551 |  0:23:30s
epoch 12 | loss: 2.17794 | eval_custom_logloss: 2.18363 |  0:25:33s
epoch 13 | loss: 2.17581 | eval_custom_logloss: 2.18469 |  0:27:29s
epoch 14 | loss: 2.17386 | eval_custom_logloss: 2.21652 |  0:29:35s
epoch 15 | loss: 2.17816 | eval_custom_logloss: 2.18139 |  0:31:39s
epoch 16 | loss: 2.17414 | eval_custom_logloss: 2.18256 |  0:33:28s
epoch 17 | loss: 2.17254 | eval_custom_logloss: 2.18509 |  0:35:03s
epoch 18 | loss: 2.16998 | eval_custom_logloss: 2.17941 |  0:36:40s
epoch 19 | loss: 2.17115 | eval_custom_logloss: 2.17724 |  0:38:17s
epoch 20 | loss: 2.16991 | eval_custom_logloss: 2.22741 |  0:39:55s
epoch 21 | loss: 2.168   | eval_custom_logloss: 2.17756 |  0:41:32s
epoch 22 | loss: 2.16819 | eval_custom_logloss: 2.17912 |  0:43:13s
epoch 23 | loss: 2.1678  | eval_custom_logloss: 2.17848 |  0:44:57s
epoch 24 | loss: 2.16757 | eval_custom_logloss: 2.17595 |  0:46:35s
epoch 25 | loss: 2.16724 | eval_custom_logloss: 2.18514 |  0:48:12s
epoch 26 | loss: 2.16883 | eval_custom_logloss: 2.1721  |  0:49:50s
epoch 27 | loss: 2.16438 | eval_custom_logloss: 2.17848 |  0:51:36s
epoch 28 | loss: 2.16568 | eval_custom_logloss: 2.1762  |  0:53:12s
epoch 29 | loss: 2.16551 | eval_custom_logloss: 2.17192 |  0:54:50s
epoch 30 | loss: 2.16054 | eval_custom_logloss: 2.19366 |  0:56:30s
epoch 31 | loss: 2.16021 | eval_custom_logloss: 2.17978 |  0:58:08s
epoch 32 | loss: 2.15914 | eval_custom_logloss: 2.17936 |  0:59:46s
epoch 33 | loss: 2.15939 | eval_custom_logloss: 2.18157 |  1:01:25s
epoch 34 | loss: 2.15853 | eval_custom_logloss: 2.17651 |  1:03:03s
epoch 35 | loss: 2.17249 | eval_custom_logloss: 2.177   |  1:04:41s
epoch 36 | loss: 2.15623 | eval_custom_logloss: 2.18493 |  1:06:22s
epoch 37 | loss: 2.15794 | eval_custom_logloss: 2.18601 |  1:08:06s
epoch 38 | loss: 2.17761 | eval_custom_logloss: 2.17425 |  1:09:41s
epoch 39 | loss: 2.15643 | eval_custom_logloss: 2.17374 |  1:11:25s
epoch 40 | loss: 2.15617 | eval_custom_logloss: 2.17923 |  1:13:09s
epoch 41 | loss: 2.16407 | eval_custom_logloss: 2.18998 |  1:14:52s
epoch 42 | loss: 2.16485 | eval_custom_logloss: 2.18629 |  1:16:31s
epoch 43 | loss: 2.15584 | eval_custom_logloss: 2.18522 |  1:18:10s
epoch 44 | loss: 2.15582 | eval_custom_logloss: 2.19185 |  1:19:49s
epoch 45 | loss: 2.15439 | eval_custom_logloss: 2.1782  |  1:21:27s
epoch 46 | loss: 2.15454 | eval_custom_logloss: 2.18384 |  1:23:17s
epoch 47 | loss: 2.15306 | eval_custom_logloss: 2.17429 |  1:25:11s
epoch 48 | loss: 2.15627 | eval_custom_logloss: 2.17795 |  1:26:54s
epoch 49 | loss: 2.15136 | eval_custom_logloss: 2.17279 |  1:28:43s

Early stopping occurred at epoch 49 with best_epoch = 29 and best_eval_custom_logloss = 2.17192
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1576000000000004, 'Log Loss - std': 0.008998055345462207} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax', 'n_a': 49, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.40867 | eval_custom_logloss: 2.20668 |  0:01:49s
epoch 1  | loss: 2.19496 | eval_custom_logloss: 2.1852  |  0:03:27s
epoch 2  | loss: 2.20308 | eval_custom_logloss: 2.18879 |  0:05:36s
epoch 3  | loss: 2.18519 | eval_custom_logloss: 2.18156 |  0:07:37s
epoch 4  | loss: 2.17879 | eval_custom_logloss: 2.18677 |  0:09:38s
epoch 5  | loss: 2.17212 | eval_custom_logloss: 3.42828 |  0:11:45s
epoch 6  | loss: 2.17237 | eval_custom_logloss: 2.1643  |  0:13:50s
epoch 7  | loss: 2.17203 | eval_custom_logloss: 2.17636 |  0:15:37s
epoch 8  | loss: 2.16683 | eval_custom_logloss: 2.62371 |  0:17:15s
epoch 9  | loss: 2.17389 | eval_custom_logloss: 2.17541 |  0:18:54s
epoch 10 | loss: 2.16754 | eval_custom_logloss: 2.37667 |  0:20:33s
epoch 11 | loss: 2.16523 | eval_custom_logloss: 2.16417 |  0:22:23s
epoch 12 | loss: 2.18567 | eval_custom_logloss: 2.18402 |  0:24:31s
epoch 13 | loss: 2.17389 | eval_custom_logloss: 2.18601 |  0:26:40s
epoch 14 | loss: 2.17285 | eval_custom_logloss: 2.17578 |  0:28:37s
epoch 15 | loss: 2.16986 | eval_custom_logloss: 2.17912 |  0:30:38s
epoch 16 | loss: 2.17117 | eval_custom_logloss: 2.20232 |  0:32:43s
epoch 17 | loss: 2.16781 | eval_custom_logloss: 2.17576 |  0:34:50s
epoch 18 | loss: 2.16707 | eval_custom_logloss: 2.21688 |  0:36:46s
epoch 19 | loss: 2.16496 | eval_custom_logloss: 2.23046 |  0:38:24s
epoch 20 | loss: 2.16431 | eval_custom_logloss: 2.20566 |  0:40:02s
epoch 21 | loss: 2.16285 | eval_custom_logloss: 2.21437 |  0:41:41s
epoch 22 | loss: 2.16923 | eval_custom_logloss: 2.20874 |  0:43:19s
epoch 23 | loss: 2.16595 | eval_custom_logloss: 2.19518 |  0:44:57s
epoch 24 | loss: 2.16565 | eval_custom_logloss: 2.28653 |  0:46:51s
epoch 25 | loss: 2.16002 | eval_custom_logloss: 2.18555 |  0:48:55s
epoch 26 | loss: 2.16038 | eval_custom_logloss: 2.17856 |  0:50:55s
epoch 27 | loss: 2.15741 | eval_custom_logloss: 2.16873 |  0:53:05s
epoch 28 | loss: 2.15563 | eval_custom_logloss: 2.16635 |  0:55:12s
epoch 29 | loss: 2.15668 | eval_custom_logloss: 2.21783 |  0:57:23s
epoch 30 | loss: 2.15375 | eval_custom_logloss: 2.18162 |  0:59:25s
epoch 31 | loss: 2.15319 | eval_custom_logloss: 2.17511 |  1:01:25s

Early stopping occurred at epoch 31 with best_epoch = 11 and best_eval_custom_logloss = 2.16417
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1589600000000004, 'Log Loss - std': 0.00849531635667565} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 2.1589600000000004 and parameters: {'n_d': 49, 'n_steps': 9, 'gamma': 1.369955001148331, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.004367655077816197, 'mask_type': 'entmax'}. Best is trial 3 with value: 2.17544.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133456, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133456, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]] 
 
 
Val : (133456, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax', 'n_a': 22, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.34213 | eval_custom_logloss: 2.28162 |  0:01:12s
epoch 1  | loss: 2.19581 | eval_custom_logloss: 2.34605 |  0:02:24s
epoch 2  | loss: 2.18566 | eval_custom_logloss: 2.18755 |  0:03:35s
epoch 3  | loss: 2.18475 | eval_custom_logloss: 2.20942 |  0:04:48s
epoch 4  | loss: 2.1795  | eval_custom_logloss: 2.19347 |  0:06:05s
epoch 5  | loss: 2.17599 | eval_custom_logloss: 2.17612 |  0:07:18s
epoch 6  | loss: 2.17351 | eval_custom_logloss: 2.16909 |  0:08:42s
epoch 7  | loss: 2.17181 | eval_custom_logloss: 2.24443 |  0:10:06s
epoch 8  | loss: 2.16761 | eval_custom_logloss: 2.16863 |  0:11:29s
epoch 9  | loss: 2.16882 | eval_custom_logloss: 2.16848 |  0:12:53s
epoch 10 | loss: 2.16703 | eval_custom_logloss: 2.16532 |  0:14:14s
epoch 11 | loss: 2.16606 | eval_custom_logloss: 2.16562 |  0:15:29s
epoch 12 | loss: 2.16517 | eval_custom_logloss: 2.16455 |  0:16:42s
epoch 13 | loss: 2.16652 | eval_custom_logloss: 2.17207 |  0:17:54s
epoch 14 | loss: 2.16358 | eval_custom_logloss: 2.16289 |  0:19:04s
epoch 15 | loss: 2.16365 | eval_custom_logloss: 2.16132 |  0:20:17s
epoch 16 | loss: 2.16471 | eval_custom_logloss: 2.28895 |  0:21:32s
epoch 17 | loss: 2.16265 | eval_custom_logloss: 2.1669  |  0:22:46s
epoch 18 | loss: 2.16244 | eval_custom_logloss: 2.15991 |  0:23:59s
epoch 19 | loss: 2.16151 | eval_custom_logloss: 2.16204 |  0:25:13s
epoch 20 | loss: 2.16037 | eval_custom_logloss: 2.16182 |  0:26:28s
epoch 21 | loss: 2.15961 | eval_custom_logloss: 2.16302 |  0:27:43s
epoch 22 | loss: 2.16017 | eval_custom_logloss: 2.16876 |  0:28:57s
epoch 23 | loss: 2.16195 | eval_custom_logloss: 2.16033 |  0:30:10s
epoch 24 | loss: 2.16092 | eval_custom_logloss: 2.16683 |  0:31:24s
epoch 25 | loss: 2.15855 | eval_custom_logloss: 2.1712  |  0:32:37s
epoch 26 | loss: 2.18512 | eval_custom_logloss: 2.18004 |  0:33:51s
epoch 27 | loss: 2.16174 | eval_custom_logloss: 2.17167 |  0:35:04s
epoch 28 | loss: 2.1623  | eval_custom_logloss: 2.16157 |  0:36:19s
epoch 29 | loss: 2.16982 | eval_custom_logloss: 2.17592 |  0:37:34s
epoch 30 | loss: 2.19911 | eval_custom_logloss: 2.19384 |  0:38:57s
epoch 31 | loss: 2.17706 | eval_custom_logloss: 2.26025 |  0:40:20s
epoch 32 | loss: 2.1641  | eval_custom_logloss: 2.25104 |  0:41:44s
epoch 33 | loss: 2.16173 | eval_custom_logloss: 2.2564  |  0:43:04s
epoch 34 | loss: 2.15985 | eval_custom_logloss: 2.25377 |  0:44:24s
epoch 35 | loss: 2.15765 | eval_custom_logloss: 2.1654  |  0:45:37s
epoch 36 | loss: 2.16    | eval_custom_logloss: 2.16723 |  0:46:50s
epoch 37 | loss: 2.15675 | eval_custom_logloss: 2.25893 |  0:48:04s
epoch 38 | loss: 2.15528 | eval_custom_logloss: 2.16614 |  0:49:19s

Early stopping occurred at epoch 38 with best_epoch = 18 and best_eval_custom_logloss = 2.15991
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33365,)
Probabilities shape : (33365, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1602, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax', 'n_a': 22, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.32079 | eval_custom_logloss: 2.26502 |  0:01:13s
epoch 1  | loss: 2.19708 | eval_custom_logloss: 2.25797 |  0:02:26s
epoch 2  | loss: 2.18    | eval_custom_logloss: 2.22597 |  0:03:40s
epoch 3  | loss: 2.17107 | eval_custom_logloss: 2.16958 |  0:04:54s
epoch 4  | loss: 2.17203 | eval_custom_logloss: 2.16293 |  0:06:08s
epoch 5  | loss: 2.16498 | eval_custom_logloss: 2.16236 |  0:07:22s
epoch 6  | loss: 2.16555 | eval_custom_logloss: 2.15822 |  0:08:35s
epoch 7  | loss: 2.16141 | eval_custom_logloss: 2.16726 |  0:09:53s
epoch 8  | loss: 2.16005 | eval_custom_logloss: 2.15581 |  0:11:16s
epoch 9  | loss: 2.16091 | eval_custom_logloss: 2.15589 |  0:12:31s
epoch 10 | loss: 2.15866 | eval_custom_logloss: 2.15942 |  0:13:45s
epoch 11 | loss: 2.15564 | eval_custom_logloss: 2.15796 |  0:14:59s
epoch 12 | loss: 2.15427 | eval_custom_logloss: 2.15294 |  0:16:16s
epoch 13 | loss: 2.15322 | eval_custom_logloss: 2.16556 |  0:17:33s
epoch 14 | loss: 2.152   | eval_custom_logloss: 2.15052 |  0:18:47s
epoch 15 | loss: 2.15279 | eval_custom_logloss: 2.1566  |  0:20:00s
epoch 16 | loss: 2.15103 | eval_custom_logloss: 2.14929 |  0:21:14s
epoch 17 | loss: 2.14822 | eval_custom_logloss: 2.16275 |  0:22:28s
epoch 18 | loss: 2.15147 | eval_custom_logloss: 2.15434 |  0:23:46s
epoch 19 | loss: 2.15797 | eval_custom_logloss: 2.17932 |  0:25:08s
epoch 20 | loss: 2.15168 | eval_custom_logloss: 2.16649 |  0:26:30s
epoch 21 | loss: 2.14768 | eval_custom_logloss: 2.16077 |  0:27:43s
epoch 22 | loss: 2.14788 | eval_custom_logloss: 2.17173 |  0:28:58s
epoch 23 | loss: 2.14491 | eval_custom_logloss: 2.16434 |  0:30:12s
epoch 24 | loss: 2.14509 | eval_custom_logloss: 2.15414 |  0:31:25s
epoch 25 | loss: 2.14349 | eval_custom_logloss: 2.14675 |  0:32:38s
epoch 26 | loss: 2.14902 | eval_custom_logloss: 2.16845 |  0:33:50s
epoch 27 | loss: 2.14463 | eval_custom_logloss: 2.15431 |  0:35:03s
epoch 28 | loss: 2.15391 | eval_custom_logloss: 2.17025 |  0:36:16s
epoch 29 | loss: 2.15627 | eval_custom_logloss: 2.15318 |  0:37:30s
epoch 30 | loss: 2.14817 | eval_custom_logloss: 2.1733  |  0:38:44s
epoch 31 | loss: 2.14721 | eval_custom_logloss: 2.16148 |  0:39:58s
epoch 32 | loss: 2.14894 | eval_custom_logloss: 2.15299 |  0:41:12s
epoch 33 | loss: 2.14339 | eval_custom_logloss: 2.16563 |  0:42:25s
epoch 34 | loss: 2.14804 | eval_custom_logloss: 2.16628 |  0:43:38s
epoch 35 | loss: 2.14703 | eval_custom_logloss: 2.16282 |  0:44:51s
epoch 36 | loss: 2.14488 | eval_custom_logloss: 2.16338 |  0:46:04s
epoch 37 | loss: 2.14072 | eval_custom_logloss: 2.16115 |  0:47:17s
epoch 38 | loss: 2.14231 | eval_custom_logloss: 2.16346 |  0:48:31s
epoch 39 | loss: 2.14906 | eval_custom_logloss: 2.18346 |  0:49:53s
epoch 40 | loss: 2.13989 | eval_custom_logloss: 2.1813  |  0:51:17s
epoch 41 | loss: 2.13886 | eval_custom_logloss: 2.17952 |  0:52:42s
epoch 42 | loss: 2.13821 | eval_custom_logloss: 2.18128 |  0:54:03s
epoch 43 | loss: 2.13935 | eval_custom_logloss: 2.15927 |  0:55:25s
epoch 44 | loss: 2.13739 | eval_custom_logloss: 2.17902 |  0:56:49s
epoch 45 | loss: 2.14417 | eval_custom_logloss: 2.18391 |  0:58:14s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 2.14675
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.15365, 'Log Loss - std': 0.006550000000000056} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax', 'n_a': 22, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.37607 | eval_custom_logloss: 2.31174 |  0:01:12s
epoch 1  | loss: 2.20601 | eval_custom_logloss: 2.19541 |  0:02:23s
epoch 2  | loss: 2.19327 | eval_custom_logloss: 2.18522 |  0:03:37s
epoch 3  | loss: 2.18646 | eval_custom_logloss: 2.19604 |  0:04:52s
epoch 4  | loss: 2.18121 | eval_custom_logloss: 2.18128 |  0:06:06s
epoch 5  | loss: 2.18226 | eval_custom_logloss: 2.17339 |  0:07:20s
epoch 6  | loss: 2.18834 | eval_custom_logloss: 2.25504 |  0:08:34s
epoch 7  | loss: 2.21817 | eval_custom_logloss: 2.20789 |  0:09:48s
epoch 8  | loss: 2.21369 | eval_custom_logloss: 2.20543 |  0:10:58s
epoch 9  | loss: 2.21389 | eval_custom_logloss: 2.23549 |  0:12:13s
epoch 10 | loss: 2.21115 | eval_custom_logloss: 2.2079  |  0:13:28s
epoch 11 | loss: 2.20843 | eval_custom_logloss: 2.18765 |  0:14:42s
epoch 12 | loss: 2.17963 | eval_custom_logloss: 2.17185 |  0:15:58s
epoch 13 | loss: 2.17643 | eval_custom_logloss: 2.17443 |  0:17:13s
epoch 14 | loss: 2.17287 | eval_custom_logloss: 2.17385 |  0:18:26s
epoch 15 | loss: 2.17203 | eval_custom_logloss: 2.17393 |  0:19:39s
epoch 16 | loss: 2.17086 | eval_custom_logloss: 2.17332 |  0:20:53s
epoch 17 | loss: 2.17165 | eval_custom_logloss: 2.1741  |  0:21:54s
epoch 18 | loss: 2.18135 | eval_custom_logloss: 2.16675 |  0:23:05s
epoch 19 | loss: 2.16892 | eval_custom_logloss: 2.21159 |  0:24:18s
epoch 20 | loss: 2.17281 | eval_custom_logloss: 2.18457 |  0:25:30s
epoch 21 | loss: 2.17148 | eval_custom_logloss: 2.17008 |  0:26:42s
epoch 22 | loss: 2.1659  | eval_custom_logloss: 2.17471 |  0:27:55s
epoch 23 | loss: 2.17122 | eval_custom_logloss: 2.17781 |  0:29:08s
epoch 24 | loss: 2.17481 | eval_custom_logloss: 2.17223 |  0:30:20s
epoch 25 | loss: 2.17234 | eval_custom_logloss: 2.18367 |  0:31:33s
epoch 26 | loss: 2.17156 | eval_custom_logloss: 2.17911 |  0:32:45s
epoch 27 | loss: 2.19017 | eval_custom_logloss: 2.18479 |  0:33:58s
epoch 28 | loss: 2.17052 | eval_custom_logloss: 2.16327 |  0:35:11s
epoch 29 | loss: 2.16403 | eval_custom_logloss: 2.16222 |  0:36:24s
epoch 30 | loss: 2.16159 | eval_custom_logloss: 2.1694  |  0:37:37s
epoch 31 | loss: 2.17829 | eval_custom_logloss: 2.1613  |  0:38:50s
epoch 32 | loss: 2.15933 | eval_custom_logloss: 2.16079 |  0:40:02s
epoch 33 | loss: 2.15842 | eval_custom_logloss: 2.1621  |  0:41:16s
epoch 34 | loss: 2.1568  | eval_custom_logloss: 2.17939 |  0:42:29s
epoch 35 | loss: 2.1595  | eval_custom_logloss: 2.17828 |  0:43:44s
epoch 36 | loss: 2.15632 | eval_custom_logloss: 2.17512 |  0:44:56s
epoch 37 | loss: 2.15575 | eval_custom_logloss: 2.16193 |  0:46:09s
epoch 38 | loss: 2.15421 | eval_custom_logloss: 2.16319 |  0:47:23s
epoch 39 | loss: 2.15821 | eval_custom_logloss: 2.16905 |  0:48:36s
epoch 40 | loss: 2.16757 | eval_custom_logloss: 2.183   |  0:49:49s
epoch 41 | loss: 2.1552  | eval_custom_logloss: 2.1721  |  0:51:02s
epoch 42 | loss: 2.1581  | eval_custom_logloss: 2.16858 |  0:52:16s
epoch 43 | loss: 2.15393 | eval_custom_logloss: 2.16663 |  0:53:28s
epoch 44 | loss: 2.15572 | eval_custom_logloss: 2.17018 |  0:54:42s
epoch 45 | loss: 2.15179 | eval_custom_logloss: 2.17186 |  0:55:55s
epoch 46 | loss: 2.15165 | eval_custom_logloss: 2.17551 |  0:57:10s
epoch 47 | loss: 2.15236 | eval_custom_logloss: 2.19753 |  0:58:25s
epoch 48 | loss: 2.15314 | eval_custom_logloss: 2.16857 |  0:59:39s
epoch 49 | loss: 2.15195 | eval_custom_logloss: 2.15926 |  1:00:54s
epoch 50 | loss: 2.15064 | eval_custom_logloss: 2.1676  |  1:02:08s
epoch 51 | loss: 2.15026 | eval_custom_logloss: 2.17794 |  1:03:23s
epoch 52 | loss: 2.15034 | eval_custom_logloss: 2.26923 |  1:04:37s
epoch 53 | loss: 2.15779 | eval_custom_logloss: 2.53549 |  1:05:51s
epoch 54 | loss: 2.15027 | eval_custom_logloss: 2.16981 |  1:07:05s
epoch 55 | loss: 2.14974 | eval_custom_logloss: 2.22716 |  1:08:20s
epoch 56 | loss: 2.15037 | eval_custom_logloss: 2.16976 |  1:09:36s
epoch 57 | loss: 2.14843 | eval_custom_logloss: 2.1662  |  1:11:00s
epoch 58 | loss: 2.17754 | eval_custom_logloss: 2.18734 |  1:12:22s
epoch 59 | loss: 2.17232 | eval_custom_logloss: 2.19393 |  1:13:45s
epoch 60 | loss: 2.18085 | eval_custom_logloss: 2.18757 |  1:15:00s
epoch 61 | loss: 2.16995 | eval_custom_logloss: 2.19394 |  1:16:14s
epoch 62 | loss: 2.17095 | eval_custom_logloss: 2.23068 |  1:17:28s
epoch 63 | loss: 2.17126 | eval_custom_logloss: 2.25345 |  1:18:42s
epoch 64 | loss: 2.23232 | eval_custom_logloss: 3.02474 |  1:19:55s
epoch 65 | loss: 2.26921 | eval_custom_logloss: 2.18378 |  1:21:09s
epoch 66 | loss: 2.17307 | eval_custom_logloss: 2.18836 |  1:22:23s
epoch 67 | loss: 2.16865 | eval_custom_logloss: 2.2225  |  1:23:36s
epoch 68 | loss: 2.16465 | eval_custom_logloss: 2.18246 |  1:24:49s
epoch 69 | loss: 2.15666 | eval_custom_logloss: 2.1691  |  1:26:04s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 2.15926
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1555666666666666, 'Log Loss - std': 0.005995739227892566} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax', 'n_a': 22, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.30595 | eval_custom_logloss: 2.25485 |  0:01:20s
epoch 1  | loss: 2.20423 | eval_custom_logloss: 2.21038 |  0:02:34s
epoch 2  | loss: 2.19756 | eval_custom_logloss: 2.20027 |  0:03:48s
epoch 3  | loss: 2.18831 | eval_custom_logloss: 2.20076 |  0:05:00s
epoch 4  | loss: 2.20319 | eval_custom_logloss: 2.21103 |  0:06:14s
epoch 5  | loss: 2.19047 | eval_custom_logloss: 2.24658 |  0:07:28s
epoch 6  | loss: 2.18227 | eval_custom_logloss: 2.19504 |  0:08:40s
epoch 7  | loss: 2.18479 | eval_custom_logloss: 2.44583 |  0:09:53s
epoch 8  | loss: 2.20284 | eval_custom_logloss: 2.22797 |  0:11:07s
epoch 9  | loss: 2.1842  | eval_custom_logloss: 2.1901  |  0:12:18s
epoch 10 | loss: 2.17772 | eval_custom_logloss: 2.18824 |  0:13:32s
epoch 11 | loss: 2.17732 | eval_custom_logloss: 2.18864 |  0:14:45s
epoch 12 | loss: 2.20813 | eval_custom_logloss: 2.19868 |  0:15:58s
epoch 13 | loss: 2.18028 | eval_custom_logloss: 2.19043 |  0:17:12s
epoch 14 | loss: 2.17694 | eval_custom_logloss: 2.19649 |  0:18:27s
epoch 15 | loss: 2.17357 | eval_custom_logloss: 2.19083 |  0:19:40s
epoch 16 | loss: 2.17322 | eval_custom_logloss: 2.20314 |  0:20:54s
epoch 17 | loss: 2.17729 | eval_custom_logloss: 2.22711 |  0:22:08s
epoch 18 | loss: 2.16828 | eval_custom_logloss: 2.18457 |  0:23:22s
epoch 19 | loss: 2.16937 | eval_custom_logloss: 2.19527 |  0:24:37s
epoch 20 | loss: 2.16636 | eval_custom_logloss: 2.18206 |  0:25:51s
epoch 21 | loss: 2.17939 | eval_custom_logloss: 2.22191 |  0:27:06s
epoch 22 | loss: 2.18196 | eval_custom_logloss: 2.23962 |  0:28:21s
epoch 23 | loss: 2.17692 | eval_custom_logloss: 2.29877 |  0:29:35s
epoch 24 | loss: 2.17327 | eval_custom_logloss: 2.25187 |  0:30:48s
epoch 25 | loss: 2.20961 | eval_custom_logloss: 2.26241 |  0:32:02s
epoch 26 | loss: 2.21957 | eval_custom_logloss: 2.22759 |  0:33:15s
epoch 27 | loss: 2.19975 | eval_custom_logloss: 2.21081 |  0:34:28s
epoch 28 | loss: 2.18993 | eval_custom_logloss: 2.21713 |  0:35:41s
epoch 29 | loss: 2.18738 | eval_custom_logloss: 2.28429 |  0:36:54s
epoch 30 | loss: 2.18547 | eval_custom_logloss: 2.31308 |  0:38:07s
epoch 31 | loss: 2.18609 | eval_custom_logloss: 2.31547 |  0:39:20s
epoch 32 | loss: 2.18932 | eval_custom_logloss: 2.30305 |  0:40:34s
epoch 33 | loss: 2.20102 | eval_custom_logloss: 2.21895 |  0:41:47s
epoch 34 | loss: 2.18777 | eval_custom_logloss: 2.23571 |  0:43:01s
epoch 35 | loss: 2.18608 | eval_custom_logloss: 2.40992 |  0:44:14s
epoch 36 | loss: 2.18322 | eval_custom_logloss: 2.36953 |  0:45:27s
epoch 37 | loss: 2.18559 | eval_custom_logloss: 2.2159  |  0:46:40s
epoch 38 | loss: 2.18358 | eval_custom_logloss: 2.2293  |  0:47:54s
epoch 39 | loss: 2.18541 | eval_custom_logloss: 2.21129 |  0:49:07s
epoch 40 | loss: 2.18266 | eval_custom_logloss: 2.20945 |  0:50:21s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 2.18206
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.161625, 'Log Loss - std': 0.011707769855954689} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax', 'n_a': 22, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.29814 | eval_custom_logloss: 2.19528 |  0:01:13s
epoch 1  | loss: 2.18834 | eval_custom_logloss: 2.20255 |  0:02:23s
epoch 2  | loss: 2.17818 | eval_custom_logloss: 2.41934 |  0:03:37s
epoch 3  | loss: 2.17172 | eval_custom_logloss: 2.7439  |  0:04:50s
epoch 4  | loss: 2.16895 | eval_custom_logloss: 2.16308 |  0:06:04s
epoch 5  | loss: 2.16391 | eval_custom_logloss: 2.15654 |  0:07:17s
epoch 6  | loss: 2.16099 | eval_custom_logloss: 2.17389 |  0:08:29s
epoch 7  | loss: 2.16149 | eval_custom_logloss: 2.15438 |  0:09:42s
epoch 8  | loss: 2.15586 | eval_custom_logloss: 2.14885 |  0:10:55s
epoch 9  | loss: 2.15819 | eval_custom_logloss: 2.16578 |  0:12:17s
epoch 10 | loss: 2.18104 | eval_custom_logloss: 2.16927 |  0:13:37s
epoch 11 | loss: 2.16328 | eval_custom_logloss: 2.19227 |  0:14:56s
epoch 12 | loss: 2.24968 | eval_custom_logloss: 2.31016 |  0:16:09s
epoch 13 | loss: 2.229   | eval_custom_logloss: 2.19731 |  0:17:22s
epoch 14 | loss: 2.19609 | eval_custom_logloss: 2.19482 |  0:18:35s
epoch 15 | loss: 2.19217 | eval_custom_logloss: 2.18833 |  0:19:48s
epoch 16 | loss: 2.19153 | eval_custom_logloss: 2.19689 |  0:20:59s
epoch 17 | loss: 2.18994 | eval_custom_logloss: 2.19899 |  0:22:11s
epoch 18 | loss: 2.18812 | eval_custom_logloss: 2.18476 |  0:23:22s
epoch 19 | loss: 2.1921  | eval_custom_logloss: 2.19385 |  0:24:34s
epoch 20 | loss: 2.18789 | eval_custom_logloss: 2.18299 |  0:25:45s
epoch 21 | loss: 2.1847  | eval_custom_logloss: 2.20436 |  0:27:02s
epoch 22 | loss: 2.18404 | eval_custom_logloss: 2.18256 |  0:28:14s
epoch 23 | loss: 2.18126 | eval_custom_logloss: 2.17937 |  0:29:26s
epoch 24 | loss: 2.19018 | eval_custom_logloss: 2.5101  |  0:30:37s
epoch 25 | loss: 2.22761 | eval_custom_logloss: 2.23058 |  0:31:49s
epoch 26 | loss: 2.21265 | eval_custom_logloss: 2.22287 |  0:33:01s
epoch 27 | loss: 2.20473 | eval_custom_logloss: 2.21409 |  0:34:12s
epoch 28 | loss: 2.2026  | eval_custom_logloss: 2.21828 |  0:35:24s

Early stopping occurred at epoch 28 with best_epoch = 8 and best_eval_custom_logloss = 2.14885
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.15912, 'Log Loss - std': 0.011608514116802478} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 2.15912 and parameters: {'n_d': 22, 'n_steps': 4, 'gamma': 1.3681974124917793, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.0019192924647460012, 'mask_type': 'entmax'}. Best is trial 3 with value: 2.17544.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133456, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133456, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]] 
 
 
Val : (133456, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax', 'n_a': 21, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.39482 | eval_custom_logloss: 2.25232 |  0:01:11s
epoch 1  | loss: 2.25793 | eval_custom_logloss: 2.24514 |  0:02:15s
epoch 2  | loss: 2.24409 | eval_custom_logloss: 2.22744 |  0:03:20s
epoch 3  | loss: 2.22627 | eval_custom_logloss: 2.21962 |  0:04:26s
epoch 4  | loss: 2.21771 | eval_custom_logloss: 2.20868 |  0:05:32s
epoch 5  | loss: 2.21679 | eval_custom_logloss: 2.19247 |  0:06:38s
epoch 6  | loss: 2.20217 | eval_custom_logloss: 2.19965 |  0:07:44s
epoch 7  | loss: 2.19489 | eval_custom_logloss: 2.18707 |  0:08:49s
epoch 8  | loss: 2.19384 | eval_custom_logloss: 2.1857  |  0:09:56s
epoch 9  | loss: 2.19763 | eval_custom_logloss: 2.18976 |  0:11:02s
epoch 10 | loss: 2.19832 | eval_custom_logloss: 2.19007 |  0:12:08s
epoch 11 | loss: 2.19737 | eval_custom_logloss: 2.19066 |  0:13:13s
epoch 12 | loss: 2.19355 | eval_custom_logloss: 2.18575 |  0:14:19s
epoch 13 | loss: 2.19138 | eval_custom_logloss: 2.1908  |  0:15:25s
epoch 14 | loss: 2.19036 | eval_custom_logloss: 2.18637 |  0:16:31s
epoch 15 | loss: 2.21131 | eval_custom_logloss: 2.18382 |  0:17:37s
epoch 16 | loss: 2.18726 | eval_custom_logloss: 2.17916 |  0:18:42s
epoch 17 | loss: 2.19163 | eval_custom_logloss: 2.2106  |  0:19:47s
epoch 18 | loss: 2.186   | eval_custom_logloss: 2.17743 |  0:20:53s
epoch 19 | loss: 2.18064 | eval_custom_logloss: 2.179   |  0:21:58s
epoch 20 | loss: 2.1844  | eval_custom_logloss: 2.18368 |  0:23:02s
epoch 21 | loss: 2.17813 | eval_custom_logloss: 2.18145 |  0:24:06s
epoch 22 | loss: 2.1769  | eval_custom_logloss: 2.17846 |  0:25:11s
epoch 23 | loss: 2.17518 | eval_custom_logloss: 2.17728 |  0:26:15s
epoch 24 | loss: 2.17336 | eval_custom_logloss: 2.17012 |  0:27:21s
epoch 25 | loss: 2.17305 | eval_custom_logloss: 2.17083 |  0:28:27s
epoch 26 | loss: 2.1732  | eval_custom_logloss: 2.16972 |  0:29:33s
epoch 27 | loss: 2.1729  | eval_custom_logloss: 2.1688  |  0:30:39s
epoch 28 | loss: 2.17063 | eval_custom_logloss: 2.17503 |  0:31:44s
epoch 29 | loss: 2.17045 | eval_custom_logloss: 2.18038 |  0:32:52s
epoch 30 | loss: 2.17479 | eval_custom_logloss: 2.16811 |  0:34:00s
epoch 31 | loss: 2.16677 | eval_custom_logloss: 2.16861 |  0:35:09s
epoch 32 | loss: 2.16969 | eval_custom_logloss: 2.16702 |  0:36:12s
epoch 33 | loss: 2.16478 | eval_custom_logloss: 2.16857 |  0:37:17s
epoch 34 | loss: 2.16943 | eval_custom_logloss: 2.1705  |  0:38:20s
epoch 35 | loss: 2.16768 | eval_custom_logloss: 2.17347 |  0:39:25s
epoch 36 | loss: 2.16433 | eval_custom_logloss: 2.16528 |  0:40:28s
epoch 37 | loss: 2.18642 | eval_custom_logloss: 2.17979 |  0:41:33s
epoch 38 | loss: 2.17868 | eval_custom_logloss: 2.16604 |  0:42:37s
epoch 39 | loss: 2.17277 | eval_custom_logloss: 2.16175 |  0:43:41s
epoch 40 | loss: 2.16246 | eval_custom_logloss: 2.18379 |  0:44:45s
epoch 41 | loss: 2.19641 | eval_custom_logloss: 2.18214 |  0:45:49s
epoch 42 | loss: 2.19201 | eval_custom_logloss: 2.17183 |  0:46:53s
epoch 43 | loss: 2.16553 | eval_custom_logloss: 2.16503 |  0:48:02s
epoch 44 | loss: 2.16248 | eval_custom_logloss: 2.16506 |  0:49:11s
epoch 45 | loss: 2.16182 | eval_custom_logloss: 2.16421 |  0:50:19s
epoch 46 | loss: 2.15977 | eval_custom_logloss: 2.16342 |  0:51:23s
epoch 47 | loss: 2.15908 | eval_custom_logloss: 2.16487 |  0:52:38s
epoch 48 | loss: 2.15878 | eval_custom_logloss: 2.16521 |  0:53:45s
epoch 49 | loss: 2.15865 | eval_custom_logloss: 2.16455 |  0:54:50s
epoch 50 | loss: 2.15933 | eval_custom_logloss: 2.18058 |  0:55:55s
epoch 51 | loss: 2.15789 | eval_custom_logloss: 2.16772 |  0:57:06s
epoch 52 | loss: 2.15635 | eval_custom_logloss: 2.16898 |  0:58:12s
epoch 53 | loss: 2.159   | eval_custom_logloss: 2.17014 |  0:59:21s
epoch 54 | loss: 2.15579 | eval_custom_logloss: 2.16313 |  1:00:35s
epoch 55 | loss: 2.15808 | eval_custom_logloss: 2.16046 |  1:01:46s
epoch 56 | loss: 2.15551 | eval_custom_logloss: 2.16062 |  1:02:54s
epoch 57 | loss: 2.15448 | eval_custom_logloss: 2.18127 |  1:04:04s
epoch 58 | loss: 2.15914 | eval_custom_logloss: 2.16497 |  1:05:09s
epoch 59 | loss: 2.15698 | eval_custom_logloss: 2.16611 |  1:06:13s
epoch 60 | loss: 2.15402 | eval_custom_logloss: 2.18451 |  1:07:23s
epoch 61 | loss: 2.15619 | eval_custom_logloss: 2.18142 |  1:08:27s
epoch 62 | loss: 2.15621 | eval_custom_logloss: 2.19521 |  1:09:32s
epoch 63 | loss: 2.15366 | eval_custom_logloss: 2.16269 |  1:10:36s
epoch 64 | loss: 2.15354 | eval_custom_logloss: 2.17187 |  1:11:50s
epoch 65 | loss: 2.15425 | eval_custom_logloss: 2.16932 |  1:12:54s
epoch 66 | loss: 2.15275 | eval_custom_logloss: 2.19226 |  1:14:09s
epoch 67 | loss: 2.15425 | eval_custom_logloss: 2.16603 |  1:15:16s
epoch 68 | loss: 2.16078 | eval_custom_logloss: 2.1699  |  1:16:26s
epoch 69 | loss: 2.15191 | eval_custom_logloss: 2.17803 |  1:17:30s
epoch 70 | loss: 2.15085 | eval_custom_logloss: 2.16741 |  1:18:34s
epoch 71 | loss: 2.15561 | eval_custom_logloss: 2.1613  |  1:19:37s
epoch 72 | loss: 2.1557  | eval_custom_logloss: 2.16448 |  1:20:41s
epoch 73 | loss: 2.15284 | eval_custom_logloss: 2.1753  |  1:21:45s
epoch 74 | loss: 2.15434 | eval_custom_logloss: 2.16274 |  1:22:49s
epoch 75 | loss: 2.16136 | eval_custom_logloss: 2.1592  |  1:23:53s
epoch 76 | loss: 2.15231 | eval_custom_logloss: 2.168   |  1:24:57s
epoch 77 | loss: 2.15818 | eval_custom_logloss: 2.16624 |  1:26:01s
epoch 78 | loss: 2.15034 | eval_custom_logloss: 2.16462 |  1:27:06s
epoch 79 | loss: 2.15261 | eval_custom_logloss: 2.17132 |  1:28:13s
epoch 80 | loss: 2.1498  | eval_custom_logloss: 2.16252 |  1:29:19s
epoch 81 | loss: 2.14854 | eval_custom_logloss: 2.22995 |  1:30:24s
epoch 82 | loss: 2.15105 | eval_custom_logloss: 2.17136 |  1:31:30s
epoch 83 | loss: 2.15307 | eval_custom_logloss: 2.16294 |  1:32:38s
epoch 84 | loss: 2.15354 | eval_custom_logloss: 2.16306 |  1:33:43s
epoch 85 | loss: 2.15679 | eval_custom_logloss: 2.16021 |  1:34:49s
epoch 86 | loss: 2.15994 | eval_custom_logloss: 2.18727 |  1:35:54s
epoch 87 | loss: 2.15027 | eval_custom_logloss: 2.16681 |  1:36:59s
epoch 88 | loss: 2.15066 | eval_custom_logloss: 2.1707  |  1:38:05s
epoch 89 | loss: 2.14905 | eval_custom_logloss: 2.16185 |  1:39:11s
epoch 90 | loss: 2.149   | eval_custom_logloss: 2.18649 |  1:40:16s
epoch 91 | loss: 2.15856 | eval_custom_logloss: 2.16678 |  1:41:22s
epoch 92 | loss: 2.14855 | eval_custom_logloss: 2.16219 |  1:42:28s
epoch 93 | loss: 2.148   | eval_custom_logloss: 2.17196 |  1:43:34s
epoch 94 | loss: 2.14721 | eval_custom_logloss: 2.16083 |  1:44:38s
epoch 95 | loss: 2.15168 | eval_custom_logloss: 2.18496 |  1:45:42s

Early stopping occurred at epoch 95 with best_epoch = 75 and best_eval_custom_logloss = 2.1592
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33365,)
Probabilities shape : (33365, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1595, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax', 'n_a': 21, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.32393 | eval_custom_logloss: 2.20317 |  0:01:13s
epoch 1  | loss: 2.20815 | eval_custom_logloss: 2.20813 |  0:02:18s
epoch 2  | loss: 2.20271 | eval_custom_logloss: 2.20027 |  0:03:28s
epoch 3  | loss: 2.19911 | eval_custom_logloss: 2.1865  |  0:04:42s
epoch 4  | loss: 2.18642 | eval_custom_logloss: 2.17862 |  0:05:47s
epoch 5  | loss: 2.1798  | eval_custom_logloss: 2.1797  |  0:06:52s
epoch 6  | loss: 2.17959 | eval_custom_logloss: 2.18351 |  0:07:57s
epoch 7  | loss: 2.17712 | eval_custom_logloss: 2.1732  |  0:09:01s
epoch 8  | loss: 2.17614 | eval_custom_logloss: 2.17776 |  0:10:09s
epoch 9  | loss: 2.17448 | eval_custom_logloss: 2.17532 |  0:11:25s
epoch 10 | loss: 2.17607 | eval_custom_logloss: 2.17209 |  0:12:35s
epoch 11 | loss: 2.17385 | eval_custom_logloss: 2.18056 |  0:13:41s
epoch 12 | loss: 2.17562 | eval_custom_logloss: 2.17523 |  0:14:55s
epoch 13 | loss: 2.17295 | eval_custom_logloss: 2.17484 |  0:16:10s
epoch 14 | loss: 2.17246 | eval_custom_logloss: 2.18169 |  0:17:21s
epoch 15 | loss: 2.17237 | eval_custom_logloss: 2.17481 |  0:18:32s
epoch 16 | loss: 2.17082 | eval_custom_logloss: 2.1693  |  0:19:40s
epoch 17 | loss: 2.17257 | eval_custom_logloss: 2.19133 |  0:20:56s
epoch 18 | loss: 2.17237 | eval_custom_logloss: 2.18527 |  0:22:11s
epoch 19 | loss: 2.16961 | eval_custom_logloss: 2.1704  |  0:23:24s
epoch 20 | loss: 2.16803 | eval_custom_logloss: 2.17178 |  0:24:37s
epoch 21 | loss: 2.16805 | eval_custom_logloss: 2.1716  |  0:25:48s
epoch 22 | loss: 2.16955 | eval_custom_logloss: 2.20207 |  0:26:56s
epoch 23 | loss: 2.16676 | eval_custom_logloss: 2.16761 |  0:28:11s
epoch 24 | loss: 2.16957 | eval_custom_logloss: 2.1789  |  0:29:25s
epoch 25 | loss: 2.17073 | eval_custom_logloss: 2.16734 |  0:30:29s
epoch 26 | loss: 2.16687 | eval_custom_logloss: 2.17492 |  0:31:33s
epoch 27 | loss: 2.16517 | eval_custom_logloss: 2.17432 |  0:32:37s
epoch 28 | loss: 2.16548 | eval_custom_logloss: 2.17007 |  0:33:41s
epoch 29 | loss: 2.16581 | eval_custom_logloss: 2.16821 |  0:34:44s
epoch 30 | loss: 2.16455 | eval_custom_logloss: 2.16763 |  0:35:48s
epoch 31 | loss: 2.16419 | eval_custom_logloss: 2.16764 |  0:36:52s
epoch 32 | loss: 2.16424 | eval_custom_logloss: 2.17156 |  0:37:56s
epoch 33 | loss: 2.16479 | eval_custom_logloss: 2.16465 |  0:39:00s
epoch 34 | loss: 2.16233 | eval_custom_logloss: 2.17073 |  0:40:04s
epoch 35 | loss: 2.16236 | eval_custom_logloss: 2.16757 |  0:41:08s
epoch 36 | loss: 2.16208 | eval_custom_logloss: 2.16627 |  0:42:12s
epoch 37 | loss: 2.16847 | eval_custom_logloss: 2.17389 |  0:43:16s
epoch 38 | loss: 2.16328 | eval_custom_logloss: 2.16408 |  0:44:23s
epoch 39 | loss: 2.16671 | eval_custom_logloss: 2.16741 |  0:45:30s
epoch 40 | loss: 2.1678  | eval_custom_logloss: 2.17319 |  0:46:37s
epoch 41 | loss: 2.16182 | eval_custom_logloss: 2.16514 |  0:47:43s
epoch 42 | loss: 2.16189 | eval_custom_logloss: 2.16966 |  0:48:48s
epoch 43 | loss: 2.16005 | eval_custom_logloss: 2.1942  |  0:49:51s
epoch 44 | loss: 2.15926 | eval_custom_logloss: 2.16778 |  0:50:55s
epoch 45 | loss: 2.15938 | eval_custom_logloss: 2.17184 |  0:52:03s
epoch 46 | loss: 2.16893 | eval_custom_logloss: 2.17146 |  0:53:10s
epoch 47 | loss: 2.16061 | eval_custom_logloss: 2.166   |  0:54:16s
epoch 48 | loss: 2.15934 | eval_custom_logloss: 2.18573 |  0:55:24s
epoch 49 | loss: 2.15923 | eval_custom_logloss: 2.17268 |  0:56:31s
epoch 50 | loss: 2.16224 | eval_custom_logloss: 2.16618 |  0:57:39s
epoch 51 | loss: 2.15937 | eval_custom_logloss: 2.17846 |  0:58:46s
epoch 52 | loss: 2.15761 | eval_custom_logloss: 2.1695  |  0:59:54s
epoch 53 | loss: 2.1632  | eval_custom_logloss: 2.16815 |  1:01:01s
epoch 54 | loss: 2.15929 | eval_custom_logloss: 2.17154 |  1:02:08s
epoch 55 | loss: 2.158   | eval_custom_logloss: 2.16909 |  1:03:16s
epoch 56 | loss: 2.1578  | eval_custom_logloss: 2.16853 |  1:04:24s
epoch 57 | loss: 2.16385 | eval_custom_logloss: 2.18021 |  1:05:32s
epoch 58 | loss: 2.15848 | eval_custom_logloss: 2.22032 |  1:06:41s

Early stopping occurred at epoch 58 with best_epoch = 38 and best_eval_custom_logloss = 2.16408
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.16195, 'Log Loss - std': 0.0024500000000000632} 
 

Fold 3
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax', 'n_a': 21, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.342   | eval_custom_logloss: 2.21382 |  0:01:09s
epoch 1  | loss: 2.21914 | eval_custom_logloss: 2.23207 |  0:02:17s
epoch 2  | loss: 2.21538 | eval_custom_logloss: 2.19897 |  0:03:29s
epoch 3  | loss: 2.203   | eval_custom_logloss: 2.19724 |  0:04:39s
epoch 4  | loss: 2.19552 | eval_custom_logloss: 2.18432 |  0:05:51s
epoch 5  | loss: 2.1925  | eval_custom_logloss: 2.18439 |  0:07:02s
epoch 6  | loss: 2.18845 | eval_custom_logloss: 2.18563 |  0:08:17s
epoch 7  | loss: 2.18833 | eval_custom_logloss: 2.18088 |  0:09:27s
epoch 8  | loss: 2.18681 | eval_custom_logloss: 2.18484 |  0:10:36s
epoch 9  | loss: 2.18568 | eval_custom_logloss: 2.17658 |  0:11:47s
epoch 10 | loss: 2.20263 | eval_custom_logloss: 2.1977  |  0:13:02s
epoch 11 | loss: 2.19984 | eval_custom_logloss: 2.20943 |  0:14:12s
epoch 12 | loss: 2.23659 | eval_custom_logloss: 2.20287 |  0:15:25s
epoch 13 | loss: 2.20448 | eval_custom_logloss: 2.20479 |  0:16:33s
epoch 14 | loss: 2.20406 | eval_custom_logloss: 2.20073 |  0:17:42s
epoch 15 | loss: 2.19299 | eval_custom_logloss: 2.17567 |  0:18:56s
epoch 16 | loss: 2.17737 | eval_custom_logloss: 2.17309 |  0:20:09s
epoch 17 | loss: 2.17809 | eval_custom_logloss: 2.17682 |  0:21:13s
epoch 18 | loss: 2.17557 | eval_custom_logloss: 2.17447 |  0:22:17s
epoch 19 | loss: 2.1784  | eval_custom_logloss: 2.17803 |  0:23:21s
epoch 20 | loss: 2.17675 | eval_custom_logloss: 2.17652 |  0:24:25s
epoch 21 | loss: 2.17406 | eval_custom_logloss: 2.17228 |  0:25:32s
epoch 22 | loss: 2.17298 | eval_custom_logloss: 2.1728  |  0:26:36s
epoch 23 | loss: 2.17241 | eval_custom_logloss: 2.17419 |  0:27:46s
epoch 24 | loss: 2.17205 | eval_custom_logloss: 2.17039 |  0:28:52s
epoch 25 | loss: 2.16818 | eval_custom_logloss: 2.17078 |  0:29:56s
epoch 26 | loss: 2.20454 | eval_custom_logloss: 2.19381 |  0:31:00s
epoch 27 | loss: 2.19512 | eval_custom_logloss: 2.1868  |  0:32:05s
epoch 28 | loss: 2.18396 | eval_custom_logloss: 2.18193 |  0:33:09s
epoch 29 | loss: 2.17333 | eval_custom_logloss: 2.17326 |  0:34:14s
epoch 30 | loss: 2.17168 | eval_custom_logloss: 2.16924 |  0:35:19s
epoch 31 | loss: 2.1688  | eval_custom_logloss: 2.16563 |  0:36:24s
epoch 32 | loss: 2.16755 | eval_custom_logloss: 2.18678 |  0:37:29s
epoch 33 | loss: 2.16868 | eval_custom_logloss: 2.17277 |  0:38:35s
epoch 34 | loss: 2.1663  | eval_custom_logloss: 2.17001 |  0:39:40s
epoch 35 | loss: 2.16787 | eval_custom_logloss: 2.16808 |  0:40:45s
epoch 36 | loss: 2.1609  | eval_custom_logloss: 2.164   |  0:41:50s
epoch 37 | loss: 2.16526 | eval_custom_logloss: 2.16466 |  0:42:55s
epoch 38 | loss: 2.16415 | eval_custom_logloss: 2.16915 |  0:43:59s
epoch 39 | loss: 2.16062 | eval_custom_logloss: 2.16939 |  0:45:05s
epoch 40 | loss: 2.16262 | eval_custom_logloss: 2.16266 |  0:46:11s
epoch 41 | loss: 2.15849 | eval_custom_logloss: 2.16485 |  0:47:18s
epoch 42 | loss: 2.1582  | eval_custom_logloss: 2.16714 |  0:48:25s
epoch 43 | loss: 2.15759 | eval_custom_logloss: 2.17494 |  0:49:32s
epoch 44 | loss: 2.15961 | eval_custom_logloss: 2.17477 |  0:50:39s
epoch 45 | loss: 2.15813 | eval_custom_logloss: 2.17598 |  0:51:46s
epoch 46 | loss: 2.15916 | eval_custom_logloss: 2.16152 |  0:52:52s
epoch 47 | loss: 2.15631 | eval_custom_logloss: 2.16451 |  0:53:57s
epoch 48 | loss: 2.1557  | eval_custom_logloss: 2.16087 |  0:55:02s
epoch 49 | loss: 2.15484 | eval_custom_logloss: 2.16394 |  0:56:09s
epoch 50 | loss: 2.15497 | eval_custom_logloss: 2.16118 |  0:57:16s
epoch 51 | loss: 2.15429 | eval_custom_logloss: 2.15753 |  0:58:22s
epoch 52 | loss: 2.15659 | eval_custom_logloss: 2.16246 |  0:59:29s
epoch 53 | loss: 2.1586  | eval_custom_logloss: 2.17619 |  1:00:36s
epoch 54 | loss: 2.15478 | eval_custom_logloss: 2.17353 |  1:01:43s
epoch 55 | loss: 2.1555  | eval_custom_logloss: 2.16221 |  1:02:49s
epoch 56 | loss: 2.15799 | eval_custom_logloss: 2.15873 |  1:03:56s
epoch 57 | loss: 2.18839 | eval_custom_logloss: 2.16627 |  1:05:03s
epoch 58 | loss: 2.15291 | eval_custom_logloss: 2.16376 |  1:06:10s
epoch 59 | loss: 2.16791 | eval_custom_logloss: 2.16948 |  1:07:17s
epoch 60 | loss: 2.15558 | eval_custom_logloss: 2.15948 |  1:08:24s
epoch 61 | loss: 2.15126 | eval_custom_logloss: 2.161   |  1:09:31s
epoch 62 | loss: 2.17404 | eval_custom_logloss: 2.16216 |  1:10:38s
epoch 63 | loss: 2.1509  | eval_custom_logloss: 2.15769 |  1:11:44s
epoch 64 | loss: 2.15261 | eval_custom_logloss: 2.15954 |  1:12:51s
epoch 65 | loss: 2.154   | eval_custom_logloss: 2.17227 |  1:13:55s
epoch 66 | loss: 2.18786 | eval_custom_logloss: 2.16221 |  1:15:01s
epoch 67 | loss: 2.15677 | eval_custom_logloss: 2.17183 |  1:16:06s
epoch 68 | loss: 2.15714 | eval_custom_logloss: 2.162   |  1:17:11s
epoch 69 | loss: 2.15137 | eval_custom_logloss: 2.16369 |  1:18:16s
epoch 70 | loss: 2.16896 | eval_custom_logloss: 2.23763 |  1:19:22s
epoch 71 | loss: 2.1542  | eval_custom_logloss: 2.25806 |  1:20:28s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 2.15753
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1605666666666665, 'Log Loss - std': 0.002798015169524511} 
 

Fold 4
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax', 'n_a': 21, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.36021 | eval_custom_logloss: 2.22985 |  0:01:05s
epoch 1  | loss: 2.21545 | eval_custom_logloss: 2.20669 |  0:02:10s
epoch 2  | loss: 2.203   | eval_custom_logloss: 2.20652 |  0:03:14s
epoch 3  | loss: 2.20081 | eval_custom_logloss: 2.19944 |  0:04:20s
epoch 4  | loss: 2.19574 | eval_custom_logloss: 2.19796 |  0:05:24s
epoch 5  | loss: 2.19632 | eval_custom_logloss: 2.199   |  0:06:28s
epoch 6  | loss: 2.19481 | eval_custom_logloss: 2.20061 |  0:07:33s
epoch 7  | loss: 2.1934  | eval_custom_logloss: 2.19306 |  0:08:38s
epoch 8  | loss: 2.19024 | eval_custom_logloss: 2.19158 |  0:09:44s
epoch 9  | loss: 2.18963 | eval_custom_logloss: 2.1929  |  0:10:49s
epoch 10 | loss: 2.18566 | eval_custom_logloss: 2.18593 |  0:11:54s
epoch 11 | loss: 2.18467 | eval_custom_logloss: 2.18609 |  0:13:00s
epoch 12 | loss: 2.18176 | eval_custom_logloss: 2.20053 |  0:14:06s
epoch 13 | loss: 2.18548 | eval_custom_logloss: 2.18627 |  0:15:11s
epoch 14 | loss: 2.18454 | eval_custom_logloss: 2.1985  |  0:16:15s
epoch 15 | loss: 2.18125 | eval_custom_logloss: 2.19276 |  0:17:21s
epoch 16 | loss: 2.18156 | eval_custom_logloss: 2.18552 |  0:18:27s
epoch 17 | loss: 2.18073 | eval_custom_logloss: 2.18277 |  0:19:37s
epoch 18 | loss: 2.17713 | eval_custom_logloss: 2.18273 |  0:20:52s
epoch 19 | loss: 2.17641 | eval_custom_logloss: 2.18544 |  0:21:57s
epoch 20 | loss: 2.17414 | eval_custom_logloss: 2.19838 |  0:23:02s
epoch 21 | loss: 2.1756  | eval_custom_logloss: 2.18174 |  0:24:09s
epoch 22 | loss: 2.17748 | eval_custom_logloss: 2.19519 |  0:25:17s
epoch 23 | loss: 2.17292 | eval_custom_logloss: 2.1862  |  0:26:24s
epoch 24 | loss: 2.17107 | eval_custom_logloss: 2.17915 |  0:27:33s
epoch 25 | loss: 2.18488 | eval_custom_logloss: 2.18158 |  0:28:40s
epoch 26 | loss: 2.17044 | eval_custom_logloss: 2.18221 |  0:29:47s
epoch 27 | loss: 2.16895 | eval_custom_logloss: 2.18115 |  0:31:00s
epoch 28 | loss: 2.1692  | eval_custom_logloss: 2.18138 |  0:32:13s
epoch 29 | loss: 2.16885 | eval_custom_logloss: 2.18093 |  0:33:27s
epoch 30 | loss: 2.17077 | eval_custom_logloss: 2.1797  |  0:34:43s
epoch 31 | loss: 2.1695  | eval_custom_logloss: 2.19219 |  0:35:59s
epoch 32 | loss: 2.1679  | eval_custom_logloss: 2.20291 |  0:37:12s
epoch 33 | loss: 2.167   | eval_custom_logloss: 2.18883 |  0:38:20s
epoch 34 | loss: 2.16585 | eval_custom_logloss: 2.1944  |  0:39:26s
epoch 35 | loss: 2.16591 | eval_custom_logloss: 2.18377 |  0:40:32s
epoch 36 | loss: 2.16667 | eval_custom_logloss: 2.18032 |  0:41:39s
epoch 37 | loss: 2.1646  | eval_custom_logloss: 2.18772 |  0:42:46s
epoch 38 | loss: 2.16689 | eval_custom_logloss: 2.18837 |  0:43:53s
epoch 39 | loss: 2.16387 | eval_custom_logloss: 2.17856 |  0:45:00s
epoch 40 | loss: 2.16661 | eval_custom_logloss: 2.18114 |  0:46:14s
epoch 41 | loss: 2.16328 | eval_custom_logloss: 2.18455 |  0:47:27s
epoch 42 | loss: 2.15783 | eval_custom_logloss: 2.17046 |  0:48:40s
epoch 43 | loss: 2.15653 | eval_custom_logloss: 2.16504 |  0:49:45s
epoch 44 | loss: 2.15241 | eval_custom_logloss: 2.1663  |  0:50:52s
epoch 45 | loss: 2.15102 | eval_custom_logloss: 2.18045 |  0:51:58s
epoch 46 | loss: 2.15696 | eval_custom_logloss: 2.16981 |  0:53:05s
epoch 47 | loss: 2.15046 | eval_custom_logloss: 2.16834 |  0:54:18s
epoch 48 | loss: 2.14978 | eval_custom_logloss: 2.18243 |  0:55:24s
epoch 49 | loss: 2.15005 | eval_custom_logloss: 2.18369 |  0:56:32s
epoch 50 | loss: 2.15456 | eval_custom_logloss: 2.1684  |  0:57:38s
epoch 51 | loss: 2.15208 | eval_custom_logloss: 2.17542 |  0:58:44s
epoch 52 | loss: 2.14889 | eval_custom_logloss: 2.17418 |  0:59:51s
epoch 53 | loss: 2.1528  | eval_custom_logloss: 2.17415 |  1:00:56s
epoch 54 | loss: 2.15072 | eval_custom_logloss: 2.17589 |  1:02:01s
epoch 55 | loss: 2.15032 | eval_custom_logloss: 2.18733 |  1:03:09s
epoch 56 | loss: 2.15843 | eval_custom_logloss: 2.17797 |  1:04:16s
epoch 57 | loss: 2.15674 | eval_custom_logloss: 2.17542 |  1:05:22s
epoch 58 | loss: 2.14847 | eval_custom_logloss: 2.17574 |  1:06:27s
epoch 59 | loss: 2.14791 | eval_custom_logloss: 2.16389 |  1:07:32s
epoch 60 | loss: 2.14762 | eval_custom_logloss: 2.18481 |  1:08:37s
epoch 61 | loss: 2.14823 | eval_custom_logloss: 2.16964 |  1:09:43s
epoch 62 | loss: 2.15014 | eval_custom_logloss: 2.16827 |  1:10:48s
epoch 63 | loss: 2.15018 | eval_custom_logloss: 2.16927 |  1:11:55s
epoch 64 | loss: 2.14771 | eval_custom_logloss: 2.16854 |  1:13:04s
epoch 65 | loss: 2.23804 | eval_custom_logloss: 2.17602 |  1:14:18s
epoch 66 | loss: 2.14856 | eval_custom_logloss: 2.16443 |  1:15:33s
epoch 67 | loss: 2.15178 | eval_custom_logloss: 2.16622 |  1:16:44s
epoch 68 | loss: 2.16472 | eval_custom_logloss: 2.16552 |  1:17:55s
epoch 69 | loss: 2.14716 | eval_custom_logloss: 2.16689 |  1:19:00s
epoch 70 | loss: 2.14941 | eval_custom_logloss: 2.17015 |  1:20:05s
epoch 71 | loss: 2.14713 | eval_custom_logloss: 2.16397 |  1:21:13s
epoch 72 | loss: 2.1469  | eval_custom_logloss: 2.1803  |  1:22:28s
epoch 73 | loss: 2.14533 | eval_custom_logloss: 2.16614 |  1:23:43s
epoch 74 | loss: 2.14555 | eval_custom_logloss: 2.16412 |  1:24:55s
epoch 75 | loss: 2.14776 | eval_custom_logloss: 2.17685 |  1:26:11s
epoch 76 | loss: 2.14576 | eval_custom_logloss: 2.16328 |  1:27:20s
epoch 77 | loss: 2.14668 | eval_custom_logloss: 2.1809  |  1:28:26s
epoch 78 | loss: 2.14796 | eval_custom_logloss: 2.1651  |  1:29:33s
epoch 79 | loss: 2.14415 | eval_custom_logloss: 2.17153 |  1:30:38s
epoch 80 | loss: 2.16754 | eval_custom_logloss: 2.19433 |  1:31:45s
epoch 81 | loss: 2.15945 | eval_custom_logloss: 2.18956 |  1:32:52s
epoch 82 | loss: 2.15252 | eval_custom_logloss: 2.2184  |  1:33:57s
epoch 83 | loss: 2.15413 | eval_custom_logloss: 2.17857 |  1:35:02s
epoch 84 | loss: 2.15956 | eval_custom_logloss: 2.17219 |  1:36:07s
epoch 85 | loss: 2.14459 | eval_custom_logloss: 2.16867 |  1:37:14s
epoch 86 | loss: 2.14419 | eval_custom_logloss: 2.16564 |  1:38:28s
epoch 87 | loss: 2.14682 | eval_custom_logloss: 2.16596 |  1:39:34s
epoch 88 | loss: 2.14526 | eval_custom_logloss: 2.17235 |  1:40:41s
epoch 89 | loss: 2.14703 | eval_custom_logloss: 2.18892 |  1:41:47s
epoch 90 | loss: 2.14331 | eval_custom_logloss: 2.17188 |  1:42:54s
epoch 91 | loss: 2.14425 | eval_custom_logloss: 2.17062 |  1:44:00s
epoch 92 | loss: 2.14612 | eval_custom_logloss: 2.17251 |  1:45:06s
epoch 93 | loss: 2.16997 | eval_custom_logloss: 2.1983  |  1:46:12s
epoch 94 | loss: 2.22958 | eval_custom_logloss: 2.18799 |  1:47:24s
epoch 95 | loss: 2.1586  | eval_custom_logloss: 2.18504 |  1:48:31s
epoch 96 | loss: 2.15247 | eval_custom_logloss: 2.16987 |  1:49:39s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 2.16328
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.161225, 'Log Loss - std': 0.002678035660703583} 
 

Fold 5
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax', 'n_a': 21, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.40322 | eval_custom_logloss: 2.21159 |  0:01:14s
epoch 1  | loss: 2.21039 | eval_custom_logloss: 2.19704 |  0:02:30s
epoch 2  | loss: 2.22662 | eval_custom_logloss: 2.20797 |  0:03:39s
epoch 3  | loss: 2.20851 | eval_custom_logloss: 2.19804 |  0:04:46s
epoch 4  | loss: 2.20315 | eval_custom_logloss: 2.20824 |  0:05:58s
epoch 5  | loss: 2.22232 | eval_custom_logloss: 2.21253 |  0:07:10s
epoch 6  | loss: 2.21243 | eval_custom_logloss: 2.19626 |  0:08:19s
epoch 7  | loss: 2.19868 | eval_custom_logloss: 2.19589 |  0:09:27s
epoch 8  | loss: 2.19803 | eval_custom_logloss: 2.19075 |  0:10:34s
epoch 9  | loss: 2.20398 | eval_custom_logloss: 2.20655 |  0:11:41s
epoch 10 | loss: 2.22666 | eval_custom_logloss: 2.26639 |  0:12:48s
epoch 11 | loss: 2.21446 | eval_custom_logloss: 2.2157  |  0:13:54s
epoch 12 | loss: 2.21465 | eval_custom_logloss: 2.20907 |  0:15:01s
epoch 13 | loss: 2.19805 | eval_custom_logloss: 2.18826 |  0:16:07s
epoch 14 | loss: 2.1935  | eval_custom_logloss: 2.19449 |  0:17:12s
epoch 15 | loss: 2.19243 | eval_custom_logloss: 2.19127 |  0:18:25s
epoch 16 | loss: 2.19145 | eval_custom_logloss: 2.18922 |  0:19:39s
epoch 17 | loss: 2.19079 | eval_custom_logloss: 2.19007 |  0:20:45s
epoch 18 | loss: 2.19219 | eval_custom_logloss: 2.20515 |  0:22:01s
epoch 19 | loss: 2.18905 | eval_custom_logloss: 2.18698 |  0:23:08s
epoch 20 | loss: 2.1946  | eval_custom_logloss: 2.2019  |  0:24:19s
epoch 21 | loss: 2.19255 | eval_custom_logloss: 2.22369 |  0:25:32s
epoch 22 | loss: 2.19033 | eval_custom_logloss: 2.18651 |  0:26:42s
epoch 23 | loss: 2.18919 | eval_custom_logloss: 2.18708 |  0:27:54s
epoch 24 | loss: 2.18777 | eval_custom_logloss: 2.18776 |  0:29:04s
epoch 25 | loss: 2.2602  | eval_custom_logloss: 2.31158 |  0:30:18s
epoch 26 | loss: 2.27166 | eval_custom_logloss: 2.25411 |  0:31:28s
epoch 27 | loss: 2.25928 | eval_custom_logloss: 2.25583 |  0:32:40s
epoch 28 | loss: 2.25522 | eval_custom_logloss: 2.25305 |  0:33:52s
epoch 29 | loss: 2.31881 | eval_custom_logloss: 2.33732 |  0:35:08s
epoch 30 | loss: 2.33772 | eval_custom_logloss: 2.33463 |  0:36:22s
epoch 31 | loss: 2.33768 | eval_custom_logloss: 2.33369 |  0:37:27s
epoch 32 | loss: 2.35238 | eval_custom_logloss: 2.34807 |  0:38:33s
epoch 33 | loss: 2.34984 | eval_custom_logloss: 2.341   |  0:39:40s
epoch 34 | loss: 2.3434  | eval_custom_logloss: 2.34064 |  0:40:47s
epoch 35 | loss: 2.34138 | eval_custom_logloss: 2.33265 |  0:41:53s
epoch 36 | loss: 2.33405 | eval_custom_logloss: 2.33214 |  0:43:00s
epoch 37 | loss: 2.33383 | eval_custom_logloss: 2.32741 |  0:44:06s
epoch 38 | loss: 2.32913 | eval_custom_logloss: 2.33075 |  0:45:13s
epoch 39 | loss: 2.32516 | eval_custom_logloss: 2.31994 |  0:46:20s
epoch 40 | loss: 2.32136 | eval_custom_logloss: 2.44407 |  0:47:27s
epoch 41 | loss: 2.31911 | eval_custom_logloss: 2.31751 |  0:48:34s
epoch 42 | loss: 2.31792 | eval_custom_logloss: 2.33179 |  0:49:41s

Early stopping occurred at epoch 42 with best_epoch = 22 and best_eval_custom_logloss = 2.18651
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33364,)
Probabilities shape : (33364, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.16634, 'Log Loss - std': 0.01050668358712678} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 2.16634 and parameters: {'n_d': 21, 'n_steps': 4, 'gamma': 1.119585645694996, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 1, 'momentum': 0.03867972592887196, 'mask_type': 'sparsemax'}. Best is trial 3 with value: 2.17544.
In get_device
Fold 1
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133456, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133456, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]] 
 
 
Val : (133456, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.8111066002994543, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.016854266170877395, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.68675 | eval_custom_logloss: 2.33388 |  0:04:01s
epoch 1  | loss: 2.2753  | eval_custom_logloss: 2.21601 |  0:07:34s
epoch 2  | loss: 2.22165 | eval_custom_logloss: 2.31867 |  0:11:36s
epoch 3  | loss: 2.2429  | eval_custom_logloss: 4.53702 |  0:15:36s
epoch 4  | loss: 2.21715 | eval_custom_logloss: 2.29443 |  0:19:09s
epoch 5  | loss: 2.24349 | eval_custom_logloss: 2.24668 |  0:22:43s
epoch 6  | loss: 2.22505 | eval_custom_logloss: 2.94152 |  0:26:20s
epoch 7  | loss: 2.23997 | eval_custom_logloss: 2.48613 |  0:29:50s
epoch 8  | loss: 2.21269 | eval_custom_logloss: 7.25775 |  0:33:39s
epoch 9  | loss: 2.20695 | eval_custom_logloss: 7.7445  |  0:37:39s
epoch 10 | loss: 2.2048  | eval_custom_logloss: 2.99873 |  0:41:08s
epoch 11 | loss: 2.25656 | eval_custom_logloss: 2.49442 |  0:44:37s
epoch 12 | loss: 2.20617 | eval_custom_logloss: 2.21164 |  0:48:07s
epoch 13 | loss: 2.19166 | eval_custom_logloss: 2.43231 |  0:51:36s
epoch 14 | loss: 2.1837  | eval_custom_logloss: 2.39628 |  0:55:05s
epoch 15 | loss: 2.20379 | eval_custom_logloss: 2.44312 |  0:58:52s
epoch 16 | loss: 2.19956 | eval_custom_logloss: 2.38444 |  1:02:52s
epoch 17 | loss: 2.20838 | eval_custom_logloss: 2.63434 |  1:06:53s
epoch 18 | loss: 2.2218  | eval_custom_logloss: 2.78687 |  1:10:55s
epoch 19 | loss: 2.19467 | eval_custom_logloss: 3.82884 |  1:14:57s
epoch 20 | loss: 2.18559 | eval_custom_logloss: 7.65271 |  1:18:57s
epoch 21 | loss: 2.1796  | eval_custom_logloss: 3.08789 |  1:22:57s
epoch 22 | loss: 2.1976  | eval_custom_logloss: 2.32194 |  1:26:48s
epoch 23 | loss: 2.20302 | eval_custom_logloss: 2.45755 |  1:30:50s
epoch 24 | loss: 2.20097 | eval_custom_logloss: 5.51921 |  1:34:53s
epoch 25 | loss: 2.1913  | eval_custom_logloss: 2.51083 |  1:38:56s
epoch 26 | loss: 2.18326 | eval_custom_logloss: 2.51897 |  1:42:59s
epoch 27 | loss: 2.18753 | eval_custom_logloss: 4.3686  |  1:47:02s
epoch 28 | loss: 2.19415 | eval_custom_logloss: 12.66584|  1:51:05s
epoch 29 | loss: 2.19253 | eval_custom_logloss: 3.5489  |  1:55:07s
epoch 30 | loss: 2.18139 | eval_custom_logloss: 12.27589|  1:59:10s
epoch 31 | loss: 2.17038 | eval_custom_logloss: 2.95503 |  2:03:12s
epoch 32 | loss: 2.1731  | eval_custom_logloss: 3.93208 |  2:07:13s

Early stopping occurred at epoch 32 with best_epoch = 12 and best_eval_custom_logloss = 2.21164
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 46
Class label len :46
Class labels : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]
Unique y_true : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] 

Prediction shape : (33365,)
Probabilities shape : (33365, 46) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2119, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :9
num_classes : 1
cat_idx : [0]
nominal_idx : [0, 2, 3, 4, 5, 6, 7, 8]
ordinal_idx : [1]
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (133457, 9)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : []
Cat Dims V1 : [8]
Cat Idx V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [['0-17' 1.0 0.0 0.0 0.0]
 ['46-50' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['51-55' 1.0 0.0 0.0 0.0]
 ['36-45' 0.0 1.0 0.0 1.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['26-35' 0.0 1.0 0.0 0.0]
 ['36-45' 1.0 0.0 0.0 1.0]
 ['36-45' 1.0 0.0 0.0 1.0]] 
 
 
Val : (133457, 75) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [] 


OHE Idx : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]


Ordinal Idx V2: [0]


Cat Dims V2 : [8]
Cat Idx V2 : [0] 
 

Train: [[1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [5.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 1.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [6.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 0.0 0.0]
 [3.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]
 [4.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 0.0 0.0]] 
 
 
Val : (133457, 75) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 46
Unique values in y_train: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
Unique values in y_test: (array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,
       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,
       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,
       39., 40., 41., 42., 43., 44., 45.]), 46)
No need to shift labels.
VERIFY SHIFT
Train after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Test after shift : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45], Length : 46
Number of Classes After Bin Verifier: 46
In get_device
{'n_d': 38, 'n_steps': 10, 'gamma': 1.8111066002994543, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.016854266170877395, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [0], 'cat_dims': [8], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.69643 | eval_custom_logloss: 2.2846  |  0:03:29s
epoch 1  | loss: 2.25687 | eval_custom_logloss: 2.25618 |  0:06:58s
epoch 2  | loss: 2.23632 | eval_custom_logloss: 2.24048 |  0:10:27s
epoch 3  | loss: 2.2288  | eval_custom_logloss: 2.2647  |  0:14:27s
epoch 4  | loss: 2.22974 | eval_custom_logloss: 3.0909  |  0:18:26s
epoch 5  | loss: 2.21013 | eval_custom_logloss: 2.2857  |  0:22:16s
epoch 6  | loss: 2.20738 | eval_custom_logloss: 2.22228 |  0:26:09s
epoch 7  | loss: 2.20206 | eval_custom_logloss: 2.19441 |  0:29:46s
epoch 8  | loss: 2.20276 | eval_custom_logloss: 14.22625|  0:33:21s
epoch 9  | loss: 2.20865 | eval_custom_logloss: 2.6742  |  0:37:22s
epoch 10 | loss: 2.21119 | eval_custom_logloss: 2.50483 |  0:41:24s
Trial 7 failed with parameters: {'n_d': 38, 'n_steps': 10, 'gamma': 1.8111066002994543, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.016854266170877395, 'mask_type': 'entmax'} because of the following error: RuntimeError('CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n').
Traceback (most recent call last):
  File "/home/mburu/.local/lib/python3.8/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 416, in __call__
    sc, time = cross_validation(model, self.X, self.y, args_cp, visual=False, save_model=False)#Dont save model during HPT
  File "train.py", line 307, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
  File "/home/mburu/Master_Thesis/master-thesis-da/DNN_Trial/models/tabnet.py", line 79, in fit
    self.model.fit(X, y, eval_set=[(X_val, y_val)], eval_name=["eval"], eval_metric=self.metric,
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py", line 258, in fit
    self._train_epoch(train_dataloader)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py", line 489, in _train_epoch
    batch_logs = self._train_batch(X, y)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py", line 527, in _train_batch
    output, M_loss = self.network(X)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py", line 616, in forward
    return self.tabnet(x)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py", line 492, in forward
    steps_output, M_loss = self.encoder(x)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py", line 181, in forward
    out = self.feat_transformers[step](masked_x)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py", line 737, in forward
    x = self.shared(x)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mburu/anaconda3/envs/TabSurvey/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mburu/.local/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py", line 772, in forward
    scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Trial 7 failed with value None.
