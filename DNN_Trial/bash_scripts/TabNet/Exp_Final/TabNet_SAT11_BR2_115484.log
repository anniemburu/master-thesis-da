

----------------------------------------------------------------------------
Training TabNet Vesion 1 with Dataset: config/sat11.yml 



----------------------------------------------------------------------------
Namespace(batch_size=128, bin_alt=None, cat_dims=[2], cat_idx=[0], config='config/sat11.yml', data_parallel=False, dataset='SAT11', direction='maximize', dropna_idx=[116], early_stopping_rounds=20, epochs=100, frequency_reg=False, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='TabNet', n_trials=50, nominal_idx=[115], num_bins=10, num_classes=1, num_features=117, num_idx=None, num_splits=5, objective='probabilistic_regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256, y_distribution='bimodial')
Start hyperparameter optimization
Loading dataset SAT11...
Dataset loaded! 

(1725, 116)
Using an existing study with name 'TabNet_SAT11' instead of creating a new one.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.13035 | eval_custom_logloss: 0.83953 |  0:00:00s
epoch 1  | loss: 1.22966 | eval_custom_logloss: 0.85287 |  0:00:01s
epoch 2  | loss: 1.0443  | eval_custom_logloss: 0.82862 |  0:00:01s
epoch 3  | loss: 0.91905 | eval_custom_logloss: 0.71343 |  0:00:02s
epoch 4  | loss: 0.71273 | eval_custom_logloss: 0.61797 |  0:00:02s
epoch 5  | loss: 0.63005 | eval_custom_logloss: 0.61426 |  0:00:02s
epoch 6  | loss: 0.64799 | eval_custom_logloss: 0.7083  |  0:00:03s
epoch 7  | loss: 0.60466 | eval_custom_logloss: 0.60443 |  0:00:03s
epoch 8  | loss: 0.59331 | eval_custom_logloss: 0.52598 |  0:00:04s
epoch 9  | loss: 0.56075 | eval_custom_logloss: 0.58618 |  0:00:04s
epoch 10 | loss: 0.54145 | eval_custom_logloss: 0.54447 |  0:00:04s
epoch 11 | loss: 0.50103 | eval_custom_logloss: 0.54915 |  0:00:05s
epoch 12 | loss: 0.47957 | eval_custom_logloss: 0.51314 |  0:00:05s
epoch 13 | loss: 0.49748 | eval_custom_logloss: 0.56853 |  0:00:05s
epoch 14 | loss: 0.48477 | eval_custom_logloss: 0.52665 |  0:00:06s
epoch 15 | loss: 0.4715  | eval_custom_logloss: 0.49697 |  0:00:06s
epoch 16 | loss: 0.41999 | eval_custom_logloss: 0.4858  |  0:00:07s
epoch 17 | loss: 0.39363 | eval_custom_logloss: 0.4797  |  0:00:07s
epoch 18 | loss: 0.38776 | eval_custom_logloss: 0.47409 |  0:00:07s
epoch 19 | loss: 0.38207 | eval_custom_logloss: 0.47518 |  0:00:08s
epoch 20 | loss: 0.36117 | eval_custom_logloss: 0.46921 |  0:00:08s
epoch 21 | loss: 0.32745 | eval_custom_logloss: 0.4402  |  0:00:09s
epoch 22 | loss: 0.34653 | eval_custom_logloss: 0.43286 |  0:00:09s
epoch 23 | loss: 0.34234 | eval_custom_logloss: 0.49243 |  0:00:10s
epoch 24 | loss: 0.34752 | eval_custom_logloss: 0.41691 |  0:00:10s
epoch 25 | loss: 0.30232 | eval_custom_logloss: 0.46482 |  0:00:10s
epoch 26 | loss: 0.29658 | eval_custom_logloss: 0.468   |  0:00:11s
epoch 27 | loss: 0.29538 | eval_custom_logloss: 0.40474 |  0:00:11s
epoch 28 | loss: 0.27677 | eval_custom_logloss: 0.4371  |  0:00:12s
epoch 29 | loss: 0.26943 | eval_custom_logloss: 0.42756 |  0:00:12s
epoch 30 | loss: 0.26598 | eval_custom_logloss: 0.50824 |  0:00:12s
epoch 31 | loss: 0.3154  | eval_custom_logloss: 0.41048 |  0:00:13s
epoch 32 | loss: 0.30141 | eval_custom_logloss: 0.41345 |  0:00:13s
epoch 33 | loss: 0.26833 | eval_custom_logloss: 0.43162 |  0:00:13s
epoch 34 | loss: 0.29132 | eval_custom_logloss: 0.40035 |  0:00:14s
epoch 35 | loss: 0.26529 | eval_custom_logloss: 0.36199 |  0:00:14s
epoch 36 | loss: 0.24567 | eval_custom_logloss: 0.46272 |  0:00:15s
epoch 37 | loss: 0.24775 | eval_custom_logloss: 0.45952 |  0:00:15s
epoch 38 | loss: 0.24923 | eval_custom_logloss: 0.43407 |  0:00:15s
epoch 39 | loss: 0.25814 | eval_custom_logloss: 0.43181 |  0:00:16s
epoch 40 | loss: 0.2376  | eval_custom_logloss: 0.48774 |  0:00:16s
epoch 41 | loss: 0.25401 | eval_custom_logloss: 0.41419 |  0:00:17s
epoch 42 | loss: 0.21915 | eval_custom_logloss: 0.44918 |  0:00:17s
epoch 43 | loss: 0.24925 | eval_custom_logloss: 0.44299 |  0:00:17s
epoch 44 | loss: 0.23333 | eval_custom_logloss: 0.46438 |  0:00:18s
epoch 45 | loss: 0.21729 | eval_custom_logloss: 0.42916 |  0:00:18s
epoch 46 | loss: 0.21811 | eval_custom_logloss: 0.48408 |  0:00:19s
epoch 47 | loss: 0.21787 | eval_custom_logloss: 0.51105 |  0:00:19s
epoch 48 | loss: 0.20952 | eval_custom_logloss: 0.49529 |  0:00:19s
epoch 49 | loss: 0.25472 | eval_custom_logloss: 0.45847 |  0:00:20s
epoch 50 | loss: 0.21842 | eval_custom_logloss: 0.41727 |  0:00:20s
epoch 51 | loss: 0.21582 | eval_custom_logloss: 0.50604 |  0:00:21s
epoch 52 | loss: 0.22167 | eval_custom_logloss: 0.47182 |  0:00:21s
epoch 53 | loss: 0.20183 | eval_custom_logloss: 0.43131 |  0:00:21s
epoch 54 | loss: 0.19582 | eval_custom_logloss: 0.41433 |  0:00:22s
epoch 55 | loss: 0.18533 | eval_custom_logloss: 0.41156 |  0:00:22s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 0.36199
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.362, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.85456 | eval_custom_logloss: 0.8071  |  0:00:00s
epoch 1  | loss: 1.01343 | eval_custom_logloss: 0.9299  |  0:00:00s
epoch 2  | loss: 0.89225 | eval_custom_logloss: 0.81407 |  0:00:01s
epoch 3  | loss: 0.89457 | eval_custom_logloss: 0.98213 |  0:00:01s
epoch 4  | loss: 0.71613 | eval_custom_logloss: 0.77031 |  0:00:01s
epoch 5  | loss: 0.68715 | eval_custom_logloss: 0.84185 |  0:00:02s
epoch 6  | loss: 0.61617 | eval_custom_logloss: 0.82406 |  0:00:02s
epoch 7  | loss: 0.59702 | eval_custom_logloss: 0.77966 |  0:00:03s
epoch 8  | loss: 0.54573 | eval_custom_logloss: 0.67939 |  0:00:03s
epoch 9  | loss: 0.50432 | eval_custom_logloss: 0.6434  |  0:00:03s
epoch 10 | loss: 0.45741 | eval_custom_logloss: 0.60304 |  0:00:04s
epoch 11 | loss: 0.45161 | eval_custom_logloss: 0.73982 |  0:00:04s
epoch 12 | loss: 0.43263 | eval_custom_logloss: 0.59422 |  0:00:05s
epoch 13 | loss: 0.3951  | eval_custom_logloss: 0.63623 |  0:00:05s
epoch 14 | loss: 0.40577 | eval_custom_logloss: 0.60463 |  0:00:05s
epoch 15 | loss: 0.39877 | eval_custom_logloss: 0.60142 |  0:00:06s
epoch 16 | loss: 0.37332 | eval_custom_logloss: 0.56063 |  0:00:06s
epoch 17 | loss: 0.3976  | eval_custom_logloss: 0.52002 |  0:00:07s
epoch 18 | loss: 0.35121 | eval_custom_logloss: 0.55991 |  0:00:07s
epoch 19 | loss: 0.32483 | eval_custom_logloss: 0.59021 |  0:00:07s
epoch 20 | loss: 0.34582 | eval_custom_logloss: 0.56915 |  0:00:08s
epoch 21 | loss: 0.33246 | eval_custom_logloss: 0.60433 |  0:00:08s
epoch 22 | loss: 0.32298 | eval_custom_logloss: 0.51457 |  0:00:09s
epoch 23 | loss: 0.33061 | eval_custom_logloss: 0.57438 |  0:00:09s
epoch 24 | loss: 0.30052 | eval_custom_logloss: 0.62233 |  0:00:09s
epoch 25 | loss: 0.30471 | eval_custom_logloss: 0.59779 |  0:00:10s
epoch 26 | loss: 0.31638 | eval_custom_logloss: 0.54282 |  0:00:10s
epoch 27 | loss: 0.32734 | eval_custom_logloss: 0.54001 |  0:00:11s
epoch 28 | loss: 0.31947 | eval_custom_logloss: 0.55316 |  0:00:11s
epoch 29 | loss: 0.30917 | eval_custom_logloss: 0.51289 |  0:00:11s
epoch 30 | loss: 0.29467 | eval_custom_logloss: 0.53779 |  0:00:12s
epoch 31 | loss: 0.29781 | eval_custom_logloss: 0.53758 |  0:00:12s
epoch 32 | loss: 0.27968 | eval_custom_logloss: 0.53847 |  0:00:13s
epoch 33 | loss: 0.29911 | eval_custom_logloss: 0.53893 |  0:00:13s
epoch 34 | loss: 0.2796  | eval_custom_logloss: 0.51817 |  0:00:13s
epoch 35 | loss: 0.25861 | eval_custom_logloss: 0.54056 |  0:00:14s
epoch 36 | loss: 0.29643 | eval_custom_logloss: 0.56106 |  0:00:14s
epoch 37 | loss: 0.24411 | eval_custom_logloss: 0.54581 |  0:00:15s
epoch 38 | loss: 0.23585 | eval_custom_logloss: 0.46711 |  0:00:15s
epoch 39 | loss: 0.24923 | eval_custom_logloss: 0.47804 |  0:00:15s
epoch 40 | loss: 0.23515 | eval_custom_logloss: 0.48946 |  0:00:16s
epoch 41 | loss: 0.21257 | eval_custom_logloss: 0.49206 |  0:00:16s
epoch 42 | loss: 0.22738 | eval_custom_logloss: 0.55019 |  0:00:17s
epoch 43 | loss: 0.21624 | eval_custom_logloss: 0.45013 |  0:00:17s
epoch 44 | loss: 0.21669 | eval_custom_logloss: 0.45158 |  0:00:17s
epoch 45 | loss: 0.21562 | eval_custom_logloss: 0.53434 |  0:00:18s
epoch 46 | loss: 0.23293 | eval_custom_logloss: 0.60444 |  0:00:18s
epoch 47 | loss: 0.21418 | eval_custom_logloss: 0.50429 |  0:00:19s
epoch 48 | loss: 0.22685 | eval_custom_logloss: 0.53991 |  0:00:19s
epoch 49 | loss: 0.21122 | eval_custom_logloss: 0.53131 |  0:00:19s
epoch 50 | loss: 0.21969 | eval_custom_logloss: 0.63258 |  0:00:20s
epoch 51 | loss: 0.20407 | eval_custom_logloss: 0.59077 |  0:00:20s
epoch 52 | loss: 0.20906 | eval_custom_logloss: 0.55704 |  0:00:21s
epoch 53 | loss: 0.20859 | eval_custom_logloss: 0.54279 |  0:00:21s
epoch 54 | loss: 0.18065 | eval_custom_logloss: 0.5585  |  0:00:21s
epoch 55 | loss: 0.16872 | eval_custom_logloss: 0.58165 |  0:00:22s
epoch 56 | loss: 0.18789 | eval_custom_logloss: 0.53126 |  0:00:22s
epoch 57 | loss: 0.19828 | eval_custom_logloss: 0.60354 |  0:00:23s
epoch 58 | loss: 0.1991  | eval_custom_logloss: 0.67763 |  0:00:23s
epoch 59 | loss: 0.21461 | eval_custom_logloss: 0.62533 |  0:00:23s
epoch 60 | loss: 0.18581 | eval_custom_logloss: 0.54251 |  0:00:24s
epoch 61 | loss: 0.20516 | eval_custom_logloss: 0.46586 |  0:00:24s
epoch 62 | loss: 0.13883 | eval_custom_logloss: 0.53276 |  0:00:25s
epoch 63 | loss: 0.17012 | eval_custom_logloss: 0.63514 |  0:00:25s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 0.45013
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40605, 'Log Loss - std': 0.044050000000000006} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.95137 | eval_custom_logloss: 0.93118 |  0:00:00s
epoch 1  | loss: 1.22904 | eval_custom_logloss: 0.97486 |  0:00:00s
epoch 2  | loss: 1.03055 | eval_custom_logloss: 0.68823 |  0:00:01s
epoch 3  | loss: 0.83679 | eval_custom_logloss: 0.71091 |  0:00:01s
epoch 4  | loss: 0.79781 | eval_custom_logloss: 0.66913 |  0:00:02s
epoch 5  | loss: 0.6828  | eval_custom_logloss: 0.80294 |  0:00:02s
epoch 6  | loss: 0.68125 | eval_custom_logloss: 0.69141 |  0:00:02s
epoch 7  | loss: 0.64932 | eval_custom_logloss: 0.7364  |  0:00:03s
epoch 8  | loss: 0.65979 | eval_custom_logloss: 0.62691 |  0:00:03s
epoch 9  | loss: 0.59389 | eval_custom_logloss: 0.6375  |  0:00:04s
epoch 10 | loss: 0.57276 | eval_custom_logloss: 0.57894 |  0:00:04s
epoch 11 | loss: 0.62681 | eval_custom_logloss: 0.60305 |  0:00:04s
epoch 12 | loss: 0.60794 | eval_custom_logloss: 0.57784 |  0:00:05s
epoch 13 | loss: 0.53087 | eval_custom_logloss: 0.59942 |  0:00:05s
epoch 14 | loss: 0.53503 | eval_custom_logloss: 0.59317 |  0:00:06s
epoch 15 | loss: 0.52581 | eval_custom_logloss: 0.54732 |  0:00:06s
epoch 16 | loss: 0.50269 | eval_custom_logloss: 0.5765  |  0:00:06s
epoch 17 | loss: 0.51411 | eval_custom_logloss: 0.58497 |  0:00:07s
epoch 18 | loss: 0.49428 | eval_custom_logloss: 0.55414 |  0:00:07s
epoch 19 | loss: 0.46445 | eval_custom_logloss: 0.55712 |  0:00:08s
epoch 20 | loss: 0.45318 | eval_custom_logloss: 0.48325 |  0:00:08s
epoch 21 | loss: 0.45522 | eval_custom_logloss: 0.55995 |  0:00:09s
epoch 22 | loss: 0.43614 | eval_custom_logloss: 0.44791 |  0:00:09s
epoch 23 | loss: 0.42483 | eval_custom_logloss: 0.46694 |  0:00:10s
epoch 24 | loss: 0.39616 | eval_custom_logloss: 0.45032 |  0:00:10s
epoch 25 | loss: 0.40795 | eval_custom_logloss: 0.45409 |  0:00:10s
epoch 26 | loss: 0.41527 | eval_custom_logloss: 0.45597 |  0:00:11s
epoch 27 | loss: 0.38137 | eval_custom_logloss: 0.46087 |  0:00:11s
epoch 28 | loss: 0.38567 | eval_custom_logloss: 0.52216 |  0:00:12s
epoch 29 | loss: 0.3842  | eval_custom_logloss: 0.48726 |  0:00:12s
epoch 30 | loss: 0.37267 | eval_custom_logloss: 0.43765 |  0:00:12s
epoch 31 | loss: 0.33151 | eval_custom_logloss: 0.45808 |  0:00:13s
epoch 32 | loss: 0.33766 | eval_custom_logloss: 0.49235 |  0:00:13s
epoch 33 | loss: 0.37514 | eval_custom_logloss: 0.45209 |  0:00:13s
epoch 34 | loss: 0.36845 | eval_custom_logloss: 0.47367 |  0:00:14s
epoch 35 | loss: 0.34325 | eval_custom_logloss: 0.49333 |  0:00:14s
epoch 36 | loss: 0.34142 | eval_custom_logloss: 0.49047 |  0:00:15s
epoch 37 | loss: 0.37821 | eval_custom_logloss: 0.5014  |  0:00:15s
epoch 38 | loss: 0.34017 | eval_custom_logloss: 0.43409 |  0:00:15s
epoch 39 | loss: 0.37977 | eval_custom_logloss: 0.52239 |  0:00:16s
epoch 40 | loss: 0.35553 | eval_custom_logloss: 0.5058  |  0:00:16s
epoch 41 | loss: 0.33608 | eval_custom_logloss: 0.50697 |  0:00:17s
epoch 42 | loss: 0.34855 | eval_custom_logloss: 0.41884 |  0:00:17s
epoch 43 | loss: 0.30121 | eval_custom_logloss: 0.38625 |  0:00:17s
epoch 44 | loss: 0.34095 | eval_custom_logloss: 0.45838 |  0:00:18s
epoch 45 | loss: 0.33833 | eval_custom_logloss: 0.39375 |  0:00:18s
epoch 46 | loss: 0.32143 | eval_custom_logloss: 0.45017 |  0:00:19s
epoch 47 | loss: 0.28362 | eval_custom_logloss: 0.34211 |  0:00:19s
epoch 48 | loss: 0.27317 | eval_custom_logloss: 0.38914 |  0:00:19s
epoch 49 | loss: 0.27632 | eval_custom_logloss: 0.39083 |  0:00:20s
epoch 50 | loss: 0.28227 | eval_custom_logloss: 0.42211 |  0:00:20s
epoch 51 | loss: 0.28225 | eval_custom_logloss: 0.42444 |  0:00:21s
epoch 52 | loss: 0.29562 | eval_custom_logloss: 0.50083 |  0:00:21s
epoch 53 | loss: 0.2874  | eval_custom_logloss: 0.38995 |  0:00:21s
epoch 54 | loss: 0.26965 | eval_custom_logloss: 0.41242 |  0:00:22s
epoch 55 | loss: 0.2416  | eval_custom_logloss: 0.39996 |  0:00:22s
epoch 56 | loss: 0.28319 | eval_custom_logloss: 0.44657 |  0:00:23s
epoch 57 | loss: 0.278   | eval_custom_logloss: 0.36779 |  0:00:23s
epoch 58 | loss: 0.27489 | eval_custom_logloss: 0.39805 |  0:00:23s
epoch 59 | loss: 0.23976 | eval_custom_logloss: 0.39236 |  0:00:24s
epoch 60 | loss: 0.2541  | eval_custom_logloss: 0.48828 |  0:00:24s
epoch 61 | loss: 0.28779 | eval_custom_logloss: 0.50146 |  0:00:25s
epoch 62 | loss: 0.25726 | eval_custom_logloss: 0.42526 |  0:00:25s
epoch 63 | loss: 0.2167  | eval_custom_logloss: 0.36966 |  0:00:25s
epoch 64 | loss: 0.20404 | eval_custom_logloss: 0.3704  |  0:00:26s
epoch 65 | loss: 0.20944 | eval_custom_logloss: 0.40177 |  0:00:26s
epoch 66 | loss: 0.17817 | eval_custom_logloss: 0.4127  |  0:00:27s
epoch 67 | loss: 0.15682 | eval_custom_logloss: 0.38975 |  0:00:27s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.34211
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.38473333333333337, 'Log Loss - std': 0.0469297583865741} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.93668 | eval_custom_logloss: 1.12791 |  0:00:00s
epoch 1  | loss: 1.21275 | eval_custom_logloss: 0.84546 |  0:00:00s
epoch 2  | loss: 0.9215  | eval_custom_logloss: 0.78515 |  0:00:01s
epoch 3  | loss: 0.84311 | eval_custom_logloss: 1.0353  |  0:00:01s
epoch 4  | loss: 0.89651 | eval_custom_logloss: 0.92816 |  0:00:01s
epoch 5  | loss: 0.70298 | eval_custom_logloss: 0.79308 |  0:00:02s
epoch 6  | loss: 0.59634 | eval_custom_logloss: 0.69039 |  0:00:02s
epoch 7  | loss: 0.55123 | eval_custom_logloss: 0.62159 |  0:00:02s
epoch 8  | loss: 0.56644 | eval_custom_logloss: 0.57965 |  0:00:03s
epoch 9  | loss: 0.50996 | eval_custom_logloss: 0.63279 |  0:00:03s
epoch 10 | loss: 0.52082 | eval_custom_logloss: 0.66216 |  0:00:03s
epoch 11 | loss: 0.49824 | eval_custom_logloss: 0.63843 |  0:00:04s
epoch 12 | loss: 0.48939 | eval_custom_logloss: 0.64656 |  0:00:04s
epoch 13 | loss: 0.49049 | eval_custom_logloss: 0.57218 |  0:00:05s
epoch 14 | loss: 0.42446 | eval_custom_logloss: 0.58454 |  0:00:05s
epoch 15 | loss: 0.43584 | eval_custom_logloss: 0.5268  |  0:00:05s
epoch 16 | loss: 0.41627 | eval_custom_logloss: 0.49029 |  0:00:06s
epoch 17 | loss: 0.42211 | eval_custom_logloss: 0.48965 |  0:00:06s
epoch 18 | loss: 0.40218 | eval_custom_logloss: 0.57832 |  0:00:07s
epoch 19 | loss: 0.40442 | eval_custom_logloss: 0.4829  |  0:00:07s
epoch 20 | loss: 0.36718 | eval_custom_logloss: 0.50822 |  0:00:08s
epoch 21 | loss: 0.37596 | eval_custom_logloss: 0.54036 |  0:00:08s
epoch 22 | loss: 0.37107 | eval_custom_logloss: 0.50172 |  0:00:09s
epoch 23 | loss: 0.33551 | eval_custom_logloss: 0.48478 |  0:00:09s
epoch 24 | loss: 0.3132  | eval_custom_logloss: 0.49793 |  0:00:10s
epoch 25 | loss: 0.29886 | eval_custom_logloss: 0.54829 |  0:00:10s
epoch 26 | loss: 0.34237 | eval_custom_logloss: 0.46985 |  0:00:10s
epoch 27 | loss: 0.32161 | eval_custom_logloss: 0.49407 |  0:00:11s
epoch 28 | loss: 0.30736 | eval_custom_logloss: 0.49704 |  0:00:11s
epoch 29 | loss: 0.28495 | eval_custom_logloss: 0.4896  |  0:00:12s
epoch 30 | loss: 0.24766 | eval_custom_logloss: 0.50922 |  0:00:12s
epoch 31 | loss: 0.24698 | eval_custom_logloss: 0.6107  |  0:00:13s
epoch 32 | loss: 0.29187 | eval_custom_logloss: 0.55134 |  0:00:13s
epoch 33 | loss: 0.30731 | eval_custom_logloss: 0.50936 |  0:00:13s
epoch 34 | loss: 0.27611 | eval_custom_logloss: 0.50393 |  0:00:14s
epoch 35 | loss: 0.28142 | eval_custom_logloss: 0.60584 |  0:00:14s
epoch 36 | loss: 0.28927 | eval_custom_logloss: 0.54578 |  0:00:15s
epoch 37 | loss: 0.26627 | eval_custom_logloss: 0.5215  |  0:00:15s
epoch 38 | loss: 0.22639 | eval_custom_logloss: 0.58227 |  0:00:15s
epoch 39 | loss: 0.23076 | eval_custom_logloss: 0.51216 |  0:00:16s
epoch 40 | loss: 0.23406 | eval_custom_logloss: 0.58606 |  0:00:16s
epoch 41 | loss: 0.22549 | eval_custom_logloss: 0.65857 |  0:00:16s
epoch 42 | loss: 0.21436 | eval_custom_logloss: 0.59234 |  0:00:17s
epoch 43 | loss: 0.22702 | eval_custom_logloss: 0.67696 |  0:00:17s
epoch 44 | loss: 0.21967 | eval_custom_logloss: 0.58208 |  0:00:18s
epoch 45 | loss: 0.22763 | eval_custom_logloss: 0.55157 |  0:00:18s
epoch 46 | loss: 0.24683 | eval_custom_logloss: 0.6079  |  0:00:18s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 0.46985
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40537500000000004, 'Log Loss - std': 0.05412981502831873} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.1139  | eval_custom_logloss: 1.03182 |  0:00:00s
epoch 1  | loss: 1.32621 | eval_custom_logloss: 0.73194 |  0:00:00s
epoch 2  | loss: 1.12181 | eval_custom_logloss: 0.68447 |  0:00:01s
epoch 3  | loss: 0.91455 | eval_custom_logloss: 0.71396 |  0:00:01s
epoch 4  | loss: 0.76204 | eval_custom_logloss: 0.66715 |  0:00:02s
epoch 5  | loss: 0.76259 | eval_custom_logloss: 0.75744 |  0:00:02s
epoch 6  | loss: 0.66599 | eval_custom_logloss: 0.71109 |  0:00:02s
epoch 7  | loss: 0.59841 | eval_custom_logloss: 0.6791  |  0:00:03s
epoch 8  | loss: 0.67783 | eval_custom_logloss: 0.47745 |  0:00:03s
epoch 9  | loss: 0.57136 | eval_custom_logloss: 0.43969 |  0:00:04s
epoch 10 | loss: 0.57163 | eval_custom_logloss: 0.43933 |  0:00:04s
epoch 11 | loss: 0.50107 | eval_custom_logloss: 0.4076  |  0:00:05s
epoch 12 | loss: 0.52748 | eval_custom_logloss: 0.47514 |  0:00:05s
epoch 13 | loss: 0.51439 | eval_custom_logloss: 0.48803 |  0:00:06s
epoch 14 | loss: 0.4558  | eval_custom_logloss: 0.40693 |  0:00:06s
epoch 15 | loss: 0.45562 | eval_custom_logloss: 0.38473 |  0:00:07s
epoch 16 | loss: 0.43252 | eval_custom_logloss: 0.41728 |  0:00:07s
epoch 17 | loss: 0.46761 | eval_custom_logloss: 0.40142 |  0:00:08s
epoch 18 | loss: 0.41413 | eval_custom_logloss: 0.39022 |  0:00:08s
epoch 19 | loss: 0.4136  | eval_custom_logloss: 0.37948 |  0:00:08s
epoch 20 | loss: 0.40455 | eval_custom_logloss: 0.39308 |  0:00:09s
epoch 21 | loss: 0.39125 | eval_custom_logloss: 0.31001 |  0:00:09s
epoch 22 | loss: 0.38645 | eval_custom_logloss: 0.32599 |  0:00:10s
epoch 23 | loss: 0.32436 | eval_custom_logloss: 0.30922 |  0:00:10s
epoch 24 | loss: 0.32766 | eval_custom_logloss: 0.34211 |  0:00:11s
epoch 25 | loss: 0.3441  | eval_custom_logloss: 0.35917 |  0:00:11s
epoch 26 | loss: 0.32024 | eval_custom_logloss: 0.31667 |  0:00:12s
epoch 27 | loss: 0.32173 | eval_custom_logloss: 0.31025 |  0:00:12s
epoch 28 | loss: 0.33451 | eval_custom_logloss: 0.29126 |  0:00:13s
epoch 29 | loss: 0.29332 | eval_custom_logloss: 0.27935 |  0:00:13s
epoch 30 | loss: 0.29401 | eval_custom_logloss: 0.29008 |  0:00:13s
epoch 31 | loss: 0.2857  | eval_custom_logloss: 0.29765 |  0:00:14s
epoch 32 | loss: 0.30127 | eval_custom_logloss: 0.3061  |  0:00:14s
epoch 33 | loss: 0.3494  | eval_custom_logloss: 0.34814 |  0:00:15s
epoch 34 | loss: 0.30464 | eval_custom_logloss: 0.34815 |  0:00:15s
epoch 35 | loss: 0.27276 | eval_custom_logloss: 0.30732 |  0:00:15s
epoch 36 | loss: 0.26121 | eval_custom_logloss: 0.30921 |  0:00:16s
epoch 37 | loss: 0.26076 | eval_custom_logloss: 0.34342 |  0:00:16s
epoch 38 | loss: 0.29795 | eval_custom_logloss: 0.33108 |  0:00:16s
epoch 39 | loss: 0.26435 | eval_custom_logloss: 0.275   |  0:00:17s
epoch 40 | loss: 0.2638  | eval_custom_logloss: 0.30129 |  0:00:17s
epoch 41 | loss: 0.25994 | eval_custom_logloss: 0.3519  |  0:00:18s
epoch 42 | loss: 0.24174 | eval_custom_logloss: 0.32041 |  0:00:18s
epoch 43 | loss: 0.25366 | eval_custom_logloss: 0.31091 |  0:00:18s
epoch 44 | loss: 0.26489 | eval_custom_logloss: 0.33075 |  0:00:19s
epoch 45 | loss: 0.24668 | eval_custom_logloss: 0.38537 |  0:00:19s
epoch 46 | loss: 0.25264 | eval_custom_logloss: 0.36926 |  0:00:20s
epoch 47 | loss: 0.2542  | eval_custom_logloss: 0.32176 |  0:00:20s
epoch 48 | loss: 0.25484 | eval_custom_logloss: 0.29157 |  0:00:20s
epoch 49 | loss: 0.2709  | eval_custom_logloss: 0.34102 |  0:00:21s
epoch 50 | loss: 0.21038 | eval_custom_logloss: 0.31509 |  0:00:21s
epoch 51 | loss: 0.21015 | eval_custom_logloss: 0.29175 |  0:00:22s
epoch 52 | loss: 0.19887 | eval_custom_logloss: 0.2944  |  0:00:22s
epoch 53 | loss: 0.19824 | eval_custom_logloss: 0.3107  |  0:00:22s
epoch 54 | loss: 0.19375 | eval_custom_logloss: 0.31243 |  0:00:23s
epoch 55 | loss: 0.20457 | eval_custom_logloss: 0.27171 |  0:00:23s
epoch 56 | loss: 0.22266 | eval_custom_logloss: 0.29863 |  0:00:24s
epoch 57 | loss: 0.2101  | eval_custom_logloss: 0.30503 |  0:00:24s
epoch 58 | loss: 0.20084 | eval_custom_logloss: 0.30778 |  0:00:24s
epoch 59 | loss: 0.17227 | eval_custom_logloss: 0.2948  |  0:00:25s
epoch 60 | loss: 0.15892 | eval_custom_logloss: 0.29079 |  0:00:25s
epoch 61 | loss: 0.18408 | eval_custom_logloss: 0.30338 |  0:00:25s
epoch 62 | loss: 0.15264 | eval_custom_logloss: 0.34452 |  0:00:26s
epoch 63 | loss: 0.18894 | eval_custom_logloss: 0.39343 |  0:00:26s
epoch 64 | loss: 0.26785 | eval_custom_logloss: 0.29083 |  0:00:27s
epoch 65 | loss: 0.2426  | eval_custom_logloss: 0.2867  |  0:00:27s
epoch 66 | loss: 0.21706 | eval_custom_logloss: 0.31921 |  0:00:27s
epoch 67 | loss: 0.18493 | eval_custom_logloss: 0.32862 |  0:00:28s
epoch 68 | loss: 0.1762  | eval_custom_logloss: 0.36247 |  0:00:28s
epoch 69 | loss: 0.20891 | eval_custom_logloss: 0.34481 |  0:00:29s
epoch 70 | loss: 0.19694 | eval_custom_logloss: 0.30071 |  0:00:29s
epoch 71 | loss: 0.17828 | eval_custom_logloss: 0.37222 |  0:00:29s
epoch 72 | loss: 0.16483 | eval_custom_logloss: 0.40245 |  0:00:30s
epoch 73 | loss: 0.19409 | eval_custom_logloss: 0.33163 |  0:00:30s
epoch 74 | loss: 0.17863 | eval_custom_logloss: 0.30289 |  0:00:31s
epoch 75 | loss: 0.1628  | eval_custom_logloss: 0.38681 |  0:00:31s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.27171
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3760800000000001, 'Log Loss - std': 0.07600537875703271} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 0.3760800000000001 and parameters: {'n_d': 64, 'n_steps': 6, 'gamma': 1.0792832619432806, 'cat_emb_dim': 1, 'n_independent': 2, 'n_shared': 2, 'momentum': 0.07454631061889978, 'mask_type': 'entmax'}. Best is trial 3 with value: 0.3760800000000001.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.31703 | eval_custom_logloss: 0.81952 |  0:00:00s
epoch 1  | loss: 0.98891 | eval_custom_logloss: 1.14264 |  0:00:01s
epoch 2  | loss: 0.90963 | eval_custom_logloss: 0.74344 |  0:00:01s
epoch 3  | loss: 0.83183 | eval_custom_logloss: 1.24551 |  0:00:02s
epoch 4  | loss: 0.82148 | eval_custom_logloss: 0.85408 |  0:00:02s
epoch 5  | loss: 0.78531 | eval_custom_logloss: 0.68623 |  0:00:03s
epoch 6  | loss: 0.78011 | eval_custom_logloss: 0.75764 |  0:00:03s
epoch 7  | loss: 0.7059  | eval_custom_logloss: 0.59518 |  0:00:04s
epoch 8  | loss: 0.65686 | eval_custom_logloss: 0.59383 |  0:00:04s
epoch 9  | loss: 0.65438 | eval_custom_logloss: 0.58536 |  0:00:05s
epoch 10 | loss: 0.63893 | eval_custom_logloss: 0.58638 |  0:00:05s
epoch 11 | loss: 0.57334 | eval_custom_logloss: 0.57208 |  0:00:05s
epoch 12 | loss: 0.56714 | eval_custom_logloss: 0.57591 |  0:00:06s
epoch 13 | loss: 0.60438 | eval_custom_logloss: 0.54216 |  0:00:06s
epoch 14 | loss: 0.56263 | eval_custom_logloss: 0.54632 |  0:00:07s
epoch 15 | loss: 0.53114 | eval_custom_logloss: 0.528   |  0:00:07s
epoch 16 | loss: 0.51526 | eval_custom_logloss: 0.50936 |  0:00:08s
epoch 17 | loss: 0.49619 | eval_custom_logloss: 0.48188 |  0:00:08s
epoch 18 | loss: 0.50645 | eval_custom_logloss: 0.51267 |  0:00:09s
epoch 19 | loss: 0.49735 | eval_custom_logloss: 0.50376 |  0:00:09s
epoch 20 | loss: 0.49958 | eval_custom_logloss: 0.51469 |  0:00:10s
epoch 21 | loss: 0.49507 | eval_custom_logloss: 0.47916 |  0:00:10s
epoch 22 | loss: 0.48967 | eval_custom_logloss: 0.48413 |  0:00:11s
epoch 23 | loss: 0.46074 | eval_custom_logloss: 0.51165 |  0:00:11s
epoch 24 | loss: 0.47304 | eval_custom_logloss: 0.51741 |  0:00:12s
epoch 25 | loss: 0.46274 | eval_custom_logloss: 0.52557 |  0:00:12s
epoch 26 | loss: 0.47948 | eval_custom_logloss: 0.50664 |  0:00:13s
epoch 27 | loss: 0.47434 | eval_custom_logloss: 0.49845 |  0:00:13s
epoch 28 | loss: 0.46528 | eval_custom_logloss: 0.48087 |  0:00:14s
epoch 29 | loss: 0.48955 | eval_custom_logloss: 0.51152 |  0:00:14s
epoch 30 | loss: 0.46532 | eval_custom_logloss: 0.50394 |  0:00:15s
epoch 31 | loss: 0.45118 | eval_custom_logloss: 0.50421 |  0:00:15s
epoch 32 | loss: 0.44313 | eval_custom_logloss: 0.49716 |  0:00:16s
epoch 33 | loss: 0.43991 | eval_custom_logloss: 0.44882 |  0:00:16s
epoch 34 | loss: 0.43603 | eval_custom_logloss: 0.45546 |  0:00:17s
epoch 35 | loss: 0.4401  | eval_custom_logloss: 0.45959 |  0:00:17s
epoch 36 | loss: 0.42588 | eval_custom_logloss: 0.46125 |  0:00:18s
epoch 37 | loss: 0.41935 | eval_custom_logloss: 0.47486 |  0:00:18s
epoch 38 | loss: 0.42825 | eval_custom_logloss: 0.47988 |  0:00:19s
epoch 39 | loss: 0.45673 | eval_custom_logloss: 0.48809 |  0:00:19s
epoch 40 | loss: 0.42946 | eval_custom_logloss: 0.45429 |  0:00:20s
epoch 41 | loss: 0.4044  | eval_custom_logloss: 0.43793 |  0:00:20s
epoch 42 | loss: 0.42972 | eval_custom_logloss: 0.46251 |  0:00:21s
epoch 43 | loss: 0.39809 | eval_custom_logloss: 0.44885 |  0:00:21s
epoch 44 | loss: 0.40668 | eval_custom_logloss: 0.45083 |  0:00:22s
epoch 45 | loss: 0.41391 | eval_custom_logloss: 0.45053 |  0:00:22s
epoch 46 | loss: 0.39759 | eval_custom_logloss: 0.45699 |  0:00:23s
epoch 47 | loss: 0.39858 | eval_custom_logloss: 0.45173 |  0:00:23s
epoch 48 | loss: 0.39174 | eval_custom_logloss: 0.50573 |  0:00:23s
epoch 49 | loss: 0.43307 | eval_custom_logloss: 0.47016 |  0:00:24s
epoch 50 | loss: 0.41293 | eval_custom_logloss: 0.41803 |  0:00:24s
epoch 51 | loss: 0.39023 | eval_custom_logloss: 0.42251 |  0:00:25s
epoch 52 | loss: 0.4154  | eval_custom_logloss: 0.45119 |  0:00:25s
epoch 53 | loss: 0.3893  | eval_custom_logloss: 0.4484  |  0:00:26s
epoch 54 | loss: 0.38896 | eval_custom_logloss: 0.46    |  0:00:26s
epoch 55 | loss: 0.40938 | eval_custom_logloss: 0.46848 |  0:00:27s
epoch 56 | loss: 0.43476 | eval_custom_logloss: 0.4719  |  0:00:27s
epoch 57 | loss: 0.39818 | eval_custom_logloss: 0.48585 |  0:00:28s
epoch 58 | loss: 0.39401 | eval_custom_logloss: 0.44266 |  0:00:28s
epoch 59 | loss: 0.37937 | eval_custom_logloss: 0.44454 |  0:00:29s
epoch 60 | loss: 0.34402 | eval_custom_logloss: 0.3963  |  0:00:29s
epoch 61 | loss: 0.34547 | eval_custom_logloss: 0.41743 |  0:00:30s
epoch 62 | loss: 0.33725 | eval_custom_logloss: 0.40183 |  0:00:30s
epoch 63 | loss: 0.33444 | eval_custom_logloss: 0.43347 |  0:00:31s
epoch 64 | loss: 0.40353 | eval_custom_logloss: 0.48137 |  0:00:31s
epoch 65 | loss: 0.37105 | eval_custom_logloss: 0.41494 |  0:00:32s
epoch 66 | loss: 0.35312 | eval_custom_logloss: 0.38891 |  0:00:32s
epoch 67 | loss: 0.33506 | eval_custom_logloss: 0.45275 |  0:00:33s
epoch 68 | loss: 0.31272 | eval_custom_logloss: 0.43213 |  0:00:33s
epoch 69 | loss: 0.36171 | eval_custom_logloss: 0.43882 |  0:00:34s
epoch 70 | loss: 0.30349 | eval_custom_logloss: 0.42615 |  0:00:34s
epoch 71 | loss: 0.29733 | eval_custom_logloss: 0.4084  |  0:00:35s
epoch 72 | loss: 0.3062  | eval_custom_logloss: 0.42524 |  0:00:35s
epoch 73 | loss: 0.28979 | eval_custom_logloss: 0.39056 |  0:00:36s
epoch 74 | loss: 0.31354 | eval_custom_logloss: 0.43919 |  0:00:36s
epoch 75 | loss: 0.30869 | eval_custom_logloss: 0.51835 |  0:00:37s
epoch 76 | loss: 0.3534  | eval_custom_logloss: 0.43225 |  0:00:37s
epoch 77 | loss: 0.32256 | eval_custom_logloss: 0.40489 |  0:00:38s
epoch 78 | loss: 0.29326 | eval_custom_logloss: 0.37558 |  0:00:38s
epoch 79 | loss: 0.3671  | eval_custom_logloss: 0.47627 |  0:00:39s
epoch 80 | loss: 0.36942 | eval_custom_logloss: 0.48366 |  0:00:39s
epoch 81 | loss: 0.3478  | eval_custom_logloss: 0.50736 |  0:00:40s
epoch 82 | loss: 0.37985 | eval_custom_logloss: 0.47902 |  0:00:40s
epoch 83 | loss: 0.34297 | eval_custom_logloss: 0.42406 |  0:00:40s
epoch 84 | loss: 0.33979 | eval_custom_logloss: 0.40602 |  0:00:41s
epoch 85 | loss: 0.30589 | eval_custom_logloss: 0.4354  |  0:00:41s
epoch 86 | loss: 0.31439 | eval_custom_logloss: 0.4546  |  0:00:42s
epoch 87 | loss: 0.3021  | eval_custom_logloss: 0.47907 |  0:00:42s
epoch 88 | loss: 0.3055  | eval_custom_logloss: 0.44962 |  0:00:43s
epoch 89 | loss: 0.26267 | eval_custom_logloss: 0.43936 |  0:00:43s
epoch 90 | loss: 0.29598 | eval_custom_logloss: 0.44189 |  0:00:44s
epoch 91 | loss: 0.2915  | eval_custom_logloss: 0.4371  |  0:00:44s
epoch 92 | loss: 0.26929 | eval_custom_logloss: 0.44645 |  0:00:45s
epoch 93 | loss: 0.29217 | eval_custom_logloss: 0.40528 |  0:00:45s
epoch 94 | loss: 0.29019 | eval_custom_logloss: 0.43087 |  0:00:46s
epoch 95 | loss: 0.23536 | eval_custom_logloss: 0.46327 |  0:00:46s
epoch 96 | loss: 0.21739 | eval_custom_logloss: 0.44099 |  0:00:47s
epoch 97 | loss: 0.2397  | eval_custom_logloss: 0.45237 |  0:00:47s
epoch 98 | loss: 0.21656 | eval_custom_logloss: 0.44062 |  0:00:48s

Early stopping occurred at epoch 98 with best_epoch = 78 and best_eval_custom_logloss = 0.37558
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3756, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.24379 | eval_custom_logloss: 0.8911  |  0:00:00s
epoch 1  | loss: 0.97893 | eval_custom_logloss: 0.92968 |  0:00:01s
epoch 2  | loss: 0.87008 | eval_custom_logloss: 0.84436 |  0:00:01s
epoch 3  | loss: 0.80369 | eval_custom_logloss: 0.76738 |  0:00:02s
epoch 4  | loss: 0.84396 | eval_custom_logloss: 1.36063 |  0:00:02s
epoch 5  | loss: 0.77209 | eval_custom_logloss: 0.79996 |  0:00:03s
epoch 6  | loss: 0.65562 | eval_custom_logloss: 0.79498 |  0:00:03s
epoch 7  | loss: 0.68266 | eval_custom_logloss: 0.7788  |  0:00:04s
epoch 8  | loss: 0.63716 | eval_custom_logloss: 0.71259 |  0:00:04s
epoch 9  | loss: 0.63352 | eval_custom_logloss: 0.77078 |  0:00:05s
epoch 10 | loss: 0.63853 | eval_custom_logloss: 0.7404  |  0:00:05s
epoch 11 | loss: 0.59506 | eval_custom_logloss: 0.69885 |  0:00:06s
epoch 12 | loss: 0.57517 | eval_custom_logloss: 0.66507 |  0:00:06s
epoch 13 | loss: 0.55482 | eval_custom_logloss: 0.61636 |  0:00:07s
epoch 14 | loss: 0.55173 | eval_custom_logloss: 0.56335 |  0:00:07s
epoch 15 | loss: 0.5256  | eval_custom_logloss: 0.62583 |  0:00:08s
epoch 16 | loss: 0.53306 | eval_custom_logloss: 0.60687 |  0:00:08s
epoch 17 | loss: 0.48505 | eval_custom_logloss: 0.56698 |  0:00:09s
epoch 18 | loss: 0.46581 | eval_custom_logloss: 0.56966 |  0:00:09s
epoch 19 | loss: 0.46978 | eval_custom_logloss: 0.565   |  0:00:10s
epoch 20 | loss: 0.47522 | eval_custom_logloss: 0.61754 |  0:00:10s
epoch 21 | loss: 0.43393 | eval_custom_logloss: 0.5636  |  0:00:11s
epoch 22 | loss: 0.4448  | eval_custom_logloss: 0.56475 |  0:00:11s
epoch 23 | loss: 0.42962 | eval_custom_logloss: 0.56602 |  0:00:12s
epoch 24 | loss: 0.44688 | eval_custom_logloss: 0.55227 |  0:00:12s
epoch 25 | loss: 0.42388 | eval_custom_logloss: 0.56989 |  0:00:13s
epoch 26 | loss: 0.42468 | eval_custom_logloss: 0.52897 |  0:00:13s
epoch 27 | loss: 0.39319 | eval_custom_logloss: 0.52746 |  0:00:14s
epoch 28 | loss: 0.38723 | eval_custom_logloss: 0.49745 |  0:00:14s
epoch 29 | loss: 0.38225 | eval_custom_logloss: 0.58431 |  0:00:15s
epoch 30 | loss: 0.35713 | eval_custom_logloss: 0.5181  |  0:00:15s
epoch 31 | loss: 0.36413 | eval_custom_logloss: 0.51145 |  0:00:16s
epoch 32 | loss: 0.34798 | eval_custom_logloss: 0.49503 |  0:00:16s
epoch 33 | loss: 0.35722 | eval_custom_logloss: 0.48643 |  0:00:17s
epoch 34 | loss: 0.34802 | eval_custom_logloss: 0.47276 |  0:00:17s
epoch 35 | loss: 0.33412 | eval_custom_logloss: 0.5097  |  0:00:18s
epoch 36 | loss: 0.3417  | eval_custom_logloss: 0.51836 |  0:00:18s
epoch 37 | loss: 0.35512 | eval_custom_logloss: 0.58526 |  0:00:19s
epoch 38 | loss: 0.34186 | eval_custom_logloss: 0.61989 |  0:00:19s
epoch 39 | loss: 0.31835 | eval_custom_logloss: 0.57003 |  0:00:20s
epoch 40 | loss: 0.35505 | eval_custom_logloss: 0.57837 |  0:00:20s
epoch 41 | loss: 0.37085 | eval_custom_logloss: 0.57499 |  0:00:21s
epoch 42 | loss: 0.38633 | eval_custom_logloss: 0.56502 |  0:00:21s
epoch 43 | loss: 0.3544  | eval_custom_logloss: 0.53538 |  0:00:22s
epoch 44 | loss: 0.32031 | eval_custom_logloss: 0.47221 |  0:00:22s
epoch 45 | loss: 0.32438 | eval_custom_logloss: 0.53042 |  0:00:22s
epoch 46 | loss: 0.31786 | eval_custom_logloss: 0.49765 |  0:00:23s
epoch 47 | loss: 0.29501 | eval_custom_logloss: 0.48806 |  0:00:23s
epoch 48 | loss: 0.30076 | eval_custom_logloss: 0.55382 |  0:00:24s
epoch 49 | loss: 0.31288 | eval_custom_logloss: 0.52623 |  0:00:24s
epoch 50 | loss: 0.28921 | eval_custom_logloss: 0.5488  |  0:00:25s
epoch 51 | loss: 0.29633 | eval_custom_logloss: 0.50944 |  0:00:25s
epoch 52 | loss: 0.29224 | eval_custom_logloss: 0.54794 |  0:00:26s
epoch 53 | loss: 0.31746 | eval_custom_logloss: 0.47407 |  0:00:26s
epoch 54 | loss: 0.2899  | eval_custom_logloss: 0.45612 |  0:00:27s
epoch 55 | loss: 0.27476 | eval_custom_logloss: 0.46508 |  0:00:27s
epoch 56 | loss: 0.24447 | eval_custom_logloss: 0.46694 |  0:00:28s
epoch 57 | loss: 0.23171 | eval_custom_logloss: 0.49186 |  0:00:28s
epoch 58 | loss: 0.25376 | eval_custom_logloss: 0.47205 |  0:00:29s
epoch 59 | loss: 0.26786 | eval_custom_logloss: 0.53535 |  0:00:29s
epoch 60 | loss: 0.25129 | eval_custom_logloss: 0.4492  |  0:00:30s
epoch 61 | loss: 0.24459 | eval_custom_logloss: 0.44516 |  0:00:30s
epoch 62 | loss: 0.2457  | eval_custom_logloss: 0.45588 |  0:00:31s
epoch 63 | loss: 0.21924 | eval_custom_logloss: 0.52484 |  0:00:31s
epoch 64 | loss: 0.22163 | eval_custom_logloss: 0.53266 |  0:00:32s
epoch 65 | loss: 0.23143 | eval_custom_logloss: 0.58201 |  0:00:32s
epoch 66 | loss: 0.22428 | eval_custom_logloss: 0.53137 |  0:00:33s
epoch 67 | loss: 0.2291  | eval_custom_logloss: 0.59143 |  0:00:33s
epoch 68 | loss: 0.21515 | eval_custom_logloss: 0.44398 |  0:00:34s
epoch 69 | loss: 0.22427 | eval_custom_logloss: 0.45132 |  0:00:34s
epoch 70 | loss: 0.20402 | eval_custom_logloss: 0.45511 |  0:00:35s
epoch 71 | loss: 0.20618 | eval_custom_logloss: 0.48803 |  0:00:35s
epoch 72 | loss: 0.21036 | eval_custom_logloss: 0.54521 |  0:00:36s
epoch 73 | loss: 0.17552 | eval_custom_logloss: 0.50635 |  0:00:36s
epoch 74 | loss: 0.18164 | eval_custom_logloss: 0.46634 |  0:00:37s
epoch 75 | loss: 0.16198 | eval_custom_logloss: 0.43845 |  0:00:37s
epoch 76 | loss: 0.16561 | eval_custom_logloss: 0.48121 |  0:00:38s
epoch 77 | loss: 0.20391 | eval_custom_logloss: 0.52453 |  0:00:38s
epoch 78 | loss: 0.17432 | eval_custom_logloss: 0.48174 |  0:00:39s
epoch 79 | loss: 0.17801 | eval_custom_logloss: 0.51721 |  0:00:39s
epoch 80 | loss: 0.16587 | eval_custom_logloss: 0.54665 |  0:00:40s
epoch 81 | loss: 0.1583  | eval_custom_logloss: 0.48949 |  0:00:40s
epoch 82 | loss: 0.18088 | eval_custom_logloss: 0.51353 |  0:00:41s
epoch 83 | loss: 0.20439 | eval_custom_logloss: 0.50849 |  0:00:41s
epoch 84 | loss: 0.16798 | eval_custom_logloss: 0.52413 |  0:00:42s
epoch 85 | loss: 0.16944 | eval_custom_logloss: 0.43837 |  0:00:42s
epoch 86 | loss: 0.19057 | eval_custom_logloss: 0.65523 |  0:00:43s
epoch 87 | loss: 0.20462 | eval_custom_logloss: 0.59452 |  0:00:43s
epoch 88 | loss: 0.20044 | eval_custom_logloss: 0.50773 |  0:00:44s
epoch 89 | loss: 0.17538 | eval_custom_logloss: 0.64487 |  0:00:44s
epoch 90 | loss: 0.16791 | eval_custom_logloss: 0.53783 |  0:00:45s
epoch 91 | loss: 0.1672  | eval_custom_logloss: 0.56706 |  0:00:45s
epoch 92 | loss: 0.15289 | eval_custom_logloss: 0.64383 |  0:00:46s
epoch 93 | loss: 0.15723 | eval_custom_logloss: 0.67007 |  0:00:46s
epoch 94 | loss: 0.17889 | eval_custom_logloss: 0.68052 |  0:00:47s
epoch 95 | loss: 0.14916 | eval_custom_logloss: 0.66404 |  0:00:47s
epoch 96 | loss: 0.1459  | eval_custom_logloss: 0.73573 |  0:00:48s
epoch 97 | loss: 0.21438 | eval_custom_logloss: 0.6264  |  0:00:48s
epoch 98 | loss: 0.22504 | eval_custom_logloss: 0.71045 |  0:00:49s
epoch 99 | loss: 0.23564 | eval_custom_logloss: 0.57834 |  0:00:49s
Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_eval_custom_logloss = 0.43837
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40700000000000003, 'Log Loss - std': 0.03140000000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.27632 | eval_custom_logloss: 0.84531 |  0:00:00s
epoch 1  | loss: 0.99573 | eval_custom_logloss: 0.84777 |  0:00:01s
epoch 2  | loss: 0.94131 | eval_custom_logloss: 0.80523 |  0:00:01s
epoch 3  | loss: 0.85721 | eval_custom_logloss: 0.91693 |  0:00:02s
epoch 4  | loss: 0.79187 | eval_custom_logloss: 0.70535 |  0:00:02s
epoch 5  | loss: 0.78722 | eval_custom_logloss: 0.83623 |  0:00:03s
epoch 6  | loss: 0.74338 | eval_custom_logloss: 0.66573 |  0:00:03s
epoch 7  | loss: 0.68751 | eval_custom_logloss: 0.63034 |  0:00:04s
epoch 8  | loss: 0.66816 | eval_custom_logloss: 0.67497 |  0:00:04s
epoch 9  | loss: 0.64746 | eval_custom_logloss: 0.67077 |  0:00:05s
epoch 10 | loss: 0.63473 | eval_custom_logloss: 0.61973 |  0:00:05s
epoch 11 | loss: 0.63875 | eval_custom_logloss: 0.57544 |  0:00:06s
epoch 12 | loss: 0.59911 | eval_custom_logloss: 0.53544 |  0:00:06s
epoch 13 | loss: 0.57547 | eval_custom_logloss: 0.5535  |  0:00:07s
epoch 14 | loss: 0.53754 | eval_custom_logloss: 0.5271  |  0:00:07s
epoch 15 | loss: 0.53256 | eval_custom_logloss: 0.5703  |  0:00:08s
epoch 16 | loss: 0.5037  | eval_custom_logloss: 0.53936 |  0:00:08s
epoch 17 | loss: 0.48741 | eval_custom_logloss: 0.56455 |  0:00:09s
epoch 18 | loss: 0.50956 | eval_custom_logloss: 0.5094  |  0:00:09s
epoch 19 | loss: 0.49968 | eval_custom_logloss: 0.52356 |  0:00:10s
epoch 20 | loss: 0.51806 | eval_custom_logloss: 0.52325 |  0:00:10s
epoch 21 | loss: 0.50228 | eval_custom_logloss: 0.50179 |  0:00:11s
epoch 22 | loss: 0.48949 | eval_custom_logloss: 0.4994  |  0:00:11s
epoch 23 | loss: 0.49418 | eval_custom_logloss: 0.4804  |  0:00:12s
epoch 24 | loss: 0.47378 | eval_custom_logloss: 0.47608 |  0:00:12s
epoch 25 | loss: 0.45404 | eval_custom_logloss: 0.47331 |  0:00:13s
epoch 26 | loss: 0.46195 | eval_custom_logloss: 0.49768 |  0:00:13s
epoch 27 | loss: 0.44647 | eval_custom_logloss: 0.4641  |  0:00:14s
epoch 28 | loss: 0.45689 | eval_custom_logloss: 0.49914 |  0:00:14s
epoch 29 | loss: 0.44623 | eval_custom_logloss: 0.51914 |  0:00:15s
epoch 30 | loss: 0.4112  | eval_custom_logloss: 0.50319 |  0:00:15s
epoch 31 | loss: 0.4134  | eval_custom_logloss: 0.44541 |  0:00:16s
epoch 32 | loss: 0.43621 | eval_custom_logloss: 0.46624 |  0:00:16s
epoch 33 | loss: 0.44947 | eval_custom_logloss: 0.44882 |  0:00:17s
epoch 34 | loss: 0.43426 | eval_custom_logloss: 0.46068 |  0:00:17s
epoch 35 | loss: 0.41687 | eval_custom_logloss: 0.43474 |  0:00:18s
epoch 36 | loss: 0.41392 | eval_custom_logloss: 0.4792  |  0:00:18s
epoch 37 | loss: 0.43413 | eval_custom_logloss: 0.43885 |  0:00:19s
epoch 38 | loss: 0.42061 | eval_custom_logloss: 0.48374 |  0:00:19s
epoch 39 | loss: 0.39835 | eval_custom_logloss: 0.45532 |  0:00:20s
epoch 40 | loss: 0.40237 | eval_custom_logloss: 0.4439  |  0:00:20s
epoch 41 | loss: 0.41944 | eval_custom_logloss: 0.43203 |  0:00:21s
epoch 42 | loss: 0.4225  | eval_custom_logloss: 0.47469 |  0:00:21s
epoch 43 | loss: 0.3867  | eval_custom_logloss: 0.45199 |  0:00:22s
epoch 44 | loss: 0.39435 | eval_custom_logloss: 0.46046 |  0:00:22s
epoch 45 | loss: 0.41421 | eval_custom_logloss: 0.4605  |  0:00:23s
epoch 46 | loss: 0.39285 | eval_custom_logloss: 0.42667 |  0:00:23s
epoch 47 | loss: 0.3706  | eval_custom_logloss: 0.43384 |  0:00:24s
epoch 48 | loss: 0.3837  | eval_custom_logloss: 0.44125 |  0:00:24s
epoch 49 | loss: 0.37564 | eval_custom_logloss: 0.47423 |  0:00:25s
epoch 50 | loss: 0.39145 | eval_custom_logloss: 0.43532 |  0:00:25s
epoch 51 | loss: 0.41092 | eval_custom_logloss: 0.427   |  0:00:26s
epoch 52 | loss: 0.38458 | eval_custom_logloss: 0.4152  |  0:00:26s
epoch 53 | loss: 0.35909 | eval_custom_logloss: 0.46056 |  0:00:27s
epoch 54 | loss: 0.35775 | eval_custom_logloss: 0.4852  |  0:00:27s
epoch 55 | loss: 0.39054 | eval_custom_logloss: 0.45206 |  0:00:28s
epoch 56 | loss: 0.35421 | eval_custom_logloss: 0.44988 |  0:00:28s
epoch 57 | loss: 0.35126 | eval_custom_logloss: 0.46232 |  0:00:29s
epoch 58 | loss: 0.35383 | eval_custom_logloss: 0.52074 |  0:00:29s
epoch 59 | loss: 0.39769 | eval_custom_logloss: 0.47978 |  0:00:30s
epoch 60 | loss: 0.36986 | eval_custom_logloss: 0.44124 |  0:00:30s
epoch 61 | loss: 0.37509 | eval_custom_logloss: 0.44733 |  0:00:31s
epoch 62 | loss: 0.35745 | eval_custom_logloss: 0.39822 |  0:00:31s
epoch 63 | loss: 0.37785 | eval_custom_logloss: 0.43427 |  0:00:32s
epoch 64 | loss: 0.34256 | eval_custom_logloss: 0.40425 |  0:00:32s
epoch 65 | loss: 0.32947 | eval_custom_logloss: 0.43022 |  0:00:33s
epoch 66 | loss: 0.30765 | eval_custom_logloss: 0.44939 |  0:00:33s
epoch 67 | loss: 0.29067 | eval_custom_logloss: 0.42844 |  0:00:34s
epoch 68 | loss: 0.28114 | eval_custom_logloss: 0.47354 |  0:00:34s
epoch 69 | loss: 0.33047 | eval_custom_logloss: 0.44797 |  0:00:35s
epoch 70 | loss: 0.29019 | eval_custom_logloss: 0.42053 |  0:00:35s
epoch 71 | loss: 0.29619 | eval_custom_logloss: 0.42792 |  0:00:36s
epoch 72 | loss: 0.314   | eval_custom_logloss: 0.52776 |  0:00:36s
epoch 73 | loss: 0.32176 | eval_custom_logloss: 0.48573 |  0:00:37s
epoch 74 | loss: 0.34027 | eval_custom_logloss: 0.46214 |  0:00:37s
epoch 75 | loss: 0.31934 | eval_custom_logloss: 0.39173 |  0:00:38s
epoch 76 | loss: 0.31551 | eval_custom_logloss: 0.39631 |  0:00:38s
epoch 77 | loss: 0.36774 | eval_custom_logloss: 0.44661 |  0:00:39s
epoch 78 | loss: 0.32936 | eval_custom_logloss: 0.37839 |  0:00:39s
epoch 79 | loss: 0.30905 | eval_custom_logloss: 0.38864 |  0:00:40s
epoch 80 | loss: 0.28697 | eval_custom_logloss: 0.39544 |  0:00:40s
epoch 81 | loss: 0.27945 | eval_custom_logloss: 0.43176 |  0:00:41s
epoch 82 | loss: 0.27838 | eval_custom_logloss: 0.4764  |  0:00:41s
epoch 83 | loss: 0.2688  | eval_custom_logloss: 0.44546 |  0:00:42s
epoch 84 | loss: 0.28626 | eval_custom_logloss: 0.36775 |  0:00:42s
epoch 85 | loss: 0.27748 | eval_custom_logloss: 0.36049 |  0:00:43s
epoch 86 | loss: 0.27198 | eval_custom_logloss: 0.3824  |  0:00:43s
epoch 87 | loss: 0.28148 | eval_custom_logloss: 0.37499 |  0:00:44s
epoch 88 | loss: 0.29318 | eval_custom_logloss: 0.39937 |  0:00:44s
epoch 89 | loss: 0.28782 | eval_custom_logloss: 0.3748  |  0:00:45s
epoch 90 | loss: 0.26293 | eval_custom_logloss: 0.34611 |  0:00:45s
epoch 91 | loss: 0.24061 | eval_custom_logloss: 0.38127 |  0:00:46s
epoch 92 | loss: 0.25282 | eval_custom_logloss: 0.37576 |  0:00:46s
epoch 93 | loss: 0.25557 | eval_custom_logloss: 0.37114 |  0:00:47s
epoch 94 | loss: 0.24231 | eval_custom_logloss: 0.38072 |  0:00:47s
epoch 95 | loss: 0.24574 | eval_custom_logloss: 0.39655 |  0:00:48s
epoch 96 | loss: 0.24342 | eval_custom_logloss: 0.43923 |  0:00:48s
epoch 97 | loss: 0.28712 | eval_custom_logloss: 0.39458 |  0:00:49s
epoch 98 | loss: 0.25026 | eval_custom_logloss: 0.44394 |  0:00:49s
epoch 99 | loss: 0.22696 | eval_custom_logloss: 0.40151 |  0:00:50s
Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_eval_custom_logloss = 0.34611
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.38620000000000004, 'Log Loss - std': 0.03902033657808024} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.23525 | eval_custom_logloss: 1.00036 |  0:00:00s
epoch 1  | loss: 0.9572  | eval_custom_logloss: 0.83685 |  0:00:00s
epoch 2  | loss: 0.83469 | eval_custom_logloss: 0.73285 |  0:00:01s
epoch 3  | loss: 0.76837 | eval_custom_logloss: 0.90582 |  0:00:01s
epoch 4  | loss: 0.71209 | eval_custom_logloss: 0.70641 |  0:00:02s
epoch 5  | loss: 0.66948 | eval_custom_logloss: 0.70587 |  0:00:02s
epoch 6  | loss: 0.62716 | eval_custom_logloss: 0.70331 |  0:00:03s
epoch 7  | loss: 0.62484 | eval_custom_logloss: 0.69885 |  0:00:03s
epoch 8  | loss: 0.66994 | eval_custom_logloss: 0.65751 |  0:00:04s
epoch 9  | loss: 0.59038 | eval_custom_logloss: 0.61142 |  0:00:04s
epoch 10 | loss: 0.58181 | eval_custom_logloss: 0.59743 |  0:00:05s
epoch 11 | loss: 0.57468 | eval_custom_logloss: 0.63746 |  0:00:05s
epoch 12 | loss: 0.53546 | eval_custom_logloss: 0.59937 |  0:00:06s
epoch 13 | loss: 0.52399 | eval_custom_logloss: 0.57401 |  0:00:06s
epoch 14 | loss: 0.50973 | eval_custom_logloss: 0.59055 |  0:00:07s
epoch 15 | loss: 0.52149 | eval_custom_logloss: 0.60735 |  0:00:07s
epoch 16 | loss: 0.50045 | eval_custom_logloss: 0.60011 |  0:00:08s
epoch 17 | loss: 0.4982  | eval_custom_logloss: 0.57636 |  0:00:08s
epoch 18 | loss: 0.48591 | eval_custom_logloss: 0.5554  |  0:00:09s
epoch 19 | loss: 0.48634 | eval_custom_logloss: 0.53206 |  0:00:09s
epoch 20 | loss: 0.50189 | eval_custom_logloss: 0.57197 |  0:00:10s
epoch 21 | loss: 0.48337 | eval_custom_logloss: 0.53143 |  0:00:10s
epoch 22 | loss: 0.47403 | eval_custom_logloss: 0.53093 |  0:00:11s
epoch 23 | loss: 0.49586 | eval_custom_logloss: 0.55645 |  0:00:11s
epoch 24 | loss: 0.48449 | eval_custom_logloss: 0.57207 |  0:00:12s
epoch 25 | loss: 0.4568  | eval_custom_logloss: 0.60902 |  0:00:12s
epoch 26 | loss: 0.45671 | eval_custom_logloss: 0.54645 |  0:00:13s
epoch 27 | loss: 0.44568 | eval_custom_logloss: 0.51391 |  0:00:13s
epoch 28 | loss: 0.45469 | eval_custom_logloss: 0.53466 |  0:00:14s
epoch 29 | loss: 0.43181 | eval_custom_logloss: 0.56895 |  0:00:14s
epoch 30 | loss: 0.42338 | eval_custom_logloss: 0.5066  |  0:00:15s
epoch 31 | loss: 0.45606 | eval_custom_logloss: 0.46307 |  0:00:15s
epoch 32 | loss: 0.41849 | eval_custom_logloss: 0.49256 |  0:00:16s
epoch 33 | loss: 0.39147 | eval_custom_logloss: 0.51604 |  0:00:16s
epoch 34 | loss: 0.39789 | eval_custom_logloss: 0.50568 |  0:00:17s
epoch 35 | loss: 0.39835 | eval_custom_logloss: 0.49408 |  0:00:17s
epoch 36 | loss: 0.38073 | eval_custom_logloss: 0.4795  |  0:00:18s
epoch 37 | loss: 0.36274 | eval_custom_logloss: 0.51028 |  0:00:18s
epoch 38 | loss: 0.34917 | eval_custom_logloss: 0.46883 |  0:00:19s
epoch 39 | loss: 0.34555 | eval_custom_logloss: 0.45849 |  0:00:19s
epoch 40 | loss: 0.33142 | eval_custom_logloss: 0.44513 |  0:00:20s
epoch 41 | loss: 0.32953 | eval_custom_logloss: 0.44667 |  0:00:20s
epoch 42 | loss: 0.32379 | eval_custom_logloss: 0.44769 |  0:00:21s
epoch 43 | loss: 0.30358 | eval_custom_logloss: 0.44127 |  0:00:21s
epoch 44 | loss: 0.31004 | eval_custom_logloss: 0.42279 |  0:00:22s
epoch 45 | loss: 0.30112 | eval_custom_logloss: 0.44695 |  0:00:22s
epoch 46 | loss: 0.32627 | eval_custom_logloss: 0.46223 |  0:00:23s
epoch 47 | loss: 0.30373 | eval_custom_logloss: 0.47793 |  0:00:23s
epoch 48 | loss: 0.2661  | eval_custom_logloss: 0.5855  |  0:00:24s
epoch 49 | loss: 0.28832 | eval_custom_logloss: 0.53832 |  0:00:24s
epoch 50 | loss: 0.30716 | eval_custom_logloss: 0.48692 |  0:00:24s
epoch 51 | loss: 0.29042 | eval_custom_logloss: 0.47246 |  0:00:25s
epoch 52 | loss: 0.27809 | eval_custom_logloss: 0.45808 |  0:00:25s
epoch 53 | loss: 0.26903 | eval_custom_logloss: 0.47055 |  0:00:26s
epoch 54 | loss: 0.26749 | eval_custom_logloss: 0.44523 |  0:00:26s
epoch 55 | loss: 0.28616 | eval_custom_logloss: 0.51351 |  0:00:27s
epoch 56 | loss: 0.30251 | eval_custom_logloss: 0.56305 |  0:00:27s
epoch 57 | loss: 0.25361 | eval_custom_logloss: 0.48912 |  0:00:28s
epoch 58 | loss: 0.27019 | eval_custom_logloss: 0.51059 |  0:00:28s
epoch 59 | loss: 0.25814 | eval_custom_logloss: 0.56061 |  0:00:29s
epoch 60 | loss: 0.2666  | eval_custom_logloss: 0.55162 |  0:00:29s
epoch 61 | loss: 0.25377 | eval_custom_logloss: 0.46206 |  0:00:30s
epoch 62 | loss: 0.23383 | eval_custom_logloss: 0.51112 |  0:00:30s
epoch 63 | loss: 0.26936 | eval_custom_logloss: 0.47371 |  0:00:31s
epoch 64 | loss: 0.25877 | eval_custom_logloss: 0.58448 |  0:00:31s

Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_custom_logloss = 0.42279
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.39535000000000003, 'Log Loss - std': 0.037324355319281806} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.23541 | eval_custom_logloss: 0.9304  |  0:00:00s
epoch 1  | loss: 0.91634 | eval_custom_logloss: 0.77333 |  0:00:01s
epoch 2  | loss: 0.89512 | eval_custom_logloss: 0.7535  |  0:00:01s
epoch 3  | loss: 0.85221 | eval_custom_logloss: 0.69175 |  0:00:02s
epoch 4  | loss: 0.83035 | eval_custom_logloss: 0.71163 |  0:00:02s
epoch 5  | loss: 0.78103 | eval_custom_logloss: 0.62811 |  0:00:03s
epoch 6  | loss: 0.73132 | eval_custom_logloss: 0.74778 |  0:00:03s
epoch 7  | loss: 0.73312 | eval_custom_logloss: 0.62755 |  0:00:04s
epoch 8  | loss: 0.78043 | eval_custom_logloss: 0.63228 |  0:00:04s
epoch 9  | loss: 0.69706 | eval_custom_logloss: 0.68614 |  0:00:05s
epoch 10 | loss: 0.65486 | eval_custom_logloss: 0.57901 |  0:00:05s
epoch 11 | loss: 0.64571 | eval_custom_logloss: 0.55259 |  0:00:06s
epoch 12 | loss: 0.6327  | eval_custom_logloss: 0.585   |  0:00:06s
epoch 13 | loss: 0.6083  | eval_custom_logloss: 0.51283 |  0:00:07s
epoch 14 | loss: 0.55404 | eval_custom_logloss: 0.44484 |  0:00:07s
epoch 15 | loss: 0.58406 | eval_custom_logloss: 0.5545  |  0:00:08s
epoch 16 | loss: 0.56819 | eval_custom_logloss: 0.42519 |  0:00:08s
epoch 17 | loss: 0.54697 | eval_custom_logloss: 0.45336 |  0:00:09s
epoch 18 | loss: 0.52686 | eval_custom_logloss: 0.45129 |  0:00:09s
epoch 19 | loss: 0.51934 | eval_custom_logloss: 0.45517 |  0:00:10s
epoch 20 | loss: 0.5286  | eval_custom_logloss: 0.45236 |  0:00:10s
epoch 21 | loss: 0.529   | eval_custom_logloss: 0.46786 |  0:00:11s
epoch 22 | loss: 0.52292 | eval_custom_logloss: 0.4638  |  0:00:11s
epoch 23 | loss: 0.5008  | eval_custom_logloss: 0.4625  |  0:00:12s
epoch 24 | loss: 0.50452 | eval_custom_logloss: 0.4333  |  0:00:12s
epoch 25 | loss: 0.54689 | eval_custom_logloss: 0.5572  |  0:00:13s
epoch 26 | loss: 0.5747  | eval_custom_logloss: 0.57013 |  0:00:13s
epoch 27 | loss: 0.56919 | eval_custom_logloss: 0.52622 |  0:00:14s
epoch 28 | loss: 0.52737 | eval_custom_logloss: 0.46296 |  0:00:14s
epoch 29 | loss: 0.48157 | eval_custom_logloss: 0.40915 |  0:00:15s
epoch 30 | loss: 0.48137 | eval_custom_logloss: 0.40864 |  0:00:15s
epoch 31 | loss: 0.46073 | eval_custom_logloss: 0.40588 |  0:00:16s
epoch 32 | loss: 0.45197 | eval_custom_logloss: 0.40905 |  0:00:16s
epoch 33 | loss: 0.43045 | eval_custom_logloss: 0.39283 |  0:00:17s
epoch 34 | loss: 0.43461 | eval_custom_logloss: 0.40552 |  0:00:17s
epoch 35 | loss: 0.42913 | eval_custom_logloss: 0.43018 |  0:00:18s
epoch 36 | loss: 0.43052 | eval_custom_logloss: 0.44093 |  0:00:18s
epoch 37 | loss: 0.40609 | eval_custom_logloss: 0.38582 |  0:00:19s
epoch 38 | loss: 0.39358 | eval_custom_logloss: 0.3533  |  0:00:19s
epoch 39 | loss: 0.39352 | eval_custom_logloss: 0.37296 |  0:00:20s
epoch 40 | loss: 0.3912  | eval_custom_logloss: 0.41657 |  0:00:20s
epoch 41 | loss: 0.39823 | eval_custom_logloss: 0.40133 |  0:00:21s
epoch 42 | loss: 0.39509 | eval_custom_logloss: 0.38819 |  0:00:21s
epoch 43 | loss: 0.38157 | eval_custom_logloss: 0.35248 |  0:00:22s
epoch 44 | loss: 0.38819 | eval_custom_logloss: 0.35333 |  0:00:22s
epoch 45 | loss: 0.38528 | eval_custom_logloss: 0.43367 |  0:00:23s
epoch 46 | loss: 0.41659 | eval_custom_logloss: 0.38698 |  0:00:23s
epoch 47 | loss: 0.38221 | eval_custom_logloss: 0.34268 |  0:00:24s
epoch 48 | loss: 0.37958 | eval_custom_logloss: 0.34091 |  0:00:24s
epoch 49 | loss: 0.39391 | eval_custom_logloss: 0.4323  |  0:00:25s
epoch 50 | loss: 0.37904 | eval_custom_logloss: 0.42508 |  0:00:25s
epoch 51 | loss: 0.39987 | eval_custom_logloss: 0.45317 |  0:00:26s
epoch 52 | loss: 0.40152 | eval_custom_logloss: 0.38509 |  0:00:26s
epoch 53 | loss: 0.39023 | eval_custom_logloss: 0.36622 |  0:00:27s
epoch 54 | loss: 0.37053 | eval_custom_logloss: 0.39623 |  0:00:27s
epoch 55 | loss: 0.36533 | eval_custom_logloss: 0.4075  |  0:00:28s
epoch 56 | loss: 0.39123 | eval_custom_logloss: 0.42243 |  0:00:28s
epoch 57 | loss: 0.37483 | eval_custom_logloss: 0.39431 |  0:00:29s
epoch 58 | loss: 0.35365 | eval_custom_logloss: 0.40315 |  0:00:29s
epoch 59 | loss: 0.33621 | eval_custom_logloss: 0.36728 |  0:00:30s
epoch 60 | loss: 0.33733 | eval_custom_logloss: 0.37534 |  0:00:30s
epoch 61 | loss: 0.3308  | eval_custom_logloss: 0.3518  |  0:00:31s
epoch 62 | loss: 0.34721 | eval_custom_logloss: 0.41975 |  0:00:31s
epoch 63 | loss: 0.37621 | eval_custom_logloss: 0.38046 |  0:00:32s
epoch 64 | loss: 0.36344 | eval_custom_logloss: 0.36357 |  0:00:32s
epoch 65 | loss: 0.3511  | eval_custom_logloss: 0.37414 |  0:00:33s
epoch 66 | loss: 0.30945 | eval_custom_logloss: 0.38169 |  0:00:33s
epoch 67 | loss: 0.34896 | eval_custom_logloss: 0.38714 |  0:00:34s
epoch 68 | loss: 0.33869 | eval_custom_logloss: 0.40216 |  0:00:34s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 0.34091
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.38446, 'Log Loss - std': 0.03986043652545718} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 0.38446 and parameters: {'n_d': 20, 'n_steps': 5, 'gamma': 1.062575215616779, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.13195415509886038, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.38446.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.16451 | eval_custom_logloss: 0.95086 |  0:00:00s
epoch 1  | loss: 0.87761 | eval_custom_logloss: 0.93672 |  0:00:00s
epoch 2  | loss: 0.74554 | eval_custom_logloss: 0.8512  |  0:00:01s
epoch 3  | loss: 0.74674 | eval_custom_logloss: 0.67102 |  0:00:01s
epoch 4  | loss: 0.72082 | eval_custom_logloss: 0.62107 |  0:00:01s
epoch 5  | loss: 0.66165 | eval_custom_logloss: 0.66329 |  0:00:02s
epoch 6  | loss: 0.65237 | eval_custom_logloss: 0.66175 |  0:00:02s
epoch 7  | loss: 0.59346 | eval_custom_logloss: 0.57857 |  0:00:03s
epoch 8  | loss: 0.54969 | eval_custom_logloss: 0.61694 |  0:00:03s
epoch 9  | loss: 0.58335 | eval_custom_logloss: 0.53497 |  0:00:04s
epoch 10 | loss: 0.53811 | eval_custom_logloss: 0.48996 |  0:00:04s
epoch 11 | loss: 0.54301 | eval_custom_logloss: 0.54921 |  0:00:04s
epoch 12 | loss: 0.50423 | eval_custom_logloss: 0.54164 |  0:00:05s
epoch 13 | loss: 0.4957  | eval_custom_logloss: 0.51901 |  0:00:05s
epoch 14 | loss: 0.44911 | eval_custom_logloss: 0.54671 |  0:00:06s
epoch 15 | loss: 0.48917 | eval_custom_logloss: 0.48122 |  0:00:06s
epoch 16 | loss: 0.45873 | eval_custom_logloss: 0.50214 |  0:00:07s
epoch 17 | loss: 0.41867 | eval_custom_logloss: 0.49356 |  0:00:07s
epoch 18 | loss: 0.40018 | eval_custom_logloss: 0.51628 |  0:00:08s
epoch 19 | loss: 0.40358 | eval_custom_logloss: 0.47292 |  0:00:08s
epoch 20 | loss: 0.39992 | eval_custom_logloss: 0.4576  |  0:00:08s
epoch 21 | loss: 0.38913 | eval_custom_logloss: 0.49956 |  0:00:09s
epoch 22 | loss: 0.35251 | eval_custom_logloss: 0.58034 |  0:00:09s
epoch 23 | loss: 0.39466 | eval_custom_logloss: 0.43086 |  0:00:10s
epoch 24 | loss: 0.36716 | eval_custom_logloss: 0.48941 |  0:00:10s
epoch 25 | loss: 0.39253 | eval_custom_logloss: 0.57648 |  0:00:11s
epoch 26 | loss: 0.38901 | eval_custom_logloss: 0.45823 |  0:00:11s
epoch 27 | loss: 0.38601 | eval_custom_logloss: 0.43435 |  0:00:12s
epoch 28 | loss: 0.3577  | eval_custom_logloss: 0.39995 |  0:00:12s
epoch 29 | loss: 0.3302  | eval_custom_logloss: 0.46202 |  0:00:12s
epoch 30 | loss: 0.31698 | eval_custom_logloss: 0.40631 |  0:00:13s
epoch 31 | loss: 0.31051 | eval_custom_logloss: 0.39144 |  0:00:13s
epoch 32 | loss: 0.32667 | eval_custom_logloss: 0.41015 |  0:00:14s
epoch 33 | loss: 0.29165 | eval_custom_logloss: 0.43392 |  0:00:14s
epoch 34 | loss: 0.2833  | eval_custom_logloss: 0.43796 |  0:00:15s
epoch 35 | loss: 0.28644 | eval_custom_logloss: 0.41795 |  0:00:15s
epoch 36 | loss: 0.28696 | eval_custom_logloss: 0.4189  |  0:00:16s
epoch 37 | loss: 0.26203 | eval_custom_logloss: 0.38782 |  0:00:16s
epoch 38 | loss: 0.27668 | eval_custom_logloss: 0.40357 |  0:00:16s
epoch 39 | loss: 0.26727 | eval_custom_logloss: 0.38816 |  0:00:17s
epoch 40 | loss: 0.27325 | eval_custom_logloss: 0.3953  |  0:00:17s
epoch 41 | loss: 0.25133 | eval_custom_logloss: 0.43322 |  0:00:18s
epoch 42 | loss: 0.24462 | eval_custom_logloss: 0.37309 |  0:00:18s
epoch 43 | loss: 0.23113 | eval_custom_logloss: 0.42964 |  0:00:19s
epoch 44 | loss: 0.21439 | eval_custom_logloss: 0.44114 |  0:00:19s
epoch 45 | loss: 0.27583 | eval_custom_logloss: 0.43961 |  0:00:19s
epoch 46 | loss: 0.25521 | eval_custom_logloss: 0.39303 |  0:00:20s
epoch 47 | loss: 0.24463 | eval_custom_logloss: 0.4043  |  0:00:20s
epoch 48 | loss: 0.23071 | eval_custom_logloss: 0.37073 |  0:00:20s
epoch 49 | loss: 0.20411 | eval_custom_logloss: 0.50286 |  0:00:21s
epoch 50 | loss: 0.23749 | eval_custom_logloss: 0.42058 |  0:00:21s
epoch 51 | loss: 0.2301  | eval_custom_logloss: 0.41065 |  0:00:22s
epoch 52 | loss: 0.22183 | eval_custom_logloss: 0.43989 |  0:00:22s
epoch 53 | loss: 0.21533 | eval_custom_logloss: 0.44611 |  0:00:23s
epoch 54 | loss: 0.1885  | eval_custom_logloss: 0.43067 |  0:00:23s
epoch 55 | loss: 0.18731 | eval_custom_logloss: 0.45828 |  0:00:24s
epoch 56 | loss: 0.17888 | eval_custom_logloss: 0.43367 |  0:00:24s
epoch 57 | loss: 0.1834  | eval_custom_logloss: 0.45535 |  0:00:24s
epoch 58 | loss: 0.18651 | eval_custom_logloss: 0.44269 |  0:00:25s
epoch 59 | loss: 0.22933 | eval_custom_logloss: 0.41272 |  0:00:25s
epoch 60 | loss: 0.22595 | eval_custom_logloss: 0.50133 |  0:00:26s
epoch 61 | loss: 0.21429 | eval_custom_logloss: 0.44853 |  0:00:26s
epoch 62 | loss: 0.18975 | eval_custom_logloss: 0.41846 |  0:00:27s
epoch 63 | loss: 0.1917  | eval_custom_logloss: 0.42807 |  0:00:27s
epoch 64 | loss: 0.19408 | eval_custom_logloss: 0.41077 |  0:00:27s
epoch 65 | loss: 0.15917 | eval_custom_logloss: 0.42203 |  0:00:28s
epoch 66 | loss: 0.19662 | eval_custom_logloss: 0.50602 |  0:00:28s
epoch 67 | loss: 0.18861 | eval_custom_logloss: 0.46974 |  0:00:29s
epoch 68 | loss: 0.19791 | eval_custom_logloss: 0.44481 |  0:00:29s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 0.37073
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3707, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.1381  | eval_custom_logloss: 0.93089 |  0:00:00s
epoch 1  | loss: 0.97585 | eval_custom_logloss: 0.92909 |  0:00:00s
epoch 2  | loss: 0.78359 | eval_custom_logloss: 0.93124 |  0:00:01s
epoch 3  | loss: 0.69152 | eval_custom_logloss: 0.76    |  0:00:01s
epoch 4  | loss: 0.61781 | eval_custom_logloss: 0.75572 |  0:00:01s
epoch 5  | loss: 0.64563 | eval_custom_logloss: 0.74974 |  0:00:02s
epoch 6  | loss: 0.59784 | eval_custom_logloss: 0.70571 |  0:00:02s
epoch 7  | loss: 0.5596  | eval_custom_logloss: 0.7503  |  0:00:03s
epoch 8  | loss: 0.53168 | eval_custom_logloss: 0.68437 |  0:00:03s
epoch 9  | loss: 0.50697 | eval_custom_logloss: 0.69094 |  0:00:03s
epoch 10 | loss: 0.45872 | eval_custom_logloss: 0.67166 |  0:00:04s
epoch 11 | loss: 0.44321 | eval_custom_logloss: 0.68981 |  0:00:04s
epoch 12 | loss: 0.40488 | eval_custom_logloss: 0.72744 |  0:00:05s
epoch 13 | loss: 0.37964 | eval_custom_logloss: 0.63062 |  0:00:05s
epoch 14 | loss: 0.39836 | eval_custom_logloss: 0.66346 |  0:00:05s
epoch 15 | loss: 0.38589 | eval_custom_logloss: 0.6027  |  0:00:06s
epoch 16 | loss: 0.35446 | eval_custom_logloss: 0.63211 |  0:00:06s
epoch 17 | loss: 0.36162 | eval_custom_logloss: 0.58596 |  0:00:06s
epoch 18 | loss: 0.38789 | eval_custom_logloss: 0.5955  |  0:00:07s
epoch 19 | loss: 0.34098 | eval_custom_logloss: 0.55245 |  0:00:07s
epoch 20 | loss: 0.34533 | eval_custom_logloss: 0.54142 |  0:00:08s
epoch 21 | loss: 0.32972 | eval_custom_logloss: 0.56647 |  0:00:08s
epoch 22 | loss: 0.29764 | eval_custom_logloss: 0.64075 |  0:00:08s
epoch 23 | loss: 0.30192 | eval_custom_logloss: 0.58288 |  0:00:09s
epoch 24 | loss: 0.30663 | eval_custom_logloss: 0.65796 |  0:00:09s
epoch 25 | loss: 0.31432 | eval_custom_logloss: 0.55857 |  0:00:09s
epoch 26 | loss: 0.27309 | eval_custom_logloss: 0.59204 |  0:00:10s
epoch 27 | loss: 0.27279 | eval_custom_logloss: 0.60245 |  0:00:10s
epoch 28 | loss: 0.28281 | eval_custom_logloss: 0.56224 |  0:00:11s
epoch 29 | loss: 0.27705 | eval_custom_logloss: 0.54138 |  0:00:11s
epoch 30 | loss: 0.27646 | eval_custom_logloss: 0.54926 |  0:00:11s
epoch 31 | loss: 0.26552 | eval_custom_logloss: 0.56376 |  0:00:12s
epoch 32 | loss: 0.24765 | eval_custom_logloss: 0.5365  |  0:00:12s
epoch 33 | loss: 0.23783 | eval_custom_logloss: 0.71384 |  0:00:13s
epoch 34 | loss: 0.24465 | eval_custom_logloss: 0.56954 |  0:00:13s
epoch 35 | loss: 0.24879 | eval_custom_logloss: 0.59629 |  0:00:13s
epoch 36 | loss: 0.25256 | eval_custom_logloss: 0.58909 |  0:00:14s
epoch 37 | loss: 0.24466 | eval_custom_logloss: 0.51786 |  0:00:14s
epoch 38 | loss: 0.24069 | eval_custom_logloss: 0.56247 |  0:00:14s
epoch 39 | loss: 0.24642 | eval_custom_logloss: 0.47981 |  0:00:15s
epoch 40 | loss: 0.23974 | eval_custom_logloss: 0.55106 |  0:00:15s
epoch 41 | loss: 0.2258  | eval_custom_logloss: 0.65592 |  0:00:16s
epoch 42 | loss: 0.24493 | eval_custom_logloss: 0.53669 |  0:00:16s
epoch 43 | loss: 0.23476 | eval_custom_logloss: 0.48895 |  0:00:16s
epoch 44 | loss: 0.23752 | eval_custom_logloss: 0.43891 |  0:00:17s
epoch 45 | loss: 0.21825 | eval_custom_logloss: 0.57011 |  0:00:17s
epoch 46 | loss: 0.20521 | eval_custom_logloss: 0.56984 |  0:00:18s
epoch 47 | loss: 0.21406 | eval_custom_logloss: 0.58762 |  0:00:18s
epoch 48 | loss: 0.22564 | eval_custom_logloss: 0.57883 |  0:00:18s
epoch 49 | loss: 0.19504 | eval_custom_logloss: 0.62522 |  0:00:19s
epoch 50 | loss: 0.20746 | eval_custom_logloss: 0.51687 |  0:00:19s
epoch 51 | loss: 0.21075 | eval_custom_logloss: 0.48082 |  0:00:19s
epoch 52 | loss: 0.17464 | eval_custom_logloss: 0.6547  |  0:00:20s
epoch 53 | loss: 0.16972 | eval_custom_logloss: 0.66993 |  0:00:20s
epoch 54 | loss: 0.18286 | eval_custom_logloss: 0.58554 |  0:00:21s
epoch 55 | loss: 0.1744  | eval_custom_logloss: 0.57581 |  0:00:21s
epoch 56 | loss: 0.18192 | eval_custom_logloss: 0.69322 |  0:00:21s
epoch 57 | loss: 0.20109 | eval_custom_logloss: 0.63271 |  0:00:22s
epoch 58 | loss: 0.17434 | eval_custom_logloss: 0.71143 |  0:00:22s
epoch 59 | loss: 0.2107  | eval_custom_logloss: 0.65809 |  0:00:23s
epoch 60 | loss: 0.21835 | eval_custom_logloss: 0.71128 |  0:00:23s
epoch 61 | loss: 0.19722 | eval_custom_logloss: 0.65871 |  0:00:23s
epoch 62 | loss: 0.19189 | eval_custom_logloss: 0.54097 |  0:00:24s
epoch 63 | loss: 0.17639 | eval_custom_logloss: 0.60598 |  0:00:24s
epoch 64 | loss: 0.17856 | eval_custom_logloss: 0.60822 |  0:00:25s

Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_custom_logloss = 0.43891
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3991, 'Log Loss - std': 0.02840000000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.17106 | eval_custom_logloss: 1.3281  |  0:00:00s
epoch 1  | loss: 0.97112 | eval_custom_logloss: 1.24326 |  0:00:00s
epoch 2  | loss: 0.82192 | eval_custom_logloss: 0.71772 |  0:00:01s
epoch 3  | loss: 0.82133 | eval_custom_logloss: 0.78614 |  0:00:01s
epoch 4  | loss: 0.78158 | eval_custom_logloss: 0.72583 |  0:00:01s
epoch 5  | loss: 0.69434 | eval_custom_logloss: 0.6687  |  0:00:01s
epoch 6  | loss: 0.63322 | eval_custom_logloss: 0.66043 |  0:00:02s
epoch 7  | loss: 0.60761 | eval_custom_logloss: 0.69031 |  0:00:02s
epoch 8  | loss: 0.57726 | eval_custom_logloss: 0.60696 |  0:00:02s
epoch 9  | loss: 0.59233 | eval_custom_logloss: 0.60737 |  0:00:03s
epoch 10 | loss: 0.59802 | eval_custom_logloss: 0.57311 |  0:00:03s
epoch 11 | loss: 0.56442 | eval_custom_logloss: 0.58087 |  0:00:03s
epoch 12 | loss: 0.52665 | eval_custom_logloss: 0.59343 |  0:00:04s
epoch 13 | loss: 0.53646 | eval_custom_logloss: 0.55419 |  0:00:04s
epoch 14 | loss: 0.54457 | eval_custom_logloss: 0.6123  |  0:00:04s
epoch 15 | loss: 0.52513 | eval_custom_logloss: 0.57964 |  0:00:05s
epoch 16 | loss: 0.51328 | eval_custom_logloss: 0.56166 |  0:00:05s
epoch 17 | loss: 0.56043 | eval_custom_logloss: 0.54163 |  0:00:05s
epoch 18 | loss: 0.51492 | eval_custom_logloss: 0.59469 |  0:00:06s
epoch 19 | loss: 0.4961  | eval_custom_logloss: 0.54946 |  0:00:06s
epoch 20 | loss: 0.52389 | eval_custom_logloss: 0.54634 |  0:00:06s
epoch 21 | loss: 0.53448 | eval_custom_logloss: 0.57966 |  0:00:07s
epoch 22 | loss: 0.5105  | eval_custom_logloss: 0.57405 |  0:00:07s
epoch 23 | loss: 0.4925  | eval_custom_logloss: 0.53078 |  0:00:07s
epoch 24 | loss: 0.50342 | eval_custom_logloss: 0.51133 |  0:00:08s
epoch 25 | loss: 0.50095 | eval_custom_logloss: 0.46934 |  0:00:08s
epoch 26 | loss: 0.50108 | eval_custom_logloss: 0.50296 |  0:00:08s
epoch 27 | loss: 0.47392 | eval_custom_logloss: 0.46063 |  0:00:09s
epoch 28 | loss: 0.45011 | eval_custom_logloss: 0.45406 |  0:00:09s
epoch 29 | loss: 0.44089 | eval_custom_logloss: 0.4603  |  0:00:09s
epoch 30 | loss: 0.45123 | eval_custom_logloss: 0.45566 |  0:00:10s
epoch 31 | loss: 0.43108 | eval_custom_logloss: 0.48093 |  0:00:10s
epoch 32 | loss: 0.44216 | eval_custom_logloss: 0.48315 |  0:00:10s
epoch 33 | loss: 0.44644 | eval_custom_logloss: 0.47814 |  0:00:11s
epoch 34 | loss: 0.43455 | eval_custom_logloss: 0.49413 |  0:00:11s
epoch 35 | loss: 0.42911 | eval_custom_logloss: 0.45153 |  0:00:11s
epoch 36 | loss: 0.4385  | eval_custom_logloss: 0.45027 |  0:00:12s
epoch 37 | loss: 0.42663 | eval_custom_logloss: 0.50107 |  0:00:12s
epoch 38 | loss: 0.43093 | eval_custom_logloss: 0.42784 |  0:00:12s
epoch 39 | loss: 0.41849 | eval_custom_logloss: 0.45264 |  0:00:13s
epoch 40 | loss: 0.38611 | eval_custom_logloss: 0.4479  |  0:00:13s
epoch 41 | loss: 0.37307 | eval_custom_logloss: 0.42722 |  0:00:13s
epoch 42 | loss: 0.36399 | eval_custom_logloss: 0.44201 |  0:00:14s
epoch 43 | loss: 0.37221 | eval_custom_logloss: 0.44266 |  0:00:14s
epoch 44 | loss: 0.40179 | eval_custom_logloss: 0.4161  |  0:00:14s
epoch 45 | loss: 0.37478 | eval_custom_logloss: 0.44865 |  0:00:15s
epoch 46 | loss: 0.39586 | eval_custom_logloss: 0.46388 |  0:00:15s
epoch 47 | loss: 0.39968 | eval_custom_logloss: 0.48682 |  0:00:15s
epoch 48 | loss: 0.40946 | eval_custom_logloss: 0.53974 |  0:00:16s
epoch 49 | loss: 0.38797 | eval_custom_logloss: 0.51619 |  0:00:16s
epoch 50 | loss: 0.38674 | eval_custom_logloss: 0.616   |  0:00:16s
epoch 51 | loss: 0.36536 | eval_custom_logloss: 0.5066  |  0:00:17s
epoch 52 | loss: 0.33992 | eval_custom_logloss: 0.48955 |  0:00:17s
epoch 53 | loss: 0.36844 | eval_custom_logloss: 0.45312 |  0:00:17s
epoch 54 | loss: 0.3812  | eval_custom_logloss: 0.47233 |  0:00:18s
epoch 55 | loss: 0.35924 | eval_custom_logloss: 0.50037 |  0:00:18s
epoch 56 | loss: 0.36306 | eval_custom_logloss: 0.45896 |  0:00:18s
epoch 57 | loss: 0.33907 | eval_custom_logloss: 0.45786 |  0:00:19s
epoch 58 | loss: 0.32276 | eval_custom_logloss: 0.49163 |  0:00:19s
epoch 59 | loss: 0.32411 | eval_custom_logloss: 0.42901 |  0:00:19s
epoch 60 | loss: 0.30688 | eval_custom_logloss: 0.40067 |  0:00:20s
epoch 61 | loss: 0.31912 | eval_custom_logloss: 0.40255 |  0:00:20s
epoch 62 | loss: 0.32461 | eval_custom_logloss: 0.40356 |  0:00:20s
epoch 63 | loss: 0.3085  | eval_custom_logloss: 0.51236 |  0:00:21s
epoch 64 | loss: 0.32337 | eval_custom_logloss: 0.53106 |  0:00:21s
epoch 65 | loss: 0.30501 | eval_custom_logloss: 0.39823 |  0:00:21s
epoch 66 | loss: 0.28526 | eval_custom_logloss: 0.42529 |  0:00:22s
epoch 67 | loss: 0.28775 | eval_custom_logloss: 0.4239  |  0:00:22s
epoch 68 | loss: 0.29261 | eval_custom_logloss: 0.45349 |  0:00:23s
epoch 69 | loss: 0.29835 | eval_custom_logloss: 0.44413 |  0:00:23s
epoch 70 | loss: 0.29545 | eval_custom_logloss: 0.4374  |  0:00:23s
epoch 71 | loss: 0.29212 | eval_custom_logloss: 0.45501 |  0:00:24s
epoch 72 | loss: 0.30411 | eval_custom_logloss: 0.49803 |  0:00:24s
epoch 73 | loss: 0.35923 | eval_custom_logloss: 0.51896 |  0:00:24s
epoch 74 | loss: 0.37867 | eval_custom_logloss: 0.52088 |  0:00:25s
epoch 75 | loss: 0.3574  | eval_custom_logloss: 0.46564 |  0:00:25s
epoch 76 | loss: 0.31267 | eval_custom_logloss: 0.39906 |  0:00:26s
epoch 77 | loss: 0.28264 | eval_custom_logloss: 0.37981 |  0:00:26s
epoch 78 | loss: 0.2816  | eval_custom_logloss: 0.39985 |  0:00:26s
epoch 79 | loss: 0.27495 | eval_custom_logloss: 0.46233 |  0:00:27s
epoch 80 | loss: 0.27629 | eval_custom_logloss: 0.53412 |  0:00:27s
epoch 81 | loss: 0.27531 | eval_custom_logloss: 0.41804 |  0:00:28s
epoch 82 | loss: 0.26789 | eval_custom_logloss: 0.39887 |  0:00:28s
epoch 83 | loss: 0.26516 | eval_custom_logloss: 0.4368  |  0:00:28s
epoch 84 | loss: 0.25243 | eval_custom_logloss: 0.45248 |  0:00:29s
epoch 85 | loss: 0.22852 | eval_custom_logloss: 0.45981 |  0:00:29s
epoch 86 | loss: 0.23445 | eval_custom_logloss: 0.47828 |  0:00:29s
epoch 87 | loss: 0.28078 | eval_custom_logloss: 0.46368 |  0:00:30s
epoch 88 | loss: 0.29004 | eval_custom_logloss: 0.47376 |  0:00:30s
epoch 89 | loss: 0.27798 | eval_custom_logloss: 0.49773 |  0:00:30s
epoch 90 | loss: 0.24456 | eval_custom_logloss: 0.44312 |  0:00:31s
epoch 91 | loss: 0.26814 | eval_custom_logloss: 0.48211 |  0:00:31s
epoch 92 | loss: 0.24696 | eval_custom_logloss: 0.48206 |  0:00:32s
epoch 93 | loss: 0.23743 | eval_custom_logloss: 0.51679 |  0:00:32s
epoch 94 | loss: 0.24766 | eval_custom_logloss: 0.50906 |  0:00:32s
epoch 95 | loss: 0.27603 | eval_custom_logloss: 0.45026 |  0:00:33s
epoch 96 | loss: 0.23901 | eval_custom_logloss: 0.4221  |  0:00:33s
epoch 97 | loss: 0.21849 | eval_custom_logloss: 0.44818 |  0:00:33s

Early stopping occurred at epoch 97 with best_epoch = 77 and best_eval_custom_logloss = 0.37981
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.39266666666666666, 'Log Loss - std': 0.024909480569097024} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.15638 | eval_custom_logloss: 1.01702 |  0:00:00s
epoch 1  | loss: 0.9114  | eval_custom_logloss: 1.04463 |  0:00:00s
epoch 2  | loss: 0.72911 | eval_custom_logloss: 0.79921 |  0:00:01s
epoch 3  | loss: 0.72287 | eval_custom_logloss: 0.77731 |  0:00:01s
epoch 4  | loss: 0.67559 | eval_custom_logloss: 0.79315 |  0:00:01s
epoch 5  | loss: 0.71177 | eval_custom_logloss: 0.77919 |  0:00:02s
epoch 6  | loss: 0.60983 | eval_custom_logloss: 0.63837 |  0:00:02s
epoch 7  | loss: 0.58414 | eval_custom_logloss: 0.66548 |  0:00:02s
epoch 8  | loss: 0.53898 | eval_custom_logloss: 0.64263 |  0:00:03s
epoch 9  | loss: 0.52216 | eval_custom_logloss: 0.56558 |  0:00:03s
epoch 10 | loss: 0.5013  | eval_custom_logloss: 0.59959 |  0:00:03s
epoch 11 | loss: 0.47505 | eval_custom_logloss: 0.57323 |  0:00:04s
epoch 12 | loss: 0.48133 | eval_custom_logloss: 0.6376  |  0:00:04s
epoch 13 | loss: 0.47895 | eval_custom_logloss: 0.53232 |  0:00:04s
epoch 14 | loss: 0.47127 | eval_custom_logloss: 0.54766 |  0:00:05s
epoch 15 | loss: 0.4336  | eval_custom_logloss: 0.56233 |  0:00:05s
epoch 16 | loss: 0.39873 | eval_custom_logloss: 0.57498 |  0:00:06s
epoch 17 | loss: 0.42467 | eval_custom_logloss: 0.53854 |  0:00:06s
epoch 18 | loss: 0.41153 | eval_custom_logloss: 0.51741 |  0:00:06s
epoch 19 | loss: 0.39733 | eval_custom_logloss: 0.49643 |  0:00:07s
epoch 20 | loss: 0.35234 | eval_custom_logloss: 0.53155 |  0:00:07s
epoch 21 | loss: 0.33154 | eval_custom_logloss: 0.52297 |  0:00:07s
epoch 22 | loss: 0.36761 | eval_custom_logloss: 0.48745 |  0:00:08s
epoch 23 | loss: 0.37243 | eval_custom_logloss: 0.53761 |  0:00:08s
epoch 24 | loss: 0.34016 | eval_custom_logloss: 0.53845 |  0:00:08s
epoch 25 | loss: 0.35608 | eval_custom_logloss: 0.51926 |  0:00:09s
epoch 26 | loss: 0.30217 | eval_custom_logloss: 0.46786 |  0:00:09s
epoch 27 | loss: 0.28136 | eval_custom_logloss: 0.4981  |  0:00:10s
epoch 28 | loss: 0.29333 | eval_custom_logloss: 0.58927 |  0:00:10s
epoch 29 | loss: 0.30029 | eval_custom_logloss: 0.50631 |  0:00:10s
epoch 30 | loss: 0.28191 | eval_custom_logloss: 0.50775 |  0:00:11s
epoch 31 | loss: 0.29338 | eval_custom_logloss: 0.51085 |  0:00:11s
epoch 32 | loss: 0.30407 | eval_custom_logloss: 0.50485 |  0:00:11s
epoch 33 | loss: 0.27479 | eval_custom_logloss: 0.54771 |  0:00:12s
epoch 34 | loss: 0.29395 | eval_custom_logloss: 0.4695  |  0:00:12s
epoch 35 | loss: 0.27514 | eval_custom_logloss: 0.47651 |  0:00:12s
epoch 36 | loss: 0.2523  | eval_custom_logloss: 0.46946 |  0:00:13s
epoch 37 | loss: 0.24647 | eval_custom_logloss: 0.50444 |  0:00:13s
epoch 38 | loss: 0.26224 | eval_custom_logloss: 0.49872 |  0:00:13s
epoch 39 | loss: 0.23531 | eval_custom_logloss: 0.47594 |  0:00:14s
epoch 40 | loss: 0.21911 | eval_custom_logloss: 0.45788 |  0:00:14s
epoch 41 | loss: 0.21907 | eval_custom_logloss: 0.47664 |  0:00:14s
epoch 42 | loss: 0.20032 | eval_custom_logloss: 0.50515 |  0:00:15s
epoch 43 | loss: 0.20493 | eval_custom_logloss: 0.53321 |  0:00:15s
epoch 44 | loss: 0.22867 | eval_custom_logloss: 0.4894  |  0:00:16s
epoch 45 | loss: 0.18083 | eval_custom_logloss: 0.50054 |  0:00:16s
epoch 46 | loss: 0.18479 | eval_custom_logloss: 0.47714 |  0:00:16s
epoch 47 | loss: 0.17324 | eval_custom_logloss: 0.53795 |  0:00:17s
epoch 48 | loss: 0.21111 | eval_custom_logloss: 0.49074 |  0:00:17s
epoch 49 | loss: 0.20029 | eval_custom_logloss: 0.50811 |  0:00:17s
epoch 50 | loss: 0.21288 | eval_custom_logloss: 0.57476 |  0:00:18s
epoch 51 | loss: 0.20566 | eval_custom_logloss: 0.56244 |  0:00:18s
epoch 52 | loss: 0.20732 | eval_custom_logloss: 0.50295 |  0:00:18s
epoch 53 | loss: 0.18611 | eval_custom_logloss: 0.56767 |  0:00:19s
epoch 54 | loss: 0.18399 | eval_custom_logloss: 0.56493 |  0:00:19s
epoch 55 | loss: 0.23337 | eval_custom_logloss: 0.54693 |  0:00:19s
epoch 56 | loss: 0.20484 | eval_custom_logloss: 0.57406 |  0:00:20s
epoch 57 | loss: 0.20121 | eval_custom_logloss: 0.52056 |  0:00:20s
epoch 58 | loss: 0.18115 | eval_custom_logloss: 0.53558 |  0:00:21s
epoch 59 | loss: 0.22491 | eval_custom_logloss: 0.51395 |  0:00:21s
epoch 60 | loss: 0.20764 | eval_custom_logloss: 0.48274 |  0:00:21s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.45788
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.408975, 'Log Loss - std': 0.0355421844432781} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.22094 | eval_custom_logloss: 1.05859 |  0:00:00s
epoch 1  | loss: 0.9136  | eval_custom_logloss: 0.7346  |  0:00:00s
epoch 2  | loss: 0.79338 | eval_custom_logloss: 0.70986 |  0:00:01s
epoch 3  | loss: 0.74169 | eval_custom_logloss: 0.64736 |  0:00:01s
epoch 4  | loss: 0.70879 | eval_custom_logloss: 0.68268 |  0:00:01s
epoch 5  | loss: 0.67519 | eval_custom_logloss: 0.58908 |  0:00:02s
epoch 6  | loss: 0.59598 | eval_custom_logloss: 0.50159 |  0:00:02s
epoch 7  | loss: 0.58358 | eval_custom_logloss: 0.61403 |  0:00:03s
epoch 8  | loss: 0.5364  | eval_custom_logloss: 0.55216 |  0:00:03s
epoch 9  | loss: 0.53164 | eval_custom_logloss: 0.47967 |  0:00:03s
epoch 10 | loss: 0.52564 | eval_custom_logloss: 0.47337 |  0:00:04s
epoch 11 | loss: 0.53751 | eval_custom_logloss: 0.41613 |  0:00:04s
epoch 12 | loss: 0.51665 | eval_custom_logloss: 0.45268 |  0:00:04s
epoch 13 | loss: 0.50153 | eval_custom_logloss: 0.46149 |  0:00:05s
epoch 14 | loss: 0.46679 | eval_custom_logloss: 0.40951 |  0:00:05s
epoch 15 | loss: 0.48137 | eval_custom_logloss: 0.45241 |  0:00:06s
epoch 16 | loss: 0.50839 | eval_custom_logloss: 0.47999 |  0:00:06s
epoch 17 | loss: 0.47477 | eval_custom_logloss: 0.3976  |  0:00:06s
epoch 18 | loss: 0.44267 | eval_custom_logloss: 0.38681 |  0:00:07s
epoch 19 | loss: 0.49345 | eval_custom_logloss: 0.40653 |  0:00:07s
epoch 20 | loss: 0.46663 | eval_custom_logloss: 0.42989 |  0:00:07s
epoch 21 | loss: 0.45062 | eval_custom_logloss: 0.46794 |  0:00:08s
epoch 22 | loss: 0.43348 | eval_custom_logloss: 0.40429 |  0:00:08s
epoch 23 | loss: 0.40537 | eval_custom_logloss: 0.37859 |  0:00:08s
epoch 24 | loss: 0.3694  | eval_custom_logloss: 0.34482 |  0:00:09s
epoch 25 | loss: 0.41337 | eval_custom_logloss: 0.32241 |  0:00:09s
epoch 26 | loss: 0.37529 | eval_custom_logloss: 0.32559 |  0:00:10s
epoch 27 | loss: 0.37835 | eval_custom_logloss: 0.30411 |  0:00:10s
epoch 28 | loss: 0.37487 | eval_custom_logloss: 0.31115 |  0:00:10s
epoch 29 | loss: 0.33852 | eval_custom_logloss: 0.29257 |  0:00:11s
epoch 30 | loss: 0.35041 | eval_custom_logloss: 0.34225 |  0:00:11s
epoch 31 | loss: 0.32142 | eval_custom_logloss: 0.31996 |  0:00:11s
epoch 32 | loss: 0.35282 | eval_custom_logloss: 0.2988  |  0:00:12s
epoch 33 | loss: 0.35116 | eval_custom_logloss: 0.31432 |  0:00:12s
epoch 34 | loss: 0.31612 | eval_custom_logloss: 0.32326 |  0:00:13s
epoch 35 | loss: 0.30408 | eval_custom_logloss: 0.29516 |  0:00:13s
epoch 36 | loss: 0.31365 | eval_custom_logloss: 0.30761 |  0:00:13s
epoch 37 | loss: 0.29661 | eval_custom_logloss: 0.32978 |  0:00:14s
epoch 38 | loss: 0.31617 | eval_custom_logloss: 0.33794 |  0:00:14s
epoch 39 | loss: 0.33347 | eval_custom_logloss: 0.327   |  0:00:14s
epoch 40 | loss: 0.35682 | eval_custom_logloss: 0.32726 |  0:00:15s
epoch 41 | loss: 0.33249 | eval_custom_logloss: 0.33238 |  0:00:15s
epoch 42 | loss: 0.32069 | eval_custom_logloss: 0.32245 |  0:00:16s
epoch 43 | loss: 0.30325 | eval_custom_logloss: 0.34935 |  0:00:16s
epoch 44 | loss: 0.31007 | eval_custom_logloss: 0.30213 |  0:00:16s
epoch 45 | loss: 0.30902 | eval_custom_logloss: 0.31861 |  0:00:17s
epoch 46 | loss: 0.32368 | eval_custom_logloss: 0.31411 |  0:00:17s
epoch 47 | loss: 0.2989  | eval_custom_logloss: 0.2687  |  0:00:18s
epoch 48 | loss: 0.26266 | eval_custom_logloss: 0.32322 |  0:00:18s
epoch 49 | loss: 0.29205 | eval_custom_logloss: 0.32827 |  0:00:19s
epoch 50 | loss: 0.2944  | eval_custom_logloss: 0.31946 |  0:00:19s
epoch 51 | loss: 0.31996 | eval_custom_logloss: 0.30642 |  0:00:20s
epoch 52 | loss: 0.30548 | eval_custom_logloss: 0.31933 |  0:00:20s
epoch 53 | loss: 0.2625  | eval_custom_logloss: 0.30497 |  0:00:20s
epoch 54 | loss: 0.24028 | eval_custom_logloss: 0.31217 |  0:00:21s
epoch 55 | loss: 0.24594 | eval_custom_logloss: 0.31827 |  0:00:21s
epoch 56 | loss: 0.23893 | eval_custom_logloss: 0.30019 |  0:00:22s
epoch 57 | loss: 0.26318 | eval_custom_logloss: 0.30296 |  0:00:22s
epoch 58 | loss: 0.25999 | eval_custom_logloss: 0.2769  |  0:00:23s
epoch 59 | loss: 0.24081 | eval_custom_logloss: 0.33309 |  0:00:23s
epoch 60 | loss: 0.25956 | eval_custom_logloss: 0.38565 |  0:00:24s
epoch 61 | loss: 0.27774 | eval_custom_logloss: 0.39028 |  0:00:24s
epoch 62 | loss: 0.25449 | eval_custom_logloss: 0.34538 |  0:00:24s
epoch 63 | loss: 0.23018 | eval_custom_logloss: 0.37998 |  0:00:25s
epoch 64 | loss: 0.20891 | eval_custom_logloss: 0.32793 |  0:00:25s
epoch 65 | loss: 0.24304 | eval_custom_logloss: 0.33751 |  0:00:26s
epoch 66 | loss: 0.23059 | eval_custom_logloss: 0.35505 |  0:00:26s
epoch 67 | loss: 0.19892 | eval_custom_logloss: 0.33532 |  0:00:26s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.2687
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.38092, 'Log Loss - std': 0.06448976352879579} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 0.38092 and parameters: {'n_d': 31, 'n_steps': 4, 'gamma': 1.3353229699760054, 'cat_emb_dim': 1, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.049173450125556316, 'mask_type': 'entmax'}. Best is trial 4 with value: 0.38446.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax', 'n_a': 61, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65966 | eval_custom_logloss: 1.52989 |  0:00:01s
epoch 1  | loss: 1.7087  | eval_custom_logloss: 1.51384 |  0:00:01s
epoch 2  | loss: 1.41709 | eval_custom_logloss: 1.08055 |  0:00:01s
epoch 3  | loss: 1.17456 | eval_custom_logloss: 1.28717 |  0:00:02s
epoch 4  | loss: 1.09172 | eval_custom_logloss: 1.34985 |  0:00:02s
epoch 5  | loss: 0.90361 | eval_custom_logloss: 0.86577 |  0:00:03s
epoch 6  | loss: 1.00506 | eval_custom_logloss: 1.16822 |  0:00:03s
epoch 7  | loss: 1.12175 | eval_custom_logloss: 0.82711 |  0:00:04s
epoch 8  | loss: 0.80579 | eval_custom_logloss: 0.92079 |  0:00:04s
epoch 9  | loss: 0.73033 | eval_custom_logloss: 0.8389  |  0:00:05s
epoch 10 | loss: 0.72386 | eval_custom_logloss: 0.67661 |  0:00:05s
epoch 11 | loss: 0.74513 | eval_custom_logloss: 0.92463 |  0:00:06s
epoch 12 | loss: 0.72607 | eval_custom_logloss: 0.64568 |  0:00:06s
epoch 13 | loss: 0.72039 | eval_custom_logloss: 0.74452 |  0:00:07s
epoch 14 | loss: 0.73046 | eval_custom_logloss: 0.66423 |  0:00:07s
epoch 15 | loss: 0.72922 | eval_custom_logloss: 0.71841 |  0:00:08s
epoch 16 | loss: 0.68126 | eval_custom_logloss: 0.60167 |  0:00:08s
epoch 17 | loss: 0.67175 | eval_custom_logloss: 0.58462 |  0:00:09s
epoch 18 | loss: 0.67193 | eval_custom_logloss: 0.59953 |  0:00:09s
epoch 19 | loss: 0.63909 | eval_custom_logloss: 0.63231 |  0:00:10s
epoch 20 | loss: 0.64307 | eval_custom_logloss: 0.59825 |  0:00:10s
epoch 21 | loss: 0.63812 | eval_custom_logloss: 0.646   |  0:00:11s
epoch 22 | loss: 0.58032 | eval_custom_logloss: 0.60018 |  0:00:11s
epoch 23 | loss: 0.56281 | eval_custom_logloss: 0.56176 |  0:00:12s
epoch 24 | loss: 0.54893 | eval_custom_logloss: 0.54162 |  0:00:12s
epoch 25 | loss: 0.54701 | eval_custom_logloss: 0.54637 |  0:00:13s
epoch 26 | loss: 0.52738 | eval_custom_logloss: 0.48795 |  0:00:13s
epoch 27 | loss: 0.51303 | eval_custom_logloss: 0.49939 |  0:00:14s
epoch 28 | loss: 0.49506 | eval_custom_logloss: 0.49375 |  0:00:14s
epoch 29 | loss: 0.51271 | eval_custom_logloss: 0.51753 |  0:00:15s
epoch 30 | loss: 0.52283 | eval_custom_logloss: 0.53823 |  0:00:15s
epoch 31 | loss: 0.51426 | eval_custom_logloss: 0.55843 |  0:00:16s
epoch 32 | loss: 0.49171 | eval_custom_logloss: 0.50594 |  0:00:16s
epoch 33 | loss: 0.49192 | eval_custom_logloss: 0.47267 |  0:00:17s
epoch 34 | loss: 0.47301 | eval_custom_logloss: 0.45745 |  0:00:17s
epoch 35 | loss: 0.4811  | eval_custom_logloss: 0.47716 |  0:00:18s
epoch 36 | loss: 0.5151  | eval_custom_logloss: 0.50828 |  0:00:18s
epoch 37 | loss: 0.47944 | eval_custom_logloss: 0.50163 |  0:00:18s
epoch 38 | loss: 0.45299 | eval_custom_logloss: 0.4897  |  0:00:19s
epoch 39 | loss: 0.43523 | eval_custom_logloss: 0.44756 |  0:00:19s
epoch 40 | loss: 0.41277 | eval_custom_logloss: 0.42838 |  0:00:20s
epoch 41 | loss: 0.40472 | eval_custom_logloss: 0.44867 |  0:00:20s
epoch 42 | loss: 0.43731 | eval_custom_logloss: 0.43504 |  0:00:21s
epoch 43 | loss: 0.38984 | eval_custom_logloss: 0.45047 |  0:00:21s
epoch 44 | loss: 0.39095 | eval_custom_logloss: 0.45459 |  0:00:22s
epoch 45 | loss: 0.3835  | eval_custom_logloss: 0.47859 |  0:00:22s
epoch 46 | loss: 0.40918 | eval_custom_logloss: 0.43673 |  0:00:23s
epoch 47 | loss: 0.36968 | eval_custom_logloss: 0.4428  |  0:00:23s
epoch 48 | loss: 0.36742 | eval_custom_logloss: 0.45745 |  0:00:23s
epoch 49 | loss: 0.35683 | eval_custom_logloss: 0.45488 |  0:00:24s
epoch 50 | loss: 0.39861 | eval_custom_logloss: 0.42733 |  0:00:24s
epoch 51 | loss: 0.39672 | eval_custom_logloss: 0.44835 |  0:00:25s
epoch 52 | loss: 0.39119 | eval_custom_logloss: 0.4411  |  0:00:25s
epoch 53 | loss: 0.3813  | eval_custom_logloss: 0.41847 |  0:00:26s
epoch 54 | loss: 0.40226 | eval_custom_logloss: 0.48731 |  0:00:26s
epoch 55 | loss: 0.3796  | eval_custom_logloss: 0.39509 |  0:00:27s
epoch 56 | loss: 0.3681  | eval_custom_logloss: 0.43522 |  0:00:27s
epoch 57 | loss: 0.37271 | eval_custom_logloss: 0.42818 |  0:00:28s
epoch 58 | loss: 0.35686 | eval_custom_logloss: 0.46411 |  0:00:28s
epoch 59 | loss: 0.3812  | eval_custom_logloss: 0.41047 |  0:00:28s
epoch 60 | loss: 0.40721 | eval_custom_logloss: 0.44457 |  0:00:29s
epoch 61 | loss: 0.3975  | eval_custom_logloss: 0.48705 |  0:00:29s
epoch 62 | loss: 0.42297 | eval_custom_logloss: 0.46097 |  0:00:30s
epoch 63 | loss: 0.42323 | eval_custom_logloss: 0.4389  |  0:00:30s
epoch 64 | loss: 0.4215  | eval_custom_logloss: 0.43434 |  0:00:31s
epoch 65 | loss: 0.39219 | eval_custom_logloss: 0.39733 |  0:00:31s
epoch 66 | loss: 0.37672 | eval_custom_logloss: 0.43414 |  0:00:32s
epoch 67 | loss: 0.38493 | eval_custom_logloss: 0.43609 |  0:00:32s
epoch 68 | loss: 0.3661  | eval_custom_logloss: 0.45182 |  0:00:32s
epoch 69 | loss: 0.38777 | eval_custom_logloss: 0.44334 |  0:00:33s
epoch 70 | loss: 0.34536 | eval_custom_logloss: 0.44806 |  0:00:33s
epoch 71 | loss: 0.32699 | eval_custom_logloss: 0.41585 |  0:00:34s
epoch 72 | loss: 0.35695 | eval_custom_logloss: 0.42691 |  0:00:34s
epoch 73 | loss: 0.34535 | eval_custom_logloss: 0.4195  |  0:00:35s
epoch 74 | loss: 0.34412 | eval_custom_logloss: 0.38313 |  0:00:35s
epoch 75 | loss: 0.33974 | eval_custom_logloss: 0.39579 |  0:00:36s
epoch 76 | loss: 0.32808 | eval_custom_logloss: 0.40083 |  0:00:36s
epoch 77 | loss: 0.32655 | eval_custom_logloss: 0.42943 |  0:00:37s
epoch 78 | loss: 0.32588 | eval_custom_logloss: 0.40198 |  0:00:37s
epoch 79 | loss: 0.32666 | eval_custom_logloss: 0.409   |  0:00:37s
epoch 80 | loss: 0.30946 | eval_custom_logloss: 0.42261 |  0:00:38s
epoch 81 | loss: 0.33128 | eval_custom_logloss: 0.39097 |  0:00:38s
epoch 82 | loss: 0.33345 | eval_custom_logloss: 0.38212 |  0:00:39s
epoch 83 | loss: 0.32082 | eval_custom_logloss: 0.36136 |  0:00:39s
epoch 84 | loss: 0.32436 | eval_custom_logloss: 0.40952 |  0:00:40s
epoch 85 | loss: 0.30302 | eval_custom_logloss: 0.39085 |  0:00:40s
epoch 86 | loss: 0.29902 | eval_custom_logloss: 0.4138  |  0:00:41s
epoch 87 | loss: 0.28284 | eval_custom_logloss: 0.41076 |  0:00:41s
epoch 88 | loss: 0.28582 | eval_custom_logloss: 0.38698 |  0:00:42s
epoch 89 | loss: 0.33189 | eval_custom_logloss: 0.41616 |  0:00:42s
epoch 90 | loss: 0.31108 | eval_custom_logloss: 0.37872 |  0:00:42s
epoch 91 | loss: 0.30587 | eval_custom_logloss: 0.39871 |  0:00:43s
epoch 92 | loss: 0.29915 | eval_custom_logloss: 0.37851 |  0:00:43s
epoch 93 | loss: 0.28133 | eval_custom_logloss: 0.39556 |  0:00:44s
epoch 94 | loss: 0.26873 | eval_custom_logloss: 0.38127 |  0:00:44s
epoch 95 | loss: 0.29817 | eval_custom_logloss: 0.39348 |  0:00:45s
epoch 96 | loss: 0.30285 | eval_custom_logloss: 0.38355 |  0:00:45s
epoch 97 | loss: 0.29057 | eval_custom_logloss: 0.39383 |  0:00:46s
epoch 98 | loss: 0.30727 | eval_custom_logloss: 0.34221 |  0:00:46s
epoch 99 | loss: 0.271   | eval_custom_logloss: 0.37547 |  0:00:46s
Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_eval_custom_logloss = 0.34221
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3422, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax', 'n_a': 61, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.68833 | eval_custom_logloss: 1.24531 |  0:00:00s
epoch 1  | loss: 1.13904 | eval_custom_logloss: 1.2899  |  0:00:01s
epoch 2  | loss: 1.21186 | eval_custom_logloss: 0.93639 |  0:00:01s
epoch 3  | loss: 0.96568 | eval_custom_logloss: 1.0007  |  0:00:01s
epoch 4  | loss: 0.92819 | eval_custom_logloss: 1.02266 |  0:00:02s
epoch 5  | loss: 0.84568 | eval_custom_logloss: 1.11333 |  0:00:02s
epoch 6  | loss: 0.80708 | eval_custom_logloss: 0.88456 |  0:00:03s
epoch 7  | loss: 0.79666 | eval_custom_logloss: 1.12933 |  0:00:03s
epoch 8  | loss: 0.85773 | eval_custom_logloss: 0.82896 |  0:00:04s
epoch 9  | loss: 0.77571 | eval_custom_logloss: 0.77146 |  0:00:04s
epoch 10 | loss: 0.72925 | eval_custom_logloss: 0.84996 |  0:00:05s
epoch 11 | loss: 0.71749 | eval_custom_logloss: 0.80806 |  0:00:05s
epoch 12 | loss: 0.69396 | eval_custom_logloss: 0.90308 |  0:00:05s
epoch 13 | loss: 0.67753 | eval_custom_logloss: 0.6961  |  0:00:06s
epoch 14 | loss: 0.66412 | eval_custom_logloss: 0.72714 |  0:00:06s
epoch 15 | loss: 0.68071 | eval_custom_logloss: 0.90097 |  0:00:07s
epoch 16 | loss: 0.67524 | eval_custom_logloss: 0.83672 |  0:00:07s
epoch 17 | loss: 0.6778  | eval_custom_logloss: 0.75429 |  0:00:08s
epoch 18 | loss: 0.70455 | eval_custom_logloss: 0.81493 |  0:00:08s
epoch 19 | loss: 0.5984  | eval_custom_logloss: 0.80935 |  0:00:09s
epoch 20 | loss: 0.60904 | eval_custom_logloss: 0.7298  |  0:00:09s
epoch 21 | loss: 0.59982 | eval_custom_logloss: 0.7316  |  0:00:09s
epoch 22 | loss: 0.57561 | eval_custom_logloss: 0.64755 |  0:00:10s
epoch 23 | loss: 0.58213 | eval_custom_logloss: 0.70843 |  0:00:10s
epoch 24 | loss: 0.60081 | eval_custom_logloss: 0.65891 |  0:00:11s
epoch 25 | loss: 0.52941 | eval_custom_logloss: 0.60513 |  0:00:11s
epoch 26 | loss: 0.58238 | eval_custom_logloss: 0.67898 |  0:00:12s
epoch 27 | loss: 0.60677 | eval_custom_logloss: 0.68501 |  0:00:12s
epoch 28 | loss: 0.58826 | eval_custom_logloss: 0.67655 |  0:00:13s
epoch 29 | loss: 0.578   | eval_custom_logloss: 0.68252 |  0:00:13s
epoch 30 | loss: 0.53271 | eval_custom_logloss: 0.61747 |  0:00:13s
epoch 31 | loss: 0.52982 | eval_custom_logloss: 0.60828 |  0:00:14s
epoch 32 | loss: 0.50741 | eval_custom_logloss: 0.62276 |  0:00:14s
epoch 33 | loss: 0.53065 | eval_custom_logloss: 0.59394 |  0:00:15s
epoch 34 | loss: 0.49255 | eval_custom_logloss: 0.60747 |  0:00:15s
epoch 35 | loss: 0.49251 | eval_custom_logloss: 0.61892 |  0:00:16s
epoch 36 | loss: 0.51526 | eval_custom_logloss: 0.57232 |  0:00:16s
epoch 37 | loss: 0.48328 | eval_custom_logloss: 0.58719 |  0:00:16s
epoch 38 | loss: 0.48252 | eval_custom_logloss: 0.61166 |  0:00:17s
epoch 39 | loss: 0.47771 | eval_custom_logloss: 0.58549 |  0:00:17s
epoch 40 | loss: 0.49622 | eval_custom_logloss: 0.54112 |  0:00:18s
epoch 41 | loss: 0.47596 | eval_custom_logloss: 0.56669 |  0:00:18s
epoch 42 | loss: 0.45398 | eval_custom_logloss: 0.56525 |  0:00:19s
epoch 43 | loss: 0.46621 | eval_custom_logloss: 0.60713 |  0:00:19s
epoch 44 | loss: 0.48944 | eval_custom_logloss: 0.53365 |  0:00:20s
epoch 45 | loss: 0.44456 | eval_custom_logloss: 0.5685  |  0:00:20s
epoch 46 | loss: 0.46569 | eval_custom_logloss: 0.57655 |  0:00:21s
epoch 47 | loss: 0.44329 | eval_custom_logloss: 0.54459 |  0:00:21s
epoch 48 | loss: 0.42253 | eval_custom_logloss: 0.55109 |  0:00:21s
epoch 49 | loss: 0.45036 | eval_custom_logloss: 0.65223 |  0:00:22s
epoch 50 | loss: 0.44813 | eval_custom_logloss: 0.59006 |  0:00:22s
epoch 51 | loss: 0.43728 | eval_custom_logloss: 0.58455 |  0:00:23s
epoch 52 | loss: 0.42741 | eval_custom_logloss: 0.53086 |  0:00:23s
epoch 53 | loss: 0.42283 | eval_custom_logloss: 0.56541 |  0:00:24s
epoch 54 | loss: 0.42373 | eval_custom_logloss: 0.53215 |  0:00:24s
epoch 55 | loss: 0.41506 | eval_custom_logloss: 0.54567 |  0:00:25s
epoch 56 | loss: 0.39749 | eval_custom_logloss: 0.51042 |  0:00:25s
epoch 57 | loss: 0.40986 | eval_custom_logloss: 0.52152 |  0:00:25s
epoch 58 | loss: 0.38819 | eval_custom_logloss: 0.55088 |  0:00:26s
epoch 59 | loss: 0.40281 | eval_custom_logloss: 0.55363 |  0:00:26s
epoch 60 | loss: 0.38456 | eval_custom_logloss: 0.57359 |  0:00:27s
epoch 61 | loss: 0.37413 | eval_custom_logloss: 0.51505 |  0:00:27s
epoch 62 | loss: 0.37109 | eval_custom_logloss: 0.53026 |  0:00:28s
epoch 63 | loss: 0.41104 | eval_custom_logloss: 0.53464 |  0:00:28s
epoch 64 | loss: 0.39295 | eval_custom_logloss: 0.50091 |  0:00:28s
epoch 65 | loss: 0.37665 | eval_custom_logloss: 0.47509 |  0:00:29s
epoch 66 | loss: 0.3727  | eval_custom_logloss: 0.52709 |  0:00:29s
epoch 67 | loss: 0.41456 | eval_custom_logloss: 0.48889 |  0:00:30s
epoch 68 | loss: 0.39086 | eval_custom_logloss: 0.54252 |  0:00:30s
epoch 69 | loss: 0.42864 | eval_custom_logloss: 0.57945 |  0:00:31s
epoch 70 | loss: 0.51362 | eval_custom_logloss: 0.64271 |  0:00:31s
epoch 71 | loss: 0.49726 | eval_custom_logloss: 0.57308 |  0:00:32s
epoch 72 | loss: 0.42447 | eval_custom_logloss: 0.53102 |  0:00:32s
epoch 73 | loss: 0.38028 | eval_custom_logloss: 0.54717 |  0:00:32s
epoch 74 | loss: 0.36274 | eval_custom_logloss: 0.49211 |  0:00:33s
epoch 75 | loss: 0.45242 | eval_custom_logloss: 0.607   |  0:00:33s
epoch 76 | loss: 0.4215  | eval_custom_logloss: 0.51217 |  0:00:34s
epoch 77 | loss: 0.39258 | eval_custom_logloss: 0.48254 |  0:00:34s
epoch 78 | loss: 0.39776 | eval_custom_logloss: 0.47779 |  0:00:35s
epoch 79 | loss: 0.36486 | eval_custom_logloss: 0.53272 |  0:00:35s
epoch 80 | loss: 0.36114 | eval_custom_logloss: 0.55559 |  0:00:36s
epoch 81 | loss: 0.34573 | eval_custom_logloss: 0.52922 |  0:00:36s
epoch 82 | loss: 0.35314 | eval_custom_logloss: 0.55893 |  0:00:36s
epoch 83 | loss: 0.33511 | eval_custom_logloss: 0.48477 |  0:00:37s
epoch 84 | loss: 0.33843 | eval_custom_logloss: 0.45122 |  0:00:37s
epoch 85 | loss: 0.37695 | eval_custom_logloss: 0.49366 |  0:00:38s
epoch 86 | loss: 0.39358 | eval_custom_logloss: 0.47991 |  0:00:38s
epoch 87 | loss: 0.38513 | eval_custom_logloss: 0.48765 |  0:00:39s
epoch 88 | loss: 0.33624 | eval_custom_logloss: 0.47189 |  0:00:39s
epoch 89 | loss: 0.3553  | eval_custom_logloss: 0.4476  |  0:00:40s
epoch 90 | loss: 0.31612 | eval_custom_logloss: 0.45553 |  0:00:40s
epoch 91 | loss: 0.37305 | eval_custom_logloss: 0.73114 |  0:00:40s
epoch 92 | loss: 0.3346  | eval_custom_logloss: 0.56426 |  0:00:41s
epoch 93 | loss: 0.34283 | eval_custom_logloss: 0.53347 |  0:00:41s
epoch 94 | loss: 0.34023 | eval_custom_logloss: 0.53535 |  0:00:42s
epoch 95 | loss: 0.33481 | eval_custom_logloss: 0.50662 |  0:00:42s
epoch 96 | loss: 0.43205 | eval_custom_logloss: 0.56529 |  0:00:43s
epoch 97 | loss: 0.3788  | eval_custom_logloss: 0.57256 |  0:00:43s
epoch 98 | loss: 0.37165 | eval_custom_logloss: 0.52265 |  0:00:44s
epoch 99 | loss: 0.38173 | eval_custom_logloss: 0.57054 |  0:00:44s
Stop training because you reached max_epochs = 100 with best_epoch = 89 and best_eval_custom_logloss = 0.4476
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.39490000000000003, 'Log Loss - std': 0.0527} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax', 'n_a': 61, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.83842 | eval_custom_logloss: 1.35338 |  0:00:00s
epoch 1  | loss: 1.42638 | eval_custom_logloss: 0.76258 |  0:00:00s
epoch 2  | loss: 1.09942 | eval_custom_logloss: 1.04162 |  0:00:01s
epoch 3  | loss: 1.14627 | eval_custom_logloss: 1.44567 |  0:00:01s
epoch 4  | loss: 1.18861 | eval_custom_logloss: 0.9713  |  0:00:02s
epoch 5  | loss: 1.04598 | eval_custom_logloss: 1.02296 |  0:00:02s
epoch 6  | loss: 0.92226 | eval_custom_logloss: 0.77153 |  0:00:03s
epoch 7  | loss: 0.85589 | eval_custom_logloss: 0.91789 |  0:00:03s
epoch 8  | loss: 0.83039 | eval_custom_logloss: 1.15097 |  0:00:04s
epoch 9  | loss: 0.85326 | eval_custom_logloss: 0.90305 |  0:00:04s
epoch 10 | loss: 0.80072 | eval_custom_logloss: 0.79052 |  0:00:04s
epoch 11 | loss: 0.68727 | eval_custom_logloss: 0.75863 |  0:00:05s
epoch 12 | loss: 0.72415 | eval_custom_logloss: 0.69569 |  0:00:05s
epoch 13 | loss: 0.72352 | eval_custom_logloss: 0.7814  |  0:00:06s
epoch 14 | loss: 0.68343 | eval_custom_logloss: 0.72367 |  0:00:06s
epoch 15 | loss: 0.6563  | eval_custom_logloss: 0.68474 |  0:00:07s
epoch 16 | loss: 0.63141 | eval_custom_logloss: 0.63477 |  0:00:07s
epoch 17 | loss: 0.65291 | eval_custom_logloss: 0.65162 |  0:00:08s
epoch 18 | loss: 0.60482 | eval_custom_logloss: 0.61523 |  0:00:08s
epoch 19 | loss: 0.64552 | eval_custom_logloss: 0.70537 |  0:00:09s
epoch 20 | loss: 0.60671 | eval_custom_logloss: 0.82226 |  0:00:09s
epoch 21 | loss: 0.64969 | eval_custom_logloss: 0.63308 |  0:00:09s
epoch 22 | loss: 0.62108 | eval_custom_logloss: 0.63579 |  0:00:10s
epoch 23 | loss: 0.61652 | eval_custom_logloss: 0.60529 |  0:00:10s
epoch 24 | loss: 0.61346 | eval_custom_logloss: 0.61713 |  0:00:11s
epoch 25 | loss: 0.57243 | eval_custom_logloss: 0.5382  |  0:00:11s
epoch 26 | loss: 0.53153 | eval_custom_logloss: 0.54291 |  0:00:12s
epoch 27 | loss: 0.5529  | eval_custom_logloss: 0.51488 |  0:00:12s
epoch 28 | loss: 0.56332 | eval_custom_logloss: 0.5721  |  0:00:13s
epoch 29 | loss: 0.53868 | eval_custom_logloss: 0.55336 |  0:00:13s
epoch 30 | loss: 0.59001 | eval_custom_logloss: 0.54435 |  0:00:13s
epoch 31 | loss: 0.55139 | eval_custom_logloss: 0.54688 |  0:00:14s
epoch 32 | loss: 0.50469 | eval_custom_logloss: 0.50689 |  0:00:14s
epoch 33 | loss: 0.52532 | eval_custom_logloss: 0.49838 |  0:00:15s
epoch 34 | loss: 0.49085 | eval_custom_logloss: 0.55418 |  0:00:15s
epoch 35 | loss: 0.50735 | eval_custom_logloss: 0.54097 |  0:00:16s
epoch 36 | loss: 0.53253 | eval_custom_logloss: 0.52615 |  0:00:16s
epoch 37 | loss: 0.54434 | eval_custom_logloss: 0.47283 |  0:00:17s
epoch 38 | loss: 0.51122 | eval_custom_logloss: 0.53706 |  0:00:17s
epoch 39 | loss: 0.50987 | eval_custom_logloss: 0.49779 |  0:00:17s
epoch 40 | loss: 0.52032 | eval_custom_logloss: 0.47676 |  0:00:18s
epoch 41 | loss: 0.50607 | eval_custom_logloss: 0.48055 |  0:00:18s
epoch 42 | loss: 0.50352 | eval_custom_logloss: 0.526   |  0:00:19s
epoch 43 | loss: 0.47534 | eval_custom_logloss: 0.49007 |  0:00:19s
epoch 44 | loss: 0.48393 | eval_custom_logloss: 0.49697 |  0:00:20s
epoch 45 | loss: 0.48506 | eval_custom_logloss: 0.55645 |  0:00:20s
epoch 46 | loss: 0.51384 | eval_custom_logloss: 0.49139 |  0:00:21s
epoch 47 | loss: 0.44367 | eval_custom_logloss: 0.53164 |  0:00:21s
epoch 48 | loss: 0.42847 | eval_custom_logloss: 0.52372 |  0:00:21s
epoch 49 | loss: 0.42073 | eval_custom_logloss: 0.44237 |  0:00:22s
epoch 50 | loss: 0.42227 | eval_custom_logloss: 0.44001 |  0:00:22s
epoch 51 | loss: 0.43529 | eval_custom_logloss: 0.48783 |  0:00:23s
epoch 52 | loss: 0.42877 | eval_custom_logloss: 0.50004 |  0:00:23s
epoch 53 | loss: 0.41377 | eval_custom_logloss: 0.41801 |  0:00:24s
epoch 54 | loss: 0.39301 | eval_custom_logloss: 0.5337  |  0:00:24s
epoch 55 | loss: 0.47757 | eval_custom_logloss: 0.45548 |  0:00:25s
epoch 56 | loss: 0.41801 | eval_custom_logloss: 0.45545 |  0:00:25s
epoch 57 | loss: 0.38855 | eval_custom_logloss: 0.45774 |  0:00:25s
epoch 58 | loss: 0.39919 | eval_custom_logloss: 0.40968 |  0:00:26s
epoch 59 | loss: 0.39234 | eval_custom_logloss: 0.4635  |  0:00:26s
epoch 60 | loss: 0.38317 | eval_custom_logloss: 0.39898 |  0:00:27s
epoch 61 | loss: 0.36443 | eval_custom_logloss: 0.36301 |  0:00:27s
epoch 62 | loss: 0.36916 | eval_custom_logloss: 0.37163 |  0:00:28s
epoch 63 | loss: 0.39845 | eval_custom_logloss: 0.41505 |  0:00:28s
epoch 64 | loss: 0.39582 | eval_custom_logloss: 0.37686 |  0:00:29s
epoch 65 | loss: 0.35374 | eval_custom_logloss: 0.47342 |  0:00:29s
epoch 66 | loss: 0.36995 | eval_custom_logloss: 0.42478 |  0:00:30s
epoch 67 | loss: 0.38433 | eval_custom_logloss: 0.39318 |  0:00:30s
epoch 68 | loss: 0.3676  | eval_custom_logloss: 0.3341  |  0:00:30s
epoch 69 | loss: 0.36542 | eval_custom_logloss: 0.37313 |  0:00:31s
epoch 70 | loss: 0.33408 | eval_custom_logloss: 0.36049 |  0:00:31s
epoch 71 | loss: 0.39041 | eval_custom_logloss: 0.4181  |  0:00:32s
epoch 72 | loss: 0.39968 | eval_custom_logloss: 0.45545 |  0:00:32s
epoch 73 | loss: 0.39667 | eval_custom_logloss: 0.47634 |  0:00:33s
epoch 74 | loss: 0.41304 | eval_custom_logloss: 0.45428 |  0:00:33s
epoch 75 | loss: 0.38446 | eval_custom_logloss: 0.49514 |  0:00:34s
epoch 76 | loss: 0.41264 | eval_custom_logloss: 0.49915 |  0:00:34s
epoch 77 | loss: 0.37498 | eval_custom_logloss: 0.46731 |  0:00:34s
epoch 78 | loss: 0.38941 | eval_custom_logloss: 0.43537 |  0:00:35s
epoch 79 | loss: 0.39713 | eval_custom_logloss: 0.47197 |  0:00:35s
epoch 80 | loss: 0.38241 | eval_custom_logloss: 0.45539 |  0:00:36s
epoch 81 | loss: 0.36297 | eval_custom_logloss: 0.43232 |  0:00:36s
epoch 82 | loss: 0.36732 | eval_custom_logloss: 0.5564  |  0:00:37s
epoch 83 | loss: 0.38934 | eval_custom_logloss: 0.47592 |  0:00:37s
epoch 84 | loss: 0.37328 | eval_custom_logloss: 0.43667 |  0:00:38s
epoch 85 | loss: 0.36396 | eval_custom_logloss: 0.39649 |  0:00:38s
epoch 86 | loss: 0.34797 | eval_custom_logloss: 0.4426  |  0:00:38s
epoch 87 | loss: 0.35088 | eval_custom_logloss: 0.43823 |  0:00:39s
epoch 88 | loss: 0.36134 | eval_custom_logloss: 0.51248 |  0:00:39s

Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_custom_logloss = 0.3341
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3746333333333334, 'Log Loss - std': 0.05170108530990642} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax', 'n_a': 61, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65526 | eval_custom_logloss: 1.06276 |  0:00:00s
epoch 1  | loss: 1.33721 | eval_custom_logloss: 1.36469 |  0:00:00s
epoch 2  | loss: 1.13268 | eval_custom_logloss: 1.21546 |  0:00:01s
epoch 3  | loss: 0.95127 | eval_custom_logloss: 1.19736 |  0:00:01s
epoch 4  | loss: 0.90374 | eval_custom_logloss: 1.02288 |  0:00:02s
epoch 5  | loss: 0.89304 | eval_custom_logloss: 1.12673 |  0:00:02s
epoch 6  | loss: 0.83829 | eval_custom_logloss: 0.92568 |  0:00:03s
epoch 7  | loss: 0.76927 | eval_custom_logloss: 0.85035 |  0:00:03s
epoch 8  | loss: 0.7298  | eval_custom_logloss: 0.80215 |  0:00:04s
epoch 9  | loss: 0.74048 | eval_custom_logloss: 0.80842 |  0:00:04s
epoch 10 | loss: 0.69677 | eval_custom_logloss: 0.73059 |  0:00:04s
epoch 11 | loss: 0.66682 | eval_custom_logloss: 0.72398 |  0:00:05s
epoch 12 | loss: 0.65592 | eval_custom_logloss: 0.6716  |  0:00:05s
epoch 13 | loss: 0.60103 | eval_custom_logloss: 0.77865 |  0:00:06s
epoch 14 | loss: 0.56778 | eval_custom_logloss: 0.61776 |  0:00:06s
epoch 15 | loss: 0.61806 | eval_custom_logloss: 0.72812 |  0:00:07s
epoch 16 | loss: 0.59141 | eval_custom_logloss: 0.69115 |  0:00:07s
epoch 17 | loss: 0.58176 | eval_custom_logloss: 0.75385 |  0:00:08s
epoch 18 | loss: 0.57855 | eval_custom_logloss: 0.65485 |  0:00:08s
epoch 19 | loss: 0.57008 | eval_custom_logloss: 0.67559 |  0:00:09s
epoch 20 | loss: 0.54658 | eval_custom_logloss: 0.63376 |  0:00:09s
epoch 21 | loss: 0.59815 | eval_custom_logloss: 0.59084 |  0:00:09s
epoch 22 | loss: 0.53772 | eval_custom_logloss: 0.64348 |  0:00:10s
epoch 23 | loss: 0.5199  | eval_custom_logloss: 0.61486 |  0:00:10s
epoch 24 | loss: 0.51241 | eval_custom_logloss: 0.57246 |  0:00:11s
epoch 25 | loss: 0.53694 | eval_custom_logloss: 0.5665  |  0:00:11s
epoch 26 | loss: 0.51686 | eval_custom_logloss: 0.60627 |  0:00:12s
epoch 27 | loss: 0.50802 | eval_custom_logloss: 0.5986  |  0:00:12s
epoch 28 | loss: 0.49076 | eval_custom_logloss: 0.59165 |  0:00:13s
epoch 29 | loss: 0.47196 | eval_custom_logloss: 0.56833 |  0:00:13s
epoch 30 | loss: 0.4745  | eval_custom_logloss: 0.6163  |  0:00:13s
epoch 31 | loss: 0.50644 | eval_custom_logloss: 0.59746 |  0:00:14s
epoch 32 | loss: 0.50433 | eval_custom_logloss: 0.60684 |  0:00:14s
epoch 33 | loss: 0.50549 | eval_custom_logloss: 0.55657 |  0:00:15s
epoch 34 | loss: 0.50576 | eval_custom_logloss: 0.55714 |  0:00:15s
epoch 35 | loss: 0.4823  | eval_custom_logloss: 0.61021 |  0:00:16s
epoch 36 | loss: 0.47313 | eval_custom_logloss: 0.61988 |  0:00:16s
epoch 37 | loss: 0.50984 | eval_custom_logloss: 0.5765  |  0:00:17s
epoch 38 | loss: 0.49271 | eval_custom_logloss: 0.53908 |  0:00:17s
epoch 39 | loss: 0.50617 | eval_custom_logloss: 0.60209 |  0:00:18s
epoch 40 | loss: 0.53456 | eval_custom_logloss: 0.55965 |  0:00:18s
epoch 41 | loss: 0.47997 | eval_custom_logloss: 0.57472 |  0:00:18s
epoch 42 | loss: 0.47139 | eval_custom_logloss: 0.57896 |  0:00:19s
epoch 43 | loss: 0.45764 | eval_custom_logloss: 0.54363 |  0:00:19s
epoch 44 | loss: 0.45643 | eval_custom_logloss: 0.52201 |  0:00:20s
epoch 45 | loss: 0.46118 | eval_custom_logloss: 0.47515 |  0:00:20s
epoch 46 | loss: 0.47438 | eval_custom_logloss: 0.52585 |  0:00:21s
epoch 47 | loss: 0.4445  | eval_custom_logloss: 0.49234 |  0:00:21s
epoch 48 | loss: 0.4289  | eval_custom_logloss: 0.50004 |  0:00:22s
epoch 49 | loss: 0.44007 | eval_custom_logloss: 0.5244  |  0:00:22s
epoch 50 | loss: 0.4514  | eval_custom_logloss: 0.53893 |  0:00:22s
epoch 51 | loss: 0.47774 | eval_custom_logloss: 0.57539 |  0:00:23s
epoch 52 | loss: 0.49032 | eval_custom_logloss: 0.60177 |  0:00:23s
epoch 53 | loss: 0.47875 | eval_custom_logloss: 0.53694 |  0:00:24s
epoch 54 | loss: 0.45472 | eval_custom_logloss: 0.5418  |  0:00:24s
epoch 55 | loss: 0.46488 | eval_custom_logloss: 0.54653 |  0:00:25s
epoch 56 | loss: 0.49403 | eval_custom_logloss: 0.58543 |  0:00:25s
epoch 57 | loss: 0.50665 | eval_custom_logloss: 0.54377 |  0:00:26s
epoch 58 | loss: 0.54605 | eval_custom_logloss: 0.57577 |  0:00:26s
epoch 59 | loss: 0.55053 | eval_custom_logloss: 0.55862 |  0:00:26s
epoch 60 | loss: 0.49637 | eval_custom_logloss: 0.51375 |  0:00:27s
epoch 61 | loss: 0.4848  | eval_custom_logloss: 0.54982 |  0:00:27s
epoch 62 | loss: 0.51895 | eval_custom_logloss: 0.57992 |  0:00:28s
epoch 63 | loss: 0.46382 | eval_custom_logloss: 0.56059 |  0:00:28s
epoch 64 | loss: 0.47192 | eval_custom_logloss: 0.49975 |  0:00:29s
epoch 65 | loss: 0.45405 | eval_custom_logloss: 0.51637 |  0:00:29s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 0.47515
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.39975000000000005, 'Log Loss - std': 0.06242829887158548} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax', 'n_a': 61, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.64056 | eval_custom_logloss: 1.86043 |  0:00:00s
epoch 1  | loss: 1.24408 | eval_custom_logloss: 0.83769 |  0:00:01s
epoch 2  | loss: 1.05091 | eval_custom_logloss: 1.08955 |  0:00:01s
epoch 3  | loss: 1.03649 | eval_custom_logloss: 0.84341 |  0:00:02s
epoch 4  | loss: 1.04205 | eval_custom_logloss: 0.78212 |  0:00:02s
epoch 5  | loss: 0.9606  | eval_custom_logloss: 0.98003 |  0:00:03s
epoch 6  | loss: 0.88125 | eval_custom_logloss: 0.82035 |  0:00:03s
epoch 7  | loss: 0.93007 | eval_custom_logloss: 0.73678 |  0:00:03s
epoch 8  | loss: 0.83196 | eval_custom_logloss: 0.68162 |  0:00:04s
epoch 9  | loss: 0.72651 | eval_custom_logloss: 0.61307 |  0:00:04s
epoch 10 | loss: 0.74301 | eval_custom_logloss: 0.70454 |  0:00:05s
epoch 11 | loss: 0.74052 | eval_custom_logloss: 0.64013 |  0:00:05s
epoch 12 | loss: 0.68711 | eval_custom_logloss: 0.61886 |  0:00:06s
epoch 13 | loss: 0.66728 | eval_custom_logloss: 0.54785 |  0:00:06s
epoch 14 | loss: 0.60123 | eval_custom_logloss: 0.54644 |  0:00:07s
epoch 15 | loss: 0.61805 | eval_custom_logloss: 0.53881 |  0:00:07s
epoch 16 | loss: 0.64414 | eval_custom_logloss: 0.55297 |  0:00:08s
epoch 17 | loss: 0.61985 | eval_custom_logloss: 0.54851 |  0:00:08s
epoch 18 | loss: 0.65889 | eval_custom_logloss: 0.66567 |  0:00:09s
epoch 19 | loss: 0.69022 | eval_custom_logloss: 0.64562 |  0:00:09s
epoch 20 | loss: 0.65081 | eval_custom_logloss: 0.60704 |  0:00:10s
epoch 21 | loss: 0.62239 | eval_custom_logloss: 0.58355 |  0:00:10s
epoch 22 | loss: 0.5896  | eval_custom_logloss: 0.47086 |  0:00:11s
epoch 23 | loss: 0.54775 | eval_custom_logloss: 0.43164 |  0:00:11s
epoch 24 | loss: 0.55305 | eval_custom_logloss: 0.50776 |  0:00:12s
epoch 25 | loss: 0.51488 | eval_custom_logloss: 0.49537 |  0:00:12s
epoch 26 | loss: 0.51995 | eval_custom_logloss: 0.47688 |  0:00:13s
epoch 27 | loss: 0.53156 | eval_custom_logloss: 0.46466 |  0:00:13s
epoch 28 | loss: 0.50662 | eval_custom_logloss: 0.47247 |  0:00:13s
epoch 29 | loss: 0.48941 | eval_custom_logloss: 0.44024 |  0:00:14s
epoch 30 | loss: 0.46571 | eval_custom_logloss: 0.44168 |  0:00:14s
epoch 31 | loss: 0.48534 | eval_custom_logloss: 0.39398 |  0:00:15s
epoch 32 | loss: 0.45809 | eval_custom_logloss: 0.41277 |  0:00:15s
epoch 33 | loss: 0.4621  | eval_custom_logloss: 0.409   |  0:00:16s
epoch 34 | loss: 0.44951 | eval_custom_logloss: 0.41887 |  0:00:16s
epoch 35 | loss: 0.45251 | eval_custom_logloss: 0.43271 |  0:00:17s
epoch 36 | loss: 0.48122 | eval_custom_logloss: 0.41496 |  0:00:17s
epoch 37 | loss: 0.50113 | eval_custom_logloss: 0.43789 |  0:00:18s
epoch 38 | loss: 0.46619 | eval_custom_logloss: 0.40442 |  0:00:18s
epoch 39 | loss: 0.43563 | eval_custom_logloss: 0.38751 |  0:00:19s
epoch 40 | loss: 0.46412 | eval_custom_logloss: 0.39999 |  0:00:19s
epoch 41 | loss: 0.43744 | eval_custom_logloss: 0.43225 |  0:00:19s
epoch 42 | loss: 0.42352 | eval_custom_logloss: 0.37269 |  0:00:20s
epoch 43 | loss: 0.41183 | eval_custom_logloss: 0.40518 |  0:00:20s
epoch 44 | loss: 0.4614  | eval_custom_logloss: 0.42146 |  0:00:21s
epoch 45 | loss: 0.47353 | eval_custom_logloss: 0.45971 |  0:00:21s
epoch 46 | loss: 0.46579 | eval_custom_logloss: 0.41305 |  0:00:22s
epoch 47 | loss: 0.45515 | eval_custom_logloss: 0.39097 |  0:00:22s
epoch 48 | loss: 0.42575 | eval_custom_logloss: 0.37517 |  0:00:23s
epoch 49 | loss: 0.41327 | eval_custom_logloss: 0.61045 |  0:00:23s
epoch 50 | loss: 0.43854 | eval_custom_logloss: 0.39476 |  0:00:24s
epoch 51 | loss: 0.39214 | eval_custom_logloss: 0.37794 |  0:00:24s
epoch 52 | loss: 0.37452 | eval_custom_logloss: 0.36535 |  0:00:25s
epoch 53 | loss: 0.40462 | eval_custom_logloss: 0.34662 |  0:00:25s
epoch 54 | loss: 0.38786 | eval_custom_logloss: 0.33846 |  0:00:26s
epoch 55 | loss: 0.37384 | eval_custom_logloss: 0.32619 |  0:00:26s
epoch 56 | loss: 0.36253 | eval_custom_logloss: 0.33866 |  0:00:27s
epoch 57 | loss: 0.37336 | eval_custom_logloss: 0.38137 |  0:00:27s
epoch 58 | loss: 0.38585 | eval_custom_logloss: 0.36998 |  0:00:28s
epoch 59 | loss: 0.37398 | eval_custom_logloss: 0.3572  |  0:00:28s
epoch 60 | loss: 0.38078 | eval_custom_logloss: 0.31702 |  0:00:29s
epoch 61 | loss: 0.35936 | eval_custom_logloss: 0.34361 |  0:00:29s
epoch 62 | loss: 0.37356 | eval_custom_logloss: 0.33592 |  0:00:30s
epoch 63 | loss: 0.36553 | eval_custom_logloss: 0.33109 |  0:00:30s
epoch 64 | loss: 0.3578  | eval_custom_logloss: 0.29416 |  0:00:31s
epoch 65 | loss: 0.38466 | eval_custom_logloss: 0.35157 |  0:00:31s
epoch 66 | loss: 0.35932 | eval_custom_logloss: 0.33918 |  0:00:32s
epoch 67 | loss: 0.34872 | eval_custom_logloss: 0.31554 |  0:00:32s
epoch 68 | loss: 0.33913 | eval_custom_logloss: 0.31892 |  0:00:33s
epoch 69 | loss: 0.3553  | eval_custom_logloss: 0.35221 |  0:00:33s
epoch 70 | loss: 0.32782 | eval_custom_logloss: 0.29285 |  0:00:33s
epoch 71 | loss: 0.38768 | eval_custom_logloss: 0.36431 |  0:00:34s
epoch 72 | loss: 0.39041 | eval_custom_logloss: 0.36096 |  0:00:34s
epoch 73 | loss: 0.35231 | eval_custom_logloss: 0.31872 |  0:00:35s
epoch 74 | loss: 0.34156 | eval_custom_logloss: 0.33663 |  0:00:35s
epoch 75 | loss: 0.33741 | eval_custom_logloss: 0.31225 |  0:00:36s
epoch 76 | loss: 0.33211 | eval_custom_logloss: 0.33256 |  0:00:36s
epoch 77 | loss: 0.34019 | eval_custom_logloss: 0.35146 |  0:00:37s
epoch 78 | loss: 0.36449 | eval_custom_logloss: 0.35202 |  0:00:37s
epoch 79 | loss: 0.33903 | eval_custom_logloss: 0.3508  |  0:00:38s
epoch 80 | loss: 0.37176 | eval_custom_logloss: 0.3543  |  0:00:38s
epoch 81 | loss: 0.41904 | eval_custom_logloss: 0.39634 |  0:00:39s
epoch 82 | loss: 0.38923 | eval_custom_logloss: 0.42055 |  0:00:39s
epoch 83 | loss: 0.36504 | eval_custom_logloss: 0.35716 |  0:00:40s
epoch 84 | loss: 0.3551  | eval_custom_logloss: 0.33338 |  0:00:40s
epoch 85 | loss: 0.35396 | eval_custom_logloss: 0.37069 |  0:00:41s
epoch 86 | loss: 0.33656 | eval_custom_logloss: 0.32133 |  0:00:41s
epoch 87 | loss: 0.30095 | eval_custom_logloss: 0.32487 |  0:00:42s
epoch 88 | loss: 0.28962 | eval_custom_logloss: 0.31289 |  0:00:42s
epoch 89 | loss: 0.29556 | eval_custom_logloss: 0.31385 |  0:00:43s
epoch 90 | loss: 0.31192 | eval_custom_logloss: 0.31894 |  0:00:43s

Early stopping occurred at epoch 90 with best_epoch = 70 and best_eval_custom_logloss = 0.29285
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.37836000000000003, 'Log Loss - std': 0.07034175431420515} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 0.37836000000000003 and parameters: {'n_d': 61, 'n_steps': 7, 'gamma': 1.4048615648819123, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 2, 'momentum': 0.07908079218746916, 'mask_type': 'sparsemax'}. Best is trial 4 with value: 0.38446.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.04204 | eval_custom_logloss: 3.34241 |  0:00:00s
epoch 1  | loss: 0.88064 | eval_custom_logloss: 5.46284 |  0:00:00s
epoch 2  | loss: 0.88297 | eval_custom_logloss: 4.72259 |  0:00:01s
epoch 3  | loss: 0.77514 | eval_custom_logloss: 5.3192  |  0:00:01s
epoch 4  | loss: 0.69326 | eval_custom_logloss: 4.76689 |  0:00:01s
epoch 5  | loss: 0.66522 | eval_custom_logloss: 5.85898 |  0:00:02s
epoch 6  | loss: 0.6602  | eval_custom_logloss: 7.26366 |  0:00:02s
epoch 7  | loss: 0.6181  | eval_custom_logloss: 7.39062 |  0:00:03s
epoch 8  | loss: 0.6043  | eval_custom_logloss: 6.21443 |  0:00:03s
epoch 9  | loss: 0.56657 | eval_custom_logloss: 5.46674 |  0:00:03s
epoch 10 | loss: 0.64444 | eval_custom_logloss: 5.11941 |  0:00:04s
epoch 11 | loss: 0.56667 | eval_custom_logloss: 3.59462 |  0:00:04s
epoch 12 | loss: 0.55801 | eval_custom_logloss: 3.96738 |  0:00:04s
epoch 13 | loss: 0.59326 | eval_custom_logloss: 4.22757 |  0:00:05s
epoch 14 | loss: 0.5506  | eval_custom_logloss: 4.2007  |  0:00:05s
epoch 15 | loss: 0.51976 | eval_custom_logloss: 5.18178 |  0:00:06s
epoch 16 | loss: 0.55383 | eval_custom_logloss: 5.77412 |  0:00:06s
epoch 17 | loss: 0.52732 | eval_custom_logloss: 6.01196 |  0:00:07s
epoch 18 | loss: 0.55862 | eval_custom_logloss: 5.69152 |  0:00:07s
epoch 19 | loss: 0.52105 | eval_custom_logloss: 3.18066 |  0:00:07s
epoch 20 | loss: 0.54316 | eval_custom_logloss: 3.43631 |  0:00:08s
epoch 21 | loss: 0.49687 | eval_custom_logloss: 3.03928 |  0:00:08s
epoch 22 | loss: 0.46791 | eval_custom_logloss: 3.0708  |  0:00:09s
epoch 23 | loss: 0.48147 | eval_custom_logloss: 2.97879 |  0:00:09s
epoch 24 | loss: 0.47809 | eval_custom_logloss: 2.66228 |  0:00:10s
epoch 25 | loss: 0.4791  | eval_custom_logloss: 2.85092 |  0:00:10s
epoch 26 | loss: 0.49536 | eval_custom_logloss: 2.91803 |  0:00:11s
epoch 27 | loss: 0.4652  | eval_custom_logloss: 2.73737 |  0:00:11s
epoch 28 | loss: 0.47092 | eval_custom_logloss: 3.12077 |  0:00:11s
epoch 29 | loss: 0.4725  | eval_custom_logloss: 2.59898 |  0:00:12s
epoch 30 | loss: 0.43196 | eval_custom_logloss: 2.98507 |  0:00:12s
epoch 31 | loss: 0.42802 | eval_custom_logloss: 2.67567 |  0:00:13s
epoch 32 | loss: 0.47474 | eval_custom_logloss: 3.92476 |  0:00:13s
epoch 33 | loss: 0.43012 | eval_custom_logloss: 3.4411  |  0:00:14s
epoch 34 | loss: 0.4055  | eval_custom_logloss: 3.66609 |  0:00:14s
epoch 35 | loss: 0.38527 | eval_custom_logloss: 3.26295 |  0:00:15s
epoch 36 | loss: 0.39625 | eval_custom_logloss: 3.54302 |  0:00:15s
epoch 37 | loss: 0.39651 | eval_custom_logloss: 3.75926 |  0:00:15s
epoch 38 | loss: 0.39717 | eval_custom_logloss: 2.9047  |  0:00:16s
epoch 39 | loss: 0.38612 | eval_custom_logloss: 2.68191 |  0:00:16s
epoch 40 | loss: 0.3871  | eval_custom_logloss: 3.1411  |  0:00:17s
epoch 41 | loss: 0.39018 | eval_custom_logloss: 2.62164 |  0:00:17s
epoch 42 | loss: 0.41488 | eval_custom_logloss: 3.1813  |  0:00:18s
epoch 43 | loss: 0.40285 | eval_custom_logloss: 2.8969  |  0:00:18s
epoch 44 | loss: 0.37044 | eval_custom_logloss: 2.82136 |  0:00:18s
epoch 45 | loss: 0.4123  | eval_custom_logloss: 2.33795 |  0:00:19s
epoch 46 | loss: 0.38628 | eval_custom_logloss: 2.69093 |  0:00:19s
epoch 47 | loss: 0.38518 | eval_custom_logloss: 3.34146 |  0:00:20s
epoch 48 | loss: 0.36212 | eval_custom_logloss: 2.39106 |  0:00:20s
epoch 49 | loss: 0.3752  | eval_custom_logloss: 2.59183 |  0:00:21s
epoch 50 | loss: 0.38707 | eval_custom_logloss: 2.04678 |  0:00:21s
epoch 51 | loss: 0.38925 | eval_custom_logloss: 2.20085 |  0:00:22s
epoch 52 | loss: 0.36067 | eval_custom_logloss: 2.50693 |  0:00:22s
epoch 53 | loss: 0.33527 | eval_custom_logloss: 2.14322 |  0:00:22s
epoch 54 | loss: 0.31751 | eval_custom_logloss: 2.35746 |  0:00:23s
epoch 55 | loss: 0.29782 | eval_custom_logloss: 3.39626 |  0:00:23s
epoch 56 | loss: 0.32567 | eval_custom_logloss: 2.94957 |  0:00:24s
epoch 57 | loss: 0.33092 | eval_custom_logloss: 2.754   |  0:00:24s
epoch 58 | loss: 0.31918 | eval_custom_logloss: 2.89288 |  0:00:25s
epoch 59 | loss: 0.35907 | eval_custom_logloss: 3.18961 |  0:00:25s
epoch 60 | loss: 0.36239 | eval_custom_logloss: 2.57753 |  0:00:25s
epoch 61 | loss: 0.32568 | eval_custom_logloss: 2.06138 |  0:00:26s
epoch 62 | loss: 0.32111 | eval_custom_logloss: 1.66689 |  0:00:26s
epoch 63 | loss: 0.3186  | eval_custom_logloss: 1.63208 |  0:00:27s
epoch 64 | loss: 0.3096  | eval_custom_logloss: 1.80795 |  0:00:27s
epoch 65 | loss: 0.31967 | eval_custom_logloss: 2.03028 |  0:00:28s
epoch 66 | loss: 0.28646 | eval_custom_logloss: 1.9266  |  0:00:28s
epoch 67 | loss: 0.33444 | eval_custom_logloss: 1.80626 |  0:00:28s
epoch 68 | loss: 0.30196 | eval_custom_logloss: 1.93365 |  0:00:29s
epoch 69 | loss: 0.30017 | eval_custom_logloss: 2.10494 |  0:00:29s
epoch 70 | loss: 0.3187  | eval_custom_logloss: 2.25805 |  0:00:30s
epoch 71 | loss: 0.30033 | eval_custom_logloss: 2.10079 |  0:00:30s
epoch 72 | loss: 0.32488 | eval_custom_logloss: 2.19486 |  0:00:31s
epoch 73 | loss: 0.30578 | eval_custom_logloss: 2.21367 |  0:00:31s
epoch 74 | loss: 0.286   | eval_custom_logloss: 1.7833  |  0:00:32s
epoch 75 | loss: 0.30058 | eval_custom_logloss: 2.56915 |  0:00:32s
epoch 76 | loss: 0.30712 | eval_custom_logloss: 2.63948 |  0:00:32s
epoch 77 | loss: 0.26888 | eval_custom_logloss: 2.85773 |  0:00:33s
epoch 78 | loss: 0.26678 | eval_custom_logloss: 3.24915 |  0:00:33s
epoch 79 | loss: 0.27432 | eval_custom_logloss: 3.08631 |  0:00:34s
epoch 80 | loss: 0.25799 | eval_custom_logloss: 3.30001 |  0:00:34s
epoch 81 | loss: 0.26326 | eval_custom_logloss: 2.98923 |  0:00:35s
epoch 82 | loss: 0.28783 | eval_custom_logloss: 3.18873 |  0:00:35s
epoch 83 | loss: 0.33747 | eval_custom_logloss: 1.81235 |  0:00:35s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 1.63208
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5213, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.04645 | eval_custom_logloss: 3.34763 |  0:00:00s
epoch 1  | loss: 0.86225 | eval_custom_logloss: 4.26378 |  0:00:00s
epoch 2  | loss: 0.86449 | eval_custom_logloss: 4.34624 |  0:00:01s
epoch 3  | loss: 0.78535 | eval_custom_logloss: 3.33644 |  0:00:01s
epoch 4  | loss: 0.72813 | eval_custom_logloss: 5.23571 |  0:00:01s
epoch 5  | loss: 0.73063 | eval_custom_logloss: 4.90116 |  0:00:02s
epoch 6  | loss: 0.67846 | eval_custom_logloss: 6.28452 |  0:00:02s
epoch 7  | loss: 0.65879 | eval_custom_logloss: 6.3891  |  0:00:03s
epoch 8  | loss: 0.60251 | eval_custom_logloss: 7.2934  |  0:00:03s
epoch 9  | loss: 0.5862  | eval_custom_logloss: 6.2311  |  0:00:03s
epoch 10 | loss: 0.59185 | eval_custom_logloss: 5.26329 |  0:00:04s
epoch 11 | loss: 0.57445 | eval_custom_logloss: 5.28392 |  0:00:04s
epoch 12 | loss: 0.57951 | eval_custom_logloss: 4.28466 |  0:00:04s
epoch 13 | loss: 0.58826 | eval_custom_logloss: 3.75076 |  0:00:05s
epoch 14 | loss: 0.5768  | eval_custom_logloss: 4.66775 |  0:00:05s
epoch 15 | loss: 0.5379  | eval_custom_logloss: 4.94767 |  0:00:06s
epoch 16 | loss: 0.5008  | eval_custom_logloss: 4.38331 |  0:00:06s
epoch 17 | loss: 0.49586 | eval_custom_logloss: 3.77052 |  0:00:06s
epoch 18 | loss: 0.49921 | eval_custom_logloss: 4.02267 |  0:00:07s
epoch 19 | loss: 0.48635 | eval_custom_logloss: 3.66615 |  0:00:07s
epoch 20 | loss: 0.47797 | eval_custom_logloss: 2.62071 |  0:00:07s
epoch 21 | loss: 0.49512 | eval_custom_logloss: 3.08847 |  0:00:08s
epoch 22 | loss: 0.48496 | eval_custom_logloss: 3.75554 |  0:00:08s
epoch 23 | loss: 0.47431 | eval_custom_logloss: 3.71033 |  0:00:09s
epoch 24 | loss: 0.4653  | eval_custom_logloss: 2.82181 |  0:00:09s
epoch 25 | loss: 0.46816 | eval_custom_logloss: 3.08665 |  0:00:09s
epoch 26 | loss: 0.48382 | eval_custom_logloss: 2.6688  |  0:00:10s
epoch 27 | loss: 0.46225 | eval_custom_logloss: 2.71132 |  0:00:10s
epoch 28 | loss: 0.46958 | eval_custom_logloss: 2.90132 |  0:00:11s
epoch 29 | loss: 0.45244 | eval_custom_logloss: 3.19238 |  0:00:11s
epoch 30 | loss: 0.41047 | eval_custom_logloss: 3.738   |  0:00:12s
epoch 31 | loss: 0.42207 | eval_custom_logloss: 3.85047 |  0:00:12s
epoch 32 | loss: 0.45966 | eval_custom_logloss: 3.11478 |  0:00:12s
epoch 33 | loss: 0.41727 | eval_custom_logloss: 3.68188 |  0:00:13s
epoch 34 | loss: 0.41988 | eval_custom_logloss: 3.44283 |  0:00:13s
epoch 35 | loss: 0.39186 | eval_custom_logloss: 3.23781 |  0:00:14s
epoch 36 | loss: 0.405   | eval_custom_logloss: 4.37986 |  0:00:14s
epoch 37 | loss: 0.46183 | eval_custom_logloss: 4.5172  |  0:00:15s
epoch 38 | loss: 0.44241 | eval_custom_logloss: 2.52655 |  0:00:15s
epoch 39 | loss: 0.42161 | eval_custom_logloss: 3.41047 |  0:00:15s
epoch 40 | loss: 0.39491 | eval_custom_logloss: 3.46728 |  0:00:16s
epoch 41 | loss: 0.39978 | eval_custom_logloss: 3.53391 |  0:00:16s
epoch 42 | loss: 0.36183 | eval_custom_logloss: 4.00484 |  0:00:17s
epoch 43 | loss: 0.35039 | eval_custom_logloss: 4.13436 |  0:00:17s
epoch 44 | loss: 0.38283 | eval_custom_logloss: 4.1363  |  0:00:18s
epoch 45 | loss: 0.36198 | eval_custom_logloss: 4.48853 |  0:00:18s
epoch 46 | loss: 0.40766 | eval_custom_logloss: 4.06049 |  0:00:19s
epoch 47 | loss: 0.36407 | eval_custom_logloss: 3.78602 |  0:00:19s
epoch 48 | loss: 0.33566 | eval_custom_logloss: 3.90685 |  0:00:19s
epoch 49 | loss: 0.33706 | eval_custom_logloss: 3.16428 |  0:00:20s
epoch 50 | loss: 0.35322 | eval_custom_logloss: 3.0802  |  0:00:20s
epoch 51 | loss: 0.32405 | eval_custom_logloss: 3.24723 |  0:00:21s
epoch 52 | loss: 0.30352 | eval_custom_logloss: 3.15047 |  0:00:21s
epoch 53 | loss: 0.2946  | eval_custom_logloss: 3.45957 |  0:00:22s
epoch 54 | loss: 0.31801 | eval_custom_logloss: 3.3188  |  0:00:22s
epoch 55 | loss: 0.31483 | eval_custom_logloss: 2.65405 |  0:00:22s
epoch 56 | loss: 0.29439 | eval_custom_logloss: 2.9291  |  0:00:23s
epoch 57 | loss: 0.31461 | eval_custom_logloss: 2.60267 |  0:00:23s
epoch 58 | loss: 0.29915 | eval_custom_logloss: 2.27617 |  0:00:24s
epoch 59 | loss: 0.29868 | eval_custom_logloss: 2.48195 |  0:00:24s
epoch 60 | loss: 0.30623 | eval_custom_logloss: 2.03367 |  0:00:25s
epoch 61 | loss: 0.29803 | eval_custom_logloss: 2.40883 |  0:00:25s
epoch 62 | loss: 0.29643 | eval_custom_logloss: 2.01109 |  0:00:25s
epoch 63 | loss: 0.32453 | eval_custom_logloss: 2.82482 |  0:00:26s
epoch 64 | loss: 0.29444 | eval_custom_logloss: 2.45021 |  0:00:26s
epoch 65 | loss: 0.27099 | eval_custom_logloss: 1.82844 |  0:00:27s
epoch 66 | loss: 0.28686 | eval_custom_logloss: 1.9473  |  0:00:27s
epoch 67 | loss: 0.28408 | eval_custom_logloss: 2.19097 |  0:00:28s
epoch 68 | loss: 0.28044 | eval_custom_logloss: 2.24129 |  0:00:28s
epoch 69 | loss: 0.2576  | eval_custom_logloss: 2.17421 |  0:00:29s
epoch 70 | loss: 0.26987 | eval_custom_logloss: 2.15487 |  0:00:29s
epoch 71 | loss: 0.2952  | eval_custom_logloss: 2.20982 |  0:00:29s
epoch 72 | loss: 0.26535 | eval_custom_logloss: 2.13409 |  0:00:30s
epoch 73 | loss: 0.26873 | eval_custom_logloss: 2.40727 |  0:00:30s
epoch 74 | loss: 0.2467  | eval_custom_logloss: 2.43416 |  0:00:31s
epoch 75 | loss: 0.22184 | eval_custom_logloss: 3.02576 |  0:00:31s
epoch 76 | loss: 0.22967 | eval_custom_logloss: 2.61585 |  0:00:32s
epoch 77 | loss: 0.2334  | eval_custom_logloss: 2.68881 |  0:00:32s
epoch 78 | loss: 0.25316 | eval_custom_logloss: 2.85253 |  0:00:32s
epoch 79 | loss: 0.26936 | eval_custom_logloss: 3.021   |  0:00:33s
epoch 80 | loss: 0.25585 | eval_custom_logloss: 2.64306 |  0:00:33s
epoch 81 | loss: 0.2292  | eval_custom_logloss: 2.96225 |  0:00:34s
epoch 82 | loss: 0.20098 | eval_custom_logloss: 2.46177 |  0:00:34s
epoch 83 | loss: 0.19317 | eval_custom_logloss: 2.79663 |  0:00:35s
epoch 84 | loss: 0.21024 | eval_custom_logloss: 2.89771 |  0:00:35s
epoch 85 | loss: 0.19443 | eval_custom_logloss: 2.70564 |  0:00:36s

Early stopping occurred at epoch 85 with best_epoch = 65 and best_eval_custom_logloss = 1.82844
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.6209, 'Log Loss - std': 0.09959999999999991} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.07712 | eval_custom_logloss: 2.74892 |  0:00:00s
epoch 1  | loss: 0.90336 | eval_custom_logloss: 3.45241 |  0:00:00s
epoch 2  | loss: 0.8553  | eval_custom_logloss: 4.56051 |  0:00:01s
epoch 3  | loss: 0.76173 | eval_custom_logloss: 4.01825 |  0:00:01s
epoch 4  | loss: 0.7118  | eval_custom_logloss: 5.16607 |  0:00:01s
epoch 5  | loss: 0.67042 | eval_custom_logloss: 3.10576 |  0:00:02s
epoch 6  | loss: 0.66297 | eval_custom_logloss: 2.46213 |  0:00:02s
epoch 7  | loss: 0.68051 | eval_custom_logloss: 5.16607 |  0:00:03s
epoch 8  | loss: 0.66144 | eval_custom_logloss: 4.66708 |  0:00:03s
epoch 9  | loss: 0.60413 | eval_custom_logloss: 4.87761 |  0:00:03s
epoch 10 | loss: 0.60497 | eval_custom_logloss: 4.90773 |  0:00:04s
epoch 11 | loss: 0.59148 | eval_custom_logloss: 5.01599 |  0:00:04s
epoch 12 | loss: 0.5526  | eval_custom_logloss: 6.63896 |  0:00:04s
epoch 13 | loss: 0.54734 | eval_custom_logloss: 6.59195 |  0:00:05s
epoch 14 | loss: 0.57466 | eval_custom_logloss: 4.20819 |  0:00:05s
epoch 15 | loss: 0.59261 | eval_custom_logloss: 5.902   |  0:00:05s
epoch 16 | loss: 0.55512 | eval_custom_logloss: 4.21042 |  0:00:06s
epoch 17 | loss: 0.54544 | eval_custom_logloss: 3.74383 |  0:00:06s
epoch 18 | loss: 0.52065 | eval_custom_logloss: 3.73457 |  0:00:07s
epoch 19 | loss: 0.5236  | eval_custom_logloss: 4.37199 |  0:00:07s
epoch 20 | loss: 0.53541 | eval_custom_logloss: 3.43596 |  0:00:07s
epoch 21 | loss: 0.53999 | eval_custom_logloss: 2.98886 |  0:00:08s
epoch 22 | loss: 0.52776 | eval_custom_logloss: 2.43672 |  0:00:08s
epoch 23 | loss: 0.56442 | eval_custom_logloss: 2.7435  |  0:00:08s
epoch 24 | loss: 0.54476 | eval_custom_logloss: 1.9309  |  0:00:09s
epoch 25 | loss: 0.51995 | eval_custom_logloss: 2.5495  |  0:00:09s
epoch 26 | loss: 0.51938 | eval_custom_logloss: 2.71131 |  0:00:10s
epoch 27 | loss: 0.50825 | eval_custom_logloss: 2.48876 |  0:00:10s
epoch 28 | loss: 0.51616 | eval_custom_logloss: 3.24271 |  0:00:10s
epoch 29 | loss: 0.49942 | eval_custom_logloss: 2.35272 |  0:00:11s
epoch 30 | loss: 0.47809 | eval_custom_logloss: 3.97267 |  0:00:11s
epoch 31 | loss: 0.45964 | eval_custom_logloss: 3.07883 |  0:00:11s
epoch 32 | loss: 0.48077 | eval_custom_logloss: 2.90171 |  0:00:12s
epoch 33 | loss: 0.46324 | eval_custom_logloss: 2.91819 |  0:00:12s
epoch 34 | loss: 0.43734 | eval_custom_logloss: 3.69535 |  0:00:13s
epoch 35 | loss: 0.45196 | eval_custom_logloss: 2.82228 |  0:00:13s
epoch 36 | loss: 0.45664 | eval_custom_logloss: 2.30392 |  0:00:13s
epoch 37 | loss: 0.43535 | eval_custom_logloss: 2.10025 |  0:00:14s
epoch 38 | loss: 0.43151 | eval_custom_logloss: 1.85854 |  0:00:14s
epoch 39 | loss: 0.4271  | eval_custom_logloss: 3.26675 |  0:00:14s
epoch 40 | loss: 0.42227 | eval_custom_logloss: 2.74316 |  0:00:15s
epoch 41 | loss: 0.50094 | eval_custom_logloss: 2.82089 |  0:00:15s
epoch 42 | loss: 0.49088 | eval_custom_logloss: 2.59397 |  0:00:16s
epoch 43 | loss: 0.52266 | eval_custom_logloss: 2.39981 |  0:00:16s
epoch 44 | loss: 0.47226 | eval_custom_logloss: 2.65746 |  0:00:16s
epoch 45 | loss: 0.45454 | eval_custom_logloss: 1.85105 |  0:00:17s
epoch 46 | loss: 0.44738 | eval_custom_logloss: 2.41572 |  0:00:17s
epoch 47 | loss: 0.45346 | eval_custom_logloss: 2.34763 |  0:00:18s
epoch 48 | loss: 0.40806 | eval_custom_logloss: 2.67169 |  0:00:18s
epoch 49 | loss: 0.40346 | eval_custom_logloss: 2.8096  |  0:00:19s
epoch 50 | loss: 0.40616 | eval_custom_logloss: 2.34414 |  0:00:19s
epoch 51 | loss: 0.40906 | eval_custom_logloss: 2.27526 |  0:00:19s
epoch 52 | loss: 0.38825 | eval_custom_logloss: 3.1258  |  0:00:20s
epoch 53 | loss: 0.37394 | eval_custom_logloss: 2.64673 |  0:00:20s
epoch 54 | loss: 0.41917 | eval_custom_logloss: 2.55511 |  0:00:21s
epoch 55 | loss: 0.37552 | eval_custom_logloss: 2.38151 |  0:00:21s
epoch 56 | loss: 0.36007 | eval_custom_logloss: 2.43777 |  0:00:22s
epoch 57 | loss: 0.34617 | eval_custom_logloss: 2.89984 |  0:00:22s
epoch 58 | loss: 0.34097 | eval_custom_logloss: 3.61754 |  0:00:23s
epoch 59 | loss: 0.31734 | eval_custom_logloss: 4.03415 |  0:00:23s
epoch 60 | loss: 0.32958 | eval_custom_logloss: 4.2111  |  0:00:23s
epoch 61 | loss: 0.34272 | eval_custom_logloss: 3.78929 |  0:00:24s
epoch 62 | loss: 0.29519 | eval_custom_logloss: 2.95913 |  0:00:24s
epoch 63 | loss: 0.33536 | eval_custom_logloss: 3.24337 |  0:00:25s
epoch 64 | loss: 0.32287 | eval_custom_logloss: 2.95037 |  0:00:25s
epoch 65 | loss: 0.32561 | eval_custom_logloss: 2.55631 |  0:00:26s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 1.85105
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.6891333333333334, 'Log Loss - std': 0.12619435627457962} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.08204 | eval_custom_logloss: 2.86383 |  0:00:00s
epoch 1  | loss: 0.8739  | eval_custom_logloss: 4.16929 |  0:00:00s
epoch 2  | loss: 0.81436 | eval_custom_logloss: 3.95066 |  0:00:01s
epoch 3  | loss: 0.74962 | eval_custom_logloss: 5.82352 |  0:00:01s
epoch 4  | loss: 0.76664 | eval_custom_logloss: 6.10484 |  0:00:01s
epoch 5  | loss: 0.72396 | eval_custom_logloss: 6.76289 |  0:00:02s
epoch 6  | loss: 0.67179 | eval_custom_logloss: 5.77916 |  0:00:02s
epoch 7  | loss: 0.63683 | eval_custom_logloss: 4.36101 |  0:00:03s
epoch 8  | loss: 0.6288  | eval_custom_logloss: 4.86967 |  0:00:03s
epoch 9  | loss: 0.60211 | eval_custom_logloss: 4.75727 |  0:00:03s
epoch 10 | loss: 0.55502 | eval_custom_logloss: 4.75941 |  0:00:04s
epoch 11 | loss: 0.54031 | eval_custom_logloss: 4.83983 |  0:00:04s
epoch 12 | loss: 0.55254 | eval_custom_logloss: 3.95572 |  0:00:04s
epoch 13 | loss: 0.52042 | eval_custom_logloss: 4.25174 |  0:00:05s
epoch 14 | loss: 0.52639 | eval_custom_logloss: 5.68266 |  0:00:05s
epoch 15 | loss: 0.52948 | eval_custom_logloss: 5.41773 |  0:00:05s
epoch 16 | loss: 0.48647 | eval_custom_logloss: 4.4973  |  0:00:06s
epoch 17 | loss: 0.43999 | eval_custom_logloss: 3.81791 |  0:00:06s
epoch 18 | loss: 0.44423 | eval_custom_logloss: 4.61231 |  0:00:07s
epoch 19 | loss: 0.44739 | eval_custom_logloss: 3.97428 |  0:00:07s
epoch 20 | loss: 0.42098 | eval_custom_logloss: 4.13747 |  0:00:07s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.86383
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.89445, 'Log Loss - std': 0.3720330126480714} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.03145 | eval_custom_logloss: 3.39936 |  0:00:00s
epoch 1  | loss: 0.90821 | eval_custom_logloss: 5.06419 |  0:00:00s
epoch 2  | loss: 0.82717 | eval_custom_logloss: 4.51986 |  0:00:00s
epoch 3  | loss: 0.77826 | eval_custom_logloss: 5.08263 |  0:00:01s
epoch 4  | loss: 0.74024 | eval_custom_logloss: 3.76756 |  0:00:01s
epoch 5  | loss: 0.72803 | eval_custom_logloss: 3.08417 |  0:00:01s
epoch 6  | loss: 0.68793 | eval_custom_logloss: 4.62332 |  0:00:02s
epoch 7  | loss: 0.67144 | eval_custom_logloss: 7.44435 |  0:00:02s
epoch 8  | loss: 0.65746 | eval_custom_logloss: 5.79501 |  0:00:02s
epoch 9  | loss: 0.66178 | eval_custom_logloss: 4.78714 |  0:00:03s
epoch 10 | loss: 0.60736 | eval_custom_logloss: 6.59061 |  0:00:03s
epoch 11 | loss: 0.63045 | eval_custom_logloss: 5.44574 |  0:00:04s
epoch 12 | loss: 0.59292 | eval_custom_logloss: 5.2886  |  0:00:04s
epoch 13 | loss: 0.58249 | eval_custom_logloss: 5.05875 |  0:00:04s
epoch 14 | loss: 0.56644 | eval_custom_logloss: 4.63721 |  0:00:05s
epoch 15 | loss: 0.53965 | eval_custom_logloss: 4.40378 |  0:00:05s
epoch 16 | loss: 0.54095 | eval_custom_logloss: 5.35409 |  0:00:05s
epoch 17 | loss: 0.53855 | eval_custom_logloss: 5.20548 |  0:00:06s
epoch 18 | loss: 0.50912 | eval_custom_logloss: 5.03156 |  0:00:06s
epoch 19 | loss: 0.50859 | eval_custom_logloss: 3.83622 |  0:00:06s
epoch 20 | loss: 0.50969 | eval_custom_logloss: 4.08851 |  0:00:07s
epoch 21 | loss: 0.48917 | eval_custom_logloss: 4.30681 |  0:00:07s
epoch 22 | loss: 0.4895  | eval_custom_logloss: 4.4871  |  0:00:08s
epoch 23 | loss: 0.47339 | eval_custom_logloss: 3.09283 |  0:00:08s
epoch 24 | loss: 0.48225 | eval_custom_logloss: 2.85418 |  0:00:09s
epoch 25 | loss: 0.49213 | eval_custom_logloss: 2.43015 |  0:00:09s
epoch 26 | loss: 0.47104 | eval_custom_logloss: 3.56204 |  0:00:10s
epoch 27 | loss: 0.46905 | eval_custom_logloss: 2.67323 |  0:00:10s
epoch 28 | loss: 0.45525 | eval_custom_logloss: 2.62076 |  0:00:11s
epoch 29 | loss: 0.45137 | eval_custom_logloss: 2.53967 |  0:00:11s
epoch 30 | loss: 0.43568 | eval_custom_logloss: 2.26495 |  0:00:11s
epoch 31 | loss: 0.41732 | eval_custom_logloss: 2.03498 |  0:00:12s
epoch 32 | loss: 0.42276 | eval_custom_logloss: 3.11653 |  0:00:12s
epoch 33 | loss: 0.39913 | eval_custom_logloss: 2.67417 |  0:00:13s
epoch 34 | loss: 0.38479 | eval_custom_logloss: 2.98157 |  0:00:13s
epoch 35 | loss: 0.38524 | eval_custom_logloss: 2.41261 |  0:00:14s
epoch 36 | loss: 0.41084 | eval_custom_logloss: 2.69312 |  0:00:14s
epoch 37 | loss: 0.38886 | eval_custom_logloss: 3.59982 |  0:00:14s
epoch 38 | loss: 0.40174 | eval_custom_logloss: 3.93254 |  0:00:15s
epoch 39 | loss: 0.42782 | eval_custom_logloss: 4.43464 |  0:00:15s
epoch 40 | loss: 0.43426 | eval_custom_logloss: 3.29752 |  0:00:16s
epoch 41 | loss: 0.42881 | eval_custom_logloss: 3.10396 |  0:00:16s
epoch 42 | loss: 0.3961  | eval_custom_logloss: 3.3849  |  0:00:17s
epoch 43 | loss: 0.38932 | eval_custom_logloss: 2.96768 |  0:00:17s
epoch 44 | loss: 0.34609 | eval_custom_logloss: 3.64465 |  0:00:17s
epoch 45 | loss: 0.35229 | eval_custom_logloss: 4.1879  |  0:00:18s
epoch 46 | loss: 0.3608  | eval_custom_logloss: 4.11723 |  0:00:18s
epoch 47 | loss: 0.32654 | eval_custom_logloss: 4.33663 |  0:00:19s
epoch 48 | loss: 0.34449 | eval_custom_logloss: 3.47655 |  0:00:19s
epoch 49 | loss: 0.34812 | eval_custom_logloss: 3.54927 |  0:00:20s
epoch 50 | loss: 0.34927 | eval_custom_logloss: 3.84147 |  0:00:20s
epoch 51 | loss: 0.3344  | eval_custom_logloss: 3.50877 |  0:00:21s

Early stopping occurred at epoch 51 with best_epoch = 31 and best_eval_custom_logloss = 2.03498
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8950399999999998, 'Log Loss - std': 0.33275853467642275} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 1.8950399999999998 and parameters: {'n_d': 14, 'n_steps': 3, 'gamma': 1.6695338236276092, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011704656525214727, 'mask_type': 'entmax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax', 'n_a': 46, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.55057 | eval_custom_logloss: 5.9802  |  0:00:00s
epoch 1  | loss: 1.33764 | eval_custom_logloss: 8.15293 |  0:00:01s
epoch 2  | loss: 1.11173 | eval_custom_logloss: 6.9515  |  0:00:01s
epoch 3  | loss: 1.21328 | eval_custom_logloss: 7.67989 |  0:00:02s
epoch 4  | loss: 1.12252 | eval_custom_logloss: 7.30472 |  0:00:02s
epoch 5  | loss: 1.2529  | eval_custom_logloss: 7.8512  |  0:00:03s
epoch 6  | loss: 1.04371 | eval_custom_logloss: 5.61122 |  0:00:04s
epoch 7  | loss: 0.88292 | eval_custom_logloss: 4.78793 |  0:00:04s
epoch 8  | loss: 0.86507 | eval_custom_logloss: 6.56957 |  0:00:05s
epoch 9  | loss: 0.859   | eval_custom_logloss: 3.86479 |  0:00:05s
epoch 10 | loss: 0.70224 | eval_custom_logloss: 4.45563 |  0:00:06s
epoch 11 | loss: 0.70727 | eval_custom_logloss: 3.36122 |  0:00:06s
epoch 12 | loss: 0.68198 | eval_custom_logloss: 3.32592 |  0:00:07s
epoch 13 | loss: 0.70314 | eval_custom_logloss: 4.04661 |  0:00:07s
epoch 14 | loss: 0.70417 | eval_custom_logloss: 3.39045 |  0:00:08s
epoch 15 | loss: 0.64895 | eval_custom_logloss: 3.14844 |  0:00:09s
epoch 16 | loss: 0.69881 | eval_custom_logloss: 3.45302 |  0:00:09s
epoch 17 | loss: 0.63072 | eval_custom_logloss: 2.48972 |  0:00:10s
epoch 18 | loss: 0.61577 | eval_custom_logloss: 2.55701 |  0:00:10s
epoch 19 | loss: 0.61522 | eval_custom_logloss: 3.21766 |  0:00:11s
epoch 20 | loss: 0.61039 | eval_custom_logloss: 3.23736 |  0:00:11s
epoch 21 | loss: 0.54939 | eval_custom_logloss: 2.35166 |  0:00:12s
epoch 22 | loss: 0.57693 | eval_custom_logloss: 2.38315 |  0:00:12s
epoch 23 | loss: 0.5511  | eval_custom_logloss: 2.4162  |  0:00:13s
epoch 24 | loss: 0.51361 | eval_custom_logloss: 2.93924 |  0:00:13s
epoch 25 | loss: 0.49607 | eval_custom_logloss: 2.90934 |  0:00:14s
epoch 26 | loss: 0.48944 | eval_custom_logloss: 2.79001 |  0:00:15s
epoch 27 | loss: 0.4655  | eval_custom_logloss: 2.57289 |  0:00:15s
epoch 28 | loss: 0.51992 | eval_custom_logloss: 1.54961 |  0:00:16s
epoch 29 | loss: 0.46982 | eval_custom_logloss: 1.81028 |  0:00:16s
epoch 30 | loss: 0.44278 | eval_custom_logloss: 2.00915 |  0:00:17s
epoch 31 | loss: 0.45777 | eval_custom_logloss: 2.06861 |  0:00:17s
epoch 32 | loss: 0.47017 | eval_custom_logloss: 1.53425 |  0:00:18s
epoch 33 | loss: 0.47757 | eval_custom_logloss: 2.03057 |  0:00:18s
epoch 34 | loss: 0.44471 | eval_custom_logloss: 2.24782 |  0:00:19s
epoch 35 | loss: 0.45226 | eval_custom_logloss: 1.61785 |  0:00:19s
epoch 36 | loss: 0.45181 | eval_custom_logloss: 1.74363 |  0:00:20s
epoch 37 | loss: 0.43802 | eval_custom_logloss: 1.81063 |  0:00:21s
epoch 38 | loss: 0.42042 | eval_custom_logloss: 1.33077 |  0:00:21s
epoch 39 | loss: 0.38826 | eval_custom_logloss: 1.12598 |  0:00:22s
epoch 40 | loss: 0.49489 | eval_custom_logloss: 1.18121 |  0:00:22s
epoch 41 | loss: 0.43735 | eval_custom_logloss: 1.31052 |  0:00:23s
epoch 42 | loss: 0.4206  | eval_custom_logloss: 1.30193 |  0:00:23s
epoch 43 | loss: 0.37189 | eval_custom_logloss: 1.61059 |  0:00:24s
epoch 44 | loss: 0.35874 | eval_custom_logloss: 1.62068 |  0:00:24s
epoch 45 | loss: 0.37516 | eval_custom_logloss: 1.57888 |  0:00:25s
epoch 46 | loss: 0.39623 | eval_custom_logloss: 1.17499 |  0:00:25s
epoch 47 | loss: 0.40064 | eval_custom_logloss: 1.00685 |  0:00:26s
epoch 48 | loss: 0.38547 | eval_custom_logloss: 1.18691 |  0:00:27s
epoch 49 | loss: 0.34936 | eval_custom_logloss: 1.21625 |  0:00:27s
epoch 50 | loss: 0.37576 | eval_custom_logloss: 1.09964 |  0:00:28s
epoch 51 | loss: 0.42872 | eval_custom_logloss: 0.7283  |  0:00:28s
epoch 52 | loss: 0.40937 | eval_custom_logloss: 0.85559 |  0:00:29s
epoch 53 | loss: 0.40147 | eval_custom_logloss: 0.88351 |  0:00:29s
epoch 54 | loss: 0.39218 | eval_custom_logloss: 1.00601 |  0:00:30s
epoch 55 | loss: 0.38551 | eval_custom_logloss: 0.81297 |  0:00:30s
epoch 56 | loss: 0.38323 | eval_custom_logloss: 0.80163 |  0:00:31s
epoch 57 | loss: 0.38865 | eval_custom_logloss: 0.91404 |  0:00:31s
epoch 58 | loss: 0.35202 | eval_custom_logloss: 0.76372 |  0:00:32s
epoch 59 | loss: 0.34844 | eval_custom_logloss: 0.77115 |  0:00:32s
epoch 60 | loss: 0.33781 | eval_custom_logloss: 0.837   |  0:00:33s
epoch 61 | loss: 0.39685 | eval_custom_logloss: 0.65893 |  0:00:34s
epoch 62 | loss: 0.34152 | eval_custom_logloss: 0.84473 |  0:00:34s
epoch 63 | loss: 0.32389 | eval_custom_logloss: 0.74041 |  0:00:35s
epoch 64 | loss: 0.33085 | eval_custom_logloss: 0.70787 |  0:00:35s
epoch 65 | loss: 0.32182 | eval_custom_logloss: 0.82412 |  0:00:36s
epoch 66 | loss: 0.33919 | eval_custom_logloss: 0.72909 |  0:00:36s
epoch 67 | loss: 0.32107 | eval_custom_logloss: 0.70112 |  0:00:37s
epoch 68 | loss: 0.31262 | eval_custom_logloss: 0.72717 |  0:00:37s
epoch 69 | loss: 0.34888 | eval_custom_logloss: 0.73742 |  0:00:38s
epoch 70 | loss: 0.36354 | eval_custom_logloss: 0.66934 |  0:00:38s
epoch 71 | loss: 0.39298 | eval_custom_logloss: 0.62355 |  0:00:39s
epoch 72 | loss: 0.34983 | eval_custom_logloss: 0.75606 |  0:00:39s
epoch 73 | loss: 0.32203 | eval_custom_logloss: 0.7363  |  0:00:40s
epoch 74 | loss: 0.31917 | eval_custom_logloss: 0.8315  |  0:00:41s
epoch 75 | loss: 0.28138 | eval_custom_logloss: 0.92085 |  0:00:41s
epoch 76 | loss: 0.27217 | eval_custom_logloss: 0.84585 |  0:00:42s
epoch 77 | loss: 0.25861 | eval_custom_logloss: 0.80273 |  0:00:42s
epoch 78 | loss: 0.2691  | eval_custom_logloss: 0.87806 |  0:00:43s
epoch 79 | loss: 0.27512 | eval_custom_logloss: 0.83831 |  0:00:43s
epoch 80 | loss: 0.27153 | eval_custom_logloss: 0.76147 |  0:00:44s
epoch 81 | loss: 0.27083 | eval_custom_logloss: 0.72874 |  0:00:44s
epoch 82 | loss: 0.24071 | eval_custom_logloss: 0.74367 |  0:00:45s
epoch 83 | loss: 0.26214 | eval_custom_logloss: 0.79962 |  0:00:45s
epoch 84 | loss: 0.24246 | eval_custom_logloss: 0.84419 |  0:00:46s
epoch 85 | loss: 0.27014 | eval_custom_logloss: 0.79866 |  0:00:46s
epoch 86 | loss: 0.29765 | eval_custom_logloss: 0.61411 |  0:00:47s
epoch 87 | loss: 0.28693 | eval_custom_logloss: 0.76056 |  0:00:47s
epoch 88 | loss: 0.28647 | eval_custom_logloss: 0.69684 |  0:00:48s
epoch 89 | loss: 0.26186 | eval_custom_logloss: 0.73071 |  0:00:49s
epoch 90 | loss: 0.24443 | eval_custom_logloss: 0.77696 |  0:00:49s
epoch 91 | loss: 0.26469 | eval_custom_logloss: 0.82839 |  0:00:50s
epoch 92 | loss: 0.28754 | eval_custom_logloss: 0.63029 |  0:00:50s
epoch 93 | loss: 0.24632 | eval_custom_logloss: 0.64816 |  0:00:51s
epoch 94 | loss: 0.23904 | eval_custom_logloss: 0.68419 |  0:00:51s
epoch 95 | loss: 0.21475 | eval_custom_logloss: 0.70474 |  0:00:52s
epoch 96 | loss: 0.22983 | eval_custom_logloss: 0.68443 |  0:00:52s
epoch 97 | loss: 0.24341 | eval_custom_logloss: 0.62407 |  0:00:53s
epoch 98 | loss: 0.23484 | eval_custom_logloss: 0.70514 |  0:00:53s
epoch 99 | loss: 0.2364  | eval_custom_logloss: 0.62619 |  0:00:54s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.61411
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6024, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax', 'n_a': 46, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.37679 | eval_custom_logloss: 6.67731 |  0:00:00s
epoch 1  | loss: 1.32304 | eval_custom_logloss: 7.93809 |  0:00:01s
epoch 2  | loss: 1.08126 | eval_custom_logloss: 6.41552 |  0:00:01s
epoch 3  | loss: 0.89584 | eval_custom_logloss: 7.41128 |  0:00:02s
epoch 4  | loss: 0.86551 | eval_custom_logloss: 6.90968 |  0:00:02s
epoch 5  | loss: 1.30939 | eval_custom_logloss: 7.69606 |  0:00:03s
epoch 6  | loss: 0.86791 | eval_custom_logloss: 3.76146 |  0:00:03s
epoch 7  | loss: 0.78449 | eval_custom_logloss: 5.16063 |  0:00:04s
epoch 8  | loss: 0.77034 | eval_custom_logloss: 3.58114 |  0:00:05s
epoch 9  | loss: 0.782   | eval_custom_logloss: 6.12532 |  0:00:05s
epoch 10 | loss: 0.70998 | eval_custom_logloss: 4.15139 |  0:00:06s
epoch 11 | loss: 0.69373 | eval_custom_logloss: 4.90217 |  0:00:06s
epoch 12 | loss: 0.66923 | eval_custom_logloss: 5.66117 |  0:00:07s
epoch 13 | loss: 0.66649 | eval_custom_logloss: 5.31854 |  0:00:07s
epoch 14 | loss: 0.65019 | eval_custom_logloss: 5.74447 |  0:00:08s
epoch 15 | loss: 0.60716 | eval_custom_logloss: 5.12279 |  0:00:08s
epoch 16 | loss: 0.6129  | eval_custom_logloss: 3.62308 |  0:00:09s
epoch 17 | loss: 0.60714 | eval_custom_logloss: 2.78036 |  0:00:10s
epoch 18 | loss: 0.57474 | eval_custom_logloss: 3.53809 |  0:00:10s
epoch 19 | loss: 0.55975 | eval_custom_logloss: 3.12468 |  0:00:11s
epoch 20 | loss: 0.54392 | eval_custom_logloss: 2.63598 |  0:00:11s
epoch 21 | loss: 0.54621 | eval_custom_logloss: 2.59359 |  0:00:12s
epoch 22 | loss: 0.5368  | eval_custom_logloss: 1.8871  |  0:00:12s
epoch 23 | loss: 0.50795 | eval_custom_logloss: 3.32533 |  0:00:13s
epoch 24 | loss: 0.51198 | eval_custom_logloss: 2.07212 |  0:00:14s
epoch 25 | loss: 0.52184 | eval_custom_logloss: 2.67002 |  0:00:14s
epoch 26 | loss: 0.49801 | eval_custom_logloss: 2.35276 |  0:00:15s
epoch 27 | loss: 0.46301 | eval_custom_logloss: 2.08261 |  0:00:15s
epoch 28 | loss: 0.46873 | eval_custom_logloss: 2.09869 |  0:00:16s
epoch 29 | loss: 0.44814 | eval_custom_logloss: 1.83257 |  0:00:16s
epoch 30 | loss: 0.47302 | eval_custom_logloss: 1.56843 |  0:00:17s
epoch 31 | loss: 0.45423 | eval_custom_logloss: 1.72357 |  0:00:17s
epoch 32 | loss: 0.4414  | eval_custom_logloss: 1.71203 |  0:00:18s
epoch 33 | loss: 0.43466 | eval_custom_logloss: 2.11448 |  0:00:18s
epoch 34 | loss: 0.55517 | eval_custom_logloss: 1.25627 |  0:00:19s
epoch 35 | loss: 0.50584 | eval_custom_logloss: 1.46428 |  0:00:20s
epoch 36 | loss: 0.50162 | eval_custom_logloss: 1.46035 |  0:00:20s
epoch 37 | loss: 0.4496  | eval_custom_logloss: 1.49197 |  0:00:21s
epoch 38 | loss: 0.44087 | eval_custom_logloss: 1.31785 |  0:00:21s
epoch 39 | loss: 0.42859 | eval_custom_logloss: 1.20444 |  0:00:22s
epoch 40 | loss: 0.41875 | eval_custom_logloss: 1.23662 |  0:00:22s
epoch 41 | loss: 0.44789 | eval_custom_logloss: 1.23752 |  0:00:23s
epoch 42 | loss: 0.48073 | eval_custom_logloss: 1.51384 |  0:00:23s
epoch 43 | loss: 0.48329 | eval_custom_logloss: 1.1709  |  0:00:24s
epoch 44 | loss: 0.466   | eval_custom_logloss: 1.30931 |  0:00:25s
epoch 45 | loss: 0.51191 | eval_custom_logloss: 1.52383 |  0:00:25s
epoch 46 | loss: 0.46854 | eval_custom_logloss: 1.4088  |  0:00:26s
epoch 47 | loss: 0.45194 | eval_custom_logloss: 1.11469 |  0:00:26s
epoch 48 | loss: 0.44793 | eval_custom_logloss: 1.11101 |  0:00:27s
epoch 49 | loss: 0.42084 | eval_custom_logloss: 1.20746 |  0:00:27s
epoch 50 | loss: 0.43171 | eval_custom_logloss: 1.14972 |  0:00:28s
epoch 51 | loss: 0.47597 | eval_custom_logloss: 1.4841  |  0:00:28s
epoch 52 | loss: 0.4367  | eval_custom_logloss: 1.60677 |  0:00:29s
epoch 53 | loss: 0.40261 | eval_custom_logloss: 2.15037 |  0:00:29s
epoch 54 | loss: 0.37543 | eval_custom_logloss: 2.19179 |  0:00:30s
epoch 55 | loss: 0.43993 | eval_custom_logloss: 1.74956 |  0:00:31s
epoch 56 | loss: 0.46853 | eval_custom_logloss: 1.38149 |  0:00:31s
epoch 57 | loss: 0.4696  | eval_custom_logloss: 1.48243 |  0:00:32s
epoch 58 | loss: 0.47221 | eval_custom_logloss: 1.55524 |  0:00:32s
epoch 59 | loss: 0.44596 | eval_custom_logloss: 1.61102 |  0:00:33s
epoch 60 | loss: 0.41888 | eval_custom_logloss: 1.34446 |  0:00:33s
epoch 61 | loss: 0.42165 | eval_custom_logloss: 1.29712 |  0:00:34s
epoch 62 | loss: 0.41944 | eval_custom_logloss: 1.01275 |  0:00:34s
epoch 63 | loss: 0.42984 | eval_custom_logloss: 0.86411 |  0:00:35s
epoch 64 | loss: 0.41833 | eval_custom_logloss: 1.16482 |  0:00:36s
epoch 65 | loss: 0.43347 | eval_custom_logloss: 1.28037 |  0:00:36s
epoch 66 | loss: 0.41913 | eval_custom_logloss: 2.01983 |  0:00:37s
epoch 67 | loss: 0.44841 | eval_custom_logloss: 2.61512 |  0:00:37s
epoch 68 | loss: 0.41126 | eval_custom_logloss: 2.11808 |  0:00:38s
epoch 69 | loss: 0.40095 | eval_custom_logloss: 1.56496 |  0:00:38s
epoch 70 | loss: 0.39666 | eval_custom_logloss: 1.37404 |  0:00:39s
epoch 71 | loss: 0.35562 | eval_custom_logloss: 1.14351 |  0:00:40s
epoch 72 | loss: 0.3761  | eval_custom_logloss: 1.28431 |  0:00:40s
epoch 73 | loss: 0.4223  | eval_custom_logloss: 2.28426 |  0:00:41s
epoch 74 | loss: 0.43441 | eval_custom_logloss: 3.61941 |  0:00:41s
epoch 75 | loss: 0.42606 | eval_custom_logloss: 3.32584 |  0:00:42s
epoch 76 | loss: 0.40766 | eval_custom_logloss: 1.67953 |  0:00:42s
epoch 77 | loss: 0.40455 | eval_custom_logloss: 1.6978  |  0:00:43s
epoch 78 | loss: 0.40982 | eval_custom_logloss: 1.55594 |  0:00:43s
epoch 79 | loss: 0.38807 | eval_custom_logloss: 1.41311 |  0:00:44s
epoch 80 | loss: 0.43954 | eval_custom_logloss: 1.31261 |  0:00:45s
epoch 81 | loss: 0.39551 | eval_custom_logloss: 1.26376 |  0:00:45s
epoch 82 | loss: 0.39947 | eval_custom_logloss: 1.18314 |  0:00:46s
epoch 83 | loss: 0.38562 | eval_custom_logloss: 1.37684 |  0:00:46s

Early stopping occurred at epoch 83 with best_epoch = 63 and best_eval_custom_logloss = 0.86411
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7204, 'Log Loss - std': 0.118} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax', 'n_a': 46, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.47951 | eval_custom_logloss: 9.05898 |  0:00:00s
epoch 1  | loss: 1.56873 | eval_custom_logloss: 6.88262 |  0:00:01s
epoch 2  | loss: 1.26686 | eval_custom_logloss: 6.34758 |  0:00:01s
epoch 3  | loss: 1.49417 | eval_custom_logloss: 7.93926 |  0:00:02s
epoch 4  | loss: 1.10957 | eval_custom_logloss: 7.16066 |  0:00:02s
epoch 5  | loss: 0.88796 | eval_custom_logloss: 6.53429 |  0:00:03s
epoch 6  | loss: 0.85773 | eval_custom_logloss: 4.44146 |  0:00:03s
epoch 7  | loss: 0.85886 | eval_custom_logloss: 5.78167 |  0:00:04s
epoch 8  | loss: 0.90759 | eval_custom_logloss: 5.23997 |  0:00:05s
epoch 9  | loss: 0.79733 | eval_custom_logloss: 4.09563 |  0:00:05s
epoch 10 | loss: 0.94074 | eval_custom_logloss: 5.05192 |  0:00:06s
epoch 11 | loss: 0.759   | eval_custom_logloss: 6.41812 |  0:00:06s
epoch 12 | loss: 0.68719 | eval_custom_logloss: 4.32687 |  0:00:07s
epoch 13 | loss: 0.67879 | eval_custom_logloss: 3.52942 |  0:00:07s
epoch 14 | loss: 0.68526 | eval_custom_logloss: 1.4647  |  0:00:08s
epoch 15 | loss: 0.64809 | eval_custom_logloss: 1.8803  |  0:00:08s
epoch 16 | loss: 0.63645 | eval_custom_logloss: 2.95944 |  0:00:09s
epoch 17 | loss: 0.61428 | eval_custom_logloss: 2.88998 |  0:00:10s
epoch 18 | loss: 0.61293 | eval_custom_logloss: 2.14983 |  0:00:10s
epoch 19 | loss: 0.59231 | eval_custom_logloss: 1.93201 |  0:00:11s
epoch 20 | loss: 0.57775 | eval_custom_logloss: 1.88514 |  0:00:11s
epoch 21 | loss: 0.57435 | eval_custom_logloss: 1.96487 |  0:00:12s
epoch 22 | loss: 0.61307 | eval_custom_logloss: 1.51408 |  0:00:12s
epoch 23 | loss: 0.60503 | eval_custom_logloss: 0.86533 |  0:00:13s
epoch 24 | loss: 0.62673 | eval_custom_logloss: 1.10726 |  0:00:13s
epoch 25 | loss: 0.57339 | eval_custom_logloss: 1.26881 |  0:00:14s
epoch 26 | loss: 0.58982 | eval_custom_logloss: 1.44711 |  0:00:14s
epoch 27 | loss: 0.59484 | eval_custom_logloss: 1.99061 |  0:00:15s
epoch 28 | loss: 0.61302 | eval_custom_logloss: 2.64179 |  0:00:16s
epoch 29 | loss: 0.58968 | eval_custom_logloss: 1.54299 |  0:00:16s
epoch 30 | loss: 0.59877 | eval_custom_logloss: 1.61671 |  0:00:17s
epoch 31 | loss: 0.60263 | eval_custom_logloss: 1.29727 |  0:00:17s
epoch 32 | loss: 0.60799 | eval_custom_logloss: 1.97793 |  0:00:18s
epoch 33 | loss: 0.60932 | eval_custom_logloss: 2.67783 |  0:00:18s
epoch 34 | loss: 0.56489 | eval_custom_logloss: 3.65794 |  0:00:19s
epoch 35 | loss: 0.55765 | eval_custom_logloss: 4.05801 |  0:00:19s
epoch 36 | loss: 0.64153 | eval_custom_logloss: 2.88428 |  0:00:20s
epoch 37 | loss: 0.66301 | eval_custom_logloss: 1.82929 |  0:00:20s
epoch 38 | loss: 0.64524 | eval_custom_logloss: 1.65622 |  0:00:21s
epoch 39 | loss: 0.63161 | eval_custom_logloss: 1.26624 |  0:00:22s
epoch 40 | loss: 0.6073  | eval_custom_logloss: 0.93217 |  0:00:22s
epoch 41 | loss: 0.60041 | eval_custom_logloss: 1.20087 |  0:00:23s
epoch 42 | loss: 0.59875 | eval_custom_logloss: 1.89526 |  0:00:23s
epoch 43 | loss: 0.58845 | eval_custom_logloss: 1.70748 |  0:00:24s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.86533
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7686999999999999, 'Log Loss - std': 0.11810354214275989} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax', 'n_a': 46, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.57934 | eval_custom_logloss: 8.3732  |  0:00:00s
epoch 1  | loss: 1.33272 | eval_custom_logloss: 7.93177 |  0:00:01s
epoch 2  | loss: 1.13693 | eval_custom_logloss: 7.63344 |  0:00:01s
epoch 3  | loss: 0.98941 | eval_custom_logloss: 7.01843 |  0:00:02s
epoch 4  | loss: 1.00825 | eval_custom_logloss: 7.58587 |  0:00:02s
epoch 5  | loss: 1.39918 | eval_custom_logloss: 5.85609 |  0:00:03s
epoch 6  | loss: 0.91531 | eval_custom_logloss: 8.14646 |  0:00:03s
epoch 7  | loss: 1.02032 | eval_custom_logloss: 7.03981 |  0:00:04s
epoch 8  | loss: 0.86389 | eval_custom_logloss: 4.65495 |  0:00:05s
epoch 9  | loss: 0.78182 | eval_custom_logloss: 3.77296 |  0:00:05s
epoch 10 | loss: 0.69984 | eval_custom_logloss: 4.33439 |  0:00:06s
epoch 11 | loss: 0.71435 | eval_custom_logloss: 4.01766 |  0:00:06s
epoch 12 | loss: 0.72065 | eval_custom_logloss: 3.23942 |  0:00:07s
epoch 13 | loss: 0.6708  | eval_custom_logloss: 3.03006 |  0:00:07s
epoch 14 | loss: 0.67353 | eval_custom_logloss: 3.93885 |  0:00:08s
epoch 15 | loss: 0.65802 | eval_custom_logloss: 4.44289 |  0:00:08s
epoch 16 | loss: 0.63211 | eval_custom_logloss: 3.82956 |  0:00:09s
epoch 17 | loss: 0.64123 | eval_custom_logloss: 3.93261 |  0:00:09s
epoch 18 | loss: 0.63417 | eval_custom_logloss: 3.08831 |  0:00:10s
epoch 19 | loss: 0.63723 | eval_custom_logloss: 3.35132 |  0:00:11s
epoch 20 | loss: 0.66647 | eval_custom_logloss: 2.32608 |  0:00:11s
epoch 21 | loss: 0.59983 | eval_custom_logloss: 2.74272 |  0:00:12s
epoch 22 | loss: 0.61226 | eval_custom_logloss: 2.1451  |  0:00:12s
epoch 23 | loss: 0.60648 | eval_custom_logloss: 1.67878 |  0:00:13s
epoch 24 | loss: 0.59718 | eval_custom_logloss: 1.94015 |  0:00:13s
epoch 25 | loss: 0.60729 | eval_custom_logloss: 1.68367 |  0:00:14s
epoch 26 | loss: 0.60256 | eval_custom_logloss: 1.75991 |  0:00:14s
epoch 27 | loss: 0.60645 | eval_custom_logloss: 1.52413 |  0:00:15s
epoch 28 | loss: 0.62754 | eval_custom_logloss: 2.23483 |  0:00:16s
epoch 29 | loss: 0.60685 | eval_custom_logloss: 1.86003 |  0:00:16s
epoch 30 | loss: 0.59833 | eval_custom_logloss: 1.76726 |  0:00:17s
epoch 31 | loss: 0.53232 | eval_custom_logloss: 1.87319 |  0:00:17s
epoch 32 | loss: 0.53882 | eval_custom_logloss: 1.99805 |  0:00:18s
epoch 33 | loss: 0.55487 | eval_custom_logloss: 1.90505 |  0:00:18s
epoch 34 | loss: 0.52783 | eval_custom_logloss: 1.6839  |  0:00:19s
epoch 35 | loss: 0.50075 | eval_custom_logloss: 1.75593 |  0:00:19s
epoch 36 | loss: 0.50201 | eval_custom_logloss: 1.70841 |  0:00:20s
epoch 37 | loss: 0.50828 | eval_custom_logloss: 1.27817 |  0:00:20s
epoch 38 | loss: 0.54164 | eval_custom_logloss: 1.34644 |  0:00:21s
epoch 39 | loss: 0.48298 | eval_custom_logloss: 1.51911 |  0:00:21s
epoch 40 | loss: 0.46892 | eval_custom_logloss: 1.41724 |  0:00:22s
epoch 41 | loss: 0.49315 | eval_custom_logloss: 1.06676 |  0:00:23s
epoch 42 | loss: 0.45938 | eval_custom_logloss: 1.28785 |  0:00:23s
epoch 43 | loss: 0.48063 | eval_custom_logloss: 1.47872 |  0:00:24s
epoch 44 | loss: 0.45956 | eval_custom_logloss: 1.52316 |  0:00:24s
epoch 45 | loss: 0.47543 | eval_custom_logloss: 1.503   |  0:00:25s
epoch 46 | loss: 0.44466 | eval_custom_logloss: 1.82964 |  0:00:25s
epoch 47 | loss: 0.4565  | eval_custom_logloss: 1.80409 |  0:00:26s
epoch 48 | loss: 0.48135 | eval_custom_logloss: 1.86316 |  0:00:26s
epoch 49 | loss: 0.52733 | eval_custom_logloss: 1.90456 |  0:00:27s
epoch 50 | loss: 0.45945 | eval_custom_logloss: 1.55874 |  0:00:27s
epoch 51 | loss: 0.45848 | eval_custom_logloss: 1.32407 |  0:00:28s
epoch 52 | loss: 0.45995 | eval_custom_logloss: 1.88547 |  0:00:29s
epoch 53 | loss: 0.44362 | eval_custom_logloss: 1.55832 |  0:00:29s
epoch 54 | loss: 0.46054 | eval_custom_logloss: 1.55755 |  0:00:30s
epoch 55 | loss: 0.45193 | eval_custom_logloss: 1.52991 |  0:00:30s
epoch 56 | loss: 0.45381 | eval_custom_logloss: 1.44386 |  0:00:31s
epoch 57 | loss: 0.45649 | eval_custom_logloss: 1.91076 |  0:00:31s
epoch 58 | loss: 0.50764 | eval_custom_logloss: 3.09523 |  0:00:32s
epoch 59 | loss: 0.51276 | eval_custom_logloss: 2.81817 |  0:00:32s
epoch 60 | loss: 0.48081 | eval_custom_logloss: 2.2713  |  0:00:33s
epoch 61 | loss: 0.52898 | eval_custom_logloss: 1.79765 |  0:00:33s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 1.06676
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.83315, 'Log Loss - std': 0.1514025841919483} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax', 'n_a': 46, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.77021 | eval_custom_logloss: 7.32071 |  0:00:00s
epoch 1  | loss: 1.37343 | eval_custom_logloss: 5.82891 |  0:00:01s
epoch 2  | loss: 1.2681  | eval_custom_logloss: 6.07162 |  0:00:01s
epoch 3  | loss: 1.01317 | eval_custom_logloss: 7.0528  |  0:00:02s
epoch 4  | loss: 0.90522 | eval_custom_logloss: 7.10111 |  0:00:02s
epoch 5  | loss: 0.95994 | eval_custom_logloss: 6.815   |  0:00:03s
epoch 6  | loss: 1.14535 | eval_custom_logloss: 6.89947 |  0:00:03s
epoch 7  | loss: 1.0216  | eval_custom_logloss: 3.15749 |  0:00:04s
epoch 8  | loss: 0.73568 | eval_custom_logloss: 3.00566 |  0:00:04s
epoch 9  | loss: 0.72867 | eval_custom_logloss: 3.39298 |  0:00:05s
epoch 10 | loss: 0.74545 | eval_custom_logloss: 4.30252 |  0:00:05s
epoch 11 | loss: 0.74865 | eval_custom_logloss: 4.19288 |  0:00:06s
epoch 12 | loss: 0.67537 | eval_custom_logloss: 2.88522 |  0:00:07s
epoch 13 | loss: 0.66129 | eval_custom_logloss: 4.33548 |  0:00:07s
epoch 14 | loss: 0.67098 | eval_custom_logloss: 5.098   |  0:00:08s
epoch 15 | loss: 0.63503 | eval_custom_logloss: 4.47398 |  0:00:08s
epoch 16 | loss: 0.61263 | eval_custom_logloss: 2.23969 |  0:00:09s
epoch 17 | loss: 0.60458 | eval_custom_logloss: 2.10337 |  0:00:09s
epoch 18 | loss: 0.6281  | eval_custom_logloss: 2.16791 |  0:00:10s
epoch 19 | loss: 0.60858 | eval_custom_logloss: 1.45918 |  0:00:10s
epoch 20 | loss: 0.5949  | eval_custom_logloss: 1.23377 |  0:00:11s
epoch 21 | loss: 0.58108 | eval_custom_logloss: 1.59964 |  0:00:12s
epoch 22 | loss: 0.58949 | eval_custom_logloss: 1.3523  |  0:00:12s
epoch 23 | loss: 0.58985 | eval_custom_logloss: 1.12894 |  0:00:13s
epoch 24 | loss: 0.59888 | eval_custom_logloss: 0.97004 |  0:00:13s
epoch 25 | loss: 0.58985 | eval_custom_logloss: 1.02422 |  0:00:14s
epoch 26 | loss: 0.56971 | eval_custom_logloss: 0.88671 |  0:00:14s
epoch 27 | loss: 0.54822 | eval_custom_logloss: 0.96564 |  0:00:15s
epoch 28 | loss: 0.52277 | eval_custom_logloss: 1.08008 |  0:00:15s
epoch 29 | loss: 0.52381 | eval_custom_logloss: 1.04713 |  0:00:16s
epoch 30 | loss: 0.50649 | eval_custom_logloss: 0.8375  |  0:00:17s
epoch 31 | loss: 0.51204 | eval_custom_logloss: 0.81717 |  0:00:17s
epoch 32 | loss: 0.50602 | eval_custom_logloss: 0.81643 |  0:00:18s
epoch 33 | loss: 0.4943  | eval_custom_logloss: 0.7994  |  0:00:18s
epoch 34 | loss: 0.48673 | eval_custom_logloss: 0.59251 |  0:00:19s
epoch 35 | loss: 0.53307 | eval_custom_logloss: 0.75426 |  0:00:19s
epoch 36 | loss: 0.5162  | eval_custom_logloss: 0.65913 |  0:00:20s
epoch 37 | loss: 0.47273 | eval_custom_logloss: 1.00956 |  0:00:21s
epoch 38 | loss: 0.50324 | eval_custom_logloss: 0.82103 |  0:00:21s
epoch 39 | loss: 0.49582 | eval_custom_logloss: 0.95648 |  0:00:22s
epoch 40 | loss: 0.52271 | eval_custom_logloss: 0.84027 |  0:00:22s
epoch 41 | loss: 0.5351  | eval_custom_logloss: 0.93598 |  0:00:23s
epoch 42 | loss: 0.55546 | eval_custom_logloss: 0.8359  |  0:00:23s
epoch 43 | loss: 0.54593 | eval_custom_logloss: 0.93912 |  0:00:24s
epoch 44 | loss: 0.53572 | eval_custom_logloss: 0.89208 |  0:00:24s
epoch 45 | loss: 0.53655 | eval_custom_logloss: 0.98455 |  0:00:25s
epoch 46 | loss: 0.52574 | eval_custom_logloss: 0.97013 |  0:00:25s
epoch 47 | loss: 0.47501 | eval_custom_logloss: 1.21518 |  0:00:26s
epoch 48 | loss: 0.46143 | eval_custom_logloss: 0.95156 |  0:00:27s
epoch 49 | loss: 0.43572 | eval_custom_logloss: 1.03615 |  0:00:27s
epoch 50 | loss: 0.45454 | eval_custom_logloss: 0.89577 |  0:00:28s
epoch 51 | loss: 0.42509 | eval_custom_logloss: 0.83338 |  0:00:28s
epoch 52 | loss: 0.43395 | eval_custom_logloss: 0.78859 |  0:00:29s
epoch 53 | loss: 0.41116 | eval_custom_logloss: 0.9188  |  0:00:29s
epoch 54 | loss: 0.40674 | eval_custom_logloss: 0.91317 |  0:00:30s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 0.59251
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7850199999999999, 'Log Loss - std': 0.1661450619187943} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 0.7850199999999999 and parameters: {'n_d': 46, 'n_steps': 7, 'gamma': 1.6551738974360375, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.002724226353157474, 'mask_type': 'sparsemax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax', 'n_a': 39, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.15612 | eval_custom_logloss: 1.02731 |  0:00:00s
epoch 1  | loss: 1.34001 | eval_custom_logloss: 1.30925 |  0:00:01s
epoch 2  | loss: 1.37014 | eval_custom_logloss: 1.21491 |  0:00:01s
epoch 3  | loss: 1.10469 | eval_custom_logloss: 1.04839 |  0:00:02s
epoch 4  | loss: 1.14672 | eval_custom_logloss: 1.9034  |  0:00:03s
epoch 5  | loss: 1.24274 | eval_custom_logloss: 0.92636 |  0:00:03s
epoch 6  | loss: 1.00253 | eval_custom_logloss: 0.92865 |  0:00:04s
epoch 7  | loss: 1.05272 | eval_custom_logloss: 0.93971 |  0:00:04s
epoch 8  | loss: 0.93632 | eval_custom_logloss: 0.88099 |  0:00:05s
epoch 9  | loss: 0.92185 | eval_custom_logloss: 0.86735 |  0:00:06s
epoch 10 | loss: 0.8538  | eval_custom_logloss: 0.66739 |  0:00:06s
epoch 11 | loss: 0.75479 | eval_custom_logloss: 0.72885 |  0:00:07s
epoch 12 | loss: 0.77452 | eval_custom_logloss: 0.71948 |  0:00:08s
epoch 13 | loss: 0.7826  | eval_custom_logloss: 0.77061 |  0:00:08s
epoch 14 | loss: 0.77349 | eval_custom_logloss: 0.73937 |  0:00:09s
epoch 15 | loss: 0.7672  | eval_custom_logloss: 0.74786 |  0:00:09s
epoch 16 | loss: 0.78854 | eval_custom_logloss: 0.74038 |  0:00:10s
epoch 17 | loss: 0.75429 | eval_custom_logloss: 0.65021 |  0:00:11s
epoch 18 | loss: 0.68652 | eval_custom_logloss: 0.67151 |  0:00:11s
epoch 19 | loss: 0.69786 | eval_custom_logloss: 0.65623 |  0:00:12s
epoch 20 | loss: 0.72269 | eval_custom_logloss: 0.64394 |  0:00:12s
epoch 21 | loss: 0.69829 | eval_custom_logloss: 0.65514 |  0:00:13s
epoch 22 | loss: 0.70565 | eval_custom_logloss: 0.73272 |  0:00:14s
epoch 23 | loss: 0.66083 | eval_custom_logloss: 0.61831 |  0:00:14s
epoch 24 | loss: 0.69056 | eval_custom_logloss: 0.62573 |  0:00:15s
epoch 25 | loss: 0.67854 | eval_custom_logloss: 0.61962 |  0:00:15s
epoch 26 | loss: 0.6418  | eval_custom_logloss: 0.54079 |  0:00:16s
epoch 27 | loss: 0.61557 | eval_custom_logloss: 0.57678 |  0:00:17s
epoch 28 | loss: 0.61716 | eval_custom_logloss: 0.52891 |  0:00:17s
epoch 29 | loss: 0.56769 | eval_custom_logloss: 0.53559 |  0:00:18s
epoch 30 | loss: 0.58041 | eval_custom_logloss: 0.56652 |  0:00:19s
epoch 31 | loss: 0.57742 | eval_custom_logloss: 0.54713 |  0:00:19s
epoch 32 | loss: 0.60563 | eval_custom_logloss: 0.52607 |  0:00:20s
epoch 33 | loss: 0.56138 | eval_custom_logloss: 0.51411 |  0:00:20s
epoch 34 | loss: 0.58035 | eval_custom_logloss: 0.64849 |  0:00:21s
epoch 35 | loss: 0.5668  | eval_custom_logloss: 0.56583 |  0:00:22s
epoch 36 | loss: 0.59133 | eval_custom_logloss: 0.59135 |  0:00:22s
epoch 37 | loss: 0.56105 | eval_custom_logloss: 0.55783 |  0:00:23s
epoch 38 | loss: 0.54948 | eval_custom_logloss: 0.54702 |  0:00:23s
epoch 39 | loss: 0.57086 | eval_custom_logloss: 0.51879 |  0:00:24s
epoch 40 | loss: 0.51919 | eval_custom_logloss: 0.52474 |  0:00:25s
epoch 41 | loss: 0.53153 | eval_custom_logloss: 0.52043 |  0:00:25s
epoch 42 | loss: 0.52339 | eval_custom_logloss: 0.55293 |  0:00:26s
epoch 43 | loss: 0.57581 | eval_custom_logloss: 0.55523 |  0:00:26s
epoch 44 | loss: 0.55147 | eval_custom_logloss: 0.5895  |  0:00:27s
epoch 45 | loss: 0.57311 | eval_custom_logloss: 0.53955 |  0:00:28s
epoch 46 | loss: 0.54158 | eval_custom_logloss: 0.54813 |  0:00:28s
epoch 47 | loss: 0.54108 | eval_custom_logloss: 0.56858 |  0:00:29s
epoch 48 | loss: 0.55819 | eval_custom_logloss: 0.49681 |  0:00:30s
epoch 49 | loss: 0.54998 | eval_custom_logloss: 0.54648 |  0:00:30s
epoch 50 | loss: 0.53508 | eval_custom_logloss: 0.5193  |  0:00:31s
epoch 51 | loss: 0.5161  | eval_custom_logloss: 0.54441 |  0:00:31s
epoch 52 | loss: 0.5139  | eval_custom_logloss: 0.50198 |  0:00:32s
epoch 53 | loss: 0.50245 | eval_custom_logloss: 0.49641 |  0:00:33s
epoch 54 | loss: 0.48214 | eval_custom_logloss: 0.50011 |  0:00:33s
epoch 55 | loss: 0.48569 | eval_custom_logloss: 0.49019 |  0:00:34s
epoch 56 | loss: 0.54125 | eval_custom_logloss: 0.50342 |  0:00:34s
epoch 57 | loss: 0.50246 | eval_custom_logloss: 0.56321 |  0:00:35s
epoch 58 | loss: 0.52376 | eval_custom_logloss: 0.52904 |  0:00:36s
epoch 59 | loss: 0.55541 | eval_custom_logloss: 0.53389 |  0:00:36s
epoch 60 | loss: 0.49744 | eval_custom_logloss: 0.52876 |  0:00:37s
epoch 61 | loss: 0.49893 | eval_custom_logloss: 0.51052 |  0:00:37s
epoch 62 | loss: 0.46851 | eval_custom_logloss: 0.51223 |  0:00:38s
epoch 63 | loss: 0.46049 | eval_custom_logloss: 0.51898 |  0:00:39s
epoch 64 | loss: 0.46606 | eval_custom_logloss: 0.50984 |  0:00:39s
epoch 65 | loss: 0.45445 | eval_custom_logloss: 0.48403 |  0:00:40s
epoch 66 | loss: 0.4532  | eval_custom_logloss: 0.50319 |  0:00:40s
epoch 67 | loss: 0.4253  | eval_custom_logloss: 0.49091 |  0:00:41s
epoch 68 | loss: 0.41589 | eval_custom_logloss: 0.51387 |  0:00:42s
epoch 69 | loss: 0.40961 | eval_custom_logloss: 0.46918 |  0:00:42s
epoch 70 | loss: 0.39395 | eval_custom_logloss: 0.52463 |  0:00:43s
epoch 71 | loss: 0.40219 | eval_custom_logloss: 0.47502 |  0:00:43s
epoch 72 | loss: 0.42092 | eval_custom_logloss: 0.46176 |  0:00:44s
epoch 73 | loss: 0.41439 | eval_custom_logloss: 0.4379  |  0:00:45s
epoch 74 | loss: 0.39483 | eval_custom_logloss: 0.42627 |  0:00:45s
epoch 75 | loss: 0.36878 | eval_custom_logloss: 0.40236 |  0:00:46s
epoch 76 | loss: 0.40625 | eval_custom_logloss: 0.4529  |  0:00:47s
epoch 77 | loss: 0.38385 | eval_custom_logloss: 0.41756 |  0:00:47s
epoch 78 | loss: 0.38518 | eval_custom_logloss: 0.4194  |  0:00:48s
epoch 79 | loss: 0.41755 | eval_custom_logloss: 0.43936 |  0:00:48s
epoch 80 | loss: 0.38434 | eval_custom_logloss: 0.44352 |  0:00:49s
epoch 81 | loss: 0.39586 | eval_custom_logloss: 0.46642 |  0:00:50s
epoch 82 | loss: 0.43094 | eval_custom_logloss: 0.45859 |  0:00:50s
epoch 83 | loss: 0.42532 | eval_custom_logloss: 0.47135 |  0:00:51s
epoch 84 | loss: 0.41094 | eval_custom_logloss: 0.43878 |  0:00:51s
epoch 85 | loss: 0.37523 | eval_custom_logloss: 0.44616 |  0:00:52s
epoch 86 | loss: 0.39292 | eval_custom_logloss: 0.44609 |  0:00:53s
epoch 87 | loss: 0.38241 | eval_custom_logloss: 0.4782  |  0:00:53s
epoch 88 | loss: 0.42941 | eval_custom_logloss: 0.47476 |  0:00:54s
epoch 89 | loss: 0.3813  | eval_custom_logloss: 0.42855 |  0:00:54s
epoch 90 | loss: 0.3435  | eval_custom_logloss: 0.40618 |  0:00:55s
epoch 91 | loss: 0.37452 | eval_custom_logloss: 0.41185 |  0:00:56s
epoch 92 | loss: 0.35112 | eval_custom_logloss: 0.38709 |  0:00:56s
epoch 93 | loss: 0.32287 | eval_custom_logloss: 0.37148 |  0:00:57s
epoch 94 | loss: 0.32751 | eval_custom_logloss: 0.36612 |  0:00:57s
epoch 95 | loss: 0.31929 | eval_custom_logloss: 0.38963 |  0:00:58s
epoch 96 | loss: 0.33439 | eval_custom_logloss: 0.41747 |  0:00:59s
epoch 97 | loss: 0.31575 | eval_custom_logloss: 0.41272 |  0:00:59s
epoch 98 | loss: 0.32494 | eval_custom_logloss: 0.39465 |  0:01:00s
epoch 99 | loss: 0.31015 | eval_custom_logloss: 0.40967 |  0:01:01s
Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_eval_custom_logloss = 0.36612
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3661, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax', 'n_a': 39, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.0678  | eval_custom_logloss: 1.3995  |  0:00:00s
epoch 1  | loss: 1.29682 | eval_custom_logloss: 1.2709  |  0:00:01s
epoch 2  | loss: 1.21788 | eval_custom_logloss: 1.14197 |  0:00:01s
epoch 3  | loss: 1.0475  | eval_custom_logloss: 1.32541 |  0:00:02s
epoch 4  | loss: 1.038   | eval_custom_logloss: 1.02779 |  0:00:03s
epoch 5  | loss: 1.57178 | eval_custom_logloss: 1.56307 |  0:00:03s
epoch 6  | loss: 1.38445 | eval_custom_logloss: 1.71277 |  0:00:04s
epoch 7  | loss: 1.17374 | eval_custom_logloss: 1.53535 |  0:00:04s
epoch 8  | loss: 1.13191 | eval_custom_logloss: 1.10226 |  0:00:05s
epoch 9  | loss: 0.90714 | eval_custom_logloss: 1.08346 |  0:00:06s
epoch 10 | loss: 0.78265 | eval_custom_logloss: 0.91271 |  0:00:06s
epoch 11 | loss: 0.73077 | eval_custom_logloss: 0.83896 |  0:00:07s
epoch 12 | loss: 0.81818 | eval_custom_logloss: 0.95977 |  0:00:07s
epoch 13 | loss: 0.75875 | eval_custom_logloss: 0.78895 |  0:00:08s
epoch 14 | loss: 0.74081 | eval_custom_logloss: 0.80516 |  0:00:09s
epoch 15 | loss: 0.6974  | eval_custom_logloss: 0.81295 |  0:00:09s
epoch 16 | loss: 0.66623 | eval_custom_logloss: 0.73347 |  0:00:10s
epoch 17 | loss: 0.72743 | eval_custom_logloss: 0.75555 |  0:00:11s
epoch 18 | loss: 0.63943 | eval_custom_logloss: 0.70694 |  0:00:11s
epoch 19 | loss: 0.67349 | eval_custom_logloss: 0.71214 |  0:00:12s
epoch 20 | loss: 0.67969 | eval_custom_logloss: 0.72279 |  0:00:12s
epoch 21 | loss: 0.63479 | eval_custom_logloss: 0.7348  |  0:00:13s
epoch 22 | loss: 0.60949 | eval_custom_logloss: 0.69512 |  0:00:14s
epoch 23 | loss: 0.58611 | eval_custom_logloss: 0.65682 |  0:00:14s
epoch 24 | loss: 0.58253 | eval_custom_logloss: 0.64602 |  0:00:15s
epoch 25 | loss: 0.57429 | eval_custom_logloss: 0.63551 |  0:00:15s
epoch 26 | loss: 0.56367 | eval_custom_logloss: 0.65051 |  0:00:16s
epoch 27 | loss: 0.58599 | eval_custom_logloss: 0.69498 |  0:00:17s
epoch 28 | loss: 0.55632 | eval_custom_logloss: 0.64461 |  0:00:17s
epoch 29 | loss: 0.56234 | eval_custom_logloss: 0.64753 |  0:00:18s
epoch 30 | loss: 0.54107 | eval_custom_logloss: 0.63226 |  0:00:19s
epoch 31 | loss: 0.53683 | eval_custom_logloss: 0.69371 |  0:00:19s
epoch 32 | loss: 0.55373 | eval_custom_logloss: 0.62617 |  0:00:20s
epoch 33 | loss: 0.53222 | eval_custom_logloss: 0.63689 |  0:00:21s
epoch 34 | loss: 0.53319 | eval_custom_logloss: 0.61746 |  0:00:21s
epoch 35 | loss: 0.53987 | eval_custom_logloss: 0.66872 |  0:00:22s
epoch 36 | loss: 0.53239 | eval_custom_logloss: 0.69083 |  0:00:22s
epoch 37 | loss: 0.54594 | eval_custom_logloss: 0.62816 |  0:00:23s
epoch 38 | loss: 0.55153 | eval_custom_logloss: 0.63446 |  0:00:24s
epoch 39 | loss: 0.54247 | eval_custom_logloss: 0.62636 |  0:00:24s
epoch 40 | loss: 0.51823 | eval_custom_logloss: 0.62021 |  0:00:25s
epoch 41 | loss: 0.52121 | eval_custom_logloss: 0.63168 |  0:00:26s
epoch 42 | loss: 0.50571 | eval_custom_logloss: 0.6562  |  0:00:26s
epoch 43 | loss: 0.53267 | eval_custom_logloss: 0.6198  |  0:00:27s
epoch 44 | loss: 0.52035 | eval_custom_logloss: 0.58275 |  0:00:28s
epoch 45 | loss: 0.51325 | eval_custom_logloss: 0.64054 |  0:00:28s
epoch 46 | loss: 0.50748 | eval_custom_logloss: 0.59988 |  0:00:29s
epoch 47 | loss: 0.5193  | eval_custom_logloss: 0.6406  |  0:00:29s
epoch 48 | loss: 0.51105 | eval_custom_logloss: 0.60241 |  0:00:30s
epoch 49 | loss: 0.49802 | eval_custom_logloss: 0.59052 |  0:00:31s
epoch 50 | loss: 0.48004 | eval_custom_logloss: 0.58481 |  0:00:31s
epoch 51 | loss: 0.4767  | eval_custom_logloss: 0.57504 |  0:00:32s
epoch 52 | loss: 0.474   | eval_custom_logloss: 0.57207 |  0:00:32s
epoch 53 | loss: 0.49941 | eval_custom_logloss: 0.57015 |  0:00:33s
epoch 54 | loss: 0.4861  | eval_custom_logloss: 0.58608 |  0:00:34s
epoch 55 | loss: 0.4786  | eval_custom_logloss: 0.63513 |  0:00:34s
epoch 56 | loss: 0.51237 | eval_custom_logloss: 0.57949 |  0:00:35s
epoch 57 | loss: 0.53486 | eval_custom_logloss: 0.5973  |  0:00:36s
epoch 58 | loss: 0.48811 | eval_custom_logloss: 0.58428 |  0:00:36s
epoch 59 | loss: 0.47167 | eval_custom_logloss: 0.58877 |  0:00:37s
epoch 60 | loss: 0.4473  | eval_custom_logloss: 0.57636 |  0:00:37s
epoch 61 | loss: 0.45061 | eval_custom_logloss: 0.56788 |  0:00:38s
epoch 62 | loss: 0.44585 | eval_custom_logloss: 0.5882  |  0:00:39s
epoch 63 | loss: 0.45093 | eval_custom_logloss: 0.60465 |  0:00:39s
epoch 64 | loss: 0.45781 | eval_custom_logloss: 0.57913 |  0:00:40s
epoch 65 | loss: 0.42881 | eval_custom_logloss: 0.57923 |  0:00:40s
epoch 66 | loss: 0.43827 | eval_custom_logloss: 0.56228 |  0:00:41s
epoch 67 | loss: 0.47025 | eval_custom_logloss: 0.57337 |  0:00:42s
epoch 68 | loss: 0.47206 | eval_custom_logloss: 0.59196 |  0:00:42s
epoch 69 | loss: 0.46114 | eval_custom_logloss: 0.57815 |  0:00:43s
epoch 70 | loss: 0.45206 | eval_custom_logloss: 0.54404 |  0:00:43s
epoch 71 | loss: 0.44218 | eval_custom_logloss: 0.55204 |  0:00:44s
epoch 72 | loss: 0.4719  | eval_custom_logloss: 0.56852 |  0:00:45s
epoch 73 | loss: 0.44631 | eval_custom_logloss: 0.55085 |  0:00:45s
epoch 74 | loss: 0.41174 | eval_custom_logloss: 0.56107 |  0:00:46s
epoch 75 | loss: 0.3994  | eval_custom_logloss: 0.51311 |  0:00:46s
epoch 76 | loss: 0.40949 | eval_custom_logloss: 0.51465 |  0:00:47s
epoch 77 | loss: 0.43544 | eval_custom_logloss: 0.50619 |  0:00:48s
epoch 78 | loss: 0.41133 | eval_custom_logloss: 0.54646 |  0:00:48s
epoch 79 | loss: 0.4242  | eval_custom_logloss: 0.53003 |  0:00:49s
epoch 80 | loss: 0.43312 | eval_custom_logloss: 0.60324 |  0:00:50s
epoch 81 | loss: 0.41843 | eval_custom_logloss: 0.56649 |  0:00:50s
epoch 82 | loss: 0.4693  | eval_custom_logloss: 0.55173 |  0:00:51s
epoch 83 | loss: 0.44923 | eval_custom_logloss: 0.54727 |  0:00:51s
epoch 84 | loss: 0.45164 | eval_custom_logloss: 0.56423 |  0:00:52s
epoch 85 | loss: 0.45964 | eval_custom_logloss: 0.59783 |  0:00:53s
epoch 86 | loss: 0.47662 | eval_custom_logloss: 0.60039 |  0:00:53s
epoch 87 | loss: 0.46249 | eval_custom_logloss: 0.59391 |  0:00:54s
epoch 88 | loss: 0.42991 | eval_custom_logloss: 0.60546 |  0:00:54s
epoch 89 | loss: 0.42117 | eval_custom_logloss: 0.5722  |  0:00:55s
epoch 90 | loss: 0.4203  | eval_custom_logloss: 0.54635 |  0:00:56s
epoch 91 | loss: 0.42548 | eval_custom_logloss: 0.50893 |  0:00:56s
epoch 92 | loss: 0.38115 | eval_custom_logloss: 0.54143 |  0:00:57s
epoch 93 | loss: 0.37233 | eval_custom_logloss: 0.58225 |  0:00:57s
epoch 94 | loss: 0.36293 | eval_custom_logloss: 0.57138 |  0:00:58s
epoch 95 | loss: 0.37931 | eval_custom_logloss: 0.54376 |  0:00:59s
epoch 96 | loss: 0.3815  | eval_custom_logloss: 0.60041 |  0:00:59s
epoch 97 | loss: 0.37705 | eval_custom_logloss: 0.53081 |  0:01:00s

Early stopping occurred at epoch 97 with best_epoch = 77 and best_eval_custom_logloss = 0.50619
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.43615, 'Log Loss - std': 0.07005} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax', 'n_a': 39, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.05193 | eval_custom_logloss: 1.26342 |  0:00:00s
epoch 1  | loss: 1.32095 | eval_custom_logloss: 1.00989 |  0:00:01s
epoch 2  | loss: 1.29329 | eval_custom_logloss: 0.99399 |  0:00:01s
epoch 3  | loss: 1.13349 | eval_custom_logloss: 0.90639 |  0:00:02s
epoch 4  | loss: 1.06461 | eval_custom_logloss: 1.44294 |  0:00:03s
epoch 5  | loss: 1.07526 | eval_custom_logloss: 0.86971 |  0:00:03s
epoch 6  | loss: 0.97732 | eval_custom_logloss: 0.79868 |  0:00:04s
epoch 7  | loss: 0.98602 | eval_custom_logloss: 0.86554 |  0:00:04s
epoch 8  | loss: 0.95672 | eval_custom_logloss: 0.84231 |  0:00:05s
epoch 9  | loss: 1.04181 | eval_custom_logloss: 0.7774  |  0:00:06s
epoch 10 | loss: 0.73857 | eval_custom_logloss: 0.73567 |  0:00:06s
epoch 11 | loss: 0.76888 | eval_custom_logloss: 0.75146 |  0:00:07s
epoch 12 | loss: 0.77053 | eval_custom_logloss: 0.89057 |  0:00:08s
epoch 13 | loss: 0.78181 | eval_custom_logloss: 0.70762 |  0:00:08s
epoch 14 | loss: 0.72637 | eval_custom_logloss: 0.78908 |  0:00:09s
epoch 15 | loss: 0.71575 | eval_custom_logloss: 0.68327 |  0:00:09s
epoch 16 | loss: 0.7568  | eval_custom_logloss: 0.6975  |  0:00:10s
epoch 17 | loss: 0.72748 | eval_custom_logloss: 0.67301 |  0:00:11s
epoch 18 | loss: 0.70173 | eval_custom_logloss: 0.62565 |  0:00:11s
epoch 19 | loss: 0.66573 | eval_custom_logloss: 0.59976 |  0:00:12s
epoch 20 | loss: 0.6173  | eval_custom_logloss: 0.6983  |  0:00:13s
epoch 21 | loss: 0.64865 | eval_custom_logloss: 0.71428 |  0:00:13s
epoch 22 | loss: 0.68188 | eval_custom_logloss: 0.60777 |  0:00:14s
epoch 23 | loss: 0.61739 | eval_custom_logloss: 0.61436 |  0:00:14s
epoch 24 | loss: 0.57612 | eval_custom_logloss: 0.56546 |  0:00:15s
epoch 25 | loss: 0.56011 | eval_custom_logloss: 0.63856 |  0:00:16s
epoch 26 | loss: 0.56073 | eval_custom_logloss: 0.56256 |  0:00:16s
epoch 27 | loss: 0.55709 | eval_custom_logloss: 0.56937 |  0:00:17s
epoch 28 | loss: 0.5697  | eval_custom_logloss: 0.54726 |  0:00:17s
epoch 29 | loss: 0.56832 | eval_custom_logloss: 0.55944 |  0:00:18s
epoch 30 | loss: 0.56396 | eval_custom_logloss: 0.58748 |  0:00:19s
epoch 31 | loss: 0.55601 | eval_custom_logloss: 0.54184 |  0:00:19s
epoch 32 | loss: 0.52074 | eval_custom_logloss: 0.57428 |  0:00:20s
epoch 33 | loss: 0.52342 | eval_custom_logloss: 0.54604 |  0:00:21s
epoch 34 | loss: 0.56656 | eval_custom_logloss: 0.63439 |  0:00:21s
epoch 35 | loss: 0.57737 | eval_custom_logloss: 0.55895 |  0:00:22s
epoch 36 | loss: 0.53924 | eval_custom_logloss: 0.56275 |  0:00:22s
epoch 37 | loss: 0.53781 | eval_custom_logloss: 0.55916 |  0:00:23s
epoch 38 | loss: 0.57738 | eval_custom_logloss: 0.54004 |  0:00:24s
epoch 39 | loss: 0.5492  | eval_custom_logloss: 0.64741 |  0:00:24s
epoch 40 | loss: 0.5932  | eval_custom_logloss: 0.58355 |  0:00:25s
epoch 41 | loss: 0.56707 | eval_custom_logloss: 0.60333 |  0:00:26s
epoch 42 | loss: 0.52715 | eval_custom_logloss: 0.57243 |  0:00:26s
epoch 43 | loss: 0.54028 | eval_custom_logloss: 0.67803 |  0:00:27s
epoch 44 | loss: 0.57813 | eval_custom_logloss: 0.52707 |  0:00:27s
epoch 45 | loss: 0.54645 | eval_custom_logloss: 0.50724 |  0:00:28s
epoch 46 | loss: 0.53179 | eval_custom_logloss: 0.48652 |  0:00:29s
epoch 47 | loss: 0.51098 | eval_custom_logloss: 0.48748 |  0:00:29s
epoch 48 | loss: 0.51721 | eval_custom_logloss: 0.5395  |  0:00:30s
epoch 49 | loss: 0.50069 | eval_custom_logloss: 0.48552 |  0:00:31s
epoch 50 | loss: 0.49783 | eval_custom_logloss: 0.49368 |  0:00:31s
epoch 51 | loss: 0.47673 | eval_custom_logloss: 0.52645 |  0:00:32s
epoch 52 | loss: 0.47342 | eval_custom_logloss: 0.48306 |  0:00:32s
epoch 53 | loss: 0.45734 | eval_custom_logloss: 0.468   |  0:00:33s
epoch 54 | loss: 0.43388 | eval_custom_logloss: 0.46645 |  0:00:34s
epoch 55 | loss: 0.42835 | eval_custom_logloss: 0.46501 |  0:00:34s
epoch 56 | loss: 0.43938 | eval_custom_logloss: 0.45063 |  0:00:35s
epoch 57 | loss: 0.51542 | eval_custom_logloss: 0.49619 |  0:00:36s
epoch 58 | loss: 0.48226 | eval_custom_logloss: 0.45883 |  0:00:36s
epoch 59 | loss: 0.46675 | eval_custom_logloss: 0.46652 |  0:00:37s
epoch 60 | loss: 0.45106 | eval_custom_logloss: 0.48782 |  0:00:37s
epoch 61 | loss: 0.46546 | eval_custom_logloss: 0.45893 |  0:00:38s
epoch 62 | loss: 0.48978 | eval_custom_logloss: 0.48717 |  0:00:39s
epoch 63 | loss: 0.50173 | eval_custom_logloss: 0.5029  |  0:00:39s
epoch 64 | loss: 0.47435 | eval_custom_logloss: 0.48181 |  0:00:40s
epoch 65 | loss: 0.45142 | eval_custom_logloss: 0.47522 |  0:00:40s
epoch 66 | loss: 0.42549 | eval_custom_logloss: 0.46631 |  0:00:41s
epoch 67 | loss: 0.44092 | eval_custom_logloss: 0.5047  |  0:00:42s
epoch 68 | loss: 0.4257  | eval_custom_logloss: 0.48619 |  0:00:42s
epoch 69 | loss: 0.44991 | eval_custom_logloss: 0.48635 |  0:00:43s
epoch 70 | loss: 0.45228 | eval_custom_logloss: 0.45067 |  0:00:44s
epoch 71 | loss: 0.45251 | eval_custom_logloss: 0.4559  |  0:00:44s
epoch 72 | loss: 0.45007 | eval_custom_logloss: 0.44769 |  0:00:45s
epoch 73 | loss: 0.47267 | eval_custom_logloss: 0.44864 |  0:00:45s
epoch 74 | loss: 0.44754 | eval_custom_logloss: 0.44969 |  0:00:46s
epoch 75 | loss: 0.42466 | eval_custom_logloss: 0.43027 |  0:00:47s
epoch 76 | loss: 0.42927 | eval_custom_logloss: 0.44238 |  0:00:47s
epoch 77 | loss: 0.41684 | eval_custom_logloss: 0.43848 |  0:00:48s
epoch 78 | loss: 0.41172 | eval_custom_logloss: 0.43722 |  0:00:49s
epoch 79 | loss: 0.42924 | eval_custom_logloss: 0.46132 |  0:00:49s
epoch 80 | loss: 0.42117 | eval_custom_logloss: 0.45726 |  0:00:50s
epoch 81 | loss: 0.39428 | eval_custom_logloss: 0.44798 |  0:00:50s
epoch 82 | loss: 0.39657 | eval_custom_logloss: 0.45605 |  0:00:51s
epoch 83 | loss: 0.38057 | eval_custom_logloss: 0.45529 |  0:00:52s
epoch 84 | loss: 0.43589 | eval_custom_logloss: 0.49726 |  0:00:52s
epoch 85 | loss: 0.40794 | eval_custom_logloss: 0.44463 |  0:00:53s
epoch 86 | loss: 0.39749 | eval_custom_logloss: 0.44281 |  0:00:53s
epoch 87 | loss: 0.41613 | eval_custom_logloss: 0.44801 |  0:00:54s
epoch 88 | loss: 0.41198 | eval_custom_logloss: 0.45558 |  0:00:55s
epoch 89 | loss: 0.42973 | eval_custom_logloss: 0.43525 |  0:00:55s
epoch 90 | loss: 0.3967  | eval_custom_logloss: 0.42939 |  0:00:56s
epoch 91 | loss: 0.37805 | eval_custom_logloss: 0.39569 |  0:00:56s
epoch 92 | loss: 0.42912 | eval_custom_logloss: 0.48111 |  0:00:57s
epoch 93 | loss: 0.39684 | eval_custom_logloss: 0.4032  |  0:00:58s
epoch 94 | loss: 0.38218 | eval_custom_logloss: 0.42102 |  0:00:58s
epoch 95 | loss: 0.38534 | eval_custom_logloss: 0.42016 |  0:00:59s
epoch 96 | loss: 0.37258 | eval_custom_logloss: 0.45784 |  0:01:00s
epoch 97 | loss: 0.36237 | eval_custom_logloss: 0.44915 |  0:01:00s
epoch 98 | loss: 0.32837 | eval_custom_logloss: 0.44245 |  0:01:01s
epoch 99 | loss: 0.34811 | eval_custom_logloss: 0.46556 |  0:01:01s
Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_eval_custom_logloss = 0.39569
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4226666666666667, 'Log Loss - std': 0.06029042673223963} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax', 'n_a': 39, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.91555 | eval_custom_logloss: 1.23122 |  0:00:00s
epoch 1  | loss: 1.31347 | eval_custom_logloss: 1.27997 |  0:00:01s
epoch 2  | loss: 1.24    | eval_custom_logloss: 1.65353 |  0:00:01s
epoch 3  | loss: 1.57386 | eval_custom_logloss: 1.23021 |  0:00:02s
epoch 4  | loss: 1.19534 | eval_custom_logloss: 1.38073 |  0:00:03s
epoch 5  | loss: 1.00504 | eval_custom_logloss: 0.95855 |  0:00:03s
epoch 6  | loss: 0.93337 | eval_custom_logloss: 1.13718 |  0:00:04s
epoch 7  | loss: 0.92795 | eval_custom_logloss: 0.89967 |  0:00:04s
epoch 8  | loss: 0.88119 | eval_custom_logloss: 0.79914 |  0:00:05s
epoch 9  | loss: 0.80995 | eval_custom_logloss: 0.72188 |  0:00:06s
epoch 10 | loss: 0.84776 | eval_custom_logloss: 1.07976 |  0:00:06s
epoch 11 | loss: 1.02857 | eval_custom_logloss: 0.85132 |  0:00:07s
epoch 12 | loss: 0.73474 | eval_custom_logloss: 0.83813 |  0:00:08s
epoch 13 | loss: 0.72482 | eval_custom_logloss: 0.72003 |  0:00:08s
epoch 14 | loss: 0.67873 | eval_custom_logloss: 0.74838 |  0:00:09s
epoch 15 | loss: 0.66247 | eval_custom_logloss: 0.72029 |  0:00:10s
epoch 16 | loss: 0.67783 | eval_custom_logloss: 0.72047 |  0:00:10s
epoch 17 | loss: 0.64126 | eval_custom_logloss: 0.6695  |  0:00:11s
epoch 18 | loss: 0.62979 | eval_custom_logloss: 0.64182 |  0:00:11s
epoch 19 | loss: 0.62556 | eval_custom_logloss: 0.74944 |  0:00:12s
epoch 20 | loss: 0.6807  | eval_custom_logloss: 0.70505 |  0:00:13s
epoch 21 | loss: 0.62365 | eval_custom_logloss: 0.68654 |  0:00:13s
epoch 22 | loss: 0.63128 | eval_custom_logloss: 0.67996 |  0:00:14s
epoch 23 | loss: 0.65205 | eval_custom_logloss: 0.72728 |  0:00:14s
epoch 24 | loss: 0.65987 | eval_custom_logloss: 0.77459 |  0:00:15s
epoch 25 | loss: 0.6373  | eval_custom_logloss: 0.67368 |  0:00:16s
epoch 26 | loss: 0.5986  | eval_custom_logloss: 0.65524 |  0:00:16s
epoch 27 | loss: 0.60892 | eval_custom_logloss: 0.64611 |  0:00:17s
epoch 28 | loss: 0.57557 | eval_custom_logloss: 0.64085 |  0:00:17s
epoch 29 | loss: 0.56995 | eval_custom_logloss: 0.59949 |  0:00:18s
epoch 30 | loss: 0.5968  | eval_custom_logloss: 0.60944 |  0:00:19s
epoch 31 | loss: 0.55844 | eval_custom_logloss: 0.60357 |  0:00:19s
epoch 32 | loss: 0.53092 | eval_custom_logloss: 0.61054 |  0:00:20s
epoch 33 | loss: 0.54613 | eval_custom_logloss: 0.69516 |  0:00:21s
epoch 34 | loss: 0.55964 | eval_custom_logloss: 0.64308 |  0:00:21s
epoch 35 | loss: 0.55562 | eval_custom_logloss: 0.62074 |  0:00:22s
epoch 36 | loss: 0.57531 | eval_custom_logloss: 0.63822 |  0:00:22s
epoch 37 | loss: 0.57686 | eval_custom_logloss: 0.67371 |  0:00:23s
epoch 38 | loss: 0.55842 | eval_custom_logloss: 0.60539 |  0:00:24s
epoch 39 | loss: 0.54518 | eval_custom_logloss: 0.76087 |  0:00:24s
epoch 40 | loss: 0.5653  | eval_custom_logloss: 0.5802  |  0:00:25s
epoch 41 | loss: 0.54244 | eval_custom_logloss: 0.6239  |  0:00:25s
epoch 42 | loss: 0.59228 | eval_custom_logloss: 0.65422 |  0:00:26s
epoch 43 | loss: 0.56561 | eval_custom_logloss: 0.67033 |  0:00:27s
epoch 44 | loss: 0.56364 | eval_custom_logloss: 0.60782 |  0:00:27s
epoch 45 | loss: 0.53372 | eval_custom_logloss: 0.68188 |  0:00:28s
epoch 46 | loss: 0.50942 | eval_custom_logloss: 0.69543 |  0:00:29s
epoch 47 | loss: 0.52785 | eval_custom_logloss: 0.61626 |  0:00:29s
epoch 48 | loss: 0.50392 | eval_custom_logloss: 0.59526 |  0:00:30s
epoch 49 | loss: 0.48272 | eval_custom_logloss: 0.57012 |  0:00:30s
epoch 50 | loss: 0.54193 | eval_custom_logloss: 0.60815 |  0:00:31s
epoch 51 | loss: 0.52407 | eval_custom_logloss: 0.61304 |  0:00:32s
epoch 52 | loss: 0.50096 | eval_custom_logloss: 0.66884 |  0:00:32s
epoch 53 | loss: 0.57078 | eval_custom_logloss: 0.62537 |  0:00:33s
epoch 54 | loss: 0.52827 | eval_custom_logloss: 0.61328 |  0:00:33s
epoch 55 | loss: 0.5668  | eval_custom_logloss: 0.64177 |  0:00:34s
epoch 56 | loss: 0.54954 | eval_custom_logloss: 0.60132 |  0:00:35s
epoch 57 | loss: 0.55558 | eval_custom_logloss: 0.64147 |  0:00:35s
epoch 58 | loss: 0.54644 | eval_custom_logloss: 0.60596 |  0:00:36s
epoch 59 | loss: 0.49682 | eval_custom_logloss: 0.59779 |  0:00:36s
epoch 60 | loss: 0.53321 | eval_custom_logloss: 0.59708 |  0:00:37s
epoch 61 | loss: 0.51596 | eval_custom_logloss: 0.58794 |  0:00:38s
epoch 62 | loss: 0.49765 | eval_custom_logloss: 0.59026 |  0:00:38s
epoch 63 | loss: 0.5099  | eval_custom_logloss: 0.58034 |  0:00:39s
epoch 64 | loss: 0.48828 | eval_custom_logloss: 0.58938 |  0:00:39s
epoch 65 | loss: 0.43933 | eval_custom_logloss: 0.61152 |  0:00:40s
epoch 66 | loss: 0.44017 | eval_custom_logloss: 0.55786 |  0:00:41s
epoch 67 | loss: 0.42177 | eval_custom_logloss: 0.52218 |  0:00:41s
epoch 68 | loss: 0.44122 | eval_custom_logloss: 0.53037 |  0:00:42s
epoch 69 | loss: 0.43321 | eval_custom_logloss: 0.63018 |  0:00:42s
epoch 70 | loss: 0.43929 | eval_custom_logloss: 0.59209 |  0:00:43s
epoch 71 | loss: 0.42056 | eval_custom_logloss: 0.59258 |  0:00:44s
epoch 72 | loss: 0.4478  | eval_custom_logloss: 0.53459 |  0:00:44s
epoch 73 | loss: 0.41379 | eval_custom_logloss: 0.63659 |  0:00:45s
epoch 74 | loss: 0.43025 | eval_custom_logloss: 0.58431 |  0:00:45s
epoch 75 | loss: 0.40532 | eval_custom_logloss: 0.55748 |  0:00:46s
epoch 76 | loss: 0.41335 | eval_custom_logloss: 0.58746 |  0:00:47s
epoch 77 | loss: 0.4033  | eval_custom_logloss: 0.54288 |  0:00:47s
epoch 78 | loss: 0.3905  | eval_custom_logloss: 0.53507 |  0:00:48s
epoch 79 | loss: 0.38752 | eval_custom_logloss: 0.54334 |  0:00:48s
epoch 80 | loss: 0.36025 | eval_custom_logloss: 0.52707 |  0:00:49s
epoch 81 | loss: 0.37805 | eval_custom_logloss: 0.5011  |  0:00:50s
epoch 82 | loss: 0.36183 | eval_custom_logloss: 0.55134 |  0:00:50s
epoch 83 | loss: 0.3538  | eval_custom_logloss: 0.59478 |  0:00:51s
epoch 84 | loss: 0.32764 | eval_custom_logloss: 0.56527 |  0:00:51s
epoch 85 | loss: 0.33102 | eval_custom_logloss: 0.50172 |  0:00:52s
epoch 86 | loss: 0.32094 | eval_custom_logloss: 0.45665 |  0:00:53s
epoch 87 | loss: 0.33591 | eval_custom_logloss: 0.51417 |  0:00:53s
epoch 88 | loss: 0.32715 | eval_custom_logloss: 0.5479  |  0:00:54s
epoch 89 | loss: 0.33615 | eval_custom_logloss: 0.54956 |  0:00:55s
epoch 90 | loss: 0.33474 | eval_custom_logloss: 0.57084 |  0:00:55s
epoch 91 | loss: 0.33703 | eval_custom_logloss: 0.50338 |  0:00:56s
epoch 92 | loss: 0.33401 | eval_custom_logloss: 0.49312 |  0:00:56s
epoch 93 | loss: 0.31114 | eval_custom_logloss: 0.50616 |  0:00:57s
epoch 94 | loss: 0.30291 | eval_custom_logloss: 0.54212 |  0:00:57s
epoch 95 | loss: 0.29076 | eval_custom_logloss: 0.50092 |  0:00:58s
epoch 96 | loss: 0.2872  | eval_custom_logloss: 0.51277 |  0:00:59s
epoch 97 | loss: 0.30982 | eval_custom_logloss: 0.51091 |  0:00:59s
epoch 98 | loss: 0.32267 | eval_custom_logloss: 0.51123 |  0:01:00s
epoch 99 | loss: 0.29883 | eval_custom_logloss: 0.51177 |  0:01:00s
Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_custom_logloss = 0.45665
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.431175, 'Log Loss - std': 0.05425289738806583} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax', 'n_a': 39, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.9957  | eval_custom_logloss: 1.12581 |  0:00:00s
epoch 1  | loss: 1.29601 | eval_custom_logloss: 1.33785 |  0:00:01s
epoch 2  | loss: 1.27903 | eval_custom_logloss: 0.85767 |  0:00:01s
epoch 3  | loss: 1.15214 | eval_custom_logloss: 0.89633 |  0:00:02s
epoch 4  | loss: 1.00483 | eval_custom_logloss: 1.01991 |  0:00:03s
epoch 5  | loss: 1.36778 | eval_custom_logloss: 0.8902  |  0:00:03s
epoch 6  | loss: 1.0426  | eval_custom_logloss: 1.02783 |  0:00:04s
epoch 7  | loss: 1.41991 | eval_custom_logloss: 0.96566 |  0:00:04s
epoch 8  | loss: 1.20403 | eval_custom_logloss: 1.27403 |  0:00:05s
epoch 9  | loss: 0.92937 | eval_custom_logloss: 0.70801 |  0:00:05s
epoch 10 | loss: 0.89074 | eval_custom_logloss: 0.79116 |  0:00:06s
epoch 11 | loss: 0.77213 | eval_custom_logloss: 0.67914 |  0:00:07s
epoch 12 | loss: 0.77229 | eval_custom_logloss: 0.65809 |  0:00:07s
epoch 13 | loss: 0.73136 | eval_custom_logloss: 0.65563 |  0:00:08s
epoch 14 | loss: 0.69328 | eval_custom_logloss: 0.69971 |  0:00:09s
epoch 15 | loss: 0.70911 | eval_custom_logloss: 0.61835 |  0:00:09s
epoch 16 | loss: 0.6734  | eval_custom_logloss: 0.65082 |  0:00:10s
epoch 17 | loss: 0.65933 | eval_custom_logloss: 0.65015 |  0:00:10s
epoch 18 | loss: 0.68917 | eval_custom_logloss: 0.66194 |  0:00:11s
epoch 19 | loss: 0.64083 | eval_custom_logloss: 0.56049 |  0:00:12s
epoch 20 | loss: 0.6193  | eval_custom_logloss: 0.56962 |  0:00:12s
epoch 21 | loss: 0.61775 | eval_custom_logloss: 0.54437 |  0:00:13s
epoch 22 | loss: 0.62021 | eval_custom_logloss: 0.56377 |  0:00:13s
epoch 23 | loss: 0.61423 | eval_custom_logloss: 0.58725 |  0:00:14s
epoch 24 | loss: 0.63934 | eval_custom_logloss: 0.56797 |  0:00:15s
epoch 25 | loss: 0.60404 | eval_custom_logloss: 0.62308 |  0:00:15s
epoch 26 | loss: 0.58077 | eval_custom_logloss: 0.52461 |  0:00:16s
epoch 27 | loss: 0.59123 | eval_custom_logloss: 0.5339  |  0:00:16s
epoch 28 | loss: 0.55104 | eval_custom_logloss: 0.52049 |  0:00:17s
epoch 29 | loss: 0.56989 | eval_custom_logloss: 0.49417 |  0:00:18s
epoch 30 | loss: 0.55893 | eval_custom_logloss: 0.55265 |  0:00:18s
epoch 31 | loss: 0.55606 | eval_custom_logloss: 0.57954 |  0:00:19s
epoch 32 | loss: 0.57007 | eval_custom_logloss: 0.5622  |  0:00:19s
epoch 33 | loss: 0.60505 | eval_custom_logloss: 0.60222 |  0:00:20s
epoch 34 | loss: 0.61501 | eval_custom_logloss: 0.56617 |  0:00:21s
epoch 35 | loss: 0.59614 | eval_custom_logloss: 0.55237 |  0:00:21s
epoch 36 | loss: 0.5859  | eval_custom_logloss: 0.52743 |  0:00:22s
epoch 37 | loss: 0.58738 | eval_custom_logloss: 0.53378 |  0:00:22s
epoch 38 | loss: 0.58914 | eval_custom_logloss: 0.54247 |  0:00:23s
epoch 39 | loss: 0.61273 | eval_custom_logloss: 0.51748 |  0:00:24s
epoch 40 | loss: 0.58447 | eval_custom_logloss: 0.48887 |  0:00:24s
epoch 41 | loss: 0.57183 | eval_custom_logloss: 0.5221  |  0:00:25s
epoch 42 | loss: 0.58185 | eval_custom_logloss: 0.49303 |  0:00:26s
epoch 43 | loss: 0.53053 | eval_custom_logloss: 0.47203 |  0:00:26s
epoch 44 | loss: 0.54042 | eval_custom_logloss: 0.48462 |  0:00:27s
epoch 45 | loss: 0.56625 | eval_custom_logloss: 0.49634 |  0:00:27s
epoch 46 | loss: 0.53054 | eval_custom_logloss: 0.43688 |  0:00:28s
epoch 47 | loss: 0.49714 | eval_custom_logloss: 0.39909 |  0:00:29s
epoch 48 | loss: 0.48781 | eval_custom_logloss: 0.40195 |  0:00:29s
epoch 49 | loss: 0.49952 | eval_custom_logloss: 0.4091  |  0:00:30s
epoch 50 | loss: 0.47388 | eval_custom_logloss: 0.41832 |  0:00:30s
epoch 51 | loss: 0.50008 | eval_custom_logloss: 0.50013 |  0:00:31s
epoch 52 | loss: 0.49534 | eval_custom_logloss: 0.41639 |  0:00:32s
epoch 53 | loss: 0.48893 | eval_custom_logloss: 0.40553 |  0:00:32s
epoch 54 | loss: 0.49151 | eval_custom_logloss: 0.42088 |  0:00:33s
epoch 55 | loss: 0.47187 | eval_custom_logloss: 0.39201 |  0:00:33s
epoch 56 | loss: 0.45323 | eval_custom_logloss: 0.38822 |  0:00:34s
epoch 57 | loss: 0.46114 | eval_custom_logloss: 0.41013 |  0:00:35s
epoch 58 | loss: 0.45252 | eval_custom_logloss: 0.35601 |  0:00:35s
epoch 59 | loss: 0.42538 | eval_custom_logloss: 0.36958 |  0:00:36s
epoch 60 | loss: 0.4159  | eval_custom_logloss: 0.37678 |  0:00:36s
epoch 61 | loss: 0.433   | eval_custom_logloss: 0.34643 |  0:00:37s
epoch 62 | loss: 0.43447 | eval_custom_logloss: 0.37998 |  0:00:38s
epoch 63 | loss: 0.44886 | eval_custom_logloss: 0.37179 |  0:00:38s
epoch 64 | loss: 0.40976 | eval_custom_logloss: 0.34599 |  0:00:39s
epoch 65 | loss: 0.41942 | eval_custom_logloss: 0.35058 |  0:00:40s
epoch 66 | loss: 0.42552 | eval_custom_logloss: 0.37468 |  0:00:40s
epoch 67 | loss: 0.41661 | eval_custom_logloss: 0.35332 |  0:00:41s
epoch 68 | loss: 0.43337 | eval_custom_logloss: 0.4039  |  0:00:41s
epoch 69 | loss: 0.43171 | eval_custom_logloss: 0.35681 |  0:00:42s
epoch 70 | loss: 0.40509 | eval_custom_logloss: 0.35762 |  0:00:42s
epoch 71 | loss: 0.39905 | eval_custom_logloss: 0.33525 |  0:00:43s
epoch 72 | loss: 0.43205 | eval_custom_logloss: 0.37025 |  0:00:44s
epoch 73 | loss: 0.47213 | eval_custom_logloss: 0.38262 |  0:00:44s
epoch 74 | loss: 0.41264 | eval_custom_logloss: 0.36311 |  0:00:45s
epoch 75 | loss: 0.40699 | eval_custom_logloss: 0.35234 |  0:00:45s
epoch 76 | loss: 0.40996 | eval_custom_logloss: 0.47917 |  0:00:46s
epoch 77 | loss: 0.43626 | eval_custom_logloss: 0.37568 |  0:00:47s
epoch 78 | loss: 0.42097 | eval_custom_logloss: 0.37319 |  0:00:47s
epoch 79 | loss: 0.39865 | eval_custom_logloss: 0.38402 |  0:00:48s
epoch 80 | loss: 0.39922 | eval_custom_logloss: 0.35234 |  0:00:48s
epoch 81 | loss: 0.37315 | eval_custom_logloss: 0.34299 |  0:00:49s
epoch 82 | loss: 0.39316 | eval_custom_logloss: 0.41682 |  0:00:50s
epoch 83 | loss: 0.44644 | eval_custom_logloss: 0.43542 |  0:00:50s
epoch 84 | loss: 0.42423 | eval_custom_logloss: 0.40345 |  0:00:51s
epoch 85 | loss: 0.38603 | eval_custom_logloss: 0.36298 |  0:00:51s
epoch 86 | loss: 0.37809 | eval_custom_logloss: 0.36197 |  0:00:52s
epoch 87 | loss: 0.36458 | eval_custom_logloss: 0.39819 |  0:00:53s
epoch 88 | loss: 0.34718 | eval_custom_logloss: 0.34461 |  0:00:53s
epoch 89 | loss: 0.34242 | eval_custom_logloss: 0.3897  |  0:00:54s
epoch 90 | loss: 0.37268 | eval_custom_logloss: 0.35076 |  0:00:54s
epoch 91 | loss: 0.37155 | eval_custom_logloss: 0.34346 |  0:00:55s

Early stopping occurred at epoch 91 with best_epoch = 71 and best_eval_custom_logloss = 0.33525
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.41200000000000003, 'Log Loss - std': 0.06185001212611037} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 0.41200000000000003 and parameters: {'n_d': 39, 'n_steps': 8, 'gamma': 1.7028360719369438, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 5, 'momentum': 0.22456579460592516, 'mask_type': 'sparsemax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.21133 | eval_custom_logloss: 1.04162 |  0:00:00s
epoch 1  | loss: 1.20919 | eval_custom_logloss: 1.03594 |  0:00:01s
epoch 2  | loss: 1.1051  | eval_custom_logloss: 1.06353 |  0:00:01s
epoch 3  | loss: 1.10797 | eval_custom_logloss: 0.75714 |  0:00:02s
epoch 4  | loss: 0.98305 | eval_custom_logloss: 1.00501 |  0:00:02s
epoch 5  | loss: 0.86528 | eval_custom_logloss: 0.77103 |  0:00:03s
epoch 6  | loss: 0.84996 | eval_custom_logloss: 1.05644 |  0:00:03s
epoch 7  | loss: 0.73868 | eval_custom_logloss: 0.65694 |  0:00:04s
epoch 8  | loss: 0.76289 | eval_custom_logloss: 0.83627 |  0:00:04s
epoch 9  | loss: 0.75755 | eval_custom_logloss: 0.63382 |  0:00:05s
epoch 10 | loss: 0.71278 | eval_custom_logloss: 0.73566 |  0:00:05s
epoch 11 | loss: 0.76626 | eval_custom_logloss: 0.82829 |  0:00:06s
epoch 12 | loss: 0.72167 | eval_custom_logloss: 0.65701 |  0:00:06s
epoch 13 | loss: 0.6944  | eval_custom_logloss: 0.7188  |  0:00:07s
epoch 14 | loss: 0.69438 | eval_custom_logloss: 0.7908  |  0:00:07s
epoch 15 | loss: 0.66855 | eval_custom_logloss: 0.63592 |  0:00:08s
epoch 16 | loss: 0.65587 | eval_custom_logloss: 0.72286 |  0:00:08s
epoch 17 | loss: 0.6361  | eval_custom_logloss: 0.67502 |  0:00:09s
epoch 18 | loss: 0.66121 | eval_custom_logloss: 0.7499  |  0:00:09s
epoch 19 | loss: 0.67025 | eval_custom_logloss: 0.65171 |  0:00:10s
epoch 20 | loss: 0.62648 | eval_custom_logloss: 0.72665 |  0:00:10s
epoch 21 | loss: 0.63752 | eval_custom_logloss: 0.74482 |  0:00:11s
epoch 22 | loss: 0.59312 | eval_custom_logloss: 0.6836  |  0:00:11s
epoch 23 | loss: 0.57307 | eval_custom_logloss: 0.5548  |  0:00:12s
epoch 24 | loss: 0.56955 | eval_custom_logloss: 0.6516  |  0:00:12s
epoch 25 | loss: 0.61185 | eval_custom_logloss: 0.54181 |  0:00:13s
epoch 26 | loss: 0.59214 | eval_custom_logloss: 0.60101 |  0:00:13s
epoch 27 | loss: 0.57577 | eval_custom_logloss: 0.61967 |  0:00:14s
epoch 28 | loss: 0.58329 | eval_custom_logloss: 0.58395 |  0:00:14s
epoch 29 | loss: 0.58543 | eval_custom_logloss: 0.62027 |  0:00:15s
epoch 30 | loss: 0.5947  | eval_custom_logloss: 0.60531 |  0:00:15s
epoch 31 | loss: 0.55324 | eval_custom_logloss: 0.60541 |  0:00:15s
epoch 32 | loss: 0.55684 | eval_custom_logloss: 0.59534 |  0:00:16s
epoch 33 | loss: 0.54596 | eval_custom_logloss: 0.62013 |  0:00:16s
epoch 34 | loss: 0.5524  | eval_custom_logloss: 0.51765 |  0:00:17s
epoch 35 | loss: 0.47183 | eval_custom_logloss: 0.54313 |  0:00:17s
epoch 36 | loss: 0.49065 | eval_custom_logloss: 0.52909 |  0:00:18s
epoch 37 | loss: 0.48265 | eval_custom_logloss: 0.4859  |  0:00:19s
epoch 38 | loss: 0.48376 | eval_custom_logloss: 0.53986 |  0:00:19s
epoch 39 | loss: 0.51648 | eval_custom_logloss: 0.57749 |  0:00:19s
epoch 40 | loss: 0.47427 | eval_custom_logloss: 0.54111 |  0:00:20s
epoch 41 | loss: 0.49017 | eval_custom_logloss: 0.5305  |  0:00:20s
epoch 42 | loss: 0.48599 | eval_custom_logloss: 0.52736 |  0:00:21s
epoch 43 | loss: 0.46147 | eval_custom_logloss: 0.55069 |  0:00:21s
epoch 44 | loss: 0.46427 | eval_custom_logloss: 0.49648 |  0:00:22s
epoch 45 | loss: 0.48974 | eval_custom_logloss: 0.50633 |  0:00:22s
epoch 46 | loss: 0.46946 | eval_custom_logloss: 0.47825 |  0:00:23s
epoch 47 | loss: 0.46163 | eval_custom_logloss: 0.48882 |  0:00:23s
epoch 48 | loss: 0.44746 | eval_custom_logloss: 0.48608 |  0:00:24s
epoch 49 | loss: 0.442   | eval_custom_logloss: 0.4676  |  0:00:24s
epoch 50 | loss: 0.42726 | eval_custom_logloss: 0.44828 |  0:00:25s
epoch 51 | loss: 0.38698 | eval_custom_logloss: 0.42735 |  0:00:26s
epoch 52 | loss: 0.43705 | eval_custom_logloss: 0.50973 |  0:00:26s
epoch 53 | loss: 0.40885 | eval_custom_logloss: 0.39166 |  0:00:27s
epoch 54 | loss: 0.39644 | eval_custom_logloss: 0.42212 |  0:00:27s
epoch 55 | loss: 0.37903 | eval_custom_logloss: 0.41883 |  0:00:28s
epoch 56 | loss: 0.38313 | eval_custom_logloss: 0.40845 |  0:00:28s
epoch 57 | loss: 0.36864 | eval_custom_logloss: 0.45702 |  0:00:29s
epoch 58 | loss: 0.35152 | eval_custom_logloss: 0.43154 |  0:00:29s
epoch 59 | loss: 0.36718 | eval_custom_logloss: 0.46283 |  0:00:30s
epoch 60 | loss: 0.33342 | eval_custom_logloss: 0.43523 |  0:00:30s
epoch 61 | loss: 0.34017 | eval_custom_logloss: 0.43711 |  0:00:31s
epoch 62 | loss: 0.36075 | eval_custom_logloss: 0.49704 |  0:00:31s
epoch 63 | loss: 0.43745 | eval_custom_logloss: 0.45822 |  0:00:32s
epoch 64 | loss: 0.42627 | eval_custom_logloss: 0.51971 |  0:00:32s
epoch 65 | loss: 0.42263 | eval_custom_logloss: 0.43989 |  0:00:33s
epoch 66 | loss: 0.35848 | eval_custom_logloss: 0.50682 |  0:00:33s
epoch 67 | loss: 0.38109 | eval_custom_logloss: 0.47741 |  0:00:34s
epoch 68 | loss: 0.3606  | eval_custom_logloss: 0.49001 |  0:00:34s
epoch 69 | loss: 0.3556  | eval_custom_logloss: 0.46671 |  0:00:35s
epoch 70 | loss: 0.35419 | eval_custom_logloss: 0.44747 |  0:00:35s
epoch 71 | loss: 0.31745 | eval_custom_logloss: 0.46779 |  0:00:36s
epoch 72 | loss: 0.3101  | eval_custom_logloss: 0.40533 |  0:00:36s
epoch 73 | loss: 0.32221 | eval_custom_logloss: 0.41647 |  0:00:37s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 0.39166
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3917, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.43095 | eval_custom_logloss: 1.37971 |  0:00:00s
epoch 1  | loss: 1.27869 | eval_custom_logloss: 0.99795 |  0:00:01s
epoch 2  | loss: 1.03137 | eval_custom_logloss: 0.98882 |  0:00:01s
epoch 3  | loss: 0.82597 | eval_custom_logloss: 1.0497  |  0:00:02s
epoch 4  | loss: 0.78986 | eval_custom_logloss: 1.00379 |  0:00:02s
epoch 5  | loss: 0.86479 | eval_custom_logloss: 1.23889 |  0:00:02s
epoch 6  | loss: 0.76102 | eval_custom_logloss: 0.88664 |  0:00:03s
epoch 7  | loss: 0.68464 | eval_custom_logloss: 0.9384  |  0:00:03s
epoch 8  | loss: 0.71666 | eval_custom_logloss: 1.11847 |  0:00:04s
epoch 9  | loss: 0.70921 | eval_custom_logloss: 0.79102 |  0:00:04s
epoch 10 | loss: 0.64989 | eval_custom_logloss: 0.94005 |  0:00:05s
epoch 11 | loss: 0.66704 | eval_custom_logloss: 0.90555 |  0:00:05s
epoch 12 | loss: 0.68157 | eval_custom_logloss: 0.84677 |  0:00:06s
epoch 13 | loss: 0.64573 | eval_custom_logloss: 1.02374 |  0:00:06s
epoch 14 | loss: 0.65002 | eval_custom_logloss: 0.82384 |  0:00:07s
epoch 15 | loss: 0.69398 | eval_custom_logloss: 0.92846 |  0:00:07s
epoch 16 | loss: 0.59204 | eval_custom_logloss: 0.77689 |  0:00:08s
epoch 17 | loss: 0.59526 | eval_custom_logloss: 0.75431 |  0:00:08s
epoch 18 | loss: 0.58641 | eval_custom_logloss: 0.72004 |  0:00:09s
epoch 19 | loss: 0.56437 | eval_custom_logloss: 0.71629 |  0:00:10s
epoch 20 | loss: 0.57388 | eval_custom_logloss: 0.719   |  0:00:10s
epoch 21 | loss: 0.55455 | eval_custom_logloss: 0.69598 |  0:00:10s
epoch 22 | loss: 0.54832 | eval_custom_logloss: 0.63581 |  0:00:11s
epoch 23 | loss: 0.53314 | eval_custom_logloss: 0.65436 |  0:00:12s
epoch 24 | loss: 0.50876 | eval_custom_logloss: 0.66293 |  0:00:12s
epoch 25 | loss: 0.51657 | eval_custom_logloss: 0.61851 |  0:00:13s
epoch 26 | loss: 0.46641 | eval_custom_logloss: 0.59664 |  0:00:13s
epoch 27 | loss: 0.46198 | eval_custom_logloss: 0.586   |  0:00:14s
epoch 28 | loss: 0.46486 | eval_custom_logloss: 0.58163 |  0:00:14s
epoch 29 | loss: 0.50912 | eval_custom_logloss: 0.5488  |  0:00:15s
epoch 30 | loss: 0.46248 | eval_custom_logloss: 0.57076 |  0:00:15s
epoch 31 | loss: 0.46001 | eval_custom_logloss: 0.58109 |  0:00:16s
epoch 32 | loss: 0.47315 | eval_custom_logloss: 0.62647 |  0:00:16s
epoch 33 | loss: 0.45942 | eval_custom_logloss: 0.64614 |  0:00:17s
epoch 34 | loss: 0.49535 | eval_custom_logloss: 0.5933  |  0:00:17s
epoch 35 | loss: 0.48097 | eval_custom_logloss: 0.60807 |  0:00:18s
epoch 36 | loss: 0.4227  | eval_custom_logloss: 0.63786 |  0:00:18s
epoch 37 | loss: 0.43846 | eval_custom_logloss: 0.60981 |  0:00:19s
epoch 38 | loss: 0.41697 | eval_custom_logloss: 0.58696 |  0:00:19s
epoch 39 | loss: 0.43997 | eval_custom_logloss: 0.60569 |  0:00:20s
epoch 40 | loss: 0.41607 | eval_custom_logloss: 0.58488 |  0:00:20s
epoch 41 | loss: 0.44351 | eval_custom_logloss: 0.58872 |  0:00:21s
epoch 42 | loss: 0.42966 | eval_custom_logloss: 0.50656 |  0:00:21s
epoch 43 | loss: 0.40494 | eval_custom_logloss: 0.51292 |  0:00:22s
epoch 44 | loss: 0.38903 | eval_custom_logloss: 0.49721 |  0:00:22s
epoch 45 | loss: 0.39712 | eval_custom_logloss: 0.56001 |  0:00:23s
epoch 46 | loss: 0.37501 | eval_custom_logloss: 0.53135 |  0:00:24s
epoch 47 | loss: 0.37944 | eval_custom_logloss: 0.52267 |  0:00:24s
epoch 48 | loss: 0.3877  | eval_custom_logloss: 0.53074 |  0:00:25s
epoch 49 | loss: 0.38063 | eval_custom_logloss: 0.51898 |  0:00:25s
epoch 50 | loss: 0.39433 | eval_custom_logloss: 0.51151 |  0:00:26s
epoch 51 | loss: 0.36576 | eval_custom_logloss: 0.55749 |  0:00:27s
epoch 52 | loss: 0.38812 | eval_custom_logloss: 0.53476 |  0:00:27s
epoch 53 | loss: 0.38202 | eval_custom_logloss: 0.50351 |  0:00:28s
epoch 54 | loss: 0.42028 | eval_custom_logloss: 0.50802 |  0:00:28s
epoch 55 | loss: 0.39029 | eval_custom_logloss: 0.50155 |  0:00:29s
epoch 56 | loss: 0.35289 | eval_custom_logloss: 0.54173 |  0:00:29s
epoch 57 | loss: 0.3468  | eval_custom_logloss: 0.50868 |  0:00:30s
epoch 58 | loss: 0.33654 | eval_custom_logloss: 0.49887 |  0:00:30s
epoch 59 | loss: 0.35258 | eval_custom_logloss: 0.60052 |  0:00:31s
epoch 60 | loss: 0.34586 | eval_custom_logloss: 0.50276 |  0:00:31s
epoch 61 | loss: 0.33791 | eval_custom_logloss: 0.50416 |  0:00:32s
epoch 62 | loss: 0.31437 | eval_custom_logloss: 0.54852 |  0:00:32s
epoch 63 | loss: 0.33195 | eval_custom_logloss: 0.51286 |  0:00:33s
epoch 64 | loss: 0.35623 | eval_custom_logloss: 0.51423 |  0:00:33s

Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_custom_logloss = 0.49721
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.44445, 'Log Loss - std': 0.05274999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.31782 | eval_custom_logloss: 1.18699 |  0:00:00s
epoch 1  | loss: 1.24192 | eval_custom_logloss: 1.00821 |  0:00:01s
epoch 2  | loss: 1.00525 | eval_custom_logloss: 0.89921 |  0:00:01s
epoch 3  | loss: 0.95598 | eval_custom_logloss: 0.84846 |  0:00:02s
epoch 4  | loss: 0.91074 | eval_custom_logloss: 0.71748 |  0:00:02s
epoch 5  | loss: 0.81137 | eval_custom_logloss: 0.89674 |  0:00:03s
epoch 6  | loss: 0.92085 | eval_custom_logloss: 0.78052 |  0:00:03s
epoch 7  | loss: 0.85639 | eval_custom_logloss: 1.1078  |  0:00:04s
epoch 8  | loss: 0.75968 | eval_custom_logloss: 0.6986  |  0:00:04s
epoch 9  | loss: 0.73539 | eval_custom_logloss: 0.72219 |  0:00:05s
epoch 10 | loss: 0.75241 | eval_custom_logloss: 0.87602 |  0:00:05s
epoch 11 | loss: 0.74026 | eval_custom_logloss: 0.79766 |  0:00:06s
epoch 12 | loss: 0.6799  | eval_custom_logloss: 0.79466 |  0:00:06s
epoch 13 | loss: 0.64108 | eval_custom_logloss: 0.67821 |  0:00:07s
epoch 14 | loss: 0.65833 | eval_custom_logloss: 0.66551 |  0:00:07s
epoch 15 | loss: 0.68587 | eval_custom_logloss: 0.69298 |  0:00:08s
epoch 16 | loss: 0.65466 | eval_custom_logloss: 0.73142 |  0:00:08s
epoch 17 | loss: 0.64128 | eval_custom_logloss: 0.6502  |  0:00:09s
epoch 18 | loss: 0.60127 | eval_custom_logloss: 0.6847  |  0:00:09s
epoch 19 | loss: 0.58958 | eval_custom_logloss: 0.68754 |  0:00:10s
epoch 20 | loss: 0.62747 | eval_custom_logloss: 0.78345 |  0:00:10s
epoch 21 | loss: 0.64994 | eval_custom_logloss: 0.71832 |  0:00:11s
epoch 22 | loss: 0.59171 | eval_custom_logloss: 0.64423 |  0:00:11s
epoch 23 | loss: 0.62437 | eval_custom_logloss: 0.65087 |  0:00:12s
epoch 24 | loss: 0.60409 | eval_custom_logloss: 0.62132 |  0:00:13s
epoch 25 | loss: 0.5739  | eval_custom_logloss: 0.5875  |  0:00:13s
epoch 26 | loss: 0.54806 | eval_custom_logloss: 0.57591 |  0:00:14s
epoch 27 | loss: 0.59094 | eval_custom_logloss: 0.57761 |  0:00:14s
epoch 28 | loss: 0.62937 | eval_custom_logloss: 0.71748 |  0:00:15s
epoch 29 | loss: 0.58233 | eval_custom_logloss: 0.62231 |  0:00:15s
epoch 30 | loss: 0.58405 | eval_custom_logloss: 0.66956 |  0:00:16s
epoch 31 | loss: 0.5771  | eval_custom_logloss: 0.58142 |  0:00:16s
epoch 32 | loss: 0.54111 | eval_custom_logloss: 0.60105 |  0:00:17s
epoch 33 | loss: 0.56631 | eval_custom_logloss: 0.59167 |  0:00:17s
epoch 34 | loss: 0.60227 | eval_custom_logloss: 0.56191 |  0:00:18s
epoch 35 | loss: 0.54634 | eval_custom_logloss: 0.5332  |  0:00:18s
epoch 36 | loss: 0.51714 | eval_custom_logloss: 0.50742 |  0:00:19s
epoch 37 | loss: 0.48928 | eval_custom_logloss: 0.47513 |  0:00:19s
epoch 38 | loss: 0.47584 | eval_custom_logloss: 0.49132 |  0:00:20s
epoch 39 | loss: 0.50719 | eval_custom_logloss: 0.48791 |  0:00:20s
epoch 40 | loss: 0.511   | eval_custom_logloss: 0.51954 |  0:00:21s
epoch 41 | loss: 0.50746 | eval_custom_logloss: 0.53356 |  0:00:21s
epoch 42 | loss: 0.47074 | eval_custom_logloss: 0.53812 |  0:00:22s
epoch 43 | loss: 0.48167 | eval_custom_logloss: 0.4835  |  0:00:22s
epoch 44 | loss: 0.46851 | eval_custom_logloss: 0.46914 |  0:00:22s
epoch 45 | loss: 0.46219 | eval_custom_logloss: 0.47244 |  0:00:23s
epoch 46 | loss: 0.434   | eval_custom_logloss: 0.46507 |  0:00:23s
epoch 47 | loss: 0.44559 | eval_custom_logloss: 0.47934 |  0:00:24s
epoch 48 | loss: 0.44903 | eval_custom_logloss: 0.46925 |  0:00:24s
epoch 49 | loss: 0.47333 | eval_custom_logloss: 0.52206 |  0:00:25s
epoch 50 | loss: 0.42646 | eval_custom_logloss: 0.47686 |  0:00:25s
epoch 51 | loss: 0.45382 | eval_custom_logloss: 0.49578 |  0:00:26s
epoch 52 | loss: 0.46207 | eval_custom_logloss: 0.4815  |  0:00:26s
epoch 53 | loss: 0.44876 | eval_custom_logloss: 0.47095 |  0:00:27s
epoch 54 | loss: 0.45623 | eval_custom_logloss: 0.43799 |  0:00:27s
epoch 55 | loss: 0.44598 | eval_custom_logloss: 0.47596 |  0:00:28s
epoch 56 | loss: 0.4489  | eval_custom_logloss: 0.49458 |  0:00:28s
epoch 57 | loss: 0.43608 | eval_custom_logloss: 0.46619 |  0:00:29s
epoch 58 | loss: 0.42051 | eval_custom_logloss: 0.42566 |  0:00:29s
epoch 59 | loss: 0.43421 | eval_custom_logloss: 0.46056 |  0:00:30s
epoch 60 | loss: 0.40538 | eval_custom_logloss: 0.44011 |  0:00:30s
epoch 61 | loss: 0.40749 | eval_custom_logloss: 0.44186 |  0:00:31s
epoch 62 | loss: 0.43613 | eval_custom_logloss: 0.44486 |  0:00:31s
epoch 63 | loss: 0.43026 | eval_custom_logloss: 0.46423 |  0:00:32s
epoch 64 | loss: 0.43954 | eval_custom_logloss: 0.49137 |  0:00:32s
epoch 65 | loss: 0.42744 | eval_custom_logloss: 0.46107 |  0:00:33s
epoch 66 | loss: 0.38805 | eval_custom_logloss: 0.43944 |  0:00:33s
epoch 67 | loss: 0.36384 | eval_custom_logloss: 0.43681 |  0:00:34s
epoch 68 | loss: 0.40259 | eval_custom_logloss: 0.49749 |  0:00:34s
epoch 69 | loss: 0.37958 | eval_custom_logloss: 0.48356 |  0:00:35s
epoch 70 | loss: 0.39338 | eval_custom_logloss: 0.42489 |  0:00:35s
epoch 71 | loss: 0.38393 | eval_custom_logloss: 0.43314 |  0:00:36s
epoch 72 | loss: 0.36551 | eval_custom_logloss: 0.40963 |  0:00:36s
epoch 73 | loss: 0.37257 | eval_custom_logloss: 0.40535 |  0:00:37s
epoch 74 | loss: 0.34028 | eval_custom_logloss: 0.41162 |  0:00:37s
epoch 75 | loss: 0.31766 | eval_custom_logloss: 0.39154 |  0:00:38s
epoch 76 | loss: 0.33076 | eval_custom_logloss: 0.39505 |  0:00:38s
epoch 77 | loss: 0.32128 | eval_custom_logloss: 0.41924 |  0:00:39s
epoch 78 | loss: 0.31265 | eval_custom_logloss: 0.4069  |  0:00:39s
epoch 79 | loss: 0.32291 | eval_custom_logloss: 0.42345 |  0:00:40s
epoch 80 | loss: 0.31506 | eval_custom_logloss: 0.36616 |  0:00:40s
epoch 81 | loss: 0.31664 | eval_custom_logloss: 0.40744 |  0:00:41s
epoch 82 | loss: 0.33342 | eval_custom_logloss: 0.41372 |  0:00:41s
epoch 83 | loss: 0.30877 | eval_custom_logloss: 0.42412 |  0:00:42s
epoch 84 | loss: 0.33783 | eval_custom_logloss: 0.41675 |  0:00:42s
epoch 85 | loss: 0.33232 | eval_custom_logloss: 0.41532 |  0:00:43s
epoch 86 | loss: 0.32454 | eval_custom_logloss: 0.44785 |  0:00:43s
epoch 87 | loss: 0.3053  | eval_custom_logloss: 0.43089 |  0:00:43s
epoch 88 | loss: 0.29357 | eval_custom_logloss: 0.47641 |  0:00:44s
epoch 89 | loss: 0.29193 | eval_custom_logloss: 0.47374 |  0:00:44s
epoch 90 | loss: 0.33494 | eval_custom_logloss: 0.49833 |  0:00:45s
epoch 91 | loss: 0.32871 | eval_custom_logloss: 0.47771 |  0:00:45s
epoch 92 | loss: 0.31266 | eval_custom_logloss: 0.45884 |  0:00:46s
epoch 93 | loss: 0.29754 | eval_custom_logloss: 0.44015 |  0:00:46s
epoch 94 | loss: 0.29392 | eval_custom_logloss: 0.41434 |  0:00:47s
epoch 95 | loss: 0.28832 | eval_custom_logloss: 0.37634 |  0:00:47s
epoch 96 | loss: 0.26855 | eval_custom_logloss: 0.42106 |  0:00:48s
epoch 97 | loss: 0.27628 | eval_custom_logloss: 0.38566 |  0:00:48s
epoch 98 | loss: 0.26656 | eval_custom_logloss: 0.39708 |  0:00:49s
epoch 99 | loss: 0.25461 | eval_custom_logloss: 0.39291 |  0:00:49s
Stop training because you reached max_epochs = 100 with best_epoch = 80 and best_eval_custom_logloss = 0.36616
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4183666666666667, 'Log Loss - std': 0.05670733834542246} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.3428  | eval_custom_logloss: 1.36002 |  0:00:00s
epoch 1  | loss: 1.21489 | eval_custom_logloss: 1.45885 |  0:00:01s
epoch 2  | loss: 1.16077 | eval_custom_logloss: 0.89731 |  0:00:01s
epoch 3  | loss: 0.95542 | eval_custom_logloss: 0.98247 |  0:00:01s
epoch 4  | loss: 0.93663 | eval_custom_logloss: 0.84445 |  0:00:02s
epoch 5  | loss: 0.85344 | eval_custom_logloss: 1.10347 |  0:00:02s
epoch 6  | loss: 0.78305 | eval_custom_logloss: 0.93215 |  0:00:03s
epoch 7  | loss: 0.7119  | eval_custom_logloss: 0.94055 |  0:00:03s
epoch 8  | loss: 0.67809 | eval_custom_logloss: 0.87545 |  0:00:04s
epoch 9  | loss: 0.80379 | eval_custom_logloss: 1.01306 |  0:00:04s
epoch 10 | loss: 0.75485 | eval_custom_logloss: 0.93265 |  0:00:05s
epoch 11 | loss: 0.69762 | eval_custom_logloss: 0.75569 |  0:00:05s
epoch 12 | loss: 0.64759 | eval_custom_logloss: 0.69052 |  0:00:06s
epoch 13 | loss: 0.65154 | eval_custom_logloss: 0.74384 |  0:00:06s
epoch 14 | loss: 0.63782 | eval_custom_logloss: 0.84601 |  0:00:07s
epoch 15 | loss: 0.66453 | eval_custom_logloss: 0.70385 |  0:00:07s
epoch 16 | loss: 0.62695 | eval_custom_logloss: 0.75708 |  0:00:08s
epoch 17 | loss: 0.60561 | eval_custom_logloss: 0.65954 |  0:00:08s
epoch 18 | loss: 0.62252 | eval_custom_logloss: 0.68383 |  0:00:09s
epoch 19 | loss: 0.62011 | eval_custom_logloss: 0.82256 |  0:00:09s
epoch 20 | loss: 0.60314 | eval_custom_logloss: 0.74537 |  0:00:10s
epoch 21 | loss: 0.5484  | eval_custom_logloss: 0.84909 |  0:00:10s
epoch 22 | loss: 0.54745 | eval_custom_logloss: 0.65652 |  0:00:11s
epoch 23 | loss: 0.54037 | eval_custom_logloss: 0.60307 |  0:00:11s
epoch 24 | loss: 0.50899 | eval_custom_logloss: 0.59703 |  0:00:12s
epoch 25 | loss: 0.51866 | eval_custom_logloss: 0.7476  |  0:00:12s
epoch 26 | loss: 0.53816 | eval_custom_logloss: 0.60442 |  0:00:13s
epoch 27 | loss: 0.49997 | eval_custom_logloss: 0.61895 |  0:00:13s
epoch 28 | loss: 0.48442 | eval_custom_logloss: 0.69084 |  0:00:14s
epoch 29 | loss: 0.46161 | eval_custom_logloss: 0.70617 |  0:00:14s
epoch 30 | loss: 0.46698 | eval_custom_logloss: 0.65555 |  0:00:15s
epoch 31 | loss: 0.47722 | eval_custom_logloss: 0.60369 |  0:00:15s
epoch 32 | loss: 0.42957 | eval_custom_logloss: 0.70681 |  0:00:16s
epoch 33 | loss: 0.42203 | eval_custom_logloss: 0.91292 |  0:00:16s
epoch 34 | loss: 0.45528 | eval_custom_logloss: 0.582   |  0:00:17s
epoch 35 | loss: 0.46103 | eval_custom_logloss: 0.61346 |  0:00:17s
epoch 36 | loss: 0.416   | eval_custom_logloss: 0.61019 |  0:00:18s
epoch 37 | loss: 0.40794 | eval_custom_logloss: 0.59127 |  0:00:18s
epoch 38 | loss: 0.41011 | eval_custom_logloss: 0.57314 |  0:00:19s
epoch 39 | loss: 0.39802 | eval_custom_logloss: 0.56488 |  0:00:19s
epoch 40 | loss: 0.40681 | eval_custom_logloss: 0.54274 |  0:00:20s
epoch 41 | loss: 0.4033  | eval_custom_logloss: 0.62459 |  0:00:20s
epoch 42 | loss: 0.35541 | eval_custom_logloss: 0.64275 |  0:00:21s
epoch 43 | loss: 0.41162 | eval_custom_logloss: 0.58285 |  0:00:21s
epoch 44 | loss: 0.36114 | eval_custom_logloss: 0.58214 |  0:00:22s
epoch 45 | loss: 0.37121 | eval_custom_logloss: 0.63128 |  0:00:22s
epoch 46 | loss: 0.34218 | eval_custom_logloss: 0.59259 |  0:00:23s
epoch 47 | loss: 0.35494 | eval_custom_logloss: 0.70034 |  0:00:23s
epoch 48 | loss: 0.35449 | eval_custom_logloss: 0.56349 |  0:00:24s
epoch 49 | loss: 0.3366  | eval_custom_logloss: 0.62412 |  0:00:24s
epoch 50 | loss: 0.35918 | eval_custom_logloss: 0.64529 |  0:00:25s
epoch 51 | loss: 0.35487 | eval_custom_logloss: 0.81483 |  0:00:25s
epoch 52 | loss: 0.37182 | eval_custom_logloss: 0.60613 |  0:00:26s
epoch 53 | loss: 0.37373 | eval_custom_logloss: 0.67222 |  0:00:26s
epoch 54 | loss: 0.43505 | eval_custom_logloss: 0.5689  |  0:00:27s
epoch 55 | loss: 0.38726 | eval_custom_logloss: 0.57258 |  0:00:27s
epoch 56 | loss: 0.39652 | eval_custom_logloss: 0.657   |  0:00:28s
epoch 57 | loss: 0.42868 | eval_custom_logloss: 0.59784 |  0:00:28s
epoch 58 | loss: 0.4043  | eval_custom_logloss: 0.94597 |  0:00:29s
epoch 59 | loss: 0.48609 | eval_custom_logloss: 0.57321 |  0:00:29s
epoch 60 | loss: 0.38584 | eval_custom_logloss: 0.62558 |  0:00:30s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.54274
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.44945, 'Log Loss - std': 0.07287189101429986} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax', 'n_a': 31, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.35403 | eval_custom_logloss: 1.1847  |  0:00:00s
epoch 1  | loss: 1.36272 | eval_custom_logloss: 1.04391 |  0:00:01s
epoch 2  | loss: 1.0367  | eval_custom_logloss: 0.70417 |  0:00:01s
epoch 3  | loss: 0.86855 | eval_custom_logloss: 0.95288 |  0:00:02s
epoch 4  | loss: 0.88452 | eval_custom_logloss: 0.92344 |  0:00:02s
epoch 5  | loss: 0.88746 | eval_custom_logloss: 0.78169 |  0:00:03s
epoch 6  | loss: 0.7759  | eval_custom_logloss: 0.65512 |  0:00:04s
epoch 7  | loss: 0.71355 | eval_custom_logloss: 1.12254 |  0:00:04s
epoch 8  | loss: 0.83766 | eval_custom_logloss: 0.80622 |  0:00:05s
epoch 9  | loss: 0.77576 | eval_custom_logloss: 0.93409 |  0:00:05s
epoch 10 | loss: 0.72858 | eval_custom_logloss: 0.82317 |  0:00:06s
epoch 11 | loss: 0.72375 | eval_custom_logloss: 0.71118 |  0:00:06s
epoch 12 | loss: 0.74718 | eval_custom_logloss: 0.68318 |  0:00:07s
epoch 13 | loss: 0.71539 | eval_custom_logloss: 0.67678 |  0:00:07s
epoch 14 | loss: 0.658   | eval_custom_logloss: 0.58753 |  0:00:08s
epoch 15 | loss: 0.61871 | eval_custom_logloss: 0.67356 |  0:00:08s
epoch 16 | loss: 0.63576 | eval_custom_logloss: 0.68251 |  0:00:09s
epoch 17 | loss: 0.6316  | eval_custom_logloss: 0.606   |  0:00:09s
epoch 18 | loss: 0.56423 | eval_custom_logloss: 0.5584  |  0:00:10s
epoch 19 | loss: 0.53542 | eval_custom_logloss: 0.5017  |  0:00:10s
epoch 20 | loss: 0.56275 | eval_custom_logloss: 0.53825 |  0:00:11s
epoch 21 | loss: 0.57822 | eval_custom_logloss: 0.53009 |  0:00:11s
epoch 22 | loss: 0.54808 | eval_custom_logloss: 0.52572 |  0:00:12s
epoch 23 | loss: 0.53912 | eval_custom_logloss: 0.50005 |  0:00:12s
epoch 24 | loss: 0.53906 | eval_custom_logloss: 0.50209 |  0:00:13s
epoch 25 | loss: 0.55339 | eval_custom_logloss: 0.49649 |  0:00:13s
epoch 26 | loss: 0.52366 | eval_custom_logloss: 0.49515 |  0:00:14s
epoch 27 | loss: 0.51913 | eval_custom_logloss: 0.51497 |  0:00:14s
epoch 28 | loss: 0.52002 | eval_custom_logloss: 0.41529 |  0:00:15s
epoch 29 | loss: 0.50828 | eval_custom_logloss: 0.44398 |  0:00:15s
epoch 30 | loss: 0.50824 | eval_custom_logloss: 0.44939 |  0:00:16s
epoch 31 | loss: 0.4883  | eval_custom_logloss: 0.44715 |  0:00:16s
epoch 32 | loss: 0.46903 | eval_custom_logloss: 0.44934 |  0:00:17s
epoch 33 | loss: 0.48677 | eval_custom_logloss: 0.43212 |  0:00:17s
epoch 34 | loss: 0.50844 | eval_custom_logloss: 0.43478 |  0:00:18s
epoch 35 | loss: 0.48612 | eval_custom_logloss: 0.43502 |  0:00:19s
epoch 36 | loss: 0.50163 | eval_custom_logloss: 0.4713  |  0:00:19s
epoch 37 | loss: 0.47623 | eval_custom_logloss: 0.43306 |  0:00:20s
epoch 38 | loss: 0.47858 | eval_custom_logloss: 0.45458 |  0:00:20s
epoch 39 | loss: 0.47286 | eval_custom_logloss: 0.43474 |  0:00:21s
epoch 40 | loss: 0.44956 | eval_custom_logloss: 0.42787 |  0:00:21s
epoch 41 | loss: 0.46527 | eval_custom_logloss: 0.4598  |  0:00:22s
epoch 42 | loss: 0.44185 | eval_custom_logloss: 0.39638 |  0:00:22s
epoch 43 | loss: 0.45072 | eval_custom_logloss: 0.41475 |  0:00:23s
epoch 44 | loss: 0.4009  | eval_custom_logloss: 0.36459 |  0:00:23s
epoch 45 | loss: 0.46358 | eval_custom_logloss: 0.43334 |  0:00:24s
epoch 46 | loss: 0.44861 | eval_custom_logloss: 0.39928 |  0:00:24s
epoch 47 | loss: 0.43228 | eval_custom_logloss: 0.362   |  0:00:25s
epoch 48 | loss: 0.39665 | eval_custom_logloss: 0.33841 |  0:00:25s
epoch 49 | loss: 0.3557  | eval_custom_logloss: 0.33932 |  0:00:26s
epoch 50 | loss: 0.36766 | eval_custom_logloss: 0.33412 |  0:00:26s
epoch 51 | loss: 0.37896 | eval_custom_logloss: 0.31164 |  0:00:27s
epoch 52 | loss: 0.35366 | eval_custom_logloss: 0.33155 |  0:00:27s
epoch 53 | loss: 0.34243 | eval_custom_logloss: 0.31325 |  0:00:28s
epoch 54 | loss: 0.32936 | eval_custom_logloss: 0.31861 |  0:00:28s
epoch 55 | loss: 0.34172 | eval_custom_logloss: 0.29466 |  0:00:29s
epoch 56 | loss: 0.34752 | eval_custom_logloss: 0.45139 |  0:00:29s
epoch 57 | loss: 0.32279 | eval_custom_logloss: 0.32545 |  0:00:30s
epoch 58 | loss: 0.3265  | eval_custom_logloss: 0.33555 |  0:00:31s
epoch 59 | loss: 0.36519 | eval_custom_logloss: 0.33794 |  0:00:31s
epoch 60 | loss: 0.34602 | eval_custom_logloss: 0.29625 |  0:00:32s
epoch 61 | loss: 0.30768 | eval_custom_logloss: 0.29458 |  0:00:32s
epoch 62 | loss: 0.30983 | eval_custom_logloss: 0.26442 |  0:00:33s
epoch 63 | loss: 0.29963 | eval_custom_logloss: 0.36751 |  0:00:33s
epoch 64 | loss: 0.32785 | eval_custom_logloss: 0.34668 |  0:00:34s
epoch 65 | loss: 0.34147 | eval_custom_logloss: 0.37862 |  0:00:34s
epoch 66 | loss: 0.31295 | eval_custom_logloss: 0.30432 |  0:00:35s
epoch 67 | loss: 0.31451 | eval_custom_logloss: 0.2682  |  0:00:35s
epoch 68 | loss: 0.34324 | eval_custom_logloss: 0.28225 |  0:00:36s
epoch 69 | loss: 0.28737 | eval_custom_logloss: 0.27362 |  0:00:36s
epoch 70 | loss: 0.29784 | eval_custom_logloss: 0.25013 |  0:00:37s
epoch 71 | loss: 0.26354 | eval_custom_logloss: 0.33585 |  0:00:37s
epoch 72 | loss: 0.29498 | eval_custom_logloss: 0.33063 |  0:00:38s
epoch 73 | loss: 0.30746 | eval_custom_logloss: 0.26736 |  0:00:38s
epoch 74 | loss: 0.2743  | eval_custom_logloss: 0.24817 |  0:00:39s
epoch 75 | loss: 0.24356 | eval_custom_logloss: 0.2373  |  0:00:40s
epoch 76 | loss: 0.27335 | eval_custom_logloss: 0.29431 |  0:00:40s
epoch 77 | loss: 0.25026 | eval_custom_logloss: 0.28278 |  0:00:41s
epoch 78 | loss: 0.22361 | eval_custom_logloss: 0.26885 |  0:00:41s
epoch 79 | loss: 0.20734 | eval_custom_logloss: 0.29291 |  0:00:41s
epoch 80 | loss: 0.2491  | eval_custom_logloss: 0.262   |  0:00:42s
epoch 81 | loss: 0.23334 | eval_custom_logloss: 0.24908 |  0:00:43s
epoch 82 | loss: 0.21731 | eval_custom_logloss: 0.25864 |  0:00:43s
epoch 83 | loss: 0.21947 | eval_custom_logloss: 0.24866 |  0:00:44s
epoch 84 | loss: 0.21329 | eval_custom_logloss: 0.23989 |  0:00:44s
epoch 85 | loss: 0.22106 | eval_custom_logloss: 0.24273 |  0:00:45s
epoch 86 | loss: 0.22597 | eval_custom_logloss: 0.24902 |  0:00:45s
epoch 87 | loss: 0.20812 | eval_custom_logloss: 0.24249 |  0:00:46s
epoch 88 | loss: 0.23342 | eval_custom_logloss: 0.29244 |  0:00:46s
epoch 89 | loss: 0.21671 | eval_custom_logloss: 0.3024  |  0:00:47s
epoch 90 | loss: 0.22267 | eval_custom_logloss: 0.32288 |  0:00:47s
epoch 91 | loss: 0.24784 | eval_custom_logloss: 0.2681  |  0:00:48s
epoch 92 | loss: 0.21198 | eval_custom_logloss: 0.33283 |  0:00:48s
epoch 93 | loss: 0.22689 | eval_custom_logloss: 0.23074 |  0:00:49s
epoch 94 | loss: 0.21266 | eval_custom_logloss: 0.25023 |  0:00:49s
epoch 95 | loss: 0.22352 | eval_custom_logloss: 0.26431 |  0:00:50s
epoch 96 | loss: 0.22222 | eval_custom_logloss: 0.30956 |  0:00:50s
epoch 97 | loss: 0.24206 | eval_custom_logloss: 0.25955 |  0:00:51s
epoch 98 | loss: 0.22141 | eval_custom_logloss: 0.25605 |  0:00:51s
epoch 99 | loss: 0.23945 | eval_custom_logloss: 0.25239 |  0:00:52s
Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_eval_custom_logloss = 0.23074
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40570000000000006, 'Log Loss - std': 0.10910774491299871} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 0.40570000000000006 and parameters: {'n_d': 31, 'n_steps': 8, 'gamma': 1.2542694541681303, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.03382427269542986, 'mask_type': 'entmax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax', 'n_a': 43, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.50444 | eval_custom_logloss: 7.28076 |  0:00:00s
epoch 1  | loss: 1.36666 | eval_custom_logloss: 6.62933 |  0:00:01s
epoch 2  | loss: 1.19748 | eval_custom_logloss: 7.42565 |  0:00:01s
epoch 3  | loss: 1.01778 | eval_custom_logloss: 6.71567 |  0:00:02s
epoch 4  | loss: 0.90588 | eval_custom_logloss: 5.57042 |  0:00:02s
epoch 5  | loss: 0.94826 | eval_custom_logloss: 6.06656 |  0:00:03s
epoch 6  | loss: 0.8236  | eval_custom_logloss: 7.13244 |  0:00:04s
epoch 7  | loss: 0.75878 | eval_custom_logloss: 7.18345 |  0:00:04s
epoch 8  | loss: 0.7598  | eval_custom_logloss: 5.27196 |  0:00:05s
epoch 9  | loss: 0.74856 | eval_custom_logloss: 7.02112 |  0:00:05s
epoch 10 | loss: 0.73547 | eval_custom_logloss: 7.66935 |  0:00:06s
epoch 11 | loss: 0.73394 | eval_custom_logloss: 6.4789  |  0:00:06s
epoch 12 | loss: 0.66284 | eval_custom_logloss: 6.3593  |  0:00:07s
epoch 13 | loss: 0.65996 | eval_custom_logloss: 4.99358 |  0:00:07s
epoch 14 | loss: 0.65795 | eval_custom_logloss: 5.22645 |  0:00:08s
epoch 15 | loss: 0.68453 | eval_custom_logloss: 7.54399 |  0:00:08s
epoch 16 | loss: 0.6989  | eval_custom_logloss: 6.55391 |  0:00:09s
epoch 17 | loss: 0.6493  | eval_custom_logloss: 6.20779 |  0:00:09s
epoch 18 | loss: 0.61873 | eval_custom_logloss: 3.84659 |  0:00:10s
epoch 19 | loss: 0.59661 | eval_custom_logloss: 4.31616 |  0:00:11s
epoch 20 | loss: 0.60122 | eval_custom_logloss: 4.47928 |  0:00:11s
epoch 21 | loss: 0.64093 | eval_custom_logloss: 3.83407 |  0:00:12s
epoch 22 | loss: 0.62713 | eval_custom_logloss: 3.29837 |  0:00:12s
epoch 23 | loss: 0.60965 | eval_custom_logloss: 5.27767 |  0:00:13s
epoch 24 | loss: 0.62746 | eval_custom_logloss: 4.47496 |  0:00:13s
epoch 25 | loss: 0.61573 | eval_custom_logloss: 4.09891 |  0:00:14s
epoch 26 | loss: 0.59088 | eval_custom_logloss: 3.19485 |  0:00:14s
epoch 27 | loss: 0.55971 | eval_custom_logloss: 4.26835 |  0:00:15s
epoch 28 | loss: 0.5789  | eval_custom_logloss: 4.30954 |  0:00:15s
epoch 29 | loss: 0.61087 | eval_custom_logloss: 3.49637 |  0:00:16s
epoch 30 | loss: 0.56537 | eval_custom_logloss: 4.08848 |  0:00:17s
epoch 31 | loss: 0.59494 | eval_custom_logloss: 3.95532 |  0:00:17s
epoch 32 | loss: 0.58097 | eval_custom_logloss: 3.89377 |  0:00:18s
epoch 33 | loss: 0.56339 | eval_custom_logloss: 4.21986 |  0:00:18s
epoch 34 | loss: 0.54789 | eval_custom_logloss: 3.09583 |  0:00:19s
epoch 35 | loss: 0.58542 | eval_custom_logloss: 3.03593 |  0:00:19s
epoch 36 | loss: 0.58463 | eval_custom_logloss: 3.23301 |  0:00:20s
epoch 37 | loss: 0.55957 | eval_custom_logloss: 2.81587 |  0:00:20s
epoch 38 | loss: 0.55806 | eval_custom_logloss: 3.03586 |  0:00:21s
epoch 39 | loss: 0.54774 | eval_custom_logloss: 3.10849 |  0:00:21s
epoch 40 | loss: 0.53141 | eval_custom_logloss: 3.8911  |  0:00:22s
epoch 41 | loss: 0.54887 | eval_custom_logloss: 3.09901 |  0:00:22s
epoch 42 | loss: 0.52562 | eval_custom_logloss: 3.22931 |  0:00:23s
epoch 43 | loss: 0.55382 | eval_custom_logloss: 2.84474 |  0:00:24s
epoch 44 | loss: 0.53492 | eval_custom_logloss: 3.06944 |  0:00:24s
epoch 45 | loss: 0.53697 | eval_custom_logloss: 2.22213 |  0:00:25s
epoch 46 | loss: 0.49694 | eval_custom_logloss: 2.31381 |  0:00:25s
epoch 47 | loss: 0.53774 | eval_custom_logloss: 2.31201 |  0:00:26s
epoch 48 | loss: 0.49327 | eval_custom_logloss: 2.40199 |  0:00:26s
epoch 49 | loss: 0.49562 | eval_custom_logloss: 2.66598 |  0:00:27s
epoch 50 | loss: 0.49573 | eval_custom_logloss: 2.52757 |  0:00:27s
epoch 51 | loss: 0.50993 | eval_custom_logloss: 2.05151 |  0:00:28s
epoch 52 | loss: 0.50745 | eval_custom_logloss: 1.81607 |  0:00:28s
epoch 53 | loss: 0.46742 | eval_custom_logloss: 2.12698 |  0:00:29s
epoch 54 | loss: 0.4639  | eval_custom_logloss: 2.40846 |  0:00:29s
epoch 55 | loss: 0.44912 | eval_custom_logloss: 2.2818  |  0:00:30s
epoch 56 | loss: 0.45064 | eval_custom_logloss: 2.6757  |  0:00:31s
epoch 57 | loss: 0.47298 | eval_custom_logloss: 2.40129 |  0:00:31s
epoch 58 | loss: 0.42771 | eval_custom_logloss: 2.30447 |  0:00:32s
epoch 59 | loss: 0.4427  | eval_custom_logloss: 2.90317 |  0:00:32s
epoch 60 | loss: 0.47072 | eval_custom_logloss: 3.00781 |  0:00:33s
epoch 61 | loss: 0.46871 | eval_custom_logloss: 3.78496 |  0:00:33s
epoch 62 | loss: 0.45637 | eval_custom_logloss: 3.25894 |  0:00:34s
epoch 63 | loss: 0.46127 | eval_custom_logloss: 3.41824 |  0:00:34s
epoch 64 | loss: 0.4546  | eval_custom_logloss: 3.1363  |  0:00:35s
epoch 65 | loss: 0.4323  | eval_custom_logloss: 3.43081 |  0:00:35s
epoch 66 | loss: 0.46882 | eval_custom_logloss: 3.76275 |  0:00:36s
epoch 67 | loss: 0.4611  | eval_custom_logloss: 3.43053 |  0:00:37s
epoch 68 | loss: 0.47911 | eval_custom_logloss: 2.79198 |  0:00:37s
epoch 69 | loss: 0.43976 | eval_custom_logloss: 3.56061 |  0:00:38s
epoch 70 | loss: 0.45537 | eval_custom_logloss: 3.1994  |  0:00:38s
epoch 71 | loss: 0.46017 | eval_custom_logloss: 3.20854 |  0:00:39s
epoch 72 | loss: 0.44766 | eval_custom_logloss: 2.6663  |  0:00:39s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 1.81607
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5845, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax', 'n_a': 43, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.64662 | eval_custom_logloss: 7.72417 |  0:00:00s
epoch 1  | loss: 1.31811 | eval_custom_logloss: 8.62112 |  0:00:01s
epoch 2  | loss: 1.10425 | eval_custom_logloss: 7.48655 |  0:00:01s
epoch 3  | loss: 0.99981 | eval_custom_logloss: 7.36278 |  0:00:02s
epoch 4  | loss: 1.10847 | eval_custom_logloss: 7.65687 |  0:00:02s
epoch 5  | loss: 0.92516 | eval_custom_logloss: 7.89805 |  0:00:03s
epoch 6  | loss: 0.90198 | eval_custom_logloss: 7.59578 |  0:00:04s
epoch 7  | loss: 0.9132  | eval_custom_logloss: 8.34127 |  0:00:04s
epoch 8  | loss: 0.83532 | eval_custom_logloss: 5.97469 |  0:00:05s
epoch 9  | loss: 0.71641 | eval_custom_logloss: 5.39518 |  0:00:05s
epoch 10 | loss: 0.71135 | eval_custom_logloss: 6.40949 |  0:00:06s
epoch 11 | loss: 0.77112 | eval_custom_logloss: 5.94885 |  0:00:07s
epoch 12 | loss: 0.72906 | eval_custom_logloss: 4.99556 |  0:00:07s
epoch 13 | loss: 0.6362  | eval_custom_logloss: 6.25878 |  0:00:08s
epoch 14 | loss: 0.6281  | eval_custom_logloss: 6.81926 |  0:00:08s
epoch 15 | loss: 0.62842 | eval_custom_logloss: 6.96928 |  0:00:09s
epoch 16 | loss: 0.64233 | eval_custom_logloss: 7.60691 |  0:00:09s
epoch 17 | loss: 0.63615 | eval_custom_logloss: 6.47186 |  0:00:10s
epoch 18 | loss: 0.60244 | eval_custom_logloss: 5.25626 |  0:00:10s
epoch 19 | loss: 0.64195 | eval_custom_logloss: 6.70244 |  0:00:11s
epoch 20 | loss: 0.66117 | eval_custom_logloss: 4.65549 |  0:00:11s
epoch 21 | loss: 0.6949  | eval_custom_logloss: 4.94347 |  0:00:12s
epoch 22 | loss: 0.69242 | eval_custom_logloss: 5.46022 |  0:00:13s
epoch 23 | loss: 0.68795 | eval_custom_logloss: 5.182   |  0:00:13s
epoch 24 | loss: 0.66907 | eval_custom_logloss: 5.82676 |  0:00:14s
epoch 25 | loss: 0.61871 | eval_custom_logloss: 5.70948 |  0:00:14s
epoch 26 | loss: 0.63537 | eval_custom_logloss: 4.51011 |  0:00:15s
epoch 27 | loss: 0.62983 | eval_custom_logloss: 5.15893 |  0:00:15s
epoch 28 | loss: 0.62689 | eval_custom_logloss: 5.25629 |  0:00:16s
epoch 29 | loss: 0.61693 | eval_custom_logloss: 4.59147 |  0:00:16s
epoch 30 | loss: 0.62478 | eval_custom_logloss: 3.37051 |  0:00:17s
epoch 31 | loss: 0.58099 | eval_custom_logloss: 4.69352 |  0:00:18s
epoch 32 | loss: 0.57499 | eval_custom_logloss: 4.3037  |  0:00:18s
epoch 33 | loss: 0.56546 | eval_custom_logloss: 4.81693 |  0:00:19s
epoch 34 | loss: 0.59157 | eval_custom_logloss: 4.4859  |  0:00:19s
epoch 35 | loss: 0.56826 | eval_custom_logloss: 3.83256 |  0:00:20s
epoch 36 | loss: 0.57937 | eval_custom_logloss: 3.88115 |  0:00:20s
epoch 37 | loss: 0.55401 | eval_custom_logloss: 3.30221 |  0:00:21s
epoch 38 | loss: 0.52177 | eval_custom_logloss: 3.86905 |  0:00:22s
epoch 39 | loss: 0.58204 | eval_custom_logloss: 4.39036 |  0:00:22s
epoch 40 | loss: 0.55733 | eval_custom_logloss: 4.28263 |  0:00:23s
epoch 41 | loss: 0.53154 | eval_custom_logloss: 4.70177 |  0:00:23s
epoch 42 | loss: 0.55123 | eval_custom_logloss: 4.61569 |  0:00:24s
epoch 43 | loss: 0.56542 | eval_custom_logloss: 4.57614 |  0:00:25s
epoch 44 | loss: 0.51757 | eval_custom_logloss: 5.07911 |  0:00:25s
epoch 45 | loss: 0.56601 | eval_custom_logloss: 3.95571 |  0:00:26s
epoch 46 | loss: 0.5103  | eval_custom_logloss: 2.25669 |  0:00:26s
epoch 47 | loss: 0.53595 | eval_custom_logloss: 2.91827 |  0:00:27s
epoch 48 | loss: 0.53345 | eval_custom_logloss: 2.88585 |  0:00:27s
epoch 49 | loss: 0.533   | eval_custom_logloss: 2.36752 |  0:00:28s
epoch 50 | loss: 0.51272 | eval_custom_logloss: 2.92444 |  0:00:28s
epoch 51 | loss: 0.51471 | eval_custom_logloss: 2.83825 |  0:00:29s
epoch 52 | loss: 0.49183 | eval_custom_logloss: 2.83841 |  0:00:30s
epoch 53 | loss: 0.50119 | eval_custom_logloss: 2.8108  |  0:00:30s
epoch 54 | loss: 0.43476 | eval_custom_logloss: 2.8736  |  0:00:31s
epoch 55 | loss: 0.43181 | eval_custom_logloss: 2.8081  |  0:00:31s
epoch 56 | loss: 0.44439 | eval_custom_logloss: 3.17216 |  0:00:32s
epoch 57 | loss: 0.41936 | eval_custom_logloss: 3.98478 |  0:00:33s
epoch 58 | loss: 0.40397 | eval_custom_logloss: 3.54012 |  0:00:33s
epoch 59 | loss: 0.46697 | eval_custom_logloss: 3.77214 |  0:00:34s
epoch 60 | loss: 0.44816 | eval_custom_logloss: 2.98805 |  0:00:34s
epoch 61 | loss: 0.44012 | eval_custom_logloss: 2.21361 |  0:00:35s
epoch 62 | loss: 0.41511 | eval_custom_logloss: 2.01091 |  0:00:36s
epoch 63 | loss: 0.44925 | eval_custom_logloss: 1.62794 |  0:00:36s
epoch 64 | loss: 0.41768 | eval_custom_logloss: 1.25351 |  0:00:37s
epoch 65 | loss: 0.40971 | eval_custom_logloss: 1.16351 |  0:00:37s
epoch 66 | loss: 0.43164 | eval_custom_logloss: 1.38678 |  0:00:38s
epoch 67 | loss: 0.46435 | eval_custom_logloss: 2.11    |  0:00:38s
epoch 68 | loss: 0.42439 | eval_custom_logloss: 2.23442 |  0:00:39s
epoch 69 | loss: 0.42994 | eval_custom_logloss: 1.42528 |  0:00:40s
epoch 70 | loss: 0.42264 | eval_custom_logloss: 1.50993 |  0:00:40s
epoch 71 | loss: 0.39891 | eval_custom_logloss: 1.33646 |  0:00:41s
epoch 72 | loss: 0.44023 | eval_custom_logloss: 1.55526 |  0:00:41s
epoch 73 | loss: 0.39474 | eval_custom_logloss: 1.77092 |  0:00:42s
epoch 74 | loss: 0.38919 | eval_custom_logloss: 1.8626  |  0:00:42s
epoch 75 | loss: 0.36442 | eval_custom_logloss: 1.63132 |  0:00:43s
epoch 76 | loss: 0.38272 | eval_custom_logloss: 1.69175 |  0:00:43s
epoch 77 | loss: 0.39494 | eval_custom_logloss: 1.27345 |  0:00:44s
epoch 78 | loss: 0.37666 | eval_custom_logloss: 1.68619 |  0:00:44s
epoch 79 | loss: 0.38233 | eval_custom_logloss: 1.53966 |  0:00:45s
epoch 80 | loss: 0.38362 | eval_custom_logloss: 1.16985 |  0:00:46s
epoch 81 | loss: 0.41246 | eval_custom_logloss: 1.22607 |  0:00:46s
epoch 82 | loss: 0.39097 | eval_custom_logloss: 1.49426 |  0:00:47s
epoch 83 | loss: 0.40837 | eval_custom_logloss: 1.56587 |  0:00:47s
epoch 84 | loss: 0.41135 | eval_custom_logloss: 2.25404 |  0:00:48s
epoch 85 | loss: 0.39629 | eval_custom_logloss: 2.22856 |  0:00:48s

Early stopping occurred at epoch 85 with best_epoch = 65 and best_eval_custom_logloss = 1.16351
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.374, 'Log Loss - std': 0.21050000000000002} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax', 'n_a': 43, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.39352 | eval_custom_logloss: 8.16152 |  0:00:00s
epoch 1  | loss: 1.18808 | eval_custom_logloss: 6.98161 |  0:00:01s
epoch 2  | loss: 1.3196  | eval_custom_logloss: 7.91217 |  0:00:01s
epoch 3  | loss: 1.42997 | eval_custom_logloss: 7.05879 |  0:00:02s
epoch 4  | loss: 1.08688 | eval_custom_logloss: 7.26234 |  0:00:03s
epoch 5  | loss: 0.89003 | eval_custom_logloss: 6.14433 |  0:00:03s
epoch 6  | loss: 0.83885 | eval_custom_logloss: 10.58386|  0:00:04s
epoch 7  | loss: 0.83669 | eval_custom_logloss: 6.21761 |  0:00:04s
epoch 8  | loss: 0.89659 | eval_custom_logloss: 5.84641 |  0:00:05s
epoch 9  | loss: 0.78807 | eval_custom_logloss: 5.95751 |  0:00:05s
epoch 10 | loss: 0.71081 | eval_custom_logloss: 8.62888 |  0:00:06s
epoch 11 | loss: 0.79535 | eval_custom_logloss: 6.6073  |  0:00:07s
epoch 12 | loss: 0.72207 | eval_custom_logloss: 5.22632 |  0:00:07s
epoch 13 | loss: 0.71344 | eval_custom_logloss: 6.75465 |  0:00:08s
epoch 14 | loss: 0.70108 | eval_custom_logloss: 4.79555 |  0:00:08s
epoch 15 | loss: 0.65812 | eval_custom_logloss: 4.8897  |  0:00:09s
epoch 16 | loss: 0.62517 | eval_custom_logloss: 7.18941 |  0:00:10s
epoch 17 | loss: 0.60577 | eval_custom_logloss: 6.06037 |  0:00:10s
epoch 18 | loss: 0.61226 | eval_custom_logloss: 6.00812 |  0:00:11s
epoch 19 | loss: 0.63138 | eval_custom_logloss: 4.34531 |  0:00:11s
epoch 20 | loss: 0.63636 | eval_custom_logloss: 4.25544 |  0:00:12s
epoch 21 | loss: 0.6603  | eval_custom_logloss: 3.61712 |  0:00:13s
epoch 22 | loss: 0.61388 | eval_custom_logloss: 3.51312 |  0:00:13s
epoch 23 | loss: 0.63213 | eval_custom_logloss: 6.20371 |  0:00:14s
epoch 24 | loss: 0.6139  | eval_custom_logloss: 7.20952 |  0:00:14s
epoch 25 | loss: 0.62517 | eval_custom_logloss: 5.10025 |  0:00:15s
epoch 26 | loss: 0.59463 | eval_custom_logloss: 4.30192 |  0:00:16s
epoch 27 | loss: 0.57656 | eval_custom_logloss: 4.2467  |  0:00:16s
epoch 28 | loss: 0.58041 | eval_custom_logloss: 5.54992 |  0:00:17s
epoch 29 | loss: 0.56514 | eval_custom_logloss: 3.98818 |  0:00:17s
epoch 30 | loss: 0.63178 | eval_custom_logloss: 3.72858 |  0:00:18s
epoch 31 | loss: 0.61329 | eval_custom_logloss: 3.36285 |  0:00:19s
epoch 32 | loss: 0.60354 | eval_custom_logloss: 3.02165 |  0:00:19s
epoch 33 | loss: 0.56575 | eval_custom_logloss: 3.83846 |  0:00:20s
epoch 34 | loss: 0.574   | eval_custom_logloss: 3.51068 |  0:00:20s
epoch 35 | loss: 0.55006 | eval_custom_logloss: 3.47869 |  0:00:21s
epoch 36 | loss: 0.54908 | eval_custom_logloss: 3.88295 |  0:00:22s
epoch 37 | loss: 0.53152 | eval_custom_logloss: 3.6121  |  0:00:22s
epoch 38 | loss: 0.54871 | eval_custom_logloss: 2.91268 |  0:00:23s
epoch 39 | loss: 0.53807 | eval_custom_logloss: 3.14244 |  0:00:23s
epoch 40 | loss: 0.5303  | eval_custom_logloss: 2.70756 |  0:00:24s
epoch 41 | loss: 0.50975 | eval_custom_logloss: 3.16095 |  0:00:25s
epoch 42 | loss: 0.53665 | eval_custom_logloss: 3.12492 |  0:00:25s
epoch 43 | loss: 0.58782 | eval_custom_logloss: 2.61609 |  0:00:26s
epoch 44 | loss: 0.52477 | eval_custom_logloss: 2.14425 |  0:00:26s
epoch 45 | loss: 0.54663 | eval_custom_logloss: 2.6304  |  0:00:27s
epoch 46 | loss: 0.55724 | eval_custom_logloss: 2.25389 |  0:00:28s
epoch 47 | loss: 0.51597 | eval_custom_logloss: 1.64301 |  0:00:28s
epoch 48 | loss: 0.49415 | eval_custom_logloss: 1.79503 |  0:00:29s
epoch 49 | loss: 0.48174 | eval_custom_logloss: 1.77428 |  0:00:29s
epoch 50 | loss: 0.4528  | eval_custom_logloss: 1.66628 |  0:00:30s
epoch 51 | loss: 0.47592 | eval_custom_logloss: 2.46846 |  0:00:30s
epoch 52 | loss: 0.51948 | eval_custom_logloss: 1.99144 |  0:00:31s
epoch 53 | loss: 0.4546  | eval_custom_logloss: 3.93784 |  0:00:31s
epoch 54 | loss: 0.4483  | eval_custom_logloss: 4.14858 |  0:00:32s
epoch 55 | loss: 0.45138 | eval_custom_logloss: 4.35124 |  0:00:33s
epoch 56 | loss: 0.43258 | eval_custom_logloss: 4.3789  |  0:00:33s
epoch 57 | loss: 0.43885 | eval_custom_logloss: 3.73398 |  0:00:34s
epoch 58 | loss: 0.43762 | eval_custom_logloss: 3.67957 |  0:00:34s
epoch 59 | loss: 0.44773 | eval_custom_logloss: 2.22718 |  0:00:35s
epoch 60 | loss: 0.42809 | eval_custom_logloss: 2.50507 |  0:00:36s
epoch 61 | loss: 0.40944 | eval_custom_logloss: 1.87627 |  0:00:36s
epoch 62 | loss: 0.42226 | eval_custom_logloss: 1.56907 |  0:00:37s
epoch 63 | loss: 0.42906 | eval_custom_logloss: 1.63355 |  0:00:37s
epoch 64 | loss: 0.42864 | eval_custom_logloss: 1.6868  |  0:00:38s
epoch 65 | loss: 0.4625  | eval_custom_logloss: 1.23591 |  0:00:39s
epoch 66 | loss: 0.46403 | eval_custom_logloss: 1.39324 |  0:00:39s
epoch 67 | loss: 0.4421  | eval_custom_logloss: 2.14294 |  0:00:40s
epoch 68 | loss: 0.4042  | eval_custom_logloss: 2.26044 |  0:00:40s
epoch 69 | loss: 0.38937 | eval_custom_logloss: 2.42074 |  0:00:41s
epoch 70 | loss: 0.44343 | eval_custom_logloss: 1.7226  |  0:00:41s
epoch 71 | loss: 0.42112 | eval_custom_logloss: 1.93358 |  0:00:42s
epoch 72 | loss: 0.43169 | eval_custom_logloss: 2.23125 |  0:00:43s
epoch 73 | loss: 0.37168 | eval_custom_logloss: 2.06268 |  0:00:43s
epoch 74 | loss: 0.36524 | eval_custom_logloss: 2.22744 |  0:00:44s
epoch 75 | loss: 0.37951 | eval_custom_logloss: 2.09927 |  0:00:45s
epoch 76 | loss: 0.37804 | eval_custom_logloss: 2.25157 |  0:00:45s
epoch 77 | loss: 0.35803 | eval_custom_logloss: 2.48463 |  0:00:46s
epoch 78 | loss: 0.34764 | eval_custom_logloss: 2.29309 |  0:00:46s
epoch 79 | loss: 0.3588  | eval_custom_logloss: 2.09123 |  0:00:47s
epoch 80 | loss: 0.40079 | eval_custom_logloss: 2.04406 |  0:00:47s
epoch 81 | loss: 0.43604 | eval_custom_logloss: 1.80372 |  0:00:48s
epoch 82 | loss: 0.36763 | eval_custom_logloss: 1.59864 |  0:00:48s
epoch 83 | loss: 0.35602 | eval_custom_logloss: 1.52385 |  0:00:49s
epoch 84 | loss: 0.33507 | eval_custom_logloss: 1.82407 |  0:00:49s
epoch 85 | loss: 0.33201 | eval_custom_logloss: 2.05839 |  0:00:50s

Early stopping occurred at epoch 85 with best_epoch = 65 and best_eval_custom_logloss = 1.23591
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3057333333333334, 'Log Loss - std': 0.19713153533167194} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax', 'n_a': 43, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.54472 | eval_custom_logloss: 7.35174 |  0:00:00s
epoch 1  | loss: 1.2272  | eval_custom_logloss: 7.61483 |  0:00:01s
epoch 2  | loss: 1.5588  | eval_custom_logloss: 7.52672 |  0:00:01s
epoch 3  | loss: 1.15129 | eval_custom_logloss: 7.96952 |  0:00:02s
epoch 4  | loss: 1.11543 | eval_custom_logloss: 7.88218 |  0:00:02s
epoch 5  | loss: 1.19772 | eval_custom_logloss: 6.90776 |  0:00:03s
epoch 6  | loss: 1.47817 | eval_custom_logloss: 6.68751 |  0:00:03s
epoch 7  | loss: 0.99765 | eval_custom_logloss: 7.52045 |  0:00:04s
epoch 8  | loss: 0.86988 | eval_custom_logloss: 7.15436 |  0:00:04s
epoch 9  | loss: 0.69123 | eval_custom_logloss: 6.97318 |  0:00:05s
epoch 10 | loss: 0.65928 | eval_custom_logloss: 5.94642 |  0:00:06s
epoch 11 | loss: 0.62128 | eval_custom_logloss: 5.02211 |  0:00:06s
epoch 12 | loss: 0.61491 | eval_custom_logloss: 5.91256 |  0:00:07s
epoch 13 | loss: 0.57549 | eval_custom_logloss: 6.05344 |  0:00:07s
epoch 14 | loss: 0.53327 | eval_custom_logloss: 5.566   |  0:00:08s
epoch 15 | loss: 0.57739 | eval_custom_logloss: 3.99747 |  0:00:08s
epoch 16 | loss: 0.5695  | eval_custom_logloss: 6.58332 |  0:00:09s
epoch 17 | loss: 0.54567 | eval_custom_logloss: 6.23566 |  0:00:09s
epoch 18 | loss: 0.51322 | eval_custom_logloss: 5.9058  |  0:00:10s
epoch 19 | loss: 0.51655 | eval_custom_logloss: 4.85483 |  0:00:11s
epoch 20 | loss: 0.51728 | eval_custom_logloss: 3.92207 |  0:00:11s
epoch 21 | loss: 0.60971 | eval_custom_logloss: 6.53375 |  0:00:12s
epoch 22 | loss: 0.60761 | eval_custom_logloss: 5.99789 |  0:00:12s
epoch 23 | loss: 0.60816 | eval_custom_logloss: 5.71706 |  0:00:13s
epoch 24 | loss: 0.64034 | eval_custom_logloss: 4.56753 |  0:00:13s
epoch 25 | loss: 0.5766  | eval_custom_logloss: 4.75311 |  0:00:14s
epoch 26 | loss: 0.61526 | eval_custom_logloss: 4.55524 |  0:00:14s
epoch 27 | loss: 0.61714 | eval_custom_logloss: 3.70624 |  0:00:15s
epoch 28 | loss: 0.64238 | eval_custom_logloss: 4.94354 |  0:00:15s
epoch 29 | loss: 0.57731 | eval_custom_logloss: 5.48786 |  0:00:16s
epoch 30 | loss: 0.58527 | eval_custom_logloss: 5.4755  |  0:00:17s
epoch 31 | loss: 0.59109 | eval_custom_logloss: 4.2161  |  0:00:17s
epoch 32 | loss: 0.59483 | eval_custom_logloss: 4.15622 |  0:00:18s
epoch 33 | loss: 0.56039 | eval_custom_logloss: 4.54344 |  0:00:18s
epoch 34 | loss: 0.56259 | eval_custom_logloss: 4.39078 |  0:00:19s
epoch 35 | loss: 0.53375 | eval_custom_logloss: 5.01372 |  0:00:19s
epoch 36 | loss: 0.52753 | eval_custom_logloss: 4.36559 |  0:00:20s
epoch 37 | loss: 0.55109 | eval_custom_logloss: 4.128   |  0:00:20s
epoch 38 | loss: 0.49285 | eval_custom_logloss: 3.38901 |  0:00:21s
epoch 39 | loss: 0.51461 | eval_custom_logloss: 2.83806 |  0:00:22s
epoch 40 | loss: 0.53706 | eval_custom_logloss: 3.00426 |  0:00:22s
epoch 41 | loss: 0.51724 | eval_custom_logloss: 2.32449 |  0:00:23s
epoch 42 | loss: 0.50778 | eval_custom_logloss: 2.32055 |  0:00:23s
epoch 43 | loss: 0.48422 | eval_custom_logloss: 2.70189 |  0:00:24s
epoch 44 | loss: 0.48209 | eval_custom_logloss: 3.01868 |  0:00:24s
epoch 45 | loss: 0.5165  | eval_custom_logloss: 3.10039 |  0:00:25s
epoch 46 | loss: 0.55141 | eval_custom_logloss: 4.06128 |  0:00:25s
epoch 47 | loss: 0.59221 | eval_custom_logloss: 2.49412 |  0:00:26s
epoch 48 | loss: 0.55123 | eval_custom_logloss: 2.40498 |  0:00:27s
epoch 49 | loss: 0.48484 | eval_custom_logloss: 3.05493 |  0:00:27s
epoch 50 | loss: 0.46272 | eval_custom_logloss: 3.24964 |  0:00:28s
epoch 51 | loss: 0.4531  | eval_custom_logloss: 2.70942 |  0:00:28s
epoch 52 | loss: 0.4856  | eval_custom_logloss: 2.484   |  0:00:29s
epoch 53 | loss: 0.47566 | eval_custom_logloss: 2.71605 |  0:00:29s
epoch 54 | loss: 0.48121 | eval_custom_logloss: 3.34983 |  0:00:30s
epoch 55 | loss: 0.47318 | eval_custom_logloss: 3.32615 |  0:00:30s
epoch 56 | loss: 0.47383 | eval_custom_logloss: 2.78965 |  0:00:31s
epoch 57 | loss: 0.47514 | eval_custom_logloss: 3.41601 |  0:00:31s
epoch 58 | loss: 0.44873 | eval_custom_logloss: 2.32074 |  0:00:32s
epoch 59 | loss: 0.4415  | eval_custom_logloss: 2.8024  |  0:00:33s
epoch 60 | loss: 0.40564 | eval_custom_logloss: 2.1312  |  0:00:33s
epoch 61 | loss: 0.40539 | eval_custom_logloss: 2.33623 |  0:00:34s
epoch 62 | loss: 0.41914 | eval_custom_logloss: 2.19633 |  0:00:34s
epoch 63 | loss: 0.42165 | eval_custom_logloss: 2.37449 |  0:00:35s
epoch 64 | loss: 0.41024 | eval_custom_logloss: 2.69449 |  0:00:35s
epoch 65 | loss: 0.48451 | eval_custom_logloss: 2.16005 |  0:00:36s
epoch 66 | loss: 0.48002 | eval_custom_logloss: 2.51751 |  0:00:36s
epoch 67 | loss: 0.43566 | eval_custom_logloss: 2.61412 |  0:00:37s
epoch 68 | loss: 0.45695 | eval_custom_logloss: 3.26454 |  0:00:37s
epoch 69 | loss: 0.42019 | eval_custom_logloss: 3.15456 |  0:00:38s
epoch 70 | loss: 0.43356 | eval_custom_logloss: 2.7012  |  0:00:39s
epoch 71 | loss: 0.39784 | eval_custom_logloss: 3.1663  |  0:00:39s
epoch 72 | loss: 0.38941 | eval_custom_logloss: 3.51488 |  0:00:40s
epoch 73 | loss: 0.46707 | eval_custom_logloss: 2.57029 |  0:00:40s
epoch 74 | loss: 0.45734 | eval_custom_logloss: 3.27667 |  0:00:41s
epoch 75 | loss: 0.47642 | eval_custom_logloss: 2.80186 |  0:00:41s
epoch 76 | loss: 0.46547 | eval_custom_logloss: 2.77801 |  0:00:42s
epoch 77 | loss: 0.47342 | eval_custom_logloss: 4.6678  |  0:00:42s
epoch 78 | loss: 0.43015 | eval_custom_logloss: 3.03572 |  0:00:43s
epoch 79 | loss: 0.4222  | eval_custom_logloss: 3.42682 |  0:00:43s
epoch 80 | loss: 0.40874 | eval_custom_logloss: 4.00138 |  0:00:44s

Early stopping occurred at epoch 80 with best_epoch = 60 and best_eval_custom_logloss = 2.1312
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.4688750000000002, 'Log Loss - std': 0.330138216320074} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax', 'n_a': 43, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.58064 | eval_custom_logloss: 6.397   |  0:00:00s
epoch 1  | loss: 1.2297  | eval_custom_logloss: 7.79848 |  0:00:01s
epoch 2  | loss: 1.24798 | eval_custom_logloss: 7.7049  |  0:00:01s
epoch 3  | loss: 1.14267 | eval_custom_logloss: 6.55442 |  0:00:02s
epoch 4  | loss: 1.1194  | eval_custom_logloss: 7.95638 |  0:00:02s
epoch 5  | loss: 0.97649 | eval_custom_logloss: 6.65094 |  0:00:03s
epoch 6  | loss: 2.42078 | eval_custom_logloss: 6.51118 |  0:00:03s
epoch 7  | loss: 0.9482  | eval_custom_logloss: 6.89945 |  0:00:04s
epoch 8  | loss: 0.75017 | eval_custom_logloss: 5.75932 |  0:00:04s
epoch 9  | loss: 0.71214 | eval_custom_logloss: 5.03092 |  0:00:05s
epoch 10 | loss: 0.71236 | eval_custom_logloss: 4.65269 |  0:00:06s
epoch 11 | loss: 0.67486 | eval_custom_logloss: 5.97917 |  0:00:06s
epoch 12 | loss: 0.71979 | eval_custom_logloss: 4.70644 |  0:00:07s
epoch 13 | loss: 0.6795  | eval_custom_logloss: 4.42746 |  0:00:07s
epoch 14 | loss: 0.68063 | eval_custom_logloss: 4.3942  |  0:00:08s
epoch 15 | loss: 0.63278 | eval_custom_logloss: 5.04998 |  0:00:08s
epoch 16 | loss: 0.64309 | eval_custom_logloss: 6.40228 |  0:00:09s
epoch 17 | loss: 0.68397 | eval_custom_logloss: 4.91134 |  0:00:09s
epoch 18 | loss: 0.65836 | eval_custom_logloss: 3.19173 |  0:00:10s
epoch 19 | loss: 0.69418 | eval_custom_logloss: 3.5082  |  0:00:11s
epoch 20 | loss: 0.69254 | eval_custom_logloss: 6.05931 |  0:00:11s
epoch 21 | loss: 0.66258 | eval_custom_logloss: 3.551   |  0:00:12s
epoch 22 | loss: 0.61659 | eval_custom_logloss: 4.20825 |  0:00:12s
epoch 23 | loss: 0.63354 | eval_custom_logloss: 5.53557 |  0:00:13s
epoch 24 | loss: 0.605   | eval_custom_logloss: 4.02576 |  0:00:13s
epoch 25 | loss: 0.5932  | eval_custom_logloss: 3.83927 |  0:00:14s
epoch 26 | loss: 0.58512 | eval_custom_logloss: 3.4901  |  0:00:14s
epoch 27 | loss: 0.57054 | eval_custom_logloss: 4.22241 |  0:00:15s
epoch 28 | loss: 0.56671 | eval_custom_logloss: 2.89538 |  0:00:15s
epoch 29 | loss: 0.57397 | eval_custom_logloss: 2.5858  |  0:00:16s
epoch 30 | loss: 0.57536 | eval_custom_logloss: 2.62482 |  0:00:17s
epoch 31 | loss: 0.56008 | eval_custom_logloss: 2.36846 |  0:00:17s
epoch 32 | loss: 0.54463 | eval_custom_logloss: 2.01287 |  0:00:18s
epoch 33 | loss: 0.53199 | eval_custom_logloss: 2.62265 |  0:00:18s
epoch 34 | loss: 0.54545 | eval_custom_logloss: 3.35693 |  0:00:19s
epoch 35 | loss: 0.51608 | eval_custom_logloss: 3.46323 |  0:00:19s
epoch 36 | loss: 0.50632 | eval_custom_logloss: 2.5754  |  0:00:20s
epoch 37 | loss: 0.48455 | eval_custom_logloss: 2.37285 |  0:00:20s
epoch 38 | loss: 0.48335 | eval_custom_logloss: 2.25899 |  0:00:21s
epoch 39 | loss: 0.47619 | eval_custom_logloss: 2.72382 |  0:00:22s
epoch 40 | loss: 0.50521 | eval_custom_logloss: 2.76516 |  0:00:22s
epoch 41 | loss: 0.47858 | eval_custom_logloss: 2.60821 |  0:00:23s
epoch 42 | loss: 0.52356 | eval_custom_logloss: 2.3184  |  0:00:23s
epoch 43 | loss: 0.50884 | eval_custom_logloss: 2.32953 |  0:00:24s
epoch 44 | loss: 0.47586 | eval_custom_logloss: 2.83157 |  0:00:24s
epoch 45 | loss: 0.4775  | eval_custom_logloss: 2.27114 |  0:00:25s
epoch 46 | loss: 0.44163 | eval_custom_logloss: 2.46078 |  0:00:25s
epoch 47 | loss: 0.43288 | eval_custom_logloss: 2.99817 |  0:00:26s
epoch 48 | loss: 0.41878 | eval_custom_logloss: 2.28499 |  0:00:27s
epoch 49 | loss: 0.40915 | eval_custom_logloss: 2.28493 |  0:00:27s
epoch 50 | loss: 0.42789 | eval_custom_logloss: 2.56981 |  0:00:28s
epoch 51 | loss: 0.44926 | eval_custom_logloss: 2.66991 |  0:00:28s
epoch 52 | loss: 0.50384 | eval_custom_logloss: 2.43884 |  0:00:29s

Early stopping occurred at epoch 52 with best_epoch = 32 and best_eval_custom_logloss = 2.01287
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5441200000000002, 'Log Loss - std': 0.33142153460510076} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 1.5441200000000002 and parameters: {'n_d': 43, 'n_steps': 7, 'gamma': 1.6824736154686237, 'cat_emb_dim': 1, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.001006339962448626, 'mask_type': 'sparsemax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax', 'n_a': 36, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65463 | eval_custom_logloss: 1.01018 |  0:00:00s
epoch 1  | loss: 1.08845 | eval_custom_logloss: 0.79587 |  0:00:01s
epoch 2  | loss: 1.09583 | eval_custom_logloss: 0.91934 |  0:00:01s
epoch 3  | loss: 0.97764 | eval_custom_logloss: 1.03797 |  0:00:02s
epoch 4  | loss: 0.93798 | eval_custom_logloss: 0.70758 |  0:00:02s
epoch 5  | loss: 0.82031 | eval_custom_logloss: 0.72494 |  0:00:03s
epoch 6  | loss: 0.76891 | eval_custom_logloss: 0.92085 |  0:00:03s
epoch 7  | loss: 0.72904 | eval_custom_logloss: 0.65626 |  0:00:04s
epoch 8  | loss: 0.71301 | eval_custom_logloss: 0.82463 |  0:00:04s
epoch 9  | loss: 0.69344 | eval_custom_logloss: 0.60078 |  0:00:05s
epoch 10 | loss: 0.66249 | eval_custom_logloss: 0.76877 |  0:00:05s
epoch 11 | loss: 0.63992 | eval_custom_logloss: 0.66197 |  0:00:06s
epoch 12 | loss: 0.6541  | eval_custom_logloss: 0.66099 |  0:00:06s
epoch 13 | loss: 0.62468 | eval_custom_logloss: 0.62287 |  0:00:07s
epoch 14 | loss: 0.60823 | eval_custom_logloss: 0.60131 |  0:00:07s
epoch 15 | loss: 0.57657 | eval_custom_logloss: 0.58578 |  0:00:08s
epoch 16 | loss: 0.61047 | eval_custom_logloss: 0.63289 |  0:00:08s
epoch 17 | loss: 0.60317 | eval_custom_logloss: 0.56331 |  0:00:09s
epoch 18 | loss: 0.57153 | eval_custom_logloss: 0.52461 |  0:00:09s
epoch 19 | loss: 0.52694 | eval_custom_logloss: 0.55461 |  0:00:10s
epoch 20 | loss: 0.57072 | eval_custom_logloss: 0.52645 |  0:00:10s
epoch 21 | loss: 0.53826 | eval_custom_logloss: 0.53933 |  0:00:11s
epoch 22 | loss: 0.54568 | eval_custom_logloss: 0.5833  |  0:00:11s
epoch 23 | loss: 0.51061 | eval_custom_logloss: 0.56312 |  0:00:11s
epoch 24 | loss: 0.47877 | eval_custom_logloss: 0.50778 |  0:00:12s
epoch 25 | loss: 0.461   | eval_custom_logloss: 0.52213 |  0:00:12s
epoch 26 | loss: 0.45929 | eval_custom_logloss: 0.48032 |  0:00:13s
epoch 27 | loss: 0.44369 | eval_custom_logloss: 0.49266 |  0:00:13s
epoch 28 | loss: 0.43753 | eval_custom_logloss: 0.4447  |  0:00:14s
epoch 29 | loss: 0.42907 | eval_custom_logloss: 0.56219 |  0:00:14s
epoch 30 | loss: 0.42747 | eval_custom_logloss: 0.48306 |  0:00:15s
epoch 31 | loss: 0.41855 | eval_custom_logloss: 0.70163 |  0:00:15s
epoch 32 | loss: 0.40939 | eval_custom_logloss: 0.49304 |  0:00:16s
epoch 33 | loss: 0.41098 | eval_custom_logloss: 0.535   |  0:00:16s
epoch 34 | loss: 0.51163 | eval_custom_logloss: 0.49295 |  0:00:17s
epoch 35 | loss: 0.44992 | eval_custom_logloss: 0.48696 |  0:00:17s
epoch 36 | loss: 0.52358 | eval_custom_logloss: 0.51299 |  0:00:18s
epoch 37 | loss: 0.47943 | eval_custom_logloss: 0.5359  |  0:00:18s
epoch 38 | loss: 0.51769 | eval_custom_logloss: 0.52917 |  0:00:19s
epoch 39 | loss: 0.51242 | eval_custom_logloss: 0.54204 |  0:00:19s
epoch 40 | loss: 0.49712 | eval_custom_logloss: 0.50635 |  0:00:20s
epoch 41 | loss: 0.48668 | eval_custom_logloss: 0.46967 |  0:00:20s
epoch 42 | loss: 0.46416 | eval_custom_logloss: 0.51194 |  0:00:21s
epoch 43 | loss: 0.45883 | eval_custom_logloss: 0.47283 |  0:00:21s
epoch 44 | loss: 0.47033 | eval_custom_logloss: 0.47078 |  0:00:22s
epoch 45 | loss: 0.43258 | eval_custom_logloss: 0.45275 |  0:00:22s
epoch 46 | loss: 0.40852 | eval_custom_logloss: 0.47718 |  0:00:23s
epoch 47 | loss: 0.37166 | eval_custom_logloss: 0.39858 |  0:00:23s
epoch 48 | loss: 0.33048 | eval_custom_logloss: 0.41755 |  0:00:24s
epoch 49 | loss: 0.36346 | eval_custom_logloss: 0.47377 |  0:00:24s
epoch 50 | loss: 0.36021 | eval_custom_logloss: 0.44606 |  0:00:25s
epoch 51 | loss: 0.36082 | eval_custom_logloss: 0.40513 |  0:00:25s
epoch 52 | loss: 0.32802 | eval_custom_logloss: 0.36988 |  0:00:25s
epoch 53 | loss: 0.32804 | eval_custom_logloss: 0.36949 |  0:00:26s
epoch 54 | loss: 0.3625  | eval_custom_logloss: 0.39664 |  0:00:26s
epoch 55 | loss: 0.37732 | eval_custom_logloss: 0.42158 |  0:00:27s
epoch 56 | loss: 0.33209 | eval_custom_logloss: 0.41998 |  0:00:27s
epoch 57 | loss: 0.32228 | eval_custom_logloss: 0.37556 |  0:00:28s
epoch 58 | loss: 0.31105 | eval_custom_logloss: 0.38546 |  0:00:28s
epoch 59 | loss: 0.31603 | eval_custom_logloss: 0.37387 |  0:00:29s
epoch 60 | loss: 0.31818 | eval_custom_logloss: 0.37167 |  0:00:29s
epoch 61 | loss: 0.30647 | eval_custom_logloss: 0.44617 |  0:00:30s
epoch 62 | loss: 0.29834 | eval_custom_logloss: 0.34357 |  0:00:30s
epoch 63 | loss: 0.29894 | eval_custom_logloss: 0.42072 |  0:00:31s
epoch 64 | loss: 0.27488 | eval_custom_logloss: 0.35294 |  0:00:31s
epoch 65 | loss: 0.28354 | eval_custom_logloss: 0.40675 |  0:00:32s
epoch 66 | loss: 0.30869 | eval_custom_logloss: 0.44068 |  0:00:32s
epoch 67 | loss: 0.30193 | eval_custom_logloss: 0.39781 |  0:00:33s
epoch 68 | loss: 0.3945  | eval_custom_logloss: 0.44655 |  0:00:33s
epoch 69 | loss: 0.323   | eval_custom_logloss: 0.35919 |  0:00:34s
epoch 70 | loss: 0.30199 | eval_custom_logloss: 0.35457 |  0:00:34s
epoch 71 | loss: 0.30429 | eval_custom_logloss: 0.37262 |  0:00:35s
epoch 72 | loss: 0.25657 | eval_custom_logloss: 0.3936  |  0:00:35s
epoch 73 | loss: 0.26342 | eval_custom_logloss: 0.47523 |  0:00:35s
epoch 74 | loss: 0.29825 | eval_custom_logloss: 0.47967 |  0:00:36s
epoch 75 | loss: 0.32319 | eval_custom_logloss: 0.40236 |  0:00:36s
epoch 76 | loss: 0.28029 | eval_custom_logloss: 0.42034 |  0:00:37s
epoch 77 | loss: 0.2495  | eval_custom_logloss: 0.4182  |  0:00:37s
epoch 78 | loss: 0.25532 | eval_custom_logloss: 0.41223 |  0:00:38s
epoch 79 | loss: 0.26729 | eval_custom_logloss: 0.43016 |  0:00:38s
epoch 80 | loss: 0.21846 | eval_custom_logloss: 0.36147 |  0:00:39s
epoch 81 | loss: 0.24769 | eval_custom_logloss: 0.39634 |  0:00:39s
epoch 82 | loss: 0.22393 | eval_custom_logloss: 0.37463 |  0:00:40s

Early stopping occurred at epoch 82 with best_epoch = 62 and best_eval_custom_logloss = 0.34357
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3436, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax', 'n_a': 36, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.63875 | eval_custom_logloss: 0.99509 |  0:00:00s
epoch 1  | loss: 1.14245 | eval_custom_logloss: 1.21759 |  0:00:01s
epoch 2  | loss: 0.90906 | eval_custom_logloss: 0.9751  |  0:00:01s
epoch 3  | loss: 0.88244 | eval_custom_logloss: 1.13523 |  0:00:01s
epoch 4  | loss: 0.84643 | eval_custom_logloss: 0.95482 |  0:00:02s
epoch 5  | loss: 0.73733 | eval_custom_logloss: 1.06558 |  0:00:02s
epoch 6  | loss: 0.72636 | eval_custom_logloss: 0.70232 |  0:00:03s
epoch 7  | loss: 0.72147 | eval_custom_logloss: 0.72307 |  0:00:03s
epoch 8  | loss: 0.66477 | eval_custom_logloss: 0.88875 |  0:00:04s
epoch 9  | loss: 0.66434 | eval_custom_logloss: 0.79689 |  0:00:04s
epoch 10 | loss: 0.61919 | eval_custom_logloss: 0.80441 |  0:00:05s
epoch 11 | loss: 0.58807 | eval_custom_logloss: 0.73646 |  0:00:05s
epoch 12 | loss: 0.56527 | eval_custom_logloss: 0.72443 |  0:00:06s
epoch 13 | loss: 0.5984  | eval_custom_logloss: 0.72546 |  0:00:06s
epoch 14 | loss: 0.54183 | eval_custom_logloss: 0.60016 |  0:00:07s
epoch 15 | loss: 0.53018 | eval_custom_logloss: 0.7091  |  0:00:07s
epoch 16 | loss: 0.53199 | eval_custom_logloss: 0.61882 |  0:00:08s
epoch 17 | loss: 0.54123 | eval_custom_logloss: 0.62441 |  0:00:08s
epoch 18 | loss: 0.54252 | eval_custom_logloss: 0.60443 |  0:00:09s
epoch 19 | loss: 0.55313 | eval_custom_logloss: 0.63027 |  0:00:09s
epoch 20 | loss: 0.54438 | eval_custom_logloss: 0.59448 |  0:00:10s
epoch 21 | loss: 0.5473  | eval_custom_logloss: 0.61468 |  0:00:10s
epoch 22 | loss: 0.5407  | eval_custom_logloss: 0.59003 |  0:00:11s
epoch 23 | loss: 0.49968 | eval_custom_logloss: 0.62172 |  0:00:11s
epoch 24 | loss: 0.50055 | eval_custom_logloss: 0.60761 |  0:00:11s
epoch 25 | loss: 0.49216 | eval_custom_logloss: 0.59761 |  0:00:12s
epoch 26 | loss: 0.48982 | eval_custom_logloss: 0.57968 |  0:00:12s
epoch 27 | loss: 0.46159 | eval_custom_logloss: 0.62789 |  0:00:13s
epoch 28 | loss: 0.48649 | eval_custom_logloss: 0.58518 |  0:00:13s
epoch 29 | loss: 0.47896 | eval_custom_logloss: 0.60855 |  0:00:14s
epoch 30 | loss: 0.47103 | eval_custom_logloss: 0.60294 |  0:00:14s
epoch 31 | loss: 0.49411 | eval_custom_logloss: 0.59845 |  0:00:15s
epoch 32 | loss: 0.46808 | eval_custom_logloss: 0.56822 |  0:00:15s
epoch 33 | loss: 0.4541  | eval_custom_logloss: 0.57718 |  0:00:16s
epoch 34 | loss: 0.46778 | eval_custom_logloss: 0.60297 |  0:00:16s
epoch 35 | loss: 0.45334 | eval_custom_logloss: 0.57597 |  0:00:17s
epoch 36 | loss: 0.44854 | eval_custom_logloss: 0.54673 |  0:00:17s
epoch 37 | loss: 0.43321 | eval_custom_logloss: 0.54423 |  0:00:18s
epoch 38 | loss: 0.42924 | eval_custom_logloss: 0.53762 |  0:00:18s
epoch 39 | loss: 0.48114 | eval_custom_logloss: 0.548   |  0:00:19s
epoch 40 | loss: 0.45438 | eval_custom_logloss: 0.58142 |  0:00:19s
epoch 41 | loss: 0.48532 | eval_custom_logloss: 0.59212 |  0:00:20s
epoch 42 | loss: 0.45302 | eval_custom_logloss: 0.5517  |  0:00:20s
epoch 43 | loss: 0.41747 | eval_custom_logloss: 0.56159 |  0:00:21s
epoch 44 | loss: 0.41318 | eval_custom_logloss: 0.54175 |  0:00:21s
epoch 45 | loss: 0.39617 | eval_custom_logloss: 0.53667 |  0:00:22s
epoch 46 | loss: 0.39796 | eval_custom_logloss: 0.53326 |  0:00:22s
epoch 47 | loss: 0.39092 | eval_custom_logloss: 0.52531 |  0:00:22s
epoch 48 | loss: 0.38625 | eval_custom_logloss: 0.53106 |  0:00:23s
epoch 49 | loss: 0.39182 | eval_custom_logloss: 0.54604 |  0:00:23s
epoch 50 | loss: 0.37981 | eval_custom_logloss: 0.47598 |  0:00:24s
epoch 51 | loss: 0.34603 | eval_custom_logloss: 0.55241 |  0:00:24s
epoch 52 | loss: 0.35321 | eval_custom_logloss: 0.50654 |  0:00:25s
epoch 53 | loss: 0.35639 | eval_custom_logloss: 0.51246 |  0:00:25s
epoch 54 | loss: 0.33463 | eval_custom_logloss: 0.4829  |  0:00:26s
epoch 55 | loss: 0.31954 | eval_custom_logloss: 0.57801 |  0:00:26s
epoch 56 | loss: 0.34407 | eval_custom_logloss: 0.56638 |  0:00:27s
epoch 57 | loss: 0.33557 | eval_custom_logloss: 0.49371 |  0:00:27s
epoch 58 | loss: 0.32329 | eval_custom_logloss: 0.49159 |  0:00:28s
epoch 59 | loss: 0.30922 | eval_custom_logloss: 0.53012 |  0:00:28s
epoch 60 | loss: 0.30047 | eval_custom_logloss: 0.50745 |  0:00:29s
epoch 61 | loss: 0.30172 | eval_custom_logloss: 0.48821 |  0:00:29s
epoch 62 | loss: 0.29236 | eval_custom_logloss: 0.50872 |  0:00:30s
epoch 63 | loss: 0.27689 | eval_custom_logloss: 0.53569 |  0:00:30s
epoch 64 | loss: 0.32127 | eval_custom_logloss: 0.53724 |  0:00:31s
epoch 65 | loss: 0.31263 | eval_custom_logloss: 0.61691 |  0:00:31s
epoch 66 | loss: 0.27411 | eval_custom_logloss: 0.49758 |  0:00:32s
epoch 67 | loss: 0.29849 | eval_custom_logloss: 0.54502 |  0:00:32s
epoch 68 | loss: 0.3015  | eval_custom_logloss: 0.5543  |  0:00:32s
epoch 69 | loss: 0.31128 | eval_custom_logloss: 0.51289 |  0:00:33s
epoch 70 | loss: 0.26092 | eval_custom_logloss: 0.51105 |  0:00:33s

Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_custom_logloss = 0.47598
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4098, 'Log Loss - std': 0.06619999999999998} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax', 'n_a': 36, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.55358 | eval_custom_logloss: 1.06302 |  0:00:00s
epoch 1  | loss: 1.12762 | eval_custom_logloss: 0.85324 |  0:00:01s
epoch 2  | loss: 0.98656 | eval_custom_logloss: 1.006   |  0:00:01s
epoch 3  | loss: 0.90882 | eval_custom_logloss: 0.87224 |  0:00:02s
epoch 4  | loss: 0.78281 | eval_custom_logloss: 0.86676 |  0:00:02s
epoch 5  | loss: 0.82737 | eval_custom_logloss: 0.80036 |  0:00:03s
epoch 6  | loss: 0.75832 | eval_custom_logloss: 0.77475 |  0:00:03s
epoch 7  | loss: 0.73961 | eval_custom_logloss: 0.73001 |  0:00:04s
epoch 8  | loss: 0.72946 | eval_custom_logloss: 0.65581 |  0:00:04s
epoch 9  | loss: 0.70812 | eval_custom_logloss: 0.60615 |  0:00:05s
epoch 10 | loss: 0.61889 | eval_custom_logloss: 0.60439 |  0:00:05s
epoch 11 | loss: 0.60099 | eval_custom_logloss: 0.66259 |  0:00:06s
epoch 12 | loss: 0.64334 | eval_custom_logloss: 0.68222 |  0:00:06s
epoch 13 | loss: 0.63509 | eval_custom_logloss: 0.69918 |  0:00:06s
epoch 14 | loss: 0.62208 | eval_custom_logloss: 0.62412 |  0:00:07s
epoch 15 | loss: 0.58882 | eval_custom_logloss: 0.58021 |  0:00:07s
epoch 16 | loss: 0.57088 | eval_custom_logloss: 0.64937 |  0:00:08s
epoch 17 | loss: 0.58331 | eval_custom_logloss: 0.55701 |  0:00:08s
epoch 18 | loss: 0.55273 | eval_custom_logloss: 0.52866 |  0:00:09s
epoch 19 | loss: 0.52901 | eval_custom_logloss: 0.55295 |  0:00:09s
epoch 20 | loss: 0.54073 | eval_custom_logloss: 0.54948 |  0:00:10s
epoch 21 | loss: 0.49901 | eval_custom_logloss: 0.52222 |  0:00:10s
epoch 22 | loss: 0.48987 | eval_custom_logloss: 0.51938 |  0:00:11s
epoch 23 | loss: 0.48645 | eval_custom_logloss: 0.48924 |  0:00:11s
epoch 24 | loss: 0.46108 | eval_custom_logloss: 0.48088 |  0:00:12s
epoch 25 | loss: 0.46901 | eval_custom_logloss: 0.4601  |  0:00:12s
epoch 26 | loss: 0.42862 | eval_custom_logloss: 0.4614  |  0:00:13s
epoch 27 | loss: 0.4152  | eval_custom_logloss: 0.45141 |  0:00:13s
epoch 28 | loss: 0.44528 | eval_custom_logloss: 0.4583  |  0:00:14s
epoch 29 | loss: 0.40337 | eval_custom_logloss: 0.42957 |  0:00:14s
epoch 30 | loss: 0.37913 | eval_custom_logloss: 0.40868 |  0:00:15s
epoch 31 | loss: 0.36094 | eval_custom_logloss: 0.45514 |  0:00:15s
epoch 32 | loss: 0.38525 | eval_custom_logloss: 0.45892 |  0:00:16s
epoch 33 | loss: 0.38955 | eval_custom_logloss: 0.45358 |  0:00:16s
epoch 34 | loss: 0.40846 | eval_custom_logloss: 0.43074 |  0:00:17s
epoch 35 | loss: 0.37013 | eval_custom_logloss: 0.43605 |  0:00:17s
epoch 36 | loss: 0.36151 | eval_custom_logloss: 0.38782 |  0:00:18s
epoch 37 | loss: 0.32872 | eval_custom_logloss: 0.45668 |  0:00:18s
epoch 38 | loss: 0.36133 | eval_custom_logloss: 0.44627 |  0:00:19s
epoch 39 | loss: 0.35769 | eval_custom_logloss: 0.4373  |  0:00:19s
epoch 40 | loss: 0.31949 | eval_custom_logloss: 0.38473 |  0:00:20s
epoch 41 | loss: 0.31063 | eval_custom_logloss: 0.39411 |  0:00:20s
epoch 42 | loss: 0.30845 | eval_custom_logloss: 0.40693 |  0:00:21s
epoch 43 | loss: 0.30194 | eval_custom_logloss: 0.40272 |  0:00:21s
epoch 44 | loss: 0.33073 | eval_custom_logloss: 0.39361 |  0:00:22s
epoch 45 | loss: 0.27975 | eval_custom_logloss: 0.39489 |  0:00:22s
epoch 46 | loss: 0.29131 | eval_custom_logloss: 0.43715 |  0:00:23s
epoch 47 | loss: 0.29764 | eval_custom_logloss: 0.45097 |  0:00:23s
epoch 48 | loss: 0.29861 | eval_custom_logloss: 0.50424 |  0:00:24s
epoch 49 | loss: 0.27117 | eval_custom_logloss: 0.3908  |  0:00:24s
epoch 50 | loss: 0.25705 | eval_custom_logloss: 0.3972  |  0:00:25s
epoch 51 | loss: 0.27133 | eval_custom_logloss: 0.37786 |  0:00:25s
epoch 52 | loss: 0.27998 | eval_custom_logloss: 0.36903 |  0:00:26s
epoch 53 | loss: 0.26555 | eval_custom_logloss: 0.40892 |  0:00:26s
epoch 54 | loss: 0.26157 | eval_custom_logloss: 0.4141  |  0:00:27s
epoch 55 | loss: 0.27625 | eval_custom_logloss: 0.42374 |  0:00:27s
epoch 56 | loss: 0.22131 | eval_custom_logloss: 0.40403 |  0:00:28s
epoch 57 | loss: 0.22389 | eval_custom_logloss: 0.38053 |  0:00:28s
epoch 58 | loss: 0.20696 | eval_custom_logloss: 0.44039 |  0:00:29s
epoch 59 | loss: 0.22155 | eval_custom_logloss: 0.46788 |  0:00:29s
epoch 60 | loss: 0.18021 | eval_custom_logloss: 0.43353 |  0:00:30s
epoch 61 | loss: 0.23355 | eval_custom_logloss: 0.46325 |  0:00:30s
epoch 62 | loss: 0.21375 | eval_custom_logloss: 0.45498 |  0:00:31s
epoch 63 | loss: 0.21875 | eval_custom_logloss: 0.4224  |  0:00:31s
epoch 64 | loss: 0.22537 | eval_custom_logloss: 0.44069 |  0:00:32s
epoch 65 | loss: 0.24467 | eval_custom_logloss: 0.42197 |  0:00:32s
epoch 66 | loss: 0.21727 | eval_custom_logloss: 0.43054 |  0:00:33s
epoch 67 | loss: 0.20769 | eval_custom_logloss: 0.4204  |  0:00:33s
epoch 68 | loss: 0.20285 | eval_custom_logloss: 0.4455  |  0:00:34s
epoch 69 | loss: 0.18995 | eval_custom_logloss: 0.47305 |  0:00:34s
epoch 70 | loss: 0.28487 | eval_custom_logloss: 0.55397 |  0:00:35s
epoch 71 | loss: 0.26262 | eval_custom_logloss: 0.43233 |  0:00:35s
epoch 72 | loss: 0.24283 | eval_custom_logloss: 0.43497 |  0:00:36s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 0.36903
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.39620000000000005, 'Log Loss - std': 0.057372002463454815} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax', 'n_a': 36, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.69982 | eval_custom_logloss: 0.84634 |  0:00:00s
epoch 1  | loss: 1.1326  | eval_custom_logloss: 0.85881 |  0:00:01s
epoch 2  | loss: 0.96766 | eval_custom_logloss: 1.00618 |  0:00:01s
epoch 3  | loss: 0.88665 | eval_custom_logloss: 0.88246 |  0:00:02s
epoch 4  | loss: 0.80591 | eval_custom_logloss: 1.11327 |  0:00:02s
epoch 5  | loss: 0.80636 | eval_custom_logloss: 0.9766  |  0:00:03s
epoch 6  | loss: 0.907   | eval_custom_logloss: 0.81792 |  0:00:03s
epoch 7  | loss: 0.71763 | eval_custom_logloss: 0.78967 |  0:00:04s
epoch 8  | loss: 0.66132 | eval_custom_logloss: 0.71816 |  0:00:04s
epoch 9  | loss: 0.63635 | eval_custom_logloss: 0.74626 |  0:00:05s
epoch 10 | loss: 0.65991 | eval_custom_logloss: 0.76941 |  0:00:05s
epoch 11 | loss: 0.64339 | eval_custom_logloss: 0.64537 |  0:00:06s
epoch 12 | loss: 0.6383  | eval_custom_logloss: 0.68769 |  0:00:06s
epoch 13 | loss: 0.55298 | eval_custom_logloss: 0.61555 |  0:00:07s
epoch 14 | loss: 0.56904 | eval_custom_logloss: 0.63659 |  0:00:07s
epoch 15 | loss: 0.55247 | eval_custom_logloss: 0.59762 |  0:00:07s
epoch 16 | loss: 0.5256  | eval_custom_logloss: 0.64828 |  0:00:08s
epoch 17 | loss: 0.51401 | eval_custom_logloss: 0.58127 |  0:00:08s
epoch 18 | loss: 0.51612 | eval_custom_logloss: 0.57785 |  0:00:09s
epoch 19 | loss: 0.49948 | eval_custom_logloss: 0.60632 |  0:00:09s
epoch 20 | loss: 0.49587 | eval_custom_logloss: 0.59066 |  0:00:10s
epoch 21 | loss: 0.51117 | eval_custom_logloss: 0.60825 |  0:00:10s
epoch 22 | loss: 0.46215 | eval_custom_logloss: 0.53982 |  0:00:11s
epoch 23 | loss: 0.47353 | eval_custom_logloss: 0.52221 |  0:00:11s
epoch 24 | loss: 0.45611 | eval_custom_logloss: 0.5246  |  0:00:12s
epoch 25 | loss: 0.43733 | eval_custom_logloss: 0.53658 |  0:00:12s
epoch 26 | loss: 0.44297 | eval_custom_logloss: 0.51521 |  0:00:13s
epoch 27 | loss: 0.41624 | eval_custom_logloss: 0.52175 |  0:00:13s
epoch 28 | loss: 0.4556  | eval_custom_logloss: 0.54258 |  0:00:14s
epoch 29 | loss: 0.39534 | eval_custom_logloss: 0.50937 |  0:00:14s
epoch 30 | loss: 0.38887 | eval_custom_logloss: 0.47604 |  0:00:15s
epoch 31 | loss: 0.38209 | eval_custom_logloss: 0.46877 |  0:00:15s
epoch 32 | loss: 0.3714  | eval_custom_logloss: 0.47254 |  0:00:16s
epoch 33 | loss: 0.37209 | eval_custom_logloss: 0.46729 |  0:00:16s
epoch 34 | loss: 0.36111 | eval_custom_logloss: 0.50387 |  0:00:17s
epoch 35 | loss: 0.34195 | eval_custom_logloss: 0.4377  |  0:00:17s
epoch 36 | loss: 0.33949 | eval_custom_logloss: 0.44002 |  0:00:18s
epoch 37 | loss: 0.33933 | eval_custom_logloss: 0.44062 |  0:00:18s
epoch 38 | loss: 0.33284 | eval_custom_logloss: 0.42967 |  0:00:19s
epoch 39 | loss: 0.38555 | eval_custom_logloss: 0.53009 |  0:00:19s
epoch 40 | loss: 0.34536 | eval_custom_logloss: 0.47933 |  0:00:20s
epoch 41 | loss: 0.3166  | eval_custom_logloss: 0.51043 |  0:00:20s
epoch 42 | loss: 0.29449 | eval_custom_logloss: 0.5276  |  0:00:21s
epoch 43 | loss: 0.31067 | eval_custom_logloss: 0.45929 |  0:00:21s
epoch 44 | loss: 0.30488 | eval_custom_logloss: 0.46921 |  0:00:22s
epoch 45 | loss: 0.37424 | eval_custom_logloss: 0.5618  |  0:00:22s
epoch 46 | loss: 0.32461 | eval_custom_logloss: 0.52883 |  0:00:23s
epoch 47 | loss: 0.32541 | eval_custom_logloss: 0.49066 |  0:00:23s
epoch 48 | loss: 0.32426 | eval_custom_logloss: 0.51197 |  0:00:24s
epoch 49 | loss: 0.29311 | eval_custom_logloss: 0.53963 |  0:00:24s
epoch 50 | loss: 0.29347 | eval_custom_logloss: 0.53429 |  0:00:25s
epoch 51 | loss: 0.2977  | eval_custom_logloss: 0.43825 |  0:00:25s
epoch 52 | loss: 0.28428 | eval_custom_logloss: 0.44494 |  0:00:25s
epoch 53 | loss: 0.30381 | eval_custom_logloss: 0.46195 |  0:00:26s
epoch 54 | loss: 0.29508 | eval_custom_logloss: 0.51615 |  0:00:26s
epoch 55 | loss: 0.26914 | eval_custom_logloss: 0.48357 |  0:00:27s
epoch 56 | loss: 0.28141 | eval_custom_logloss: 0.48441 |  0:00:27s
epoch 57 | loss: 0.24095 | eval_custom_logloss: 0.49189 |  0:00:28s
epoch 58 | loss: 0.23577 | eval_custom_logloss: 0.4189  |  0:00:28s
epoch 59 | loss: 0.23038 | eval_custom_logloss: 0.42109 |  0:00:29s
epoch 60 | loss: 0.24545 | eval_custom_logloss: 0.47845 |  0:00:29s
epoch 61 | loss: 0.24941 | eval_custom_logloss: 0.54756 |  0:00:30s
epoch 62 | loss: 0.24689 | eval_custom_logloss: 0.58019 |  0:00:30s
epoch 63 | loss: 0.22955 | eval_custom_logloss: 0.52253 |  0:00:31s
epoch 64 | loss: 0.23276 | eval_custom_logloss: 0.45425 |  0:00:31s
epoch 65 | loss: 0.2307  | eval_custom_logloss: 0.50199 |  0:00:32s
epoch 66 | loss: 0.23079 | eval_custom_logloss: 0.61069 |  0:00:32s
epoch 67 | loss: 0.26081 | eval_custom_logloss: 0.55645 |  0:00:33s
epoch 68 | loss: 0.30786 | eval_custom_logloss: 0.57112 |  0:00:33s
epoch 69 | loss: 0.28408 | eval_custom_logloss: 0.55949 |  0:00:34s
epoch 70 | loss: 0.27611 | eval_custom_logloss: 0.54906 |  0:00:34s
epoch 71 | loss: 0.25562 | eval_custom_logloss: 0.49926 |  0:00:34s
epoch 72 | loss: 0.25659 | eval_custom_logloss: 0.52278 |  0:00:35s
epoch 73 | loss: 0.25539 | eval_custom_logloss: 0.49926 |  0:00:35s
epoch 74 | loss: 0.24212 | eval_custom_logloss: 0.50537 |  0:00:36s
epoch 75 | loss: 0.25897 | eval_custom_logloss: 0.51507 |  0:00:36s
epoch 76 | loss: 0.2386  | eval_custom_logloss: 0.54441 |  0:00:37s
epoch 77 | loss: 0.26849 | eval_custom_logloss: 0.57129 |  0:00:37s
epoch 78 | loss: 0.24912 | eval_custom_logloss: 0.56968 |  0:00:38s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.4189
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40187500000000004, 'Log Loss - std': 0.050648562417900854} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax', 'n_a': 36, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.74281 | eval_custom_logloss: 1.03814 |  0:00:00s
epoch 1  | loss: 1.18105 | eval_custom_logloss: 1.13102 |  0:00:01s
epoch 2  | loss: 0.89252 | eval_custom_logloss: 0.94444 |  0:00:01s
epoch 3  | loss: 0.92629 | eval_custom_logloss: 0.78123 |  0:00:02s
epoch 4  | loss: 0.88078 | eval_custom_logloss: 1.06648 |  0:00:02s
epoch 5  | loss: 0.87951 | eval_custom_logloss: 1.01781 |  0:00:03s
epoch 6  | loss: 0.76992 | eval_custom_logloss: 0.75501 |  0:00:03s
epoch 7  | loss: 0.74116 | eval_custom_logloss: 0.6613  |  0:00:04s
epoch 8  | loss: 0.70792 | eval_custom_logloss: 0.65257 |  0:00:04s
epoch 9  | loss: 0.68225 | eval_custom_logloss: 0.65631 |  0:00:05s
epoch 10 | loss: 0.68514 | eval_custom_logloss: 0.64504 |  0:00:05s
epoch 11 | loss: 0.65586 | eval_custom_logloss: 0.58317 |  0:00:06s
epoch 12 | loss: 0.62928 | eval_custom_logloss: 0.55555 |  0:00:06s
epoch 13 | loss: 0.62214 | eval_custom_logloss: 0.54405 |  0:00:07s
epoch 14 | loss: 0.61872 | eval_custom_logloss: 0.53437 |  0:00:07s
epoch 15 | loss: 0.59444 | eval_custom_logloss: 0.50362 |  0:00:08s
epoch 16 | loss: 0.60269 | eval_custom_logloss: 0.66983 |  0:00:08s
epoch 17 | loss: 0.61075 | eval_custom_logloss: 0.53319 |  0:00:09s
epoch 18 | loss: 0.56642 | eval_custom_logloss: 0.47237 |  0:00:09s
epoch 19 | loss: 0.5672  | eval_custom_logloss: 0.52234 |  0:00:10s
epoch 20 | loss: 0.5749  | eval_custom_logloss: 0.57249 |  0:00:10s
epoch 21 | loss: 0.56015 | eval_custom_logloss: 0.60263 |  0:00:11s
epoch 22 | loss: 0.58245 | eval_custom_logloss: 0.48949 |  0:00:11s
epoch 23 | loss: 0.52422 | eval_custom_logloss: 0.47332 |  0:00:12s
epoch 24 | loss: 0.52785 | eval_custom_logloss: 0.46406 |  0:00:12s
epoch 25 | loss: 0.50983 | eval_custom_logloss: 0.41795 |  0:00:13s
epoch 26 | loss: 0.49155 | eval_custom_logloss: 0.4426  |  0:00:13s
epoch 27 | loss: 0.46407 | eval_custom_logloss: 0.40746 |  0:00:14s
epoch 28 | loss: 0.45366 | eval_custom_logloss: 0.37837 |  0:00:14s
epoch 29 | loss: 0.43552 | eval_custom_logloss: 0.37859 |  0:00:15s
epoch 30 | loss: 0.43467 | eval_custom_logloss: 0.36016 |  0:00:15s
epoch 31 | loss: 0.39127 | eval_custom_logloss: 0.34828 |  0:00:16s
epoch 32 | loss: 0.40315 | eval_custom_logloss: 0.35186 |  0:00:16s
epoch 33 | loss: 0.41161 | eval_custom_logloss: 0.31047 |  0:00:17s
epoch 34 | loss: 0.39705 | eval_custom_logloss: 0.39248 |  0:00:18s
epoch 35 | loss: 0.42284 | eval_custom_logloss: 0.34815 |  0:00:18s
epoch 36 | loss: 0.4029  | eval_custom_logloss: 0.35763 |  0:00:19s
epoch 37 | loss: 0.36986 | eval_custom_logloss: 0.32765 |  0:00:19s
epoch 38 | loss: 0.33605 | eval_custom_logloss: 0.33702 |  0:00:20s
epoch 39 | loss: 0.38578 | eval_custom_logloss: 0.32333 |  0:00:20s
epoch 40 | loss: 0.36316 | eval_custom_logloss: 0.38259 |  0:00:21s
epoch 41 | loss: 0.41998 | eval_custom_logloss: 0.37779 |  0:00:21s
epoch 42 | loss: 0.38006 | eval_custom_logloss: 0.31142 |  0:00:22s
epoch 43 | loss: 0.34809 | eval_custom_logloss: 0.38219 |  0:00:22s
epoch 44 | loss: 0.38092 | eval_custom_logloss: 0.41182 |  0:00:23s
epoch 45 | loss: 0.37931 | eval_custom_logloss: 0.35411 |  0:00:23s
epoch 46 | loss: 0.37476 | eval_custom_logloss: 0.27667 |  0:00:24s
epoch 47 | loss: 0.3295  | eval_custom_logloss: 0.28737 |  0:00:24s
epoch 48 | loss: 0.37424 | eval_custom_logloss: 0.29201 |  0:00:25s
epoch 49 | loss: 0.35721 | eval_custom_logloss: 0.26114 |  0:00:25s
epoch 50 | loss: 0.34088 | eval_custom_logloss: 0.26354 |  0:00:26s
epoch 51 | loss: 0.31979 | eval_custom_logloss: 0.26863 |  0:00:26s
epoch 52 | loss: 0.29881 | eval_custom_logloss: 0.35738 |  0:00:27s
epoch 53 | loss: 0.33367 | eval_custom_logloss: 0.2936  |  0:00:27s
epoch 54 | loss: 0.3412  | eval_custom_logloss: 0.34304 |  0:00:28s
epoch 55 | loss: 0.3042  | eval_custom_logloss: 0.25322 |  0:00:28s
epoch 56 | loss: 0.28247 | eval_custom_logloss: 0.26191 |  0:00:29s
epoch 57 | loss: 0.25865 | eval_custom_logloss: 0.27978 |  0:00:29s
epoch 58 | loss: 0.2905  | eval_custom_logloss: 0.29356 |  0:00:30s
epoch 59 | loss: 0.28284 | eval_custom_logloss: 0.26414 |  0:00:30s
epoch 60 | loss: 0.27937 | eval_custom_logloss: 0.30902 |  0:00:31s
epoch 61 | loss: 0.30932 | eval_custom_logloss: 0.31414 |  0:00:31s
epoch 62 | loss: 0.29013 | eval_custom_logloss: 0.32384 |  0:00:32s
epoch 63 | loss: 0.27585 | eval_custom_logloss: 0.26016 |  0:00:32s
epoch 64 | loss: 0.28259 | eval_custom_logloss: 0.28245 |  0:00:32s
epoch 65 | loss: 0.26202 | eval_custom_logloss: 0.26087 |  0:00:33s
epoch 66 | loss: 0.2901  | eval_custom_logloss: 0.29009 |  0:00:33s
epoch 67 | loss: 0.26026 | eval_custom_logloss: 0.26582 |  0:00:34s
epoch 68 | loss: 0.27674 | eval_custom_logloss: 0.27329 |  0:00:34s
epoch 69 | loss: 0.24334 | eval_custom_logloss: 0.29429 |  0:00:35s
epoch 70 | loss: 0.29332 | eval_custom_logloss: 0.27217 |  0:00:35s
epoch 71 | loss: 0.27206 | eval_custom_logloss: 0.26115 |  0:00:36s
epoch 72 | loss: 0.29216 | eval_custom_logloss: 0.34923 |  0:00:36s
epoch 73 | loss: 0.2688  | eval_custom_logloss: 0.26807 |  0:00:37s
epoch 74 | loss: 0.26881 | eval_custom_logloss: 0.32541 |  0:00:37s
epoch 75 | loss: 0.26598 | eval_custom_logloss: 0.28115 |  0:00:38s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 0.25322
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.37214, 'Log Loss - std': 0.07475896200456504} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 0.37214 and parameters: {'n_d': 36, 'n_steps': 6, 'gamma': 1.2879697909916161, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 2, 'momentum': 0.09808991167616203, 'mask_type': 'entmax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.18376 | eval_custom_logloss: 0.87785 |  0:00:00s
epoch 1  | loss: 0.91686 | eval_custom_logloss: 0.86984 |  0:00:00s
epoch 2  | loss: 0.83097 | eval_custom_logloss: 0.77782 |  0:00:01s
epoch 3  | loss: 0.80892 | eval_custom_logloss: 0.77575 |  0:00:01s
epoch 4  | loss: 0.77547 | eval_custom_logloss: 0.74468 |  0:00:01s
epoch 5  | loss: 0.72319 | eval_custom_logloss: 0.82881 |  0:00:02s
epoch 6  | loss: 0.71055 | eval_custom_logloss: 0.68087 |  0:00:02s
epoch 7  | loss: 0.67248 | eval_custom_logloss: 0.74846 |  0:00:02s
epoch 8  | loss: 0.63772 | eval_custom_logloss: 0.7784  |  0:00:03s
epoch 9  | loss: 0.596   | eval_custom_logloss: 0.69713 |  0:00:03s
epoch 10 | loss: 0.61718 | eval_custom_logloss: 0.69142 |  0:00:03s
epoch 11 | loss: 0.57596 | eval_custom_logloss: 0.57953 |  0:00:04s
epoch 12 | loss: 0.57145 | eval_custom_logloss: 0.60552 |  0:00:04s
epoch 13 | loss: 0.57016 | eval_custom_logloss: 0.65457 |  0:00:04s
epoch 14 | loss: 0.57359 | eval_custom_logloss: 0.63388 |  0:00:05s
epoch 15 | loss: 0.54894 | eval_custom_logloss: 0.60588 |  0:00:05s
epoch 16 | loss: 0.54549 | eval_custom_logloss: 0.55008 |  0:00:05s
epoch 17 | loss: 0.54841 | eval_custom_logloss: 0.58348 |  0:00:06s
epoch 18 | loss: 0.54233 | eval_custom_logloss: 0.5769  |  0:00:06s
epoch 19 | loss: 0.56607 | eval_custom_logloss: 0.62241 |  0:00:06s
epoch 20 | loss: 0.55118 | eval_custom_logloss: 0.58634 |  0:00:07s
epoch 21 | loss: 0.52752 | eval_custom_logloss: 0.55069 |  0:00:07s
epoch 22 | loss: 0.51599 | eval_custom_logloss: 0.67922 |  0:00:08s
epoch 23 | loss: 0.49496 | eval_custom_logloss: 0.56014 |  0:00:08s
epoch 24 | loss: 0.52116 | eval_custom_logloss: 0.65333 |  0:00:08s
epoch 25 | loss: 0.5108  | eval_custom_logloss: 0.50934 |  0:00:09s
epoch 26 | loss: 0.4788  | eval_custom_logloss: 0.51356 |  0:00:09s
epoch 27 | loss: 0.48072 | eval_custom_logloss: 0.5094  |  0:00:09s
epoch 28 | loss: 0.46    | eval_custom_logloss: 0.4824  |  0:00:10s
epoch 29 | loss: 0.43844 | eval_custom_logloss: 0.51657 |  0:00:10s
epoch 30 | loss: 0.44592 | eval_custom_logloss: 0.51072 |  0:00:11s
epoch 31 | loss: 0.43892 | eval_custom_logloss: 0.49436 |  0:00:11s
epoch 32 | loss: 0.41487 | eval_custom_logloss: 0.4528  |  0:00:11s
epoch 33 | loss: 0.4717  | eval_custom_logloss: 0.46202 |  0:00:12s
epoch 34 | loss: 0.44701 | eval_custom_logloss: 0.48501 |  0:00:12s
epoch 35 | loss: 0.44496 | eval_custom_logloss: 0.49427 |  0:00:12s
epoch 36 | loss: 0.44685 | eval_custom_logloss: 0.47421 |  0:00:13s
epoch 37 | loss: 0.42927 | eval_custom_logloss: 0.51959 |  0:00:13s
epoch 38 | loss: 0.42035 | eval_custom_logloss: 0.51198 |  0:00:14s
epoch 39 | loss: 0.40372 | eval_custom_logloss: 0.53286 |  0:00:14s
epoch 40 | loss: 0.3808  | eval_custom_logloss: 0.58558 |  0:00:14s
epoch 41 | loss: 0.37968 | eval_custom_logloss: 0.54117 |  0:00:15s
epoch 42 | loss: 0.38136 | eval_custom_logloss: 0.44895 |  0:00:15s
epoch 43 | loss: 0.36871 | eval_custom_logloss: 0.50255 |  0:00:15s
epoch 44 | loss: 0.36257 | eval_custom_logloss: 0.46269 |  0:00:16s
epoch 45 | loss: 0.34325 | eval_custom_logloss: 0.52087 |  0:00:16s
epoch 46 | loss: 0.36219 | eval_custom_logloss: 0.64566 |  0:00:17s
epoch 47 | loss: 0.33843 | eval_custom_logloss: 0.57223 |  0:00:17s
epoch 48 | loss: 0.341   | eval_custom_logloss: 0.58607 |  0:00:17s
epoch 49 | loss: 0.32394 | eval_custom_logloss: 0.49196 |  0:00:18s
epoch 50 | loss: 0.35457 | eval_custom_logloss: 0.4467  |  0:00:18s
epoch 51 | loss: 0.34139 | eval_custom_logloss: 0.4263  |  0:00:18s
epoch 52 | loss: 0.33262 | eval_custom_logloss: 0.4164  |  0:00:19s
epoch 53 | loss: 0.31607 | eval_custom_logloss: 0.43785 |  0:00:19s
epoch 54 | loss: 0.30801 | eval_custom_logloss: 0.43787 |  0:00:20s
epoch 55 | loss: 0.28037 | eval_custom_logloss: 0.41076 |  0:00:20s
epoch 56 | loss: 0.3006  | eval_custom_logloss: 0.43278 |  0:00:20s
epoch 57 | loss: 0.2935  | eval_custom_logloss: 0.44384 |  0:00:21s
epoch 58 | loss: 0.31535 | eval_custom_logloss: 0.39552 |  0:00:21s
epoch 59 | loss: 0.26926 | eval_custom_logloss: 0.46654 |  0:00:21s
epoch 60 | loss: 0.26729 | eval_custom_logloss: 0.58882 |  0:00:22s
epoch 61 | loss: 0.28578 | eval_custom_logloss: 0.66139 |  0:00:22s
epoch 62 | loss: 0.28944 | eval_custom_logloss: 0.46898 |  0:00:23s
epoch 63 | loss: 0.27177 | eval_custom_logloss: 0.51396 |  0:00:23s
epoch 64 | loss: 0.28759 | eval_custom_logloss: 0.52695 |  0:00:23s
epoch 65 | loss: 0.28452 | eval_custom_logloss: 0.44717 |  0:00:24s
epoch 66 | loss: 0.28309 | eval_custom_logloss: 0.51201 |  0:00:24s
epoch 67 | loss: 0.26393 | eval_custom_logloss: 0.56276 |  0:00:24s
epoch 68 | loss: 0.2871  | eval_custom_logloss: 0.5057  |  0:00:25s
epoch 69 | loss: 0.32624 | eval_custom_logloss: 0.56474 |  0:00:25s
epoch 70 | loss: 0.25586 | eval_custom_logloss: 0.55825 |  0:00:25s
epoch 71 | loss: 0.23492 | eval_custom_logloss: 0.55739 |  0:00:26s
epoch 72 | loss: 0.23205 | eval_custom_logloss: 0.61524 |  0:00:26s
epoch 73 | loss: 0.27223 | eval_custom_logloss: 0.56205 |  0:00:26s
epoch 74 | loss: 0.27559 | eval_custom_logloss: 0.58917 |  0:00:27s
epoch 75 | loss: 0.26946 | eval_custom_logloss: 0.56655 |  0:00:27s
epoch 76 | loss: 0.28723 | eval_custom_logloss: 0.54358 |  0:00:27s
epoch 77 | loss: 0.27319 | eval_custom_logloss: 0.49058 |  0:00:28s
epoch 78 | loss: 0.25624 | eval_custom_logloss: 0.59063 |  0:00:28s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.39552
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3955, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.14985 | eval_custom_logloss: 0.96631 |  0:00:00s
epoch 1  | loss: 0.9221  | eval_custom_logloss: 0.88115 |  0:00:00s
epoch 2  | loss: 0.81324 | eval_custom_logloss: 0.89881 |  0:00:01s
epoch 3  | loss: 0.74952 | eval_custom_logloss: 0.97263 |  0:00:01s
epoch 4  | loss: 0.70318 | eval_custom_logloss: 1.04711 |  0:00:01s
epoch 5  | loss: 0.67624 | eval_custom_logloss: 1.33017 |  0:00:02s
epoch 6  | loss: 0.66326 | eval_custom_logloss: 1.02535 |  0:00:02s
epoch 7  | loss: 0.64276 | eval_custom_logloss: 0.9557  |  0:00:02s
epoch 8  | loss: 0.63669 | eval_custom_logloss: 0.90988 |  0:00:03s
epoch 9  | loss: 0.61344 | eval_custom_logloss: 0.92348 |  0:00:03s
epoch 10 | loss: 0.60016 | eval_custom_logloss: 0.80091 |  0:00:03s
epoch 11 | loss: 0.58775 | eval_custom_logloss: 0.71755 |  0:00:04s
epoch 12 | loss: 0.53123 | eval_custom_logloss: 0.72684 |  0:00:04s
epoch 13 | loss: 0.53481 | eval_custom_logloss: 0.76576 |  0:00:05s
epoch 14 | loss: 0.5595  | eval_custom_logloss: 0.69123 |  0:00:05s
epoch 15 | loss: 0.51552 | eval_custom_logloss: 0.71565 |  0:00:05s
epoch 16 | loss: 0.49062 | eval_custom_logloss: 0.7231  |  0:00:06s
epoch 17 | loss: 0.49066 | eval_custom_logloss: 0.76848 |  0:00:06s
epoch 18 | loss: 0.48116 | eval_custom_logloss: 0.80832 |  0:00:06s
epoch 19 | loss: 0.47126 | eval_custom_logloss: 0.69304 |  0:00:07s
epoch 20 | loss: 0.47762 | eval_custom_logloss: 0.75226 |  0:00:07s
epoch 21 | loss: 0.48297 | eval_custom_logloss: 0.64869 |  0:00:08s
epoch 22 | loss: 0.47031 | eval_custom_logloss: 0.69404 |  0:00:08s
epoch 23 | loss: 0.4216  | eval_custom_logloss: 0.6927  |  0:00:08s
epoch 24 | loss: 0.42627 | eval_custom_logloss: 0.74732 |  0:00:09s
epoch 25 | loss: 0.43294 | eval_custom_logloss: 0.65096 |  0:00:09s
epoch 26 | loss: 0.39761 | eval_custom_logloss: 0.62888 |  0:00:09s
epoch 27 | loss: 0.40389 | eval_custom_logloss: 0.68125 |  0:00:10s
epoch 28 | loss: 0.38325 | eval_custom_logloss: 0.67911 |  0:00:10s
epoch 29 | loss: 0.36387 | eval_custom_logloss: 0.67209 |  0:00:10s
epoch 30 | loss: 0.36714 | eval_custom_logloss: 0.74746 |  0:00:11s
epoch 31 | loss: 0.37006 | eval_custom_logloss: 0.71999 |  0:00:11s
epoch 32 | loss: 0.33747 | eval_custom_logloss: 0.6807  |  0:00:12s
epoch 33 | loss: 0.32588 | eval_custom_logloss: 0.78135 |  0:00:12s
epoch 34 | loss: 0.3333  | eval_custom_logloss: 0.7251  |  0:00:12s
epoch 35 | loss: 0.3312  | eval_custom_logloss: 0.60714 |  0:00:13s
epoch 36 | loss: 0.33669 | eval_custom_logloss: 0.59061 |  0:00:13s
epoch 37 | loss: 0.30507 | eval_custom_logloss: 0.58223 |  0:00:14s
epoch 38 | loss: 0.30847 | eval_custom_logloss: 0.57988 |  0:00:14s
epoch 39 | loss: 0.30318 | eval_custom_logloss: 0.59621 |  0:00:14s
epoch 40 | loss: 0.31583 | eval_custom_logloss: 0.59109 |  0:00:15s
epoch 41 | loss: 0.29998 | eval_custom_logloss: 0.56537 |  0:00:15s
epoch 42 | loss: 0.29428 | eval_custom_logloss: 0.54665 |  0:00:15s
epoch 43 | loss: 0.29716 | eval_custom_logloss: 0.58858 |  0:00:16s
epoch 44 | loss: 0.28372 | eval_custom_logloss: 0.52869 |  0:00:16s
epoch 45 | loss: 0.2602  | eval_custom_logloss: 0.52522 |  0:00:17s
epoch 46 | loss: 0.27238 | eval_custom_logloss: 0.57789 |  0:00:17s
epoch 47 | loss: 0.26397 | eval_custom_logloss: 0.58078 |  0:00:17s
epoch 48 | loss: 0.26699 | eval_custom_logloss: 0.57458 |  0:00:18s
epoch 49 | loss: 0.25536 | eval_custom_logloss: 0.5125  |  0:00:18s
epoch 50 | loss: 0.24501 | eval_custom_logloss: 0.64375 |  0:00:18s
epoch 51 | loss: 0.28457 | eval_custom_logloss: 0.69709 |  0:00:19s
epoch 52 | loss: 0.29593 | eval_custom_logloss: 0.51228 |  0:00:19s
epoch 53 | loss: 0.26855 | eval_custom_logloss: 0.50295 |  0:00:20s
epoch 54 | loss: 0.26231 | eval_custom_logloss: 0.54178 |  0:00:20s
epoch 55 | loss: 0.24183 | eval_custom_logloss: 0.54087 |  0:00:20s
epoch 56 | loss: 0.22687 | eval_custom_logloss: 0.60689 |  0:00:21s
epoch 57 | loss: 0.25798 | eval_custom_logloss: 0.55485 |  0:00:21s
epoch 58 | loss: 0.25725 | eval_custom_logloss: 0.57846 |  0:00:21s
epoch 59 | loss: 0.24357 | eval_custom_logloss: 0.50482 |  0:00:22s
epoch 60 | loss: 0.21158 | eval_custom_logloss: 0.57202 |  0:00:22s
epoch 61 | loss: 0.23764 | eval_custom_logloss: 0.55587 |  0:00:22s
epoch 62 | loss: 0.22059 | eval_custom_logloss: 0.51176 |  0:00:23s
epoch 63 | loss: 0.22717 | eval_custom_logloss: 0.57323 |  0:00:23s
epoch 64 | loss: 0.21745 | eval_custom_logloss: 0.52702 |  0:00:23s
epoch 65 | loss: 0.22509 | eval_custom_logloss: 0.55473 |  0:00:24s
epoch 66 | loss: 0.21883 | eval_custom_logloss: 0.60656 |  0:00:24s
epoch 67 | loss: 0.2241  | eval_custom_logloss: 0.52297 |  0:00:24s
epoch 68 | loss: 0.20237 | eval_custom_logloss: 0.50107 |  0:00:25s
epoch 69 | loss: 0.2047  | eval_custom_logloss: 0.53138 |  0:00:25s
epoch 70 | loss: 0.20407 | eval_custom_logloss: 0.44994 |  0:00:26s
epoch 71 | loss: 0.20156 | eval_custom_logloss: 0.55199 |  0:00:26s
epoch 72 | loss: 0.19646 | eval_custom_logloss: 0.58928 |  0:00:26s
epoch 73 | loss: 0.23047 | eval_custom_logloss: 0.49518 |  0:00:27s
epoch 74 | loss: 0.2019  | eval_custom_logloss: 0.53811 |  0:00:27s
epoch 75 | loss: 0.19585 | eval_custom_logloss: 0.63535 |  0:00:28s
epoch 76 | loss: 0.21011 | eval_custom_logloss: 0.55053 |  0:00:28s
epoch 77 | loss: 0.20521 | eval_custom_logloss: 0.55424 |  0:00:28s
epoch 78 | loss: 0.21403 | eval_custom_logloss: 0.49946 |  0:00:29s
epoch 79 | loss: 0.18866 | eval_custom_logloss: 0.49674 |  0:00:29s
epoch 80 | loss: 0.21184 | eval_custom_logloss: 0.54978 |  0:00:29s
epoch 81 | loss: 0.20739 | eval_custom_logloss: 0.52536 |  0:00:30s
epoch 82 | loss: 0.21941 | eval_custom_logloss: 0.56978 |  0:00:30s
epoch 83 | loss: 0.20429 | eval_custom_logloss: 0.63293 |  0:00:30s
epoch 84 | loss: 0.20434 | eval_custom_logloss: 0.54914 |  0:00:31s
epoch 85 | loss: 0.21796 | eval_custom_logloss: 0.51438 |  0:00:31s
epoch 86 | loss: 0.20812 | eval_custom_logloss: 0.60117 |  0:00:32s
epoch 87 | loss: 0.21247 | eval_custom_logloss: 0.61368 |  0:00:32s
epoch 88 | loss: 0.21124 | eval_custom_logloss: 0.59249 |  0:00:32s
epoch 89 | loss: 0.20634 | eval_custom_logloss: 0.51862 |  0:00:33s
epoch 90 | loss: 0.18796 | eval_custom_logloss: 0.55307 |  0:00:33s

Early stopping occurred at epoch 90 with best_epoch = 70 and best_eval_custom_logloss = 0.44994
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4227, 'Log Loss - std': 0.027200000000000002} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.16481 | eval_custom_logloss: 0.88987 |  0:00:00s
epoch 1  | loss: 0.88057 | eval_custom_logloss: 0.84603 |  0:00:00s
epoch 2  | loss: 0.81988 | eval_custom_logloss: 0.81581 |  0:00:01s
epoch 3  | loss: 0.77522 | eval_custom_logloss: 0.86674 |  0:00:01s
epoch 4  | loss: 0.73926 | eval_custom_logloss: 0.93721 |  0:00:01s
epoch 5  | loss: 0.66325 | eval_custom_logloss: 0.89512 |  0:00:02s
epoch 6  | loss: 0.65055 | eval_custom_logloss: 0.84142 |  0:00:02s
epoch 7  | loss: 0.6225  | eval_custom_logloss: 1.02674 |  0:00:03s
epoch 8  | loss: 0.6206  | eval_custom_logloss: 0.9704  |  0:00:03s
epoch 9  | loss: 0.59685 | eval_custom_logloss: 0.87588 |  0:00:03s
epoch 10 | loss: 0.56656 | eval_custom_logloss: 0.80419 |  0:00:04s
epoch 11 | loss: 0.55398 | eval_custom_logloss: 0.90721 |  0:00:04s
epoch 12 | loss: 0.528   | eval_custom_logloss: 0.71109 |  0:00:04s
epoch 13 | loss: 0.53067 | eval_custom_logloss: 0.72381 |  0:00:05s
epoch 14 | loss: 0.52897 | eval_custom_logloss: 0.65943 |  0:00:05s
epoch 15 | loss: 0.51954 | eval_custom_logloss: 0.71402 |  0:00:06s
epoch 16 | loss: 0.51162 | eval_custom_logloss: 0.78939 |  0:00:06s
epoch 17 | loss: 0.4855  | eval_custom_logloss: 0.61699 |  0:00:06s
epoch 18 | loss: 0.4975  | eval_custom_logloss: 0.5817  |  0:00:07s
epoch 19 | loss: 0.46041 | eval_custom_logloss: 0.62662 |  0:00:07s
epoch 20 | loss: 0.45353 | eval_custom_logloss: 0.51399 |  0:00:07s
epoch 21 | loss: 0.433   | eval_custom_logloss: 0.57501 |  0:00:08s
epoch 22 | loss: 0.44399 | eval_custom_logloss: 0.49219 |  0:00:08s
epoch 23 | loss: 0.44213 | eval_custom_logloss: 0.51507 |  0:00:08s
epoch 24 | loss: 0.42011 | eval_custom_logloss: 0.49365 |  0:00:09s
epoch 25 | loss: 0.41492 | eval_custom_logloss: 0.47874 |  0:00:09s
epoch 26 | loss: 0.40917 | eval_custom_logloss: 0.48254 |  0:00:09s
epoch 27 | loss: 0.38524 | eval_custom_logloss: 0.44479 |  0:00:10s
epoch 28 | loss: 0.39814 | eval_custom_logloss: 0.44806 |  0:00:10s
epoch 29 | loss: 0.37391 | eval_custom_logloss: 0.46575 |  0:00:11s
epoch 30 | loss: 0.35612 | eval_custom_logloss: 0.48406 |  0:00:11s
epoch 31 | loss: 0.38835 | eval_custom_logloss: 0.50648 |  0:00:11s
epoch 32 | loss: 0.35813 | eval_custom_logloss: 0.47666 |  0:00:12s
epoch 33 | loss: 0.34137 | eval_custom_logloss: 0.51019 |  0:00:12s
epoch 34 | loss: 0.33602 | eval_custom_logloss: 0.45251 |  0:00:12s
epoch 35 | loss: 0.30486 | eval_custom_logloss: 0.57919 |  0:00:13s
epoch 36 | loss: 0.33006 | eval_custom_logloss: 0.4104  |  0:00:13s
epoch 37 | loss: 0.37324 | eval_custom_logloss: 0.58528 |  0:00:14s
epoch 38 | loss: 0.38592 | eval_custom_logloss: 0.5585  |  0:00:14s
epoch 39 | loss: 0.34997 | eval_custom_logloss: 0.4783  |  0:00:14s
epoch 40 | loss: 0.36667 | eval_custom_logloss: 0.63627 |  0:00:15s
epoch 41 | loss: 0.35292 | eval_custom_logloss: 0.4825  |  0:00:15s
epoch 42 | loss: 0.31358 | eval_custom_logloss: 0.49539 |  0:00:15s
epoch 43 | loss: 0.30088 | eval_custom_logloss: 0.61945 |  0:00:16s
epoch 44 | loss: 0.32884 | eval_custom_logloss: 0.44734 |  0:00:16s
epoch 45 | loss: 0.33103 | eval_custom_logloss: 0.61897 |  0:00:17s
epoch 46 | loss: 0.31179 | eval_custom_logloss: 0.61723 |  0:00:17s
epoch 47 | loss: 0.28317 | eval_custom_logloss: 0.60549 |  0:00:17s
epoch 48 | loss: 0.27985 | eval_custom_logloss: 0.42398 |  0:00:18s
epoch 49 | loss: 0.24957 | eval_custom_logloss: 0.44385 |  0:00:18s
epoch 50 | loss: 0.26601 | eval_custom_logloss: 0.49547 |  0:00:18s
epoch 51 | loss: 0.29152 | eval_custom_logloss: 0.50147 |  0:00:19s
epoch 52 | loss: 0.28416 | eval_custom_logloss: 0.44363 |  0:00:19s
epoch 53 | loss: 0.2587  | eval_custom_logloss: 0.47515 |  0:00:19s
epoch 54 | loss: 0.26334 | eval_custom_logloss: 0.45    |  0:00:20s
epoch 55 | loss: 0.24384 | eval_custom_logloss: 0.44085 |  0:00:20s
epoch 56 | loss: 0.23647 | eval_custom_logloss: 0.44671 |  0:00:20s

Early stopping occurred at epoch 56 with best_epoch = 36 and best_eval_custom_logloss = 0.4104
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4186, 'Log Loss - std': 0.0229531406710861} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.16374 | eval_custom_logloss: 0.96203 |  0:00:00s
epoch 1  | loss: 0.92842 | eval_custom_logloss: 0.94488 |  0:00:00s
epoch 2  | loss: 0.83    | eval_custom_logloss: 0.97549 |  0:00:01s
epoch 3  | loss: 0.734   | eval_custom_logloss: 0.99821 |  0:00:01s
epoch 4  | loss: 0.6783  | eval_custom_logloss: 1.23332 |  0:00:01s
epoch 5  | loss: 0.6492  | eval_custom_logloss: 1.2357  |  0:00:02s
epoch 6  | loss: 0.67736 | eval_custom_logloss: 1.00319 |  0:00:02s
epoch 7  | loss: 0.63389 | eval_custom_logloss: 0.87173 |  0:00:02s
epoch 8  | loss: 0.67264 | eval_custom_logloss: 0.92516 |  0:00:03s
epoch 9  | loss: 0.60278 | eval_custom_logloss: 0.89994 |  0:00:03s
epoch 10 | loss: 0.59237 | eval_custom_logloss: 0.79139 |  0:00:04s
epoch 11 | loss: 0.63796 | eval_custom_logloss: 0.77148 |  0:00:04s
epoch 12 | loss: 0.61404 | eval_custom_logloss: 0.73804 |  0:00:04s
epoch 13 | loss: 0.56543 | eval_custom_logloss: 0.71248 |  0:00:05s
epoch 14 | loss: 0.54979 | eval_custom_logloss: 0.75859 |  0:00:05s
epoch 15 | loss: 0.53841 | eval_custom_logloss: 0.71958 |  0:00:06s
epoch 16 | loss: 0.51226 | eval_custom_logloss: 0.80821 |  0:00:06s
epoch 17 | loss: 0.49947 | eval_custom_logloss: 0.70481 |  0:00:06s
epoch 18 | loss: 0.49375 | eval_custom_logloss: 0.75841 |  0:00:07s
epoch 19 | loss: 0.48747 | eval_custom_logloss: 0.88992 |  0:00:07s
epoch 20 | loss: 0.47518 | eval_custom_logloss: 0.78982 |  0:00:07s
epoch 21 | loss: 0.45234 | eval_custom_logloss: 0.76465 |  0:00:08s
epoch 22 | loss: 0.43494 | eval_custom_logloss: 0.63463 |  0:00:08s
epoch 23 | loss: 0.42323 | eval_custom_logloss: 0.7975  |  0:00:09s
epoch 24 | loss: 0.41632 | eval_custom_logloss: 0.63639 |  0:00:09s
epoch 25 | loss: 0.38943 | eval_custom_logloss: 0.69852 |  0:00:09s
epoch 26 | loss: 0.39478 | eval_custom_logloss: 0.57717 |  0:00:10s
epoch 27 | loss: 0.38987 | eval_custom_logloss: 0.62729 |  0:00:10s
epoch 28 | loss: 0.38158 | eval_custom_logloss: 0.62597 |  0:00:10s
epoch 29 | loss: 0.37119 | eval_custom_logloss: 0.63382 |  0:00:11s
epoch 30 | loss: 0.34287 | eval_custom_logloss: 0.64692 |  0:00:11s
epoch 31 | loss: 0.31885 | eval_custom_logloss: 0.6027  |  0:00:12s
epoch 32 | loss: 0.336   | eval_custom_logloss: 0.66659 |  0:00:12s
epoch 33 | loss: 0.34045 | eval_custom_logloss: 0.58792 |  0:00:12s
epoch 34 | loss: 0.33248 | eval_custom_logloss: 0.51246 |  0:00:13s
epoch 35 | loss: 0.29475 | eval_custom_logloss: 0.60255 |  0:00:13s
epoch 36 | loss: 0.30474 | eval_custom_logloss: 0.68558 |  0:00:13s
epoch 37 | loss: 0.30895 | eval_custom_logloss: 0.67381 |  0:00:14s
epoch 38 | loss: 0.30277 | eval_custom_logloss: 0.64028 |  0:00:14s
epoch 39 | loss: 0.32209 | eval_custom_logloss: 0.66155 |  0:00:14s
epoch 40 | loss: 0.3279  | eval_custom_logloss: 0.57805 |  0:00:15s
epoch 41 | loss: 0.29778 | eval_custom_logloss: 0.56458 |  0:00:15s
epoch 42 | loss: 0.30108 | eval_custom_logloss: 0.59268 |  0:00:16s
epoch 43 | loss: 0.31162 | eval_custom_logloss: 0.66381 |  0:00:16s
epoch 44 | loss: 0.28944 | eval_custom_logloss: 0.68675 |  0:00:16s
epoch 45 | loss: 0.28156 | eval_custom_logloss: 0.61558 |  0:00:17s
epoch 46 | loss: 0.27827 | eval_custom_logloss: 0.60865 |  0:00:17s
epoch 47 | loss: 0.27523 | eval_custom_logloss: 0.57107 |  0:00:17s
epoch 48 | loss: 0.28257 | eval_custom_logloss: 0.53853 |  0:00:18s
epoch 49 | loss: 0.27238 | eval_custom_logloss: 0.52346 |  0:00:18s
epoch 50 | loss: 0.26798 | eval_custom_logloss: 0.5983  |  0:00:18s
epoch 51 | loss: 0.28527 | eval_custom_logloss: 0.60325 |  0:00:19s
epoch 52 | loss: 0.28003 | eval_custom_logloss: 0.6172  |  0:00:19s
epoch 53 | loss: 0.27059 | eval_custom_logloss: 0.55317 |  0:00:19s
epoch 54 | loss: 0.27237 | eval_custom_logloss: 0.61767 |  0:00:20s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 0.51246
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.442075, 'Log Loss - std': 0.04525883201100088} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.17348 | eval_custom_logloss: 0.87878 |  0:00:00s
epoch 1  | loss: 0.93316 | eval_custom_logloss: 0.96621 |  0:00:00s
epoch 2  | loss: 0.82501 | eval_custom_logloss: 0.9171  |  0:00:00s
epoch 3  | loss: 0.76777 | eval_custom_logloss: 0.87862 |  0:00:01s
epoch 4  | loss: 0.70571 | eval_custom_logloss: 0.78469 |  0:00:01s
epoch 5  | loss: 0.73607 | eval_custom_logloss: 0.73538 |  0:00:02s
epoch 6  | loss: 0.6856  | eval_custom_logloss: 0.80263 |  0:00:02s
epoch 7  | loss: 0.67584 | eval_custom_logloss: 1.01387 |  0:00:02s
epoch 8  | loss: 0.62565 | eval_custom_logloss: 0.83926 |  0:00:03s
epoch 9  | loss: 0.6229  | eval_custom_logloss: 0.91517 |  0:00:03s
epoch 10 | loss: 0.59911 | eval_custom_logloss: 0.76075 |  0:00:03s
epoch 11 | loss: 0.58419 | eval_custom_logloss: 0.96546 |  0:00:04s
epoch 12 | loss: 0.58413 | eval_custom_logloss: 0.88518 |  0:00:04s
epoch 13 | loss: 0.55673 | eval_custom_logloss: 0.76685 |  0:00:04s
epoch 14 | loss: 0.59466 | eval_custom_logloss: 0.6759  |  0:00:04s
epoch 15 | loss: 0.62913 | eval_custom_logloss: 0.78588 |  0:00:05s
epoch 16 | loss: 0.59555 | eval_custom_logloss: 0.7482  |  0:00:05s
epoch 17 | loss: 0.58635 | eval_custom_logloss: 0.72936 |  0:00:05s
epoch 18 | loss: 0.55255 | eval_custom_logloss: 0.62159 |  0:00:06s
epoch 19 | loss: 0.52949 | eval_custom_logloss: 0.69245 |  0:00:06s
epoch 20 | loss: 0.57102 | eval_custom_logloss: 0.68141 |  0:00:07s
epoch 21 | loss: 0.56326 | eval_custom_logloss: 0.5875  |  0:00:07s
epoch 22 | loss: 0.54373 | eval_custom_logloss: 0.54673 |  0:00:07s
epoch 23 | loss: 0.52931 | eval_custom_logloss: 0.59112 |  0:00:08s
epoch 24 | loss: 0.53669 | eval_custom_logloss: 0.52722 |  0:00:08s
epoch 25 | loss: 0.5309  | eval_custom_logloss: 0.51989 |  0:00:08s
epoch 26 | loss: 0.51449 | eval_custom_logloss: 0.49307 |  0:00:09s
epoch 27 | loss: 0.50693 | eval_custom_logloss: 0.45233 |  0:00:09s
epoch 28 | loss: 0.51346 | eval_custom_logloss: 0.50144 |  0:00:10s
epoch 29 | loss: 0.52482 | eval_custom_logloss: 0.5245  |  0:00:10s
epoch 30 | loss: 0.52369 | eval_custom_logloss: 0.49893 |  0:00:10s
epoch 31 | loss: 0.51486 | eval_custom_logloss: 0.60768 |  0:00:11s
epoch 32 | loss: 0.49906 | eval_custom_logloss: 0.69632 |  0:00:11s
epoch 33 | loss: 0.4565  | eval_custom_logloss: 0.62322 |  0:00:11s
epoch 34 | loss: 0.45203 | eval_custom_logloss: 0.64038 |  0:00:12s
epoch 35 | loss: 0.49756 | eval_custom_logloss: 0.5743  |  0:00:12s
epoch 36 | loss: 0.51015 | eval_custom_logloss: 0.48429 |  0:00:13s
epoch 37 | loss: 0.5202  | eval_custom_logloss: 0.49011 |  0:00:13s
epoch 38 | loss: 0.4948  | eval_custom_logloss: 0.55742 |  0:00:13s
epoch 39 | loss: 0.52144 | eval_custom_logloss: 0.57503 |  0:00:14s
epoch 40 | loss: 0.47702 | eval_custom_logloss: 0.53219 |  0:00:14s
epoch 41 | loss: 0.52034 | eval_custom_logloss: 0.50272 |  0:00:14s
epoch 42 | loss: 0.515   | eval_custom_logloss: 0.51672 |  0:00:15s
epoch 43 | loss: 0.4887  | eval_custom_logloss: 0.46029 |  0:00:15s
epoch 44 | loss: 0.47766 | eval_custom_logloss: 0.45868 |  0:00:15s
epoch 45 | loss: 0.44063 | eval_custom_logloss: 0.47839 |  0:00:16s
epoch 46 | loss: 0.43243 | eval_custom_logloss: 0.46747 |  0:00:16s
epoch 47 | loss: 0.42769 | eval_custom_logloss: 0.47002 |  0:00:16s

Early stopping occurred at epoch 47 with best_epoch = 27 and best_eval_custom_logloss = 0.45233
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.44412, 'Log Loss - std': 0.04068682341987389} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 0.44412 and parameters: {'n_d': 8, 'n_steps': 3, 'gamma': 1.959320534243366, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 5, 'momentum': 0.008969043918216091, 'mask_type': 'entmax'}. Best is trial 7 with value: 1.8950399999999998.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.96342 | eval_custom_logloss: 9.67872 |  0:00:00s
epoch 1  | loss: 2.1078  | eval_custom_logloss: 9.2909  |  0:00:01s
epoch 2  | loss: 1.40343 | eval_custom_logloss: 8.59624 |  0:00:02s
epoch 3  | loss: 1.77023 | eval_custom_logloss: 8.57634 |  0:00:03s
epoch 4  | loss: 1.54096 | eval_custom_logloss: 9.34795 |  0:00:04s
epoch 5  | loss: 1.23324 | eval_custom_logloss: 8.23538 |  0:00:05s
epoch 6  | loss: 1.463   | eval_custom_logloss: 9.30147 |  0:00:06s
epoch 7  | loss: 1.26246 | eval_custom_logloss: 8.29532 |  0:00:06s
epoch 8  | loss: 2.07485 | eval_custom_logloss: 7.65127 |  0:00:07s
epoch 9  | loss: 1.49942 | eval_custom_logloss: 9.36927 |  0:00:08s
epoch 10 | loss: 1.31318 | eval_custom_logloss: 8.53733 |  0:00:09s
epoch 11 | loss: 1.18702 | eval_custom_logloss: 6.66896 |  0:00:10s
epoch 12 | loss: 1.15222 | eval_custom_logloss: 8.50699 |  0:00:11s
epoch 13 | loss: 0.78417 | eval_custom_logloss: 4.93332 |  0:00:11s
epoch 14 | loss: 0.76842 | eval_custom_logloss: 3.97632 |  0:00:12s
epoch 15 | loss: 0.6929  | eval_custom_logloss: 5.814   |  0:00:13s
epoch 16 | loss: 0.65139 | eval_custom_logloss: 6.46016 |  0:00:14s
epoch 17 | loss: 0.60279 | eval_custom_logloss: 4.76555 |  0:00:15s
epoch 18 | loss: 0.63677 | eval_custom_logloss: 5.01474 |  0:00:16s
epoch 19 | loss: 0.69294 | eval_custom_logloss: 5.10401 |  0:00:17s
epoch 20 | loss: 0.65865 | eval_custom_logloss: 5.1584  |  0:00:17s
epoch 21 | loss: 0.70578 | eval_custom_logloss: 4.97597 |  0:00:18s
epoch 22 | loss: 0.64397 | eval_custom_logloss: 3.8664  |  0:00:19s
epoch 23 | loss: 0.63965 | eval_custom_logloss: 5.29213 |  0:00:20s
epoch 24 | loss: 0.62726 | eval_custom_logloss: 4.61786 |  0:00:21s
epoch 25 | loss: 0.60227 | eval_custom_logloss: 6.50124 |  0:00:22s
epoch 26 | loss: 0.57694 | eval_custom_logloss: 5.98249 |  0:00:23s
epoch 27 | loss: 0.58196 | eval_custom_logloss: 6.92151 |  0:00:23s
epoch 28 | loss: 0.67163 | eval_custom_logloss: 6.37777 |  0:00:24s
epoch 29 | loss: 0.6233  | eval_custom_logloss: 5.52502 |  0:00:25s
epoch 30 | loss: 0.60837 | eval_custom_logloss: 4.91697 |  0:00:26s
epoch 31 | loss: 0.60349 | eval_custom_logloss: 5.75452 |  0:00:27s
epoch 32 | loss: 0.56572 | eval_custom_logloss: 5.1005  |  0:00:28s
epoch 33 | loss: 0.55515 | eval_custom_logloss: 6.09839 |  0:00:29s
epoch 34 | loss: 0.57177 | eval_custom_logloss: 7.04579 |  0:00:30s
epoch 35 | loss: 0.57243 | eval_custom_logloss: 5.4362  |  0:00:31s
epoch 36 | loss: 0.56733 | eval_custom_logloss: 4.74648 |  0:00:32s
epoch 37 | loss: 0.54401 | eval_custom_logloss: 4.91603 |  0:00:33s
epoch 38 | loss: 0.55513 | eval_custom_logloss: 4.82834 |  0:00:33s
epoch 39 | loss: 0.56174 | eval_custom_logloss: 4.41768 |  0:00:34s
epoch 40 | loss: 0.52709 | eval_custom_logloss: 5.67389 |  0:00:35s
epoch 41 | loss: 0.5367  | eval_custom_logloss: 6.167   |  0:00:36s
epoch 42 | loss: 0.52742 | eval_custom_logloss: 5.48514 |  0:00:37s

Early stopping occurred at epoch 42 with best_epoch = 22 and best_eval_custom_logloss = 3.8664
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.9972, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.28743 | eval_custom_logloss: 8.21497 |  0:00:00s
epoch 1  | loss: 2.21076 | eval_custom_logloss: 8.57459 |  0:00:01s
epoch 2  | loss: 1.90402 | eval_custom_logloss: 8.44401 |  0:00:02s
epoch 3  | loss: 1.63137 | eval_custom_logloss: 8.10755 |  0:00:03s
epoch 4  | loss: 2.28533 | eval_custom_logloss: 8.31553 |  0:00:04s
epoch 5  | loss: 2.33965 | eval_custom_logloss: 8.08728 |  0:00:04s
epoch 6  | loss: 3.07388 | eval_custom_logloss: 9.24438 |  0:00:05s
epoch 7  | loss: 2.41277 | eval_custom_logloss: 8.13459 |  0:00:06s
epoch 8  | loss: 2.11622 | eval_custom_logloss: 7.78962 |  0:00:07s
epoch 9  | loss: 1.11659 | eval_custom_logloss: 8.45603 |  0:00:08s
epoch 10 | loss: 0.99279 | eval_custom_logloss: 7.03485 |  0:00:09s
epoch 11 | loss: 0.95021 | eval_custom_logloss: 8.27632 |  0:00:10s
epoch 12 | loss: 0.96138 | eval_custom_logloss: 9.02782 |  0:00:10s
epoch 13 | loss: 0.80515 | eval_custom_logloss: 7.13234 |  0:00:11s
epoch 14 | loss: 0.76845 | eval_custom_logloss: 7.76058 |  0:00:12s
epoch 15 | loss: 0.75942 | eval_custom_logloss: 7.97003 |  0:00:13s
epoch 16 | loss: 0.73149 | eval_custom_logloss: 6.27667 |  0:00:14s
epoch 17 | loss: 0.67585 | eval_custom_logloss: 6.92851 |  0:00:15s
epoch 18 | loss: 0.67036 | eval_custom_logloss: 7.10873 |  0:00:15s
epoch 19 | loss: 0.69903 | eval_custom_logloss: 7.36082 |  0:00:16s
epoch 20 | loss: 0.64773 | eval_custom_logloss: 9.20033 |  0:00:17s
epoch 21 | loss: 0.62904 | eval_custom_logloss: 8.89414 |  0:00:18s
epoch 22 | loss: 0.61649 | eval_custom_logloss: 6.52367 |  0:00:19s
epoch 23 | loss: 0.6641  | eval_custom_logloss: 5.50445 |  0:00:19s
epoch 24 | loss: 0.60947 | eval_custom_logloss: 6.28684 |  0:00:20s
epoch 25 | loss: 0.59929 | eval_custom_logloss: 6.2003  |  0:00:21s
epoch 26 | loss: 0.61199 | eval_custom_logloss: 6.15777 |  0:00:22s
epoch 27 | loss: 0.60163 | eval_custom_logloss: 6.37362 |  0:00:23s
epoch 28 | loss: 0.62948 | eval_custom_logloss: 7.67134 |  0:00:24s
epoch 29 | loss: 0.60838 | eval_custom_logloss: 8.19863 |  0:00:24s
epoch 30 | loss: 0.60814 | eval_custom_logloss: 7.43641 |  0:00:25s
epoch 31 | loss: 0.629   | eval_custom_logloss: 6.45461 |  0:00:26s
epoch 32 | loss: 0.65273 | eval_custom_logloss: 5.87462 |  0:00:27s
epoch 33 | loss: 0.61542 | eval_custom_logloss: 5.34516 |  0:00:28s
epoch 34 | loss: 0.62085 | eval_custom_logloss: 6.68541 |  0:00:28s
epoch 35 | loss: 0.63866 | eval_custom_logloss: 6.02246 |  0:00:29s
epoch 36 | loss: 0.60878 | eval_custom_logloss: 5.87006 |  0:00:30s
epoch 37 | loss: 0.60511 | eval_custom_logloss: 4.67687 |  0:00:31s
epoch 38 | loss: 0.59124 | eval_custom_logloss: 5.93995 |  0:00:32s
epoch 39 | loss: 0.58339 | eval_custom_logloss: 4.52697 |  0:00:33s
epoch 40 | loss: 0.54379 | eval_custom_logloss: 5.51528 |  0:00:34s
epoch 41 | loss: 0.56884 | eval_custom_logloss: 5.1934  |  0:00:34s
epoch 42 | loss: 0.53802 | eval_custom_logloss: 5.96576 |  0:00:35s
epoch 43 | loss: 0.53446 | eval_custom_logloss: 4.4392  |  0:00:36s
epoch 44 | loss: 0.53071 | eval_custom_logloss: 4.71722 |  0:00:37s
epoch 45 | loss: 0.53353 | eval_custom_logloss: 4.35555 |  0:00:38s
epoch 46 | loss: 0.53353 | eval_custom_logloss: 4.09262 |  0:00:39s
epoch 47 | loss: 0.56286 | eval_custom_logloss: 3.72703 |  0:00:39s
epoch 48 | loss: 0.60419 | eval_custom_logloss: 4.05459 |  0:00:40s
epoch 49 | loss: 0.54376 | eval_custom_logloss: 2.90521 |  0:00:41s
epoch 50 | loss: 0.53937 | eval_custom_logloss: 4.41691 |  0:00:42s
epoch 51 | loss: 0.56211 | eval_custom_logloss: 4.0357  |  0:00:43s
epoch 52 | loss: 0.52097 | eval_custom_logloss: 3.7571  |  0:00:44s
epoch 53 | loss: 0.57943 | eval_custom_logloss: 3.6125  |  0:00:44s
epoch 54 | loss: 0.65756 | eval_custom_logloss: 4.15765 |  0:00:45s
epoch 55 | loss: 0.55452 | eval_custom_logloss: 4.10993 |  0:00:46s
epoch 56 | loss: 0.56558 | eval_custom_logloss: 3.89247 |  0:00:47s
epoch 57 | loss: 0.53342 | eval_custom_logloss: 4.20494 |  0:00:48s
epoch 58 | loss: 0.53634 | eval_custom_logloss: 4.08803 |  0:00:48s
epoch 59 | loss: 0.52505 | eval_custom_logloss: 3.93279 |  0:00:49s
epoch 60 | loss: 0.51761 | eval_custom_logloss: 4.2104  |  0:00:50s
epoch 61 | loss: 0.51467 | eval_custom_logloss: 4.02105 |  0:00:51s
epoch 62 | loss: 0.54736 | eval_custom_logloss: 3.33222 |  0:00:52s
epoch 63 | loss: 0.53794 | eval_custom_logloss: 3.55251 |  0:00:52s
epoch 64 | loss: 0.53733 | eval_custom_logloss: 3.14762 |  0:00:53s
epoch 65 | loss: 0.53045 | eval_custom_logloss: 2.76265 |  0:00:54s
epoch 66 | loss: 0.54287 | eval_custom_logloss: 2.3023  |  0:00:55s
epoch 67 | loss: 0.50383 | eval_custom_logloss: 2.62143 |  0:00:56s
epoch 68 | loss: 0.50348 | eval_custom_logloss: 2.59151 |  0:00:57s
epoch 69 | loss: 0.47046 | eval_custom_logloss: 3.19413 |  0:00:57s
epoch 70 | loss: 0.50652 | eval_custom_logloss: 3.35476 |  0:00:58s
epoch 71 | loss: 0.52625 | eval_custom_logloss: 2.7577  |  0:00:59s
epoch 72 | loss: 0.53125 | eval_custom_logloss: 2.73173 |  0:01:00s
epoch 73 | loss: 0.52506 | eval_custom_logloss: 3.02884 |  0:01:01s
epoch 74 | loss: 0.49197 | eval_custom_logloss: 3.20508 |  0:01:02s
epoch 75 | loss: 0.49575 | eval_custom_logloss: 3.26075 |  0:01:02s
epoch 76 | loss: 0.47869 | eval_custom_logloss: 3.69837 |  0:01:03s
epoch 77 | loss: 0.49202 | eval_custom_logloss: 4.28572 |  0:01:04s
epoch 78 | loss: 0.53626 | eval_custom_logloss: 4.37306 |  0:01:05s
epoch 79 | loss: 0.577   | eval_custom_logloss: 4.57896 |  0:01:06s
epoch 80 | loss: 0.58393 | eval_custom_logloss: 3.81001 |  0:01:06s
epoch 81 | loss: 0.55795 | eval_custom_logloss: 3.51453 |  0:01:07s
epoch 82 | loss: 0.56126 | eval_custom_logloss: 4.34979 |  0:01:08s
epoch 83 | loss: 0.55583 | eval_custom_logloss: 3.37466 |  0:01:09s
epoch 84 | loss: 0.55884 | eval_custom_logloss: 2.83518 |  0:01:10s
epoch 85 | loss: 0.56106 | eval_custom_logloss: 2.32309 |  0:01:10s
epoch 86 | loss: 0.54454 | eval_custom_logloss: 2.48368 |  0:01:11s

Early stopping occurred at epoch 86 with best_epoch = 66 and best_eval_custom_logloss = 2.3023
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.5439499999999997, 'Log Loss - std': 0.45324999999999993} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.11322 | eval_custom_logloss: 8.4422  |  0:00:00s
epoch 1  | loss: 1.94024 | eval_custom_logloss: 8.01574 |  0:00:01s
epoch 2  | loss: 3.12355 | eval_custom_logloss: 9.57518 |  0:00:02s
epoch 3  | loss: 3.04288 | eval_custom_logloss: 7.9416  |  0:00:03s
epoch 4  | loss: 1.58807 | eval_custom_logloss: 7.80253 |  0:00:04s
epoch 5  | loss: 1.24218 | eval_custom_logloss: 9.17778 |  0:00:05s
epoch 6  | loss: 1.40555 | eval_custom_logloss: 8.0708  |  0:00:05s
epoch 7  | loss: 1.43264 | eval_custom_logloss: 8.22356 |  0:00:06s
epoch 8  | loss: 1.41471 | eval_custom_logloss: 9.29392 |  0:00:07s
epoch 9  | loss: 0.99643 | eval_custom_logloss: 8.18585 |  0:00:08s
epoch 10 | loss: 1.21962 | eval_custom_logloss: 7.81603 |  0:00:09s
epoch 11 | loss: 1.33991 | eval_custom_logloss: 7.65418 |  0:00:09s
epoch 12 | loss: 1.21736 | eval_custom_logloss: 8.45451 |  0:00:10s
epoch 13 | loss: 0.7732  | eval_custom_logloss: 7.85913 |  0:00:11s
epoch 14 | loss: 0.69195 | eval_custom_logloss: 7.36571 |  0:00:12s
epoch 15 | loss: 0.65264 | eval_custom_logloss: 6.80866 |  0:00:13s
epoch 16 | loss: 0.66242 | eval_custom_logloss: 8.20485 |  0:00:14s
epoch 17 | loss: 0.62183 | eval_custom_logloss: 7.94147 |  0:00:14s
epoch 18 | loss: 0.63856 | eval_custom_logloss: 8.4325  |  0:00:15s
epoch 19 | loss: 0.60996 | eval_custom_logloss: 5.09438 |  0:00:16s
epoch 20 | loss: 0.59957 | eval_custom_logloss: 5.06201 |  0:00:17s
epoch 21 | loss: 0.57475 | eval_custom_logloss: 5.93162 |  0:00:18s
epoch 22 | loss: 0.56484 | eval_custom_logloss: 5.04303 |  0:00:19s
epoch 23 | loss: 0.59888 | eval_custom_logloss: 6.29297 |  0:00:19s
epoch 24 | loss: 0.63279 | eval_custom_logloss: 5.86296 |  0:00:20s
epoch 25 | loss: 0.5904  | eval_custom_logloss: 5.06791 |  0:00:21s
epoch 26 | loss: 0.58758 | eval_custom_logloss: 5.96592 |  0:00:22s
epoch 27 | loss: 0.56277 | eval_custom_logloss: 5.69581 |  0:00:23s
epoch 28 | loss: 0.57338 | eval_custom_logloss: 5.61383 |  0:00:23s
epoch 29 | loss: 0.56634 | eval_custom_logloss: 5.60686 |  0:00:24s
epoch 30 | loss: 0.5796  | eval_custom_logloss: 4.72276 |  0:00:25s
epoch 31 | loss: 0.57762 | eval_custom_logloss: 4.25746 |  0:00:26s
epoch 32 | loss: 0.55011 | eval_custom_logloss: 4.35072 |  0:00:27s
epoch 33 | loss: 0.55853 | eval_custom_logloss: 5.53327 |  0:00:28s
epoch 34 | loss: 0.56347 | eval_custom_logloss: 4.63779 |  0:00:28s
epoch 35 | loss: 0.50418 | eval_custom_logloss: 3.67468 |  0:00:29s
epoch 36 | loss: 0.50454 | eval_custom_logloss: 4.3686  |  0:00:30s
epoch 37 | loss: 0.52757 | eval_custom_logloss: 4.31333 |  0:00:31s
epoch 38 | loss: 0.54016 | eval_custom_logloss: 3.5743  |  0:00:32s
epoch 39 | loss: 0.55491 | eval_custom_logloss: 3.38261 |  0:00:33s
epoch 40 | loss: 0.55543 | eval_custom_logloss: 4.02475 |  0:00:33s
epoch 41 | loss: 0.53715 | eval_custom_logloss: 3.82018 |  0:00:34s
epoch 42 | loss: 0.55278 | eval_custom_logloss: 3.92448 |  0:00:35s
epoch 43 | loss: 0.54576 | eval_custom_logloss: 4.2352  |  0:00:36s
epoch 44 | loss: 0.51691 | eval_custom_logloss: 4.87103 |  0:00:37s
epoch 45 | loss: 0.52981 | eval_custom_logloss: 4.41732 |  0:00:37s
epoch 46 | loss: 0.5022  | eval_custom_logloss: 3.11067 |  0:00:38s
epoch 47 | loss: 0.50527 | eval_custom_logloss: 3.30867 |  0:00:39s
epoch 48 | loss: 0.48861 | eval_custom_logloss: 3.16963 |  0:00:40s
epoch 49 | loss: 0.51562 | eval_custom_logloss: 4.38608 |  0:00:41s
epoch 50 | loss: 0.4755  | eval_custom_logloss: 4.54728 |  0:00:42s
epoch 51 | loss: 0.5043  | eval_custom_logloss: 3.28615 |  0:00:42s
epoch 52 | loss: 0.48302 | eval_custom_logloss: 4.11719 |  0:00:43s
epoch 53 | loss: 0.50169 | eval_custom_logloss: 3.60557 |  0:00:44s
epoch 54 | loss: 0.49252 | eval_custom_logloss: 2.95206 |  0:00:45s
epoch 55 | loss: 0.47908 | eval_custom_logloss: 3.14393 |  0:00:46s
epoch 56 | loss: 0.48491 | eval_custom_logloss: 3.31371 |  0:00:47s
epoch 57 | loss: 0.47854 | eval_custom_logloss: 3.429   |  0:00:47s
epoch 58 | loss: 0.47232 | eval_custom_logloss: 3.06616 |  0:00:48s
epoch 59 | loss: 0.50568 | eval_custom_logloss: 3.09079 |  0:00:49s
epoch 60 | loss: 0.48668 | eval_custom_logloss: 3.35341 |  0:00:50s
epoch 61 | loss: 0.4846  | eval_custom_logloss: 3.06693 |  0:00:51s
epoch 62 | loss: 0.48395 | eval_custom_logloss: 2.3617  |  0:00:52s
epoch 63 | loss: 0.483   | eval_custom_logloss: 2.10894 |  0:00:52s
epoch 64 | loss: 0.46367 | eval_custom_logloss: 2.516   |  0:00:53s
epoch 65 | loss: 0.47227 | eval_custom_logloss: 2.77382 |  0:00:54s
epoch 66 | loss: 0.45359 | eval_custom_logloss: 2.70251 |  0:00:55s
epoch 67 | loss: 0.4669  | eval_custom_logloss: 2.70704 |  0:00:56s
epoch 68 | loss: 0.47169 | eval_custom_logloss: 2.31551 |  0:00:57s
epoch 69 | loss: 0.42904 | eval_custom_logloss: 2.55603 |  0:00:58s
epoch 70 | loss: 0.44505 | eval_custom_logloss: 2.46347 |  0:00:58s
epoch 71 | loss: 0.44351 | eval_custom_logloss: 2.73964 |  0:00:59s
epoch 72 | loss: 0.45485 | eval_custom_logloss: 2.56371 |  0:01:00s
epoch 73 | loss: 0.47195 | eval_custom_logloss: 2.02901 |  0:01:01s
epoch 74 | loss: 0.48671 | eval_custom_logloss: 1.91436 |  0:01:02s
epoch 75 | loss: 0.49593 | eval_custom_logloss: 2.66887 |  0:01:03s
epoch 76 | loss: 0.52486 | eval_custom_logloss: 4.19247 |  0:01:03s
epoch 77 | loss: 0.57064 | eval_custom_logloss: 3.2433  |  0:01:04s
epoch 78 | loss: 0.56052 | eval_custom_logloss: 2.00914 |  0:01:05s
epoch 79 | loss: 0.54795 | eval_custom_logloss: 3.69754 |  0:01:06s
epoch 80 | loss: 0.60115 | eval_custom_logloss: 2.94589 |  0:01:07s
epoch 81 | loss: 0.53825 | eval_custom_logloss: 4.78621 |  0:01:07s
epoch 82 | loss: 0.56397 | eval_custom_logloss: 3.91544 |  0:01:08s
epoch 83 | loss: 0.63385 | eval_custom_logloss: 3.48201 |  0:01:09s
epoch 84 | loss: 0.55708 | eval_custom_logloss: 4.16272 |  0:01:10s
epoch 85 | loss: 0.55182 | eval_custom_logloss: 5.33967 |  0:01:11s
epoch 86 | loss: 0.5683  | eval_custom_logloss: 3.1705  |  0:01:12s
epoch 87 | loss: 0.54676 | eval_custom_logloss: 2.40776 |  0:01:12s
epoch 88 | loss: 0.51383 | eval_custom_logloss: 1.95972 |  0:01:13s
epoch 89 | loss: 0.4715  | eval_custom_logloss: 2.31654 |  0:01:14s
epoch 90 | loss: 0.47908 | eval_custom_logloss: 3.18711 |  0:01:15s
epoch 91 | loss: 0.49537 | eval_custom_logloss: 3.53375 |  0:01:16s
epoch 92 | loss: 0.5335  | eval_custom_logloss: 2.82737 |  0:01:17s
epoch 93 | loss: 0.51185 | eval_custom_logloss: 2.53722 |  0:01:17s
epoch 94 | loss: 0.5254  | eval_custom_logloss: 2.90685 |  0:01:18s

Early stopping occurred at epoch 94 with best_epoch = 74 and best_eval_custom_logloss = 1.91436
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.264333333333333, 'Log Loss - std': 0.5415976386785878} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.24817 | eval_custom_logloss: 8.74491 |  0:00:00s
epoch 1  | loss: 3.1855  | eval_custom_logloss: 9.38253 |  0:00:01s
epoch 2  | loss: 1.97185 | eval_custom_logloss: 9.64981 |  0:00:02s
epoch 3  | loss: 2.03178 | eval_custom_logloss: 7.70011 |  0:00:03s
epoch 4  | loss: 1.76556 | eval_custom_logloss: 8.05508 |  0:00:04s
epoch 5  | loss: 1.43546 | eval_custom_logloss: 8.4683  |  0:00:05s
epoch 6  | loss: 2.40249 | eval_custom_logloss: 8.65023 |  0:00:06s
epoch 7  | loss: 1.23329 | eval_custom_logloss: 7.75679 |  0:00:06s
epoch 8  | loss: 2.14427 | eval_custom_logloss: 8.59503 |  0:00:07s
epoch 9  | loss: 1.04777 | eval_custom_logloss: 8.52477 |  0:00:08s
epoch 10 | loss: 1.0956  | eval_custom_logloss: 8.54886 |  0:00:09s
epoch 11 | loss: 0.8831  | eval_custom_logloss: 8.00647 |  0:00:10s
epoch 12 | loss: 0.71944 | eval_custom_logloss: 7.77384 |  0:00:10s
epoch 13 | loss: 0.70448 | eval_custom_logloss: 6.82954 |  0:00:11s
epoch 14 | loss: 0.72275 | eval_custom_logloss: 6.44662 |  0:00:12s
epoch 15 | loss: 0.71347 | eval_custom_logloss: 7.1718  |  0:00:13s
epoch 16 | loss: 0.68105 | eval_custom_logloss: 6.81649 |  0:00:14s
epoch 17 | loss: 0.69535 | eval_custom_logloss: 7.11274 |  0:00:15s
epoch 18 | loss: 0.63548 | eval_custom_logloss: 7.64102 |  0:00:15s
epoch 19 | loss: 0.78956 | eval_custom_logloss: 8.24374 |  0:00:16s
epoch 20 | loss: 0.68323 | eval_custom_logloss: 8.52093 |  0:00:17s
epoch 21 | loss: 0.6909  | eval_custom_logloss: 7.46433 |  0:00:18s
epoch 22 | loss: 0.6492  | eval_custom_logloss: 6.43564 |  0:00:19s
epoch 23 | loss: 0.73338 | eval_custom_logloss: 5.63145 |  0:00:19s
epoch 24 | loss: 0.63188 | eval_custom_logloss: 7.87132 |  0:00:20s
epoch 25 | loss: 0.62337 | eval_custom_logloss: 7.14727 |  0:00:21s
epoch 26 | loss: 0.67316 | eval_custom_logloss: 3.796   |  0:00:22s
epoch 27 | loss: 0.60559 | eval_custom_logloss: 6.17967 |  0:00:23s
epoch 28 | loss: 0.59074 | eval_custom_logloss: 7.19262 |  0:00:24s
epoch 29 | loss: 0.66352 | eval_custom_logloss: 5.60459 |  0:00:24s
epoch 30 | loss: 0.61475 | eval_custom_logloss: 6.4309  |  0:00:25s
epoch 31 | loss: 0.61756 | eval_custom_logloss: 4.3588  |  0:00:26s
epoch 32 | loss: 0.60598 | eval_custom_logloss: 6.71491 |  0:00:27s
epoch 33 | loss: 0.60378 | eval_custom_logloss: 5.07441 |  0:00:28s
epoch 34 | loss: 0.72075 | eval_custom_logloss: 5.42573 |  0:00:28s
epoch 35 | loss: 0.66333 | eval_custom_logloss: 6.2462  |  0:00:29s
epoch 36 | loss: 0.65381 | eval_custom_logloss: 6.3     |  0:00:30s
epoch 37 | loss: 0.61068 | eval_custom_logloss: 2.91211 |  0:00:31s
epoch 38 | loss: 0.59472 | eval_custom_logloss: 3.68809 |  0:00:32s
epoch 39 | loss: 0.59887 | eval_custom_logloss: 3.21021 |  0:00:33s
epoch 40 | loss: 0.62833 | eval_custom_logloss: 6.12797 |  0:00:33s
epoch 41 | loss: 0.61188 | eval_custom_logloss: 4.66718 |  0:00:34s
epoch 42 | loss: 0.6222  | eval_custom_logloss: 6.17015 |  0:00:35s
epoch 43 | loss: 0.59254 | eval_custom_logloss: 4.24529 |  0:00:36s
epoch 44 | loss: 0.64296 | eval_custom_logloss: 3.12374 |  0:00:37s
epoch 45 | loss: 0.63479 | eval_custom_logloss: 2.5871  |  0:00:37s
epoch 46 | loss: 0.62589 | eval_custom_logloss: 2.31099 |  0:00:38s
epoch 47 | loss: 0.61608 | eval_custom_logloss: 2.39122 |  0:00:39s
epoch 48 | loss: 0.60967 | eval_custom_logloss: 2.28944 |  0:00:40s
epoch 49 | loss: 0.64375 | eval_custom_logloss: 3.2087  |  0:00:41s
epoch 50 | loss: 0.60555 | eval_custom_logloss: 3.07955 |  0:00:42s
epoch 51 | loss: 0.62122 | eval_custom_logloss: 3.44101 |  0:00:42s
epoch 52 | loss: 0.60947 | eval_custom_logloss: 4.1696  |  0:00:43s
epoch 53 | loss: 0.57258 | eval_custom_logloss: 4.28804 |  0:00:44s
epoch 54 | loss: 0.54387 | eval_custom_logloss: 5.03472 |  0:00:45s
epoch 55 | loss: 0.55416 | eval_custom_logloss: 5.19957 |  0:00:46s
epoch 56 | loss: 0.53352 | eval_custom_logloss: 3.82948 |  0:00:46s
epoch 57 | loss: 0.567   | eval_custom_logloss: 3.59389 |  0:00:47s
epoch 58 | loss: 0.59026 | eval_custom_logloss: 3.95469 |  0:00:48s
epoch 59 | loss: 0.56803 | eval_custom_logloss: 3.52105 |  0:00:49s
epoch 60 | loss: 0.55912 | eval_custom_logloss: 3.88721 |  0:00:50s
epoch 61 | loss: 0.52546 | eval_custom_logloss: 5.05001 |  0:00:51s
epoch 62 | loss: 0.5315  | eval_custom_logloss: 3.67747 |  0:00:51s
epoch 63 | loss: 0.50074 | eval_custom_logloss: 4.21191 |  0:00:52s
epoch 64 | loss: 0.56725 | eval_custom_logloss: 2.81395 |  0:00:53s
epoch 65 | loss: 0.51807 | eval_custom_logloss: 3.28893 |  0:00:54s
epoch 66 | loss: 0.58128 | eval_custom_logloss: 2.97792 |  0:00:55s
epoch 67 | loss: 0.51254 | eval_custom_logloss: 3.00077 |  0:00:56s
epoch 68 | loss: 0.51295 | eval_custom_logloss: 6.07285 |  0:00:56s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 2.28944
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2285749999999998, 'Log Loss - std': 0.4731088425246351} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.78466 | eval_custom_logloss: 8.44292 |  0:00:00s
epoch 1  | loss: 1.81304 | eval_custom_logloss: 9.00421 |  0:00:01s
epoch 2  | loss: 1.72901 | eval_custom_logloss: 7.45088 |  0:00:02s
epoch 3  | loss: 1.50431 | eval_custom_logloss: 8.16635 |  0:00:03s
epoch 4  | loss: 2.41731 | eval_custom_logloss: 8.45352 |  0:00:04s
epoch 5  | loss: 2.0843  | eval_custom_logloss: 7.94809 |  0:00:05s
epoch 6  | loss: 3.35163 | eval_custom_logloss: 8.46355 |  0:00:05s
epoch 7  | loss: 3.09302 | eval_custom_logloss: 7.85567 |  0:00:06s
epoch 8  | loss: 2.48261 | eval_custom_logloss: 7.56303 |  0:00:07s
epoch 9  | loss: 1.26517 | eval_custom_logloss: 6.59228 |  0:00:08s
epoch 10 | loss: 1.06373 | eval_custom_logloss: 7.12599 |  0:00:09s
epoch 11 | loss: 1.05987 | eval_custom_logloss: 7.9279  |  0:00:10s
epoch 12 | loss: 1.26632 | eval_custom_logloss: 8.15135 |  0:00:10s
epoch 13 | loss: 0.83458 | eval_custom_logloss: 9.63583 |  0:00:11s
epoch 14 | loss: 0.83144 | eval_custom_logloss: 7.59253 |  0:00:12s
epoch 15 | loss: 0.83078 | eval_custom_logloss: 6.32082 |  0:00:13s
epoch 16 | loss: 0.74047 | eval_custom_logloss: 9.02468 |  0:00:14s
epoch 17 | loss: 0.74166 | eval_custom_logloss: 8.31247 |  0:00:14s
epoch 18 | loss: 0.8116  | eval_custom_logloss: 8.95129 |  0:00:15s
epoch 19 | loss: 0.7783  | eval_custom_logloss: 7.70288 |  0:00:16s
epoch 20 | loss: 0.74675 | eval_custom_logloss: 7.13077 |  0:00:17s
epoch 21 | loss: 0.68706 | eval_custom_logloss: 5.62317 |  0:00:18s
epoch 22 | loss: 0.66025 | eval_custom_logloss: 7.38048 |  0:00:19s
epoch 23 | loss: 0.62562 | eval_custom_logloss: 6.12973 |  0:00:20s
epoch 24 | loss: 0.63409 | eval_custom_logloss: 7.65125 |  0:00:20s
epoch 25 | loss: 0.61932 | eval_custom_logloss: 5.87377 |  0:00:21s
epoch 26 | loss: 0.65026 | eval_custom_logloss: 6.35673 |  0:00:22s
epoch 27 | loss: 0.60169 | eval_custom_logloss: 6.25583 |  0:00:23s
epoch 28 | loss: 0.65649 | eval_custom_logloss: 5.35577 |  0:00:24s
epoch 29 | loss: 0.72128 | eval_custom_logloss: 5.44769 |  0:00:24s
epoch 30 | loss: 0.63554 | eval_custom_logloss: 6.33725 |  0:00:25s
epoch 31 | loss: 0.61606 | eval_custom_logloss: 4.13193 |  0:00:26s
epoch 32 | loss: 0.63025 | eval_custom_logloss: 5.41192 |  0:00:27s
epoch 33 | loss: 0.58791 | eval_custom_logloss: 3.9225  |  0:00:28s
epoch 34 | loss: 0.61095 | eval_custom_logloss: 3.68129 |  0:00:29s
epoch 35 | loss: 0.62425 | eval_custom_logloss: 4.29431 |  0:00:29s
epoch 36 | loss: 0.65125 | eval_custom_logloss: 5.3735  |  0:00:30s
epoch 37 | loss: 0.68663 | eval_custom_logloss: 4.68023 |  0:00:31s
epoch 38 | loss: 0.65927 | eval_custom_logloss: 5.06373 |  0:00:32s
epoch 39 | loss: 0.67203 | eval_custom_logloss: 4.54467 |  0:00:33s
epoch 40 | loss: 0.65705 | eval_custom_logloss: 5.08843 |  0:00:34s
epoch 41 | loss: 0.64608 | eval_custom_logloss: 6.76715 |  0:00:35s
epoch 42 | loss: 0.60066 | eval_custom_logloss: 6.2729  |  0:00:35s
epoch 43 | loss: 0.5844  | eval_custom_logloss: 5.30758 |  0:00:36s
epoch 44 | loss: 0.62492 | eval_custom_logloss: 4.57049 |  0:00:37s
epoch 45 | loss: 0.64594 | eval_custom_logloss: 4.25006 |  0:00:38s
epoch 46 | loss: 0.59952 | eval_custom_logloss: 3.4689  |  0:00:39s
epoch 47 | loss: 0.59788 | eval_custom_logloss: 4.63607 |  0:00:39s
epoch 48 | loss: 0.62728 | eval_custom_logloss: 4.74678 |  0:00:40s
epoch 49 | loss: 0.60944 | eval_custom_logloss: 4.16083 |  0:00:41s
epoch 50 | loss: 0.57574 | eval_custom_logloss: 2.97292 |  0:00:42s
epoch 51 | loss: 0.5776  | eval_custom_logloss: 3.01039 |  0:00:43s
epoch 52 | loss: 0.57102 | eval_custom_logloss: 3.30379 |  0:00:44s
epoch 53 | loss: 0.60426 | eval_custom_logloss: 3.81631 |  0:00:44s
epoch 54 | loss: 0.5796  | eval_custom_logloss: 3.58535 |  0:00:45s
epoch 55 | loss: 0.56321 | eval_custom_logloss: 3.10389 |  0:00:46s
epoch 56 | loss: 0.57985 | eval_custom_logloss: 2.5542  |  0:00:47s
epoch 57 | loss: 0.60634 | eval_custom_logloss: 3.0035  |  0:00:48s
epoch 58 | loss: 0.56984 | eval_custom_logloss: 3.22759 |  0:00:48s
epoch 59 | loss: 0.55562 | eval_custom_logloss: 2.29161 |  0:00:49s
epoch 60 | loss: 0.54884 | eval_custom_logloss: 1.92521 |  0:00:50s
epoch 61 | loss: 0.52417 | eval_custom_logloss: 2.68362 |  0:00:51s
epoch 62 | loss: 0.54173 | eval_custom_logloss: 1.52819 |  0:00:52s
epoch 63 | loss: 0.49697 | eval_custom_logloss: 1.94978 |  0:00:53s
epoch 64 | loss: 0.51213 | eval_custom_logloss: 2.79213 |  0:00:53s
epoch 65 | loss: 0.50838 | eval_custom_logloss: 2.72796 |  0:00:54s
epoch 66 | loss: 0.49886 | eval_custom_logloss: 2.38273 |  0:00:55s
epoch 67 | loss: 0.51222 | eval_custom_logloss: 2.79034 |  0:00:56s
epoch 68 | loss: 0.54574 | eval_custom_logloss: 2.03372 |  0:00:57s
epoch 69 | loss: 0.53681 | eval_custom_logloss: 1.20821 |  0:00:57s
epoch 70 | loss: 0.50004 | eval_custom_logloss: 1.31888 |  0:00:58s
epoch 71 | loss: 0.52337 | eval_custom_logloss: 1.22235 |  0:00:59s
epoch 72 | loss: 0.53208 | eval_custom_logloss: 1.74159 |  0:01:00s
epoch 73 | loss: 0.50689 | eval_custom_logloss: 1.8245  |  0:01:01s
epoch 74 | loss: 0.52062 | eval_custom_logloss: 2.22233 |  0:01:02s
epoch 75 | loss: 0.517   | eval_custom_logloss: 3.12844 |  0:01:02s
epoch 76 | loss: 0.52373 | eval_custom_logloss: 3.5688  |  0:01:03s
epoch 77 | loss: 0.54164 | eval_custom_logloss: 2.93937 |  0:01:04s
epoch 78 | loss: 0.53719 | eval_custom_logloss: 3.81193 |  0:01:05s
epoch 79 | loss: 0.5625  | eval_custom_logloss: 3.6828  |  0:01:06s
epoch 80 | loss: 0.58658 | eval_custom_logloss: 3.64301 |  0:01:06s
epoch 81 | loss: 0.53086 | eval_custom_logloss: 3.51801 |  0:01:07s
epoch 82 | loss: 0.53327 | eval_custom_logloss: 3.97654 |  0:01:08s
epoch 83 | loss: 0.50082 | eval_custom_logloss: 4.06052 |  0:01:09s
epoch 84 | loss: 0.50891 | eval_custom_logloss: 4.1129  |  0:01:10s
epoch 85 | loss: 0.48325 | eval_custom_logloss: 3.74481 |  0:01:11s
epoch 86 | loss: 0.49786 | eval_custom_logloss: 3.1111  |  0:01:11s
epoch 87 | loss: 0.4769  | eval_custom_logloss: 2.82082 |  0:01:12s
epoch 88 | loss: 0.52253 | eval_custom_logloss: 2.61253 |  0:01:13s
epoch 89 | loss: 0.50127 | eval_custom_logloss: 2.55163 |  0:01:14s

Early stopping occurred at epoch 89 with best_epoch = 69 and best_eval_custom_logloss = 1.20821
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0139199999999997, 'Log Loss - std': 0.6028039960053351} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 2.0139199999999997 and parameters: {'n_d': 50, 'n_steps': 10, 'gamma': 1.6819703718654022, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001014470443533903, 'mask_type': 'sparsemax'}. Best is trial 14 with value: 2.0139199999999997.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.87486 | eval_custom_logloss: 8.84076 |  0:00:01s
epoch 1  | loss: 3.21545 | eval_custom_logloss: 8.88024 |  0:00:01s
epoch 2  | loss: 2.43944 | eval_custom_logloss: 8.27921 |  0:00:02s
epoch 3  | loss: 5.66525 | eval_custom_logloss: 8.05439 |  0:00:03s
epoch 4  | loss: 4.16332 | eval_custom_logloss: 8.8677  |  0:00:04s
epoch 5  | loss: 2.7562  | eval_custom_logloss: 8.39752 |  0:00:05s
epoch 6  | loss: 3.7777  | eval_custom_logloss: 8.54882 |  0:00:06s
epoch 7  | loss: 3.10629 | eval_custom_logloss: 7.52482 |  0:00:07s
epoch 8  | loss: 2.26183 | eval_custom_logloss: 7.76325 |  0:00:07s
epoch 9  | loss: 1.59718 | eval_custom_logloss: 6.84307 |  0:00:08s
epoch 10 | loss: 0.84881 | eval_custom_logloss: 7.24777 |  0:00:09s
epoch 11 | loss: 0.77201 | eval_custom_logloss: 7.7842  |  0:00:10s
epoch 12 | loss: 0.75921 | eval_custom_logloss: 8.55271 |  0:00:11s
epoch 13 | loss: 0.75151 | eval_custom_logloss: 7.91521 |  0:00:12s
epoch 14 | loss: 0.7204  | eval_custom_logloss: 7.86783 |  0:00:13s
epoch 15 | loss: 0.68718 | eval_custom_logloss: 7.5111  |  0:00:14s
epoch 16 | loss: 0.69012 | eval_custom_logloss: 7.16242 |  0:00:15s
epoch 17 | loss: 0.70905 | eval_custom_logloss: 6.08805 |  0:00:15s
epoch 18 | loss: 0.7708  | eval_custom_logloss: 4.89702 |  0:00:16s
epoch 19 | loss: 0.73854 | eval_custom_logloss: 5.20234 |  0:00:17s
epoch 20 | loss: 0.72617 | eval_custom_logloss: 6.77498 |  0:00:18s
epoch 21 | loss: 0.69987 | eval_custom_logloss: 3.54126 |  0:00:19s
epoch 22 | loss: 0.73138 | eval_custom_logloss: 6.05026 |  0:00:20s
epoch 23 | loss: 0.6697  | eval_custom_logloss: 5.63033 |  0:00:21s
epoch 24 | loss: 0.66766 | eval_custom_logloss: 5.82835 |  0:00:21s
epoch 25 | loss: 0.64643 | eval_custom_logloss: 6.43869 |  0:00:22s
epoch 26 | loss: 0.63583 | eval_custom_logloss: 4.77386 |  0:00:23s
epoch 27 | loss: 0.64382 | eval_custom_logloss: 6.14197 |  0:00:24s
epoch 28 | loss: 0.61639 | eval_custom_logloss: 5.73007 |  0:00:25s
epoch 29 | loss: 0.64702 | eval_custom_logloss: 3.89246 |  0:00:26s
epoch 30 | loss: 0.63919 | eval_custom_logloss: 4.049   |  0:00:27s
epoch 31 | loss: 0.59995 | eval_custom_logloss: 3.25748 |  0:00:27s
epoch 32 | loss: 0.57823 | eval_custom_logloss: 4.0358  |  0:00:28s
epoch 33 | loss: 0.58056 | eval_custom_logloss: 3.78819 |  0:00:29s
epoch 34 | loss: 0.54268 | eval_custom_logloss: 3.80356 |  0:00:30s
epoch 35 | loss: 0.54176 | eval_custom_logloss: 3.56591 |  0:00:31s
epoch 36 | loss: 0.57486 | eval_custom_logloss: 4.33122 |  0:00:32s
epoch 37 | loss: 0.58953 | eval_custom_logloss: 3.08111 |  0:00:32s
epoch 38 | loss: 0.56715 | eval_custom_logloss: 3.50588 |  0:00:33s
epoch 39 | loss: 0.55377 | eval_custom_logloss: 3.06932 |  0:00:34s
epoch 40 | loss: 0.53794 | eval_custom_logloss: 3.25135 |  0:00:35s
epoch 41 | loss: 0.522   | eval_custom_logloss: 2.85731 |  0:00:36s
epoch 42 | loss: 0.49983 | eval_custom_logloss: 2.6702  |  0:00:37s
epoch 43 | loss: 0.51319 | eval_custom_logloss: 2.93235 |  0:00:38s
epoch 44 | loss: 0.51453 | eval_custom_logloss: 3.04315 |  0:00:38s
epoch 45 | loss: 0.55497 | eval_custom_logloss: 2.79364 |  0:00:39s
epoch 46 | loss: 0.53854 | eval_custom_logloss: 2.59018 |  0:00:40s
epoch 47 | loss: 0.49634 | eval_custom_logloss: 3.19469 |  0:00:41s
epoch 48 | loss: 0.51932 | eval_custom_logloss: 2.84854 |  0:00:42s
epoch 49 | loss: 0.49718 | eval_custom_logloss: 2.73079 |  0:00:43s
epoch 50 | loss: 0.48095 | eval_custom_logloss: 2.86156 |  0:00:43s
epoch 51 | loss: 0.48127 | eval_custom_logloss: 2.86656 |  0:00:44s
epoch 52 | loss: 0.46674 | eval_custom_logloss: 2.41217 |  0:00:45s
epoch 53 | loss: 0.46427 | eval_custom_logloss: 1.76151 |  0:00:46s
epoch 54 | loss: 0.51903 | eval_custom_logloss: 2.28529 |  0:00:47s
epoch 55 | loss: 0.49294 | eval_custom_logloss: 3.04745 |  0:00:48s
epoch 56 | loss: 0.49381 | eval_custom_logloss: 2.15524 |  0:00:49s
epoch 57 | loss: 0.52841 | eval_custom_logloss: 2.66028 |  0:00:50s
epoch 58 | loss: 0.48057 | eval_custom_logloss: 2.10114 |  0:00:51s
epoch 59 | loss: 0.47956 | eval_custom_logloss: 1.80361 |  0:00:51s
epoch 60 | loss: 0.44863 | eval_custom_logloss: 3.50888 |  0:00:52s
epoch 61 | loss: 0.48424 | eval_custom_logloss: 3.47807 |  0:00:53s
epoch 62 | loss: 0.4714  | eval_custom_logloss: 2.22276 |  0:00:54s
epoch 63 | loss: 0.44644 | eval_custom_logloss: 2.5245  |  0:00:55s
epoch 64 | loss: 0.47954 | eval_custom_logloss: 2.8828  |  0:00:55s
epoch 65 | loss: 0.43306 | eval_custom_logloss: 3.48014 |  0:00:56s
epoch 66 | loss: 0.45664 | eval_custom_logloss: 2.55758 |  0:00:57s
epoch 67 | loss: 0.42921 | eval_custom_logloss: 2.19557 |  0:00:58s
epoch 68 | loss: 0.45067 | eval_custom_logloss: 1.94502 |  0:00:59s
epoch 69 | loss: 0.42851 | eval_custom_logloss: 2.13536 |  0:01:00s
epoch 70 | loss: 0.42421 | eval_custom_logloss: 2.06733 |  0:01:01s
epoch 71 | loss: 0.42353 | eval_custom_logloss: 2.90855 |  0:01:01s
epoch 72 | loss: 0.43967 | eval_custom_logloss: 2.10593 |  0:01:02s
epoch 73 | loss: 0.4552  | eval_custom_logloss: 2.78735 |  0:01:03s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 1.76151
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.5401, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.80888 | eval_custom_logloss: 7.57264 |  0:00:00s
epoch 1  | loss: 2.24449 | eval_custom_logloss: 9.32541 |  0:00:01s
epoch 2  | loss: 2.17693 | eval_custom_logloss: 9.07199 |  0:00:02s
epoch 3  | loss: 2.97579 | eval_custom_logloss: 9.01778 |  0:00:03s
epoch 4  | loss: 5.80025 | eval_custom_logloss: 8.87228 |  0:00:04s
epoch 5  | loss: 2.34265 | eval_custom_logloss: 8.64124 |  0:00:04s
epoch 6  | loss: 1.67652 | eval_custom_logloss: 8.75154 |  0:00:05s
epoch 7  | loss: 1.61481 | eval_custom_logloss: 7.58585 |  0:00:06s
epoch 8  | loss: 2.37529 | eval_custom_logloss: 8.53388 |  0:00:07s
epoch 9  | loss: 1.34008 | eval_custom_logloss: 8.45642 |  0:00:08s
epoch 10 | loss: 1.06095 | eval_custom_logloss: 7.83546 |  0:00:08s
epoch 11 | loss: 1.04513 | eval_custom_logloss: 8.08719 |  0:00:09s
epoch 12 | loss: 0.82865 | eval_custom_logloss: 7.53995 |  0:00:10s
epoch 13 | loss: 0.84087 | eval_custom_logloss: 8.04371 |  0:00:11s
epoch 14 | loss: 0.79929 | eval_custom_logloss: 8.45434 |  0:00:12s
epoch 15 | loss: 0.74045 | eval_custom_logloss: 7.97828 |  0:00:12s
epoch 16 | loss: 0.71683 | eval_custom_logloss: 7.72271 |  0:00:13s
epoch 17 | loss: 0.71023 | eval_custom_logloss: 9.1071  |  0:00:14s
epoch 18 | loss: 0.723   | eval_custom_logloss: 7.43226 |  0:00:15s
epoch 19 | loss: 0.74405 | eval_custom_logloss: 7.76824 |  0:00:16s
epoch 20 | loss: 0.6991  | eval_custom_logloss: 6.32217 |  0:00:16s
epoch 21 | loss: 0.66473 | eval_custom_logloss: 6.82885 |  0:00:17s
epoch 22 | loss: 0.65001 | eval_custom_logloss: 8.11638 |  0:00:18s
epoch 23 | loss: 0.65785 | eval_custom_logloss: 7.06721 |  0:00:19s
epoch 24 | loss: 0.67785 | eval_custom_logloss: 6.91712 |  0:00:20s
epoch 25 | loss: 0.67856 | eval_custom_logloss: 6.33764 |  0:00:20s
epoch 26 | loss: 0.66197 | eval_custom_logloss: 5.4131  |  0:00:21s
epoch 27 | loss: 0.67666 | eval_custom_logloss: 7.31785 |  0:00:22s
epoch 28 | loss: 0.6616  | eval_custom_logloss: 5.62846 |  0:00:23s
epoch 29 | loss: 0.63619 | eval_custom_logloss: 6.94736 |  0:00:24s
epoch 30 | loss: 0.62502 | eval_custom_logloss: 5.66565 |  0:00:25s
epoch 31 | loss: 0.63416 | eval_custom_logloss: 6.18223 |  0:00:25s
epoch 32 | loss: 0.64739 | eval_custom_logloss: 6.73958 |  0:00:26s
epoch 33 | loss: 0.66016 | eval_custom_logloss: 5.39264 |  0:00:27s
epoch 34 | loss: 0.64951 | eval_custom_logloss: 6.96851 |  0:00:28s
epoch 35 | loss: 0.65001 | eval_custom_logloss: 5.54327 |  0:00:29s
epoch 36 | loss: 0.63243 | eval_custom_logloss: 4.42485 |  0:00:29s
epoch 37 | loss: 0.62017 | eval_custom_logloss: 4.455   |  0:00:30s
epoch 38 | loss: 0.63352 | eval_custom_logloss: 4.33188 |  0:00:31s
epoch 39 | loss: 0.60731 | eval_custom_logloss: 3.81195 |  0:00:32s
epoch 40 | loss: 0.61161 | eval_custom_logloss: 4.23684 |  0:00:33s
epoch 41 | loss: 0.61874 | eval_custom_logloss: 4.39095 |  0:00:34s
epoch 42 | loss: 0.61837 | eval_custom_logloss: 4.13261 |  0:00:34s
epoch 43 | loss: 0.58383 | eval_custom_logloss: 4.00245 |  0:00:35s
epoch 44 | loss: 0.58548 | eval_custom_logloss: 5.49647 |  0:00:36s
epoch 45 | loss: 0.59063 | eval_custom_logloss: 4.59941 |  0:00:37s
epoch 46 | loss: 0.59117 | eval_custom_logloss: 5.58943 |  0:00:38s
epoch 47 | loss: 0.55284 | eval_custom_logloss: 4.84323 |  0:00:38s
epoch 48 | loss: 0.57284 | eval_custom_logloss: 5.049   |  0:00:39s
epoch 49 | loss: 0.60495 | eval_custom_logloss: 5.28459 |  0:00:40s
epoch 50 | loss: 0.5762  | eval_custom_logloss: 5.38674 |  0:00:41s
epoch 51 | loss: 0.6137  | eval_custom_logloss: 6.00512 |  0:00:42s
epoch 52 | loss: 0.63681 | eval_custom_logloss: 7.2489  |  0:00:42s
epoch 53 | loss: 0.61037 | eval_custom_logloss: 7.04518 |  0:00:43s
epoch 54 | loss: 0.58151 | eval_custom_logloss: 6.16237 |  0:00:44s
epoch 55 | loss: 0.54748 | eval_custom_logloss: 5.67601 |  0:00:45s
epoch 56 | loss: 0.5866  | eval_custom_logloss: 5.94602 |  0:00:46s
epoch 57 | loss: 0.57053 | eval_custom_logloss: 5.41153 |  0:00:47s
epoch 58 | loss: 0.58463 | eval_custom_logloss: 5.09318 |  0:00:47s
epoch 59 | loss: 0.56075 | eval_custom_logloss: 5.05068 |  0:00:48s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 3.81195
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.39685, 'Log Loss - std': 0.85675} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.67725 | eval_custom_logloss: 8.45826 |  0:00:00s
epoch 1  | loss: 1.98744 | eval_custom_logloss: 6.62673 |  0:00:01s
epoch 2  | loss: 1.68261 | eval_custom_logloss: 9.10187 |  0:00:02s
epoch 3  | loss: 1.53997 | eval_custom_logloss: 8.89078 |  0:00:03s
epoch 4  | loss: 1.77645 | eval_custom_logloss: 7.29744 |  0:00:04s
epoch 5  | loss: 1.56271 | eval_custom_logloss: 8.48662 |  0:00:05s
epoch 6  | loss: 1.26215 | eval_custom_logloss: 7.87408 |  0:00:06s
epoch 7  | loss: 1.69955 | eval_custom_logloss: 8.32194 |  0:00:06s
epoch 8  | loss: 1.64766 | eval_custom_logloss: 8.97195 |  0:00:07s
epoch 9  | loss: 1.3924  | eval_custom_logloss: 9.1542  |  0:00:08s
epoch 10 | loss: 1.29753 | eval_custom_logloss: 6.79796 |  0:00:09s
epoch 11 | loss: 0.7155  | eval_custom_logloss: 7.90046 |  0:00:10s
epoch 12 | loss: 0.66711 | eval_custom_logloss: 5.78228 |  0:00:10s
epoch 13 | loss: 0.67304 | eval_custom_logloss: 5.79266 |  0:00:11s
epoch 14 | loss: 0.64369 | eval_custom_logloss: 4.99107 |  0:00:12s
epoch 15 | loss: 0.64053 | eval_custom_logloss: 7.08699 |  0:00:13s
epoch 16 | loss: 0.69657 | eval_custom_logloss: 8.15862 |  0:00:14s
epoch 17 | loss: 0.65259 | eval_custom_logloss: 6.58586 |  0:00:15s
epoch 18 | loss: 0.67575 | eval_custom_logloss: 7.08491 |  0:00:15s
epoch 19 | loss: 0.76782 | eval_custom_logloss: 5.87398 |  0:00:16s
epoch 20 | loss: 0.76299 | eval_custom_logloss: 5.44869 |  0:00:17s
epoch 21 | loss: 0.64027 | eval_custom_logloss: 6.94979 |  0:00:18s
epoch 22 | loss: 0.65782 | eval_custom_logloss: 6.68373 |  0:00:19s
epoch 23 | loss: 0.67766 | eval_custom_logloss: 7.58482 |  0:00:19s
epoch 24 | loss: 0.7216  | eval_custom_logloss: 6.41899 |  0:00:20s
epoch 25 | loss: 0.69564 | eval_custom_logloss: 6.1342  |  0:00:21s
epoch 26 | loss: 0.66759 | eval_custom_logloss: 3.80961 |  0:00:22s
epoch 27 | loss: 0.63936 | eval_custom_logloss: 5.964   |  0:00:23s
epoch 28 | loss: 0.63915 | eval_custom_logloss: 6.87015 |  0:00:24s
epoch 29 | loss: 0.63123 | eval_custom_logloss: 5.50128 |  0:00:24s
epoch 30 | loss: 0.61112 | eval_custom_logloss: 5.05819 |  0:00:25s
epoch 31 | loss: 0.57888 | eval_custom_logloss: 6.55671 |  0:00:26s
epoch 32 | loss: 0.62877 | eval_custom_logloss: 6.21456 |  0:00:27s
epoch 33 | loss: 0.61877 | eval_custom_logloss: 5.81481 |  0:00:28s
epoch 34 | loss: 0.6365  | eval_custom_logloss: 7.26999 |  0:00:28s
epoch 35 | loss: 0.62775 | eval_custom_logloss: 4.9529  |  0:00:29s
epoch 36 | loss: 0.5717  | eval_custom_logloss: 5.06514 |  0:00:30s
epoch 37 | loss: 0.57571 | eval_custom_logloss: 5.85478 |  0:00:31s
epoch 38 | loss: 0.61509 | eval_custom_logloss: 6.51743 |  0:00:32s
epoch 39 | loss: 0.59792 | eval_custom_logloss: 8.09023 |  0:00:32s
epoch 40 | loss: 0.61083 | eval_custom_logloss: 6.42962 |  0:00:33s
epoch 41 | loss: 0.5851  | eval_custom_logloss: 6.38564 |  0:00:34s
epoch 42 | loss: 0.56809 | eval_custom_logloss: 4.7452  |  0:00:35s
epoch 43 | loss: 0.58329 | eval_custom_logloss: 3.59443 |  0:00:36s
epoch 44 | loss: 0.54362 | eval_custom_logloss: 4.23273 |  0:00:37s
epoch 45 | loss: 0.54352 | eval_custom_logloss: 5.74989 |  0:00:37s
epoch 46 | loss: 0.53206 | eval_custom_logloss: 4.59068 |  0:00:38s
epoch 47 | loss: 0.54463 | eval_custom_logloss: 3.46818 |  0:00:39s
epoch 48 | loss: 0.554   | eval_custom_logloss: 4.00197 |  0:00:40s
epoch 49 | loss: 0.56929 | eval_custom_logloss: 4.64429 |  0:00:41s
epoch 50 | loss: 0.60835 | eval_custom_logloss: 3.8342  |  0:00:41s
epoch 51 | loss: 0.598   | eval_custom_logloss: 3.60967 |  0:00:42s
epoch 52 | loss: 0.58321 | eval_custom_logloss: 2.70482 |  0:00:43s
epoch 53 | loss: 0.58324 | eval_custom_logloss: 3.02384 |  0:00:44s
epoch 54 | loss: 0.54235 | eval_custom_logloss: 3.84014 |  0:00:45s
epoch 55 | loss: 0.53986 | eval_custom_logloss: 4.17489 |  0:00:46s
epoch 56 | loss: 0.49797 | eval_custom_logloss: 2.51124 |  0:00:46s
epoch 57 | loss: 0.51317 | eval_custom_logloss: 2.32433 |  0:00:47s
epoch 58 | loss: 0.535   | eval_custom_logloss: 2.16059 |  0:00:48s
epoch 59 | loss: 0.50419 | eval_custom_logloss: 2.37296 |  0:00:49s
epoch 60 | loss: 0.52507 | eval_custom_logloss: 2.68085 |  0:00:50s
epoch 61 | loss: 0.57526 | eval_custom_logloss: 2.59501 |  0:00:51s
epoch 62 | loss: 0.55476 | eval_custom_logloss: 3.33332 |  0:00:51s
epoch 63 | loss: 0.59179 | eval_custom_logloss: 2.79822 |  0:00:52s
epoch 64 | loss: 0.57716 | eval_custom_logloss: 4.35445 |  0:00:53s
epoch 65 | loss: 0.53963 | eval_custom_logloss: 2.96395 |  0:00:54s
epoch 66 | loss: 0.55284 | eval_custom_logloss: 3.07497 |  0:00:55s
epoch 67 | loss: 0.56678 | eval_custom_logloss: 2.58942 |  0:00:55s
epoch 68 | loss: 0.52301 | eval_custom_logloss: 2.97008 |  0:00:56s
epoch 69 | loss: 0.51888 | eval_custom_logloss: 2.87782 |  0:00:57s
epoch 70 | loss: 0.55466 | eval_custom_logloss: 3.36502 |  0:00:58s
epoch 71 | loss: 0.49698 | eval_custom_logloss: 2.52579 |  0:00:59s
epoch 72 | loss: 0.53866 | eval_custom_logloss: 2.65579 |  0:00:59s
epoch 73 | loss: 0.53751 | eval_custom_logloss: 2.52778 |  0:01:00s
epoch 74 | loss: 0.57856 | eval_custom_logloss: 2.43626 |  0:01:01s
epoch 75 | loss: 0.56865 | eval_custom_logloss: 3.15047 |  0:01:02s
epoch 76 | loss: 0.55859 | eval_custom_logloss: 3.12036 |  0:01:03s
epoch 77 | loss: 0.55734 | eval_custom_logloss: 2.55875 |  0:01:04s
epoch 78 | loss: 0.55576 | eval_custom_logloss: 2.43845 |  0:01:04s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 2.16059
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.2659333333333334, 'Log Loss - std': 0.7236196437969942} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.91772 | eval_custom_logloss: 8.34824 |  0:00:00s
epoch 1  | loss: 2.20482 | eval_custom_logloss: 9.25818 |  0:00:01s
epoch 2  | loss: 2.59528 | eval_custom_logloss: 8.54882 |  0:00:02s
epoch 3  | loss: 6.18273 | eval_custom_logloss: 8.15724 |  0:00:03s
epoch 4  | loss: 2.6369  | eval_custom_logloss: 8.64124 |  0:00:04s
epoch 5  | loss: 3.65569 | eval_custom_logloss: 7.95637 |  0:00:04s
epoch 6  | loss: 1.89305 | eval_custom_logloss: 8.33224 |  0:00:05s
epoch 7  | loss: 1.53279 | eval_custom_logloss: 6.21712 |  0:00:06s
epoch 8  | loss: 1.26665 | eval_custom_logloss: 6.86833 |  0:00:07s
epoch 9  | loss: 1.00753 | eval_custom_logloss: 4.87937 |  0:00:08s
epoch 10 | loss: 1.03777 | eval_custom_logloss: 6.61342 |  0:00:09s
epoch 11 | loss: 0.94339 | eval_custom_logloss: 5.05645 |  0:00:09s
epoch 12 | loss: 0.77195 | eval_custom_logloss: 7.20465 |  0:00:10s
epoch 13 | loss: 0.90876 | eval_custom_logloss: 7.92947 |  0:00:11s
epoch 14 | loss: 0.77984 | eval_custom_logloss: 8.22213 |  0:00:12s
epoch 15 | loss: 0.80669 | eval_custom_logloss: 6.12906 |  0:00:13s
epoch 16 | loss: 0.69648 | eval_custom_logloss: 6.34616 |  0:00:14s
epoch 17 | loss: 0.73875 | eval_custom_logloss: 5.71094 |  0:00:14s
epoch 18 | loss: 0.71153 | eval_custom_logloss: 8.2454  |  0:00:15s
epoch 19 | loss: 0.73667 | eval_custom_logloss: 5.12613 |  0:00:16s
epoch 20 | loss: 0.67326 | eval_custom_logloss: 5.05955 |  0:00:17s
epoch 21 | loss: 0.59955 | eval_custom_logloss: 6.06973 |  0:00:18s
epoch 22 | loss: 0.65863 | eval_custom_logloss: 6.66807 |  0:00:18s
epoch 23 | loss: 0.62534 | eval_custom_logloss: 6.32652 |  0:00:19s
epoch 24 | loss: 0.65197 | eval_custom_logloss: 5.46445 |  0:00:20s
epoch 25 | loss: 0.62304 | eval_custom_logloss: 4.49297 |  0:00:21s
epoch 26 | loss: 0.62146 | eval_custom_logloss: 3.90335 |  0:00:22s
epoch 27 | loss: 0.58719 | eval_custom_logloss: 4.46692 |  0:00:23s
epoch 28 | loss: 0.58065 | eval_custom_logloss: 3.89945 |  0:00:23s
epoch 29 | loss: 0.54586 | eval_custom_logloss: 4.25909 |  0:00:24s
epoch 30 | loss: 0.55067 | eval_custom_logloss: 5.37135 |  0:00:25s
epoch 31 | loss: 0.56589 | eval_custom_logloss: 4.68221 |  0:00:26s
epoch 32 | loss: 0.53857 | eval_custom_logloss: 4.05511 |  0:00:27s
epoch 33 | loss: 0.53223 | eval_custom_logloss: 3.27008 |  0:00:28s
epoch 34 | loss: 0.50961 | eval_custom_logloss: 3.43955 |  0:00:28s
epoch 35 | loss: 0.5077  | eval_custom_logloss: 4.14184 |  0:00:29s
epoch 36 | loss: 0.49868 | eval_custom_logloss: 3.42251 |  0:00:30s
epoch 37 | loss: 0.53143 | eval_custom_logloss: 4.72297 |  0:00:31s
epoch 38 | loss: 0.52029 | eval_custom_logloss: 4.26296 |  0:00:32s
epoch 39 | loss: 0.4782  | eval_custom_logloss: 4.28259 |  0:00:33s
epoch 40 | loss: 0.48006 | eval_custom_logloss: 3.42394 |  0:00:33s
epoch 41 | loss: 0.5293  | eval_custom_logloss: 4.75195 |  0:00:34s
epoch 42 | loss: 0.50867 | eval_custom_logloss: 2.68995 |  0:00:35s
epoch 43 | loss: 0.55369 | eval_custom_logloss: 2.54867 |  0:00:36s
epoch 44 | loss: 0.51041 | eval_custom_logloss: 3.28466 |  0:00:37s
epoch 45 | loss: 0.51235 | eval_custom_logloss: 3.46635 |  0:00:37s
epoch 46 | loss: 0.49432 | eval_custom_logloss: 3.48555 |  0:00:38s
epoch 47 | loss: 0.44796 | eval_custom_logloss: 2.90714 |  0:00:39s
epoch 48 | loss: 0.45618 | eval_custom_logloss: 3.06371 |  0:00:40s
epoch 49 | loss: 0.43897 | eval_custom_logloss: 2.67989 |  0:00:41s
epoch 50 | loss: 0.44468 | eval_custom_logloss: 3.0904  |  0:00:41s
epoch 51 | loss: 0.42565 | eval_custom_logloss: 3.56437 |  0:00:42s
epoch 52 | loss: 0.41217 | eval_custom_logloss: 2.79376 |  0:00:43s
epoch 53 | loss: 0.43948 | eval_custom_logloss: 2.57113 |  0:00:44s
epoch 54 | loss: 0.46381 | eval_custom_logloss: 3.14003 |  0:00:45s
epoch 55 | loss: 0.43484 | eval_custom_logloss: 2.54609 |  0:00:46s
epoch 56 | loss: 0.4207  | eval_custom_logloss: 2.2047  |  0:00:46s
epoch 57 | loss: 0.42676 | eval_custom_logloss: 2.51059 |  0:00:47s
epoch 58 | loss: 0.39505 | eval_custom_logloss: 3.1269  |  0:00:48s
epoch 59 | loss: 0.40361 | eval_custom_logloss: 3.13288 |  0:00:49s
epoch 60 | loss: 0.38357 | eval_custom_logloss: 3.72107 |  0:00:50s
epoch 61 | loss: 0.37703 | eval_custom_logloss: 3.08159 |  0:00:50s
epoch 62 | loss: 0.37468 | eval_custom_logloss: 3.52842 |  0:00:51s
epoch 63 | loss: 0.36031 | eval_custom_logloss: 3.72557 |  0:00:52s
epoch 64 | loss: 0.35554 | eval_custom_logloss: 3.72029 |  0:00:53s
epoch 65 | loss: 0.37488 | eval_custom_logloss: 4.20807 |  0:00:54s
epoch 66 | loss: 0.39169 | eval_custom_logloss: 3.83135 |  0:00:55s
epoch 67 | loss: 0.40968 | eval_custom_logloss: 3.01551 |  0:00:55s
epoch 68 | loss: 0.34451 | eval_custom_logloss: 3.58055 |  0:00:56s
epoch 69 | loss: 0.34631 | eval_custom_logloss: 2.92286 |  0:00:57s
epoch 70 | loss: 0.33877 | eval_custom_logloss: 3.7163  |  0:00:58s
epoch 71 | loss: 0.35668 | eval_custom_logloss: 2.93012 |  0:00:59s
epoch 72 | loss: 0.32403 | eval_custom_logloss: 2.74573 |  0:01:00s
epoch 73 | loss: 0.32693 | eval_custom_logloss: 2.97835 |  0:01:00s
epoch 74 | loss: 0.32307 | eval_custom_logloss: 3.20527 |  0:01:01s
epoch 75 | loss: 0.3008  | eval_custom_logloss: 4.18266 |  0:01:02s
epoch 76 | loss: 0.3095  | eval_custom_logloss: 3.87323 |  0:01:03s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 2.2047
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.195475, 'Log Loss - std': 0.6384451204880495} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.27696 | eval_custom_logloss: 8.41134 |  0:00:00s
epoch 1  | loss: 1.94312 | eval_custom_logloss: 8.53826 |  0:00:01s
epoch 2  | loss: 1.88851 | eval_custom_logloss: 8.33097 |  0:00:02s
epoch 3  | loss: 2.18644 | eval_custom_logloss: 8.42308 |  0:00:03s
epoch 4  | loss: 2.48794 | eval_custom_logloss: 8.98066 |  0:00:04s
epoch 5  | loss: 1.7734  | eval_custom_logloss: 7.73667 |  0:00:05s
epoch 6  | loss: 1.48531 | eval_custom_logloss: 8.08109 |  0:00:05s
epoch 7  | loss: 1.85079 | eval_custom_logloss: 7.40998 |  0:00:06s
epoch 8  | loss: 1.43247 | eval_custom_logloss: 8.97407 |  0:00:07s
epoch 9  | loss: 1.55121 | eval_custom_logloss: 8.54882 |  0:00:08s
epoch 10 | loss: 2.2292  | eval_custom_logloss: 8.68744 |  0:00:09s
epoch 11 | loss: 0.92788 | eval_custom_logloss: 9.84269 |  0:00:10s
epoch 12 | loss: 0.85458 | eval_custom_logloss: 11.99867|  0:00:11s
epoch 13 | loss: 0.84368 | eval_custom_logloss: 8.15088 |  0:00:11s
epoch 14 | loss: 0.7916  | eval_custom_logloss: 8.07312 |  0:00:12s
epoch 15 | loss: 0.77809 | eval_custom_logloss: 8.64124 |  0:00:13s
epoch 16 | loss: 0.76319 | eval_custom_logloss: 8.25873 |  0:00:14s
epoch 17 | loss: 0.72749 | eval_custom_logloss: 9.47041 |  0:00:15s
epoch 18 | loss: 0.71107 | eval_custom_logloss: 9.49403 |  0:00:16s
epoch 19 | loss: 0.72862 | eval_custom_logloss: 8.28069 |  0:00:16s
epoch 20 | loss: 0.69259 | eval_custom_logloss: 8.04883 |  0:00:17s
epoch 21 | loss: 0.67807 | eval_custom_logloss: 7.93956 |  0:00:18s
epoch 22 | loss: 0.66082 | eval_custom_logloss: 8.31467 |  0:00:19s
epoch 23 | loss: 0.70341 | eval_custom_logloss: 8.89878 |  0:00:20s
epoch 24 | loss: 0.67476 | eval_custom_logloss: 8.35069 |  0:00:21s
epoch 25 | loss: 0.70818 | eval_custom_logloss: 6.86983 |  0:00:21s
epoch 26 | loss: 0.689   | eval_custom_logloss: 5.22889 |  0:00:22s
epoch 27 | loss: 0.69378 | eval_custom_logloss: 6.06513 |  0:00:23s
epoch 28 | loss: 0.67441 | eval_custom_logloss: 3.08782 |  0:00:24s
epoch 29 | loss: 0.64705 | eval_custom_logloss: 4.67804 |  0:00:25s
epoch 30 | loss: 0.6257  | eval_custom_logloss: 5.94536 |  0:00:26s
epoch 31 | loss: 0.63464 | eval_custom_logloss: 4.14993 |  0:00:27s
epoch 32 | loss: 0.64456 | eval_custom_logloss: 4.16349 |  0:00:28s
epoch 33 | loss: 0.63535 | eval_custom_logloss: 4.847   |  0:00:28s
epoch 34 | loss: 0.62114 | eval_custom_logloss: 5.34574 |  0:00:29s
epoch 35 | loss: 0.63567 | eval_custom_logloss: 4.88949 |  0:00:30s
epoch 36 | loss: 0.60178 | eval_custom_logloss: 5.6354  |  0:00:31s
epoch 37 | loss: 0.62399 | eval_custom_logloss: 4.75572 |  0:00:32s
epoch 38 | loss: 0.61097 | eval_custom_logloss: 4.58675 |  0:00:33s
epoch 39 | loss: 0.6037  | eval_custom_logloss: 5.11332 |  0:00:33s
epoch 40 | loss: 0.58747 | eval_custom_logloss: 3.80911 |  0:00:34s
epoch 41 | loss: 0.57582 | eval_custom_logloss: 3.43517 |  0:00:35s
epoch 42 | loss: 0.59543 | eval_custom_logloss: 3.2115  |  0:00:36s
epoch 43 | loss: 0.65056 | eval_custom_logloss: 5.3966  |  0:00:37s
epoch 44 | loss: 0.60682 | eval_custom_logloss: 3.13434 |  0:00:38s
epoch 45 | loss: 0.62613 | eval_custom_logloss: 3.10087 |  0:00:38s
epoch 46 | loss: 0.62468 | eval_custom_logloss: 5.53372 |  0:00:39s
epoch 47 | loss: 0.59655 | eval_custom_logloss: 6.00198 |  0:00:40s
epoch 48 | loss: 0.63683 | eval_custom_logloss: 6.25547 |  0:00:41s

Early stopping occurred at epoch 48 with best_epoch = 28 and best_eval_custom_logloss = 3.08782
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.28466, 'Log Loss - std': 0.5982521160848493} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 2.28466 and parameters: {'n_d': 53, 'n_steps': 10, 'gamma': 1.896164266454972, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.001099282207777889, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 5.11412 | eval_custom_logloss: 8.21496 |  0:00:00s
epoch 1  | loss: 2.84626 | eval_custom_logloss: 7.72936 |  0:00:01s
epoch 2  | loss: 3.1926  | eval_custom_logloss: 8.23017 |  0:00:02s
epoch 3  | loss: 2.47637 | eval_custom_logloss: 7.58484 |  0:00:03s
epoch 4  | loss: 2.58211 | eval_custom_logloss: 6.44824 |  0:00:04s
epoch 5  | loss: 3.91467 | eval_custom_logloss: 8.54882 |  0:00:05s
epoch 6  | loss: 4.88876 | eval_custom_logloss: 6.74846 |  0:00:05s
epoch 7  | loss: 1.17791 | eval_custom_logloss: 4.36822 |  0:00:06s
epoch 8  | loss: 1.2378  | eval_custom_logloss: 4.29965 |  0:00:07s
epoch 9  | loss: 1.16235 | eval_custom_logloss: 2.87846 |  0:00:08s
epoch 10 | loss: 0.87514 | eval_custom_logloss: 2.86465 |  0:00:09s
epoch 11 | loss: 0.92347 | eval_custom_logloss: 3.89357 |  0:00:10s
epoch 12 | loss: 0.8991  | eval_custom_logloss: 4.12336 |  0:00:11s
epoch 13 | loss: 0.74596 | eval_custom_logloss: 3.74189 |  0:00:11s
epoch 14 | loss: 0.79562 | eval_custom_logloss: 2.71335 |  0:00:12s
epoch 15 | loss: 0.7507  | eval_custom_logloss: 3.79632 |  0:00:13s
epoch 16 | loss: 0.75373 | eval_custom_logloss: 2.10528 |  0:00:14s
epoch 17 | loss: 0.71043 | eval_custom_logloss: 2.60856 |  0:00:15s
epoch 18 | loss: 0.71943 | eval_custom_logloss: 1.69317 |  0:00:16s
epoch 19 | loss: 0.73095 | eval_custom_logloss: 1.68236 |  0:00:16s
epoch 20 | loss: 0.74547 | eval_custom_logloss: 2.43039 |  0:00:17s
epoch 21 | loss: 0.75045 | eval_custom_logloss: 1.37577 |  0:00:18s
epoch 22 | loss: 0.70765 | eval_custom_logloss: 1.25999 |  0:00:19s
epoch 23 | loss: 0.67011 | eval_custom_logloss: 1.8276  |  0:00:20s
epoch 24 | loss: 0.69467 | eval_custom_logloss: 1.23847 |  0:00:21s
epoch 25 | loss: 0.6699  | eval_custom_logloss: 1.09137 |  0:00:21s
epoch 26 | loss: 0.64998 | eval_custom_logloss: 1.1665  |  0:00:22s
epoch 27 | loss: 0.60929 | eval_custom_logloss: 1.60151 |  0:00:23s
epoch 28 | loss: 0.56578 | eval_custom_logloss: 1.61009 |  0:00:24s
epoch 29 | loss: 0.57739 | eval_custom_logloss: 1.31981 |  0:00:25s
epoch 30 | loss: 0.58116 | eval_custom_logloss: 0.92661 |  0:00:26s
epoch 31 | loss: 0.54283 | eval_custom_logloss: 1.20115 |  0:00:26s
epoch 32 | loss: 0.58862 | eval_custom_logloss: 1.05947 |  0:00:27s
epoch 33 | loss: 0.59628 | eval_custom_logloss: 0.8348  |  0:00:28s
epoch 34 | loss: 0.62041 | eval_custom_logloss: 0.91305 |  0:00:29s
epoch 35 | loss: 0.62053 | eval_custom_logloss: 1.06492 |  0:00:30s
epoch 36 | loss: 0.56278 | eval_custom_logloss: 0.84239 |  0:00:31s
epoch 37 | loss: 0.56474 | eval_custom_logloss: 0.84256 |  0:00:31s
epoch 38 | loss: 0.52177 | eval_custom_logloss: 0.8301  |  0:00:32s
epoch 39 | loss: 0.51993 | eval_custom_logloss: 0.91966 |  0:00:33s
epoch 40 | loss: 0.54224 | eval_custom_logloss: 1.06103 |  0:00:34s
epoch 41 | loss: 0.55484 | eval_custom_logloss: 1.08419 |  0:00:35s
epoch 42 | loss: 0.54927 | eval_custom_logloss: 0.89455 |  0:00:35s
epoch 43 | loss: 0.53093 | eval_custom_logloss: 1.08729 |  0:00:36s
epoch 44 | loss: 0.54809 | eval_custom_logloss: 0.99127 |  0:00:37s
epoch 45 | loss: 0.50435 | eval_custom_logloss: 0.92032 |  0:00:38s
epoch 46 | loss: 0.48959 | eval_custom_logloss: 0.83051 |  0:00:39s
epoch 47 | loss: 0.47954 | eval_custom_logloss: 0.83594 |  0:00:40s
epoch 48 | loss: 0.46704 | eval_custom_logloss: 1.07001 |  0:00:40s
epoch 49 | loss: 0.46373 | eval_custom_logloss: 1.00893 |  0:00:41s
epoch 50 | loss: 0.4844  | eval_custom_logloss: 0.82547 |  0:00:42s
epoch 51 | loss: 0.45924 | eval_custom_logloss: 0.76286 |  0:00:43s
epoch 52 | loss: 0.45398 | eval_custom_logloss: 0.88523 |  0:00:44s
epoch 53 | loss: 0.4294  | eval_custom_logloss: 0.94318 |  0:00:44s
epoch 54 | loss: 0.42301 | eval_custom_logloss: 0.87765 |  0:00:45s
epoch 55 | loss: 0.40635 | eval_custom_logloss: 0.8136  |  0:00:46s
epoch 56 | loss: 0.39805 | eval_custom_logloss: 0.91333 |  0:00:47s
epoch 57 | loss: 0.41117 | eval_custom_logloss: 1.10992 |  0:00:48s
epoch 58 | loss: 0.43838 | eval_custom_logloss: 0.89774 |  0:00:49s
epoch 59 | loss: 0.42002 | eval_custom_logloss: 1.0523  |  0:00:49s
epoch 60 | loss: 0.39138 | eval_custom_logloss: 0.93725 |  0:00:50s
epoch 61 | loss: 0.38312 | eval_custom_logloss: 0.87522 |  0:00:51s
epoch 62 | loss: 0.39911 | eval_custom_logloss: 1.03613 |  0:00:52s
epoch 63 | loss: 0.42304 | eval_custom_logloss: 0.79218 |  0:00:53s
epoch 64 | loss: 0.39798 | eval_custom_logloss: 0.76972 |  0:00:54s
epoch 65 | loss: 0.38788 | eval_custom_logloss: 0.69476 |  0:00:54s
epoch 66 | loss: 0.37637 | eval_custom_logloss: 0.87764 |  0:00:55s
epoch 67 | loss: 0.44201 | eval_custom_logloss: 1.01294 |  0:00:56s
epoch 68 | loss: 0.42417 | eval_custom_logloss: 1.01611 |  0:00:57s
epoch 69 | loss: 0.49617 | eval_custom_logloss: 1.12221 |  0:00:58s
epoch 70 | loss: 0.45397 | eval_custom_logloss: 1.17304 |  0:00:58s
epoch 71 | loss: 0.44651 | eval_custom_logloss: 1.36361 |  0:00:59s
epoch 72 | loss: 0.47676 | eval_custom_logloss: 1.10661 |  0:01:00s
epoch 73 | loss: 0.47388 | eval_custom_logloss: 0.69292 |  0:01:01s
epoch 74 | loss: 0.48913 | eval_custom_logloss: 0.78386 |  0:01:02s
epoch 75 | loss: 0.4708  | eval_custom_logloss: 1.1806  |  0:01:03s
epoch 76 | loss: 0.51566 | eval_custom_logloss: 0.90832 |  0:01:03s
epoch 77 | loss: 0.48557 | eval_custom_logloss: 0.87433 |  0:01:04s
epoch 78 | loss: 0.50673 | eval_custom_logloss: 0.83049 |  0:01:05s
epoch 79 | loss: 0.48232 | eval_custom_logloss: 0.72473 |  0:01:06s
epoch 80 | loss: 0.45783 | eval_custom_logloss: 1.00001 |  0:01:07s
epoch 81 | loss: 0.4305  | eval_custom_logloss: 1.09845 |  0:01:07s
epoch 82 | loss: 0.38763 | eval_custom_logloss: 1.15062 |  0:01:08s
epoch 83 | loss: 0.36508 | eval_custom_logloss: 0.99339 |  0:01:09s
epoch 84 | loss: 0.38071 | eval_custom_logloss: 0.8454  |  0:01:10s
epoch 85 | loss: 0.36114 | eval_custom_logloss: 0.79473 |  0:01:11s
epoch 86 | loss: 0.35775 | eval_custom_logloss: 0.68487 |  0:01:11s
epoch 87 | loss: 0.39977 | eval_custom_logloss: 0.62402 |  0:01:12s
epoch 88 | loss: 0.39311 | eval_custom_logloss: 0.64519 |  0:01:13s
epoch 89 | loss: 0.3749  | eval_custom_logloss: 0.77919 |  0:01:14s
epoch 90 | loss: 0.39384 | eval_custom_logloss: 0.73389 |  0:01:15s
epoch 91 | loss: 0.42425 | eval_custom_logloss: 0.92093 |  0:01:16s
epoch 92 | loss: 0.40594 | eval_custom_logloss: 0.6642  |  0:01:16s
epoch 93 | loss: 0.42354 | eval_custom_logloss: 0.61705 |  0:01:17s
epoch 94 | loss: 0.37846 | eval_custom_logloss: 0.64067 |  0:01:18s
epoch 95 | loss: 0.36761 | eval_custom_logloss: 0.60682 |  0:01:19s
epoch 96 | loss: 0.35815 | eval_custom_logloss: 0.58861 |  0:01:20s
epoch 97 | loss: 0.32771 | eval_custom_logloss: 0.52239 |  0:01:21s
epoch 98 | loss: 0.36009 | eval_custom_logloss: 0.56155 |  0:01:21s
epoch 99 | loss: 0.33352 | eval_custom_logloss: 0.54589 |  0:01:22s
Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_eval_custom_logloss = 0.52239
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.517, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.40467 | eval_custom_logloss: 8.57056 |  0:00:00s
epoch 1  | loss: 2.04668 | eval_custom_logloss: 7.70484 |  0:00:01s
epoch 2  | loss: 2.27185 | eval_custom_logloss: 7.76725 |  0:00:02s
epoch 3  | loss: 2.57269 | eval_custom_logloss: 8.80633 |  0:00:03s
epoch 4  | loss: 2.69951 | eval_custom_logloss: 6.78774 |  0:00:04s
epoch 5  | loss: 1.75694 | eval_custom_logloss: 6.61462 |  0:00:04s
epoch 6  | loss: 1.29267 | eval_custom_logloss: 8.3883  |  0:00:05s
epoch 7  | loss: 1.66657 | eval_custom_logloss: 7.42684 |  0:00:06s
epoch 8  | loss: 1.30867 | eval_custom_logloss: 4.7867  |  0:00:07s
epoch 9  | loss: 1.00249 | eval_custom_logloss: 6.46039 |  0:00:08s
epoch 10 | loss: 0.97058 | eval_custom_logloss: 5.13187 |  0:00:08s
epoch 11 | loss: 1.19744 | eval_custom_logloss: 6.17146 |  0:00:09s
epoch 12 | loss: 0.87354 | eval_custom_logloss: 4.19186 |  0:00:10s
epoch 13 | loss: 0.7592  | eval_custom_logloss: 4.29242 |  0:00:11s
epoch 14 | loss: 0.71096 | eval_custom_logloss: 3.19446 |  0:00:12s
epoch 15 | loss: 0.68803 | eval_custom_logloss: 2.56533 |  0:00:13s
epoch 16 | loss: 0.65267 | eval_custom_logloss: 2.82705 |  0:00:13s
epoch 17 | loss: 0.65921 | eval_custom_logloss: 3.72171 |  0:00:14s
epoch 18 | loss: 0.62729 | eval_custom_logloss: 3.18268 |  0:00:15s
epoch 19 | loss: 0.62673 | eval_custom_logloss: 1.68943 |  0:00:16s
epoch 20 | loss: 0.64742 | eval_custom_logloss: 1.94607 |  0:00:17s
epoch 21 | loss: 0.65371 | eval_custom_logloss: 1.35841 |  0:00:17s
epoch 22 | loss: 0.64451 | eval_custom_logloss: 1.67041 |  0:00:18s
epoch 23 | loss: 0.6303  | eval_custom_logloss: 1.30152 |  0:00:19s
epoch 24 | loss: 0.58105 | eval_custom_logloss: 1.65151 |  0:00:20s
epoch 25 | loss: 0.57699 | eval_custom_logloss: 1.91037 |  0:00:21s
epoch 26 | loss: 0.60251 | eval_custom_logloss: 1.60777 |  0:00:22s
epoch 27 | loss: 0.54523 | eval_custom_logloss: 1.52108 |  0:00:22s
epoch 28 | loss: 0.5397  | eval_custom_logloss: 1.4861  |  0:00:23s
epoch 29 | loss: 0.52493 | eval_custom_logloss: 1.32501 |  0:00:24s
epoch 30 | loss: 0.51257 | eval_custom_logloss: 1.50447 |  0:00:25s
epoch 31 | loss: 0.50599 | eval_custom_logloss: 1.13093 |  0:00:26s
epoch 32 | loss: 0.4832  | eval_custom_logloss: 1.07997 |  0:00:27s
epoch 33 | loss: 0.4748  | eval_custom_logloss: 1.12054 |  0:00:28s
epoch 34 | loss: 0.49253 | eval_custom_logloss: 1.08803 |  0:00:29s
epoch 35 | loss: 0.47118 | eval_custom_logloss: 0.89349 |  0:00:29s
epoch 36 | loss: 0.46232 | eval_custom_logloss: 1.06184 |  0:00:30s
epoch 37 | loss: 0.48927 | eval_custom_logloss: 0.9258  |  0:00:31s
epoch 38 | loss: 0.46207 | eval_custom_logloss: 0.87346 |  0:00:32s
epoch 39 | loss: 0.45829 | eval_custom_logloss: 0.79683 |  0:00:33s
epoch 40 | loss: 0.4485  | eval_custom_logloss: 0.86999 |  0:00:34s
epoch 41 | loss: 0.43511 | eval_custom_logloss: 0.87716 |  0:00:35s
epoch 42 | loss: 0.41379 | eval_custom_logloss: 0.97467 |  0:00:36s
epoch 43 | loss: 0.43769 | eval_custom_logloss: 0.78064 |  0:00:36s
epoch 44 | loss: 0.43146 | eval_custom_logloss: 0.90751 |  0:00:37s
epoch 45 | loss: 0.44765 | eval_custom_logloss: 0.99174 |  0:00:38s
epoch 46 | loss: 0.4224  | eval_custom_logloss: 0.82918 |  0:00:39s
epoch 47 | loss: 0.39708 | eval_custom_logloss: 0.87113 |  0:00:40s
epoch 48 | loss: 0.44233 | eval_custom_logloss: 0.86995 |  0:00:41s
epoch 49 | loss: 0.39011 | eval_custom_logloss: 0.89731 |  0:00:41s
epoch 50 | loss: 0.3895  | eval_custom_logloss: 0.86094 |  0:00:42s
epoch 51 | loss: 0.37299 | eval_custom_logloss: 0.77188 |  0:00:43s
epoch 52 | loss: 0.39168 | eval_custom_logloss: 0.83091 |  0:00:44s
epoch 53 | loss: 0.37306 | eval_custom_logloss: 0.74516 |  0:00:45s
epoch 54 | loss: 0.38248 | eval_custom_logloss: 0.83995 |  0:00:46s
epoch 55 | loss: 0.40745 | eval_custom_logloss: 0.93059 |  0:00:46s
epoch 56 | loss: 0.36756 | eval_custom_logloss: 0.81673 |  0:00:47s
epoch 57 | loss: 0.39701 | eval_custom_logloss: 0.70885 |  0:00:48s
epoch 58 | loss: 0.39043 | eval_custom_logloss: 0.80106 |  0:00:49s
epoch 59 | loss: 0.36616 | eval_custom_logloss: 0.75818 |  0:00:50s
epoch 60 | loss: 0.35866 | eval_custom_logloss: 0.70896 |  0:00:51s
epoch 61 | loss: 0.39657 | eval_custom_logloss: 0.74437 |  0:00:52s
epoch 62 | loss: 0.35931 | eval_custom_logloss: 0.92583 |  0:00:52s
epoch 63 | loss: 0.3126  | eval_custom_logloss: 0.81083 |  0:00:53s
epoch 64 | loss: 0.32081 | eval_custom_logloss: 0.97298 |  0:00:54s
epoch 65 | loss: 0.33639 | eval_custom_logloss: 0.80991 |  0:00:55s
epoch 66 | loss: 0.36583 | eval_custom_logloss: 0.8924  |  0:00:56s
epoch 67 | loss: 0.37617 | eval_custom_logloss: 0.93149 |  0:00:57s
epoch 68 | loss: 0.34209 | eval_custom_logloss: 0.80113 |  0:00:57s
epoch 69 | loss: 0.34857 | eval_custom_logloss: 0.82923 |  0:00:58s
epoch 70 | loss: 0.30456 | eval_custom_logloss: 0.77799 |  0:00:59s
epoch 71 | loss: 0.30377 | eval_custom_logloss: 0.75184 |  0:01:00s
epoch 72 | loss: 0.35539 | eval_custom_logloss: 0.77493 |  0:01:01s
epoch 73 | loss: 0.33352 | eval_custom_logloss: 1.27107 |  0:01:02s
epoch 74 | loss: 0.34337 | eval_custom_logloss: 1.02004 |  0:01:02s
epoch 75 | loss: 0.29984 | eval_custom_logloss: 1.03902 |  0:01:03s
epoch 76 | loss: 0.33556 | eval_custom_logloss: 0.90525 |  0:01:04s
epoch 77 | loss: 0.30752 | eval_custom_logloss: 0.74952 |  0:01:05s

Early stopping occurred at epoch 77 with best_epoch = 57 and best_eval_custom_logloss = 0.70885
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6129, 'Log Loss - std': 0.09589999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 5.10972 | eval_custom_logloss: 8.56127 |  0:00:01s
epoch 1  | loss: 2.01084 | eval_custom_logloss: 10.03917|  0:00:01s
epoch 2  | loss: 1.84657 | eval_custom_logloss: 6.40717 |  0:00:02s
epoch 3  | loss: 3.36587 | eval_custom_logloss: 8.95928 |  0:00:03s
epoch 4  | loss: 4.16005 | eval_custom_logloss: 6.19679 |  0:00:04s
epoch 5  | loss: 2.57832 | eval_custom_logloss: 8.90892 |  0:00:05s
epoch 6  | loss: 2.78499 | eval_custom_logloss: 6.56978 |  0:00:06s
epoch 7  | loss: 1.60415 | eval_custom_logloss: 5.92655 |  0:00:06s
epoch 8  | loss: 1.13052 | eval_custom_logloss: 5.35551 |  0:00:07s
epoch 9  | loss: 1.11697 | eval_custom_logloss: 6.29824 |  0:00:08s
epoch 10 | loss: 1.05704 | eval_custom_logloss: 3.98906 |  0:00:09s
epoch 11 | loss: 0.98118 | eval_custom_logloss: 7.34402 |  0:00:10s
epoch 12 | loss: 1.59702 | eval_custom_logloss: 4.56767 |  0:00:11s
epoch 13 | loss: 1.00611 | eval_custom_logloss: 2.79635 |  0:00:11s
epoch 14 | loss: 0.86151 | eval_custom_logloss: 2.06925 |  0:00:12s
epoch 15 | loss: 0.77716 | eval_custom_logloss: 1.98945 |  0:00:13s
epoch 16 | loss: 0.73433 | eval_custom_logloss: 1.75915 |  0:00:14s
epoch 17 | loss: 0.68299 | eval_custom_logloss: 2.20288 |  0:00:15s
epoch 18 | loss: 0.68553 | eval_custom_logloss: 2.2141  |  0:00:16s
epoch 19 | loss: 0.67812 | eval_custom_logloss: 2.88736 |  0:00:16s
epoch 20 | loss: 0.68376 | eval_custom_logloss: 2.20808 |  0:00:17s
epoch 21 | loss: 0.71962 | eval_custom_logloss: 1.98235 |  0:00:18s
epoch 22 | loss: 0.67238 | eval_custom_logloss: 1.60236 |  0:00:19s
epoch 23 | loss: 0.70838 | eval_custom_logloss: 1.50052 |  0:00:20s
epoch 24 | loss: 0.71324 | eval_custom_logloss: 1.77579 |  0:00:21s
epoch 25 | loss: 0.62653 | eval_custom_logloss: 1.6954  |  0:00:21s
epoch 26 | loss: 0.60434 | eval_custom_logloss: 1.57739 |  0:00:22s
epoch 27 | loss: 0.57774 | eval_custom_logloss: 1.40677 |  0:00:23s
epoch 28 | loss: 0.61854 | eval_custom_logloss: 1.20974 |  0:00:24s
epoch 29 | loss: 0.5722  | eval_custom_logloss: 1.18371 |  0:00:25s
epoch 30 | loss: 0.57756 | eval_custom_logloss: 1.19708 |  0:00:26s
epoch 31 | loss: 0.57368 | eval_custom_logloss: 1.08947 |  0:00:26s
epoch 32 | loss: 0.59638 | eval_custom_logloss: 1.21481 |  0:00:27s
epoch 33 | loss: 0.58287 | eval_custom_logloss: 1.16798 |  0:00:28s
epoch 34 | loss: 0.57549 | eval_custom_logloss: 1.02444 |  0:00:29s
epoch 35 | loss: 0.5231  | eval_custom_logloss: 1.00209 |  0:00:30s
epoch 36 | loss: 0.48854 | eval_custom_logloss: 1.03325 |  0:00:31s
epoch 37 | loss: 0.49143 | eval_custom_logloss: 0.90817 |  0:00:31s
epoch 38 | loss: 0.48112 | eval_custom_logloss: 0.74659 |  0:00:32s
epoch 39 | loss: 0.51835 | eval_custom_logloss: 0.705   |  0:00:33s
epoch 40 | loss: 0.50375 | eval_custom_logloss: 0.85335 |  0:00:34s
epoch 41 | loss: 0.48843 | eval_custom_logloss: 0.72888 |  0:00:35s
epoch 42 | loss: 0.48149 | eval_custom_logloss: 0.79118 |  0:00:36s
epoch 43 | loss: 0.48427 | eval_custom_logloss: 0.68024 |  0:00:37s
epoch 44 | loss: 0.4989  | eval_custom_logloss: 1.53941 |  0:00:37s
epoch 45 | loss: 0.47975 | eval_custom_logloss: 1.02142 |  0:00:38s
epoch 46 | loss: 0.46518 | eval_custom_logloss: 0.77014 |  0:00:39s
epoch 47 | loss: 0.46057 | eval_custom_logloss: 0.79647 |  0:00:40s
epoch 48 | loss: 0.49072 | eval_custom_logloss: 0.71335 |  0:00:41s
epoch 49 | loss: 0.42524 | eval_custom_logloss: 0.85157 |  0:00:42s
epoch 50 | loss: 0.44561 | eval_custom_logloss: 0.81426 |  0:00:42s
epoch 51 | loss: 0.43965 | eval_custom_logloss: 0.66176 |  0:00:43s
epoch 52 | loss: 0.40118 | eval_custom_logloss: 0.79187 |  0:00:44s
epoch 53 | loss: 0.41646 | eval_custom_logloss: 0.66893 |  0:00:45s
epoch 54 | loss: 0.45831 | eval_custom_logloss: 0.66771 |  0:00:46s
epoch 55 | loss: 0.40872 | eval_custom_logloss: 0.75213 |  0:00:47s
epoch 56 | loss: 0.42213 | eval_custom_logloss: 0.62971 |  0:00:47s
epoch 57 | loss: 0.41546 | eval_custom_logloss: 0.82294 |  0:00:48s
epoch 58 | loss: 0.39699 | eval_custom_logloss: 0.84475 |  0:00:49s
epoch 59 | loss: 0.39091 | eval_custom_logloss: 0.68635 |  0:00:50s
epoch 60 | loss: 0.3574  | eval_custom_logloss: 0.70592 |  0:00:51s
epoch 61 | loss: 0.39648 | eval_custom_logloss: 0.71447 |  0:00:52s
epoch 62 | loss: 0.32747 | eval_custom_logloss: 0.60134 |  0:00:52s
epoch 63 | loss: 0.38823 | eval_custom_logloss: 0.86658 |  0:00:53s
epoch 64 | loss: 0.41382 | eval_custom_logloss: 0.66291 |  0:00:54s
epoch 65 | loss: 0.37159 | eval_custom_logloss: 0.62274 |  0:00:55s
epoch 66 | loss: 0.35027 | eval_custom_logloss: 0.72843 |  0:00:56s
epoch 67 | loss: 0.40354 | eval_custom_logloss: 0.79314 |  0:00:57s
epoch 68 | loss: 0.43625 | eval_custom_logloss: 0.73707 |  0:00:57s
epoch 69 | loss: 0.40828 | eval_custom_logloss: 1.00362 |  0:00:58s
epoch 70 | loss: 0.36472 | eval_custom_logloss: 0.92083 |  0:00:59s
epoch 71 | loss: 0.37122 | eval_custom_logloss: 0.82763 |  0:01:00s
epoch 72 | loss: 0.39454 | eval_custom_logloss: 0.73508 |  0:01:01s
epoch 73 | loss: 0.3329  | eval_custom_logloss: 0.56914 |  0:01:02s
epoch 74 | loss: 0.35774 | eval_custom_logloss: 0.62674 |  0:01:02s
epoch 75 | loss: 0.33366 | eval_custom_logloss: 0.59517 |  0:01:03s
epoch 76 | loss: 0.36445 | eval_custom_logloss: 0.48533 |  0:01:04s
epoch 77 | loss: 0.27889 | eval_custom_logloss: 0.64523 |  0:01:05s
epoch 78 | loss: 0.308   | eval_custom_logloss: 0.57923 |  0:01:06s
epoch 79 | loss: 0.30562 | eval_custom_logloss: 0.62546 |  0:01:07s
epoch 80 | loss: 0.33623 | eval_custom_logloss: 0.54847 |  0:01:07s
epoch 81 | loss: 0.35429 | eval_custom_logloss: 0.63736 |  0:01:08s
epoch 82 | loss: 0.31022 | eval_custom_logloss: 0.63724 |  0:01:09s
epoch 83 | loss: 0.32119 | eval_custom_logloss: 0.58837 |  0:01:10s
epoch 84 | loss: 0.32534 | eval_custom_logloss: 0.57853 |  0:01:11s
epoch 85 | loss: 0.27876 | eval_custom_logloss: 0.59948 |  0:01:12s
epoch 86 | loss: 0.29156 | eval_custom_logloss: 0.6484  |  0:01:12s
epoch 87 | loss: 0.33397 | eval_custom_logloss: 0.59452 |  0:01:13s
epoch 88 | loss: 0.35325 | eval_custom_logloss: 0.67046 |  0:01:14s
epoch 89 | loss: 0.33895 | eval_custom_logloss: 0.60252 |  0:01:15s
epoch 90 | loss: 0.29087 | eval_custom_logloss: 0.6477  |  0:01:16s
epoch 91 | loss: 0.32494 | eval_custom_logloss: 0.62396 |  0:01:17s
epoch 92 | loss: 0.28157 | eval_custom_logloss: 0.66327 |  0:01:18s
epoch 93 | loss: 0.27364 | eval_custom_logloss: 0.63654 |  0:01:18s
epoch 94 | loss: 0.32621 | eval_custom_logloss: 0.57179 |  0:01:19s
epoch 95 | loss: 0.27741 | eval_custom_logloss: 0.62872 |  0:01:20s
epoch 96 | loss: 0.27105 | eval_custom_logloss: 0.67794 |  0:01:21s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 0.48533
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5703666666666667, 'Log Loss - std': 0.09873892624267064} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.99093 | eval_custom_logloss: 9.18253 |  0:00:00s
epoch 1  | loss: 2.91014 | eval_custom_logloss: 8.04138 |  0:00:01s
epoch 2  | loss: 3.11033 | eval_custom_logloss: 8.65105 |  0:00:02s
epoch 3  | loss: 1.82519 | eval_custom_logloss: 8.3395  |  0:00:03s
epoch 4  | loss: 2.12311 | eval_custom_logloss: 8.22233 |  0:00:04s
epoch 5  | loss: 2.92661 | eval_custom_logloss: 8.74192 |  0:00:05s
epoch 6  | loss: 2.62665 | eval_custom_logloss: 6.68553 |  0:00:05s
epoch 7  | loss: 1.95577 | eval_custom_logloss: 8.45871 |  0:00:06s
epoch 8  | loss: 1.13417 | eval_custom_logloss: 7.47781 |  0:00:07s
epoch 9  | loss: 1.10182 | eval_custom_logloss: 7.02602 |  0:00:08s
epoch 10 | loss: 0.89473 | eval_custom_logloss: 5.06308 |  0:00:09s
epoch 11 | loss: 0.79822 | eval_custom_logloss: 2.89314 |  0:00:10s
epoch 12 | loss: 0.88058 | eval_custom_logloss: 2.41003 |  0:00:10s
epoch 13 | loss: 0.79432 | eval_custom_logloss: 2.67838 |  0:00:11s
epoch 14 | loss: 0.72559 | eval_custom_logloss: 2.33602 |  0:00:12s
epoch 15 | loss: 0.72113 | eval_custom_logloss: 2.42126 |  0:00:13s
epoch 16 | loss: 0.71283 | eval_custom_logloss: 2.63944 |  0:00:14s
epoch 17 | loss: 0.67502 | eval_custom_logloss: 2.74776 |  0:00:15s
epoch 18 | loss: 0.67718 | eval_custom_logloss: 2.43237 |  0:00:15s
epoch 19 | loss: 0.6597  | eval_custom_logloss: 2.636   |  0:00:16s
epoch 20 | loss: 0.62885 | eval_custom_logloss: 2.68817 |  0:00:17s
epoch 21 | loss: 0.62674 | eval_custom_logloss: 1.99894 |  0:00:18s
epoch 22 | loss: 0.65704 | eval_custom_logloss: 2.23617 |  0:00:19s
epoch 23 | loss: 0.65232 | eval_custom_logloss: 2.14374 |  0:00:19s
epoch 24 | loss: 0.63012 | eval_custom_logloss: 2.20173 |  0:00:20s
epoch 25 | loss: 0.6195  | eval_custom_logloss: 1.67691 |  0:00:21s
epoch 26 | loss: 0.6071  | eval_custom_logloss: 1.2304  |  0:00:22s
epoch 27 | loss: 0.65129 | eval_custom_logloss: 1.25333 |  0:00:23s
epoch 28 | loss: 0.65509 | eval_custom_logloss: 1.16833 |  0:00:24s
epoch 29 | loss: 0.59667 | eval_custom_logloss: 1.30522 |  0:00:25s
epoch 30 | loss: 0.65579 | eval_custom_logloss: 1.27955 |  0:00:25s
epoch 31 | loss: 0.60288 | eval_custom_logloss: 1.20464 |  0:00:26s
epoch 32 | loss: 0.57436 | eval_custom_logloss: 1.48894 |  0:00:27s
epoch 33 | loss: 0.5927  | eval_custom_logloss: 1.22402 |  0:00:28s
epoch 34 | loss: 0.59548 | eval_custom_logloss: 1.35696 |  0:00:29s
epoch 35 | loss: 0.63042 | eval_custom_logloss: 1.04927 |  0:00:30s
epoch 36 | loss: 0.56987 | eval_custom_logloss: 0.97824 |  0:00:30s
epoch 37 | loss: 0.55168 | eval_custom_logloss: 1.01691 |  0:00:31s
epoch 38 | loss: 0.54115 | eval_custom_logloss: 0.99279 |  0:00:32s
epoch 39 | loss: 0.56123 | eval_custom_logloss: 0.88625 |  0:00:33s
epoch 40 | loss: 0.51234 | eval_custom_logloss: 0.83652 |  0:00:34s
epoch 41 | loss: 0.53196 | eval_custom_logloss: 0.8081  |  0:00:35s
epoch 42 | loss: 0.51521 | eval_custom_logloss: 0.83889 |  0:00:35s
epoch 43 | loss: 0.52823 | eval_custom_logloss: 1.14696 |  0:00:36s
epoch 44 | loss: 0.51158 | eval_custom_logloss: 0.89665 |  0:00:37s
epoch 45 | loss: 0.53118 | eval_custom_logloss: 0.99431 |  0:00:38s
epoch 46 | loss: 0.54568 | eval_custom_logloss: 0.78411 |  0:00:39s
epoch 47 | loss: 0.55528 | eval_custom_logloss: 0.94001 |  0:00:40s
epoch 48 | loss: 0.52427 | eval_custom_logloss: 0.95031 |  0:00:40s
epoch 49 | loss: 0.51383 | eval_custom_logloss: 0.77884 |  0:00:41s
epoch 50 | loss: 0.51159 | eval_custom_logloss: 0.93959 |  0:00:42s
epoch 51 | loss: 0.49162 | eval_custom_logloss: 0.92898 |  0:00:43s
epoch 52 | loss: 0.47395 | eval_custom_logloss: 0.85149 |  0:00:44s
epoch 53 | loss: 0.46966 | eval_custom_logloss: 0.89839 |  0:00:44s
epoch 54 | loss: 0.45794 | eval_custom_logloss: 0.85635 |  0:00:45s
epoch 55 | loss: 0.47553 | eval_custom_logloss: 0.90092 |  0:00:46s
epoch 56 | loss: 0.51597 | eval_custom_logloss: 0.64794 |  0:00:47s
epoch 57 | loss: 0.44906 | eval_custom_logloss: 0.80693 |  0:00:48s
epoch 58 | loss: 0.46571 | eval_custom_logloss: 1.05051 |  0:00:49s
epoch 59 | loss: 0.46355 | eval_custom_logloss: 0.75677 |  0:00:49s
epoch 60 | loss: 0.41134 | eval_custom_logloss: 0.79784 |  0:00:50s
epoch 61 | loss: 0.42416 | eval_custom_logloss: 0.96178 |  0:00:51s
epoch 62 | loss: 0.42368 | eval_custom_logloss: 0.80189 |  0:00:52s
epoch 63 | loss: 0.43557 | eval_custom_logloss: 0.77892 |  0:00:53s
epoch 64 | loss: 0.43602 | eval_custom_logloss: 0.72533 |  0:00:53s
epoch 65 | loss: 0.43501 | eval_custom_logloss: 0.73806 |  0:00:54s
epoch 66 | loss: 0.41009 | eval_custom_logloss: 0.78021 |  0:00:55s
epoch 67 | loss: 0.40975 | eval_custom_logloss: 0.96532 |  0:00:56s
epoch 68 | loss: 0.35954 | eval_custom_logloss: 1.09203 |  0:00:57s
epoch 69 | loss: 0.43122 | eval_custom_logloss: 0.81368 |  0:00:58s
epoch 70 | loss: 0.39126 | eval_custom_logloss: 0.73984 |  0:00:58s
epoch 71 | loss: 0.43698 | eval_custom_logloss: 0.7366  |  0:00:59s
epoch 72 | loss: 0.45941 | eval_custom_logloss: 0.72713 |  0:01:00s
epoch 73 | loss: 0.45382 | eval_custom_logloss: 0.73065 |  0:01:01s
epoch 74 | loss: 0.46495 | eval_custom_logloss: 0.83313 |  0:01:02s
epoch 75 | loss: 0.47224 | eval_custom_logloss: 0.7846  |  0:01:03s
epoch 76 | loss: 0.45163 | eval_custom_logloss: 0.84927 |  0:01:03s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.64794
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.58975, 'Log Loss - std': 0.09186496884014057} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax', 'n_a': 52, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.6088  | eval_custom_logloss: 7.65337 |  0:00:00s
epoch 1  | loss: 2.00487 | eval_custom_logloss: 8.16816 |  0:00:01s
epoch 2  | loss: 1.76428 | eval_custom_logloss: 7.38743 |  0:00:02s
epoch 3  | loss: 2.45084 | eval_custom_logloss: 8.82607 |  0:00:03s
epoch 4  | loss: 6.60443 | eval_custom_logloss: 10.1158 |  0:00:04s
epoch 5  | loss: 3.60124 | eval_custom_logloss: 7.85567 |  0:00:05s
epoch 6  | loss: 4.67157 | eval_custom_logloss: 6.51751 |  0:00:05s
epoch 7  | loss: 1.48895 | eval_custom_logloss: 5.70732 |  0:00:06s
epoch 8  | loss: 0.97685 | eval_custom_logloss: 7.31198 |  0:00:07s
epoch 9  | loss: 1.44564 | eval_custom_logloss: 4.55676 |  0:00:08s
epoch 10 | loss: 1.18855 | eval_custom_logloss: 3.35962 |  0:00:09s
epoch 11 | loss: 0.8465  | eval_custom_logloss: 4.71116 |  0:00:10s
epoch 12 | loss: 0.96428 | eval_custom_logloss: 3.65986 |  0:00:11s
epoch 13 | loss: 0.85387 | eval_custom_logloss: 3.06548 |  0:00:11s
epoch 14 | loss: 0.70106 | eval_custom_logloss: 2.48694 |  0:00:12s
epoch 15 | loss: 0.70947 | eval_custom_logloss: 3.29784 |  0:00:13s
epoch 16 | loss: 0.69503 | eval_custom_logloss: 2.95484 |  0:00:14s
epoch 17 | loss: 0.68541 | eval_custom_logloss: 2.84039 |  0:00:15s
epoch 18 | loss: 0.70626 | eval_custom_logloss: 1.93181 |  0:00:16s
epoch 19 | loss: 0.7118  | eval_custom_logloss: 1.58255 |  0:00:16s
epoch 20 | loss: 0.70425 | eval_custom_logloss: 1.25236 |  0:00:17s
epoch 21 | loss: 0.65072 | eval_custom_logloss: 1.32159 |  0:00:18s
epoch 22 | loss: 0.66164 | eval_custom_logloss: 1.37787 |  0:00:19s
epoch 23 | loss: 0.6988  | eval_custom_logloss: 1.75371 |  0:00:20s
epoch 24 | loss: 0.63567 | eval_custom_logloss: 2.25414 |  0:00:21s
epoch 25 | loss: 0.63357 | eval_custom_logloss: 2.47877 |  0:00:21s
epoch 26 | loss: 0.61212 | eval_custom_logloss: 2.03796 |  0:00:22s
epoch 27 | loss: 0.61347 | eval_custom_logloss: 2.57236 |  0:00:23s
epoch 28 | loss: 0.59265 | eval_custom_logloss: 1.33539 |  0:00:24s
epoch 29 | loss: 0.62444 | eval_custom_logloss: 1.28675 |  0:00:25s
epoch 30 | loss: 0.56003 | eval_custom_logloss: 1.37128 |  0:00:26s
epoch 31 | loss: 0.55739 | eval_custom_logloss: 1.65011 |  0:00:26s
epoch 32 | loss: 0.54309 | eval_custom_logloss: 1.66357 |  0:00:27s
epoch 33 | loss: 0.5514  | eval_custom_logloss: 1.35125 |  0:00:28s
epoch 34 | loss: 0.53396 | eval_custom_logloss: 1.40338 |  0:00:29s
epoch 35 | loss: 0.57521 | eval_custom_logloss: 0.84856 |  0:00:30s
epoch 36 | loss: 0.50597 | eval_custom_logloss: 0.95982 |  0:00:31s
epoch 37 | loss: 0.51137 | eval_custom_logloss: 0.88338 |  0:00:31s
epoch 38 | loss: 0.48956 | eval_custom_logloss: 0.86212 |  0:00:32s
epoch 39 | loss: 0.52702 | eval_custom_logloss: 0.9099  |  0:00:33s
epoch 40 | loss: 0.5203  | eval_custom_logloss: 0.7847  |  0:00:34s
epoch 41 | loss: 0.52657 | eval_custom_logloss: 0.79767 |  0:00:35s
epoch 42 | loss: 0.48162 | eval_custom_logloss: 0.72666 |  0:00:36s
epoch 43 | loss: 0.48896 | eval_custom_logloss: 0.63498 |  0:00:37s
epoch 44 | loss: 0.48952 | eval_custom_logloss: 0.83599 |  0:00:37s
epoch 45 | loss: 0.46816 | eval_custom_logloss: 0.9789  |  0:00:38s
epoch 46 | loss: 0.4578  | eval_custom_logloss: 1.20807 |  0:00:39s
epoch 47 | loss: 0.46396 | eval_custom_logloss: 0.97705 |  0:00:40s
epoch 48 | loss: 0.43949 | eval_custom_logloss: 1.14819 |  0:00:41s
epoch 49 | loss: 0.42469 | eval_custom_logloss: 1.00618 |  0:00:42s
epoch 50 | loss: 0.45404 | eval_custom_logloss: 1.0217  |  0:00:42s
epoch 51 | loss: 0.42719 | eval_custom_logloss: 0.93928 |  0:00:43s
epoch 52 | loss: 0.44058 | eval_custom_logloss: 0.80831 |  0:00:44s
epoch 53 | loss: 0.41793 | eval_custom_logloss: 0.78568 |  0:00:45s
epoch 54 | loss: 0.41725 | eval_custom_logloss: 0.6611  |  0:00:46s
epoch 55 | loss: 0.39884 | eval_custom_logloss: 0.7886  |  0:00:47s
epoch 56 | loss: 0.41385 | eval_custom_logloss: 0.59221 |  0:00:48s
epoch 57 | loss: 0.38561 | eval_custom_logloss: 0.5696  |  0:00:48s
epoch 58 | loss: 0.38683 | eval_custom_logloss: 0.53066 |  0:00:49s
epoch 59 | loss: 0.35564 | eval_custom_logloss: 0.58896 |  0:00:50s
epoch 60 | loss: 0.33733 | eval_custom_logloss: 0.62829 |  0:00:51s
epoch 61 | loss: 0.36418 | eval_custom_logloss: 0.68479 |  0:00:52s
epoch 62 | loss: 0.38185 | eval_custom_logloss: 0.82454 |  0:00:53s
epoch 63 | loss: 0.46502 | eval_custom_logloss: 0.78143 |  0:00:53s
epoch 64 | loss: 0.45152 | eval_custom_logloss: 0.76509 |  0:00:54s
epoch 65 | loss: 0.41963 | eval_custom_logloss: 0.57448 |  0:00:55s
epoch 66 | loss: 0.36347 | eval_custom_logloss: 0.61551 |  0:00:56s
epoch 67 | loss: 0.3924  | eval_custom_logloss: 0.54124 |  0:00:57s
epoch 68 | loss: 0.36904 | eval_custom_logloss: 0.50161 |  0:00:58s
epoch 69 | loss: 0.37226 | eval_custom_logloss: 0.45865 |  0:00:58s
epoch 70 | loss: 0.3439  | eval_custom_logloss: 0.51642 |  0:00:59s
epoch 71 | loss: 0.36435 | eval_custom_logloss: 0.45528 |  0:01:00s
epoch 72 | loss: 0.39302 | eval_custom_logloss: 0.5542  |  0:01:01s
epoch 73 | loss: 0.36284 | eval_custom_logloss: 0.54939 |  0:01:02s
epoch 74 | loss: 0.35707 | eval_custom_logloss: 0.52124 |  0:01:03s
epoch 75 | loss: 0.35126 | eval_custom_logloss: 0.64092 |  0:01:03s
epoch 76 | loss: 0.33687 | eval_custom_logloss: 0.58063 |  0:01:04s
epoch 77 | loss: 0.3312  | eval_custom_logloss: 0.5952  |  0:01:05s
epoch 78 | loss: 0.34905 | eval_custom_logloss: 0.48858 |  0:01:06s
epoch 79 | loss: 0.32478 | eval_custom_logloss: 0.71565 |  0:01:07s
epoch 80 | loss: 0.34988 | eval_custom_logloss: 0.86577 |  0:01:08s
epoch 81 | loss: 0.35568 | eval_custom_logloss: 0.67535 |  0:01:08s
epoch 82 | loss: 0.3252  | eval_custom_logloss: 0.7425  |  0:01:09s
epoch 83 | loss: 0.31805 | eval_custom_logloss: 0.51874 |  0:01:10s
epoch 84 | loss: 0.31545 | eval_custom_logloss: 0.5428  |  0:01:11s
epoch 85 | loss: 0.29402 | eval_custom_logloss: 0.55756 |  0:01:12s
epoch 86 | loss: 0.30504 | eval_custom_logloss: 0.39926 |  0:01:13s
epoch 87 | loss: 0.30757 | eval_custom_logloss: 0.39423 |  0:01:14s
epoch 88 | loss: 0.31197 | eval_custom_logloss: 0.41871 |  0:01:15s
epoch 89 | loss: 0.31006 | eval_custom_logloss: 0.44532 |  0:01:16s
epoch 90 | loss: 0.31557 | eval_custom_logloss: 0.42176 |  0:01:16s
epoch 91 | loss: 0.30462 | eval_custom_logloss: 0.46771 |  0:01:17s
epoch 92 | loss: 0.30172 | eval_custom_logloss: 0.45805 |  0:01:18s
epoch 93 | loss: 0.34804 | eval_custom_logloss: 0.58949 |  0:01:19s
epoch 94 | loss: 0.33508 | eval_custom_logloss: 1.03313 |  0:01:20s
epoch 95 | loss: 0.33746 | eval_custom_logloss: 1.46025 |  0:01:21s
epoch 96 | loss: 0.30526 | eval_custom_logloss: 0.70072 |  0:01:21s
epoch 97 | loss: 0.2827  | eval_custom_logloss: 0.61886 |  0:01:22s
epoch 98 | loss: 0.27512 | eval_custom_logloss: 0.69847 |  0:01:23s
epoch 99 | loss: 0.30668 | eval_custom_logloss: 0.70567 |  0:01:24s
Stop training because you reached max_epochs = 100 with best_epoch = 87 and best_eval_custom_logloss = 0.39423
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.55064, 'Log Loss - std': 0.11344472839228802} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 0.55064 and parameters: {'n_d': 52, 'n_steps': 10, 'gamma': 1.987833456222655, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0049155442334164025, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.53926 | eval_custom_logloss: 9.22133 |  0:00:01s
epoch 1  | loss: 2.63597 | eval_custom_logloss: 7.03975 |  0:00:01s
epoch 2  | loss: 1.70929 | eval_custom_logloss: 7.30853 |  0:00:02s
epoch 3  | loss: 2.54515 | eval_custom_logloss: 7.21231 |  0:00:03s
epoch 4  | loss: 4.86836 | eval_custom_logloss: 7.66687 |  0:00:04s
epoch 5  | loss: 3.28129 | eval_custom_logloss: 6.70765 |  0:00:05s
epoch 6  | loss: 1.80955 | eval_custom_logloss: 4.81565 |  0:00:06s
epoch 7  | loss: 1.2381  | eval_custom_logloss: 3.32135 |  0:00:07s
epoch 8  | loss: 1.176   | eval_custom_logloss: 3.16828 |  0:00:07s
epoch 9  | loss: 1.18983 | eval_custom_logloss: 3.68471 |  0:00:08s
epoch 10 | loss: 1.35974 | eval_custom_logloss: 6.55204 |  0:00:09s
epoch 11 | loss: 1.08202 | eval_custom_logloss: 5.73824 |  0:00:10s
epoch 12 | loss: 0.91577 | eval_custom_logloss: 2.10504 |  0:00:11s
epoch 13 | loss: 0.80437 | eval_custom_logloss: 1.9054  |  0:00:12s
epoch 14 | loss: 0.75498 | eval_custom_logloss: 1.90154 |  0:00:12s
epoch 15 | loss: 0.73988 | eval_custom_logloss: 1.52792 |  0:00:13s
epoch 16 | loss: 0.73008 | eval_custom_logloss: 1.13796 |  0:00:14s
epoch 17 | loss: 0.72412 | eval_custom_logloss: 0.88447 |  0:00:15s
epoch 18 | loss: 0.69413 | eval_custom_logloss: 0.98339 |  0:00:16s
epoch 19 | loss: 0.71108 | eval_custom_logloss: 1.63705 |  0:00:17s
epoch 20 | loss: 0.7052  | eval_custom_logloss: 1.07803 |  0:00:18s
epoch 21 | loss: 0.66427 | eval_custom_logloss: 1.05383 |  0:00:19s
epoch 22 | loss: 0.67098 | eval_custom_logloss: 0.9477  |  0:00:20s
epoch 23 | loss: 0.62916 | eval_custom_logloss: 0.89355 |  0:00:20s
epoch 24 | loss: 0.62574 | eval_custom_logloss: 0.81933 |  0:00:21s
epoch 25 | loss: 0.67056 | eval_custom_logloss: 0.6916  |  0:00:22s
epoch 26 | loss: 0.58602 | eval_custom_logloss: 0.68487 |  0:00:23s
epoch 27 | loss: 0.58121 | eval_custom_logloss: 0.75079 |  0:00:24s
epoch 28 | loss: 0.58076 | eval_custom_logloss: 0.78224 |  0:00:25s
epoch 29 | loss: 0.54081 | eval_custom_logloss: 0.66472 |  0:00:26s
epoch 30 | loss: 0.55637 | eval_custom_logloss: 0.62836 |  0:00:27s
epoch 31 | loss: 0.5751  | eval_custom_logloss: 0.72354 |  0:00:28s
epoch 32 | loss: 0.579   | eval_custom_logloss: 0.63141 |  0:00:28s
epoch 33 | loss: 0.54408 | eval_custom_logloss: 0.59188 |  0:00:29s
epoch 34 | loss: 0.54152 | eval_custom_logloss: 0.62352 |  0:00:30s
epoch 35 | loss: 0.52361 | eval_custom_logloss: 0.58912 |  0:00:31s
epoch 36 | loss: 0.53149 | eval_custom_logloss: 0.57845 |  0:00:32s
epoch 37 | loss: 0.53151 | eval_custom_logloss: 0.5396  |  0:00:33s
epoch 38 | loss: 0.54421 | eval_custom_logloss: 0.48633 |  0:00:34s
epoch 39 | loss: 0.53697 | eval_custom_logloss: 0.52653 |  0:00:34s
epoch 40 | loss: 0.52928 | eval_custom_logloss: 0.55873 |  0:00:35s
epoch 41 | loss: 0.54389 | eval_custom_logloss: 0.57182 |  0:00:36s
epoch 42 | loss: 0.5262  | eval_custom_logloss: 0.51188 |  0:00:37s
epoch 43 | loss: 0.51522 | eval_custom_logloss: 0.53234 |  0:00:38s
epoch 44 | loss: 0.54884 | eval_custom_logloss: 0.56076 |  0:00:39s
epoch 45 | loss: 0.54211 | eval_custom_logloss: 0.57777 |  0:00:39s
epoch 46 | loss: 0.52339 | eval_custom_logloss: 0.54772 |  0:00:40s
epoch 47 | loss: 0.53278 | eval_custom_logloss: 0.65404 |  0:00:41s
epoch 48 | loss: 0.54136 | eval_custom_logloss: 0.7072  |  0:00:42s
epoch 49 | loss: 0.51532 | eval_custom_logloss: 0.66676 |  0:00:43s
epoch 50 | loss: 0.53941 | eval_custom_logloss: 0.6285  |  0:00:43s
epoch 51 | loss: 0.54478 | eval_custom_logloss: 0.63721 |  0:00:44s
epoch 52 | loss: 0.5577  | eval_custom_logloss: 0.58344 |  0:00:45s
epoch 53 | loss: 0.51178 | eval_custom_logloss: 0.72895 |  0:00:46s
epoch 54 | loss: 0.51747 | eval_custom_logloss: 0.62766 |  0:00:47s
epoch 55 | loss: 0.49547 | eval_custom_logloss: 0.69237 |  0:00:48s
epoch 56 | loss: 0.4826  | eval_custom_logloss: 0.65934 |  0:00:48s
epoch 57 | loss: 0.52404 | eval_custom_logloss: 0.68705 |  0:00:49s
epoch 58 | loss: 0.50795 | eval_custom_logloss: 0.69019 |  0:00:50s

Early stopping occurred at epoch 58 with best_epoch = 38 and best_eval_custom_logloss = 0.48633
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4863, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.63507 | eval_custom_logloss: 7.99828 |  0:00:00s
epoch 1  | loss: 2.13683 | eval_custom_logloss: 6.97575 |  0:00:01s
epoch 2  | loss: 3.59828 | eval_custom_logloss: 7.01436 |  0:00:02s
epoch 3  | loss: 2.28054 | eval_custom_logloss: 5.55131 |  0:00:03s
epoch 4  | loss: 1.88482 | eval_custom_logloss: 6.44043 |  0:00:04s
epoch 5  | loss: 2.54834 | eval_custom_logloss: 5.76417 |  0:00:05s
epoch 6  | loss: 1.57494 | eval_custom_logloss: 5.4763  |  0:00:05s
epoch 7  | loss: 1.35442 | eval_custom_logloss: 5.18564 |  0:00:06s
epoch 8  | loss: 1.08039 | eval_custom_logloss: 3.03905 |  0:00:07s
epoch 9  | loss: 1.13051 | eval_custom_logloss: 2.49047 |  0:00:08s
epoch 10 | loss: 0.96121 | eval_custom_logloss: 3.12164 |  0:00:09s
epoch 11 | loss: 0.87993 | eval_custom_logloss: 1.37628 |  0:00:10s
epoch 12 | loss: 0.83776 | eval_custom_logloss: 1.54731 |  0:00:10s
epoch 13 | loss: 0.78296 | eval_custom_logloss: 1.35793 |  0:00:11s
epoch 14 | loss: 0.79671 | eval_custom_logloss: 1.63885 |  0:00:12s
epoch 15 | loss: 0.77917 | eval_custom_logloss: 1.50638 |  0:00:13s
epoch 16 | loss: 0.83742 | eval_custom_logloss: 1.07035 |  0:00:14s
epoch 17 | loss: 0.77384 | eval_custom_logloss: 0.88217 |  0:00:15s
epoch 18 | loss: 0.73148 | eval_custom_logloss: 1.25819 |  0:00:15s
epoch 19 | loss: 0.70594 | eval_custom_logloss: 1.0945  |  0:00:16s
epoch 20 | loss: 0.68449 | eval_custom_logloss: 1.14934 |  0:00:17s
epoch 21 | loss: 0.63063 | eval_custom_logloss: 0.97578 |  0:00:18s
epoch 22 | loss: 0.67076 | eval_custom_logloss: 0.91257 |  0:00:19s
epoch 23 | loss: 0.65364 | eval_custom_logloss: 1.03489 |  0:00:20s
epoch 24 | loss: 0.67358 | eval_custom_logloss: 0.88297 |  0:00:20s
epoch 25 | loss: 0.63722 | eval_custom_logloss: 1.04637 |  0:00:21s
epoch 26 | loss: 0.61086 | eval_custom_logloss: 0.96033 |  0:00:22s
epoch 27 | loss: 0.63405 | eval_custom_logloss: 0.99446 |  0:00:23s
epoch 28 | loss: 0.60865 | eval_custom_logloss: 1.0097  |  0:00:24s
epoch 29 | loss: 0.60509 | eval_custom_logloss: 0.93862 |  0:00:24s
epoch 30 | loss: 0.5756  | eval_custom_logloss: 0.99262 |  0:00:25s
epoch 31 | loss: 0.62585 | eval_custom_logloss: 0.9298  |  0:00:26s
epoch 32 | loss: 0.63372 | eval_custom_logloss: 0.93518 |  0:00:27s
epoch 33 | loss: 0.60338 | eval_custom_logloss: 0.82154 |  0:00:28s
epoch 34 | loss: 0.57077 | eval_custom_logloss: 0.72566 |  0:00:29s
epoch 35 | loss: 0.57566 | eval_custom_logloss: 0.75542 |  0:00:30s
epoch 36 | loss: 0.54503 | eval_custom_logloss: 0.72564 |  0:00:30s
epoch 37 | loss: 0.60401 | eval_custom_logloss: 0.75158 |  0:00:31s
epoch 38 | loss: 0.6032  | eval_custom_logloss: 0.66892 |  0:00:32s
epoch 39 | loss: 0.58388 | eval_custom_logloss: 0.67806 |  0:00:33s
epoch 40 | loss: 0.58217 | eval_custom_logloss: 0.73667 |  0:00:34s
epoch 41 | loss: 0.57735 | eval_custom_logloss: 0.81094 |  0:00:35s
epoch 42 | loss: 0.56411 | eval_custom_logloss: 0.65673 |  0:00:35s
epoch 43 | loss: 0.52989 | eval_custom_logloss: 0.80739 |  0:00:36s
epoch 44 | loss: 0.55084 | eval_custom_logloss: 0.73925 |  0:00:37s
epoch 45 | loss: 0.56263 | eval_custom_logloss: 0.88838 |  0:00:38s
epoch 46 | loss: 0.54022 | eval_custom_logloss: 0.79196 |  0:00:39s
epoch 47 | loss: 0.50811 | eval_custom_logloss: 0.76126 |  0:00:39s
epoch 48 | loss: 0.58339 | eval_custom_logloss: 0.83173 |  0:00:40s
epoch 49 | loss: 0.59845 | eval_custom_logloss: 0.80327 |  0:00:41s
epoch 50 | loss: 0.54216 | eval_custom_logloss: 0.84526 |  0:00:42s
epoch 51 | loss: 0.54593 | eval_custom_logloss: 0.82106 |  0:00:43s
epoch 52 | loss: 0.58085 | eval_custom_logloss: 0.86253 |  0:00:44s
epoch 53 | loss: 0.51724 | eval_custom_logloss: 0.79004 |  0:00:45s
epoch 54 | loss: 0.52073 | eval_custom_logloss: 0.67682 |  0:00:45s
epoch 55 | loss: 0.52125 | eval_custom_logloss: 0.76303 |  0:00:46s
epoch 56 | loss: 0.52316 | eval_custom_logloss: 0.66561 |  0:00:47s
epoch 57 | loss: 0.51129 | eval_custom_logloss: 0.70688 |  0:00:48s
epoch 58 | loss: 0.50385 | eval_custom_logloss: 0.74764 |  0:00:49s
epoch 59 | loss: 0.48347 | eval_custom_logloss: 0.78145 |  0:00:50s
epoch 60 | loss: 0.4805  | eval_custom_logloss: 0.74251 |  0:00:50s
epoch 61 | loss: 0.4577  | eval_custom_logloss: 0.80831 |  0:00:51s
epoch 62 | loss: 0.51218 | eval_custom_logloss: 0.75558 |  0:00:52s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.65673
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5715, 'Log Loss - std': 0.08519999999999997} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.7962  | eval_custom_logloss: 7.63448 |  0:00:00s
epoch 1  | loss: 2.28138 | eval_custom_logloss: 7.30899 |  0:00:01s
epoch 2  | loss: 3.24192 | eval_custom_logloss: 8.26056 |  0:00:02s
epoch 3  | loss: 3.57635 | eval_custom_logloss: 4.79609 |  0:00:03s
epoch 4  | loss: 3.30675 | eval_custom_logloss: 4.64083 |  0:00:04s
epoch 5  | loss: 1.97068 | eval_custom_logloss: 4.5718  |  0:00:05s
epoch 6  | loss: 2.30201 | eval_custom_logloss: 6.34564 |  0:00:06s
epoch 7  | loss: 1.81859 | eval_custom_logloss: 4.98307 |  0:00:06s
epoch 8  | loss: 1.44423 | eval_custom_logloss: 4.76157 |  0:00:07s
epoch 9  | loss: 1.19019 | eval_custom_logloss: 2.36732 |  0:00:08s
epoch 10 | loss: 1.08373 | eval_custom_logloss: 2.56528 |  0:00:09s
epoch 11 | loss: 0.89428 | eval_custom_logloss: 2.65069 |  0:00:10s
epoch 12 | loss: 0.89373 | eval_custom_logloss: 1.27097 |  0:00:11s
epoch 13 | loss: 0.84855 | eval_custom_logloss: 1.8555  |  0:00:12s
epoch 14 | loss: 0.80578 | eval_custom_logloss: 1.4568  |  0:00:12s
epoch 15 | loss: 0.68956 | eval_custom_logloss: 1.11415 |  0:00:13s
epoch 16 | loss: 0.69772 | eval_custom_logloss: 1.07653 |  0:00:14s
epoch 17 | loss: 0.68224 | eval_custom_logloss: 0.8703  |  0:00:15s
epoch 18 | loss: 0.68572 | eval_custom_logloss: 1.11386 |  0:00:16s
epoch 19 | loss: 0.7832  | eval_custom_logloss: 1.04206 |  0:00:17s
epoch 20 | loss: 0.71092 | eval_custom_logloss: 1.05441 |  0:00:17s
epoch 21 | loss: 0.65794 | eval_custom_logloss: 1.08627 |  0:00:18s
epoch 22 | loss: 0.67453 | eval_custom_logloss: 1.01073 |  0:00:19s
epoch 23 | loss: 0.66704 | eval_custom_logloss: 0.96197 |  0:00:20s
epoch 24 | loss: 0.61479 | eval_custom_logloss: 0.82182 |  0:00:21s
epoch 25 | loss: 0.66732 | eval_custom_logloss: 1.65107 |  0:00:22s
epoch 26 | loss: 0.64293 | eval_custom_logloss: 0.96503 |  0:00:23s
epoch 27 | loss: 0.6098  | eval_custom_logloss: 1.07133 |  0:00:24s
epoch 28 | loss: 0.60795 | eval_custom_logloss: 0.94828 |  0:00:25s
epoch 29 | loss: 0.6116  | eval_custom_logloss: 0.84886 |  0:00:25s
epoch 30 | loss: 0.65463 | eval_custom_logloss: 1.04478 |  0:00:26s
epoch 31 | loss: 0.71874 | eval_custom_logloss: 1.11117 |  0:00:27s
epoch 32 | loss: 0.67842 | eval_custom_logloss: 0.74947 |  0:00:28s
epoch 33 | loss: 0.60799 | eval_custom_logloss: 0.71561 |  0:00:29s
epoch 34 | loss: 0.57631 | eval_custom_logloss: 0.87618 |  0:00:30s
epoch 35 | loss: 0.57647 | eval_custom_logloss: 0.6769  |  0:00:31s
epoch 36 | loss: 0.55585 | eval_custom_logloss: 0.68435 |  0:00:32s
epoch 37 | loss: 0.57196 | eval_custom_logloss: 0.76707 |  0:00:32s
epoch 38 | loss: 0.56103 | eval_custom_logloss: 0.72346 |  0:00:33s
epoch 39 | loss: 0.54188 | eval_custom_logloss: 0.7879  |  0:00:34s
epoch 40 | loss: 0.52432 | eval_custom_logloss: 0.77925 |  0:00:35s
epoch 41 | loss: 0.53034 | eval_custom_logloss: 0.75962 |  0:00:36s
epoch 42 | loss: 0.513   | eval_custom_logloss: 0.79568 |  0:00:37s
epoch 43 | loss: 0.60348 | eval_custom_logloss: 0.75259 |  0:00:37s
epoch 44 | loss: 0.58528 | eval_custom_logloss: 0.85757 |  0:00:38s
epoch 45 | loss: 0.56751 | eval_custom_logloss: 0.7466  |  0:00:39s
epoch 46 | loss: 0.57551 | eval_custom_logloss: 0.66343 |  0:00:40s
epoch 47 | loss: 0.55091 | eval_custom_logloss: 0.72471 |  0:00:41s
epoch 48 | loss: 0.61074 | eval_custom_logloss: 0.64314 |  0:00:42s
epoch 49 | loss: 0.62674 | eval_custom_logloss: 0.77533 |  0:00:42s
epoch 50 | loss: 0.64018 | eval_custom_logloss: 0.73454 |  0:00:43s
epoch 51 | loss: 0.63247 | eval_custom_logloss: 0.76373 |  0:00:44s
epoch 52 | loss: 0.61998 | eval_custom_logloss: 0.79301 |  0:00:45s
epoch 53 | loss: 0.6347  | eval_custom_logloss: 0.69371 |  0:00:46s
epoch 54 | loss: 0.60197 | eval_custom_logloss: 0.68887 |  0:00:47s
epoch 55 | loss: 0.59537 | eval_custom_logloss: 0.62666 |  0:00:48s
epoch 56 | loss: 0.55069 | eval_custom_logloss: 0.56672 |  0:00:48s
epoch 57 | loss: 0.54324 | eval_custom_logloss: 0.73846 |  0:00:49s
epoch 58 | loss: 0.5696  | eval_custom_logloss: 0.7404  |  0:00:50s
epoch 59 | loss: 0.50413 | eval_custom_logloss: 0.67533 |  0:00:51s
epoch 60 | loss: 0.52309 | eval_custom_logloss: 0.67612 |  0:00:52s
epoch 61 | loss: 0.50598 | eval_custom_logloss: 0.70984 |  0:00:53s
epoch 62 | loss: 0.49961 | eval_custom_logloss: 0.63953 |  0:00:54s
epoch 63 | loss: 0.512   | eval_custom_logloss: 0.54188 |  0:00:54s
epoch 64 | loss: 0.49404 | eval_custom_logloss: 0.56729 |  0:00:55s
epoch 65 | loss: 0.49124 | eval_custom_logloss: 0.61478 |  0:00:56s
epoch 66 | loss: 0.51792 | eval_custom_logloss: 0.74459 |  0:00:57s
epoch 67 | loss: 0.5508  | eval_custom_logloss: 0.65815 |  0:00:58s
epoch 68 | loss: 0.51728 | eval_custom_logloss: 0.70442 |  0:00:59s
epoch 69 | loss: 0.49838 | eval_custom_logloss: 0.60718 |  0:00:59s
epoch 70 | loss: 0.5115  | eval_custom_logloss: 0.66236 |  0:01:00s
epoch 71 | loss: 0.50704 | eval_custom_logloss: 0.66239 |  0:01:01s
epoch 72 | loss: 0.51917 | eval_custom_logloss: 0.55473 |  0:01:02s
epoch 73 | loss: 0.47592 | eval_custom_logloss: 0.50982 |  0:01:03s
epoch 74 | loss: 0.475   | eval_custom_logloss: 0.56785 |  0:01:04s
epoch 75 | loss: 0.46501 | eval_custom_logloss: 0.50192 |  0:01:05s
epoch 76 | loss: 0.4766  | eval_custom_logloss: 0.49343 |  0:01:05s
epoch 77 | loss: 0.44518 | eval_custom_logloss: 0.55952 |  0:01:06s
epoch 78 | loss: 0.46459 | eval_custom_logloss: 0.47079 |  0:01:07s
epoch 79 | loss: 0.42351 | eval_custom_logloss: 0.48638 |  0:01:08s
epoch 80 | loss: 0.43455 | eval_custom_logloss: 0.51012 |  0:01:09s
epoch 81 | loss: 0.46313 | eval_custom_logloss: 0.50896 |  0:01:10s
epoch 82 | loss: 0.44847 | eval_custom_logloss: 0.49619 |  0:01:11s
epoch 83 | loss: 0.45347 | eval_custom_logloss: 0.46856 |  0:01:11s
epoch 84 | loss: 0.43029 | eval_custom_logloss: 0.45731 |  0:01:12s
epoch 85 | loss: 0.43926 | eval_custom_logloss: 0.56375 |  0:01:13s
epoch 86 | loss: 0.44186 | eval_custom_logloss: 0.51496 |  0:01:14s
epoch 87 | loss: 0.45777 | eval_custom_logloss: 0.51446 |  0:01:15s
epoch 88 | loss: 0.45772 | eval_custom_logloss: 0.47621 |  0:01:16s
epoch 89 | loss: 0.43062 | eval_custom_logloss: 0.57786 |  0:01:17s
epoch 90 | loss: 0.42041 | eval_custom_logloss: 0.4653  |  0:01:18s
epoch 91 | loss: 0.40459 | eval_custom_logloss: 0.4764  |  0:01:19s
epoch 92 | loss: 0.40295 | eval_custom_logloss: 0.48716 |  0:01:19s
epoch 93 | loss: 0.42069 | eval_custom_logloss: 0.4529  |  0:01:20s
epoch 94 | loss: 0.41221 | eval_custom_logloss: 0.48356 |  0:01:21s
epoch 95 | loss: 0.4066  | eval_custom_logloss: 0.42859 |  0:01:22s
epoch 96 | loss: 0.40415 | eval_custom_logloss: 0.42762 |  0:01:23s
epoch 97 | loss: 0.47699 | eval_custom_logloss: 0.47071 |  0:01:24s
epoch 98 | loss: 0.41916 | eval_custom_logloss: 0.46241 |  0:01:25s
epoch 99 | loss: 0.39806 | eval_custom_logloss: 0.39771 |  0:01:26s
Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_eval_custom_logloss = 0.39771
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5135666666666666, 'Log Loss - std': 0.1074797758753814} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.30307 | eval_custom_logloss: 7.00167 |  0:00:00s
epoch 1  | loss: 2.3823  | eval_custom_logloss: 6.81844 |  0:00:01s
epoch 2  | loss: 1.66802 | eval_custom_logloss: 6.53263 |  0:00:02s
epoch 3  | loss: 2.20942 | eval_custom_logloss: 7.10602 |  0:00:03s
epoch 4  | loss: 6.88462 | eval_custom_logloss: 8.38693 |  0:00:04s
epoch 5  | loss: 8.25505 | eval_custom_logloss: 7.9319  |  0:00:05s
epoch 6  | loss: 3.9202  | eval_custom_logloss: 5.08484 |  0:00:05s
epoch 7  | loss: 2.46521 | eval_custom_logloss: 4.53446 |  0:00:06s
epoch 8  | loss: 2.04211 | eval_custom_logloss: 3.87    |  0:00:07s
epoch 9  | loss: 1.4955  | eval_custom_logloss: 5.2708  |  0:00:08s
epoch 10 | loss: 1.68698 | eval_custom_logloss: 2.79553 |  0:00:09s
epoch 11 | loss: 1.06115 | eval_custom_logloss: 3.38981 |  0:00:10s
epoch 12 | loss: 0.85234 | eval_custom_logloss: 3.19513 |  0:00:10s
epoch 13 | loss: 0.83187 | eval_custom_logloss: 1.6132  |  0:00:11s
epoch 14 | loss: 0.75095 | eval_custom_logloss: 1.26825 |  0:00:12s
epoch 15 | loss: 0.73762 | eval_custom_logloss: 1.24619 |  0:00:13s
epoch 16 | loss: 0.70354 | eval_custom_logloss: 1.42754 |  0:00:14s
epoch 17 | loss: 0.69162 | eval_custom_logloss: 1.13332 |  0:00:15s
epoch 18 | loss: 0.70763 | eval_custom_logloss: 1.28021 |  0:00:15s
epoch 19 | loss: 0.69387 | eval_custom_logloss: 1.10131 |  0:00:16s
epoch 20 | loss: 0.65301 | eval_custom_logloss: 1.13393 |  0:00:17s
epoch 21 | loss: 0.6046  | eval_custom_logloss: 1.03709 |  0:00:18s
epoch 22 | loss: 0.60635 | eval_custom_logloss: 1.10072 |  0:00:19s
epoch 23 | loss: 0.53682 | eval_custom_logloss: 0.90945 |  0:00:19s
epoch 24 | loss: 0.54744 | eval_custom_logloss: 0.75498 |  0:00:20s
epoch 25 | loss: 0.5327  | eval_custom_logloss: 0.7608  |  0:00:21s
epoch 26 | loss: 0.5167  | eval_custom_logloss: 0.82463 |  0:00:22s
epoch 27 | loss: 0.64185 | eval_custom_logloss: 0.83623 |  0:00:23s
epoch 28 | loss: 0.58778 | eval_custom_logloss: 0.73573 |  0:00:24s
epoch 29 | loss: 0.57385 | eval_custom_logloss: 0.8834  |  0:00:25s
epoch 30 | loss: 0.56174 | eval_custom_logloss: 0.90976 |  0:00:25s
epoch 31 | loss: 0.56531 | eval_custom_logloss: 0.74136 |  0:00:26s
epoch 32 | loss: 0.52904 | eval_custom_logloss: 0.7519  |  0:00:27s
epoch 33 | loss: 0.5476  | eval_custom_logloss: 0.69291 |  0:00:28s
epoch 34 | loss: 0.52076 | eval_custom_logloss: 0.70303 |  0:00:29s
epoch 35 | loss: 0.50387 | eval_custom_logloss: 0.67468 |  0:00:29s
epoch 36 | loss: 0.51024 | eval_custom_logloss: 0.62152 |  0:00:30s
epoch 37 | loss: 0.5203  | eval_custom_logloss: 0.5861  |  0:00:31s
epoch 38 | loss: 0.54407 | eval_custom_logloss: 0.77881 |  0:00:32s
epoch 39 | loss: 0.54731 | eval_custom_logloss: 0.74421 |  0:00:33s
epoch 40 | loss: 0.52598 | eval_custom_logloss: 0.69656 |  0:00:34s
epoch 41 | loss: 0.53934 | eval_custom_logloss: 0.7769  |  0:00:34s
epoch 42 | loss: 0.53472 | eval_custom_logloss: 0.70591 |  0:00:35s
epoch 43 | loss: 0.51732 | eval_custom_logloss: 0.65585 |  0:00:36s
epoch 44 | loss: 0.53282 | eval_custom_logloss: 0.74845 |  0:00:37s
epoch 45 | loss: 0.61926 | eval_custom_logloss: 0.70019 |  0:00:38s
epoch 46 | loss: 0.56373 | eval_custom_logloss: 0.74644 |  0:00:39s
epoch 47 | loss: 0.56364 | eval_custom_logloss: 0.6899  |  0:00:39s
epoch 48 | loss: 0.5351  | eval_custom_logloss: 0.94103 |  0:00:40s
epoch 49 | loss: 0.56304 | eval_custom_logloss: 0.9283  |  0:00:41s
epoch 50 | loss: 0.61043 | eval_custom_logloss: 0.77073 |  0:00:42s
epoch 51 | loss: 0.56072 | eval_custom_logloss: 0.67002 |  0:00:43s
epoch 52 | loss: 0.50195 | eval_custom_logloss: 0.68826 |  0:00:43s
epoch 53 | loss: 0.514   | eval_custom_logloss: 0.85716 |  0:00:44s
epoch 54 | loss: 0.56263 | eval_custom_logloss: 0.71755 |  0:00:45s
epoch 55 | loss: 0.58365 | eval_custom_logloss: 0.6326  |  0:00:46s
epoch 56 | loss: 0.58985 | eval_custom_logloss: 0.64529 |  0:00:47s
epoch 57 | loss: 0.53292 | eval_custom_logloss: 0.63492 |  0:00:47s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 0.5861
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5317, 'Log Loss - std': 0.09823634765197653} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax', 'n_a': 53, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.10671 | eval_custom_logloss: 7.25444 |  0:00:00s
epoch 1  | loss: 3.19795 | eval_custom_logloss: 7.50303 |  0:00:01s
epoch 2  | loss: 2.82547 | eval_custom_logloss: 5.7086  |  0:00:02s
epoch 3  | loss: 2.5663  | eval_custom_logloss: 5.63    |  0:00:03s
epoch 4  | loss: 2.25398 | eval_custom_logloss: 4.63084 |  0:00:04s
epoch 5  | loss: 1.39465 | eval_custom_logloss: 4.03832 |  0:00:05s
epoch 6  | loss: 1.44239 | eval_custom_logloss: 5.19583 |  0:00:06s
epoch 7  | loss: 1.07731 | eval_custom_logloss: 4.66624 |  0:00:06s
epoch 8  | loss: 1.04833 | eval_custom_logloss: 2.03117 |  0:00:07s
epoch 9  | loss: 1.20359 | eval_custom_logloss: 2.99998 |  0:00:08s
epoch 10 | loss: 1.2333  | eval_custom_logloss: 2.65439 |  0:00:09s
epoch 11 | loss: 0.97291 | eval_custom_logloss: 1.65403 |  0:00:10s
epoch 12 | loss: 0.86307 | eval_custom_logloss: 1.51124 |  0:00:10s
epoch 13 | loss: 0.80842 | eval_custom_logloss: 1.36775 |  0:00:11s
epoch 14 | loss: 0.86995 | eval_custom_logloss: 2.06323 |  0:00:12s
epoch 15 | loss: 0.74575 | eval_custom_logloss: 2.55397 |  0:00:13s
epoch 16 | loss: 0.75444 | eval_custom_logloss: 1.33279 |  0:00:14s
epoch 17 | loss: 0.72109 | eval_custom_logloss: 1.07309 |  0:00:15s
epoch 18 | loss: 0.72257 | eval_custom_logloss: 0.8253  |  0:00:16s
epoch 19 | loss: 0.6944  | eval_custom_logloss: 0.89725 |  0:00:17s
epoch 20 | loss: 0.76617 | eval_custom_logloss: 1.0263  |  0:00:17s
epoch 21 | loss: 0.72736 | eval_custom_logloss: 0.66504 |  0:00:18s
epoch 22 | loss: 0.70309 | eval_custom_logloss: 0.8819  |  0:00:19s
epoch 23 | loss: 0.68455 | eval_custom_logloss: 0.89406 |  0:00:20s
epoch 24 | loss: 0.66637 | eval_custom_logloss: 0.82394 |  0:00:21s
epoch 25 | loss: 0.6257  | eval_custom_logloss: 0.83079 |  0:00:22s
epoch 26 | loss: 0.61186 | eval_custom_logloss: 0.59532 |  0:00:22s
epoch 27 | loss: 0.61342 | eval_custom_logloss: 0.64344 |  0:00:23s
epoch 28 | loss: 0.6573  | eval_custom_logloss: 0.70271 |  0:00:24s
epoch 29 | loss: 0.71218 | eval_custom_logloss: 0.81861 |  0:00:25s
epoch 30 | loss: 0.73279 | eval_custom_logloss: 0.81076 |  0:00:26s
epoch 31 | loss: 0.6636  | eval_custom_logloss: 0.68389 |  0:00:27s
epoch 32 | loss: 0.68616 | eval_custom_logloss: 0.68483 |  0:00:27s
epoch 33 | loss: 0.67883 | eval_custom_logloss: 0.74971 |  0:00:28s
epoch 34 | loss: 0.62955 | eval_custom_logloss: 0.6934  |  0:00:29s
epoch 35 | loss: 0.63377 | eval_custom_logloss: 0.77176 |  0:00:30s
epoch 36 | loss: 0.60476 | eval_custom_logloss: 0.7029  |  0:00:31s
epoch 37 | loss: 0.59912 | eval_custom_logloss: 0.69834 |  0:00:31s
epoch 38 | loss: 0.64767 | eval_custom_logloss: 0.73174 |  0:00:32s
epoch 39 | loss: 0.61737 | eval_custom_logloss: 0.66994 |  0:00:33s
epoch 40 | loss: 0.60618 | eval_custom_logloss: 0.63446 |  0:00:34s
epoch 41 | loss: 0.59586 | eval_custom_logloss: 0.62348 |  0:00:35s
epoch 42 | loss: 0.58483 | eval_custom_logloss: 0.54161 |  0:00:36s
epoch 43 | loss: 0.59924 | eval_custom_logloss: 0.59233 |  0:00:37s
epoch 44 | loss: 0.58911 | eval_custom_logloss: 0.72232 |  0:00:37s
epoch 45 | loss: 0.59927 | eval_custom_logloss: 0.75899 |  0:00:38s
epoch 46 | loss: 0.58757 | eval_custom_logloss: 0.69203 |  0:00:39s
epoch 47 | loss: 0.57069 | eval_custom_logloss: 0.60765 |  0:00:40s
epoch 48 | loss: 0.55102 | eval_custom_logloss: 0.64049 |  0:00:41s
epoch 49 | loss: 0.5617  | eval_custom_logloss: 0.57239 |  0:00:42s
epoch 50 | loss: 0.54312 | eval_custom_logloss: 0.62745 |  0:00:42s
epoch 51 | loss: 0.56819 | eval_custom_logloss: 0.53088 |  0:00:43s
epoch 52 | loss: 0.54188 | eval_custom_logloss: 0.59837 |  0:00:44s
epoch 53 | loss: 0.52275 | eval_custom_logloss: 0.71219 |  0:00:45s
epoch 54 | loss: 0.54818 | eval_custom_logloss: 0.591   |  0:00:46s
epoch 55 | loss: 0.51491 | eval_custom_logloss: 0.62909 |  0:00:47s
epoch 56 | loss: 0.50518 | eval_custom_logloss: 0.57236 |  0:00:48s
epoch 57 | loss: 0.49979 | eval_custom_logloss: 0.49335 |  0:00:49s
epoch 58 | loss: 0.51228 | eval_custom_logloss: 0.58339 |  0:00:50s
epoch 59 | loss: 0.50401 | eval_custom_logloss: 0.57064 |  0:00:51s
epoch 60 | loss: 0.56567 | eval_custom_logloss: 0.56889 |  0:00:52s
epoch 61 | loss: 0.54982 | eval_custom_logloss: 0.62228 |  0:00:53s
epoch 62 | loss: 0.54794 | eval_custom_logloss: 0.5997  |  0:00:53s
epoch 63 | loss: 0.51804 | eval_custom_logloss: 0.55197 |  0:00:54s
epoch 64 | loss: 0.53582 | eval_custom_logloss: 0.51316 |  0:00:55s
epoch 65 | loss: 0.50107 | eval_custom_logloss: 0.52574 |  0:00:56s
epoch 66 | loss: 0.50711 | eval_custom_logloss: 0.48249 |  0:00:57s
epoch 67 | loss: 0.50821 | eval_custom_logloss: 0.50087 |  0:00:58s
epoch 68 | loss: 0.48116 | eval_custom_logloss: 0.56367 |  0:00:59s
epoch 69 | loss: 0.4883  | eval_custom_logloss: 0.56033 |  0:01:00s
epoch 70 | loss: 0.50616 | eval_custom_logloss: 0.43667 |  0:01:01s
epoch 71 | loss: 0.49626 | eval_custom_logloss: 0.5074  |  0:01:02s
epoch 72 | loss: 0.49227 | eval_custom_logloss: 0.495   |  0:01:02s
epoch 73 | loss: 0.50374 | eval_custom_logloss: 0.43928 |  0:01:03s
epoch 74 | loss: 0.61795 | eval_custom_logloss: 0.58002 |  0:01:04s
epoch 75 | loss: 0.60747 | eval_custom_logloss: 0.6991  |  0:01:05s
epoch 76 | loss: 0.57658 | eval_custom_logloss: 0.62725 |  0:01:06s
epoch 77 | loss: 0.59191 | eval_custom_logloss: 0.84938 |  0:01:07s
epoch 78 | loss: 0.62768 | eval_custom_logloss: 0.65097 |  0:01:08s
epoch 79 | loss: 0.58101 | eval_custom_logloss: 0.59211 |  0:01:09s
epoch 80 | loss: 0.56072 | eval_custom_logloss: 0.50744 |  0:01:10s
epoch 81 | loss: 0.52333 | eval_custom_logloss: 0.4499  |  0:01:11s
epoch 82 | loss: 0.49597 | eval_custom_logloss: 0.49102 |  0:01:11s
epoch 83 | loss: 0.49742 | eval_custom_logloss: 0.42979 |  0:01:12s
epoch 84 | loss: 0.48653 | eval_custom_logloss: 0.40945 |  0:01:13s
epoch 85 | loss: 0.47229 | eval_custom_logloss: 0.39682 |  0:01:14s
epoch 86 | loss: 0.4743  | eval_custom_logloss: 0.43517 |  0:01:15s
epoch 87 | loss: 0.5118  | eval_custom_logloss: 0.60715 |  0:01:16s
epoch 88 | loss: 0.52209 | eval_custom_logloss: 0.45134 |  0:01:17s
epoch 89 | loss: 0.51867 | eval_custom_logloss: 0.52207 |  0:01:18s
epoch 90 | loss: 0.541   | eval_custom_logloss: 0.47187 |  0:01:19s
epoch 91 | loss: 0.5148  | eval_custom_logloss: 0.47727 |  0:01:20s
epoch 92 | loss: 0.50132 | eval_custom_logloss: 0.45464 |  0:01:21s
epoch 93 | loss: 0.48868 | eval_custom_logloss: 0.4315  |  0:01:21s
epoch 94 | loss: 0.51717 | eval_custom_logloss: 0.41268 |  0:01:22s
epoch 95 | loss: 0.46937 | eval_custom_logloss: 0.3886  |  0:01:23s
epoch 96 | loss: 0.46907 | eval_custom_logloss: 0.5094  |  0:01:24s
epoch 97 | loss: 0.49918 | eval_custom_logloss: 0.45972 |  0:01:25s
epoch 98 | loss: 0.43891 | eval_custom_logloss: 0.4337  |  0:01:26s
epoch 99 | loss: 0.46481 | eval_custom_logloss: 0.44825 |  0:01:27s
Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_eval_custom_logloss = 0.3886
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.50308, 'Log Loss - std': 0.10486525449356424} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 0.50308 and parameters: {'n_d': 53, 'n_steps': 10, 'gamma': 1.8401853660913468, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.01133585317330965, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax', 'n_a': 56, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.13158 | eval_custom_logloss: 8.01379 |  0:00:00s
epoch 1  | loss: 1.67763 | eval_custom_logloss: 8.6893  |  0:00:01s
epoch 2  | loss: 2.24936 | eval_custom_logloss: 9.07298 |  0:00:02s
epoch 3  | loss: 2.38375 | eval_custom_logloss: 8.24757 |  0:00:02s
epoch 4  | loss: 4.20707 | eval_custom_logloss: 7.30621 |  0:00:03s
epoch 5  | loss: 2.52343 | eval_custom_logloss: 8.76099 |  0:00:04s
epoch 6  | loss: 2.71232 | eval_custom_logloss: 7.48283 |  0:00:05s
epoch 7  | loss: 1.2938  | eval_custom_logloss: 6.48186 |  0:00:05s
epoch 8  | loss: 1.05861 | eval_custom_logloss: 7.15124 |  0:00:06s
epoch 9  | loss: 0.93689 | eval_custom_logloss: 5.11035 |  0:00:07s
epoch 10 | loss: 0.85007 | eval_custom_logloss: 7.80176 |  0:00:08s
epoch 11 | loss: 0.85653 | eval_custom_logloss: 7.37074 |  0:00:08s
epoch 12 | loss: 0.76857 | eval_custom_logloss: 4.37872 |  0:00:09s
epoch 13 | loss: 0.87435 | eval_custom_logloss: 4.64455 |  0:00:10s
epoch 14 | loss: 0.91333 | eval_custom_logloss: 7.92869 |  0:00:11s
epoch 15 | loss: 0.83414 | eval_custom_logloss: 3.14413 |  0:00:11s
epoch 16 | loss: 0.73537 | eval_custom_logloss: 3.06115 |  0:00:12s
epoch 17 | loss: 0.72176 | eval_custom_logloss: 3.68757 |  0:00:13s
epoch 18 | loss: 0.67497 | eval_custom_logloss: 3.83647 |  0:00:14s
epoch 19 | loss: 0.70847 | eval_custom_logloss: 3.62218 |  0:00:14s
epoch 20 | loss: 0.78133 | eval_custom_logloss: 3.29332 |  0:00:15s
epoch 21 | loss: 0.68658 | eval_custom_logloss: 5.061   |  0:00:16s
epoch 22 | loss: 0.6726  | eval_custom_logloss: 4.38242 |  0:00:16s
epoch 23 | loss: 0.68439 | eval_custom_logloss: 4.07882 |  0:00:17s
epoch 24 | loss: 0.61055 | eval_custom_logloss: 4.09084 |  0:00:18s
epoch 25 | loss: 0.63342 | eval_custom_logloss: 2.71185 |  0:00:19s
epoch 26 | loss: 0.63258 | eval_custom_logloss: 2.39925 |  0:00:19s
epoch 27 | loss: 0.61339 | eval_custom_logloss: 2.50541 |  0:00:20s
epoch 28 | loss: 0.63136 | eval_custom_logloss: 2.20375 |  0:00:21s
epoch 29 | loss: 0.64891 | eval_custom_logloss: 2.39313 |  0:00:22s
epoch 30 | loss: 0.64692 | eval_custom_logloss: 2.99976 |  0:00:22s
epoch 31 | loss: 0.6206  | eval_custom_logloss: 1.65642 |  0:00:23s
epoch 32 | loss: 0.59947 | eval_custom_logloss: 2.17374 |  0:00:24s
epoch 33 | loss: 0.59579 | eval_custom_logloss: 2.89594 |  0:00:25s
epoch 34 | loss: 0.6167  | eval_custom_logloss: 2.40318 |  0:00:25s
epoch 35 | loss: 0.60477 | eval_custom_logloss: 1.88264 |  0:00:26s
epoch 36 | loss: 0.64316 | eval_custom_logloss: 1.96883 |  0:00:27s
epoch 37 | loss: 0.61032 | eval_custom_logloss: 2.24451 |  0:00:28s
epoch 38 | loss: 0.62148 | eval_custom_logloss: 2.77476 |  0:00:28s
epoch 39 | loss: 0.62056 | eval_custom_logloss: 2.89686 |  0:00:29s
epoch 40 | loss: 0.60337 | eval_custom_logloss: 3.12142 |  0:00:30s
epoch 41 | loss: 0.63962 | eval_custom_logloss: 2.79459 |  0:00:30s
epoch 42 | loss: 0.65485 | eval_custom_logloss: 2.29292 |  0:00:31s
epoch 43 | loss: 0.60369 | eval_custom_logloss: 2.65191 |  0:00:32s
epoch 44 | loss: 0.58923 | eval_custom_logloss: 2.75075 |  0:00:33s
epoch 45 | loss: 0.58168 | eval_custom_logloss: 2.2791  |  0:00:33s
epoch 46 | loss: 0.56044 | eval_custom_logloss: 2.11357 |  0:00:34s
epoch 47 | loss: 0.53214 | eval_custom_logloss: 1.74927 |  0:00:35s
epoch 48 | loss: 0.54247 | eval_custom_logloss: 1.33875 |  0:00:36s
epoch 49 | loss: 0.53761 | eval_custom_logloss: 1.28905 |  0:00:36s
epoch 50 | loss: 0.52936 | eval_custom_logloss: 1.33504 |  0:00:37s
epoch 51 | loss: 0.51748 | eval_custom_logloss: 1.16552 |  0:00:38s
epoch 52 | loss: 0.52647 | eval_custom_logloss: 1.23623 |  0:00:39s
epoch 53 | loss: 0.52941 | eval_custom_logloss: 1.44607 |  0:00:39s
epoch 54 | loss: 0.52043 | eval_custom_logloss: 1.47759 |  0:00:40s
epoch 55 | loss: 0.49604 | eval_custom_logloss: 1.39588 |  0:00:41s
epoch 56 | loss: 0.49809 | eval_custom_logloss: 1.30392 |  0:00:42s
epoch 57 | loss: 0.51129 | eval_custom_logloss: 1.48529 |  0:00:42s
epoch 58 | loss: 0.53192 | eval_custom_logloss: 1.61303 |  0:00:43s
epoch 59 | loss: 0.53703 | eval_custom_logloss: 1.40195 |  0:00:44s
epoch 60 | loss: 0.54528 | eval_custom_logloss: 1.5237  |  0:00:44s
epoch 61 | loss: 0.53141 | eval_custom_logloss: 1.3017  |  0:00:45s
epoch 62 | loss: 0.55689 | eval_custom_logloss: 1.13217 |  0:00:46s
epoch 63 | loss: 0.56472 | eval_custom_logloss: 1.17324 |  0:00:47s
epoch 64 | loss: 0.53778 | eval_custom_logloss: 1.8447  |  0:00:47s
epoch 65 | loss: 0.53135 | eval_custom_logloss: 2.01112 |  0:00:48s
epoch 66 | loss: 0.50793 | eval_custom_logloss: 1.70515 |  0:00:49s
epoch 67 | loss: 0.50898 | eval_custom_logloss: 1.82651 |  0:00:50s
epoch 68 | loss: 0.49296 | eval_custom_logloss: 2.19771 |  0:00:50s
epoch 69 | loss: 0.51277 | eval_custom_logloss: 1.5734  |  0:00:51s
epoch 70 | loss: 0.49863 | eval_custom_logloss: 1.383   |  0:00:52s
epoch 71 | loss: 0.50806 | eval_custom_logloss: 1.35287 |  0:00:53s
epoch 72 | loss: 0.51601 | eval_custom_logloss: 1.11938 |  0:00:53s
epoch 73 | loss: 0.47986 | eval_custom_logloss: 1.4259  |  0:00:54s
epoch 74 | loss: 0.48365 | eval_custom_logloss: 1.20552 |  0:00:55s
epoch 75 | loss: 0.51289 | eval_custom_logloss: 1.24843 |  0:00:55s
epoch 76 | loss: 0.4883  | eval_custom_logloss: 1.16337 |  0:00:56s
epoch 77 | loss: 0.51001 | eval_custom_logloss: 1.16318 |  0:00:57s
epoch 78 | loss: 0.51394 | eval_custom_logloss: 1.35477 |  0:00:58s
epoch 79 | loss: 0.47119 | eval_custom_logloss: 1.1355  |  0:00:58s
epoch 80 | loss: 0.4811  | eval_custom_logloss: 1.12551 |  0:00:59s
epoch 81 | loss: 0.46156 | eval_custom_logloss: 1.16043 |  0:01:00s
epoch 82 | loss: 0.45172 | eval_custom_logloss: 1.15194 |  0:01:01s
epoch 83 | loss: 0.47816 | eval_custom_logloss: 0.88326 |  0:01:01s
epoch 84 | loss: 0.52186 | eval_custom_logloss: 0.95079 |  0:01:02s
epoch 85 | loss: 0.47219 | eval_custom_logloss: 0.74577 |  0:01:03s
epoch 86 | loss: 0.46392 | eval_custom_logloss: 0.88274 |  0:01:04s
epoch 87 | loss: 0.44719 | eval_custom_logloss: 1.29095 |  0:01:04s
epoch 88 | loss: 0.4235  | eval_custom_logloss: 1.32433 |  0:01:05s
epoch 89 | loss: 0.42674 | eval_custom_logloss: 1.44911 |  0:01:06s
epoch 90 | loss: 0.4208  | eval_custom_logloss: 1.37956 |  0:01:06s
epoch 91 | loss: 0.42491 | eval_custom_logloss: 1.49849 |  0:01:07s
epoch 92 | loss: 0.47838 | eval_custom_logloss: 1.83023 |  0:01:08s
epoch 93 | loss: 0.4661  | eval_custom_logloss: 1.7716  |  0:01:09s
epoch 94 | loss: 0.48844 | eval_custom_logloss: 1.45646 |  0:01:09s
epoch 95 | loss: 0.47136 | eval_custom_logloss: 1.39059 |  0:01:10s
epoch 96 | loss: 0.45044 | eval_custom_logloss: 1.1847  |  0:01:11s
epoch 97 | loss: 0.45144 | eval_custom_logloss: 1.16738 |  0:01:12s
epoch 98 | loss: 0.46565 | eval_custom_logloss: 0.83583 |  0:01:12s
epoch 99 | loss: 0.42795 | eval_custom_logloss: 0.86697 |  0:01:13s
Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_eval_custom_logloss = 0.74577
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7088, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax', 'n_a': 56, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.43431 | eval_custom_logloss: 7.2839  |  0:00:00s
epoch 1  | loss: 1.81292 | eval_custom_logloss: 7.59654 |  0:00:01s
epoch 2  | loss: 3.08532 | eval_custom_logloss: 7.88008 |  0:00:02s
epoch 3  | loss: 2.69037 | eval_custom_logloss: 8.54665 |  0:00:03s
epoch 4  | loss: 1.94322 | eval_custom_logloss: 8.17419 |  0:00:03s
epoch 5  | loss: 3.4295  | eval_custom_logloss: 8.4028  |  0:00:04s
epoch 6  | loss: 2.16396 | eval_custom_logloss: 9.43109 |  0:00:05s
epoch 7  | loss: 1.82584 | eval_custom_logloss: 7.29531 |  0:00:05s
epoch 8  | loss: 1.21617 | eval_custom_logloss: 6.041   |  0:00:06s
epoch 9  | loss: 0.918   | eval_custom_logloss: 7.24777 |  0:00:07s
epoch 10 | loss: 0.87843 | eval_custom_logloss: 6.6295  |  0:00:08s
epoch 11 | loss: 0.91491 | eval_custom_logloss: 4.26537 |  0:00:08s
epoch 12 | loss: 0.77125 | eval_custom_logloss: 5.9615  |  0:00:09s
epoch 13 | loss: 0.79266 | eval_custom_logloss: 7.14224 |  0:00:10s
epoch 14 | loss: 0.73429 | eval_custom_logloss: 2.96224 |  0:00:11s
epoch 15 | loss: 0.71143 | eval_custom_logloss: 4.69944 |  0:00:11s
epoch 16 | loss: 0.76171 | eval_custom_logloss: 5.1146  |  0:00:12s
epoch 17 | loss: 0.69151 | eval_custom_logloss: 3.75274 |  0:00:13s
epoch 18 | loss: 0.64659 | eval_custom_logloss: 4.06906 |  0:00:13s
epoch 19 | loss: 0.65536 | eval_custom_logloss: 2.52974 |  0:00:14s
epoch 20 | loss: 0.64997 | eval_custom_logloss: 2.71116 |  0:00:15s
epoch 21 | loss: 0.63569 | eval_custom_logloss: 2.08562 |  0:00:16s
epoch 22 | loss: 0.59274 | eval_custom_logloss: 2.579   |  0:00:16s
epoch 23 | loss: 0.55336 | eval_custom_logloss: 3.54698 |  0:00:17s
epoch 24 | loss: 0.62908 | eval_custom_logloss: 3.55531 |  0:00:18s
epoch 25 | loss: 0.66543 | eval_custom_logloss: 2.87561 |  0:00:19s
epoch 26 | loss: 0.6393  | eval_custom_logloss: 2.47801 |  0:00:19s
epoch 27 | loss: 0.57505 | eval_custom_logloss: 2.6443  |  0:00:20s
epoch 28 | loss: 0.61028 | eval_custom_logloss: 2.60362 |  0:00:21s
epoch 29 | loss: 0.64482 | eval_custom_logloss: 2.31667 |  0:00:21s
epoch 30 | loss: 0.56748 | eval_custom_logloss: 2.18219 |  0:00:22s
epoch 31 | loss: 0.61611 | eval_custom_logloss: 2.21337 |  0:00:23s
epoch 32 | loss: 0.56325 | eval_custom_logloss: 2.29626 |  0:00:24s
epoch 33 | loss: 0.51639 | eval_custom_logloss: 2.50162 |  0:00:24s
epoch 34 | loss: 0.50793 | eval_custom_logloss: 2.53469 |  0:00:25s
epoch 35 | loss: 0.53618 | eval_custom_logloss: 2.54122 |  0:00:26s
epoch 36 | loss: 0.51487 | eval_custom_logloss: 1.61959 |  0:00:26s
epoch 37 | loss: 0.48051 | eval_custom_logloss: 1.59905 |  0:00:27s
epoch 38 | loss: 0.47813 | eval_custom_logloss: 1.61071 |  0:00:28s
epoch 39 | loss: 0.46431 | eval_custom_logloss: 1.55119 |  0:00:29s
epoch 40 | loss: 0.43902 | eval_custom_logloss: 1.40337 |  0:00:29s
epoch 41 | loss: 0.41836 | eval_custom_logloss: 1.46987 |  0:00:30s
epoch 42 | loss: 0.41567 | eval_custom_logloss: 1.70694 |  0:00:31s
epoch 43 | loss: 0.40052 | eval_custom_logloss: 1.12023 |  0:00:32s
epoch 44 | loss: 0.37913 | eval_custom_logloss: 1.09072 |  0:00:32s
epoch 45 | loss: 0.37967 | eval_custom_logloss: 1.151   |  0:00:33s
epoch 46 | loss: 0.35852 | eval_custom_logloss: 1.1178  |  0:00:34s
epoch 47 | loss: 0.36758 | eval_custom_logloss: 0.99001 |  0:00:34s
epoch 48 | loss: 0.39252 | eval_custom_logloss: 0.89642 |  0:00:35s
epoch 49 | loss: 0.36316 | eval_custom_logloss: 1.12576 |  0:00:36s
epoch 50 | loss: 0.34924 | eval_custom_logloss: 0.93732 |  0:00:37s
epoch 51 | loss: 0.35944 | eval_custom_logloss: 0.92767 |  0:00:37s
epoch 52 | loss: 0.35172 | eval_custom_logloss: 0.82794 |  0:00:38s
epoch 53 | loss: 0.36419 | eval_custom_logloss: 1.03184 |  0:00:39s
epoch 54 | loss: 0.35467 | eval_custom_logloss: 0.96032 |  0:00:40s
epoch 55 | loss: 0.33393 | eval_custom_logloss: 0.99129 |  0:00:40s
epoch 56 | loss: 0.36776 | eval_custom_logloss: 1.0231  |  0:00:41s
epoch 57 | loss: 0.35667 | eval_custom_logloss: 0.90053 |  0:00:42s
epoch 58 | loss: 0.33075 | eval_custom_logloss: 0.87303 |  0:00:42s
epoch 59 | loss: 0.38167 | eval_custom_logloss: 1.3778  |  0:00:43s
epoch 60 | loss: 0.33241 | eval_custom_logloss: 1.14323 |  0:00:44s
epoch 61 | loss: 0.35767 | eval_custom_logloss: 1.24398 |  0:00:45s
epoch 62 | loss: 0.30971 | eval_custom_logloss: 1.07456 |  0:00:45s
epoch 63 | loss: 0.29876 | eval_custom_logloss: 1.17589 |  0:00:46s
epoch 64 | loss: 0.29293 | eval_custom_logloss: 1.24573 |  0:00:47s
epoch 65 | loss: 0.2925  | eval_custom_logloss: 0.93428 |  0:00:47s
epoch 66 | loss: 0.31357 | eval_custom_logloss: 1.09458 |  0:00:48s
epoch 67 | loss: 0.30843 | eval_custom_logloss: 1.12258 |  0:00:49s
epoch 68 | loss: 0.28887 | eval_custom_logloss: 1.17311 |  0:00:50s
epoch 69 | loss: 0.30727 | eval_custom_logloss: 1.10798 |  0:00:50s
epoch 70 | loss: 0.30953 | eval_custom_logloss: 0.84764 |  0:00:51s
epoch 71 | loss: 0.33715 | eval_custom_logloss: 1.08947 |  0:00:52s
epoch 72 | loss: 0.31255 | eval_custom_logloss: 1.1191  |  0:00:52s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 0.82794
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7616499999999999, 'Log Loss - std': 0.05285000000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax', 'n_a': 56, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.00697 | eval_custom_logloss: 7.71604 |  0:00:00s
epoch 1  | loss: 1.7763  | eval_custom_logloss: 7.76208 |  0:00:01s
epoch 2  | loss: 2.27124 | eval_custom_logloss: 8.38882 |  0:00:02s
epoch 3  | loss: 1.6539  | eval_custom_logloss: 8.65653 |  0:00:03s
epoch 4  | loss: 1.43872 | eval_custom_logloss: 9.5633  |  0:00:03s
epoch 5  | loss: 1.92638 | eval_custom_logloss: 6.3968  |  0:00:04s
epoch 6  | loss: 1.67041 | eval_custom_logloss: 5.96854 |  0:00:05s
epoch 7  | loss: 0.97782 | eval_custom_logloss: 5.97528 |  0:00:06s
epoch 8  | loss: 0.8862  | eval_custom_logloss: 7.56714 |  0:00:06s
epoch 9  | loss: 0.94278 | eval_custom_logloss: 4.10908 |  0:00:07s
epoch 10 | loss: 0.83092 | eval_custom_logloss: 5.01978 |  0:00:08s
epoch 11 | loss: 0.82237 | eval_custom_logloss: 4.62254 |  0:00:09s
epoch 12 | loss: 0.80007 | eval_custom_logloss: 5.24549 |  0:00:09s
epoch 13 | loss: 0.79368 | eval_custom_logloss: 6.38092 |  0:00:10s
epoch 14 | loss: 0.74113 | eval_custom_logloss: 4.44396 |  0:00:11s
epoch 15 | loss: 0.67472 | eval_custom_logloss: 5.08981 |  0:00:12s
epoch 16 | loss: 0.69665 | eval_custom_logloss: 4.69823 |  0:00:12s
epoch 17 | loss: 0.67862 | eval_custom_logloss: 4.96795 |  0:00:13s
epoch 18 | loss: 0.69508 | eval_custom_logloss: 4.73882 |  0:00:14s
epoch 19 | loss: 0.67792 | eval_custom_logloss: 3.68552 |  0:00:15s
epoch 20 | loss: 0.64685 | eval_custom_logloss: 3.98253 |  0:00:15s
epoch 21 | loss: 0.60687 | eval_custom_logloss: 3.64373 |  0:00:16s
epoch 22 | loss: 0.6008  | eval_custom_logloss: 3.14936 |  0:00:17s
epoch 23 | loss: 0.64292 | eval_custom_logloss: 2.75786 |  0:00:18s
epoch 24 | loss: 0.59159 | eval_custom_logloss: 3.05978 |  0:00:18s
epoch 25 | loss: 0.60971 | eval_custom_logloss: 2.4234  |  0:00:19s
epoch 26 | loss: 0.57048 | eval_custom_logloss: 2.73125 |  0:00:20s
epoch 27 | loss: 0.5951  | eval_custom_logloss: 1.53083 |  0:00:21s
epoch 28 | loss: 0.61432 | eval_custom_logloss: 1.29412 |  0:00:21s
epoch 29 | loss: 0.60501 | eval_custom_logloss: 1.55763 |  0:00:22s
epoch 30 | loss: 0.58661 | eval_custom_logloss: 1.29484 |  0:00:23s
epoch 31 | loss: 0.61534 | eval_custom_logloss: 1.66687 |  0:00:24s
epoch 32 | loss: 0.5848  | eval_custom_logloss: 1.7865  |  0:00:24s
epoch 33 | loss: 0.57951 | eval_custom_logloss: 1.73857 |  0:00:25s
epoch 34 | loss: 0.60615 | eval_custom_logloss: 1.53912 |  0:00:26s
epoch 35 | loss: 0.57546 | eval_custom_logloss: 1.44703 |  0:00:27s
epoch 36 | loss: 0.55272 | eval_custom_logloss: 1.6918  |  0:00:27s
epoch 37 | loss: 0.56187 | eval_custom_logloss: 2.03301 |  0:00:28s
epoch 38 | loss: 0.54706 | eval_custom_logloss: 1.41691 |  0:00:29s
epoch 39 | loss: 0.55213 | eval_custom_logloss: 2.44423 |  0:00:30s
epoch 40 | loss: 0.53914 | eval_custom_logloss: 1.98138 |  0:00:30s
epoch 41 | loss: 0.56166 | eval_custom_logloss: 1.80415 |  0:00:31s
epoch 42 | loss: 0.60306 | eval_custom_logloss: 1.60042 |  0:00:32s
epoch 43 | loss: 0.5516  | eval_custom_logloss: 1.86359 |  0:00:32s
epoch 44 | loss: 0.58553 | eval_custom_logloss: 1.40271 |  0:00:33s
epoch 45 | loss: 0.62944 | eval_custom_logloss: 1.57521 |  0:00:34s
epoch 46 | loss: 0.59227 | eval_custom_logloss: 1.30584 |  0:00:35s
epoch 47 | loss: 0.58134 | eval_custom_logloss: 1.4136  |  0:00:35s
epoch 48 | loss: 0.55273 | eval_custom_logloss: 1.22453 |  0:00:36s
epoch 49 | loss: 0.55757 | eval_custom_logloss: 1.24303 |  0:00:37s
epoch 50 | loss: 0.5488  | eval_custom_logloss: 1.29008 |  0:00:38s
epoch 51 | loss: 0.55442 | eval_custom_logloss: 1.28025 |  0:00:38s
epoch 52 | loss: 0.55272 | eval_custom_logloss: 1.27754 |  0:00:39s
epoch 53 | loss: 0.53604 | eval_custom_logloss: 1.19061 |  0:00:40s
epoch 54 | loss: 0.53134 | eval_custom_logloss: 1.06777 |  0:00:41s
epoch 55 | loss: 0.50515 | eval_custom_logloss: 1.33697 |  0:00:41s
epoch 56 | loss: 0.51476 | eval_custom_logloss: 1.70522 |  0:00:42s
epoch 57 | loss: 0.51951 | eval_custom_logloss: 1.43083 |  0:00:43s
epoch 58 | loss: 0.49581 | eval_custom_logloss: 1.57541 |  0:00:44s
epoch 59 | loss: 0.48843 | eval_custom_logloss: 1.3847  |  0:00:44s
epoch 60 | loss: 0.47287 | eval_custom_logloss: 1.46264 |  0:00:45s
epoch 61 | loss: 0.48594 | eval_custom_logloss: 1.1859  |  0:00:46s
epoch 62 | loss: 0.47631 | eval_custom_logloss: 1.03923 |  0:00:46s
epoch 63 | loss: 0.48904 | eval_custom_logloss: 1.18069 |  0:00:47s
epoch 64 | loss: 0.48936 | eval_custom_logloss: 1.16717 |  0:00:48s
epoch 65 | loss: 0.47264 | eval_custom_logloss: 1.15    |  0:00:49s
epoch 66 | loss: 0.46798 | eval_custom_logloss: 1.03368 |  0:00:49s
epoch 67 | loss: 0.48905 | eval_custom_logloss: 1.06066 |  0:00:50s
epoch 68 | loss: 0.4574  | eval_custom_logloss: 1.11021 |  0:00:51s
epoch 69 | loss: 0.46092 | eval_custom_logloss: 0.96664 |  0:00:52s
epoch 70 | loss: 0.45381 | eval_custom_logloss: 0.84914 |  0:00:52s
epoch 71 | loss: 0.42982 | eval_custom_logloss: 0.84061 |  0:00:53s
epoch 72 | loss: 0.42793 | eval_custom_logloss: 0.73088 |  0:00:54s
epoch 73 | loss: 0.4501  | eval_custom_logloss: 0.99304 |  0:00:55s
epoch 74 | loss: 0.41795 | eval_custom_logloss: 0.98787 |  0:00:55s
epoch 75 | loss: 0.44479 | eval_custom_logloss: 0.9235  |  0:00:56s
epoch 76 | loss: 0.41505 | eval_custom_logloss: 0.92634 |  0:00:57s
epoch 77 | loss: 0.42908 | eval_custom_logloss: 0.92124 |  0:00:58s
epoch 78 | loss: 0.41808 | eval_custom_logloss: 1.0176  |  0:00:58s
epoch 79 | loss: 0.42316 | eval_custom_logloss: 0.88849 |  0:00:59s
epoch 80 | loss: 0.42892 | eval_custom_logloss: 0.91983 |  0:01:00s
epoch 81 | loss: 0.42544 | eval_custom_logloss: 0.84772 |  0:01:00s
epoch 82 | loss: 0.45756 | eval_custom_logloss: 0.98906 |  0:01:01s
epoch 83 | loss: 0.45259 | eval_custom_logloss: 0.87459 |  0:01:02s
epoch 84 | loss: 0.44068 | eval_custom_logloss: 0.90705 |  0:01:03s
epoch 85 | loss: 0.39505 | eval_custom_logloss: 0.88681 |  0:01:03s
epoch 86 | loss: 0.41038 | eval_custom_logloss: 1.08729 |  0:01:04s
epoch 87 | loss: 0.45613 | eval_custom_logloss: 0.73123 |  0:01:05s
epoch 88 | loss: 0.4295  | eval_custom_logloss: 1.24907 |  0:01:06s
epoch 89 | loss: 0.40514 | eval_custom_logloss: 1.00289 |  0:01:06s
epoch 90 | loss: 0.47191 | eval_custom_logloss: 1.04717 |  0:01:07s
epoch 91 | loss: 0.45748 | eval_custom_logloss: 0.98995 |  0:01:08s
epoch 92 | loss: 0.42352 | eval_custom_logloss: 1.18614 |  0:01:08s

Early stopping occurred at epoch 92 with best_epoch = 72 and best_eval_custom_logloss = 0.73088
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7432333333333333, 'Log Loss - std': 0.050402667477911216} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax', 'n_a': 56, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.18729 | eval_custom_logloss: 8.54591 |  0:00:00s
epoch 1  | loss: 1.60874 | eval_custom_logloss: 8.07593 |  0:00:01s
epoch 2  | loss: 1.65762 | eval_custom_logloss: 7.41094 |  0:00:02s
epoch 3  | loss: 1.61412 | eval_custom_logloss: 6.74883 |  0:00:03s
epoch 4  | loss: 1.28915 | eval_custom_logloss: 6.92798 |  0:00:03s
epoch 5  | loss: 2.92997 | eval_custom_logloss: 6.41468 |  0:00:04s
epoch 6  | loss: 1.4793  | eval_custom_logloss: 7.06287 |  0:00:05s
epoch 7  | loss: 1.16374 | eval_custom_logloss: 8.50117 |  0:00:06s
epoch 8  | loss: 0.91126 | eval_custom_logloss: 7.59988 |  0:00:06s
epoch 9  | loss: 1.42031 | eval_custom_logloss: 6.59241 |  0:00:07s
epoch 10 | loss: 0.93321 | eval_custom_logloss: 5.93369 |  0:00:08s
epoch 11 | loss: 1.1946  | eval_custom_logloss: 5.01282 |  0:00:09s
epoch 12 | loss: 0.9384  | eval_custom_logloss: 5.42777 |  0:00:09s
epoch 13 | loss: 0.84083 | eval_custom_logloss: 5.49671 |  0:00:10s
epoch 14 | loss: 0.76207 | eval_custom_logloss: 6.08845 |  0:00:11s
epoch 15 | loss: 0.70199 | eval_custom_logloss: 5.17359 |  0:00:11s
epoch 16 | loss: 0.67426 | eval_custom_logloss: 4.28443 |  0:00:12s
epoch 17 | loss: 0.67787 | eval_custom_logloss: 1.9833  |  0:00:13s
epoch 18 | loss: 0.67386 | eval_custom_logloss: 3.16195 |  0:00:14s
epoch 19 | loss: 0.68923 | eval_custom_logloss: 4.8457  |  0:00:15s
epoch 20 | loss: 0.66246 | eval_custom_logloss: 4.64185 |  0:00:15s
epoch 21 | loss: 0.62778 | eval_custom_logloss: 5.12319 |  0:00:16s
epoch 22 | loss: 0.59269 | eval_custom_logloss: 5.31264 |  0:00:17s
epoch 23 | loss: 0.6417  | eval_custom_logloss: 5.2453  |  0:00:17s
epoch 24 | loss: 0.57626 | eval_custom_logloss: 3.01514 |  0:00:18s
epoch 25 | loss: 0.60515 | eval_custom_logloss: 2.31243 |  0:00:19s
epoch 26 | loss: 0.57464 | eval_custom_logloss: 2.11341 |  0:00:20s
epoch 27 | loss: 0.55147 | eval_custom_logloss: 2.18414 |  0:00:20s
epoch 28 | loss: 0.56774 | eval_custom_logloss: 2.16401 |  0:00:21s
epoch 29 | loss: 0.5231  | eval_custom_logloss: 2.62552 |  0:00:22s
epoch 30 | loss: 0.50762 | eval_custom_logloss: 1.92975 |  0:00:23s
epoch 31 | loss: 0.50593 | eval_custom_logloss: 1.9965  |  0:00:23s
epoch 32 | loss: 0.49292 | eval_custom_logloss: 2.14856 |  0:00:24s
epoch 33 | loss: 0.49737 | eval_custom_logloss: 1.92506 |  0:00:25s
epoch 34 | loss: 0.52067 | eval_custom_logloss: 2.04559 |  0:00:26s
epoch 35 | loss: 0.51394 | eval_custom_logloss: 1.60054 |  0:00:26s
epoch 36 | loss: 0.50057 | eval_custom_logloss: 1.47118 |  0:00:27s
epoch 37 | loss: 0.5172  | eval_custom_logloss: 1.32937 |  0:00:28s
epoch 38 | loss: 0.49724 | eval_custom_logloss: 1.76216 |  0:00:29s
epoch 39 | loss: 0.46954 | eval_custom_logloss: 1.98816 |  0:00:29s
epoch 40 | loss: 0.49954 | eval_custom_logloss: 1.6993  |  0:00:30s
epoch 41 | loss: 0.44805 | eval_custom_logloss: 2.20689 |  0:00:31s
epoch 42 | loss: 0.42868 | eval_custom_logloss: 1.56531 |  0:00:32s
epoch 43 | loss: 0.45205 | eval_custom_logloss: 1.87698 |  0:00:32s
epoch 44 | loss: 0.48906 | eval_custom_logloss: 1.39196 |  0:00:33s
epoch 45 | loss: 0.47214 | eval_custom_logloss: 1.24633 |  0:00:34s
epoch 46 | loss: 0.44957 | eval_custom_logloss: 1.22411 |  0:00:35s
epoch 47 | loss: 0.45342 | eval_custom_logloss: 1.49838 |  0:00:35s
epoch 48 | loss: 0.44412 | eval_custom_logloss: 1.23944 |  0:00:36s
epoch 49 | loss: 0.4447  | eval_custom_logloss: 1.26746 |  0:00:37s
epoch 50 | loss: 0.45686 | eval_custom_logloss: 1.6758  |  0:00:37s
epoch 51 | loss: 0.5156  | eval_custom_logloss: 1.39297 |  0:00:38s
epoch 52 | loss: 0.49636 | eval_custom_logloss: 1.4071  |  0:00:39s
epoch 53 | loss: 0.43583 | eval_custom_logloss: 1.55573 |  0:00:40s
epoch 54 | loss: 0.45608 | eval_custom_logloss: 1.67365 |  0:00:40s
epoch 55 | loss: 0.42478 | eval_custom_logloss: 1.39403 |  0:00:41s
epoch 56 | loss: 0.42629 | eval_custom_logloss: 1.54785 |  0:00:42s
epoch 57 | loss: 0.43071 | eval_custom_logloss: 1.28325 |  0:00:43s
epoch 58 | loss: 0.4228  | eval_custom_logloss: 1.66361 |  0:00:43s
epoch 59 | loss: 0.43596 | eval_custom_logloss: 1.56872 |  0:00:44s
epoch 60 | loss: 0.44315 | eval_custom_logloss: 1.4568  |  0:00:45s
epoch 61 | loss: 0.41637 | eval_custom_logloss: 1.67841 |  0:00:46s
epoch 62 | loss: 0.41243 | eval_custom_logloss: 1.51847 |  0:00:46s
epoch 63 | loss: 0.39312 | eval_custom_logloss: 1.34135 |  0:00:47s
epoch 64 | loss: 0.3706  | eval_custom_logloss: 1.40961 |  0:00:48s
epoch 65 | loss: 0.36535 | eval_custom_logloss: 1.45711 |  0:00:48s
epoch 66 | loss: 0.3727  | eval_custom_logloss: 1.52407 |  0:00:49s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 1.22411
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.850575, 'Log Loss - std': 0.19097649560875288} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax', 'n_a': 56, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.3785  | eval_custom_logloss: 8.42132 |  0:00:00s
epoch 1  | loss: 2.61185 | eval_custom_logloss: 7.83296 |  0:00:01s
epoch 2  | loss: 1.5847  | eval_custom_logloss: 7.84186 |  0:00:02s
epoch 3  | loss: 2.19727 | eval_custom_logloss: 8.88778 |  0:00:02s
epoch 4  | loss: 4.84678 | eval_custom_logloss: 10.05621|  0:00:03s
epoch 5  | loss: 1.72986 | eval_custom_logloss: 6.04692 |  0:00:04s
epoch 6  | loss: 1.40546 | eval_custom_logloss: 8.8478  |  0:00:05s
epoch 7  | loss: 1.24942 | eval_custom_logloss: 7.41538 |  0:00:05s
epoch 8  | loss: 1.75486 | eval_custom_logloss: 7.87142 |  0:00:06s
epoch 9  | loss: 1.08145 | eval_custom_logloss: 8.39433 |  0:00:07s
epoch 10 | loss: 1.67279 | eval_custom_logloss: 6.90188 |  0:00:07s
epoch 11 | loss: 1.1621  | eval_custom_logloss: 3.50429 |  0:00:08s
epoch 12 | loss: 0.98041 | eval_custom_logloss: 3.82687 |  0:00:09s
epoch 13 | loss: 0.83437 | eval_custom_logloss: 3.40152 |  0:00:10s
epoch 14 | loss: 0.77734 | eval_custom_logloss: 3.75643 |  0:00:11s
epoch 15 | loss: 0.76107 | eval_custom_logloss: 2.12051 |  0:00:11s
epoch 16 | loss: 0.72161 | eval_custom_logloss: 3.01319 |  0:00:12s
epoch 17 | loss: 0.73112 | eval_custom_logloss: 2.60186 |  0:00:13s
epoch 18 | loss: 0.72103 | eval_custom_logloss: 1.98909 |  0:00:14s
epoch 19 | loss: 0.75044 | eval_custom_logloss: 2.57196 |  0:00:14s
epoch 20 | loss: 0.70936 | eval_custom_logloss: 3.04872 |  0:00:15s
epoch 21 | loss: 0.73416 | eval_custom_logloss: 3.54038 |  0:00:16s
epoch 22 | loss: 0.69257 | eval_custom_logloss: 2.78412 |  0:00:17s
epoch 23 | loss: 0.72323 | eval_custom_logloss: 2.57545 |  0:00:17s
epoch 24 | loss: 0.74306 | eval_custom_logloss: 1.8929  |  0:00:18s
epoch 25 | loss: 0.71132 | eval_custom_logloss: 1.48253 |  0:00:19s
epoch 26 | loss: 0.73489 | eval_custom_logloss: 2.99614 |  0:00:20s
epoch 27 | loss: 0.7105  | eval_custom_logloss: 2.36296 |  0:00:20s
epoch 28 | loss: 0.71396 | eval_custom_logloss: 2.86897 |  0:00:21s
epoch 29 | loss: 0.72424 | eval_custom_logloss: 2.30266 |  0:00:22s
epoch 30 | loss: 0.64441 | eval_custom_logloss: 1.4278  |  0:00:23s
epoch 31 | loss: 0.63308 | eval_custom_logloss: 2.25114 |  0:00:23s
epoch 32 | loss: 0.60997 | eval_custom_logloss: 2.29057 |  0:00:24s
epoch 33 | loss: 0.64379 | eval_custom_logloss: 1.78494 |  0:00:25s
epoch 34 | loss: 0.64162 | eval_custom_logloss: 2.80771 |  0:00:26s
epoch 35 | loss: 0.61592 | eval_custom_logloss: 2.99144 |  0:00:26s
epoch 36 | loss: 0.66314 | eval_custom_logloss: 2.908   |  0:00:27s
epoch 37 | loss: 0.67274 | eval_custom_logloss: 1.98897 |  0:00:28s
epoch 38 | loss: 0.65914 | eval_custom_logloss: 1.49082 |  0:00:29s
epoch 39 | loss: 0.64016 | eval_custom_logloss: 1.62363 |  0:00:29s
epoch 40 | loss: 0.60714 | eval_custom_logloss: 1.81574 |  0:00:30s
epoch 41 | loss: 0.60792 | eval_custom_logloss: 1.92867 |  0:00:31s
epoch 42 | loss: 0.63135 | eval_custom_logloss: 1.80976 |  0:00:31s
epoch 43 | loss: 0.58994 | eval_custom_logloss: 1.65927 |  0:00:32s
epoch 44 | loss: 0.5964  | eval_custom_logloss: 1.27011 |  0:00:33s
epoch 45 | loss: 0.58869 | eval_custom_logloss: 2.19042 |  0:00:34s
epoch 46 | loss: 0.60933 | eval_custom_logloss: 1.64116 |  0:00:34s
epoch 47 | loss: 0.59177 | eval_custom_logloss: 1.86021 |  0:00:35s
epoch 48 | loss: 0.59876 | eval_custom_logloss: 1.58837 |  0:00:36s
epoch 49 | loss: 0.57672 | eval_custom_logloss: 1.53232 |  0:00:37s
epoch 50 | loss: 0.60349 | eval_custom_logloss: 1.81569 |  0:00:37s
epoch 51 | loss: 0.56942 | eval_custom_logloss: 1.4271  |  0:00:38s
epoch 52 | loss: 0.5885  | eval_custom_logloss: 2.30368 |  0:00:39s
epoch 53 | loss: 0.57731 | eval_custom_logloss: 1.15843 |  0:00:40s
epoch 54 | loss: 0.5774  | eval_custom_logloss: 1.79636 |  0:00:40s
epoch 55 | loss: 0.55713 | eval_custom_logloss: 1.18207 |  0:00:41s
epoch 56 | loss: 0.61684 | eval_custom_logloss: 1.57634 |  0:00:42s
epoch 57 | loss: 0.61586 | eval_custom_logloss: 1.27778 |  0:00:43s
epoch 58 | loss: 0.59785 | eval_custom_logloss: 1.49393 |  0:00:43s
epoch 59 | loss: 0.55901 | eval_custom_logloss: 2.14212 |  0:00:44s
epoch 60 | loss: 0.57386 | eval_custom_logloss: 1.71453 |  0:00:45s
epoch 61 | loss: 0.56944 | eval_custom_logloss: 1.6629  |  0:00:45s
epoch 62 | loss: 0.55293 | eval_custom_logloss: 1.7555  |  0:00:46s
epoch 63 | loss: 0.60737 | eval_custom_logloss: 1.78039 |  0:00:47s
epoch 64 | loss: 0.56116 | eval_custom_logloss: 3.25904 |  0:00:48s
epoch 65 | loss: 0.52812 | eval_custom_logloss: 3.35707 |  0:00:48s
epoch 66 | loss: 0.52395 | eval_custom_logloss: 3.3585  |  0:00:49s
epoch 67 | loss: 0.54177 | eval_custom_logloss: 3.82059 |  0:00:50s
epoch 68 | loss: 0.50865 | eval_custom_logloss: 4.15939 |  0:00:51s
epoch 69 | loss: 0.55929 | eval_custom_logloss: 4.08861 |  0:00:51s
epoch 70 | loss: 0.57211 | eval_custom_logloss: 2.94553 |  0:00:52s
epoch 71 | loss: 0.53808 | eval_custom_logloss: 2.26772 |  0:00:53s
epoch 72 | loss: 0.54535 | eval_custom_logloss: 1.98421 |  0:00:54s
epoch 73 | loss: 0.55063 | eval_custom_logloss: 2.06555 |  0:00:54s

Early stopping occurred at epoch 73 with best_epoch = 53 and best_eval_custom_logloss = 1.15843
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9112199999999999, 'Log Loss - std': 0.20949673410342223} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 0.9112199999999999 and parameters: {'n_d': 56, 'n_steps': 9, 'gamma': 1.826862921700155, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.0022836334408777286, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax', 'n_a': 48, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.07893 | eval_custom_logloss: 8.35932 |  0:00:00s
epoch 1  | loss: 1.79516 | eval_custom_logloss: 8.80322 |  0:00:01s
epoch 2  | loss: 1.70556 | eval_custom_logloss: 8.82397 |  0:00:02s
epoch 3  | loss: 2.37822 | eval_custom_logloss: 8.45855 |  0:00:03s
epoch 4  | loss: 2.25754 | eval_custom_logloss: 9.38074 |  0:00:04s
epoch 5  | loss: 1.24122 | eval_custom_logloss: 9.00631 |  0:00:05s
epoch 6  | loss: 1.10875 | eval_custom_logloss: 6.6205  |  0:00:06s
epoch 7  | loss: 1.28608 | eval_custom_logloss: 6.96648 |  0:00:07s
epoch 8  | loss: 1.1967  | eval_custom_logloss: 6.01949 |  0:00:08s
epoch 9  | loss: 0.98723 | eval_custom_logloss: 7.10697 |  0:00:09s
epoch 10 | loss: 0.96907 | eval_custom_logloss: 8.29786 |  0:00:10s
epoch 11 | loss: 1.07042 | eval_custom_logloss: 7.46112 |  0:00:10s
epoch 12 | loss: 0.79313 | eval_custom_logloss: 5.77214 |  0:00:11s
epoch 13 | loss: 0.85641 | eval_custom_logloss: 5.09986 |  0:00:12s
epoch 14 | loss: 0.78896 | eval_custom_logloss: 4.65667 |  0:00:13s
epoch 15 | loss: 0.78627 | eval_custom_logloss: 4.26517 |  0:00:14s
epoch 16 | loss: 0.76011 | eval_custom_logloss: 3.75362 |  0:00:15s
epoch 17 | loss: 0.78812 | eval_custom_logloss: 4.18797 |  0:00:16s
epoch 18 | loss: 0.76084 | eval_custom_logloss: 4.97713 |  0:00:17s
epoch 19 | loss: 0.77175 | eval_custom_logloss: 5.53338 |  0:00:18s
epoch 20 | loss: 0.74052 | eval_custom_logloss: 4.55797 |  0:00:19s
epoch 21 | loss: 0.74399 | eval_custom_logloss: 3.01548 |  0:00:20s
epoch 22 | loss: 0.73979 | eval_custom_logloss: 2.32663 |  0:00:21s
epoch 23 | loss: 0.72752 | eval_custom_logloss: 5.11626 |  0:00:21s
epoch 24 | loss: 0.68531 | eval_custom_logloss: 3.19347 |  0:00:22s
epoch 25 | loss: 0.67587 | eval_custom_logloss: 3.80558 |  0:00:23s
epoch 26 | loss: 0.63323 | eval_custom_logloss: 4.49292 |  0:00:24s
epoch 27 | loss: 0.6286  | eval_custom_logloss: 4.46796 |  0:00:25s
epoch 28 | loss: 0.58811 | eval_custom_logloss: 2.85705 |  0:00:26s
epoch 29 | loss: 0.60023 | eval_custom_logloss: 3.20575 |  0:00:27s
epoch 30 | loss: 0.59604 | eval_custom_logloss: 3.78922 |  0:00:28s
epoch 31 | loss: 0.58228 | eval_custom_logloss: 2.35421 |  0:00:29s
epoch 32 | loss: 0.56875 | eval_custom_logloss: 2.51783 |  0:00:30s
epoch 33 | loss: 0.55976 | eval_custom_logloss: 2.83045 |  0:00:31s
epoch 34 | loss: 0.55417 | eval_custom_logloss: 2.31309 |  0:00:31s
epoch 35 | loss: 0.56092 | eval_custom_logloss: 2.27663 |  0:00:32s
epoch 36 | loss: 0.56506 | eval_custom_logloss: 1.82352 |  0:00:33s
epoch 37 | loss: 0.576   | eval_custom_logloss: 2.26851 |  0:00:34s
epoch 38 | loss: 0.62964 | eval_custom_logloss: 2.47117 |  0:00:35s
epoch 39 | loss: 0.60455 | eval_custom_logloss: 2.14995 |  0:00:36s
epoch 40 | loss: 0.62697 | eval_custom_logloss: 2.27082 |  0:00:37s
epoch 41 | loss: 0.60399 | eval_custom_logloss: 2.73527 |  0:00:38s
epoch 42 | loss: 0.61493 | eval_custom_logloss: 2.81866 |  0:00:39s
epoch 43 | loss: 0.61807 | eval_custom_logloss: 2.19283 |  0:00:40s
epoch 44 | loss: 0.6084  | eval_custom_logloss: 2.23351 |  0:00:40s
epoch 45 | loss: 0.62551 | eval_custom_logloss: 2.64441 |  0:00:41s
epoch 46 | loss: 0.5999  | eval_custom_logloss: 1.86963 |  0:00:42s
epoch 47 | loss: 0.58716 | eval_custom_logloss: 1.69255 |  0:00:43s
epoch 48 | loss: 0.58065 | eval_custom_logloss: 2.36417 |  0:00:44s
epoch 49 | loss: 0.61702 | eval_custom_logloss: 2.75842 |  0:00:45s
epoch 50 | loss: 0.60637 | eval_custom_logloss: 3.16414 |  0:00:46s
epoch 51 | loss: 0.57775 | eval_custom_logloss: 3.03453 |  0:00:47s
epoch 52 | loss: 0.5841  | eval_custom_logloss: 2.90086 |  0:00:48s
epoch 53 | loss: 0.56795 | eval_custom_logloss: 2.61093 |  0:00:49s
epoch 54 | loss: 0.57061 | eval_custom_logloss: 2.42051 |  0:00:50s
epoch 55 | loss: 0.59886 | eval_custom_logloss: 2.29673 |  0:00:50s
epoch 56 | loss: 0.57536 | eval_custom_logloss: 2.26177 |  0:00:51s
epoch 57 | loss: 0.58186 | eval_custom_logloss: 2.15291 |  0:00:52s
epoch 58 | loss: 0.56523 | eval_custom_logloss: 1.83747 |  0:00:53s
epoch 59 | loss: 0.58707 | eval_custom_logloss: 1.79571 |  0:00:54s
epoch 60 | loss: 0.55624 | eval_custom_logloss: 1.46642 |  0:00:55s
epoch 61 | loss: 0.57203 | eval_custom_logloss: 1.83205 |  0:00:56s
epoch 62 | loss: 0.58583 | eval_custom_logloss: 1.51716 |  0:00:57s
epoch 63 | loss: 0.58689 | eval_custom_logloss: 1.08068 |  0:00:58s
epoch 64 | loss: 0.56036 | eval_custom_logloss: 1.16981 |  0:00:59s
epoch 65 | loss: 0.54038 | eval_custom_logloss: 1.16901 |  0:00:59s
epoch 66 | loss: 0.53043 | eval_custom_logloss: 1.65676 |  0:01:00s
epoch 67 | loss: 0.50759 | eval_custom_logloss: 2.08885 |  0:01:01s
epoch 68 | loss: 0.54315 | eval_custom_logloss: 1.60561 |  0:01:02s
epoch 69 | loss: 0.54085 | eval_custom_logloss: 1.25806 |  0:01:03s
epoch 70 | loss: 0.54014 | eval_custom_logloss: 1.38418 |  0:01:04s
epoch 71 | loss: 0.52013 | eval_custom_logloss: 1.1124  |  0:01:05s
epoch 72 | loss: 0.5265  | eval_custom_logloss: 1.12675 |  0:01:06s
epoch 73 | loss: 0.48387 | eval_custom_logloss: 1.16083 |  0:01:07s
epoch 74 | loss: 0.4836  | eval_custom_logloss: 1.16851 |  0:01:08s
epoch 75 | loss: 0.48056 | eval_custom_logloss: 0.94504 |  0:01:08s
epoch 76 | loss: 0.52369 | eval_custom_logloss: 1.15006 |  0:01:09s
epoch 77 | loss: 0.51533 | eval_custom_logloss: 1.0941  |  0:01:10s
epoch 78 | loss: 0.49968 | eval_custom_logloss: 1.32488 |  0:01:11s
epoch 79 | loss: 0.47763 | eval_custom_logloss: 0.79868 |  0:01:12s
epoch 80 | loss: 0.52179 | eval_custom_logloss: 1.14633 |  0:01:13s
epoch 81 | loss: 0.50864 | eval_custom_logloss: 0.7332  |  0:01:14s
epoch 82 | loss: 0.48683 | eval_custom_logloss: 0.96916 |  0:01:15s
epoch 83 | loss: 0.46196 | eval_custom_logloss: 1.01197 |  0:01:16s
epoch 84 | loss: 0.45608 | eval_custom_logloss: 1.17365 |  0:01:17s
epoch 85 | loss: 0.43743 | eval_custom_logloss: 1.28172 |  0:01:18s
epoch 86 | loss: 0.43635 | eval_custom_logloss: 1.15021 |  0:01:18s
epoch 87 | loss: 0.44094 | eval_custom_logloss: 1.12058 |  0:01:19s
epoch 88 | loss: 0.4795  | eval_custom_logloss: 0.90611 |  0:01:20s
epoch 89 | loss: 0.49606 | eval_custom_logloss: 1.30676 |  0:01:21s
epoch 90 | loss: 0.47964 | eval_custom_logloss: 0.94288 |  0:01:22s
epoch 91 | loss: 0.46776 | eval_custom_logloss: 1.19128 |  0:01:23s
epoch 92 | loss: 0.42466 | eval_custom_logloss: 1.29261 |  0:01:24s
epoch 93 | loss: 0.42543 | eval_custom_logloss: 1.04261 |  0:01:25s
epoch 94 | loss: 0.40293 | eval_custom_logloss: 0.98684 |  0:01:26s
epoch 95 | loss: 0.39069 | eval_custom_logloss: 1.46205 |  0:01:27s
epoch 96 | loss: 0.39934 | eval_custom_logloss: 1.38352 |  0:01:27s
epoch 97 | loss: 0.41858 | eval_custom_logloss: 1.1397  |  0:01:28s
epoch 98 | loss: 0.44773 | eval_custom_logloss: 1.64496 |  0:01:29s
epoch 99 | loss: 0.46354 | eval_custom_logloss: 1.29749 |  0:01:30s
Stop training because you reached max_epochs = 100 with best_epoch = 81 and best_eval_custom_logloss = 0.7332
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7332, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax', 'n_a': 48, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.9087  | eval_custom_logloss: 9.69071 |  0:00:00s
epoch 1  | loss: 1.88324 | eval_custom_logloss: 8.71551 |  0:00:01s
epoch 2  | loss: 1.42806 | eval_custom_logloss: 7.68968 |  0:00:02s
epoch 3  | loss: 1.51724 | eval_custom_logloss: 9.13832 |  0:00:03s
epoch 4  | loss: 1.67335 | eval_custom_logloss: 10.04173|  0:00:04s
epoch 5  | loss: 1.69994 | eval_custom_logloss: 7.17314 |  0:00:05s
epoch 6  | loss: 1.36774 | eval_custom_logloss: 7.89042 |  0:00:06s
epoch 7  | loss: 2.23318 | eval_custom_logloss: 9.22083 |  0:00:07s
epoch 8  | loss: 1.29125 | eval_custom_logloss: 7.50817 |  0:00:08s
epoch 9  | loss: 0.98029 | eval_custom_logloss: 8.16874 |  0:00:09s
epoch 10 | loss: 0.92909 | eval_custom_logloss: 8.43678 |  0:00:10s
epoch 11 | loss: 0.85273 | eval_custom_logloss: 7.05136 |  0:00:11s
epoch 12 | loss: 0.7946  | eval_custom_logloss: 7.38529 |  0:00:11s
epoch 13 | loss: 0.794   | eval_custom_logloss: 7.79175 |  0:00:12s
epoch 14 | loss: 0.72311 | eval_custom_logloss: 4.46868 |  0:00:13s
epoch 15 | loss: 0.69817 | eval_custom_logloss: 5.38526 |  0:00:14s
epoch 16 | loss: 0.7837  | eval_custom_logloss: 6.96903 |  0:00:15s
epoch 17 | loss: 0.7238  | eval_custom_logloss: 7.64772 |  0:00:16s
epoch 18 | loss: 0.71702 | eval_custom_logloss: 5.82155 |  0:00:17s
epoch 19 | loss: 0.66114 | eval_custom_logloss: 5.55934 |  0:00:18s
epoch 20 | loss: 0.62794 | eval_custom_logloss: 6.66459 |  0:00:19s
epoch 21 | loss: 0.64607 | eval_custom_logloss: 5.79479 |  0:00:20s
epoch 22 | loss: 0.67967 | eval_custom_logloss: 4.36211 |  0:00:21s
epoch 23 | loss: 0.61102 | eval_custom_logloss: 5.18938 |  0:00:22s
epoch 24 | loss: 0.55787 | eval_custom_logloss: 3.42446 |  0:00:23s
epoch 25 | loss: 0.57706 | eval_custom_logloss: 3.05005 |  0:00:23s
epoch 26 | loss: 0.56404 | eval_custom_logloss: 2.82317 |  0:00:24s
epoch 27 | loss: 0.56119 | eval_custom_logloss: 2.66686 |  0:00:25s
epoch 28 | loss: 0.58964 | eval_custom_logloss: 2.67254 |  0:00:26s
epoch 29 | loss: 0.58674 | eval_custom_logloss: 2.11707 |  0:00:27s
epoch 30 | loss: 0.56303 | eval_custom_logloss: 3.03149 |  0:00:28s
epoch 31 | loss: 0.56538 | eval_custom_logloss: 2.1983  |  0:00:29s
epoch 32 | loss: 0.55685 | eval_custom_logloss: 1.93384 |  0:00:30s
epoch 33 | loss: 0.55444 | eval_custom_logloss: 1.95329 |  0:00:31s
epoch 34 | loss: 0.57522 | eval_custom_logloss: 2.7939  |  0:00:32s
epoch 35 | loss: 0.58952 | eval_custom_logloss: 3.06103 |  0:00:33s
epoch 36 | loss: 0.57159 | eval_custom_logloss: 2.84579 |  0:00:34s
epoch 37 | loss: 0.54552 | eval_custom_logloss: 3.23587 |  0:00:34s
epoch 38 | loss: 0.53194 | eval_custom_logloss: 2.15222 |  0:00:35s
epoch 39 | loss: 0.54114 | eval_custom_logloss: 2.69025 |  0:00:36s
epoch 40 | loss: 0.62397 | eval_custom_logloss: 1.59566 |  0:00:37s
epoch 41 | loss: 0.59943 | eval_custom_logloss: 2.70791 |  0:00:38s
epoch 42 | loss: 0.56355 | eval_custom_logloss: 2.53515 |  0:00:39s
epoch 43 | loss: 0.56628 | eval_custom_logloss: 2.44127 |  0:00:40s
epoch 44 | loss: 0.5921  | eval_custom_logloss: 1.86582 |  0:00:41s
epoch 45 | loss: 0.54403 | eval_custom_logloss: 1.51738 |  0:00:42s
epoch 46 | loss: 0.56189 | eval_custom_logloss: 1.6906  |  0:00:43s
epoch 47 | loss: 0.54802 | eval_custom_logloss: 1.57985 |  0:00:44s
epoch 48 | loss: 0.51793 | eval_custom_logloss: 1.55057 |  0:00:45s
epoch 49 | loss: 0.50113 | eval_custom_logloss: 1.71144 |  0:00:45s
epoch 50 | loss: 0.53132 | eval_custom_logloss: 1.07953 |  0:00:46s
epoch 51 | loss: 0.55172 | eval_custom_logloss: 0.9786  |  0:00:47s
epoch 52 | loss: 0.66806 | eval_custom_logloss: 1.41504 |  0:00:48s
epoch 53 | loss: 0.58613 | eval_custom_logloss: 1.31041 |  0:00:49s
epoch 54 | loss: 0.5806  | eval_custom_logloss: 1.08427 |  0:00:50s
epoch 55 | loss: 0.58456 | eval_custom_logloss: 1.06783 |  0:00:51s
epoch 56 | loss: 0.55073 | eval_custom_logloss: 1.1769  |  0:00:52s
epoch 57 | loss: 0.54908 | eval_custom_logloss: 1.15861 |  0:00:53s
epoch 58 | loss: 0.57497 | eval_custom_logloss: 1.33178 |  0:00:54s
epoch 59 | loss: 0.58061 | eval_custom_logloss: 1.32636 |  0:00:55s
epoch 60 | loss: 0.56919 | eval_custom_logloss: 1.26534 |  0:00:56s
epoch 61 | loss: 0.58392 | eval_custom_logloss: 1.19661 |  0:00:56s
epoch 62 | loss: 0.57811 | eval_custom_logloss: 1.16651 |  0:00:58s
epoch 63 | loss: 0.64181 | eval_custom_logloss: 1.69407 |  0:00:58s
epoch 64 | loss: 0.64249 | eval_custom_logloss: 2.58365 |  0:00:59s
epoch 65 | loss: 0.59453 | eval_custom_logloss: 2.50043 |  0:01:00s
epoch 66 | loss: 0.7007  | eval_custom_logloss: 1.99869 |  0:01:01s
epoch 67 | loss: 0.61912 | eval_custom_logloss: 2.07323 |  0:01:02s
epoch 68 | loss: 0.57925 | eval_custom_logloss: 2.92465 |  0:01:03s
epoch 69 | loss: 0.571   | eval_custom_logloss: 3.43578 |  0:01:04s
epoch 70 | loss: 0.61036 | eval_custom_logloss: 2.58414 |  0:01:05s
epoch 71 | loss: 0.63814 | eval_custom_logloss: 2.88606 |  0:01:06s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 0.9786
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8559, 'Log Loss - std': 0.12270000000000003} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax', 'n_a': 48, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.91795 | eval_custom_logloss: 7.92374 |  0:00:01s
epoch 1  | loss: 1.76159 | eval_custom_logloss: 8.37975 |  0:00:02s
epoch 2  | loss: 1.91019 | eval_custom_logloss: 8.61536 |  0:00:03s
epoch 3  | loss: 1.28055 | eval_custom_logloss: 6.64563 |  0:00:04s
epoch 4  | loss: 1.7047  | eval_custom_logloss: 7.2701  |  0:00:05s
epoch 5  | loss: 1.50218 | eval_custom_logloss: 6.73757 |  0:00:06s
epoch 6  | loss: 1.60624 | eval_custom_logloss: 7.56739 |  0:00:06s
epoch 7  | loss: 1.36059 | eval_custom_logloss: 9.44327 |  0:00:07s
epoch 8  | loss: 1.30841 | eval_custom_logloss: 7.45446 |  0:00:08s
epoch 9  | loss: 1.20859 | eval_custom_logloss: 8.21566 |  0:00:09s
epoch 10 | loss: 1.13307 | eval_custom_logloss: 7.33374 |  0:00:10s
epoch 11 | loss: 1.24162 | eval_custom_logloss: 4.63721 |  0:00:11s
epoch 12 | loss: 0.80272 | eval_custom_logloss: 5.15061 |  0:00:12s
epoch 13 | loss: 0.74979 | eval_custom_logloss: 6.34078 |  0:00:13s
epoch 14 | loss: 0.8099  | eval_custom_logloss: 5.41597 |  0:00:14s
epoch 15 | loss: 0.74453 | eval_custom_logloss: 5.64393 |  0:00:14s
epoch 16 | loss: 0.72569 | eval_custom_logloss: 9.39171 |  0:00:15s
epoch 17 | loss: 0.75032 | eval_custom_logloss: 6.50342 |  0:00:16s
epoch 18 | loss: 0.71657 | eval_custom_logloss: 6.61069 |  0:00:17s
epoch 19 | loss: 0.70275 | eval_custom_logloss: 5.87575 |  0:00:18s
epoch 20 | loss: 0.66338 | eval_custom_logloss: 4.94453 |  0:00:19s
epoch 21 | loss: 0.66212 | eval_custom_logloss: 4.02796 |  0:00:20s
epoch 22 | loss: 0.63583 | eval_custom_logloss: 3.76803 |  0:00:21s
epoch 23 | loss: 0.6249  | eval_custom_logloss: 3.41712 |  0:00:21s
epoch 24 | loss: 0.62213 | eval_custom_logloss: 2.00464 |  0:00:22s
epoch 25 | loss: 0.5774  | eval_custom_logloss: 2.94543 |  0:00:23s
epoch 26 | loss: 0.5869  | eval_custom_logloss: 2.11027 |  0:00:24s
epoch 27 | loss: 0.6225  | eval_custom_logloss: 2.46848 |  0:00:25s
epoch 28 | loss: 0.62233 | eval_custom_logloss: 2.2474  |  0:00:26s
epoch 29 | loss: 0.60749 | eval_custom_logloss: 2.89236 |  0:00:27s
epoch 30 | loss: 0.59651 | eval_custom_logloss: 2.76859 |  0:00:28s
epoch 31 | loss: 0.58951 | eval_custom_logloss: 2.16312 |  0:00:29s
epoch 32 | loss: 0.58621 | eval_custom_logloss: 1.41736 |  0:00:30s
epoch 33 | loss: 0.57998 | eval_custom_logloss: 1.35354 |  0:00:30s
epoch 34 | loss: 0.55632 | eval_custom_logloss: 1.59537 |  0:00:31s
epoch 35 | loss: 0.56873 | eval_custom_logloss: 1.36249 |  0:00:32s
epoch 36 | loss: 0.55981 | eval_custom_logloss: 1.36807 |  0:00:33s
epoch 37 | loss: 0.56398 | eval_custom_logloss: 1.55807 |  0:00:34s
epoch 38 | loss: 0.54885 | eval_custom_logloss: 2.25703 |  0:00:35s
epoch 39 | loss: 0.54408 | eval_custom_logloss: 2.60296 |  0:00:36s
epoch 40 | loss: 0.516   | eval_custom_logloss: 2.48228 |  0:00:37s
epoch 41 | loss: 0.49569 | eval_custom_logloss: 2.34299 |  0:00:37s
epoch 42 | loss: 0.52434 | eval_custom_logloss: 1.90937 |  0:00:38s
epoch 43 | loss: 0.54347 | eval_custom_logloss: 1.88297 |  0:00:39s
epoch 44 | loss: 0.55532 | eval_custom_logloss: 1.73563 |  0:00:40s
epoch 45 | loss: 0.58639 | eval_custom_logloss: 1.57159 |  0:00:41s
epoch 46 | loss: 0.58078 | eval_custom_logloss: 1.28824 |  0:00:42s
epoch 47 | loss: 0.554   | eval_custom_logloss: 1.64004 |  0:00:43s
epoch 48 | loss: 0.55635 | eval_custom_logloss: 1.61873 |  0:00:44s
epoch 49 | loss: 0.51229 | eval_custom_logloss: 1.50175 |  0:00:44s
epoch 50 | loss: 0.52873 | eval_custom_logloss: 2.3221  |  0:00:45s
epoch 51 | loss: 0.56486 | eval_custom_logloss: 2.32905 |  0:00:46s
epoch 52 | loss: 0.56402 | eval_custom_logloss: 1.51812 |  0:00:47s
epoch 53 | loss: 0.54111 | eval_custom_logloss: 1.52971 |  0:00:48s
epoch 54 | loss: 0.57173 | eval_custom_logloss: 1.56462 |  0:00:49s
epoch 55 | loss: 0.54763 | eval_custom_logloss: 2.47041 |  0:00:50s
epoch 56 | loss: 0.54865 | eval_custom_logloss: 1.36999 |  0:00:51s
epoch 57 | loss: 0.51553 | eval_custom_logloss: 1.92359 |  0:00:51s
epoch 58 | loss: 0.51895 | eval_custom_logloss: 2.17056 |  0:00:52s
epoch 59 | loss: 0.52116 | eval_custom_logloss: 2.17314 |  0:00:53s
epoch 60 | loss: 0.51415 | eval_custom_logloss: 2.01552 |  0:00:54s
epoch 61 | loss: 0.47274 | eval_custom_logloss: 2.18306 |  0:00:55s
epoch 62 | loss: 0.4595  | eval_custom_logloss: 1.30434 |  0:00:56s
epoch 63 | loss: 0.47807 | eval_custom_logloss: 1.12685 |  0:00:57s
epoch 64 | loss: 0.47297 | eval_custom_logloss: 1.39126 |  0:00:58s
epoch 65 | loss: 0.50757 | eval_custom_logloss: 1.34294 |  0:00:58s
epoch 66 | loss: 0.46589 | eval_custom_logloss: 1.52123 |  0:00:59s
epoch 67 | loss: 0.45974 | eval_custom_logloss: 1.33058 |  0:01:00s
epoch 68 | loss: 0.45129 | eval_custom_logloss: 2.36305 |  0:01:01s
epoch 69 | loss: 0.48252 | eval_custom_logloss: 2.42641 |  0:01:02s
epoch 70 | loss: 0.51503 | eval_custom_logloss: 1.2016  |  0:01:03s
epoch 71 | loss: 0.46632 | eval_custom_logloss: 2.026   |  0:01:04s
epoch 72 | loss: 0.46208 | eval_custom_logloss: 0.87188 |  0:01:05s
epoch 73 | loss: 0.45196 | eval_custom_logloss: 1.08422 |  0:01:05s
epoch 74 | loss: 0.48872 | eval_custom_logloss: 0.67998 |  0:01:06s
epoch 75 | loss: 0.44936 | eval_custom_logloss: 0.77697 |  0:01:07s
epoch 76 | loss: 0.45257 | eval_custom_logloss: 1.33575 |  0:01:08s
epoch 77 | loss: 0.46341 | eval_custom_logloss: 1.13914 |  0:01:09s
epoch 78 | loss: 0.48462 | eval_custom_logloss: 0.8194  |  0:01:10s
epoch 79 | loss: 0.47977 | eval_custom_logloss: 1.22505 |  0:01:11s
epoch 80 | loss: 0.48231 | eval_custom_logloss: 1.35764 |  0:01:12s
epoch 81 | loss: 0.44642 | eval_custom_logloss: 0.96307 |  0:01:13s
epoch 82 | loss: 0.48242 | eval_custom_logloss: 1.45115 |  0:01:13s
epoch 83 | loss: 0.51466 | eval_custom_logloss: 1.15854 |  0:01:14s
epoch 84 | loss: 0.47746 | eval_custom_logloss: 1.25805 |  0:01:15s
epoch 85 | loss: 0.45597 | eval_custom_logloss: 1.44137 |  0:01:16s
epoch 86 | loss: 0.44338 | eval_custom_logloss: 1.1424  |  0:01:17s
epoch 87 | loss: 0.44872 | eval_custom_logloss: 1.54116 |  0:01:18s
epoch 88 | loss: 0.48046 | eval_custom_logloss: 1.04101 |  0:01:19s
epoch 89 | loss: 0.43221 | eval_custom_logloss: 1.248   |  0:01:20s
epoch 90 | loss: 0.43764 | eval_custom_logloss: 1.02182 |  0:01:20s
epoch 91 | loss: 0.50514 | eval_custom_logloss: 1.31489 |  0:01:21s
epoch 92 | loss: 0.49313 | eval_custom_logloss: 1.61078 |  0:01:22s
epoch 93 | loss: 0.48609 | eval_custom_logloss: 1.23468 |  0:01:23s
epoch 94 | loss: 0.4599  | eval_custom_logloss: 1.31469 |  0:01:24s

Early stopping occurred at epoch 94 with best_epoch = 74 and best_eval_custom_logloss = 0.67998
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7956, 'Log Loss - std': 0.1315638248151824} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax', 'n_a': 48, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.87213 | eval_custom_logloss: 8.35456 |  0:00:00s
epoch 1  | loss: 2.01698 | eval_custom_logloss: 7.76296 |  0:00:01s
epoch 2  | loss: 1.4776  | eval_custom_logloss: 7.98953 |  0:00:02s
epoch 3  | loss: 1.22503 | eval_custom_logloss: 6.90546 |  0:00:03s
epoch 4  | loss: 1.16525 | eval_custom_logloss: 8.12765 |  0:00:04s
epoch 5  | loss: 1.2206  | eval_custom_logloss: 7.31616 |  0:00:05s
epoch 6  | loss: 1.02039 | eval_custom_logloss: 8.41883 |  0:00:06s
epoch 7  | loss: 0.96664 | eval_custom_logloss: 10.691  |  0:00:07s
epoch 8  | loss: 2.35934 | eval_custom_logloss: 8.37632 |  0:00:07s
epoch 9  | loss: 1.19394 | eval_custom_logloss: 7.75712 |  0:00:08s
epoch 10 | loss: 0.94079 | eval_custom_logloss: 8.42303 |  0:00:09s
epoch 11 | loss: 1.26757 | eval_custom_logloss: 8.4564  |  0:00:10s
epoch 12 | loss: 1.02506 | eval_custom_logloss: 3.75288 |  0:00:11s
epoch 13 | loss: 0.79057 | eval_custom_logloss: 5.99664 |  0:00:12s
epoch 14 | loss: 0.77083 | eval_custom_logloss: 5.51896 |  0:00:13s
epoch 15 | loss: 0.72173 | eval_custom_logloss: 5.89095 |  0:00:14s
epoch 16 | loss: 0.73331 | eval_custom_logloss: 3.75632 |  0:00:14s
epoch 17 | loss: 0.73719 | eval_custom_logloss: 3.53377 |  0:00:15s
epoch 18 | loss: 0.72114 | eval_custom_logloss: 3.71897 |  0:00:16s
epoch 19 | loss: 0.76366 | eval_custom_logloss: 3.693   |  0:00:17s
epoch 20 | loss: 0.71614 | eval_custom_logloss: 3.64801 |  0:00:18s
epoch 21 | loss: 0.69385 | eval_custom_logloss: 2.97591 |  0:00:19s
epoch 22 | loss: 0.62926 | eval_custom_logloss: 3.83749 |  0:00:20s
epoch 23 | loss: 0.63823 | eval_custom_logloss: 4.25341 |  0:00:21s
epoch 24 | loss: 0.6406  | eval_custom_logloss: 4.11662 |  0:00:22s
epoch 25 | loss: 0.64507 | eval_custom_logloss: 4.06211 |  0:00:23s
epoch 26 | loss: 0.63737 | eval_custom_logloss: 4.08163 |  0:00:23s
epoch 27 | loss: 0.63258 | eval_custom_logloss: 4.86075 |  0:00:24s
epoch 28 | loss: 0.67543 | eval_custom_logloss: 3.45136 |  0:00:25s
epoch 29 | loss: 0.62845 | eval_custom_logloss: 2.34831 |  0:00:26s
epoch 30 | loss: 0.63935 | eval_custom_logloss: 2.62857 |  0:00:27s
epoch 31 | loss: 0.65959 | eval_custom_logloss: 2.55077 |  0:00:28s
epoch 32 | loss: 0.67571 | eval_custom_logloss: 2.12358 |  0:00:29s
epoch 33 | loss: 0.69621 | eval_custom_logloss: 3.08503 |  0:00:30s
epoch 34 | loss: 0.68979 | eval_custom_logloss: 2.6011  |  0:00:30s
epoch 35 | loss: 0.64604 | eval_custom_logloss: 2.85208 |  0:00:31s
epoch 36 | loss: 0.60462 | eval_custom_logloss: 3.48677 |  0:00:32s
epoch 37 | loss: 0.57057 | eval_custom_logloss: 2.82488 |  0:00:33s
epoch 38 | loss: 0.5648  | eval_custom_logloss: 3.31703 |  0:00:34s
epoch 39 | loss: 0.54268 | eval_custom_logloss: 3.24284 |  0:00:35s
epoch 40 | loss: 0.52063 | eval_custom_logloss: 3.93455 |  0:00:36s
epoch 41 | loss: 0.5652  | eval_custom_logloss: 2.93821 |  0:00:37s
epoch 42 | loss: 0.52879 | eval_custom_logloss: 2.3351  |  0:00:38s
epoch 43 | loss: 0.55635 | eval_custom_logloss: 2.31772 |  0:00:38s
epoch 44 | loss: 0.55431 | eval_custom_logloss: 2.25805 |  0:00:39s
epoch 45 | loss: 0.60856 | eval_custom_logloss: 1.59711 |  0:00:40s
epoch 46 | loss: 0.57646 | eval_custom_logloss: 2.01125 |  0:00:41s
epoch 47 | loss: 0.54536 | eval_custom_logloss: 2.68776 |  0:00:42s
epoch 48 | loss: 0.53457 | eval_custom_logloss: 2.8947  |  0:00:43s
epoch 49 | loss: 0.52134 | eval_custom_logloss: 2.79302 |  0:00:44s
epoch 50 | loss: 0.52619 | eval_custom_logloss: 2.12821 |  0:00:45s
epoch 51 | loss: 0.54071 | eval_custom_logloss: 2.5015  |  0:00:45s
epoch 52 | loss: 0.52648 | eval_custom_logloss: 2.65009 |  0:00:46s
epoch 53 | loss: 0.49655 | eval_custom_logloss: 2.33363 |  0:00:47s
epoch 54 | loss: 0.51221 | eval_custom_logloss: 2.67205 |  0:00:48s
epoch 55 | loss: 0.49911 | eval_custom_logloss: 2.63104 |  0:00:49s
epoch 56 | loss: 0.53802 | eval_custom_logloss: 1.67494 |  0:00:50s
epoch 57 | loss: 0.48456 | eval_custom_logloss: 1.64669 |  0:00:51s
epoch 58 | loss: 0.47182 | eval_custom_logloss: 1.56466 |  0:00:52s
epoch 59 | loss: 0.56062 | eval_custom_logloss: 1.79499 |  0:00:53s
epoch 60 | loss: 0.48215 | eval_custom_logloss: 1.66652 |  0:00:53s
epoch 61 | loss: 0.44966 | eval_custom_logloss: 1.34788 |  0:00:54s
epoch 62 | loss: 0.44204 | eval_custom_logloss: 1.06687 |  0:00:55s
epoch 63 | loss: 0.43329 | eval_custom_logloss: 1.3473  |  0:00:56s
epoch 64 | loss: 0.43984 | eval_custom_logloss: 1.28361 |  0:00:57s
epoch 65 | loss: 0.47898 | eval_custom_logloss: 1.54906 |  0:00:58s
epoch 66 | loss: 0.45332 | eval_custom_logloss: 1.48294 |  0:00:59s
epoch 67 | loss: 0.46865 | eval_custom_logloss: 1.26676 |  0:01:00s
epoch 68 | loss: 0.46492 | eval_custom_logloss: 1.4621  |  0:01:00s
epoch 69 | loss: 0.47262 | eval_custom_logloss: 1.619   |  0:01:01s
epoch 70 | loss: 0.42724 | eval_custom_logloss: 1.75112 |  0:01:02s
epoch 71 | loss: 0.39047 | eval_custom_logloss: 1.35878 |  0:01:03s
epoch 72 | loss: 0.37202 | eval_custom_logloss: 1.31809 |  0:01:04s
epoch 73 | loss: 0.38728 | eval_custom_logloss: 1.24921 |  0:01:05s
epoch 74 | loss: 0.39801 | eval_custom_logloss: 1.48491 |  0:01:06s
epoch 75 | loss: 0.37448 | eval_custom_logloss: 1.31412 |  0:01:07s
epoch 76 | loss: 0.37661 | eval_custom_logloss: 1.35557 |  0:01:07s
epoch 77 | loss: 0.36211 | eval_custom_logloss: 1.47004 |  0:01:08s
epoch 78 | loss: 0.34626 | eval_custom_logloss: 1.34542 |  0:01:09s
epoch 79 | loss: 0.3525  | eval_custom_logloss: 1.25869 |  0:01:10s
epoch 80 | loss: 0.36078 | eval_custom_logloss: 1.24083 |  0:01:11s
epoch 81 | loss: 0.36488 | eval_custom_logloss: 1.24351 |  0:01:12s
epoch 82 | loss: 0.3475  | eval_custom_logloss: 1.2992  |  0:01:13s

Early stopping occurred at epoch 82 with best_epoch = 62 and best_eval_custom_logloss = 1.06687
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.844025, 'Log Loss - std': 0.1414804646408825} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax', 'n_a': 48, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.95435 | eval_custom_logloss: 7.57485 |  0:00:00s
epoch 1  | loss: 2.15953 | eval_custom_logloss: 8.32078 |  0:00:01s
epoch 2  | loss: 2.38697 | eval_custom_logloss: 7.8806  |  0:00:02s
epoch 3  | loss: 1.28177 | eval_custom_logloss: 8.55516 |  0:00:03s
epoch 4  | loss: 1.28231 | eval_custom_logloss: 8.45517 |  0:00:04s
epoch 5  | loss: 1.70911 | eval_custom_logloss: 9.3083  |  0:00:05s
epoch 6  | loss: 1.80393 | eval_custom_logloss: 6.88994 |  0:00:06s
epoch 7  | loss: 1.39892 | eval_custom_logloss: 8.75253 |  0:00:07s
epoch 8  | loss: 1.01956 | eval_custom_logloss: 8.58868 |  0:00:07s
epoch 9  | loss: 0.85357 | eval_custom_logloss: 7.36355 |  0:00:08s
epoch 10 | loss: 0.96625 | eval_custom_logloss: 7.22591 |  0:00:09s
epoch 11 | loss: 0.93095 | eval_custom_logloss: 8.75696 |  0:00:10s
epoch 12 | loss: 0.79639 | eval_custom_logloss: 5.44136 |  0:00:11s
epoch 13 | loss: 0.76031 | eval_custom_logloss: 6.9272  |  0:00:12s
epoch 14 | loss: 0.76438 | eval_custom_logloss: 7.33733 |  0:00:13s
epoch 15 | loss: 0.74525 | eval_custom_logloss: 5.32938 |  0:00:14s
epoch 16 | loss: 0.7108  | eval_custom_logloss: 4.14683 |  0:00:15s
epoch 17 | loss: 0.69887 | eval_custom_logloss: 5.68968 |  0:00:15s
epoch 18 | loss: 0.71632 | eval_custom_logloss: 4.39393 |  0:00:16s
epoch 19 | loss: 0.70843 | eval_custom_logloss: 5.95157 |  0:00:17s
epoch 20 | loss: 0.66278 | eval_custom_logloss: 7.28005 |  0:00:18s
epoch 21 | loss: 0.63295 | eval_custom_logloss: 4.2739  |  0:00:19s
epoch 22 | loss: 0.63781 | eval_custom_logloss: 1.8341  |  0:00:20s
epoch 23 | loss: 0.63823 | eval_custom_logloss: 2.20145 |  0:00:21s
epoch 24 | loss: 0.64138 | eval_custom_logloss: 1.86647 |  0:00:22s
epoch 25 | loss: 0.59754 | eval_custom_logloss: 4.48632 |  0:00:22s
epoch 26 | loss: 0.61244 | eval_custom_logloss: 3.84487 |  0:00:23s
epoch 27 | loss: 0.60761 | eval_custom_logloss: 4.84765 |  0:00:24s
epoch 28 | loss: 0.59177 | eval_custom_logloss: 3.77019 |  0:00:25s
epoch 29 | loss: 0.59536 | eval_custom_logloss: 3.86182 |  0:00:26s
epoch 30 | loss: 0.6204  | eval_custom_logloss: 3.40094 |  0:00:27s
epoch 31 | loss: 0.64972 | eval_custom_logloss: 2.07076 |  0:00:28s
epoch 32 | loss: 0.64053 | eval_custom_logloss: 2.29149 |  0:00:29s
epoch 33 | loss: 0.62302 | eval_custom_logloss: 2.6811  |  0:00:30s
epoch 34 | loss: 0.61003 | eval_custom_logloss: 2.55849 |  0:00:30s
epoch 35 | loss: 0.58613 | eval_custom_logloss: 2.55414 |  0:00:31s
epoch 36 | loss: 0.64757 | eval_custom_logloss: 3.26048 |  0:00:32s
epoch 37 | loss: 0.5898  | eval_custom_logloss: 2.39383 |  0:00:33s
epoch 38 | loss: 0.58441 | eval_custom_logloss: 2.56233 |  0:00:34s
epoch 39 | loss: 0.58499 | eval_custom_logloss: 1.91331 |  0:00:35s
epoch 40 | loss: 0.62013 | eval_custom_logloss: 2.05615 |  0:00:36s
epoch 41 | loss: 0.61312 | eval_custom_logloss: 2.01793 |  0:00:37s
epoch 42 | loss: 0.60328 | eval_custom_logloss: 2.16614 |  0:00:37s

Early stopping occurred at epoch 42 with best_epoch = 22 and best_eval_custom_logloss = 1.8341
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.00192, 'Log Loss - std': 0.34020097236780494} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 1.00192 and parameters: {'n_d': 48, 'n_steps': 9, 'gamma': 1.5219038402278713, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 5, 'momentum': 0.002646381690417902, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.28432 | eval_custom_logloss: 7.59145 |  0:00:00s
epoch 1  | loss: 2.00504 | eval_custom_logloss: 5.64642 |  0:00:01s
epoch 2  | loss: 1.5611  | eval_custom_logloss: 6.746   |  0:00:02s
epoch 3  | loss: 1.65052 | eval_custom_logloss: 4.69749 |  0:00:03s
epoch 4  | loss: 2.5777  | eval_custom_logloss: 5.84383 |  0:00:04s
epoch 5  | loss: 3.66026 | eval_custom_logloss: 8.03196 |  0:00:04s
epoch 6  | loss: 4.87348 | eval_custom_logloss: 3.31565 |  0:00:05s
epoch 7  | loss: 2.75805 | eval_custom_logloss: 7.50234 |  0:00:06s
epoch 8  | loss: 2.81296 | eval_custom_logloss: 2.02264 |  0:00:06s
epoch 9  | loss: 0.86224 | eval_custom_logloss: 1.25788 |  0:00:07s
epoch 10 | loss: 0.71899 | eval_custom_logloss: 1.49803 |  0:00:08s
epoch 11 | loss: 0.83953 | eval_custom_logloss: 2.08513 |  0:00:08s
epoch 12 | loss: 0.90453 | eval_custom_logloss: 1.42504 |  0:00:09s
epoch 13 | loss: 0.81791 | eval_custom_logloss: 1.65939 |  0:00:10s
epoch 14 | loss: 0.93723 | eval_custom_logloss: 2.0459  |  0:00:10s
epoch 15 | loss: 0.81908 | eval_custom_logloss: 1.21327 |  0:00:11s
epoch 16 | loss: 0.72875 | eval_custom_logloss: 1.33727 |  0:00:12s
epoch 17 | loss: 0.69891 | eval_custom_logloss: 2.00137 |  0:00:13s
epoch 18 | loss: 0.72613 | eval_custom_logloss: 1.33383 |  0:00:13s
epoch 19 | loss: 0.72466 | eval_custom_logloss: 1.01096 |  0:00:14s
epoch 20 | loss: 0.71111 | eval_custom_logloss: 1.11961 |  0:00:15s
epoch 21 | loss: 0.69117 | eval_custom_logloss: 1.62911 |  0:00:15s
epoch 22 | loss: 0.71202 | eval_custom_logloss: 0.7633  |  0:00:16s
epoch 23 | loss: 0.67026 | eval_custom_logloss: 0.80711 |  0:00:17s
epoch 24 | loss: 0.68505 | eval_custom_logloss: 0.90313 |  0:00:17s
epoch 25 | loss: 0.65144 | eval_custom_logloss: 0.89046 |  0:00:18s
epoch 26 | loss: 0.63223 | eval_custom_logloss: 0.9571  |  0:00:19s
epoch 27 | loss: 0.61311 | eval_custom_logloss: 0.92379 |  0:00:20s
epoch 28 | loss: 0.59741 | eval_custom_logloss: 0.85824 |  0:00:20s
epoch 29 | loss: 0.56794 | eval_custom_logloss: 0.83469 |  0:00:21s
epoch 30 | loss: 0.62127 | eval_custom_logloss: 0.82266 |  0:00:22s
epoch 31 | loss: 0.57121 | eval_custom_logloss: 1.20632 |  0:00:22s
epoch 32 | loss: 0.56343 | eval_custom_logloss: 0.98523 |  0:00:23s
epoch 33 | loss: 0.57218 | eval_custom_logloss: 0.8852  |  0:00:24s
epoch 34 | loss: 0.58636 | eval_custom_logloss: 0.87521 |  0:00:24s
epoch 35 | loss: 0.576   | eval_custom_logloss: 0.9531  |  0:00:25s
epoch 36 | loss: 0.6009  | eval_custom_logloss: 0.80179 |  0:00:26s
epoch 37 | loss: 0.61992 | eval_custom_logloss: 0.72598 |  0:00:26s
epoch 38 | loss: 0.59142 | eval_custom_logloss: 0.73148 |  0:00:27s
epoch 39 | loss: 0.59227 | eval_custom_logloss: 0.7315  |  0:00:28s
epoch 40 | loss: 0.58911 | eval_custom_logloss: 0.64468 |  0:00:28s
epoch 41 | loss: 0.59406 | eval_custom_logloss: 0.6933  |  0:00:29s
epoch 42 | loss: 0.57528 | eval_custom_logloss: 0.79046 |  0:00:30s
epoch 43 | loss: 0.5988  | eval_custom_logloss: 0.85349 |  0:00:31s
epoch 44 | loss: 0.55642 | eval_custom_logloss: 0.70319 |  0:00:31s
epoch 45 | loss: 0.57828 | eval_custom_logloss: 0.80377 |  0:00:32s
epoch 46 | loss: 0.53537 | eval_custom_logloss: 0.73788 |  0:00:33s
epoch 47 | loss: 0.54182 | eval_custom_logloss: 0.65608 |  0:00:34s
epoch 48 | loss: 0.53412 | eval_custom_logloss: 0.65782 |  0:00:34s
epoch 49 | loss: 0.52265 | eval_custom_logloss: 0.82639 |  0:00:35s
epoch 50 | loss: 0.57406 | eval_custom_logloss: 0.75571 |  0:00:36s
epoch 51 | loss: 0.57902 | eval_custom_logloss: 0.87611 |  0:00:36s
epoch 52 | loss: 0.59927 | eval_custom_logloss: 0.76651 |  0:00:37s
epoch 53 | loss: 0.56085 | eval_custom_logloss: 0.64744 |  0:00:38s
epoch 54 | loss: 0.55958 | eval_custom_logloss: 0.77437 |  0:00:38s
epoch 55 | loss: 0.5233  | eval_custom_logloss: 0.66081 |  0:00:39s
epoch 56 | loss: 0.59479 | eval_custom_logloss: 0.73623 |  0:00:40s
epoch 57 | loss: 0.55704 | eval_custom_logloss: 0.79987 |  0:00:40s
epoch 58 | loss: 0.53992 | eval_custom_logloss: 0.6371  |  0:00:41s
epoch 59 | loss: 0.57044 | eval_custom_logloss: 0.64661 |  0:00:42s
epoch 60 | loss: 0.55039 | eval_custom_logloss: 0.57698 |  0:00:42s
epoch 61 | loss: 0.51833 | eval_custom_logloss: 0.61575 |  0:00:43s
epoch 62 | loss: 0.49377 | eval_custom_logloss: 0.60346 |  0:00:44s
epoch 63 | loss: 0.50332 | eval_custom_logloss: 0.65364 |  0:00:44s
epoch 64 | loss: 0.50188 | eval_custom_logloss: 0.59601 |  0:00:45s
epoch 65 | loss: 0.50401 | eval_custom_logloss: 0.64286 |  0:00:46s
epoch 66 | loss: 0.50629 | eval_custom_logloss: 0.62734 |  0:00:46s
epoch 67 | loss: 0.50071 | eval_custom_logloss: 0.59588 |  0:00:47s
epoch 68 | loss: 0.49066 | eval_custom_logloss: 0.65247 |  0:00:48s
epoch 69 | loss: 0.48389 | eval_custom_logloss: 0.61319 |  0:00:48s
epoch 70 | loss: 0.46847 | eval_custom_logloss: 0.5785  |  0:00:49s
epoch 71 | loss: 0.46384 | eval_custom_logloss: 0.53972 |  0:00:50s
epoch 72 | loss: 0.45491 | eval_custom_logloss: 0.56714 |  0:00:50s
epoch 73 | loss: 0.47113 | eval_custom_logloss: 0.58013 |  0:00:51s
epoch 74 | loss: 0.47829 | eval_custom_logloss: 0.58686 |  0:00:52s
epoch 75 | loss: 0.47779 | eval_custom_logloss: 0.51616 |  0:00:52s
epoch 76 | loss: 0.48533 | eval_custom_logloss: 0.50358 |  0:00:53s
epoch 77 | loss: 0.4839  | eval_custom_logloss: 0.51422 |  0:00:54s
epoch 78 | loss: 0.50478 | eval_custom_logloss: 0.51876 |  0:00:54s
epoch 79 | loss: 0.47097 | eval_custom_logloss: 0.53925 |  0:00:55s
epoch 80 | loss: 0.51139 | eval_custom_logloss: 0.56164 |  0:00:56s
epoch 81 | loss: 0.46288 | eval_custom_logloss: 0.57791 |  0:00:56s
epoch 82 | loss: 0.48513 | eval_custom_logloss: 0.58135 |  0:00:57s
epoch 83 | loss: 0.47171 | eval_custom_logloss: 0.53728 |  0:00:58s
epoch 84 | loss: 0.47553 | eval_custom_logloss: 0.5425  |  0:00:58s
epoch 85 | loss: 0.45081 | eval_custom_logloss: 0.54161 |  0:00:59s
epoch 86 | loss: 0.48146 | eval_custom_logloss: 0.55734 |  0:01:00s
epoch 87 | loss: 0.44244 | eval_custom_logloss: 0.52845 |  0:01:00s
epoch 88 | loss: 0.43836 | eval_custom_logloss: 0.49131 |  0:01:01s
epoch 89 | loss: 0.41588 | eval_custom_logloss: 0.54535 |  0:01:02s
epoch 90 | loss: 0.45123 | eval_custom_logloss: 0.57565 |  0:01:02s
epoch 91 | loss: 0.43698 | eval_custom_logloss: 0.52579 |  0:01:03s
epoch 92 | loss: 0.45086 | eval_custom_logloss: 0.49414 |  0:01:04s
epoch 93 | loss: 0.41722 | eval_custom_logloss: 0.56261 |  0:01:04s
epoch 94 | loss: 0.45514 | eval_custom_logloss: 0.60586 |  0:01:05s
epoch 95 | loss: 0.449   | eval_custom_logloss: 0.67347 |  0:01:05s
epoch 96 | loss: 0.46415 | eval_custom_logloss: 0.60293 |  0:01:06s
epoch 97 | loss: 0.46094 | eval_custom_logloss: 0.67154 |  0:01:07s
epoch 98 | loss: 0.47245 | eval_custom_logloss: 0.57307 |  0:01:07s
epoch 99 | loss: 0.42792 | eval_custom_logloss: 0.58126 |  0:01:08s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 0.49131
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4913, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.32154 | eval_custom_logloss: 5.81487 |  0:00:00s
epoch 1  | loss: 2.53704 | eval_custom_logloss: 5.57959 |  0:00:01s
epoch 2  | loss: 2.55233 | eval_custom_logloss: 8.35469 |  0:00:02s
epoch 3  | loss: 3.0989  | eval_custom_logloss: 6.54314 |  0:00:02s
epoch 4  | loss: 2.48089 | eval_custom_logloss: 5.95594 |  0:00:03s
epoch 5  | loss: 1.76852 | eval_custom_logloss: 4.87534 |  0:00:03s
epoch 6  | loss: 1.606   | eval_custom_logloss: 3.24166 |  0:00:04s
epoch 7  | loss: 1.36439 | eval_custom_logloss: 3.57429 |  0:00:05s
epoch 8  | loss: 1.16275 | eval_custom_logloss: 3.3573  |  0:00:05s
epoch 9  | loss: 1.27832 | eval_custom_logloss: 3.88781 |  0:00:06s
epoch 10 | loss: 0.8704  | eval_custom_logloss: 3.02695 |  0:00:07s
epoch 11 | loss: 0.75261 | eval_custom_logloss: 2.39714 |  0:00:07s
epoch 12 | loss: 0.69786 | eval_custom_logloss: 1.30349 |  0:00:08s
epoch 13 | loss: 0.69453 | eval_custom_logloss: 1.21055 |  0:00:09s
epoch 14 | loss: 0.66126 | eval_custom_logloss: 1.0572  |  0:00:09s
epoch 15 | loss: 0.64491 | eval_custom_logloss: 1.02129 |  0:00:10s
epoch 16 | loss: 0.69526 | eval_custom_logloss: 1.02162 |  0:00:11s
epoch 17 | loss: 0.65735 | eval_custom_logloss: 0.93976 |  0:00:11s
epoch 18 | loss: 0.60228 | eval_custom_logloss: 0.77292 |  0:00:12s
epoch 19 | loss: 0.5655  | eval_custom_logloss: 0.82511 |  0:00:13s
epoch 20 | loss: 0.5773  | eval_custom_logloss: 0.86301 |  0:00:13s
epoch 21 | loss: 0.5604  | eval_custom_logloss: 0.93136 |  0:00:14s
epoch 22 | loss: 0.55681 | eval_custom_logloss: 0.94865 |  0:00:15s
epoch 23 | loss: 0.5665  | eval_custom_logloss: 0.77733 |  0:00:15s
epoch 24 | loss: 0.53617 | eval_custom_logloss: 0.87795 |  0:00:16s
epoch 25 | loss: 0.54326 | eval_custom_logloss: 0.95538 |  0:00:17s
epoch 26 | loss: 0.52647 | eval_custom_logloss: 0.86386 |  0:00:17s
epoch 27 | loss: 0.54124 | eval_custom_logloss: 0.75711 |  0:00:18s
epoch 28 | loss: 0.53527 | eval_custom_logloss: 0.9115  |  0:00:19s
epoch 29 | loss: 0.52747 | eval_custom_logloss: 0.72686 |  0:00:19s
epoch 30 | loss: 0.51576 | eval_custom_logloss: 0.78507 |  0:00:20s
epoch 31 | loss: 0.48903 | eval_custom_logloss: 0.79115 |  0:00:21s
epoch 32 | loss: 0.50538 | eval_custom_logloss: 0.75121 |  0:00:21s
epoch 33 | loss: 0.51914 | eval_custom_logloss: 0.85943 |  0:00:22s
epoch 34 | loss: 0.48257 | eval_custom_logloss: 0.91423 |  0:00:23s
epoch 35 | loss: 0.51155 | eval_custom_logloss: 0.93902 |  0:00:23s
epoch 36 | loss: 0.53189 | eval_custom_logloss: 0.83521 |  0:00:24s
epoch 37 | loss: 0.46642 | eval_custom_logloss: 0.86672 |  0:00:25s
epoch 38 | loss: 0.47648 | eval_custom_logloss: 1.00265 |  0:00:25s
epoch 39 | loss: 0.47848 | eval_custom_logloss: 0.80072 |  0:00:26s
epoch 40 | loss: 0.51053 | eval_custom_logloss: 0.77629 |  0:00:27s
epoch 41 | loss: 0.47009 | eval_custom_logloss: 0.88614 |  0:00:27s
epoch 42 | loss: 0.48795 | eval_custom_logloss: 0.66194 |  0:00:28s
epoch 43 | loss: 0.47525 | eval_custom_logloss: 0.69817 |  0:00:29s
epoch 44 | loss: 0.46231 | eval_custom_logloss: 1.04646 |  0:00:29s
epoch 45 | loss: 0.4566  | eval_custom_logloss: 0.88193 |  0:00:30s
epoch 46 | loss: 0.42756 | eval_custom_logloss: 0.88468 |  0:00:31s
epoch 47 | loss: 0.42533 | eval_custom_logloss: 0.82962 |  0:00:31s
epoch 48 | loss: 0.41661 | eval_custom_logloss: 0.75304 |  0:00:32s
epoch 49 | loss: 0.4411  | eval_custom_logloss: 0.91355 |  0:00:33s
epoch 50 | loss: 0.47152 | eval_custom_logloss: 0.90834 |  0:00:33s
epoch 51 | loss: 0.52167 | eval_custom_logloss: 0.94103 |  0:00:34s
epoch 52 | loss: 0.45604 | eval_custom_logloss: 0.96783 |  0:00:34s
epoch 53 | loss: 0.42885 | eval_custom_logloss: 0.91438 |  0:00:35s
epoch 54 | loss: 0.44444 | eval_custom_logloss: 0.83427 |  0:00:36s
epoch 55 | loss: 0.4695  | eval_custom_logloss: 0.7954  |  0:00:36s
epoch 56 | loss: 0.46641 | eval_custom_logloss: 0.91839 |  0:00:37s
epoch 57 | loss: 0.43307 | eval_custom_logloss: 0.98636 |  0:00:38s
epoch 58 | loss: 0.39127 | eval_custom_logloss: 0.81421 |  0:00:38s
epoch 59 | loss: 0.41333 | eval_custom_logloss: 0.68781 |  0:00:39s
epoch 60 | loss: 0.3812  | eval_custom_logloss: 0.76583 |  0:00:40s
epoch 61 | loss: 0.38298 | eval_custom_logloss: 0.87463 |  0:00:40s
epoch 62 | loss: 0.36442 | eval_custom_logloss: 0.82948 |  0:00:41s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.66194
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5766, 'Log Loss - std': 0.08530000000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.17824 | eval_custom_logloss: 6.40244 |  0:00:00s
epoch 1  | loss: 2.53524 | eval_custom_logloss: 7.05457 |  0:00:01s
epoch 2  | loss: 3.02348 | eval_custom_logloss: 7.79207 |  0:00:01s
epoch 3  | loss: 4.37361 | eval_custom_logloss: 4.63924 |  0:00:02s
epoch 4  | loss: 2.14321 | eval_custom_logloss: 5.56829 |  0:00:03s
epoch 5  | loss: 2.24567 | eval_custom_logloss: 5.18939 |  0:00:03s
epoch 6  | loss: 2.22135 | eval_custom_logloss: 4.99837 |  0:00:04s
epoch 7  | loss: 1.36756 | eval_custom_logloss: 2.8032  |  0:00:05s
epoch 8  | loss: 1.40832 | eval_custom_logloss: 4.2265  |  0:00:05s
epoch 9  | loss: 1.34923 | eval_custom_logloss: 3.78586 |  0:00:06s
epoch 10 | loss: 1.03171 | eval_custom_logloss: 2.03707 |  0:00:07s
epoch 11 | loss: 0.84719 | eval_custom_logloss: 3.00883 |  0:00:07s
epoch 12 | loss: 0.74457 | eval_custom_logloss: 2.01318 |  0:00:08s
epoch 13 | loss: 0.6812  | eval_custom_logloss: 1.92188 |  0:00:09s
epoch 14 | loss: 0.72402 | eval_custom_logloss: 2.10068 |  0:00:09s
epoch 15 | loss: 0.74516 | eval_custom_logloss: 1.41551 |  0:00:10s
epoch 16 | loss: 0.80519 | eval_custom_logloss: 1.44428 |  0:00:11s
epoch 17 | loss: 0.71024 | eval_custom_logloss: 1.10672 |  0:00:11s
epoch 18 | loss: 0.66934 | eval_custom_logloss: 1.14939 |  0:00:12s
epoch 19 | loss: 0.64139 | eval_custom_logloss: 1.76117 |  0:00:13s
epoch 20 | loss: 0.62668 | eval_custom_logloss: 1.21906 |  0:00:13s
epoch 21 | loss: 0.60154 | eval_custom_logloss: 0.85913 |  0:00:14s
epoch 22 | loss: 0.62643 | eval_custom_logloss: 0.95162 |  0:00:15s
epoch 23 | loss: 0.61997 | eval_custom_logloss: 1.2529  |  0:00:15s
epoch 24 | loss: 0.63784 | eval_custom_logloss: 1.01656 |  0:00:16s
epoch 25 | loss: 0.6432  | eval_custom_logloss: 0.75884 |  0:00:17s
epoch 26 | loss: 0.63273 | eval_custom_logloss: 0.75427 |  0:00:17s
epoch 27 | loss: 0.57753 | eval_custom_logloss: 0.85632 |  0:00:18s
epoch 28 | loss: 0.58695 | eval_custom_logloss: 0.80029 |  0:00:19s
epoch 29 | loss: 0.54211 | eval_custom_logloss: 0.7499  |  0:00:19s
epoch 30 | loss: 0.5394  | eval_custom_logloss: 0.75996 |  0:00:20s
epoch 31 | loss: 0.55981 | eval_custom_logloss: 0.66477 |  0:00:21s
epoch 32 | loss: 0.54648 | eval_custom_logloss: 0.75993 |  0:00:21s
epoch 33 | loss: 0.51533 | eval_custom_logloss: 0.77652 |  0:00:22s
epoch 34 | loss: 0.52839 | eval_custom_logloss: 0.71441 |  0:00:23s
epoch 35 | loss: 0.50769 | eval_custom_logloss: 0.68997 |  0:00:23s
epoch 36 | loss: 0.478   | eval_custom_logloss: 0.66213 |  0:00:24s
epoch 37 | loss: 0.46391 | eval_custom_logloss: 0.62062 |  0:00:24s
epoch 38 | loss: 0.46968 | eval_custom_logloss: 0.82189 |  0:00:25s
epoch 39 | loss: 0.46629 | eval_custom_logloss: 0.70285 |  0:00:26s
epoch 40 | loss: 0.4686  | eval_custom_logloss: 0.72939 |  0:00:26s
epoch 41 | loss: 0.50253 | eval_custom_logloss: 0.76901 |  0:00:27s
epoch 42 | loss: 0.58909 | eval_custom_logloss: 0.76696 |  0:00:28s
epoch 43 | loss: 0.55555 | eval_custom_logloss: 0.65871 |  0:00:28s
epoch 44 | loss: 0.5306  | eval_custom_logloss: 0.65812 |  0:00:29s
epoch 45 | loss: 0.54844 | eval_custom_logloss: 0.78058 |  0:00:30s
epoch 46 | loss: 0.54621 | eval_custom_logloss: 0.56649 |  0:00:30s
epoch 47 | loss: 0.52752 | eval_custom_logloss: 0.74449 |  0:00:31s
epoch 48 | loss: 0.51077 | eval_custom_logloss: 0.74925 |  0:00:32s
epoch 49 | loss: 0.46553 | eval_custom_logloss: 0.70881 |  0:00:32s
epoch 50 | loss: 0.48586 | eval_custom_logloss: 0.69725 |  0:00:33s
epoch 51 | loss: 0.50694 | eval_custom_logloss: 0.85617 |  0:00:34s
epoch 52 | loss: 0.45862 | eval_custom_logloss: 0.79306 |  0:00:34s
epoch 53 | loss: 0.49373 | eval_custom_logloss: 0.84155 |  0:00:35s
epoch 54 | loss: 0.44956 | eval_custom_logloss: 0.72307 |  0:00:36s
epoch 55 | loss: 0.43537 | eval_custom_logloss: 0.79068 |  0:00:36s
epoch 56 | loss: 0.42806 | eval_custom_logloss: 0.90535 |  0:00:37s
epoch 57 | loss: 0.4221  | eval_custom_logloss: 0.98567 |  0:00:38s
epoch 58 | loss: 0.39005 | eval_custom_logloss: 0.83509 |  0:00:38s
epoch 59 | loss: 0.37738 | eval_custom_logloss: 0.89305 |  0:00:39s
epoch 60 | loss: 0.39071 | eval_custom_logloss: 0.77378 |  0:00:39s
epoch 61 | loss: 0.37698 | eval_custom_logloss: 0.68918 |  0:00:40s
epoch 62 | loss: 0.36927 | eval_custom_logloss: 0.62595 |  0:00:41s
epoch 63 | loss: 0.3779  | eval_custom_logloss: 0.67228 |  0:00:41s
epoch 64 | loss: 0.34476 | eval_custom_logloss: 0.50338 |  0:00:42s
epoch 65 | loss: 0.37556 | eval_custom_logloss: 0.67068 |  0:00:43s
epoch 66 | loss: 0.36849 | eval_custom_logloss: 0.54331 |  0:00:43s
epoch 67 | loss: 0.3842  | eval_custom_logloss: 0.64847 |  0:00:44s
epoch 68 | loss: 0.35007 | eval_custom_logloss: 0.64875 |  0:00:45s
epoch 69 | loss: 0.36931 | eval_custom_logloss: 0.83927 |  0:00:45s
epoch 70 | loss: 0.36011 | eval_custom_logloss: 0.61883 |  0:00:46s
epoch 71 | loss: 0.37944 | eval_custom_logloss: 0.61058 |  0:00:47s
epoch 72 | loss: 0.38079 | eval_custom_logloss: 0.58921 |  0:00:47s
epoch 73 | loss: 0.34616 | eval_custom_logloss: 0.62976 |  0:00:48s
epoch 74 | loss: 0.35363 | eval_custom_logloss: 0.72019 |  0:00:49s
epoch 75 | loss: 0.37271 | eval_custom_logloss: 0.63819 |  0:00:49s
epoch 76 | loss: 0.36825 | eval_custom_logloss: 0.71347 |  0:00:50s
epoch 77 | loss: 0.37648 | eval_custom_logloss: 0.51799 |  0:00:51s
epoch 78 | loss: 0.38106 | eval_custom_logloss: 0.65648 |  0:00:51s
epoch 79 | loss: 0.36076 | eval_custom_logloss: 0.57551 |  0:00:52s
epoch 80 | loss: 0.33593 | eval_custom_logloss: 0.53165 |  0:00:53s
epoch 81 | loss: 0.35596 | eval_custom_logloss: 0.6376  |  0:00:53s
epoch 82 | loss: 0.36218 | eval_custom_logloss: 0.56374 |  0:00:54s
epoch 83 | loss: 0.33733 | eval_custom_logloss: 0.51189 |  0:00:54s
epoch 84 | loss: 0.3261  | eval_custom_logloss: 0.58194 |  0:00:55s

Early stopping occurred at epoch 84 with best_epoch = 64 and best_eval_custom_logloss = 0.50338
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5522, 'Log Loss - std': 0.07772674357431082} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.2257  | eval_custom_logloss: 7.14137 |  0:00:00s
epoch 1  | loss: 1.7483  | eval_custom_logloss: 6.51101 |  0:00:01s
epoch 2  | loss: 3.0732  | eval_custom_logloss: 4.92644 |  0:00:02s
epoch 3  | loss: 1.85755 | eval_custom_logloss: 6.22304 |  0:00:02s
epoch 4  | loss: 1.76301 | eval_custom_logloss: 5.28995 |  0:00:03s
epoch 5  | loss: 1.75563 | eval_custom_logloss: 4.54031 |  0:00:04s
epoch 6  | loss: 1.53854 | eval_custom_logloss: 5.0042  |  0:00:04s
epoch 7  | loss: 1.28648 | eval_custom_logloss: 5.16295 |  0:00:05s
epoch 8  | loss: 1.8525  | eval_custom_logloss: 3.25097 |  0:00:06s
epoch 9  | loss: 2.08476 | eval_custom_logloss: 3.9264  |  0:00:07s
epoch 10 | loss: 1.25434 | eval_custom_logloss: 3.35308 |  0:00:07s
epoch 11 | loss: 0.92451 | eval_custom_logloss: 2.12778 |  0:00:08s
epoch 12 | loss: 0.81758 | eval_custom_logloss: 1.51877 |  0:00:09s
epoch 13 | loss: 0.71792 | eval_custom_logloss: 1.86293 |  0:00:10s
epoch 14 | loss: 0.68733 | eval_custom_logloss: 1.69915 |  0:00:10s
epoch 15 | loss: 0.64418 | eval_custom_logloss: 1.66774 |  0:00:11s
epoch 16 | loss: 0.6653  | eval_custom_logloss: 1.73311 |  0:00:11s
epoch 17 | loss: 0.63249 | eval_custom_logloss: 1.48888 |  0:00:12s
epoch 18 | loss: 0.61838 | eval_custom_logloss: 1.35553 |  0:00:13s
epoch 19 | loss: 0.62271 | eval_custom_logloss: 1.27793 |  0:00:13s
epoch 20 | loss: 0.57045 | eval_custom_logloss: 0.95085 |  0:00:14s
epoch 21 | loss: 0.5464  | eval_custom_logloss: 1.40644 |  0:00:15s
epoch 22 | loss: 0.56608 | eval_custom_logloss: 1.32036 |  0:00:16s
epoch 23 | loss: 0.53258 | eval_custom_logloss: 1.29807 |  0:00:16s
epoch 24 | loss: 0.57581 | eval_custom_logloss: 1.23615 |  0:00:17s
epoch 25 | loss: 0.56068 | eval_custom_logloss: 1.08099 |  0:00:18s
epoch 26 | loss: 0.53474 | eval_custom_logloss: 1.46912 |  0:00:18s
epoch 27 | loss: 0.57503 | eval_custom_logloss: 1.2231  |  0:00:19s
epoch 28 | loss: 0.5529  | eval_custom_logloss: 0.94248 |  0:00:20s
epoch 29 | loss: 0.51706 | eval_custom_logloss: 0.87474 |  0:00:20s
epoch 30 | loss: 0.5245  | eval_custom_logloss: 0.94184 |  0:00:21s
epoch 31 | loss: 0.54318 | eval_custom_logloss: 1.06165 |  0:00:22s
epoch 32 | loss: 0.54758 | eval_custom_logloss: 0.89344 |  0:00:22s
epoch 33 | loss: 0.49179 | eval_custom_logloss: 0.88595 |  0:00:23s
epoch 34 | loss: 0.49543 | eval_custom_logloss: 0.78698 |  0:00:24s
epoch 35 | loss: 0.46488 | eval_custom_logloss: 0.76223 |  0:00:24s
epoch 36 | loss: 0.47921 | eval_custom_logloss: 0.65029 |  0:00:25s
epoch 37 | loss: 0.47193 | eval_custom_logloss: 0.79774 |  0:00:26s
epoch 38 | loss: 0.45031 | eval_custom_logloss: 0.77014 |  0:00:26s
epoch 39 | loss: 0.47639 | eval_custom_logloss: 0.69488 |  0:00:27s
epoch 40 | loss: 0.46919 | eval_custom_logloss: 0.73114 |  0:00:28s
epoch 41 | loss: 0.46007 | eval_custom_logloss: 0.8309  |  0:00:28s
epoch 42 | loss: 0.47112 | eval_custom_logloss: 0.75237 |  0:00:29s
epoch 43 | loss: 0.43923 | eval_custom_logloss: 0.6821  |  0:00:30s
epoch 44 | loss: 0.41463 | eval_custom_logloss: 0.70575 |  0:00:30s
epoch 45 | loss: 0.4402  | eval_custom_logloss: 0.68573 |  0:00:31s
epoch 46 | loss: 0.44331 | eval_custom_logloss: 0.60895 |  0:00:32s
epoch 47 | loss: 0.4093  | eval_custom_logloss: 0.72742 |  0:00:32s
epoch 48 | loss: 0.39528 | eval_custom_logloss: 0.68804 |  0:00:33s
epoch 49 | loss: 0.42245 | eval_custom_logloss: 0.75915 |  0:00:34s
epoch 50 | loss: 0.43668 | eval_custom_logloss: 0.84265 |  0:00:34s
epoch 51 | loss: 0.39899 | eval_custom_logloss: 0.63018 |  0:00:35s
epoch 52 | loss: 0.37884 | eval_custom_logloss: 0.65174 |  0:00:36s
epoch 53 | loss: 0.37917 | eval_custom_logloss: 0.64594 |  0:00:36s
epoch 54 | loss: 0.37396 | eval_custom_logloss: 0.623   |  0:00:37s
epoch 55 | loss: 0.37932 | eval_custom_logloss: 0.81172 |  0:00:38s
epoch 56 | loss: 0.38155 | eval_custom_logloss: 0.68179 |  0:00:38s
epoch 57 | loss: 0.37392 | eval_custom_logloss: 0.73036 |  0:00:39s
epoch 58 | loss: 0.36701 | eval_custom_logloss: 0.68999 |  0:00:40s
epoch 59 | loss: 0.38024 | eval_custom_logloss: 0.69425 |  0:00:40s
epoch 60 | loss: 0.37802 | eval_custom_logloss: 0.65652 |  0:00:41s
epoch 61 | loss: 0.34122 | eval_custom_logloss: 0.72618 |  0:00:42s
epoch 62 | loss: 0.35841 | eval_custom_logloss: 0.74835 |  0:00:42s
epoch 63 | loss: 0.36836 | eval_custom_logloss: 0.73619 |  0:00:43s
epoch 64 | loss: 0.3412  | eval_custom_logloss: 0.73727 |  0:00:44s
epoch 65 | loss: 0.35262 | eval_custom_logloss: 0.68315 |  0:00:44s
epoch 66 | loss: 0.34666 | eval_custom_logloss: 0.83051 |  0:00:45s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 0.60895
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5663750000000001, 'Log Loss - std': 0.07165107727731665} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax', 'n_a': 59, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.28225 | eval_custom_logloss: 6.75096 |  0:00:00s
epoch 1  | loss: 1.52924 | eval_custom_logloss: 6.72375 |  0:00:01s
epoch 2  | loss: 1.66918 | eval_custom_logloss: 3.84828 |  0:00:01s
epoch 3  | loss: 1.47057 | eval_custom_logloss: 5.30258 |  0:00:02s
epoch 4  | loss: 1.53671 | eval_custom_logloss: 3.71103 |  0:00:03s
epoch 5  | loss: 1.42427 | eval_custom_logloss: 4.77052 |  0:00:03s
epoch 6  | loss: 1.55595 | eval_custom_logloss: 4.54382 |  0:00:04s
epoch 7  | loss: 1.41006 | eval_custom_logloss: 3.37101 |  0:00:05s
epoch 8  | loss: 1.40225 | eval_custom_logloss: 2.60648 |  0:00:05s
epoch 9  | loss: 1.30808 | eval_custom_logloss: 2.66609 |  0:00:06s
epoch 10 | loss: 1.09477 | eval_custom_logloss: 1.77882 |  0:00:07s
epoch 11 | loss: 0.97366 | eval_custom_logloss: 1.43404 |  0:00:07s
epoch 12 | loss: 0.86198 | eval_custom_logloss: 1.42899 |  0:00:08s
epoch 13 | loss: 0.77202 | eval_custom_logloss: 0.97475 |  0:00:09s
epoch 14 | loss: 0.7115  | eval_custom_logloss: 1.1102  |  0:00:09s
epoch 15 | loss: 0.69916 | eval_custom_logloss: 1.2023  |  0:00:10s
epoch 16 | loss: 0.7013  | eval_custom_logloss: 0.90156 |  0:00:11s
epoch 17 | loss: 0.642   | eval_custom_logloss: 1.06739 |  0:00:11s
epoch 18 | loss: 0.62479 | eval_custom_logloss: 0.83211 |  0:00:12s
epoch 19 | loss: 0.64262 | eval_custom_logloss: 0.72345 |  0:00:13s
epoch 20 | loss: 0.65037 | eval_custom_logloss: 0.86342 |  0:00:13s
epoch 21 | loss: 0.71157 | eval_custom_logloss: 0.71244 |  0:00:14s
epoch 22 | loss: 0.65817 | eval_custom_logloss: 0.73331 |  0:00:15s
epoch 23 | loss: 0.64143 | eval_custom_logloss: 0.87108 |  0:00:15s
epoch 24 | loss: 0.67378 | eval_custom_logloss: 1.04495 |  0:00:16s
epoch 25 | loss: 0.71468 | eval_custom_logloss: 0.97048 |  0:00:17s
epoch 26 | loss: 0.67319 | eval_custom_logloss: 0.96808 |  0:00:17s
epoch 27 | loss: 0.60771 | eval_custom_logloss: 1.18516 |  0:00:18s
epoch 28 | loss: 0.59899 | eval_custom_logloss: 0.7831  |  0:00:19s
epoch 29 | loss: 0.60007 | eval_custom_logloss: 0.91387 |  0:00:19s
epoch 30 | loss: 0.61337 | eval_custom_logloss: 0.99182 |  0:00:20s
epoch 31 | loss: 0.63108 | eval_custom_logloss: 0.92367 |  0:00:21s
epoch 32 | loss: 0.60883 | eval_custom_logloss: 0.71158 |  0:00:21s
epoch 33 | loss: 0.59688 | eval_custom_logloss: 0.73406 |  0:00:22s
epoch 34 | loss: 0.58019 | eval_custom_logloss: 0.94822 |  0:00:23s
epoch 35 | loss: 0.64075 | eval_custom_logloss: 0.69513 |  0:00:23s
epoch 36 | loss: 0.59086 | eval_custom_logloss: 0.66154 |  0:00:24s
epoch 37 | loss: 0.61369 | eval_custom_logloss: 0.80714 |  0:00:25s
epoch 38 | loss: 0.59777 | eval_custom_logloss: 0.91757 |  0:00:25s
epoch 39 | loss: 0.58047 | eval_custom_logloss: 0.81555 |  0:00:26s
epoch 40 | loss: 0.55877 | eval_custom_logloss: 0.77022 |  0:00:26s
epoch 41 | loss: 0.53553 | eval_custom_logloss: 0.61826 |  0:00:27s
epoch 42 | loss: 0.54749 | eval_custom_logloss: 0.64981 |  0:00:28s
epoch 43 | loss: 0.54742 | eval_custom_logloss: 0.73982 |  0:00:28s
epoch 44 | loss: 0.56312 | eval_custom_logloss: 0.6218  |  0:00:29s
epoch 45 | loss: 0.54214 | eval_custom_logloss: 0.50568 |  0:00:30s
epoch 46 | loss: 0.49898 | eval_custom_logloss: 0.55266 |  0:00:30s
epoch 47 | loss: 0.49471 | eval_custom_logloss: 0.61043 |  0:00:31s
epoch 48 | loss: 0.49025 | eval_custom_logloss: 0.60864 |  0:00:32s
epoch 49 | loss: 0.51278 | eval_custom_logloss: 0.55131 |  0:00:32s
epoch 50 | loss: 0.52733 | eval_custom_logloss: 0.61053 |  0:00:33s
epoch 51 | loss: 0.52686 | eval_custom_logloss: 0.53939 |  0:00:34s
epoch 52 | loss: 0.49021 | eval_custom_logloss: 0.46897 |  0:00:34s
epoch 53 | loss: 0.48047 | eval_custom_logloss: 0.54167 |  0:00:35s
epoch 54 | loss: 0.50376 | eval_custom_logloss: 0.52362 |  0:00:36s
epoch 55 | loss: 0.51911 | eval_custom_logloss: 0.58363 |  0:00:36s
epoch 56 | loss: 0.50019 | eval_custom_logloss: 0.50231 |  0:00:37s
epoch 57 | loss: 0.49275 | eval_custom_logloss: 0.52014 |  0:00:37s
epoch 58 | loss: 0.49825 | eval_custom_logloss: 0.64626 |  0:00:38s
epoch 59 | loss: 0.5108  | eval_custom_logloss: 0.5545  |  0:00:39s
epoch 60 | loss: 0.49234 | eval_custom_logloss: 0.582   |  0:00:39s
epoch 61 | loss: 0.48306 | eval_custom_logloss: 0.58769 |  0:00:40s
epoch 62 | loss: 0.49451 | eval_custom_logloss: 0.5245  |  0:00:41s
epoch 63 | loss: 0.47187 | eval_custom_logloss: 0.52384 |  0:00:41s
epoch 64 | loss: 0.47801 | eval_custom_logloss: 0.67771 |  0:00:42s
epoch 65 | loss: 0.48101 | eval_custom_logloss: 0.58718 |  0:00:43s
epoch 66 | loss: 0.47172 | eval_custom_logloss: 0.63874 |  0:00:43s
epoch 67 | loss: 0.43916 | eval_custom_logloss: 0.61023 |  0:00:44s
epoch 68 | loss: 0.43277 | eval_custom_logloss: 0.51316 |  0:00:44s
epoch 69 | loss: 0.42708 | eval_custom_logloss: 0.46572 |  0:00:45s
epoch 70 | loss: 0.39999 | eval_custom_logloss: 0.47113 |  0:00:46s
epoch 71 | loss: 0.39635 | eval_custom_logloss: 0.43748 |  0:00:46s
epoch 72 | loss: 0.3817  | eval_custom_logloss: 0.4342  |  0:00:47s
epoch 73 | loss: 0.36879 | eval_custom_logloss: 0.46796 |  0:00:48s
epoch 74 | loss: 0.40117 | eval_custom_logloss: 0.55983 |  0:00:48s
epoch 75 | loss: 0.36835 | eval_custom_logloss: 0.45208 |  0:00:49s
epoch 76 | loss: 0.3991  | eval_custom_logloss: 0.5776  |  0:00:50s
epoch 77 | loss: 0.39047 | eval_custom_logloss: 0.57067 |  0:00:50s
epoch 78 | loss: 0.43269 | eval_custom_logloss: 0.47219 |  0:00:51s
epoch 79 | loss: 0.38647 | eval_custom_logloss: 0.45547 |  0:00:52s
epoch 80 | loss: 0.3739  | eval_custom_logloss: 0.55861 |  0:00:52s
epoch 81 | loss: 0.39243 | eval_custom_logloss: 0.50164 |  0:00:53s
epoch 82 | loss: 0.40379 | eval_custom_logloss: 0.47227 |  0:00:54s
epoch 83 | loss: 0.36145 | eval_custom_logloss: 0.46478 |  0:00:54s
epoch 84 | loss: 0.35693 | eval_custom_logloss: 0.38004 |  0:00:55s
epoch 85 | loss: 0.35884 | eval_custom_logloss: 0.39235 |  0:00:55s
epoch 86 | loss: 0.36834 | eval_custom_logloss: 0.41883 |  0:00:56s
epoch 87 | loss: 0.35954 | eval_custom_logloss: 0.47196 |  0:00:57s
epoch 88 | loss: 0.38688 | eval_custom_logloss: 0.63005 |  0:00:57s
epoch 89 | loss: 0.38023 | eval_custom_logloss: 0.43207 |  0:00:58s
epoch 90 | loss: 0.39241 | eval_custom_logloss: 0.3768  |  0:00:59s
epoch 91 | loss: 0.35895 | eval_custom_logloss: 0.42784 |  0:00:59s
epoch 92 | loss: 0.40021 | eval_custom_logloss: 0.36954 |  0:01:00s
epoch 93 | loss: 0.3647  | eval_custom_logloss: 0.35962 |  0:01:01s
epoch 94 | loss: 0.36175 | eval_custom_logloss: 0.35264 |  0:01:01s
epoch 95 | loss: 0.3487  | eval_custom_logloss: 0.37368 |  0:01:02s
epoch 96 | loss: 0.34092 | eval_custom_logloss: 0.34657 |  0:01:03s
epoch 97 | loss: 0.32291 | eval_custom_logloss: 0.41653 |  0:01:03s
epoch 98 | loss: 0.3302  | eval_custom_logloss: 0.37661 |  0:01:04s
epoch 99 | loss: 0.35182 | eval_custom_logloss: 0.33925 |  0:01:05s
Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_eval_custom_logloss = 0.33925
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5209600000000001, 'Log Loss - std': 0.1111629002860217} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 0.5209600000000001 and parameters: {'n_d': 59, 'n_steps': 9, 'gamma': 1.8287819798633342, 'cat_emb_dim': 2, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.005967017432568481, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.01418 | eval_custom_logloss: 7.02086 |  0:00:00s
epoch 1  | loss: 2.24461 | eval_custom_logloss: 3.94689 |  0:00:01s
epoch 2  | loss: 2.03247 | eval_custom_logloss: 2.46452 |  0:00:02s
epoch 3  | loss: 1.64643 | eval_custom_logloss: 3.76098 |  0:00:03s
epoch 4  | loss: 2.96768 | eval_custom_logloss: 5.01521 |  0:00:04s
epoch 5  | loss: 2.58729 | eval_custom_logloss: 1.64266 |  0:00:05s
epoch 6  | loss: 1.4452  | eval_custom_logloss: 2.79356 |  0:00:06s
epoch 7  | loss: 2.0677  | eval_custom_logloss: 3.41129 |  0:00:07s
epoch 8  | loss: 1.44026 | eval_custom_logloss: 3.08088 |  0:00:08s
epoch 9  | loss: 1.09315 | eval_custom_logloss: 1.52719 |  0:00:09s
epoch 10 | loss: 1.03029 | eval_custom_logloss: 3.81737 |  0:00:09s
epoch 11 | loss: 1.13425 | eval_custom_logloss: 1.39755 |  0:00:10s
epoch 12 | loss: 0.80101 | eval_custom_logloss: 1.00047 |  0:00:11s
epoch 13 | loss: 0.82811 | eval_custom_logloss: 1.33008 |  0:00:12s
epoch 14 | loss: 0.7622  | eval_custom_logloss: 1.73442 |  0:00:13s
epoch 15 | loss: 0.85671 | eval_custom_logloss: 1.53123 |  0:00:14s
epoch 16 | loss: 0.88794 | eval_custom_logloss: 1.23349 |  0:00:15s
epoch 17 | loss: 0.86976 | eval_custom_logloss: 1.04713 |  0:00:16s
epoch 18 | loss: 0.80225 | eval_custom_logloss: 0.9125  |  0:00:17s
epoch 19 | loss: 0.77985 | eval_custom_logloss: 0.93917 |  0:00:18s
epoch 20 | loss: 0.79245 | eval_custom_logloss: 0.72126 |  0:00:18s
epoch 21 | loss: 0.73226 | eval_custom_logloss: 0.71511 |  0:00:19s
epoch 22 | loss: 0.74044 | eval_custom_logloss: 0.75362 |  0:00:20s
epoch 23 | loss: 0.73164 | eval_custom_logloss: 0.76675 |  0:00:21s
epoch 24 | loss: 0.68852 | eval_custom_logloss: 0.95098 |  0:00:22s
epoch 25 | loss: 0.68194 | eval_custom_logloss: 0.88943 |  0:00:23s
epoch 26 | loss: 0.71862 | eval_custom_logloss: 0.73873 |  0:00:24s
epoch 27 | loss: 0.68523 | eval_custom_logloss: 0.7308  |  0:00:25s
epoch 28 | loss: 0.6568  | eval_custom_logloss: 0.68527 |  0:00:26s
epoch 29 | loss: 0.65434 | eval_custom_logloss: 0.7028  |  0:00:27s
epoch 30 | loss: 0.65537 | eval_custom_logloss: 0.72054 |  0:00:27s
epoch 31 | loss: 0.67137 | eval_custom_logloss: 0.75728 |  0:00:28s
epoch 32 | loss: 0.62994 | eval_custom_logloss: 0.85082 |  0:00:29s
epoch 33 | loss: 0.62607 | eval_custom_logloss: 0.6975  |  0:00:30s
epoch 34 | loss: 0.61908 | eval_custom_logloss: 0.59182 |  0:00:31s
epoch 35 | loss: 0.5775  | eval_custom_logloss: 0.55738 |  0:00:32s
epoch 36 | loss: 0.62232 | eval_custom_logloss: 0.62907 |  0:00:33s
epoch 37 | loss: 0.6351  | eval_custom_logloss: 0.64143 |  0:00:34s
epoch 38 | loss: 0.63503 | eval_custom_logloss: 0.70878 |  0:00:35s
epoch 39 | loss: 0.6844  | eval_custom_logloss: 0.61886 |  0:00:35s
epoch 40 | loss: 0.61054 | eval_custom_logloss: 0.66575 |  0:00:36s
epoch 41 | loss: 0.64859 | eval_custom_logloss: 0.58187 |  0:00:37s
epoch 42 | loss: 0.61948 | eval_custom_logloss: 0.65995 |  0:00:38s
epoch 43 | loss: 0.58338 | eval_custom_logloss: 0.77646 |  0:00:39s
epoch 44 | loss: 0.60005 | eval_custom_logloss: 0.93751 |  0:00:40s
epoch 45 | loss: 0.62207 | eval_custom_logloss: 0.66255 |  0:00:41s
epoch 46 | loss: 0.59412 | eval_custom_logloss: 0.70828 |  0:00:42s
epoch 47 | loss: 0.60027 | eval_custom_logloss: 0.63546 |  0:00:43s
epoch 48 | loss: 0.57761 | eval_custom_logloss: 0.77592 |  0:00:43s
epoch 49 | loss: 0.59447 | eval_custom_logloss: 0.60207 |  0:00:44s
epoch 50 | loss: 0.6024  | eval_custom_logloss: 0.98193 |  0:00:45s
epoch 51 | loss: 0.62703 | eval_custom_logloss: 0.9409  |  0:00:46s
epoch 52 | loss: 0.59354 | eval_custom_logloss: 0.68664 |  0:00:47s
epoch 53 | loss: 0.60687 | eval_custom_logloss: 1.05788 |  0:00:48s
epoch 54 | loss: 0.63007 | eval_custom_logloss: 0.69949 |  0:00:49s
epoch 55 | loss: 0.60365 | eval_custom_logloss: 0.87189 |  0:00:50s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 0.55738
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5574, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.50262 | eval_custom_logloss: 6.25186 |  0:00:00s
epoch 1  | loss: 1.86417 | eval_custom_logloss: 4.53978 |  0:00:01s
epoch 2  | loss: 1.56949 | eval_custom_logloss: 2.34223 |  0:00:02s
epoch 3  | loss: 1.68774 | eval_custom_logloss: 2.08065 |  0:00:03s
epoch 4  | loss: 2.02435 | eval_custom_logloss: 4.41215 |  0:00:04s
epoch 5  | loss: 2.78256 | eval_custom_logloss: 4.53197 |  0:00:05s
epoch 6  | loss: 1.88802 | eval_custom_logloss: 2.47634 |  0:00:06s
epoch 7  | loss: 3.41742 | eval_custom_logloss: 3.42845 |  0:00:07s
epoch 8  | loss: 1.58924 | eval_custom_logloss: 3.67539 |  0:00:08s
epoch 9  | loss: 1.57646 | eval_custom_logloss: 3.61234 |  0:00:09s
epoch 10 | loss: 1.39544 | eval_custom_logloss: 1.69514 |  0:00:09s
epoch 11 | loss: 0.97009 | eval_custom_logloss: 2.08987 |  0:00:10s
epoch 12 | loss: 0.81957 | eval_custom_logloss: 1.2852  |  0:00:11s
epoch 13 | loss: 0.80913 | eval_custom_logloss: 1.46    |  0:00:12s
epoch 14 | loss: 0.77585 | eval_custom_logloss: 1.0352  |  0:00:13s
epoch 15 | loss: 0.75563 | eval_custom_logloss: 1.37998 |  0:00:14s
epoch 16 | loss: 0.73701 | eval_custom_logloss: 1.65943 |  0:00:15s
epoch 17 | loss: 0.70312 | eval_custom_logloss: 1.04873 |  0:00:16s
epoch 18 | loss: 0.69235 | eval_custom_logloss: 1.20222 |  0:00:17s
epoch 19 | loss: 0.75238 | eval_custom_logloss: 1.04998 |  0:00:17s
epoch 20 | loss: 0.69409 | eval_custom_logloss: 0.93286 |  0:00:18s
epoch 21 | loss: 0.71508 | eval_custom_logloss: 1.27917 |  0:00:19s
epoch 22 | loss: 0.7668  | eval_custom_logloss: 0.93782 |  0:00:20s
epoch 23 | loss: 0.67649 | eval_custom_logloss: 0.88104 |  0:00:21s
epoch 24 | loss: 0.70138 | eval_custom_logloss: 0.96352 |  0:00:22s
epoch 25 | loss: 0.63272 | eval_custom_logloss: 0.75113 |  0:00:23s
epoch 26 | loss: 0.62401 | eval_custom_logloss: 0.86568 |  0:00:24s
epoch 27 | loss: 0.62119 | eval_custom_logloss: 1.0559  |  0:00:25s
epoch 28 | loss: 0.67195 | eval_custom_logloss: 1.07946 |  0:00:25s
epoch 29 | loss: 0.69222 | eval_custom_logloss: 0.81086 |  0:00:26s
epoch 30 | loss: 0.67154 | eval_custom_logloss: 0.85427 |  0:00:27s
epoch 31 | loss: 0.6629  | eval_custom_logloss: 0.77924 |  0:00:28s
epoch 32 | loss: 0.61351 | eval_custom_logloss: 0.68173 |  0:00:29s
epoch 33 | loss: 0.63499 | eval_custom_logloss: 0.6923  |  0:00:30s
epoch 34 | loss: 0.62161 | eval_custom_logloss: 0.73163 |  0:00:31s
epoch 35 | loss: 0.633   | eval_custom_logloss: 0.80445 |  0:00:32s
epoch 36 | loss: 0.62657 | eval_custom_logloss: 0.85711 |  0:00:33s
epoch 37 | loss: 0.62219 | eval_custom_logloss: 0.74415 |  0:00:33s
epoch 38 | loss: 0.63287 | eval_custom_logloss: 0.74668 |  0:00:34s
epoch 39 | loss: 0.63514 | eval_custom_logloss: 0.70746 |  0:00:35s
epoch 40 | loss: 0.61973 | eval_custom_logloss: 0.77225 |  0:00:36s
epoch 41 | loss: 0.63804 | eval_custom_logloss: 0.74786 |  0:00:37s
epoch 42 | loss: 0.64084 | eval_custom_logloss: 0.86476 |  0:00:38s
epoch 43 | loss: 0.61617 | eval_custom_logloss: 0.68139 |  0:00:39s
epoch 44 | loss: 0.56501 | eval_custom_logloss: 0.80217 |  0:00:40s
epoch 45 | loss: 0.5549  | eval_custom_logloss: 0.85832 |  0:00:40s
epoch 46 | loss: 0.56039 | eval_custom_logloss: 0.68295 |  0:00:41s
epoch 47 | loss: 0.58901 | eval_custom_logloss: 0.75118 |  0:00:42s
epoch 48 | loss: 0.60956 | eval_custom_logloss: 0.73906 |  0:00:43s
epoch 49 | loss: 0.59614 | eval_custom_logloss: 0.67576 |  0:00:44s
epoch 50 | loss: 0.59886 | eval_custom_logloss: 0.72964 |  0:00:45s
epoch 51 | loss: 0.59209 | eval_custom_logloss: 0.80301 |  0:00:46s
epoch 52 | loss: 0.57136 | eval_custom_logloss: 0.72138 |  0:00:47s
epoch 53 | loss: 0.5412  | eval_custom_logloss: 0.76779 |  0:00:48s
epoch 54 | loss: 0.52345 | eval_custom_logloss: 0.7678  |  0:00:49s
epoch 55 | loss: 0.55781 | eval_custom_logloss: 0.70142 |  0:00:49s
epoch 56 | loss: 0.54204 | eval_custom_logloss: 0.77192 |  0:00:50s
epoch 57 | loss: 0.54359 | eval_custom_logloss: 0.69587 |  0:00:51s
epoch 58 | loss: 0.58643 | eval_custom_logloss: 0.69972 |  0:00:52s
epoch 59 | loss: 0.54873 | eval_custom_logloss: 0.70499 |  0:00:53s
epoch 60 | loss: 0.56575 | eval_custom_logloss: 0.62298 |  0:00:54s
epoch 61 | loss: 0.55094 | eval_custom_logloss: 0.65552 |  0:00:55s
epoch 62 | loss: 0.5344  | eval_custom_logloss: 0.63271 |  0:00:56s
epoch 63 | loss: 0.54073 | eval_custom_logloss: 0.6601  |  0:00:57s
epoch 64 | loss: 0.50066 | eval_custom_logloss: 0.64362 |  0:00:58s
epoch 65 | loss: 0.50102 | eval_custom_logloss: 0.69841 |  0:00:58s
epoch 66 | loss: 0.49898 | eval_custom_logloss: 0.68396 |  0:00:59s
epoch 67 | loss: 0.49732 | eval_custom_logloss: 0.62506 |  0:01:00s
epoch 68 | loss: 0.5352  | eval_custom_logloss: 0.63558 |  0:01:01s
epoch 69 | loss: 0.52263 | eval_custom_logloss: 0.64694 |  0:01:02s
epoch 70 | loss: 0.51358 | eval_custom_logloss: 0.61395 |  0:01:03s
epoch 71 | loss: 0.47772 | eval_custom_logloss: 0.60947 |  0:01:04s
epoch 72 | loss: 0.50729 | eval_custom_logloss: 0.57126 |  0:01:05s
epoch 73 | loss: 0.50832 | eval_custom_logloss: 0.63554 |  0:01:06s
epoch 74 | loss: 0.50574 | eval_custom_logloss: 0.60227 |  0:01:06s
epoch 75 | loss: 0.51259 | eval_custom_logloss: 0.60943 |  0:01:07s
epoch 76 | loss: 0.52904 | eval_custom_logloss: 0.64132 |  0:01:08s
epoch 77 | loss: 0.52257 | eval_custom_logloss: 0.70203 |  0:01:09s
epoch 78 | loss: 0.53332 | eval_custom_logloss: 0.80255 |  0:01:10s
epoch 79 | loss: 0.53832 | eval_custom_logloss: 0.75228 |  0:01:11s
epoch 80 | loss: 0.5211  | eval_custom_logloss: 0.70681 |  0:01:12s
epoch 81 | loss: 0.52095 | eval_custom_logloss: 0.65697 |  0:01:13s
epoch 82 | loss: 0.52699 | eval_custom_logloss: 0.65396 |  0:01:13s
epoch 83 | loss: 0.54    | eval_custom_logloss: 0.67122 |  0:01:14s
epoch 84 | loss: 0.5462  | eval_custom_logloss: 0.62352 |  0:01:15s
epoch 85 | loss: 0.5147  | eval_custom_logloss: 0.66446 |  0:01:16s
epoch 86 | loss: 0.49741 | eval_custom_logloss: 0.67772 |  0:01:17s
epoch 87 | loss: 0.50535 | eval_custom_logloss: 0.63273 |  0:01:18s
epoch 88 | loss: 0.50598 | eval_custom_logloss: 0.61694 |  0:01:19s
epoch 89 | loss: 0.51301 | eval_custom_logloss: 0.61379 |  0:01:20s
epoch 90 | loss: 0.52355 | eval_custom_logloss: 0.61764 |  0:01:20s
epoch 91 | loss: 0.48906 | eval_custom_logloss: 0.5984  |  0:01:21s
epoch 92 | loss: 0.49134 | eval_custom_logloss: 0.57271 |  0:01:22s

Early stopping occurred at epoch 92 with best_epoch = 72 and best_eval_custom_logloss = 0.57126
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.56435, 'Log Loss - std': 0.006950000000000012} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.69683 | eval_custom_logloss: 6.98687 |  0:00:00s
epoch 1  | loss: 1.89778 | eval_custom_logloss: 3.56872 |  0:00:01s
epoch 2  | loss: 1.58172 | eval_custom_logloss: 2.28258 |  0:00:02s
epoch 3  | loss: 1.44917 | eval_custom_logloss: 2.92497 |  0:00:03s
epoch 4  | loss: 3.3957  | eval_custom_logloss: 4.35849 |  0:00:04s
epoch 5  | loss: 2.54899 | eval_custom_logloss: 2.87761 |  0:00:05s
epoch 6  | loss: 2.75464 | eval_custom_logloss: 4.41809 |  0:00:06s
epoch 7  | loss: 2.82678 | eval_custom_logloss: 3.3108  |  0:00:07s
epoch 8  | loss: 2.34498 | eval_custom_logloss: 4.95658 |  0:00:08s
epoch 9  | loss: 1.34018 | eval_custom_logloss: 2.6582  |  0:00:09s
epoch 10 | loss: 0.96631 | eval_custom_logloss: 1.3923  |  0:00:10s
epoch 11 | loss: 0.86625 | eval_custom_logloss: 1.8685  |  0:00:10s
epoch 12 | loss: 0.87217 | eval_custom_logloss: 1.98847 |  0:00:11s
epoch 13 | loss: 0.83272 | eval_custom_logloss: 1.05477 |  0:00:12s
epoch 14 | loss: 0.74927 | eval_custom_logloss: 1.11422 |  0:00:13s
epoch 15 | loss: 0.77313 | eval_custom_logloss: 1.08255 |  0:00:14s
epoch 16 | loss: 0.68999 | eval_custom_logloss: 1.15739 |  0:00:15s
epoch 17 | loss: 0.64966 | eval_custom_logloss: 0.73135 |  0:00:16s
epoch 18 | loss: 0.64626 | eval_custom_logloss: 0.86417 |  0:00:17s
epoch 19 | loss: 0.69612 | eval_custom_logloss: 0.75262 |  0:00:18s
epoch 20 | loss: 0.71616 | eval_custom_logloss: 0.67286 |  0:00:18s
epoch 21 | loss: 0.63547 | eval_custom_logloss: 0.62047 |  0:00:19s
epoch 22 | loss: 0.64331 | eval_custom_logloss: 0.66499 |  0:00:20s
epoch 23 | loss: 0.60845 | eval_custom_logloss: 0.82988 |  0:00:21s
epoch 24 | loss: 0.65078 | eval_custom_logloss: 0.7363  |  0:00:22s
epoch 25 | loss: 0.61118 | eval_custom_logloss: 0.64271 |  0:00:23s
epoch 26 | loss: 0.5975  | eval_custom_logloss: 0.6504  |  0:00:24s
epoch 27 | loss: 0.64768 | eval_custom_logloss: 0.67589 |  0:00:25s
epoch 28 | loss: 0.65301 | eval_custom_logloss: 0.71643 |  0:00:26s
epoch 29 | loss: 0.63927 | eval_custom_logloss: 0.73462 |  0:00:26s
epoch 30 | loss: 0.60149 | eval_custom_logloss: 0.71651 |  0:00:27s
epoch 31 | loss: 0.62713 | eval_custom_logloss: 0.63046 |  0:00:28s
epoch 32 | loss: 0.57885 | eval_custom_logloss: 0.63843 |  0:00:29s
epoch 33 | loss: 0.59996 | eval_custom_logloss: 0.6186  |  0:00:30s
epoch 34 | loss: 0.57866 | eval_custom_logloss: 0.60654 |  0:00:31s
epoch 35 | loss: 0.57158 | eval_custom_logloss: 0.62889 |  0:00:32s
epoch 36 | loss: 0.58296 | eval_custom_logloss: 0.64555 |  0:00:33s
epoch 37 | loss: 0.59364 | eval_custom_logloss: 0.61321 |  0:00:34s
epoch 38 | loss: 0.53458 | eval_custom_logloss: 0.59891 |  0:00:35s
epoch 39 | loss: 0.54954 | eval_custom_logloss: 0.616   |  0:00:36s
epoch 40 | loss: 0.59125 | eval_custom_logloss: 0.57116 |  0:00:36s
epoch 41 | loss: 0.57102 | eval_custom_logloss: 0.61556 |  0:00:37s
epoch 42 | loss: 0.5784  | eval_custom_logloss: 0.65877 |  0:00:38s
epoch 43 | loss: 0.58354 | eval_custom_logloss: 0.60591 |  0:00:39s
epoch 44 | loss: 0.56223 | eval_custom_logloss: 0.59247 |  0:00:40s
epoch 45 | loss: 0.56899 | eval_custom_logloss: 0.61551 |  0:00:41s
epoch 46 | loss: 0.53729 | eval_custom_logloss: 0.56196 |  0:00:42s
epoch 47 | loss: 0.50928 | eval_custom_logloss: 0.58733 |  0:00:43s
epoch 48 | loss: 0.56235 | eval_custom_logloss: 0.57293 |  0:00:44s
epoch 49 | loss: 0.55429 | eval_custom_logloss: 0.56972 |  0:00:44s
epoch 50 | loss: 0.53276 | eval_custom_logloss: 0.58167 |  0:00:45s
epoch 51 | loss: 0.56858 | eval_custom_logloss: 0.6367  |  0:00:46s
epoch 52 | loss: 0.58409 | eval_custom_logloss: 0.59294 |  0:00:47s
epoch 53 | loss: 0.53587 | eval_custom_logloss: 0.6478  |  0:00:48s
epoch 54 | loss: 0.54316 | eval_custom_logloss: 0.60019 |  0:00:49s
epoch 55 | loss: 0.55716 | eval_custom_logloss: 0.5792  |  0:00:50s
epoch 56 | loss: 0.54381 | eval_custom_logloss: 0.57783 |  0:00:51s
epoch 57 | loss: 0.52808 | eval_custom_logloss: 0.5936  |  0:00:52s
epoch 58 | loss: 0.53374 | eval_custom_logloss: 0.58454 |  0:00:53s
epoch 59 | loss: 0.52521 | eval_custom_logloss: 0.64586 |  0:00:54s
epoch 60 | loss: 0.55949 | eval_custom_logloss: 0.66492 |  0:00:54s
epoch 61 | loss: 0.53871 | eval_custom_logloss: 0.53109 |  0:00:55s
epoch 62 | loss: 0.53588 | eval_custom_logloss: 0.58095 |  0:00:56s
epoch 63 | loss: 0.54633 | eval_custom_logloss: 0.52864 |  0:00:57s
epoch 64 | loss: 0.57427 | eval_custom_logloss: 0.56572 |  0:00:58s
epoch 65 | loss: 0.57258 | eval_custom_logloss: 0.58809 |  0:00:59s
epoch 66 | loss: 0.57108 | eval_custom_logloss: 0.57343 |  0:01:00s
epoch 67 | loss: 0.53743 | eval_custom_logloss: 0.64449 |  0:01:01s
epoch 68 | loss: 0.54906 | eval_custom_logloss: 0.60568 |  0:01:02s
epoch 69 | loss: 0.51472 | eval_custom_logloss: 0.57162 |  0:01:03s
epoch 70 | loss: 0.49852 | eval_custom_logloss: 0.50943 |  0:01:03s
epoch 71 | loss: 0.49071 | eval_custom_logloss: 0.49248 |  0:01:04s
epoch 72 | loss: 0.48437 | eval_custom_logloss: 0.53704 |  0:01:05s
epoch 73 | loss: 0.5133  | eval_custom_logloss: 0.61506 |  0:01:06s
epoch 74 | loss: 0.51859 | eval_custom_logloss: 0.51665 |  0:01:07s
epoch 75 | loss: 0.4988  | eval_custom_logloss: 0.51231 |  0:01:08s
epoch 76 | loss: 0.51105 | eval_custom_logloss: 0.5436  |  0:01:09s
epoch 77 | loss: 0.49888 | eval_custom_logloss: 0.56034 |  0:01:10s
epoch 78 | loss: 0.51593 | eval_custom_logloss: 0.53098 |  0:01:11s
epoch 79 | loss: 0.48272 | eval_custom_logloss: 0.53327 |  0:01:12s
epoch 80 | loss: 0.53884 | eval_custom_logloss: 0.76867 |  0:01:12s
epoch 81 | loss: 0.5202  | eval_custom_logloss: 0.66099 |  0:01:13s
epoch 82 | loss: 0.51975 | eval_custom_logloss: 0.67594 |  0:01:14s
epoch 83 | loss: 0.52169 | eval_custom_logloss: 0.63435 |  0:01:15s
epoch 84 | loss: 0.50473 | eval_custom_logloss: 0.53154 |  0:01:16s
epoch 85 | loss: 0.50529 | eval_custom_logloss: 0.54117 |  0:01:17s
epoch 86 | loss: 0.49714 | eval_custom_logloss: 0.58448 |  0:01:18s
epoch 87 | loss: 0.55634 | eval_custom_logloss: 0.57645 |  0:01:19s
epoch 88 | loss: 0.54763 | eval_custom_logloss: 0.64998 |  0:01:20s
epoch 89 | loss: 0.53224 | eval_custom_logloss: 0.64551 |  0:01:21s
epoch 90 | loss: 0.52732 | eval_custom_logloss: 0.61833 |  0:01:22s
epoch 91 | loss: 0.50577 | eval_custom_logloss: 0.62867 |  0:01:22s

Early stopping occurred at epoch 91 with best_epoch = 71 and best_eval_custom_logloss = 0.49248
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5404, 'Log Loss - std': 0.03434249068816453} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.78186 | eval_custom_logloss: 5.75059 |  0:00:01s
epoch 1  | loss: 1.85143 | eval_custom_logloss: 3.94868 |  0:00:01s
epoch 2  | loss: 1.45882 | eval_custom_logloss: 3.42335 |  0:00:02s
epoch 3  | loss: 1.68188 | eval_custom_logloss: 2.28106 |  0:00:03s
epoch 4  | loss: 2.02354 | eval_custom_logloss: 4.02097 |  0:00:04s
epoch 5  | loss: 1.78793 | eval_custom_logloss: 2.49734 |  0:00:05s
epoch 6  | loss: 1.45847 | eval_custom_logloss: 1.55874 |  0:00:06s
epoch 7  | loss: 1.88025 | eval_custom_logloss: 6.6612  |  0:00:07s
epoch 8  | loss: 2.41845 | eval_custom_logloss: 2.15829 |  0:00:08s
epoch 9  | loss: 1.13052 | eval_custom_logloss: 1.28427 |  0:00:09s
epoch 10 | loss: 0.87503 | eval_custom_logloss: 1.91517 |  0:00:10s
epoch 11 | loss: 0.805   | eval_custom_logloss: 1.28077 |  0:00:11s
epoch 12 | loss: 0.78189 | eval_custom_logloss: 1.05419 |  0:00:11s
epoch 13 | loss: 0.81863 | eval_custom_logloss: 1.64589 |  0:00:12s
epoch 14 | loss: 0.85922 | eval_custom_logloss: 1.84734 |  0:00:13s
epoch 15 | loss: 1.0072  | eval_custom_logloss: 0.98383 |  0:00:14s
epoch 16 | loss: 0.88264 | eval_custom_logloss: 1.39859 |  0:00:15s
epoch 17 | loss: 0.84476 | eval_custom_logloss: 1.01138 |  0:00:16s
epoch 18 | loss: 0.72801 | eval_custom_logloss: 1.07624 |  0:00:17s
epoch 19 | loss: 0.7254  | eval_custom_logloss: 0.88728 |  0:00:18s
epoch 20 | loss: 0.71238 | eval_custom_logloss: 1.09353 |  0:00:19s
epoch 21 | loss: 0.68531 | eval_custom_logloss: 1.12703 |  0:00:20s
epoch 22 | loss: 0.64913 | eval_custom_logloss: 1.13645 |  0:00:21s
epoch 23 | loss: 0.6438  | eval_custom_logloss: 1.04072 |  0:00:21s
epoch 24 | loss: 0.61984 | eval_custom_logloss: 0.93631 |  0:00:22s
epoch 25 | loss: 0.59929 | eval_custom_logloss: 0.8155  |  0:00:23s
epoch 26 | loss: 0.57013 | eval_custom_logloss: 0.80321 |  0:00:24s
epoch 27 | loss: 0.5696  | eval_custom_logloss: 0.76309 |  0:00:25s
epoch 28 | loss: 0.58786 | eval_custom_logloss: 0.6752  |  0:00:26s
epoch 29 | loss: 0.5709  | eval_custom_logloss: 0.69502 |  0:00:27s
epoch 30 | loss: 0.55682 | eval_custom_logloss: 0.84047 |  0:00:28s
epoch 31 | loss: 0.54903 | eval_custom_logloss: 1.19022 |  0:00:29s
epoch 32 | loss: 0.55845 | eval_custom_logloss: 0.69229 |  0:00:30s
epoch 33 | loss: 0.55404 | eval_custom_logloss: 0.65087 |  0:00:31s
epoch 34 | loss: 0.51863 | eval_custom_logloss: 0.65158 |  0:00:32s
epoch 35 | loss: 0.51547 | eval_custom_logloss: 0.67899 |  0:00:32s
epoch 36 | loss: 0.49844 | eval_custom_logloss: 0.71089 |  0:00:33s
epoch 37 | loss: 0.53473 | eval_custom_logloss: 0.65962 |  0:00:34s
epoch 38 | loss: 0.52641 | eval_custom_logloss: 0.63424 |  0:00:35s
epoch 39 | loss: 0.5262  | eval_custom_logloss: 0.70376 |  0:00:36s
epoch 40 | loss: 0.53222 | eval_custom_logloss: 0.64926 |  0:00:37s
epoch 41 | loss: 0.49452 | eval_custom_logloss: 0.62421 |  0:00:38s
epoch 42 | loss: 0.48169 | eval_custom_logloss: 0.5465  |  0:00:39s
epoch 43 | loss: 0.50422 | eval_custom_logloss: 0.61074 |  0:00:40s
epoch 44 | loss: 0.48252 | eval_custom_logloss: 0.65796 |  0:00:41s
epoch 45 | loss: 0.47762 | eval_custom_logloss: 0.59951 |  0:00:42s
epoch 46 | loss: 0.45857 | eval_custom_logloss: 0.64334 |  0:00:43s
epoch 47 | loss: 0.50461 | eval_custom_logloss: 0.64498 |  0:00:43s
epoch 48 | loss: 0.48803 | eval_custom_logloss: 0.58539 |  0:00:44s
epoch 49 | loss: 0.50354 | eval_custom_logloss: 0.6227  |  0:00:45s
epoch 50 | loss: 0.50826 | eval_custom_logloss: 0.59677 |  0:00:46s
epoch 51 | loss: 0.50671 | eval_custom_logloss: 0.68388 |  0:00:47s
epoch 52 | loss: 0.46378 | eval_custom_logloss: 0.71601 |  0:00:48s
epoch 53 | loss: 0.48111 | eval_custom_logloss: 0.6597  |  0:00:49s
epoch 54 | loss: 0.48764 | eval_custom_logloss: 0.58474 |  0:00:50s
epoch 55 | loss: 0.4762  | eval_custom_logloss: 0.59678 |  0:00:51s
epoch 56 | loss: 0.47189 | eval_custom_logloss: 0.54837 |  0:00:52s
epoch 57 | loss: 0.43354 | eval_custom_logloss: 0.57478 |  0:00:52s
epoch 58 | loss: 0.44532 | eval_custom_logloss: 0.58214 |  0:00:53s
epoch 59 | loss: 0.44965 | eval_custom_logloss: 0.61936 |  0:00:54s
epoch 60 | loss: 0.42318 | eval_custom_logloss: 0.57536 |  0:00:55s
epoch 61 | loss: 0.42665 | eval_custom_logloss: 0.5816  |  0:00:56s
epoch 62 | loss: 0.41752 | eval_custom_logloss: 0.65461 |  0:00:57s

Early stopping occurred at epoch 62 with best_epoch = 42 and best_eval_custom_logloss = 0.5465
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.541925, 'Log Loss - std': 0.029858531025487516} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.48167 | eval_custom_logloss: 6.02295 |  0:00:00s
epoch 1  | loss: 1.68255 | eval_custom_logloss: 3.78536 |  0:00:01s
epoch 2  | loss: 1.59112 | eval_custom_logloss: 2.79286 |  0:00:02s
epoch 3  | loss: 1.47962 | eval_custom_logloss: 1.92442 |  0:00:03s
epoch 4  | loss: 1.43245 | eval_custom_logloss: 1.92265 |  0:00:04s
epoch 5  | loss: 1.46039 | eval_custom_logloss: 2.51997 |  0:00:05s
epoch 6  | loss: 1.11185 | eval_custom_logloss: 1.50232 |  0:00:06s
epoch 7  | loss: 1.4126  | eval_custom_logloss: 3.22639 |  0:00:07s
epoch 8  | loss: 1.53824 | eval_custom_logloss: 3.98909 |  0:00:08s
epoch 9  | loss: 2.51383 | eval_custom_logloss: 3.70779 |  0:00:09s
epoch 10 | loss: 1.90107 | eval_custom_logloss: 2.00763 |  0:00:09s
epoch 11 | loss: 0.93855 | eval_custom_logloss: 1.08884 |  0:00:10s
epoch 12 | loss: 0.83469 | eval_custom_logloss: 1.0468  |  0:00:11s
epoch 13 | loss: 0.84361 | eval_custom_logloss: 1.09586 |  0:00:12s
epoch 14 | loss: 0.85648 | eval_custom_logloss: 1.5861  |  0:00:13s
epoch 15 | loss: 0.86674 | eval_custom_logloss: 1.11623 |  0:00:14s
epoch 16 | loss: 0.79702 | eval_custom_logloss: 0.96496 |  0:00:15s
epoch 17 | loss: 0.78304 | eval_custom_logloss: 0.93872 |  0:00:16s
epoch 18 | loss: 0.74366 | eval_custom_logloss: 1.15099 |  0:00:17s
epoch 19 | loss: 0.69366 | eval_custom_logloss: 1.60046 |  0:00:18s
epoch 20 | loss: 0.65399 | eval_custom_logloss: 1.15742 |  0:00:18s
epoch 21 | loss: 0.69931 | eval_custom_logloss: 0.89937 |  0:00:19s
epoch 22 | loss: 0.67234 | eval_custom_logloss: 0.68242 |  0:00:20s
epoch 23 | loss: 0.62103 | eval_custom_logloss: 0.82272 |  0:00:21s
epoch 24 | loss: 0.63983 | eval_custom_logloss: 0.63738 |  0:00:22s
epoch 25 | loss: 0.6121  | eval_custom_logloss: 0.63137 |  0:00:23s
epoch 26 | loss: 0.64402 | eval_custom_logloss: 0.59386 |  0:00:24s
epoch 27 | loss: 0.64369 | eval_custom_logloss: 0.59834 |  0:00:25s
epoch 28 | loss: 0.61601 | eval_custom_logloss: 0.60195 |  0:00:26s
epoch 29 | loss: 0.58227 | eval_custom_logloss: 0.60418 |  0:00:26s
epoch 30 | loss: 0.57723 | eval_custom_logloss: 0.56129 |  0:00:27s
epoch 31 | loss: 0.58502 | eval_custom_logloss: 0.59771 |  0:00:28s
epoch 32 | loss: 0.57766 | eval_custom_logloss: 0.57913 |  0:00:29s
epoch 33 | loss: 0.56151 | eval_custom_logloss: 0.52451 |  0:00:30s
epoch 34 | loss: 0.56263 | eval_custom_logloss: 0.54259 |  0:00:31s
epoch 35 | loss: 0.58596 | eval_custom_logloss: 0.58863 |  0:00:32s
epoch 36 | loss: 0.59568 | eval_custom_logloss: 0.50126 |  0:00:33s
epoch 37 | loss: 0.57077 | eval_custom_logloss: 0.52961 |  0:00:34s
epoch 38 | loss: 0.56957 | eval_custom_logloss: 0.56116 |  0:00:34s
epoch 39 | loss: 0.57439 | eval_custom_logloss: 0.51144 |  0:00:35s
epoch 40 | loss: 0.53405 | eval_custom_logloss: 0.4639  |  0:00:36s
epoch 41 | loss: 0.52961 | eval_custom_logloss: 0.55867 |  0:00:37s
epoch 42 | loss: 0.51698 | eval_custom_logloss: 0.49523 |  0:00:38s
epoch 43 | loss: 0.55389 | eval_custom_logloss: 0.4888  |  0:00:39s
epoch 44 | loss: 0.5405  | eval_custom_logloss: 0.52117 |  0:00:40s
epoch 45 | loss: 0.51197 | eval_custom_logloss: 0.49044 |  0:00:41s
epoch 46 | loss: 0.51188 | eval_custom_logloss: 0.49748 |  0:00:41s
epoch 47 | loss: 0.51383 | eval_custom_logloss: 0.49508 |  0:00:42s
epoch 48 | loss: 0.53971 | eval_custom_logloss: 0.4927  |  0:00:43s
epoch 49 | loss: 0.48839 | eval_custom_logloss: 0.47122 |  0:00:44s
epoch 50 | loss: 0.47688 | eval_custom_logloss: 0.46576 |  0:00:45s
epoch 51 | loss: 0.50312 | eval_custom_logloss: 0.48037 |  0:00:46s
epoch 52 | loss: 0.47144 | eval_custom_logloss: 0.43995 |  0:00:47s
epoch 53 | loss: 0.4678  | eval_custom_logloss: 0.47107 |  0:00:47s
epoch 54 | loss: 0.47956 | eval_custom_logloss: 0.40273 |  0:00:48s
epoch 55 | loss: 0.45609 | eval_custom_logloss: 0.41668 |  0:00:49s
epoch 56 | loss: 0.42751 | eval_custom_logloss: 0.47505 |  0:00:50s
epoch 57 | loss: 0.44026 | eval_custom_logloss: 0.46211 |  0:00:51s
epoch 58 | loss: 0.44363 | eval_custom_logloss: 0.43686 |  0:00:52s
epoch 59 | loss: 0.42066 | eval_custom_logloss: 0.4593  |  0:00:53s
epoch 60 | loss: 0.45451 | eval_custom_logloss: 0.41493 |  0:00:54s
epoch 61 | loss: 0.45565 | eval_custom_logloss: 0.4844  |  0:00:55s
epoch 62 | loss: 0.45114 | eval_custom_logloss: 0.43949 |  0:00:55s
epoch 63 | loss: 0.39747 | eval_custom_logloss: 0.40164 |  0:00:56s
epoch 64 | loss: 0.41971 | eval_custom_logloss: 0.43007 |  0:00:57s
epoch 65 | loss: 0.40512 | eval_custom_logloss: 0.44447 |  0:00:58s
epoch 66 | loss: 0.44475 | eval_custom_logloss: 0.42589 |  0:00:59s
epoch 67 | loss: 0.41443 | eval_custom_logloss: 0.39108 |  0:01:00s
epoch 68 | loss: 0.45495 | eval_custom_logloss: 0.34676 |  0:01:01s
epoch 69 | loss: 0.44143 | eval_custom_logloss: 0.45991 |  0:01:02s
epoch 70 | loss: 0.49366 | eval_custom_logloss: 0.52821 |  0:01:03s
epoch 71 | loss: 0.46664 | eval_custom_logloss: 0.46686 |  0:01:03s
epoch 72 | loss: 0.47699 | eval_custom_logloss: 0.46323 |  0:01:04s
epoch 73 | loss: 0.43079 | eval_custom_logloss: 0.46825 |  0:01:05s
epoch 74 | loss: 0.44261 | eval_custom_logloss: 0.40559 |  0:01:06s
epoch 75 | loss: 0.41507 | eval_custom_logloss: 0.37108 |  0:01:07s
epoch 76 | loss: 0.40689 | eval_custom_logloss: 0.46539 |  0:01:08s
epoch 77 | loss: 0.40331 | eval_custom_logloss: 0.37479 |  0:01:09s
epoch 78 | loss: 0.4075  | eval_custom_logloss: 0.41994 |  0:01:09s
epoch 79 | loss: 0.41583 | eval_custom_logloss: 0.40798 |  0:01:10s
epoch 80 | loss: 0.40317 | eval_custom_logloss: 0.37096 |  0:01:11s
epoch 81 | loss: 0.38625 | eval_custom_logloss: 0.39253 |  0:01:12s
epoch 82 | loss: 0.44024 | eval_custom_logloss: 0.43295 |  0:01:13s
epoch 83 | loss: 0.47322 | eval_custom_logloss: 0.41137 |  0:01:14s
epoch 84 | loss: 0.46356 | eval_custom_logloss: 0.44216 |  0:01:15s
epoch 85 | loss: 0.45807 | eval_custom_logloss: 0.42612 |  0:01:16s
epoch 86 | loss: 0.43976 | eval_custom_logloss: 0.51897 |  0:01:16s
epoch 87 | loss: 0.44431 | eval_custom_logloss: 0.43623 |  0:01:17s
epoch 88 | loss: 0.38342 | eval_custom_logloss: 0.42827 |  0:01:18s

Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_custom_logloss = 0.34676
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5029, 'Log Loss - std': 0.08249259360694147} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 0.5029 and parameters: {'n_d': 50, 'n_steps': 10, 'gamma': 1.5037103442138413, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 5, 'momentum': 0.021678995979149553, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 4.00136 | eval_custom_logloss: 7.97894 |  0:00:00s
epoch 1  | loss: 1.70347 | eval_custom_logloss: 7.08764 |  0:00:01s
epoch 2  | loss: 1.54649 | eval_custom_logloss: 7.45176 |  0:00:02s
epoch 3  | loss: 1.08502 | eval_custom_logloss: 7.72899 |  0:00:03s
epoch 4  | loss: 0.98824 | eval_custom_logloss: 9.2806  |  0:00:03s
epoch 5  | loss: 0.89565 | eval_custom_logloss: 6.58384 |  0:00:04s
epoch 6  | loss: 1.00022 | eval_custom_logloss: 4.87202 |  0:00:05s
epoch 7  | loss: 0.90787 | eval_custom_logloss: 7.55185 |  0:00:06s
epoch 8  | loss: 0.87459 | eval_custom_logloss: 7.2847  |  0:00:06s
epoch 9  | loss: 1.08836 | eval_custom_logloss: 8.4888  |  0:00:07s
epoch 10 | loss: 0.87869 | eval_custom_logloss: 6.35933 |  0:00:08s
epoch 11 | loss: 0.82903 | eval_custom_logloss: 6.77113 |  0:00:08s
epoch 12 | loss: 0.76227 | eval_custom_logloss: 5.82089 |  0:00:09s
epoch 13 | loss: 0.71432 | eval_custom_logloss: 5.00581 |  0:00:10s
epoch 14 | loss: 0.72145 | eval_custom_logloss: 7.42186 |  0:00:11s
epoch 15 | loss: 0.7125  | eval_custom_logloss: 7.62527 |  0:00:11s
epoch 16 | loss: 0.71672 | eval_custom_logloss: 4.74661 |  0:00:12s
epoch 17 | loss: 0.69636 | eval_custom_logloss: 5.03755 |  0:00:13s
epoch 18 | loss: 0.65287 | eval_custom_logloss: 5.7285  |  0:00:14s
epoch 19 | loss: 0.65564 | eval_custom_logloss: 6.29074 |  0:00:14s
epoch 20 | loss: 0.6234  | eval_custom_logloss: 4.88488 |  0:00:15s
epoch 21 | loss: 0.61811 | eval_custom_logloss: 5.77584 |  0:00:16s
epoch 22 | loss: 0.55866 | eval_custom_logloss: 5.34276 |  0:00:16s
epoch 23 | loss: 0.59916 | eval_custom_logloss: 5.98255 |  0:00:17s
epoch 24 | loss: 0.59203 | eval_custom_logloss: 3.42485 |  0:00:18s
epoch 25 | loss: 0.58157 | eval_custom_logloss: 2.86675 |  0:00:19s
epoch 26 | loss: 0.60139 | eval_custom_logloss: 4.25142 |  0:00:19s
epoch 27 | loss: 0.58556 | eval_custom_logloss: 2.97754 |  0:00:20s
epoch 28 | loss: 0.56453 | eval_custom_logloss: 3.20787 |  0:00:21s
epoch 29 | loss: 0.58851 | eval_custom_logloss: 3.99198 |  0:00:22s
epoch 30 | loss: 0.55617 | eval_custom_logloss: 5.44749 |  0:00:22s
epoch 31 | loss: 0.5418  | eval_custom_logloss: 4.00268 |  0:00:23s
epoch 32 | loss: 0.55218 | eval_custom_logloss: 4.60303 |  0:00:24s
epoch 33 | loss: 0.53789 | eval_custom_logloss: 5.35638 |  0:00:24s
epoch 34 | loss: 0.56131 | eval_custom_logloss: 5.29117 |  0:00:25s
epoch 35 | loss: 0.50963 | eval_custom_logloss: 4.30909 |  0:00:26s
epoch 36 | loss: 0.4996  | eval_custom_logloss: 4.23815 |  0:00:27s
epoch 37 | loss: 0.51919 | eval_custom_logloss: 5.20863 |  0:00:27s
epoch 38 | loss: 0.48443 | eval_custom_logloss: 3.05824 |  0:00:28s
epoch 39 | loss: 0.4796  | eval_custom_logloss: 2.68177 |  0:00:29s
epoch 40 | loss: 0.46331 | eval_custom_logloss: 2.39288 |  0:00:30s
epoch 41 | loss: 0.50457 | eval_custom_logloss: 1.74403 |  0:00:30s
epoch 42 | loss: 0.45858 | eval_custom_logloss: 2.49145 |  0:00:31s
epoch 43 | loss: 0.46603 | eval_custom_logloss: 2.32822 |  0:00:32s
epoch 44 | loss: 0.47013 | eval_custom_logloss: 1.81266 |  0:00:33s
epoch 45 | loss: 0.45155 | eval_custom_logloss: 2.63798 |  0:00:33s
epoch 46 | loss: 0.45861 | eval_custom_logloss: 1.65939 |  0:00:34s
epoch 47 | loss: 0.45231 | eval_custom_logloss: 1.8334  |  0:00:35s
epoch 48 | loss: 0.46427 | eval_custom_logloss: 1.52517 |  0:00:36s
epoch 49 | loss: 0.44454 | eval_custom_logloss: 1.76195 |  0:00:36s
epoch 50 | loss: 0.46228 | eval_custom_logloss: 1.56006 |  0:00:37s
epoch 51 | loss: 0.4547  | eval_custom_logloss: 1.7479  |  0:00:38s
epoch 52 | loss: 0.42727 | eval_custom_logloss: 1.33999 |  0:00:39s
epoch 53 | loss: 0.42762 | eval_custom_logloss: 1.87551 |  0:00:39s
epoch 54 | loss: 0.46938 | eval_custom_logloss: 2.33463 |  0:00:40s
epoch 55 | loss: 0.4405  | eval_custom_logloss: 1.72964 |  0:00:41s
epoch 56 | loss: 0.44116 | eval_custom_logloss: 1.5936  |  0:00:41s
epoch 57 | loss: 0.41273 | eval_custom_logloss: 1.38465 |  0:00:42s
epoch 58 | loss: 0.39955 | eval_custom_logloss: 1.27104 |  0:00:43s
epoch 59 | loss: 0.42697 | eval_custom_logloss: 1.49757 |  0:00:44s
epoch 60 | loss: 0.43983 | eval_custom_logloss: 1.54158 |  0:00:44s
epoch 61 | loss: 0.42451 | eval_custom_logloss: 1.33265 |  0:00:45s
epoch 62 | loss: 0.42232 | eval_custom_logloss: 1.7138  |  0:00:46s
epoch 63 | loss: 0.41257 | eval_custom_logloss: 1.34916 |  0:00:47s
epoch 64 | loss: 0.40786 | eval_custom_logloss: 1.19789 |  0:00:47s
epoch 65 | loss: 0.41732 | eval_custom_logloss: 1.24936 |  0:00:48s
epoch 66 | loss: 0.43928 | eval_custom_logloss: 0.97044 |  0:00:49s
epoch 67 | loss: 0.41097 | eval_custom_logloss: 0.89725 |  0:00:49s
epoch 68 | loss: 0.44565 | eval_custom_logloss: 0.88433 |  0:00:50s
epoch 69 | loss: 0.38222 | eval_custom_logloss: 0.93736 |  0:00:51s
epoch 70 | loss: 0.36622 | eval_custom_logloss: 1.07071 |  0:00:52s
epoch 71 | loss: 0.36192 | eval_custom_logloss: 1.03225 |  0:00:52s
epoch 72 | loss: 0.39493 | eval_custom_logloss: 1.0233  |  0:00:53s
epoch 73 | loss: 0.37484 | eval_custom_logloss: 1.06298 |  0:00:54s
epoch 74 | loss: 0.37128 | eval_custom_logloss: 1.3542  |  0:00:55s
epoch 75 | loss: 0.35286 | eval_custom_logloss: 1.17761 |  0:00:55s
epoch 76 | loss: 0.32971 | eval_custom_logloss: 1.14566 |  0:00:56s
epoch 77 | loss: 0.31905 | eval_custom_logloss: 1.44191 |  0:00:57s
epoch 78 | loss: 0.32231 | eval_custom_logloss: 1.80036 |  0:00:58s
epoch 79 | loss: 0.3496  | eval_custom_logloss: 1.20881 |  0:00:58s
epoch 80 | loss: 0.35239 | eval_custom_logloss: 1.08977 |  0:00:59s
epoch 81 | loss: 0.35766 | eval_custom_logloss: 0.98255 |  0:01:00s
epoch 82 | loss: 0.32573 | eval_custom_logloss: 1.04693 |  0:01:00s
epoch 83 | loss: 0.32157 | eval_custom_logloss: 1.04981 |  0:01:01s
epoch 84 | loss: 0.32911 | eval_custom_logloss: 0.9679  |  0:01:02s
epoch 85 | loss: 0.30914 | eval_custom_logloss: 0.98628 |  0:01:03s
epoch 86 | loss: 0.33241 | eval_custom_logloss: 1.04675 |  0:01:03s
epoch 87 | loss: 0.35433 | eval_custom_logloss: 1.16984 |  0:01:04s
epoch 88 | loss: 0.34726 | eval_custom_logloss: 1.32765 |  0:01:05s

Early stopping occurred at epoch 88 with best_epoch = 68 and best_eval_custom_logloss = 0.88433
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8715, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 4.18142 | eval_custom_logloss: 8.74722 |  0:00:00s
epoch 1  | loss: 1.3656  | eval_custom_logloss: 9.43484 |  0:00:01s
epoch 2  | loss: 1.13919 | eval_custom_logloss: 8.89114 |  0:00:02s
epoch 3  | loss: 0.98902 | eval_custom_logloss: 7.97409 |  0:00:02s
epoch 4  | loss: 0.92629 | eval_custom_logloss: 7.64336 |  0:00:03s
epoch 5  | loss: 1.04869 | eval_custom_logloss: 8.10961 |  0:00:04s
epoch 6  | loss: 0.98745 | eval_custom_logloss: 7.17944 |  0:00:05s
epoch 7  | loss: 0.87931 | eval_custom_logloss: 5.8516  |  0:00:05s
epoch 8  | loss: 0.93719 | eval_custom_logloss: 6.62677 |  0:00:06s
epoch 9  | loss: 0.80332 | eval_custom_logloss: 4.41375 |  0:00:07s
epoch 10 | loss: 0.81676 | eval_custom_logloss: 7.31098 |  0:00:08s
epoch 11 | loss: 0.83749 | eval_custom_logloss: 6.66127 |  0:00:08s
epoch 12 | loss: 0.81091 | eval_custom_logloss: 4.87529 |  0:00:09s
epoch 13 | loss: 0.7022  | eval_custom_logloss: 6.73229 |  0:00:10s
epoch 14 | loss: 0.75391 | eval_custom_logloss: 4.70942 |  0:00:10s
epoch 15 | loss: 0.75104 | eval_custom_logloss: 5.72301 |  0:00:11s
epoch 16 | loss: 0.70282 | eval_custom_logloss: 5.92142 |  0:00:12s
epoch 17 | loss: 0.68366 | eval_custom_logloss: 4.98063 |  0:00:13s
epoch 18 | loss: 0.68076 | eval_custom_logloss: 6.00709 |  0:00:13s
epoch 19 | loss: 0.6355  | eval_custom_logloss: 4.66111 |  0:00:14s
epoch 20 | loss: 0.60366 | eval_custom_logloss: 4.27199 |  0:00:15s
epoch 21 | loss: 0.65933 | eval_custom_logloss: 3.62534 |  0:00:16s
epoch 22 | loss: 0.63392 | eval_custom_logloss: 3.57708 |  0:00:16s
epoch 23 | loss: 0.61598 | eval_custom_logloss: 2.96492 |  0:00:17s
epoch 24 | loss: 0.61931 | eval_custom_logloss: 5.6754  |  0:00:18s
epoch 25 | loss: 0.6165  | eval_custom_logloss: 5.44961 |  0:00:19s
epoch 26 | loss: 0.57785 | eval_custom_logloss: 4.45563 |  0:00:19s
epoch 27 | loss: 0.56428 | eval_custom_logloss: 4.26971 |  0:00:20s
epoch 28 | loss: 0.59359 | eval_custom_logloss: 3.45353 |  0:00:21s
epoch 29 | loss: 0.65547 | eval_custom_logloss: 3.62549 |  0:00:21s
epoch 30 | loss: 0.6054  | eval_custom_logloss: 2.45035 |  0:00:22s
epoch 31 | loss: 0.62237 | eval_custom_logloss: 2.64773 |  0:00:23s
epoch 32 | loss: 0.62962 | eval_custom_logloss: 2.82985 |  0:00:24s
epoch 33 | loss: 0.63547 | eval_custom_logloss: 3.08395 |  0:00:24s
epoch 34 | loss: 0.61653 | eval_custom_logloss: 3.00554 |  0:00:25s
epoch 35 | loss: 0.60891 | eval_custom_logloss: 2.26522 |  0:00:26s
epoch 36 | loss: 0.62036 | eval_custom_logloss: 2.17333 |  0:00:27s
epoch 37 | loss: 0.601   | eval_custom_logloss: 2.65416 |  0:00:27s
epoch 38 | loss: 0.59998 | eval_custom_logloss: 2.73055 |  0:00:28s
epoch 39 | loss: 0.60921 | eval_custom_logloss: 2.42986 |  0:00:29s
epoch 40 | loss: 0.61059 | eval_custom_logloss: 2.60076 |  0:00:29s
epoch 41 | loss: 0.62439 | eval_custom_logloss: 3.68659 |  0:00:30s
epoch 42 | loss: 0.60266 | eval_custom_logloss: 3.05067 |  0:00:31s
epoch 43 | loss: 0.56969 | eval_custom_logloss: 2.70521 |  0:00:32s
epoch 44 | loss: 0.55575 | eval_custom_logloss: 2.16424 |  0:00:32s
epoch 45 | loss: 0.57713 | eval_custom_logloss: 2.19874 |  0:00:33s
epoch 46 | loss: 0.5598  | eval_custom_logloss: 1.93436 |  0:00:34s
epoch 47 | loss: 0.57977 | eval_custom_logloss: 2.57034 |  0:00:35s
epoch 48 | loss: 0.56816 | eval_custom_logloss: 1.83782 |  0:00:35s
epoch 49 | loss: 0.5774  | eval_custom_logloss: 2.31063 |  0:00:36s
epoch 50 | loss: 0.55593 | eval_custom_logloss: 2.44666 |  0:00:37s
epoch 51 | loss: 0.51704 | eval_custom_logloss: 1.94729 |  0:00:37s
epoch 52 | loss: 0.54452 | eval_custom_logloss: 2.90962 |  0:00:38s
epoch 53 | loss: 0.58315 | eval_custom_logloss: 2.70189 |  0:00:39s
epoch 54 | loss: 0.58907 | eval_custom_logloss: 3.22279 |  0:00:40s
epoch 55 | loss: 0.56801 | eval_custom_logloss: 1.94496 |  0:00:40s
epoch 56 | loss: 0.56828 | eval_custom_logloss: 1.86445 |  0:00:41s
epoch 57 | loss: 0.55666 | eval_custom_logloss: 1.67031 |  0:00:42s
epoch 58 | loss: 0.55663 | eval_custom_logloss: 1.70732 |  0:00:43s
epoch 59 | loss: 0.53335 | eval_custom_logloss: 2.10721 |  0:00:43s
epoch 60 | loss: 0.51352 | eval_custom_logloss: 2.12164 |  0:00:44s
epoch 61 | loss: 0.56    | eval_custom_logloss: 3.03096 |  0:00:45s
epoch 62 | loss: 0.56263 | eval_custom_logloss: 2.3031  |  0:00:46s
epoch 63 | loss: 0.5359  | eval_custom_logloss: 2.69183 |  0:00:46s
epoch 64 | loss: 0.54845 | eval_custom_logloss: 1.83476 |  0:00:47s
epoch 65 | loss: 0.54955 | eval_custom_logloss: 2.02649 |  0:00:48s
epoch 66 | loss: 0.52857 | eval_custom_logloss: 2.886   |  0:00:49s
epoch 67 | loss: 0.53578 | eval_custom_logloss: 1.85469 |  0:00:49s
epoch 68 | loss: 0.54839 | eval_custom_logloss: 2.09403 |  0:00:50s
epoch 69 | loss: 0.52819 | eval_custom_logloss: 2.17455 |  0:00:51s
epoch 70 | loss: 0.5117  | eval_custom_logloss: 2.25592 |  0:00:51s
epoch 71 | loss: 0.54189 | eval_custom_logloss: 2.01796 |  0:00:52s
epoch 72 | loss: 0.54939 | eval_custom_logloss: 1.87349 |  0:00:53s
epoch 73 | loss: 0.53293 | eval_custom_logloss: 1.90694 |  0:00:54s
epoch 74 | loss: 0.55192 | eval_custom_logloss: 1.82562 |  0:00:54s
epoch 75 | loss: 0.5398  | eval_custom_logloss: 1.68566 |  0:00:55s
epoch 76 | loss: 0.55612 | eval_custom_logloss: 1.28847 |  0:00:56s
epoch 77 | loss: 0.52865 | eval_custom_logloss: 1.3309  |  0:00:57s
epoch 78 | loss: 0.53147 | eval_custom_logloss: 1.24497 |  0:00:57s
epoch 79 | loss: 0.53332 | eval_custom_logloss: 1.48136 |  0:00:58s
epoch 80 | loss: 0.5411  | eval_custom_logloss: 1.21653 |  0:00:59s
epoch 81 | loss: 0.54669 | eval_custom_logloss: 1.52256 |  0:00:59s
epoch 82 | loss: 0.5442  | eval_custom_logloss: 1.52525 |  0:01:00s
epoch 83 | loss: 0.52928 | eval_custom_logloss: 1.59322 |  0:01:01s
epoch 84 | loss: 0.54347 | eval_custom_logloss: 2.2762  |  0:01:02s
epoch 85 | loss: 0.52991 | eval_custom_logloss: 2.87031 |  0:01:02s
epoch 86 | loss: 0.50244 | eval_custom_logloss: 2.7509  |  0:01:03s
epoch 87 | loss: 0.49558 | eval_custom_logloss: 2.12187 |  0:01:04s
epoch 88 | loss: 0.51226 | eval_custom_logloss: 2.7719  |  0:01:04s
epoch 89 | loss: 0.51428 | eval_custom_logloss: 2.56706 |  0:01:05s
epoch 90 | loss: 0.50102 | eval_custom_logloss: 2.48715 |  0:01:06s
epoch 91 | loss: 0.5163  | eval_custom_logloss: 2.48551 |  0:01:07s
epoch 92 | loss: 0.51165 | eval_custom_logloss: 1.93965 |  0:01:07s
epoch 93 | loss: 0.50713 | eval_custom_logloss: 2.14129 |  0:01:08s
epoch 94 | loss: 0.54146 | eval_custom_logloss: 1.59839 |  0:01:09s
epoch 95 | loss: 0.54709 | eval_custom_logloss: 1.25818 |  0:01:10s
epoch 96 | loss: 0.55512 | eval_custom_logloss: 0.90399 |  0:01:10s
epoch 97 | loss: 0.54316 | eval_custom_logloss: 1.11606 |  0:01:11s
epoch 98 | loss: 0.51642 | eval_custom_logloss: 1.1112  |  0:01:12s
epoch 99 | loss: 0.52477 | eval_custom_logloss: 1.3904  |  0:01:12s
Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_eval_custom_logloss = 0.90399
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.88775, 'Log Loss - std': 0.016249999999999987} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 4.12818 | eval_custom_logloss: 9.40926 |  0:00:00s
epoch 1  | loss: 1.64242 | eval_custom_logloss: 8.81272 |  0:00:01s
epoch 2  | loss: 1.25126 | eval_custom_logloss: 7.13039 |  0:00:02s
epoch 3  | loss: 1.05775 | eval_custom_logloss: 7.70255 |  0:00:03s
epoch 4  | loss: 1.1429  | eval_custom_logloss: 6.85922 |  0:00:03s
epoch 5  | loss: 1.39938 | eval_custom_logloss: 8.13336 |  0:00:04s
epoch 6  | loss: 1.37864 | eval_custom_logloss: 4.78064 |  0:00:05s
epoch 7  | loss: 1.00115 | eval_custom_logloss: 6.27471 |  0:00:06s
epoch 8  | loss: 0.8625  | eval_custom_logloss: 5.79123 |  0:00:06s
epoch 9  | loss: 0.79548 | eval_custom_logloss: 5.49074 |  0:00:07s
epoch 10 | loss: 0.80707 | eval_custom_logloss: 4.69286 |  0:00:08s
epoch 11 | loss: 0.78516 | eval_custom_logloss: 5.59459 |  0:00:08s
epoch 12 | loss: 0.86648 | eval_custom_logloss: 9.11076 |  0:00:09s
epoch 13 | loss: 0.81433 | eval_custom_logloss: 5.37414 |  0:00:10s
epoch 14 | loss: 0.7897  | eval_custom_logloss: 6.90289 |  0:00:11s
epoch 15 | loss: 0.78916 | eval_custom_logloss: 4.80019 |  0:00:11s
epoch 16 | loss: 0.73256 | eval_custom_logloss: 7.04333 |  0:00:12s
epoch 17 | loss: 0.7401  | eval_custom_logloss: 4.54354 |  0:00:13s
epoch 18 | loss: 0.7088  | eval_custom_logloss: 6.11753 |  0:00:14s
epoch 19 | loss: 0.70783 | eval_custom_logloss: 6.71805 |  0:00:14s
epoch 20 | loss: 0.66171 | eval_custom_logloss: 5.70964 |  0:00:15s
epoch 21 | loss: 0.6638  | eval_custom_logloss: 5.16835 |  0:00:16s
epoch 22 | loss: 0.67371 | eval_custom_logloss: 5.03021 |  0:00:16s
epoch 23 | loss: 0.68637 | eval_custom_logloss: 5.52135 |  0:00:17s
epoch 24 | loss: 0.67252 | eval_custom_logloss: 6.3146  |  0:00:18s
epoch 25 | loss: 0.64963 | eval_custom_logloss: 5.2554  |  0:00:19s
epoch 26 | loss: 0.67775 | eval_custom_logloss: 3.83374 |  0:00:19s
epoch 27 | loss: 0.75822 | eval_custom_logloss: 5.93969 |  0:00:20s
epoch 28 | loss: 0.62133 | eval_custom_logloss: 3.66272 |  0:00:21s
epoch 29 | loss: 0.62948 | eval_custom_logloss: 4.55491 |  0:00:21s
epoch 30 | loss: 0.61276 | eval_custom_logloss: 4.53662 |  0:00:22s
epoch 31 | loss: 0.6115  | eval_custom_logloss: 3.37721 |  0:00:23s
epoch 32 | loss: 0.62347 | eval_custom_logloss: 3.89441 |  0:00:24s
epoch 33 | loss: 0.62567 | eval_custom_logloss: 3.46298 |  0:00:24s
epoch 34 | loss: 0.64019 | eval_custom_logloss: 3.81499 |  0:00:25s
epoch 35 | loss: 0.62205 | eval_custom_logloss: 3.53195 |  0:00:26s
epoch 36 | loss: 0.5789  | eval_custom_logloss: 2.77335 |  0:00:27s
epoch 37 | loss: 0.56905 | eval_custom_logloss: 2.44492 |  0:00:27s
epoch 38 | loss: 0.60092 | eval_custom_logloss: 2.87878 |  0:00:28s
epoch 39 | loss: 0.5521  | eval_custom_logloss: 4.37833 |  0:00:29s
epoch 40 | loss: 0.55484 | eval_custom_logloss: 4.17859 |  0:00:30s
epoch 41 | loss: 0.55837 | eval_custom_logloss: 2.89768 |  0:00:30s
epoch 42 | loss: 0.54054 | eval_custom_logloss: 2.89866 |  0:00:31s
epoch 43 | loss: 0.54377 | eval_custom_logloss: 2.37794 |  0:00:32s
epoch 44 | loss: 0.53056 | eval_custom_logloss: 2.29404 |  0:00:33s
epoch 45 | loss: 0.52371 | eval_custom_logloss: 2.43714 |  0:00:33s
epoch 46 | loss: 0.5286  | eval_custom_logloss: 2.57078 |  0:00:34s
epoch 47 | loss: 0.52695 | eval_custom_logloss: 2.65781 |  0:00:35s
epoch 48 | loss: 0.50077 | eval_custom_logloss: 2.4208  |  0:00:36s
epoch 49 | loss: 0.51169 | eval_custom_logloss: 2.24052 |  0:00:36s
epoch 50 | loss: 0.52936 | eval_custom_logloss: 2.16346 |  0:00:37s
epoch 51 | loss: 0.52095 | eval_custom_logloss: 1.90835 |  0:00:38s
epoch 52 | loss: 0.5271  | eval_custom_logloss: 1.71031 |  0:00:38s
epoch 53 | loss: 0.50411 | eval_custom_logloss: 2.00022 |  0:00:39s
epoch 54 | loss: 0.54137 | eval_custom_logloss: 2.09317 |  0:00:40s
epoch 55 | loss: 0.57431 | eval_custom_logloss: 1.80982 |  0:00:41s
epoch 56 | loss: 0.59499 | eval_custom_logloss: 2.23363 |  0:00:41s
epoch 57 | loss: 0.57614 | eval_custom_logloss: 1.89626 |  0:00:42s
epoch 58 | loss: 0.53461 | eval_custom_logloss: 1.75427 |  0:00:43s
epoch 59 | loss: 0.59202 | eval_custom_logloss: 1.63698 |  0:00:44s
epoch 60 | loss: 0.56241 | eval_custom_logloss: 1.3506  |  0:00:44s
epoch 61 | loss: 0.54297 | eval_custom_logloss: 1.71768 |  0:00:45s
epoch 62 | loss: 0.54042 | eval_custom_logloss: 1.55482 |  0:00:46s
epoch 63 | loss: 0.53803 | eval_custom_logloss: 1.65826 |  0:00:46s
epoch 64 | loss: 0.54765 | eval_custom_logloss: 1.69172 |  0:00:47s
epoch 65 | loss: 0.53686 | eval_custom_logloss: 1.94778 |  0:00:48s
epoch 66 | loss: 0.54644 | eval_custom_logloss: 2.01264 |  0:00:49s
epoch 67 | loss: 0.51569 | eval_custom_logloss: 1.76101 |  0:00:49s
epoch 68 | loss: 0.51936 | eval_custom_logloss: 1.68709 |  0:00:50s
epoch 69 | loss: 0.54876 | eval_custom_logloss: 2.98179 |  0:00:51s
epoch 70 | loss: 0.51974 | eval_custom_logloss: 3.16929 |  0:00:51s
epoch 71 | loss: 0.52762 | eval_custom_logloss: 2.17984 |  0:00:52s
epoch 72 | loss: 0.5011  | eval_custom_logloss: 1.22003 |  0:00:53s
epoch 73 | loss: 0.48204 | eval_custom_logloss: 1.63752 |  0:00:54s
epoch 74 | loss: 0.50651 | eval_custom_logloss: 1.56959 |  0:00:54s
epoch 75 | loss: 0.46528 | eval_custom_logloss: 1.79862 |  0:00:55s
epoch 76 | loss: 0.46961 | eval_custom_logloss: 1.29119 |  0:00:56s
epoch 77 | loss: 0.45052 | eval_custom_logloss: 1.82009 |  0:00:57s
epoch 78 | loss: 0.45324 | eval_custom_logloss: 1.93591 |  0:00:57s
epoch 79 | loss: 0.45643 | eval_custom_logloss: 2.29877 |  0:00:58s
epoch 80 | loss: 0.44806 | eval_custom_logloss: 1.89224 |  0:00:59s
epoch 81 | loss: 0.44686 | eval_custom_logloss: 1.86062 |  0:00:59s
epoch 82 | loss: 0.42694 | eval_custom_logloss: 1.98976 |  0:01:00s
epoch 83 | loss: 0.43151 | eval_custom_logloss: 1.67521 |  0:01:01s
epoch 84 | loss: 0.40136 | eval_custom_logloss: 1.82243 |  0:01:02s
epoch 85 | loss: 0.42229 | eval_custom_logloss: 1.82881 |  0:01:02s
epoch 86 | loss: 0.44466 | eval_custom_logloss: 1.79508 |  0:01:03s
epoch 87 | loss: 0.46547 | eval_custom_logloss: 1.30876 |  0:01:04s
epoch 88 | loss: 0.44853 | eval_custom_logloss: 1.36119 |  0:01:05s
epoch 89 | loss: 0.42147 | eval_custom_logloss: 1.53993 |  0:01:05s
epoch 90 | loss: 0.41814 | eval_custom_logloss: 1.43293 |  0:01:06s
epoch 91 | loss: 0.39534 | eval_custom_logloss: 1.42817 |  0:01:07s
epoch 92 | loss: 0.40983 | eval_custom_logloss: 1.77661 |  0:01:07s

Early stopping occurred at epoch 92 with best_epoch = 72 and best_eval_custom_logloss = 1.22003
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9804, 'Log Loss - std': 0.13169695010389063} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.56159 | eval_custom_logloss: 8.2628  |  0:00:00s
epoch 1  | loss: 1.62395 | eval_custom_logloss: 8.70078 |  0:00:01s
epoch 2  | loss: 1.4675  | eval_custom_logloss: 8.43366 |  0:00:02s
epoch 3  | loss: 1.10965 | eval_custom_logloss: 8.26189 |  0:00:02s
epoch 4  | loss: 1.29406 | eval_custom_logloss: 5.81617 |  0:00:03s
epoch 5  | loss: 1.14492 | eval_custom_logloss: 7.3018  |  0:00:04s
epoch 6  | loss: 0.87744 | eval_custom_logloss: 7.95415 |  0:00:05s
epoch 7  | loss: 0.84424 | eval_custom_logloss: 5.04672 |  0:00:05s
epoch 8  | loss: 0.82522 | eval_custom_logloss: 5.52904 |  0:00:06s
epoch 9  | loss: 0.8514  | eval_custom_logloss: 5.16904 |  0:00:07s
epoch 10 | loss: 0.86562 | eval_custom_logloss: 6.59059 |  0:00:08s
epoch 11 | loss: 0.74731 | eval_custom_logloss: 6.29978 |  0:00:08s
epoch 12 | loss: 0.77745 | eval_custom_logloss: 6.21734 |  0:00:09s
epoch 13 | loss: 0.69519 | eval_custom_logloss: 4.72344 |  0:00:10s
epoch 14 | loss: 0.71767 | eval_custom_logloss: 5.81418 |  0:00:10s
epoch 15 | loss: 0.73363 | eval_custom_logloss: 6.55708 |  0:00:11s
epoch 16 | loss: 0.68944 | eval_custom_logloss: 6.10316 |  0:00:12s
epoch 17 | loss: 0.6594  | eval_custom_logloss: 7.4139  |  0:00:13s
epoch 18 | loss: 0.64065 | eval_custom_logloss: 7.72325 |  0:00:13s
epoch 19 | loss: 0.67463 | eval_custom_logloss: 8.02188 |  0:00:14s
epoch 20 | loss: 0.79301 | eval_custom_logloss: 7.68459 |  0:00:15s
epoch 21 | loss: 0.73891 | eval_custom_logloss: 7.06587 |  0:00:16s
epoch 22 | loss: 0.67536 | eval_custom_logloss: 5.33176 |  0:00:16s
epoch 23 | loss: 0.63525 | eval_custom_logloss: 4.98646 |  0:00:17s
epoch 24 | loss: 0.65144 | eval_custom_logloss: 4.85549 |  0:00:18s
epoch 25 | loss: 0.57877 | eval_custom_logloss: 4.56677 |  0:00:18s
epoch 26 | loss: 0.59441 | eval_custom_logloss: 4.39179 |  0:00:19s
epoch 27 | loss: 0.61331 | eval_custom_logloss: 4.47783 |  0:00:20s
epoch 28 | loss: 0.60268 | eval_custom_logloss: 3.33056 |  0:00:21s
epoch 29 | loss: 0.66968 | eval_custom_logloss: 4.13557 |  0:00:21s
epoch 30 | loss: 0.64105 | eval_custom_logloss: 3.26352 |  0:00:22s
epoch 31 | loss: 0.60954 | eval_custom_logloss: 3.19393 |  0:00:23s
epoch 32 | loss: 0.59813 | eval_custom_logloss: 3.96157 |  0:00:24s
epoch 33 | loss: 0.5594  | eval_custom_logloss: 3.93379 |  0:00:24s
epoch 34 | loss: 0.57308 | eval_custom_logloss: 3.78416 |  0:00:25s
epoch 35 | loss: 0.56177 | eval_custom_logloss: 2.92358 |  0:00:26s
epoch 36 | loss: 0.51492 | eval_custom_logloss: 2.2409  |  0:00:27s
epoch 37 | loss: 0.54697 | eval_custom_logloss: 4.00802 |  0:00:27s
epoch 38 | loss: 0.55455 | eval_custom_logloss: 2.9092  |  0:00:28s
epoch 39 | loss: 0.51437 | eval_custom_logloss: 2.45419 |  0:00:29s
epoch 40 | loss: 0.5071  | eval_custom_logloss: 1.89716 |  0:00:29s
epoch 41 | loss: 0.52806 | eval_custom_logloss: 1.76414 |  0:00:30s
epoch 42 | loss: 0.52707 | eval_custom_logloss: 1.97896 |  0:00:31s
epoch 43 | loss: 0.50817 | eval_custom_logloss: 1.89753 |  0:00:32s
epoch 44 | loss: 0.5026  | eval_custom_logloss: 2.27381 |  0:00:32s
epoch 45 | loss: 0.47862 | eval_custom_logloss: 1.59807 |  0:00:33s
epoch 46 | loss: 0.50467 | eval_custom_logloss: 1.42733 |  0:00:34s
epoch 47 | loss: 0.50161 | eval_custom_logloss: 1.42344 |  0:00:35s
epoch 48 | loss: 0.47466 | eval_custom_logloss: 1.53009 |  0:00:35s
epoch 49 | loss: 0.52731 | eval_custom_logloss: 1.75635 |  0:00:36s
epoch 50 | loss: 0.49037 | eval_custom_logloss: 1.85805 |  0:00:37s
epoch 51 | loss: 0.46344 | eval_custom_logloss: 1.39272 |  0:00:38s
epoch 52 | loss: 0.45171 | eval_custom_logloss: 1.34102 |  0:00:38s
epoch 53 | loss: 0.50023 | eval_custom_logloss: 1.49062 |  0:00:39s
epoch 54 | loss: 0.47736 | eval_custom_logloss: 1.21344 |  0:00:40s
epoch 55 | loss: 0.45834 | eval_custom_logloss: 1.50591 |  0:00:41s
epoch 56 | loss: 0.43962 | eval_custom_logloss: 1.36973 |  0:00:41s
epoch 57 | loss: 0.47694 | eval_custom_logloss: 1.42946 |  0:00:42s
epoch 58 | loss: 0.47334 | eval_custom_logloss: 1.4791  |  0:00:43s
epoch 59 | loss: 0.48514 | eval_custom_logloss: 1.67644 |  0:00:43s
epoch 60 | loss: 0.53925 | eval_custom_logloss: 1.72019 |  0:00:44s
epoch 61 | loss: 0.53218 | eval_custom_logloss: 1.54973 |  0:00:45s
epoch 62 | loss: 0.50847 | eval_custom_logloss: 1.96536 |  0:00:46s
epoch 63 | loss: 0.50119 | eval_custom_logloss: 2.1458  |  0:00:46s
epoch 64 | loss: 0.46415 | eval_custom_logloss: 2.26056 |  0:00:47s
epoch 65 | loss: 0.47242 | eval_custom_logloss: 1.94907 |  0:00:48s
epoch 66 | loss: 0.44425 | eval_custom_logloss: 1.96695 |  0:00:48s
epoch 67 | loss: 0.49869 | eval_custom_logloss: 1.7734  |  0:00:49s
epoch 68 | loss: 0.50279 | eval_custom_logloss: 2.8794  |  0:00:50s
epoch 69 | loss: 0.47051 | eval_custom_logloss: 3.0005  |  0:00:51s
epoch 70 | loss: 0.48698 | eval_custom_logloss: 2.25701 |  0:00:51s
epoch 71 | loss: 0.47822 | eval_custom_logloss: 2.41237 |  0:00:52s
epoch 72 | loss: 0.44917 | eval_custom_logloss: 2.45853 |  0:00:53s
epoch 73 | loss: 0.44727 | eval_custom_logloss: 2.93981 |  0:00:54s
epoch 74 | loss: 0.49616 | eval_custom_logloss: 2.32014 |  0:00:54s

Early stopping occurred at epoch 74 with best_epoch = 54 and best_eval_custom_logloss = 1.21344
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.027725, 'Log Loss - std': 0.14045295253215573} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax', 'n_a': 29, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 4.95733 | eval_custom_logloss: 9.26773 |  0:00:00s
epoch 1  | loss: 1.70032 | eval_custom_logloss: 8.93882 |  0:00:01s
epoch 2  | loss: 1.53806 | eval_custom_logloss: 7.83306 |  0:00:02s
epoch 3  | loss: 1.31931 | eval_custom_logloss: 7.05758 |  0:00:02s
epoch 4  | loss: 1.02626 | eval_custom_logloss: 7.12888 |  0:00:03s
epoch 5  | loss: 0.89821 | eval_custom_logloss: 7.15734 |  0:00:04s
epoch 6  | loss: 1.03493 | eval_custom_logloss: 9.93684 |  0:00:05s
epoch 7  | loss: 1.04674 | eval_custom_logloss: 6.90131 |  0:00:05s
epoch 8  | loss: 0.95611 | eval_custom_logloss: 8.51192 |  0:00:06s
epoch 9  | loss: 0.99536 | eval_custom_logloss: 5.9421  |  0:00:07s
epoch 10 | loss: 1.18344 | eval_custom_logloss: 7.27237 |  0:00:08s
epoch 11 | loss: 1.20564 | eval_custom_logloss: 8.23756 |  0:00:08s
epoch 12 | loss: 0.99554 | eval_custom_logloss: 6.37194 |  0:00:09s
epoch 13 | loss: 0.85143 | eval_custom_logloss: 6.45696 |  0:00:10s
epoch 14 | loss: 0.79898 | eval_custom_logloss: 4.81595 |  0:00:11s
epoch 15 | loss: 0.75846 | eval_custom_logloss: 4.93163 |  0:00:11s
epoch 16 | loss: 0.74173 | eval_custom_logloss: 4.00359 |  0:00:12s
epoch 17 | loss: 0.72695 | eval_custom_logloss: 4.20801 |  0:00:13s
epoch 18 | loss: 0.77137 | eval_custom_logloss: 6.38565 |  0:00:14s
epoch 19 | loss: 0.74567 | eval_custom_logloss: 6.33448 |  0:00:14s
epoch 20 | loss: 0.68703 | eval_custom_logloss: 6.58825 |  0:00:15s
epoch 21 | loss: 0.69417 | eval_custom_logloss: 7.85991 |  0:00:16s
epoch 22 | loss: 0.6733  | eval_custom_logloss: 4.14885 |  0:00:16s
epoch 23 | loss: 0.67592 | eval_custom_logloss: 3.58076 |  0:00:17s
epoch 24 | loss: 0.66793 | eval_custom_logloss: 2.38598 |  0:00:18s
epoch 25 | loss: 0.63646 | eval_custom_logloss: 2.66696 |  0:00:19s
epoch 26 | loss: 0.62415 | eval_custom_logloss: 1.93836 |  0:00:19s
epoch 27 | loss: 0.61055 | eval_custom_logloss: 5.41474 |  0:00:20s
epoch 28 | loss: 0.57222 | eval_custom_logloss: 3.20302 |  0:00:21s
epoch 29 | loss: 0.55963 | eval_custom_logloss: 2.47421 |  0:00:22s
epoch 30 | loss: 0.55936 | eval_custom_logloss: 2.54249 |  0:00:22s
epoch 31 | loss: 0.55312 | eval_custom_logloss: 2.36954 |  0:00:23s
epoch 32 | loss: 0.53364 | eval_custom_logloss: 2.62361 |  0:00:24s
epoch 33 | loss: 0.57383 | eval_custom_logloss: 1.87649 |  0:00:24s
epoch 34 | loss: 0.56129 | eval_custom_logloss: 1.67144 |  0:00:25s
epoch 35 | loss: 0.53953 | eval_custom_logloss: 1.80862 |  0:00:26s
epoch 36 | loss: 0.50963 | eval_custom_logloss: 1.86926 |  0:00:27s
epoch 37 | loss: 0.51575 | eval_custom_logloss: 1.51861 |  0:00:27s
epoch 38 | loss: 0.51416 | eval_custom_logloss: 1.19165 |  0:00:28s
epoch 39 | loss: 0.48941 | eval_custom_logloss: 1.20184 |  0:00:29s
epoch 40 | loss: 0.47878 | eval_custom_logloss: 0.96068 |  0:00:30s
epoch 41 | loss: 0.4869  | eval_custom_logloss: 0.85755 |  0:00:31s
epoch 42 | loss: 0.47783 | eval_custom_logloss: 0.87236 |  0:00:31s
epoch 43 | loss: 0.50686 | eval_custom_logloss: 0.98865 |  0:00:32s
epoch 44 | loss: 0.48211 | eval_custom_logloss: 1.32472 |  0:00:33s
epoch 45 | loss: 0.46474 | eval_custom_logloss: 1.64616 |  0:00:33s
epoch 46 | loss: 0.50546 | eval_custom_logloss: 1.06306 |  0:00:34s
epoch 47 | loss: 0.48041 | eval_custom_logloss: 1.63344 |  0:00:35s
epoch 48 | loss: 0.44295 | eval_custom_logloss: 1.68007 |  0:00:36s
epoch 49 | loss: 0.45966 | eval_custom_logloss: 1.37034 |  0:00:36s
epoch 50 | loss: 0.4749  | eval_custom_logloss: 2.2414  |  0:00:37s
epoch 51 | loss: 0.44907 | eval_custom_logloss: 1.701   |  0:00:38s
epoch 52 | loss: 0.40564 | eval_custom_logloss: 1.88621 |  0:00:38s
epoch 53 | loss: 0.36754 | eval_custom_logloss: 1.82746 |  0:00:39s
epoch 54 | loss: 0.40541 | eval_custom_logloss: 1.84634 |  0:00:40s
epoch 55 | loss: 0.37985 | eval_custom_logloss: 1.61485 |  0:00:41s
epoch 56 | loss: 0.39412 | eval_custom_logloss: 1.79224 |  0:00:41s
epoch 57 | loss: 0.36166 | eval_custom_logloss: 1.58162 |  0:00:42s
epoch 58 | loss: 0.36445 | eval_custom_logloss: 1.50805 |  0:00:43s
epoch 59 | loss: 0.37903 | eval_custom_logloss: 1.67031 |  0:00:43s
epoch 60 | loss: 0.35643 | eval_custom_logloss: 1.92216 |  0:00:44s
epoch 61 | loss: 0.31872 | eval_custom_logloss: 1.97457 |  0:00:45s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 0.85755
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.99056, 'Log Loss - std': 0.1459677169787895} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 0.99056 and parameters: {'n_d': 29, 'n_steps': 8, 'gamma': 1.8978829687769556, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0017062241284378822, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax', 'n_a': 40, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.35702 | eval_custom_logloss: 7.05362 |  0:00:00s
epoch 1  | loss: 1.76311 | eval_custom_logloss: 4.79855 |  0:00:01s
epoch 2  | loss: 1.27887 | eval_custom_logloss: 6.4142  |  0:00:02s
epoch 3  | loss: 1.40305 | eval_custom_logloss: 5.88653 |  0:00:02s
epoch 4  | loss: 1.14312 | eval_custom_logloss: 5.29118 |  0:00:03s
epoch 5  | loss: 1.18102 | eval_custom_logloss: 4.79104 |  0:00:04s
epoch 6  | loss: 1.72939 | eval_custom_logloss: 4.63225 |  0:00:04s
epoch 7  | loss: 1.04334 | eval_custom_logloss: 3.51577 |  0:00:05s
epoch 8  | loss: 0.88033 | eval_custom_logloss: 1.89781 |  0:00:06s
epoch 9  | loss: 0.80864 | eval_custom_logloss: 2.4916  |  0:00:06s
epoch 10 | loss: 0.92576 | eval_custom_logloss: 1.96663 |  0:00:07s
epoch 11 | loss: 0.82264 | eval_custom_logloss: 1.81339 |  0:00:08s
epoch 12 | loss: 0.74453 | eval_custom_logloss: 2.00986 |  0:00:08s
epoch 13 | loss: 0.72409 | eval_custom_logloss: 1.37644 |  0:00:09s
epoch 14 | loss: 0.68929 | eval_custom_logloss: 2.14987 |  0:00:10s
epoch 15 | loss: 0.67914 | eval_custom_logloss: 1.58899 |  0:00:10s
epoch 16 | loss: 0.74156 | eval_custom_logloss: 1.35816 |  0:00:11s
epoch 17 | loss: 0.66073 | eval_custom_logloss: 1.6792  |  0:00:12s
epoch 18 | loss: 0.65266 | eval_custom_logloss: 1.49366 |  0:00:12s
epoch 19 | loss: 0.63395 | eval_custom_logloss: 1.45672 |  0:00:13s
epoch 20 | loss: 0.60129 | eval_custom_logloss: 1.52347 |  0:00:13s
epoch 21 | loss: 0.65581 | eval_custom_logloss: 1.71045 |  0:00:14s
epoch 22 | loss: 0.61097 | eval_custom_logloss: 1.64831 |  0:00:15s
epoch 23 | loss: 0.61445 | eval_custom_logloss: 2.24149 |  0:00:15s
epoch 24 | loss: 0.59283 | eval_custom_logloss: 2.94955 |  0:00:16s
epoch 25 | loss: 0.59077 | eval_custom_logloss: 1.18767 |  0:00:17s
epoch 26 | loss: 0.57382 | eval_custom_logloss: 0.84881 |  0:00:17s
epoch 27 | loss: 0.55221 | eval_custom_logloss: 1.17144 |  0:00:18s
epoch 28 | loss: 0.54474 | eval_custom_logloss: 0.78319 |  0:00:19s
epoch 29 | loss: 0.51095 | eval_custom_logloss: 0.82384 |  0:00:19s
epoch 30 | loss: 0.55074 | eval_custom_logloss: 1.16255 |  0:00:20s
epoch 31 | loss: 0.58544 | eval_custom_logloss: 0.73902 |  0:00:21s
epoch 32 | loss: 0.57313 | eval_custom_logloss: 0.76238 |  0:00:21s
epoch 33 | loss: 0.56567 | eval_custom_logloss: 0.75857 |  0:00:22s
epoch 34 | loss: 0.52721 | eval_custom_logloss: 0.90565 |  0:00:23s
epoch 35 | loss: 0.51156 | eval_custom_logloss: 1.00764 |  0:00:23s
epoch 36 | loss: 0.52605 | eval_custom_logloss: 0.86966 |  0:00:24s
epoch 37 | loss: 0.48985 | eval_custom_logloss: 0.71466 |  0:00:25s
epoch 38 | loss: 0.49779 | eval_custom_logloss: 0.80491 |  0:00:25s
epoch 39 | loss: 0.47028 | eval_custom_logloss: 0.86372 |  0:00:26s
epoch 40 | loss: 0.4693  | eval_custom_logloss: 0.77516 |  0:00:27s
epoch 41 | loss: 0.48776 | eval_custom_logloss: 0.6684  |  0:00:27s
epoch 42 | loss: 0.46163 | eval_custom_logloss: 0.89864 |  0:00:28s
epoch 43 | loss: 0.45997 | eval_custom_logloss: 0.87946 |  0:00:29s
epoch 44 | loss: 0.45744 | eval_custom_logloss: 0.77006 |  0:00:29s
epoch 45 | loss: 0.43618 | eval_custom_logloss: 0.81765 |  0:00:30s
epoch 46 | loss: 0.42209 | eval_custom_logloss: 1.31726 |  0:00:31s
epoch 47 | loss: 0.46182 | eval_custom_logloss: 0.95635 |  0:00:31s
epoch 48 | loss: 0.44714 | eval_custom_logloss: 1.40257 |  0:00:32s
epoch 49 | loss: 0.46167 | eval_custom_logloss: 1.31749 |  0:00:33s
epoch 50 | loss: 0.51594 | eval_custom_logloss: 1.26144 |  0:00:33s
epoch 51 | loss: 0.48647 | eval_custom_logloss: 1.09836 |  0:00:34s
epoch 52 | loss: 0.49867 | eval_custom_logloss: 1.102   |  0:00:35s
epoch 53 | loss: 0.47402 | eval_custom_logloss: 1.22117 |  0:00:35s
epoch 54 | loss: 0.47038 | eval_custom_logloss: 0.72437 |  0:00:36s
epoch 55 | loss: 0.45729 | eval_custom_logloss: 0.70742 |  0:00:37s
epoch 56 | loss: 0.44532 | eval_custom_logloss: 0.74237 |  0:00:37s
epoch 57 | loss: 0.45762 | eval_custom_logloss: 0.73489 |  0:00:38s
epoch 58 | loss: 0.44149 | eval_custom_logloss: 0.64674 |  0:00:38s
epoch 59 | loss: 0.44288 | eval_custom_logloss: 0.64799 |  0:00:39s
epoch 60 | loss: 0.42802 | eval_custom_logloss: 0.74223 |  0:00:40s
epoch 61 | loss: 0.4187  | eval_custom_logloss: 0.57075 |  0:00:40s
epoch 62 | loss: 0.39742 | eval_custom_logloss: 0.82195 |  0:00:41s
epoch 63 | loss: 0.39669 | eval_custom_logloss: 0.63411 |  0:00:42s
epoch 64 | loss: 0.40915 | eval_custom_logloss: 0.58553 |  0:00:42s
epoch 65 | loss: 0.41624 | eval_custom_logloss: 0.64631 |  0:00:43s
epoch 66 | loss: 0.41046 | eval_custom_logloss: 0.55653 |  0:00:44s
epoch 67 | loss: 0.37249 | eval_custom_logloss: 0.54577 |  0:00:44s
epoch 68 | loss: 0.39196 | eval_custom_logloss: 0.74079 |  0:00:45s
epoch 69 | loss: 0.37193 | eval_custom_logloss: 0.81284 |  0:00:46s
epoch 70 | loss: 0.37322 | eval_custom_logloss: 0.81387 |  0:00:46s
epoch 71 | loss: 0.36414 | eval_custom_logloss: 0.81444 |  0:00:47s
epoch 72 | loss: 0.37286 | eval_custom_logloss: 0.84125 |  0:00:48s
epoch 73 | loss: 0.36594 | eval_custom_logloss: 0.90038 |  0:00:48s
epoch 74 | loss: 0.35659 | eval_custom_logloss: 1.00118 |  0:00:49s
epoch 75 | loss: 0.35274 | eval_custom_logloss: 0.83955 |  0:00:50s
epoch 76 | loss: 0.33299 | eval_custom_logloss: 0.53607 |  0:00:50s
epoch 77 | loss: 0.32383 | eval_custom_logloss: 0.55907 |  0:00:51s
epoch 78 | loss: 0.33022 | eval_custom_logloss: 0.54469 |  0:00:52s
epoch 79 | loss: 0.33613 | eval_custom_logloss: 0.67683 |  0:00:52s
epoch 80 | loss: 0.32598 | eval_custom_logloss: 0.44723 |  0:00:53s
epoch 81 | loss: 0.33633 | eval_custom_logloss: 0.47927 |  0:00:54s
epoch 82 | loss: 0.28992 | eval_custom_logloss: 0.43947 |  0:00:54s
epoch 83 | loss: 0.27725 | eval_custom_logloss: 0.45532 |  0:00:55s
epoch 84 | loss: 0.28116 | eval_custom_logloss: 0.47317 |  0:00:56s
epoch 85 | loss: 0.28312 | eval_custom_logloss: 0.55186 |  0:00:56s
epoch 86 | loss: 0.31825 | eval_custom_logloss: 0.49833 |  0:00:57s
epoch 87 | loss: 0.28621 | eval_custom_logloss: 0.43494 |  0:00:58s
epoch 88 | loss: 0.28139 | eval_custom_logloss: 0.44368 |  0:00:58s
epoch 89 | loss: 0.27561 | eval_custom_logloss: 0.49926 |  0:00:59s
epoch 90 | loss: 0.28962 | eval_custom_logloss: 0.47872 |  0:01:00s
epoch 91 | loss: 0.30141 | eval_custom_logloss: 0.44    |  0:01:00s
epoch 92 | loss: 0.25173 | eval_custom_logloss: 0.48101 |  0:01:01s
epoch 93 | loss: 0.3336  | eval_custom_logloss: 0.48759 |  0:01:02s
epoch 94 | loss: 0.29024 | eval_custom_logloss: 0.43582 |  0:01:02s
epoch 95 | loss: 0.31018 | eval_custom_logloss: 0.47068 |  0:01:03s
epoch 96 | loss: 0.26907 | eval_custom_logloss: 0.41312 |  0:01:04s
epoch 97 | loss: 0.25703 | eval_custom_logloss: 0.42937 |  0:01:04s
epoch 98 | loss: 0.2862  | eval_custom_logloss: 0.49416 |  0:01:05s
epoch 99 | loss: 0.32379 | eval_custom_logloss: 0.43016 |  0:01:06s
Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_eval_custom_logloss = 0.41312
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4131, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax', 'n_a': 40, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.03488 | eval_custom_logloss: 6.85368 |  0:00:00s
epoch 1  | loss: 1.41779 | eval_custom_logloss: 5.76247 |  0:00:01s
epoch 2  | loss: 1.357   | eval_custom_logloss: 6.68139 |  0:00:02s
epoch 3  | loss: 1.60967 | eval_custom_logloss: 5.78458 |  0:00:02s
epoch 4  | loss: 1.30436 | eval_custom_logloss: 5.04205 |  0:00:03s
epoch 5  | loss: 1.26793 | eval_custom_logloss: 4.15816 |  0:00:04s
epoch 6  | loss: 1.04082 | eval_custom_logloss: 4.30415 |  0:00:04s
epoch 7  | loss: 1.02002 | eval_custom_logloss: 4.12778 |  0:00:05s
epoch 8  | loss: 0.8434  | eval_custom_logloss: 2.74395 |  0:00:06s
epoch 9  | loss: 0.97827 | eval_custom_logloss: 1.91885 |  0:00:06s
epoch 10 | loss: 0.78955 | eval_custom_logloss: 2.43424 |  0:00:07s
epoch 11 | loss: 0.76068 | eval_custom_logloss: 2.8681  |  0:00:08s
epoch 12 | loss: 0.76542 | eval_custom_logloss: 2.0649  |  0:00:08s
epoch 13 | loss: 0.8612  | eval_custom_logloss: 1.9651  |  0:00:09s
epoch 14 | loss: 0.69306 | eval_custom_logloss: 2.00877 |  0:00:10s
epoch 15 | loss: 0.66145 | eval_custom_logloss: 1.96311 |  0:00:10s
epoch 16 | loss: 0.6309  | eval_custom_logloss: 1.83199 |  0:00:11s
epoch 17 | loss: 0.57296 | eval_custom_logloss: 1.89004 |  0:00:12s
epoch 18 | loss: 0.62119 | eval_custom_logloss: 2.16302 |  0:00:12s
epoch 19 | loss: 0.60421 | eval_custom_logloss: 1.90574 |  0:00:13s
epoch 20 | loss: 0.59856 | eval_custom_logloss: 1.31721 |  0:00:14s
epoch 21 | loss: 0.54784 | eval_custom_logloss: 1.32286 |  0:00:14s
epoch 22 | loss: 0.54768 | eval_custom_logloss: 1.58683 |  0:00:15s
epoch 23 | loss: 0.56451 | eval_custom_logloss: 1.37931 |  0:00:16s
epoch 24 | loss: 0.58239 | eval_custom_logloss: 1.41753 |  0:00:16s
epoch 25 | loss: 0.57616 | eval_custom_logloss: 1.30746 |  0:00:17s
epoch 26 | loss: 0.54444 | eval_custom_logloss: 1.47432 |  0:00:17s
epoch 27 | loss: 0.60386 | eval_custom_logloss: 1.33089 |  0:00:18s
epoch 28 | loss: 0.74728 | eval_custom_logloss: 2.08637 |  0:00:19s
epoch 29 | loss: 0.79951 | eval_custom_logloss: 1.18318 |  0:00:19s
epoch 30 | loss: 0.69005 | eval_custom_logloss: 1.05677 |  0:00:20s
epoch 31 | loss: 0.61238 | eval_custom_logloss: 1.08653 |  0:00:21s
epoch 32 | loss: 0.59099 | eval_custom_logloss: 0.97537 |  0:00:21s
epoch 33 | loss: 0.56665 | eval_custom_logloss: 1.23173 |  0:00:22s
epoch 34 | loss: 0.60172 | eval_custom_logloss: 1.25751 |  0:00:23s
epoch 35 | loss: 0.62081 | eval_custom_logloss: 1.12619 |  0:00:23s
epoch 36 | loss: 0.58792 | eval_custom_logloss: 1.136   |  0:00:24s
epoch 37 | loss: 0.54321 | eval_custom_logloss: 1.08708 |  0:00:25s
epoch 38 | loss: 0.54656 | eval_custom_logloss: 0.99456 |  0:00:25s
epoch 39 | loss: 0.5625  | eval_custom_logloss: 0.858   |  0:00:26s
epoch 40 | loss: 0.54152 | eval_custom_logloss: 0.8468  |  0:00:27s
epoch 41 | loss: 0.54997 | eval_custom_logloss: 0.94263 |  0:00:27s
epoch 42 | loss: 0.54399 | eval_custom_logloss: 1.24867 |  0:00:28s
epoch 43 | loss: 0.53408 | eval_custom_logloss: 1.14166 |  0:00:29s
epoch 44 | loss: 0.55717 | eval_custom_logloss: 1.07716 |  0:00:29s
epoch 45 | loss: 0.57796 | eval_custom_logloss: 0.95247 |  0:00:30s
epoch 46 | loss: 0.57686 | eval_custom_logloss: 1.08901 |  0:00:31s
epoch 47 | loss: 0.54828 | eval_custom_logloss: 0.88933 |  0:00:31s
epoch 48 | loss: 0.5534  | eval_custom_logloss: 0.97645 |  0:00:32s
epoch 49 | loss: 0.55456 | eval_custom_logloss: 0.95608 |  0:00:33s
epoch 50 | loss: 0.53322 | eval_custom_logloss: 0.78996 |  0:00:33s
epoch 51 | loss: 0.53179 | eval_custom_logloss: 0.77805 |  0:00:34s
epoch 52 | loss: 0.53238 | eval_custom_logloss: 0.89092 |  0:00:35s
epoch 53 | loss: 0.50409 | eval_custom_logloss: 1.09805 |  0:00:35s
epoch 54 | loss: 0.50721 | eval_custom_logloss: 1.01962 |  0:00:36s
epoch 55 | loss: 0.5163  | eval_custom_logloss: 1.08804 |  0:00:37s
epoch 56 | loss: 0.48862 | eval_custom_logloss: 0.96594 |  0:00:37s
epoch 57 | loss: 0.55455 | eval_custom_logloss: 0.72618 |  0:00:38s
epoch 58 | loss: 0.51606 | eval_custom_logloss: 0.74127 |  0:00:39s
epoch 59 | loss: 0.54538 | eval_custom_logloss: 0.769   |  0:00:39s
epoch 60 | loss: 0.50142 | eval_custom_logloss: 0.87231 |  0:00:40s
epoch 61 | loss: 0.48709 | eval_custom_logloss: 0.83735 |  0:00:41s
epoch 62 | loss: 0.52767 | eval_custom_logloss: 0.76484 |  0:00:41s
epoch 63 | loss: 0.48672 | eval_custom_logloss: 0.79691 |  0:00:42s
epoch 64 | loss: 0.47047 | eval_custom_logloss: 0.95549 |  0:00:43s
epoch 65 | loss: 0.5096  | eval_custom_logloss: 1.02527 |  0:00:43s
epoch 66 | loss: 0.49028 | eval_custom_logloss: 0.75176 |  0:00:44s
epoch 67 | loss: 0.47484 | eval_custom_logloss: 0.7547  |  0:00:45s
epoch 68 | loss: 0.45418 | eval_custom_logloss: 0.73487 |  0:00:45s
epoch 69 | loss: 0.47994 | eval_custom_logloss: 0.6837  |  0:00:46s
epoch 70 | loss: 0.48163 | eval_custom_logloss: 0.6944  |  0:00:47s
epoch 71 | loss: 0.48056 | eval_custom_logloss: 0.68615 |  0:00:47s
epoch 72 | loss: 0.49434 | eval_custom_logloss: 0.73277 |  0:00:48s
epoch 73 | loss: 0.48574 | eval_custom_logloss: 0.76517 |  0:00:49s
epoch 74 | loss: 0.51297 | eval_custom_logloss: 0.99413 |  0:00:49s
epoch 75 | loss: 0.56088 | eval_custom_logloss: 0.94076 |  0:00:50s
epoch 76 | loss: 0.53925 | eval_custom_logloss: 0.78518 |  0:00:50s
epoch 77 | loss: 0.4951  | eval_custom_logloss: 0.77002 |  0:00:51s
epoch 78 | loss: 0.49798 | eval_custom_logloss: 1.0715  |  0:00:52s
epoch 79 | loss: 0.50442 | eval_custom_logloss: 0.84494 |  0:00:52s
epoch 80 | loss: 0.47073 | eval_custom_logloss: 0.86322 |  0:00:53s
epoch 81 | loss: 0.45332 | eval_custom_logloss: 0.88817 |  0:00:54s
epoch 82 | loss: 0.44949 | eval_custom_logloss: 0.78408 |  0:00:54s
epoch 83 | loss: 0.40991 | eval_custom_logloss: 0.81579 |  0:00:55s
epoch 84 | loss: 0.41647 | eval_custom_logloss: 0.76686 |  0:00:56s
epoch 85 | loss: 0.40029 | eval_custom_logloss: 0.77021 |  0:00:56s
epoch 86 | loss: 0.41149 | eval_custom_logloss: 0.82054 |  0:00:57s
epoch 87 | loss: 0.44331 | eval_custom_logloss: 0.92775 |  0:00:58s
epoch 88 | loss: 0.42848 | eval_custom_logloss: 0.84668 |  0:00:58s
epoch 89 | loss: 0.43619 | eval_custom_logloss: 0.86933 |  0:00:59s

Early stopping occurred at epoch 89 with best_epoch = 69 and best_eval_custom_logloss = 0.6837
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5484, 'Log Loss - std': 0.13529999999999998} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax', 'n_a': 40, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.96575 | eval_custom_logloss: 5.15096 |  0:00:00s
epoch 1  | loss: 1.5904  | eval_custom_logloss: 7.04481 |  0:00:01s
epoch 2  | loss: 1.60153 | eval_custom_logloss: 5.69606 |  0:00:01s
epoch 3  | loss: 1.32044 | eval_custom_logloss: 5.18517 |  0:00:02s
epoch 4  | loss: 1.22745 | eval_custom_logloss: 3.24175 |  0:00:03s
epoch 5  | loss: 1.52666 | eval_custom_logloss: 5.05339 |  0:00:03s
epoch 6  | loss: 1.42692 | eval_custom_logloss: 3.91941 |  0:00:04s
epoch 7  | loss: 1.30399 | eval_custom_logloss: 5.97664 |  0:00:05s
epoch 8  | loss: 0.97269 | eval_custom_logloss: 3.14281 |  0:00:05s
epoch 9  | loss: 1.48525 | eval_custom_logloss: 5.60401 |  0:00:06s
epoch 10 | loss: 1.30032 | eval_custom_logloss: 3.00712 |  0:00:07s
epoch 11 | loss: 0.89964 | eval_custom_logloss: 4.46507 |  0:00:07s
epoch 12 | loss: 0.96937 | eval_custom_logloss: 1.87402 |  0:00:08s
epoch 13 | loss: 0.76368 | eval_custom_logloss: 1.83367 |  0:00:09s
epoch 14 | loss: 0.75948 | eval_custom_logloss: 2.42272 |  0:00:09s
epoch 15 | loss: 0.69468 | eval_custom_logloss: 1.31095 |  0:00:10s
epoch 16 | loss: 0.67722 | eval_custom_logloss: 1.95427 |  0:00:11s
epoch 17 | loss: 0.69635 | eval_custom_logloss: 1.77915 |  0:00:11s
epoch 18 | loss: 0.72225 | eval_custom_logloss: 1.77462 |  0:00:12s
epoch 19 | loss: 0.69706 | eval_custom_logloss: 1.82192 |  0:00:13s
epoch 20 | loss: 0.64969 | eval_custom_logloss: 1.3584  |  0:00:13s
epoch 21 | loss: 0.62363 | eval_custom_logloss: 1.59616 |  0:00:14s
epoch 22 | loss: 0.64787 | eval_custom_logloss: 2.4033  |  0:00:15s
epoch 23 | loss: 0.6807  | eval_custom_logloss: 1.96438 |  0:00:15s
epoch 24 | loss: 0.62979 | eval_custom_logloss: 3.25625 |  0:00:16s
epoch 25 | loss: 0.66514 | eval_custom_logloss: 2.76639 |  0:00:17s
epoch 26 | loss: 0.61963 | eval_custom_logloss: 3.04894 |  0:00:17s
epoch 27 | loss: 0.63306 | eval_custom_logloss: 2.63107 |  0:00:18s
epoch 28 | loss: 0.6042  | eval_custom_logloss: 1.66311 |  0:00:18s
epoch 29 | loss: 0.61252 | eval_custom_logloss: 1.46907 |  0:00:19s
epoch 30 | loss: 0.68858 | eval_custom_logloss: 1.19385 |  0:00:20s
epoch 31 | loss: 0.69459 | eval_custom_logloss: 1.05413 |  0:00:20s
epoch 32 | loss: 0.65487 | eval_custom_logloss: 0.94904 |  0:00:21s
epoch 33 | loss: 0.68739 | eval_custom_logloss: 0.94339 |  0:00:22s
epoch 34 | loss: 0.66817 | eval_custom_logloss: 1.14074 |  0:00:22s
epoch 35 | loss: 0.60854 | eval_custom_logloss: 0.9841  |  0:00:23s
epoch 36 | loss: 0.61222 | eval_custom_logloss: 1.15863 |  0:00:24s
epoch 37 | loss: 0.62866 | eval_custom_logloss: 1.16172 |  0:00:24s
epoch 38 | loss: 0.57799 | eval_custom_logloss: 1.10049 |  0:00:25s
epoch 39 | loss: 0.61112 | eval_custom_logloss: 0.81442 |  0:00:26s
epoch 40 | loss: 0.60085 | eval_custom_logloss: 1.04258 |  0:00:26s
epoch 41 | loss: 0.56939 | eval_custom_logloss: 0.95692 |  0:00:27s
epoch 42 | loss: 0.54386 | eval_custom_logloss: 0.81928 |  0:00:28s
epoch 43 | loss: 0.54802 | eval_custom_logloss: 1.08501 |  0:00:28s
epoch 44 | loss: 0.56516 | eval_custom_logloss: 0.90464 |  0:00:29s
epoch 45 | loss: 0.55268 | eval_custom_logloss: 0.88871 |  0:00:30s
epoch 46 | loss: 0.57371 | eval_custom_logloss: 1.07559 |  0:00:30s
epoch 47 | loss: 0.54755 | eval_custom_logloss: 0.88867 |  0:00:31s
epoch 48 | loss: 0.54006 | eval_custom_logloss: 0.73297 |  0:00:32s
epoch 49 | loss: 0.52132 | eval_custom_logloss: 0.70238 |  0:00:32s
epoch 50 | loss: 0.53256 | eval_custom_logloss: 0.70669 |  0:00:33s
epoch 51 | loss: 0.55019 | eval_custom_logloss: 0.67395 |  0:00:34s
epoch 52 | loss: 0.52487 | eval_custom_logloss: 0.83882 |  0:00:34s
epoch 53 | loss: 0.49403 | eval_custom_logloss: 0.93909 |  0:00:35s
epoch 54 | loss: 0.52512 | eval_custom_logloss: 0.74578 |  0:00:36s
epoch 55 | loss: 0.52339 | eval_custom_logloss: 0.62723 |  0:00:36s
epoch 56 | loss: 0.53218 | eval_custom_logloss: 0.69266 |  0:00:37s
epoch 57 | loss: 0.58269 | eval_custom_logloss: 0.69235 |  0:00:38s
epoch 58 | loss: 0.55039 | eval_custom_logloss: 0.58178 |  0:00:38s
epoch 59 | loss: 0.53923 | eval_custom_logloss: 0.68042 |  0:00:39s
epoch 60 | loss: 0.50247 | eval_custom_logloss: 0.70291 |  0:00:40s
epoch 61 | loss: 0.50587 | eval_custom_logloss: 0.82316 |  0:00:40s
epoch 62 | loss: 0.5299  | eval_custom_logloss: 0.74103 |  0:00:41s
epoch 63 | loss: 0.52318 | eval_custom_logloss: 0.78059 |  0:00:42s
epoch 64 | loss: 0.49589 | eval_custom_logloss: 0.84291 |  0:00:42s
epoch 65 | loss: 0.5047  | eval_custom_logloss: 0.75178 |  0:00:43s
epoch 66 | loss: 0.5257  | eval_custom_logloss: 0.90974 |  0:00:44s
epoch 67 | loss: 0.4947  | eval_custom_logloss: 0.74931 |  0:00:44s
epoch 68 | loss: 0.50793 | eval_custom_logloss: 0.7749  |  0:00:45s
epoch 69 | loss: 0.48722 | eval_custom_logloss: 0.8331  |  0:00:45s
epoch 70 | loss: 0.48143 | eval_custom_logloss: 0.70693 |  0:00:46s
epoch 71 | loss: 0.48323 | eval_custom_logloss: 0.82024 |  0:00:47s
epoch 72 | loss: 0.51861 | eval_custom_logloss: 0.73673 |  0:00:47s
epoch 73 | loss: 0.55898 | eval_custom_logloss: 0.82446 |  0:00:48s
epoch 74 | loss: 0.52528 | eval_custom_logloss: 0.64064 |  0:00:49s
epoch 75 | loss: 0.50268 | eval_custom_logloss: 0.70747 |  0:00:49s
epoch 76 | loss: 0.46887 | eval_custom_logloss: 0.67275 |  0:00:50s
epoch 77 | loss: 0.4825  | eval_custom_logloss: 0.73101 |  0:00:51s
epoch 78 | loss: 0.44822 | eval_custom_logloss: 0.75254 |  0:00:51s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.58178
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5595333333333333, 'Log Loss - std': 0.11158836060370372} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax', 'n_a': 40, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.95299 | eval_custom_logloss: 5.72423 |  0:00:00s
epoch 1  | loss: 1.37669 | eval_custom_logloss: 5.21978 |  0:00:01s
epoch 2  | loss: 1.23838 | eval_custom_logloss: 4.51413 |  0:00:01s
epoch 3  | loss: 0.99971 | eval_custom_logloss: 5.33081 |  0:00:02s
epoch 4  | loss: 1.20873 | eval_custom_logloss: 4.66206 |  0:00:03s
epoch 5  | loss: 1.20108 | eval_custom_logloss: 3.08975 |  0:00:03s
epoch 6  | loss: 0.92863 | eval_custom_logloss: 3.5755  |  0:00:04s
epoch 7  | loss: 0.89954 | eval_custom_logloss: 3.25315 |  0:00:05s
epoch 8  | loss: 0.79269 | eval_custom_logloss: 3.23339 |  0:00:05s
epoch 9  | loss: 0.87369 | eval_custom_logloss: 3.28828 |  0:00:06s
epoch 10 | loss: 0.76661 | eval_custom_logloss: 2.74749 |  0:00:07s
epoch 11 | loss: 0.79513 | eval_custom_logloss: 4.6483  |  0:00:07s
epoch 12 | loss: 0.89824 | eval_custom_logloss: 4.19185 |  0:00:08s
epoch 13 | loss: 0.73364 | eval_custom_logloss: 3.4099  |  0:00:09s
epoch 14 | loss: 0.81274 | eval_custom_logloss: 2.17787 |  0:00:09s
epoch 15 | loss: 0.74847 | eval_custom_logloss: 2.05604 |  0:00:10s
epoch 16 | loss: 0.71019 | eval_custom_logloss: 2.53958 |  0:00:11s
epoch 17 | loss: 0.68607 | eval_custom_logloss: 2.72711 |  0:00:11s
epoch 18 | loss: 0.78702 | eval_custom_logloss: 2.51975 |  0:00:12s
epoch 19 | loss: 0.68848 | eval_custom_logloss: 2.61152 |  0:00:13s
epoch 20 | loss: 0.66253 | eval_custom_logloss: 2.56414 |  0:00:13s
epoch 21 | loss: 0.61374 | eval_custom_logloss: 2.65194 |  0:00:14s
epoch 22 | loss: 0.63974 | eval_custom_logloss: 2.11358 |  0:00:15s
epoch 23 | loss: 0.65811 | eval_custom_logloss: 2.15082 |  0:00:15s
epoch 24 | loss: 0.63974 | eval_custom_logloss: 2.4637  |  0:00:16s
epoch 25 | loss: 0.62177 | eval_custom_logloss: 2.42385 |  0:00:17s
epoch 26 | loss: 0.59653 | eval_custom_logloss: 2.29029 |  0:00:17s
epoch 27 | loss: 0.59925 | eval_custom_logloss: 2.12919 |  0:00:18s
epoch 28 | loss: 0.58232 | eval_custom_logloss: 1.97402 |  0:00:19s
epoch 29 | loss: 0.57867 | eval_custom_logloss: 1.35156 |  0:00:19s
epoch 30 | loss: 0.58054 | eval_custom_logloss: 1.12862 |  0:00:20s
epoch 31 | loss: 0.64013 | eval_custom_logloss: 1.22622 |  0:00:21s
epoch 32 | loss: 0.64171 | eval_custom_logloss: 1.20246 |  0:00:21s
epoch 33 | loss: 0.59783 | eval_custom_logloss: 1.25437 |  0:00:22s
epoch 34 | loss: 0.60808 | eval_custom_logloss: 1.3736  |  0:00:23s
epoch 35 | loss: 0.63834 | eval_custom_logloss: 1.6214  |  0:00:23s
epoch 36 | loss: 0.59303 | eval_custom_logloss: 1.24928 |  0:00:24s
epoch 37 | loss: 0.58598 | eval_custom_logloss: 1.49787 |  0:00:25s
epoch 38 | loss: 0.5688  | eval_custom_logloss: 1.17683 |  0:00:25s
epoch 39 | loss: 0.56986 | eval_custom_logloss: 1.21769 |  0:00:26s
epoch 40 | loss: 0.53309 | eval_custom_logloss: 1.27493 |  0:00:27s
epoch 41 | loss: 0.54235 | eval_custom_logloss: 1.39826 |  0:00:27s
epoch 42 | loss: 0.55921 | eval_custom_logloss: 1.23674 |  0:00:28s
epoch 43 | loss: 0.53855 | eval_custom_logloss: 1.13134 |  0:00:29s
epoch 44 | loss: 0.54523 | eval_custom_logloss: 1.04153 |  0:00:29s
epoch 45 | loss: 0.50399 | eval_custom_logloss: 1.02012 |  0:00:30s
epoch 46 | loss: 0.51561 | eval_custom_logloss: 1.00512 |  0:00:31s
epoch 47 | loss: 0.50343 | eval_custom_logloss: 0.9438  |  0:00:31s
epoch 48 | loss: 0.53091 | eval_custom_logloss: 1.36257 |  0:00:32s
epoch 49 | loss: 0.51896 | eval_custom_logloss: 1.18397 |  0:00:33s
epoch 50 | loss: 0.53282 | eval_custom_logloss: 1.04709 |  0:00:33s
epoch 51 | loss: 0.52339 | eval_custom_logloss: 1.05961 |  0:00:34s
epoch 52 | loss: 0.53036 | eval_custom_logloss: 0.99864 |  0:00:35s
epoch 53 | loss: 0.50695 | eval_custom_logloss: 1.02716 |  0:00:35s
epoch 54 | loss: 0.55687 | eval_custom_logloss: 1.05979 |  0:00:36s
epoch 55 | loss: 0.53464 | eval_custom_logloss: 0.99377 |  0:00:37s
epoch 56 | loss: 0.51924 | eval_custom_logloss: 0.99753 |  0:00:37s
epoch 57 | loss: 0.50387 | eval_custom_logloss: 1.04115 |  0:00:38s
epoch 58 | loss: 0.50295 | eval_custom_logloss: 0.91832 |  0:00:39s
epoch 59 | loss: 0.53531 | eval_custom_logloss: 1.02295 |  0:00:39s
epoch 60 | loss: 0.53652 | eval_custom_logloss: 1.1052  |  0:00:40s
epoch 61 | loss: 0.58425 | eval_custom_logloss: 0.69731 |  0:00:41s
epoch 62 | loss: 0.55296 | eval_custom_logloss: 0.75629 |  0:00:41s
epoch 63 | loss: 0.56108 | eval_custom_logloss: 0.75165 |  0:00:42s
epoch 64 | loss: 0.52504 | eval_custom_logloss: 0.9842  |  0:00:43s
epoch 65 | loss: 0.56583 | eval_custom_logloss: 0.7912  |  0:00:43s
epoch 66 | loss: 0.54772 | eval_custom_logloss: 0.74567 |  0:00:44s
epoch 67 | loss: 0.54445 | eval_custom_logloss: 0.81502 |  0:00:45s
epoch 68 | loss: 0.55612 | eval_custom_logloss: 0.69601 |  0:00:45s
epoch 69 | loss: 0.53333 | eval_custom_logloss: 0.74806 |  0:00:46s
epoch 70 | loss: 0.54808 | eval_custom_logloss: 0.71977 |  0:00:47s
epoch 71 | loss: 0.5405  | eval_custom_logloss: 0.73081 |  0:00:47s
epoch 72 | loss: 0.53199 | eval_custom_logloss: 0.72704 |  0:00:48s
epoch 73 | loss: 0.53088 | eval_custom_logloss: 0.67518 |  0:00:49s
epoch 74 | loss: 0.52195 | eval_custom_logloss: 0.7824  |  0:00:49s
epoch 75 | loss: 0.5109  | eval_custom_logloss: 0.79718 |  0:00:50s
epoch 76 | loss: 0.50085 | eval_custom_logloss: 0.86509 |  0:00:51s
epoch 77 | loss: 0.55124 | eval_custom_logloss: 0.90024 |  0:00:51s
epoch 78 | loss: 0.55519 | eval_custom_logloss: 0.8266  |  0:00:52s
epoch 79 | loss: 0.57916 | eval_custom_logloss: 0.72887 |  0:00:53s
epoch 80 | loss: 0.54168 | eval_custom_logloss: 0.81108 |  0:00:53s
epoch 81 | loss: 0.54761 | eval_custom_logloss: 0.71591 |  0:00:54s
epoch 82 | loss: 0.5421  | eval_custom_logloss: 0.75689 |  0:00:55s
epoch 83 | loss: 0.54143 | eval_custom_logloss: 0.68602 |  0:00:55s
epoch 84 | loss: 0.58635 | eval_custom_logloss: 0.79866 |  0:00:56s
epoch 85 | loss: 0.58215 | eval_custom_logloss: 0.71757 |  0:00:57s
epoch 86 | loss: 0.57379 | eval_custom_logloss: 0.76277 |  0:00:57s
epoch 87 | loss: 0.54176 | eval_custom_logloss: 1.13179 |  0:00:58s
epoch 88 | loss: 0.52947 | eval_custom_logloss: 0.82411 |  0:00:59s
epoch 89 | loss: 0.55172 | eval_custom_logloss: 0.81774 |  0:00:59s
epoch 90 | loss: 0.56164 | eval_custom_logloss: 0.85275 |  0:01:00s
epoch 91 | loss: 0.5487  | eval_custom_logloss: 0.90858 |  0:01:01s
epoch 92 | loss: 0.52897 | eval_custom_logloss: 0.80644 |  0:01:01s
epoch 93 | loss: 0.55018 | eval_custom_logloss: 0.82236 |  0:01:02s

Early stopping occurred at epoch 93 with best_epoch = 73 and best_eval_custom_logloss = 0.67518
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5884499999999999, 'Log Loss - std': 0.10884618734710003} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax', 'n_a': 40, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.88057 | eval_custom_logloss: 5.54027 |  0:00:00s
epoch 1  | loss: 1.54578 | eval_custom_logloss: 5.58878 |  0:00:01s
epoch 2  | loss: 1.4809  | eval_custom_logloss: 3.96395 |  0:00:01s
epoch 3  | loss: 1.45765 | eval_custom_logloss: 2.81295 |  0:00:02s
epoch 4  | loss: 1.48528 | eval_custom_logloss: 3.94849 |  0:00:03s
epoch 5  | loss: 1.20141 | eval_custom_logloss: 3.77678 |  0:00:04s
epoch 6  | loss: 2.69352 | eval_custom_logloss: 4.67882 |  0:00:04s
epoch 7  | loss: 1.18141 | eval_custom_logloss: 5.08954 |  0:00:05s
epoch 8  | loss: 1.51388 | eval_custom_logloss: 4.91873 |  0:00:06s
epoch 9  | loss: 1.03997 | eval_custom_logloss: 2.67459 |  0:00:06s
epoch 10 | loss: 0.91442 | eval_custom_logloss: 2.39487 |  0:00:07s
epoch 11 | loss: 0.89874 | eval_custom_logloss: 2.81546 |  0:00:08s
epoch 12 | loss: 0.87106 | eval_custom_logloss: 2.29825 |  0:00:08s
epoch 13 | loss: 0.86551 | eval_custom_logloss: 2.47601 |  0:00:09s
epoch 14 | loss: 0.76043 | eval_custom_logloss: 2.06322 |  0:00:10s
epoch 15 | loss: 0.75218 | eval_custom_logloss: 1.40747 |  0:00:10s
epoch 16 | loss: 0.75157 | eval_custom_logloss: 1.41636 |  0:00:11s
epoch 17 | loss: 0.73022 | eval_custom_logloss: 2.78801 |  0:00:12s
epoch 18 | loss: 0.69217 | eval_custom_logloss: 3.41091 |  0:00:12s
epoch 19 | loss: 0.68    | eval_custom_logloss: 2.38435 |  0:00:13s
epoch 20 | loss: 0.65693 | eval_custom_logloss: 1.60849 |  0:00:14s
epoch 21 | loss: 0.63114 | eval_custom_logloss: 0.92766 |  0:00:14s
epoch 22 | loss: 0.63734 | eval_custom_logloss: 1.13273 |  0:00:15s
epoch 23 | loss: 0.6117  | eval_custom_logloss: 1.29625 |  0:00:16s
epoch 24 | loss: 0.59928 | eval_custom_logloss: 1.45703 |  0:00:16s
epoch 25 | loss: 0.62122 | eval_custom_logloss: 1.07906 |  0:00:17s
epoch 26 | loss: 0.60957 | eval_custom_logloss: 1.35146 |  0:00:18s
epoch 27 | loss: 0.60713 | eval_custom_logloss: 0.91923 |  0:00:18s
epoch 28 | loss: 0.60021 | eval_custom_logloss: 1.09014 |  0:00:19s
epoch 29 | loss: 0.58409 | eval_custom_logloss: 0.88884 |  0:00:20s
epoch 30 | loss: 0.57009 | eval_custom_logloss: 1.12535 |  0:00:20s
epoch 31 | loss: 0.56016 | eval_custom_logloss: 1.15523 |  0:00:21s
epoch 32 | loss: 0.56972 | eval_custom_logloss: 1.10145 |  0:00:22s
epoch 33 | loss: 0.5738  | eval_custom_logloss: 1.00518 |  0:00:22s
epoch 34 | loss: 0.52851 | eval_custom_logloss: 0.93662 |  0:00:23s
epoch 35 | loss: 0.54588 | eval_custom_logloss: 1.01838 |  0:00:24s
epoch 36 | loss: 0.57551 | eval_custom_logloss: 0.69308 |  0:00:24s
epoch 37 | loss: 0.54209 | eval_custom_logloss: 0.95162 |  0:00:25s
epoch 38 | loss: 0.54465 | eval_custom_logloss: 0.98871 |  0:00:25s
epoch 39 | loss: 0.57153 | eval_custom_logloss: 0.92466 |  0:00:26s
epoch 40 | loss: 0.51343 | eval_custom_logloss: 0.89598 |  0:00:27s
epoch 41 | loss: 0.55207 | eval_custom_logloss: 0.91452 |  0:00:27s
epoch 42 | loss: 0.53795 | eval_custom_logloss: 0.82522 |  0:00:28s
epoch 43 | loss: 0.52094 | eval_custom_logloss: 0.85457 |  0:00:29s
epoch 44 | loss: 0.51441 | eval_custom_logloss: 0.92275 |  0:00:29s
epoch 45 | loss: 0.4962  | eval_custom_logloss: 0.81977 |  0:00:30s
epoch 46 | loss: 0.516   | eval_custom_logloss: 0.75497 |  0:00:31s
epoch 47 | loss: 0.50216 | eval_custom_logloss: 0.7278  |  0:00:31s
epoch 48 | loss: 0.51029 | eval_custom_logloss: 0.69981 |  0:00:32s
epoch 49 | loss: 0.49913 | eval_custom_logloss: 0.613   |  0:00:33s
epoch 50 | loss: 0.48338 | eval_custom_logloss: 0.68315 |  0:00:33s
epoch 51 | loss: 0.47815 | eval_custom_logloss: 0.61592 |  0:00:34s
epoch 52 | loss: 0.48207 | eval_custom_logloss: 0.55436 |  0:00:35s
epoch 53 | loss: 0.49651 | eval_custom_logloss: 0.62965 |  0:00:35s
epoch 54 | loss: 0.55743 | eval_custom_logloss: 0.82923 |  0:00:36s
epoch 55 | loss: 0.52709 | eval_custom_logloss: 0.7695  |  0:00:37s
epoch 56 | loss: 0.54586 | eval_custom_logloss: 0.71604 |  0:00:37s
epoch 57 | loss: 0.51662 | eval_custom_logloss: 0.75651 |  0:00:38s
epoch 58 | loss: 0.50135 | eval_custom_logloss: 0.9043  |  0:00:39s
epoch 59 | loss: 0.48235 | eval_custom_logloss: 1.18153 |  0:00:39s
epoch 60 | loss: 0.55684 | eval_custom_logloss: 1.15548 |  0:00:40s
epoch 61 | loss: 0.56545 | eval_custom_logloss: 0.71564 |  0:00:41s
epoch 62 | loss: 0.55062 | eval_custom_logloss: 0.73019 |  0:00:41s
epoch 63 | loss: 0.55706 | eval_custom_logloss: 0.90629 |  0:00:42s
epoch 64 | loss: 0.56621 | eval_custom_logloss: 0.7409  |  0:00:43s
epoch 65 | loss: 0.55903 | eval_custom_logloss: 0.75225 |  0:00:43s
epoch 66 | loss: 0.5653  | eval_custom_logloss: 0.66341 |  0:00:44s
epoch 67 | loss: 0.56375 | eval_custom_logloss: 0.65411 |  0:00:45s
epoch 68 | loss: 0.50891 | eval_custom_logloss: 0.62754 |  0:00:45s
epoch 69 | loss: 0.47854 | eval_custom_logloss: 0.68126 |  0:00:46s
epoch 70 | loss: 0.48882 | eval_custom_logloss: 0.51599 |  0:00:47s
epoch 71 | loss: 0.47861 | eval_custom_logloss: 0.55679 |  0:00:47s
epoch 72 | loss: 0.48811 | eval_custom_logloss: 0.56374 |  0:00:48s
epoch 73 | loss: 0.48604 | eval_custom_logloss: 0.63281 |  0:00:49s
epoch 74 | loss: 0.47088 | eval_custom_logloss: 0.57065 |  0:00:49s
epoch 75 | loss: 0.46016 | eval_custom_logloss: 0.58    |  0:00:50s
epoch 76 | loss: 0.46191 | eval_custom_logloss: 0.50694 |  0:00:51s
epoch 77 | loss: 0.41237 | eval_custom_logloss: 0.61077 |  0:00:51s
epoch 78 | loss: 0.43503 | eval_custom_logloss: 0.56046 |  0:00:52s
epoch 79 | loss: 0.41778 | eval_custom_logloss: 0.51466 |  0:00:53s
epoch 80 | loss: 0.40744 | eval_custom_logloss: 0.56921 |  0:00:53s
epoch 81 | loss: 0.39479 | eval_custom_logloss: 0.47528 |  0:00:54s
epoch 82 | loss: 0.44213 | eval_custom_logloss: 0.53924 |  0:00:55s
epoch 83 | loss: 0.43933 | eval_custom_logloss: 0.5574  |  0:00:55s
epoch 84 | loss: 0.42385 | eval_custom_logloss: 0.50394 |  0:00:56s
epoch 85 | loss: 0.42759 | eval_custom_logloss: 0.53421 |  0:00:57s
epoch 86 | loss: 0.41717 | eval_custom_logloss: 0.50866 |  0:00:57s
epoch 87 | loss: 0.42935 | eval_custom_logloss: 0.64283 |  0:00:58s
epoch 88 | loss: 0.46143 | eval_custom_logloss: 0.43695 |  0:00:59s
epoch 89 | loss: 0.4104  | eval_custom_logloss: 0.49664 |  0:00:59s
epoch 90 | loss: 0.42137 | eval_custom_logloss: 0.56645 |  0:01:00s
epoch 91 | loss: 0.43239 | eval_custom_logloss: 0.58086 |  0:01:01s
epoch 92 | loss: 0.4106  | eval_custom_logloss: 0.71998 |  0:01:01s
epoch 93 | loss: 0.42513 | eval_custom_logloss: 0.61693 |  0:01:02s
epoch 94 | loss: 0.4172  | eval_custom_logloss: 0.7397  |  0:01:03s
epoch 95 | loss: 0.47186 | eval_custom_logloss: 0.58205 |  0:01:03s
epoch 96 | loss: 0.43253 | eval_custom_logloss: 0.62341 |  0:01:04s
epoch 97 | loss: 0.40261 | eval_custom_logloss: 0.70494 |  0:01:05s
epoch 98 | loss: 0.42419 | eval_custom_logloss: 0.88328 |  0:01:05s
epoch 99 | loss: 0.40698 | eval_custom_logloss: 0.56182 |  0:01:06s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 0.43695
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5581599999999999, 'Log Loss - std': 0.11466442517189017} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 0.5581599999999999 and parameters: {'n_d': 40, 'n_steps': 9, 'gamma': 1.5932832729858928, 'cat_emb_dim': 2, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0038678905139439674, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax', 'n_a': 10, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.06073 | eval_custom_logloss: 3.04835 |  0:00:00s
epoch 1  | loss: 0.87181 | eval_custom_logloss: 2.5073  |  0:00:00s
epoch 2  | loss: 0.85409 | eval_custom_logloss: 1.96688 |  0:00:01s
epoch 3  | loss: 0.8194  | eval_custom_logloss: 1.5606  |  0:00:01s
epoch 4  | loss: 0.81019 | eval_custom_logloss: 3.17555 |  0:00:02s
epoch 5  | loss: 0.78081 | eval_custom_logloss: 3.55343 |  0:00:02s
epoch 6  | loss: 0.76175 | eval_custom_logloss: 5.85288 |  0:00:02s
epoch 7  | loss: 0.71305 | eval_custom_logloss: 6.28304 |  0:00:03s
epoch 8  | loss: 0.68676 | eval_custom_logloss: 5.83849 |  0:00:03s
epoch 9  | loss: 0.65489 | eval_custom_logloss: 6.64265 |  0:00:04s
epoch 10 | loss: 0.61474 | eval_custom_logloss: 6.19306 |  0:00:04s
epoch 11 | loss: 0.60164 | eval_custom_logloss: 4.16748 |  0:00:04s
epoch 12 | loss: 0.58709 | eval_custom_logloss: 4.28156 |  0:00:05s
epoch 13 | loss: 0.55504 | eval_custom_logloss: 5.13271 |  0:00:05s
epoch 14 | loss: 0.55139 | eval_custom_logloss: 4.60413 |  0:00:06s
epoch 15 | loss: 0.51437 | eval_custom_logloss: 5.10858 |  0:00:06s
epoch 16 | loss: 0.52254 | eval_custom_logloss: 6.07044 |  0:00:06s
epoch 17 | loss: 0.50845 | eval_custom_logloss: 3.59162 |  0:00:07s
epoch 18 | loss: 0.50426 | eval_custom_logloss: 4.07671 |  0:00:07s
epoch 19 | loss: 0.50394 | eval_custom_logloss: 4.37699 |  0:00:08s
epoch 20 | loss: 0.52421 | eval_custom_logloss: 5.82345 |  0:00:08s
epoch 21 | loss: 0.4803  | eval_custom_logloss: 6.43302 |  0:00:08s
epoch 22 | loss: 0.47767 | eval_custom_logloss: 6.82954 |  0:00:09s
epoch 23 | loss: 0.46453 | eval_custom_logloss: 6.19655 |  0:00:09s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 1.5606
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.4779, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax', 'n_a': 10, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09457 | eval_custom_logloss: 2.18648 |  0:00:00s
epoch 1  | loss: 0.89404 | eval_custom_logloss: 2.73488 |  0:00:00s
epoch 2  | loss: 0.8273  | eval_custom_logloss: 2.09977 |  0:00:01s
epoch 3  | loss: 0.81379 | eval_custom_logloss: 2.95987 |  0:00:01s
epoch 4  | loss: 0.79776 | eval_custom_logloss: 3.20423 |  0:00:02s
epoch 5  | loss: 0.74971 | eval_custom_logloss: 3.76834 |  0:00:02s
epoch 6  | loss: 0.7112  | eval_custom_logloss: 4.12152 |  0:00:02s
epoch 7  | loss: 0.70108 | eval_custom_logloss: 7.09109 |  0:00:03s
epoch 8  | loss: 0.69468 | eval_custom_logloss: 6.3728  |  0:00:03s
epoch 9  | loss: 0.65075 | eval_custom_logloss: 8.08012 |  0:00:03s
epoch 10 | loss: 0.6383  | eval_custom_logloss: 7.3648  |  0:00:04s
epoch 11 | loss: 0.63448 | eval_custom_logloss: 6.45321 |  0:00:04s
epoch 12 | loss: 0.62178 | eval_custom_logloss: 5.57576 |  0:00:05s
epoch 13 | loss: 0.59325 | eval_custom_logloss: 5.68787 |  0:00:05s
epoch 14 | loss: 0.56371 | eval_custom_logloss: 6.17304 |  0:00:05s
epoch 15 | loss: 0.57899 | eval_custom_logloss: 7.10657 |  0:00:06s
epoch 16 | loss: 0.54907 | eval_custom_logloss: 6.60881 |  0:00:06s
epoch 17 | loss: 0.55312 | eval_custom_logloss: 6.69104 |  0:00:07s
epoch 18 | loss: 0.55499 | eval_custom_logloss: 5.762   |  0:00:07s
epoch 19 | loss: 0.54    | eval_custom_logloss: 6.00592 |  0:00:07s
epoch 20 | loss: 0.54981 | eval_custom_logloss: 5.80358 |  0:00:08s
epoch 21 | loss: 0.53307 | eval_custom_logloss: 5.67485 |  0:00:08s
epoch 22 | loss: 0.50984 | eval_custom_logloss: 5.68337 |  0:00:09s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 2.09977
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.69095, 'Log Loss - std': 0.21304999999999996} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax', 'n_a': 10, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.05431 | eval_custom_logloss: 1.53542 |  0:00:00s
epoch 1  | loss: 0.92957 | eval_custom_logloss: 2.22271 |  0:00:00s
epoch 2  | loss: 0.85263 | eval_custom_logloss: 2.18049 |  0:00:01s
epoch 3  | loss: 0.79178 | eval_custom_logloss: 2.93624 |  0:00:01s
epoch 4  | loss: 0.81637 | eval_custom_logloss: 2.81428 |  0:00:02s
epoch 5  | loss: 0.73045 | eval_custom_logloss: 3.78986 |  0:00:02s
epoch 6  | loss: 0.71425 | eval_custom_logloss: 4.67208 |  0:00:02s
epoch 7  | loss: 0.6944  | eval_custom_logloss: 4.29988 |  0:00:03s
epoch 8  | loss: 0.66955 | eval_custom_logloss: 4.52085 |  0:00:03s
epoch 9  | loss: 0.66543 | eval_custom_logloss: 4.92397 |  0:00:03s
epoch 10 | loss: 0.61424 | eval_custom_logloss: 3.67164 |  0:00:04s
epoch 11 | loss: 0.60159 | eval_custom_logloss: 4.61936 |  0:00:04s
epoch 12 | loss: 0.58356 | eval_custom_logloss: 5.37786 |  0:00:05s
epoch 13 | loss: 0.5851  | eval_custom_logloss: 4.72682 |  0:00:05s
epoch 14 | loss: 0.59174 | eval_custom_logloss: 3.97812 |  0:00:05s
epoch 15 | loss: 0.56474 | eval_custom_logloss: 5.25899 |  0:00:06s
epoch 16 | loss: 0.53458 | eval_custom_logloss: 6.24237 |  0:00:06s
epoch 17 | loss: 0.55511 | eval_custom_logloss: 3.94078 |  0:00:07s
epoch 18 | loss: 0.55401 | eval_custom_logloss: 3.31662 |  0:00:07s
epoch 19 | loss: 0.5289  | eval_custom_logloss: 3.36491 |  0:00:07s
epoch 20 | loss: 0.51152 | eval_custom_logloss: 4.2309  |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.53542
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.6297333333333333, 'Log Loss - std': 0.19430687641517527} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax', 'n_a': 10, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.06    | eval_custom_logloss: 2.03522 |  0:00:00s
epoch 1  | loss: 0.84276 | eval_custom_logloss: 1.80165 |  0:00:00s
epoch 2  | loss: 0.81383 | eval_custom_logloss: 2.06383 |  0:00:01s
epoch 3  | loss: 0.75405 | eval_custom_logloss: 3.20739 |  0:00:01s
epoch 4  | loss: 0.70292 | eval_custom_logloss: 6.04806 |  0:00:02s
epoch 5  | loss: 0.69008 | eval_custom_logloss: 4.02794 |  0:00:02s
epoch 6  | loss: 0.67593 | eval_custom_logloss: 4.79045 |  0:00:02s
epoch 7  | loss: 0.66691 | eval_custom_logloss: 6.06978 |  0:00:03s
epoch 8  | loss: 0.59854 | eval_custom_logloss: 7.17063 |  0:00:03s
epoch 9  | loss: 0.60351 | eval_custom_logloss: 6.53501 |  0:00:03s
epoch 10 | loss: 0.57103 | eval_custom_logloss: 6.64227 |  0:00:04s
epoch 11 | loss: 0.56554 | eval_custom_logloss: 5.85267 |  0:00:04s
epoch 12 | loss: 0.55649 | eval_custom_logloss: 4.87779 |  0:00:05s
epoch 13 | loss: 0.54153 | eval_custom_logloss: 4.98843 |  0:00:05s
epoch 14 | loss: 0.51406 | eval_custom_logloss: 5.17886 |  0:00:05s
epoch 15 | loss: 0.5048  | eval_custom_logloss: 5.23348 |  0:00:06s
epoch 16 | loss: 0.51536 | eval_custom_logloss: 5.83494 |  0:00:06s
epoch 17 | loss: 0.45987 | eval_custom_logloss: 5.4902  |  0:00:07s
epoch 18 | loss: 0.46276 | eval_custom_logloss: 4.12612 |  0:00:07s
epoch 19 | loss: 0.46175 | eval_custom_logloss: 4.49796 |  0:00:07s
epoch 20 | loss: 0.45087 | eval_custom_logloss: 3.59295 |  0:00:08s
epoch 21 | loss: 0.47332 | eval_custom_logloss: 2.85985 |  0:00:08s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 1.80165
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.64475, 'Log Loss - std': 0.1702729353126914} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax', 'n_a': 10, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.06933 | eval_custom_logloss: 1.38004 |  0:00:00s
epoch 1  | loss: 0.86354 | eval_custom_logloss: 2.06076 |  0:00:00s
epoch 2  | loss: 0.78704 | eval_custom_logloss: 2.33909 |  0:00:01s
epoch 3  | loss: 0.77062 | eval_custom_logloss: 2.58517 |  0:00:01s
epoch 4  | loss: 0.744   | eval_custom_logloss: 3.21439 |  0:00:02s
epoch 5  | loss: 0.76827 | eval_custom_logloss: 3.61259 |  0:00:02s
epoch 6  | loss: 0.75475 | eval_custom_logloss: 4.41733 |  0:00:02s
epoch 7  | loss: 0.65467 | eval_custom_logloss: 3.81222 |  0:00:03s
epoch 8  | loss: 0.61413 | eval_custom_logloss: 6.20336 |  0:00:03s
epoch 9  | loss: 0.60793 | eval_custom_logloss: 5.12598 |  0:00:04s
epoch 10 | loss: 0.58955 | eval_custom_logloss: 2.82966 |  0:00:04s
epoch 11 | loss: 0.58206 | eval_custom_logloss: 3.09773 |  0:00:04s
epoch 12 | loss: 0.52678 | eval_custom_logloss: 4.46046 |  0:00:05s
epoch 13 | loss: 0.53695 | eval_custom_logloss: 3.73056 |  0:00:05s
epoch 14 | loss: 0.51881 | eval_custom_logloss: 4.33467 |  0:00:06s
epoch 15 | loss: 0.51755 | eval_custom_logloss: 4.13791 |  0:00:06s
epoch 16 | loss: 0.52565 | eval_custom_logloss: 4.17752 |  0:00:06s
epoch 17 | loss: 0.5085  | eval_custom_logloss: 4.21827 |  0:00:07s
epoch 18 | loss: 0.5076  | eval_custom_logloss: 3.84256 |  0:00:07s
epoch 19 | loss: 0.48246 | eval_custom_logloss: 4.31404 |  0:00:08s
epoch 20 | loss: 0.45942 | eval_custom_logloss: 4.81069 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.38004
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.58154, 'Log Loss - std': 0.19793007452128134} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 1.58154 and parameters: {'n_d': 10, 'n_steps': 3, 'gamma': 1.75591442494138, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001070105379390903, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.30096 | eval_custom_logloss: 5.88222 |  0:00:00s
epoch 1  | loss: 1.05958 | eval_custom_logloss: 7.6587  |  0:00:01s
epoch 2  | loss: 0.91333 | eval_custom_logloss: 5.28344 |  0:00:01s
epoch 3  | loss: 0.89496 | eval_custom_logloss: 6.60556 |  0:00:02s
epoch 4  | loss: 0.8825  | eval_custom_logloss: 6.12739 |  0:00:02s
epoch 5  | loss: 0.79768 | eval_custom_logloss: 8.34005 |  0:00:03s
epoch 6  | loss: 0.77137 | eval_custom_logloss: 7.98339 |  0:00:03s
epoch 7  | loss: 0.78368 | eval_custom_logloss: 6.04477 |  0:00:04s
epoch 8  | loss: 0.77147 | eval_custom_logloss: 7.32291 |  0:00:04s
epoch 9  | loss: 0.7584  | eval_custom_logloss: 6.82734 |  0:00:05s
epoch 10 | loss: 0.80879 | eval_custom_logloss: 4.82736 |  0:00:05s
epoch 11 | loss: 0.74699 | eval_custom_logloss: 6.67997 |  0:00:06s
epoch 12 | loss: 0.73341 | eval_custom_logloss: 6.32492 |  0:00:06s
epoch 13 | loss: 0.67056 | eval_custom_logloss: 6.80573 |  0:00:07s
epoch 14 | loss: 0.69707 | eval_custom_logloss: 8.17676 |  0:00:08s
epoch 15 | loss: 0.64497 | eval_custom_logloss: 5.45439 |  0:00:08s
epoch 16 | loss: 0.63507 | eval_custom_logloss: 6.84335 |  0:00:09s
epoch 17 | loss: 0.63993 | eval_custom_logloss: 7.75088 |  0:00:09s
epoch 18 | loss: 0.63862 | eval_custom_logloss: 7.84708 |  0:00:10s
epoch 19 | loss: 0.62891 | eval_custom_logloss: 8.08672 |  0:00:10s
epoch 20 | loss: 0.61011 | eval_custom_logloss: 8.03223 |  0:00:11s
epoch 21 | loss: 0.61277 | eval_custom_logloss: 8.04051 |  0:00:11s
epoch 22 | loss: 0.59821 | eval_custom_logloss: 8.04051 |  0:00:12s
epoch 23 | loss: 0.57945 | eval_custom_logloss: 8.04051 |  0:00:12s
epoch 24 | loss: 0.5813  | eval_custom_logloss: 8.04051 |  0:00:13s
epoch 25 | loss: 0.56234 | eval_custom_logloss: 8.04051 |  0:00:13s
epoch 26 | loss: 0.59545 | eval_custom_logloss: 8.03647 |  0:00:14s
epoch 27 | loss: 0.63927 | eval_custom_logloss: 7.65207 |  0:00:14s
epoch 28 | loss: 0.60392 | eval_custom_logloss: 7.38218 |  0:00:15s
epoch 29 | loss: 0.60967 | eval_custom_logloss: 7.43558 |  0:00:16s
epoch 30 | loss: 0.60732 | eval_custom_logloss: 7.70361 |  0:00:16s

Early stopping occurred at epoch 30 with best_epoch = 10 and best_eval_custom_logloss = 4.82736
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.8339, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.23589 | eval_custom_logloss: 6.69303 |  0:00:00s
epoch 1  | loss: 0.97317 | eval_custom_logloss: 6.98905 |  0:00:01s
epoch 2  | loss: 0.86611 | eval_custom_logloss: 7.16876 |  0:00:01s
epoch 3  | loss: 0.82373 | eval_custom_logloss: 6.63319 |  0:00:02s
epoch 4  | loss: 0.81185 | eval_custom_logloss: 7.54345 |  0:00:02s
epoch 5  | loss: 0.80028 | eval_custom_logloss: 5.33921 |  0:00:03s
epoch 6  | loss: 0.77339 | eval_custom_logloss: 7.72685 |  0:00:03s
epoch 7  | loss: 0.72703 | eval_custom_logloss: 6.79307 |  0:00:04s
epoch 8  | loss: 0.68783 | eval_custom_logloss: 8.45802 |  0:00:04s
epoch 9  | loss: 0.76978 | eval_custom_logloss: 7.68587 |  0:00:05s
epoch 10 | loss: 0.72835 | eval_custom_logloss: 6.88571 |  0:00:05s
epoch 11 | loss: 0.70222 | eval_custom_logloss: 6.49959 |  0:00:06s
epoch 12 | loss: 0.65287 | eval_custom_logloss: 6.34359 |  0:00:07s
epoch 13 | loss: 0.62434 | eval_custom_logloss: 7.45968 |  0:00:07s
epoch 14 | loss: 0.64199 | eval_custom_logloss: 5.96692 |  0:00:08s
epoch 15 | loss: 0.6182  | eval_custom_logloss: 6.34291 |  0:00:08s
epoch 16 | loss: 0.58723 | eval_custom_logloss: 5.52192 |  0:00:09s
epoch 17 | loss: 0.5628  | eval_custom_logloss: 4.41324 |  0:00:09s
epoch 18 | loss: 0.57624 | eval_custom_logloss: 4.10688 |  0:00:10s
epoch 19 | loss: 0.57068 | eval_custom_logloss: 4.79048 |  0:00:10s
epoch 20 | loss: 0.57007 | eval_custom_logloss: 4.84168 |  0:00:11s
epoch 21 | loss: 0.56681 | eval_custom_logloss: 5.04546 |  0:00:11s
epoch 22 | loss: 0.5441  | eval_custom_logloss: 5.4054  |  0:00:12s
epoch 23 | loss: 0.52658 | eval_custom_logloss: 5.08507 |  0:00:12s
epoch 24 | loss: 0.52368 | eval_custom_logloss: 3.84189 |  0:00:13s
epoch 25 | loss: 0.52285 | eval_custom_logloss: 3.90201 |  0:00:14s
epoch 26 | loss: 0.48289 | eval_custom_logloss: 4.60055 |  0:00:14s
epoch 27 | loss: 0.47723 | eval_custom_logloss: 5.00139 |  0:00:15s
epoch 28 | loss: 0.49711 | eval_custom_logloss: 4.04391 |  0:00:15s
epoch 29 | loss: 0.46229 | eval_custom_logloss: 4.60411 |  0:00:16s
epoch 30 | loss: 0.48494 | eval_custom_logloss: 4.26479 |  0:00:16s
epoch 31 | loss: 0.47773 | eval_custom_logloss: 3.72175 |  0:00:17s
epoch 32 | loss: 0.45026 | eval_custom_logloss: 3.93737 |  0:00:17s
epoch 33 | loss: 0.49611 | eval_custom_logloss: 2.00077 |  0:00:18s
epoch 34 | loss: 0.47768 | eval_custom_logloss: 3.39618 |  0:00:18s
epoch 35 | loss: 0.46706 | eval_custom_logloss: 2.60141 |  0:00:19s
epoch 36 | loss: 0.45789 | eval_custom_logloss: 2.35624 |  0:00:20s
epoch 37 | loss: 0.4625  | eval_custom_logloss: 2.63168 |  0:00:20s
epoch 38 | loss: 0.4715  | eval_custom_logloss: 2.19996 |  0:00:21s
epoch 39 | loss: 0.44663 | eval_custom_logloss: 2.61308 |  0:00:21s
epoch 40 | loss: 0.44671 | eval_custom_logloss: 2.70035 |  0:00:22s
epoch 41 | loss: 0.47011 | eval_custom_logloss: 2.9836  |  0:00:22s
epoch 42 | loss: 0.44886 | eval_custom_logloss: 3.42914 |  0:00:23s
epoch 43 | loss: 0.46589 | eval_custom_logloss: 4.22626 |  0:00:23s
epoch 44 | loss: 0.4529  | eval_custom_logloss: 3.54987 |  0:00:24s
epoch 45 | loss: 0.47582 | eval_custom_logloss: 4.2742  |  0:00:24s
epoch 46 | loss: 0.46072 | eval_custom_logloss: 4.01245 |  0:00:25s
epoch 47 | loss: 0.46095 | eval_custom_logloss: 1.59912 |  0:00:25s
epoch 48 | loss: 0.43716 | eval_custom_logloss: 1.57026 |  0:00:26s
epoch 49 | loss: 0.42722 | eval_custom_logloss: 1.48741 |  0:00:26s
epoch 50 | loss: 0.41323 | eval_custom_logloss: 1.41604 |  0:00:27s
epoch 51 | loss: 0.42297 | eval_custom_logloss: 1.78643 |  0:00:28s
epoch 52 | loss: 0.39132 | eval_custom_logloss: 1.93451 |  0:00:28s
epoch 53 | loss: 0.39669 | eval_custom_logloss: 2.05173 |  0:00:29s
epoch 54 | loss: 0.4122  | eval_custom_logloss: 1.64374 |  0:00:29s
epoch 55 | loss: 0.40669 | eval_custom_logloss: 1.40228 |  0:00:30s
epoch 56 | loss: 0.43153 | eval_custom_logloss: 1.93389 |  0:00:30s
epoch 57 | loss: 0.39117 | eval_custom_logloss: 2.00078 |  0:00:31s
epoch 58 | loss: 0.39489 | eval_custom_logloss: 2.13949 |  0:00:31s
epoch 59 | loss: 0.35445 | eval_custom_logloss: 2.04583 |  0:00:32s
epoch 60 | loss: 0.35956 | eval_custom_logloss: 1.87414 |  0:00:33s
epoch 61 | loss: 0.36957 | eval_custom_logloss: 1.76384 |  0:00:33s
epoch 62 | loss: 0.3749  | eval_custom_logloss: 2.1776  |  0:00:34s
epoch 63 | loss: 0.40253 | eval_custom_logloss: 1.37612 |  0:00:34s
epoch 64 | loss: 0.40575 | eval_custom_logloss: 1.51645 |  0:00:35s
epoch 65 | loss: 0.37535 | eval_custom_logloss: 1.46121 |  0:00:35s
epoch 66 | loss: 0.38754 | eval_custom_logloss: 1.87944 |  0:00:36s
epoch 67 | loss: 0.36355 | eval_custom_logloss: 2.20009 |  0:00:36s
epoch 68 | loss: 0.35478 | eval_custom_logloss: 1.62986 |  0:00:37s
epoch 69 | loss: 0.33925 | eval_custom_logloss: 2.12269 |  0:00:37s
epoch 70 | loss: 0.32403 | eval_custom_logloss: 1.96866 |  0:00:38s
epoch 71 | loss: 0.32096 | eval_custom_logloss: 2.42072 |  0:00:38s
epoch 72 | loss: 0.30441 | eval_custom_logloss: 1.74245 |  0:00:39s
epoch 73 | loss: 0.332   | eval_custom_logloss: 1.31284 |  0:00:39s
epoch 74 | loss: 0.36286 | eval_custom_logloss: 1.82952 |  0:00:40s
epoch 75 | loss: 0.36393 | eval_custom_logloss: 2.05049 |  0:00:41s
epoch 76 | loss: 0.32399 | eval_custom_logloss: 2.54727 |  0:00:41s
epoch 77 | loss: 0.34643 | eval_custom_logloss: 2.46472 |  0:00:42s
epoch 78 | loss: 0.32357 | eval_custom_logloss: 2.0184  |  0:00:42s
epoch 79 | loss: 0.33084 | eval_custom_logloss: 2.21674 |  0:00:43s
epoch 80 | loss: 0.33105 | eval_custom_logloss: 2.13949 |  0:00:43s
epoch 81 | loss: 0.31538 | eval_custom_logloss: 2.14414 |  0:00:44s
epoch 82 | loss: 0.32573 | eval_custom_logloss: 2.12684 |  0:00:44s
epoch 83 | loss: 0.33855 | eval_custom_logloss: 2.12249 |  0:00:45s
epoch 84 | loss: 0.28104 | eval_custom_logloss: 2.12216 |  0:00:45s
epoch 85 | loss: 0.27755 | eval_custom_logloss: 2.21433 |  0:00:46s
epoch 86 | loss: 0.27243 | eval_custom_logloss: 1.85547 |  0:00:46s
epoch 87 | loss: 0.27031 | eval_custom_logloss: 1.56344 |  0:00:47s
epoch 88 | loss: 0.293   | eval_custom_logloss: 1.55152 |  0:00:47s
epoch 89 | loss: 0.28447 | eval_custom_logloss: 1.94614 |  0:00:48s
epoch 90 | loss: 0.29323 | eval_custom_logloss: 2.35952 |  0:00:49s
epoch 91 | loss: 0.30801 | eval_custom_logloss: 2.10842 |  0:00:49s
epoch 92 | loss: 0.26227 | eval_custom_logloss: 1.31339 |  0:00:50s
epoch 93 | loss: 0.24706 | eval_custom_logloss: 1.20712 |  0:00:50s
epoch 94 | loss: 0.25192 | eval_custom_logloss: 1.60795 |  0:00:51s
epoch 95 | loss: 0.25925 | eval_custom_logloss: 1.75115 |  0:00:51s
epoch 96 | loss: 0.24521 | eval_custom_logloss: 1.9474  |  0:00:52s
epoch 97 | loss: 0.24997 | eval_custom_logloss: 1.78338 |  0:00:52s
epoch 98 | loss: 0.2595  | eval_custom_logloss: 1.79817 |  0:00:53s
epoch 99 | loss: 0.24687 | eval_custom_logloss: 2.2179  |  0:00:53s
Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_eval_custom_logloss = 1.20712
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.49215, 'Log Loss - std': 1.34175} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.1768  | eval_custom_logloss: 5.44962 |  0:00:00s
epoch 1  | loss: 0.9508  | eval_custom_logloss: 6.7586  |  0:00:01s
epoch 2  | loss: 0.98142 | eval_custom_logloss: 4.96401 |  0:00:01s
epoch 3  | loss: 0.87572 | eval_custom_logloss: 7.07722 |  0:00:02s
epoch 4  | loss: 0.82189 | eval_custom_logloss: 7.08321 |  0:00:02s
epoch 5  | loss: 0.79562 | eval_custom_logloss: 6.45588 |  0:00:03s
epoch 6  | loss: 0.73953 | eval_custom_logloss: 8.03687 |  0:00:03s
epoch 7  | loss: 0.72612 | eval_custom_logloss: 5.71114 |  0:00:04s
epoch 8  | loss: 0.72647 | eval_custom_logloss: 6.86209 |  0:00:04s
epoch 9  | loss: 0.72308 | eval_custom_logloss: 6.86018 |  0:00:05s
epoch 10 | loss: 0.75299 | eval_custom_logloss: 7.77888 |  0:00:05s
epoch 11 | loss: 0.71413 | eval_custom_logloss: 7.91135 |  0:00:06s
epoch 12 | loss: 0.68327 | eval_custom_logloss: 5.35286 |  0:00:06s
epoch 13 | loss: 0.70345 | eval_custom_logloss: 5.41977 |  0:00:07s
epoch 14 | loss: 0.71299 | eval_custom_logloss: 4.89674 |  0:00:08s
epoch 15 | loss: 0.67719 | eval_custom_logloss: 4.62864 |  0:00:08s
epoch 16 | loss: 0.65228 | eval_custom_logloss: 7.65465 |  0:00:09s
epoch 17 | loss: 0.68302 | eval_custom_logloss: 6.33686 |  0:00:09s
epoch 18 | loss: 0.69037 | eval_custom_logloss: 7.29494 |  0:00:10s
epoch 19 | loss: 0.66164 | eval_custom_logloss: 3.27445 |  0:00:10s
epoch 20 | loss: 0.6263  | eval_custom_logloss: 3.14058 |  0:00:11s
epoch 21 | loss: 0.6232  | eval_custom_logloss: 3.00461 |  0:00:11s
epoch 22 | loss: 0.61542 | eval_custom_logloss: 3.19627 |  0:00:12s
epoch 23 | loss: 0.61351 | eval_custom_logloss: 3.83839 |  0:00:12s
epoch 24 | loss: 0.59273 | eval_custom_logloss: 3.5663  |  0:00:13s
epoch 25 | loss: 0.62265 | eval_custom_logloss: 2.35498 |  0:00:14s
epoch 26 | loss: 0.60312 | eval_custom_logloss: 3.52526 |  0:00:14s
epoch 27 | loss: 0.62402 | eval_custom_logloss: 4.29581 |  0:00:15s
epoch 28 | loss: 0.60753 | eval_custom_logloss: 4.22663 |  0:00:15s
epoch 29 | loss: 0.61742 | eval_custom_logloss: 5.06106 |  0:00:16s
epoch 30 | loss: 0.61449 | eval_custom_logloss: 4.44479 |  0:00:16s
epoch 31 | loss: 0.58686 | eval_custom_logloss: 4.43122 |  0:00:17s
epoch 32 | loss: 0.59233 | eval_custom_logloss: 3.83198 |  0:00:17s
epoch 33 | loss: 0.60121 | eval_custom_logloss: 3.82683 |  0:00:18s
epoch 34 | loss: 0.57561 | eval_custom_logloss: 3.89189 |  0:00:18s
epoch 35 | loss: 0.58068 | eval_custom_logloss: 3.68395 |  0:00:19s
epoch 36 | loss: 0.55868 | eval_custom_logloss: 3.20517 |  0:00:19s
epoch 37 | loss: 0.55539 | eval_custom_logloss: 4.07615 |  0:00:20s
epoch 38 | loss: 0.58375 | eval_custom_logloss: 4.17197 |  0:00:21s
epoch 39 | loss: 0.56948 | eval_custom_logloss: 2.9473  |  0:00:21s
epoch 40 | loss: 0.55972 | eval_custom_logloss: 3.13675 |  0:00:22s
epoch 41 | loss: 0.5477  | eval_custom_logloss: 3.41155 |  0:00:22s
epoch 42 | loss: 0.56476 | eval_custom_logloss: 3.49587 |  0:00:23s
epoch 43 | loss: 0.55842 | eval_custom_logloss: 3.49912 |  0:00:23s
epoch 44 | loss: 0.5509  | eval_custom_logloss: 3.3508  |  0:00:24s
epoch 45 | loss: 0.54658 | eval_custom_logloss: 3.42638 |  0:00:24s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 2.35498
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.3384666666666667, 'Log Loss - std': 1.1168851816050245} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11766 | eval_custom_logloss: 6.30739 |  0:00:00s
epoch 1  | loss: 0.94859 | eval_custom_logloss: 6.11253 |  0:00:01s
epoch 2  | loss: 0.86566 | eval_custom_logloss: 4.89724 |  0:00:01s
epoch 3  | loss: 0.86084 | eval_custom_logloss: 7.34506 |  0:00:02s
epoch 4  | loss: 0.8053  | eval_custom_logloss: 6.47647 |  0:00:02s
epoch 5  | loss: 0.74842 | eval_custom_logloss: 7.48021 |  0:00:03s
epoch 6  | loss: 0.77349 | eval_custom_logloss: 7.87016 |  0:00:03s
epoch 7  | loss: 0.7291  | eval_custom_logloss: 7.26647 |  0:00:04s
epoch 8  | loss: 0.6857  | eval_custom_logloss: 7.96276 |  0:00:04s
epoch 9  | loss: 0.67845 | eval_custom_logloss: 5.18565 |  0:00:05s
epoch 10 | loss: 0.66321 | eval_custom_logloss: 5.72067 |  0:00:05s
epoch 11 | loss: 0.62396 | eval_custom_logloss: 6.47293 |  0:00:06s
epoch 12 | loss: 0.59218 | eval_custom_logloss: 6.41429 |  0:00:07s
epoch 13 | loss: 0.61502 | eval_custom_logloss: 6.83935 |  0:00:07s
epoch 14 | loss: 0.66369 | eval_custom_logloss: 7.17202 |  0:00:08s
epoch 15 | loss: 0.61066 | eval_custom_logloss: 6.33448 |  0:00:08s
epoch 16 | loss: 0.56872 | eval_custom_logloss: 6.94936 |  0:00:09s
epoch 17 | loss: 0.54814 | eval_custom_logloss: 6.3958  |  0:00:09s
epoch 18 | loss: 0.5407  | eval_custom_logloss: 4.4436  |  0:00:10s
epoch 19 | loss: 0.53286 | eval_custom_logloss: 3.63316 |  0:00:10s
epoch 20 | loss: 0.54337 | eval_custom_logloss: 4.76687 |  0:00:11s
epoch 21 | loss: 0.51323 | eval_custom_logloss: 4.54604 |  0:00:11s
epoch 22 | loss: 0.50862 | eval_custom_logloss: 4.01456 |  0:00:12s
epoch 23 | loss: 0.48761 | eval_custom_logloss: 3.4006  |  0:00:12s
epoch 24 | loss: 0.5157  | eval_custom_logloss: 3.58831 |  0:00:13s
epoch 25 | loss: 0.47432 | eval_custom_logloss: 3.20231 |  0:00:14s
epoch 26 | loss: 0.46985 | eval_custom_logloss: 3.32852 |  0:00:14s
epoch 27 | loss: 0.49665 | eval_custom_logloss: 2.18096 |  0:00:15s
epoch 28 | loss: 0.48174 | eval_custom_logloss: 2.72847 |  0:00:15s
epoch 29 | loss: 0.46019 | eval_custom_logloss: 2.53399 |  0:00:16s
epoch 30 | loss: 0.45418 | eval_custom_logloss: 2.7072  |  0:00:16s
epoch 31 | loss: 0.44497 | eval_custom_logloss: 2.33243 |  0:00:17s
epoch 32 | loss: 0.44103 | eval_custom_logloss: 2.45127 |  0:00:17s
epoch 33 | loss: 0.43273 | eval_custom_logloss: 2.07804 |  0:00:18s
epoch 34 | loss: 0.41871 | eval_custom_logloss: 2.22599 |  0:00:18s
epoch 35 | loss: 0.40717 | eval_custom_logloss: 1.89085 |  0:00:19s
epoch 36 | loss: 0.44447 | eval_custom_logloss: 1.97331 |  0:00:20s
epoch 37 | loss: 0.43704 | eval_custom_logloss: 1.72023 |  0:00:20s
epoch 38 | loss: 0.44707 | eval_custom_logloss: 1.78471 |  0:00:21s
epoch 39 | loss: 0.44042 | eval_custom_logloss: 2.47196 |  0:00:21s
epoch 40 | loss: 0.4025  | eval_custom_logloss: 2.99827 |  0:00:22s
epoch 41 | loss: 0.37394 | eval_custom_logloss: 2.55884 |  0:00:22s
epoch 42 | loss: 0.36082 | eval_custom_logloss: 2.76038 |  0:00:23s
epoch 43 | loss: 0.33277 | eval_custom_logloss: 2.56664 |  0:00:23s
epoch 44 | loss: 0.36621 | eval_custom_logloss: 2.8854  |  0:00:24s
epoch 45 | loss: 0.35366 | eval_custom_logloss: 2.66528 |  0:00:24s
epoch 46 | loss: 0.30878 | eval_custom_logloss: 2.27217 |  0:00:25s
epoch 47 | loss: 0.32087 | eval_custom_logloss: 2.00414 |  0:00:25s
epoch 48 | loss: 0.32549 | eval_custom_logloss: 2.4126  |  0:00:26s
epoch 49 | loss: 0.33935 | eval_custom_logloss: 2.43009 |  0:00:26s
epoch 50 | loss: 0.36654 | eval_custom_logloss: 1.77594 |  0:00:27s
epoch 51 | loss: 0.32432 | eval_custom_logloss: 1.89018 |  0:00:28s
epoch 52 | loss: 0.30241 | eval_custom_logloss: 2.00357 |  0:00:28s
epoch 53 | loss: 0.29949 | eval_custom_logloss: 2.32021 |  0:00:29s
epoch 54 | loss: 0.30826 | eval_custom_logloss: 2.28404 |  0:00:29s
epoch 55 | loss: 0.31004 | eval_custom_logloss: 2.08287 |  0:00:30s
epoch 56 | loss: 0.34451 | eval_custom_logloss: 2.34646 |  0:00:30s
epoch 57 | loss: 0.34006 | eval_custom_logloss: 2.07386 |  0:00:31s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 1.72023
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1197999999999997, 'Log Loss - std': 1.0387587376287142} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.17837 | eval_custom_logloss: 6.97389 |  0:00:00s
epoch 1  | loss: 0.98205 | eval_custom_logloss: 4.63719 |  0:00:01s
epoch 2  | loss: 0.91743 | eval_custom_logloss: 5.70053 |  0:00:01s
epoch 3  | loss: 0.84595 | eval_custom_logloss: 4.56115 |  0:00:02s
epoch 4  | loss: 0.85969 | eval_custom_logloss: 5.31863 |  0:00:02s
epoch 5  | loss: 0.79252 | eval_custom_logloss: 6.29256 |  0:00:03s
epoch 6  | loss: 0.75691 | eval_custom_logloss: 5.75029 |  0:00:03s
epoch 7  | loss: 0.73382 | eval_custom_logloss: 7.46293 |  0:00:04s
epoch 8  | loss: 0.77833 | eval_custom_logloss: 7.44058 |  0:00:04s
epoch 9  | loss: 0.78351 | eval_custom_logloss: 6.7388  |  0:00:05s
epoch 10 | loss: 0.75531 | eval_custom_logloss: 7.07444 |  0:00:05s
epoch 11 | loss: 0.79798 | eval_custom_logloss: 5.79357 |  0:00:06s
epoch 12 | loss: 0.72695 | eval_custom_logloss: 8.56574 |  0:00:06s
epoch 13 | loss: 0.73259 | eval_custom_logloss: 6.89166 |  0:00:07s
epoch 14 | loss: 0.67991 | eval_custom_logloss: 4.90579 |  0:00:08s
epoch 15 | loss: 0.70134 | eval_custom_logloss: 4.87498 |  0:00:08s
epoch 16 | loss: 0.65685 | eval_custom_logloss: 5.52971 |  0:00:09s
epoch 17 | loss: 0.64271 | eval_custom_logloss: 4.92449 |  0:00:09s
epoch 18 | loss: 0.61268 | eval_custom_logloss: 4.15163 |  0:00:10s
epoch 19 | loss: 0.58982 | eval_custom_logloss: 5.55326 |  0:00:10s
epoch 20 | loss: 0.59207 | eval_custom_logloss: 4.21951 |  0:00:11s
epoch 21 | loss: 0.60244 | eval_custom_logloss: 4.69863 |  0:00:11s
epoch 22 | loss: 0.58311 | eval_custom_logloss: 3.37824 |  0:00:12s
epoch 23 | loss: 0.56083 | eval_custom_logloss: 3.11128 |  0:00:12s
epoch 24 | loss: 0.57255 | eval_custom_logloss: 2.99702 |  0:00:13s
epoch 25 | loss: 0.64012 | eval_custom_logloss: 2.97931 |  0:00:13s
epoch 26 | loss: 0.58062 | eval_custom_logloss: 2.68199 |  0:00:14s
epoch 27 | loss: 0.6031  | eval_custom_logloss: 2.50302 |  0:00:15s
epoch 28 | loss: 0.57495 | eval_custom_logloss: 2.16008 |  0:00:15s
epoch 29 | loss: 0.60215 | eval_custom_logloss: 1.9279  |  0:00:16s
epoch 30 | loss: 0.57335 | eval_custom_logloss: 2.16831 |  0:00:16s
epoch 31 | loss: 0.55838 | eval_custom_logloss: 2.43332 |  0:00:17s
epoch 32 | loss: 0.54422 | eval_custom_logloss: 2.21747 |  0:00:17s
epoch 33 | loss: 0.55328 | eval_custom_logloss: 1.75584 |  0:00:18s
epoch 34 | loss: 0.52678 | eval_custom_logloss: 1.4286  |  0:00:18s
epoch 35 | loss: 0.4895  | eval_custom_logloss: 1.69167 |  0:00:19s
epoch 36 | loss: 0.53575 | eval_custom_logloss: 1.79381 |  0:00:19s
epoch 37 | loss: 0.57242 | eval_custom_logloss: 1.65818 |  0:00:20s
epoch 38 | loss: 0.56586 | eval_custom_logloss: 2.48394 |  0:00:21s
epoch 39 | loss: 0.54712 | eval_custom_logloss: 1.76666 |  0:00:21s
epoch 40 | loss: 0.55413 | eval_custom_logloss: 1.3214  |  0:00:22s
epoch 41 | loss: 0.53478 | eval_custom_logloss: 1.07878 |  0:00:22s
epoch 42 | loss: 0.51311 | eval_custom_logloss: 1.27661 |  0:00:23s
epoch 43 | loss: 0.50448 | eval_custom_logloss: 1.06405 |  0:00:23s
epoch 44 | loss: 0.53564 | eval_custom_logloss: 1.95853 |  0:00:24s
epoch 45 | loss: 0.50626 | eval_custom_logloss: 1.15911 |  0:00:24s
epoch 46 | loss: 0.51508 | eval_custom_logloss: 1.43343 |  0:00:25s
epoch 47 | loss: 0.51396 | eval_custom_logloss: 1.08359 |  0:00:25s
epoch 48 | loss: 0.5518  | eval_custom_logloss: 1.33398 |  0:00:26s
epoch 49 | loss: 0.56653 | eval_custom_logloss: 1.31533 |  0:00:27s
epoch 50 | loss: 0.58314 | eval_custom_logloss: 1.54875 |  0:00:27s
epoch 51 | loss: 0.54224 | eval_custom_logloss: 1.66501 |  0:00:28s
epoch 52 | loss: 0.54703 | eval_custom_logloss: 1.61923 |  0:00:28s
epoch 53 | loss: 0.53743 | eval_custom_logloss: 2.27911 |  0:00:29s
epoch 54 | loss: 0.51603 | eval_custom_logloss: 2.32808 |  0:00:29s
epoch 55 | loss: 0.49234 | eval_custom_logloss: 2.21923 |  0:00:30s
epoch 56 | loss: 0.49001 | eval_custom_logloss: 1.82599 |  0:00:30s
epoch 57 | loss: 0.44828 | eval_custom_logloss: 1.98713 |  0:00:31s
epoch 58 | loss: 0.47987 | eval_custom_logloss: 1.82274 |  0:00:31s
epoch 59 | loss: 0.46563 | eval_custom_logloss: 1.85607 |  0:00:32s
epoch 60 | loss: 0.4904  | eval_custom_logloss: 2.14252 |  0:00:32s
epoch 61 | loss: 0.46831 | eval_custom_logloss: 1.62684 |  0:00:33s
epoch 62 | loss: 0.44035 | eval_custom_logloss: 1.67062 |  0:00:33s
epoch 63 | loss: 0.43916 | eval_custom_logloss: 1.81813 |  0:00:34s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 1.06405
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.9026799999999997, 'Log Loss - std': 1.0255633328078768} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 1.9026799999999997 and parameters: {'n_d': 17, 'n_steps': 5, 'gamma': 1.6097568342451434, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0015831955175763435, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax', 'n_a': 23, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.38325 | eval_custom_logloss: 4.37646 |  0:00:00s
epoch 1  | loss: 1.04238 | eval_custom_logloss: 3.883   |  0:00:00s
epoch 2  | loss: 0.90691 | eval_custom_logloss: 3.70894 |  0:00:01s
epoch 3  | loss: 0.85203 | eval_custom_logloss: 3.62323 |  0:00:01s
epoch 4  | loss: 0.80586 | eval_custom_logloss: 4.80948 |  0:00:02s
epoch 5  | loss: 0.76701 | eval_custom_logloss: 4.53032 |  0:00:02s
epoch 6  | loss: 0.76855 | eval_custom_logloss: 4.41359 |  0:00:03s
epoch 7  | loss: 0.71665 | eval_custom_logloss: 4.86157 |  0:00:03s
epoch 8  | loss: 0.67498 | eval_custom_logloss: 5.91066 |  0:00:04s
epoch 9  | loss: 0.64825 | eval_custom_logloss: 4.74416 |  0:00:04s
epoch 10 | loss: 0.65114 | eval_custom_logloss: 4.10191 |  0:00:05s
epoch 11 | loss: 0.592   | eval_custom_logloss: 3.25033 |  0:00:05s
epoch 12 | loss: 0.61806 | eval_custom_logloss: 2.71902 |  0:00:06s
epoch 13 | loss: 0.57165 | eval_custom_logloss: 2.69037 |  0:00:06s
epoch 14 | loss: 0.56256 | eval_custom_logloss: 3.44023 |  0:00:07s
epoch 15 | loss: 0.5552  | eval_custom_logloss: 3.31388 |  0:00:07s
epoch 16 | loss: 0.54083 | eval_custom_logloss: 3.64633 |  0:00:08s
epoch 17 | loss: 0.54383 | eval_custom_logloss: 3.37825 |  0:00:08s
epoch 18 | loss: 0.54394 | eval_custom_logloss: 1.93725 |  0:00:09s
epoch 19 | loss: 0.52092 | eval_custom_logloss: 1.97009 |  0:00:09s
epoch 20 | loss: 0.54311 | eval_custom_logloss: 2.21651 |  0:00:10s
epoch 21 | loss: 0.50647 | eval_custom_logloss: 2.81981 |  0:00:10s
epoch 22 | loss: 0.48049 | eval_custom_logloss: 1.81987 |  0:00:11s
epoch 23 | loss: 0.47674 | eval_custom_logloss: 2.30702 |  0:00:11s
epoch 24 | loss: 0.46458 | eval_custom_logloss: 2.86678 |  0:00:12s
epoch 25 | loss: 0.4485  | eval_custom_logloss: 2.11978 |  0:00:12s
epoch 26 | loss: 0.42128 | eval_custom_logloss: 1.87809 |  0:00:13s
epoch 27 | loss: 0.40778 | eval_custom_logloss: 2.36033 |  0:00:13s
epoch 28 | loss: 0.42069 | eval_custom_logloss: 1.99024 |  0:00:14s
epoch 29 | loss: 0.3782  | eval_custom_logloss: 1.64814 |  0:00:14s
epoch 30 | loss: 0.37834 | eval_custom_logloss: 1.8391  |  0:00:15s
epoch 31 | loss: 0.41364 | eval_custom_logloss: 1.99496 |  0:00:15s
epoch 32 | loss: 0.36766 | eval_custom_logloss: 2.14709 |  0:00:15s
epoch 33 | loss: 0.38532 | eval_custom_logloss: 1.74139 |  0:00:16s
epoch 34 | loss: 0.36114 | eval_custom_logloss: 1.51356 |  0:00:16s
epoch 35 | loss: 0.34049 | eval_custom_logloss: 1.48297 |  0:00:17s
epoch 36 | loss: 0.36405 | eval_custom_logloss: 1.32756 |  0:00:17s
epoch 37 | loss: 0.35507 | eval_custom_logloss: 1.56364 |  0:00:18s
epoch 38 | loss: 0.3763  | eval_custom_logloss: 1.37814 |  0:00:18s
epoch 39 | loss: 0.38935 | eval_custom_logloss: 1.29687 |  0:00:19s
epoch 40 | loss: 0.38818 | eval_custom_logloss: 1.3423  |  0:00:19s
epoch 41 | loss: 0.45355 | eval_custom_logloss: 1.49011 |  0:00:20s
epoch 42 | loss: 0.42959 | eval_custom_logloss: 1.49742 |  0:00:20s
epoch 43 | loss: 0.42093 | eval_custom_logloss: 1.61981 |  0:00:21s
epoch 44 | loss: 0.38363 | eval_custom_logloss: 1.46998 |  0:00:21s
epoch 45 | loss: 0.40759 | eval_custom_logloss: 1.31325 |  0:00:22s
epoch 46 | loss: 0.39593 | eval_custom_logloss: 1.51168 |  0:00:22s
epoch 47 | loss: 0.38609 | eval_custom_logloss: 1.59715 |  0:00:23s
epoch 48 | loss: 0.38422 | eval_custom_logloss: 1.2317  |  0:00:23s
epoch 49 | loss: 0.34656 | eval_custom_logloss: 1.28448 |  0:00:24s
epoch 50 | loss: 0.32496 | eval_custom_logloss: 1.46597 |  0:00:24s
epoch 51 | loss: 0.33088 | eval_custom_logloss: 1.65188 |  0:00:25s
epoch 52 | loss: 0.33254 | eval_custom_logloss: 1.53721 |  0:00:25s
epoch 53 | loss: 0.3488  | eval_custom_logloss: 1.38016 |  0:00:26s
epoch 54 | loss: 0.29741 | eval_custom_logloss: 1.36314 |  0:00:26s
epoch 55 | loss: 0.3186  | eval_custom_logloss: 1.37503 |  0:00:27s
epoch 56 | loss: 0.28286 | eval_custom_logloss: 1.20928 |  0:00:27s
epoch 57 | loss: 0.27844 | eval_custom_logloss: 1.22595 |  0:00:27s
epoch 58 | loss: 0.30468 | eval_custom_logloss: 1.03418 |  0:00:28s
epoch 59 | loss: 0.29417 | eval_custom_logloss: 1.34976 |  0:00:28s
epoch 60 | loss: 0.28405 | eval_custom_logloss: 1.22086 |  0:00:29s
epoch 61 | loss: 0.28895 | eval_custom_logloss: 1.26881 |  0:00:29s
epoch 62 | loss: 0.28797 | eval_custom_logloss: 1.05896 |  0:00:30s
epoch 63 | loss: 0.28968 | eval_custom_logloss: 1.17548 |  0:00:30s
epoch 64 | loss: 0.28343 | eval_custom_logloss: 1.0119  |  0:00:31s
epoch 65 | loss: 0.24694 | eval_custom_logloss: 1.35224 |  0:00:31s
epoch 66 | loss: 0.24817 | eval_custom_logloss: 1.29945 |  0:00:32s
epoch 67 | loss: 0.25444 | eval_custom_logloss: 1.20168 |  0:00:32s
epoch 68 | loss: 0.29122 | eval_custom_logloss: 1.25371 |  0:00:33s
epoch 69 | loss: 0.25709 | eval_custom_logloss: 1.39706 |  0:00:33s
epoch 70 | loss: 0.21411 | eval_custom_logloss: 1.42545 |  0:00:34s
epoch 71 | loss: 0.261   | eval_custom_logloss: 1.34347 |  0:00:34s
epoch 72 | loss: 0.21959 | eval_custom_logloss: 1.26838 |  0:00:35s
epoch 73 | loss: 0.20361 | eval_custom_logloss: 1.32999 |  0:00:35s
epoch 74 | loss: 0.21514 | eval_custom_logloss: 1.42662 |  0:00:36s
epoch 75 | loss: 0.21884 | eval_custom_logloss: 1.44686 |  0:00:36s
epoch 76 | loss: 0.18446 | eval_custom_logloss: 1.32231 |  0:00:37s
epoch 77 | loss: 0.21698 | eval_custom_logloss: 1.18398 |  0:00:37s
epoch 78 | loss: 0.18555 | eval_custom_logloss: 1.32808 |  0:00:38s
epoch 79 | loss: 0.17209 | eval_custom_logloss: 1.41242 |  0:00:38s
epoch 80 | loss: 0.1907  | eval_custom_logloss: 1.22153 |  0:00:39s
epoch 81 | loss: 0.16975 | eval_custom_logloss: 1.23592 |  0:00:39s
epoch 82 | loss: 0.18322 | eval_custom_logloss: 1.19552 |  0:00:40s
epoch 83 | loss: 0.1773  | eval_custom_logloss: 1.25701 |  0:00:40s
epoch 84 | loss: 0.18418 | eval_custom_logloss: 1.0264  |  0:00:40s

Early stopping occurred at epoch 84 with best_epoch = 64 and best_eval_custom_logloss = 1.0119
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9473, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax', 'n_a': 23, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.45537 | eval_custom_logloss: 6.2654  |  0:00:00s
epoch 1  | loss: 1.03643 | eval_custom_logloss: 4.78011 |  0:00:01s
epoch 2  | loss: 0.90781 | eval_custom_logloss: 4.6816  |  0:00:01s
epoch 3  | loss: 0.81311 | eval_custom_logloss: 4.40954 |  0:00:02s
epoch 4  | loss: 0.82033 | eval_custom_logloss: 5.52949 |  0:00:02s
epoch 5  | loss: 0.77766 | eval_custom_logloss: 5.03545 |  0:00:02s
epoch 6  | loss: 0.73181 | eval_custom_logloss: 3.16454 |  0:00:03s
epoch 7  | loss: 0.68433 | eval_custom_logloss: 3.65387 |  0:00:03s
epoch 8  | loss: 0.68433 | eval_custom_logloss: 4.721   |  0:00:04s
epoch 9  | loss: 0.64935 | eval_custom_logloss: 3.89164 |  0:00:04s
epoch 10 | loss: 0.66247 | eval_custom_logloss: 4.42472 |  0:00:05s
epoch 11 | loss: 0.6389  | eval_custom_logloss: 3.40103 |  0:00:05s
epoch 12 | loss: 0.65043 | eval_custom_logloss: 5.13821 |  0:00:06s
epoch 13 | loss: 0.63904 | eval_custom_logloss: 5.88043 |  0:00:06s
epoch 14 | loss: 0.62696 | eval_custom_logloss: 5.75015 |  0:00:07s
epoch 15 | loss: 0.61401 | eval_custom_logloss: 5.27142 |  0:00:07s
epoch 16 | loss: 0.58373 | eval_custom_logloss: 4.55516 |  0:00:08s
epoch 17 | loss: 0.57872 | eval_custom_logloss: 5.53219 |  0:00:08s
epoch 18 | loss: 0.54657 | eval_custom_logloss: 4.63417 |  0:00:09s
epoch 19 | loss: 0.54491 | eval_custom_logloss: 4.51109 |  0:00:09s
epoch 20 | loss: 0.5604  | eval_custom_logloss: 3.06098 |  0:00:10s
epoch 21 | loss: 0.53433 | eval_custom_logloss: 3.10743 |  0:00:10s
epoch 22 | loss: 0.50544 | eval_custom_logloss: 2.80362 |  0:00:11s
epoch 23 | loss: 0.47986 | eval_custom_logloss: 3.53656 |  0:00:11s
epoch 24 | loss: 0.49074 | eval_custom_logloss: 3.64125 |  0:00:12s
epoch 25 | loss: 0.46668 | eval_custom_logloss: 3.0832  |  0:00:12s
epoch 26 | loss: 0.49111 | eval_custom_logloss: 2.59074 |  0:00:12s
epoch 27 | loss: 0.46213 | eval_custom_logloss: 3.12953 |  0:00:13s
epoch 28 | loss: 0.48275 | eval_custom_logloss: 3.05217 |  0:00:13s
epoch 29 | loss: 0.45143 | eval_custom_logloss: 2.8543  |  0:00:14s
epoch 30 | loss: 0.45572 | eval_custom_logloss: 3.42665 |  0:00:14s
epoch 31 | loss: 0.45845 | eval_custom_logloss: 2.62862 |  0:00:15s
epoch 32 | loss: 0.43563 | eval_custom_logloss: 3.20083 |  0:00:15s
epoch 33 | loss: 0.42086 | eval_custom_logloss: 2.59852 |  0:00:16s
epoch 34 | loss: 0.39968 | eval_custom_logloss: 2.45066 |  0:00:16s
epoch 35 | loss: 0.39286 | eval_custom_logloss: 2.41628 |  0:00:17s
epoch 36 | loss: 0.37941 | eval_custom_logloss: 2.24973 |  0:00:17s
epoch 37 | loss: 0.4203  | eval_custom_logloss: 1.96854 |  0:00:18s
epoch 38 | loss: 0.41349 | eval_custom_logloss: 1.67413 |  0:00:18s
epoch 39 | loss: 0.3933  | eval_custom_logloss: 2.44883 |  0:00:19s
epoch 40 | loss: 0.39844 | eval_custom_logloss: 2.31688 |  0:00:19s
epoch 41 | loss: 0.44327 | eval_custom_logloss: 1.86628 |  0:00:20s
epoch 42 | loss: 0.35968 | eval_custom_logloss: 1.91697 |  0:00:20s
epoch 43 | loss: 0.33915 | eval_custom_logloss: 2.16203 |  0:00:21s
epoch 44 | loss: 0.32264 | eval_custom_logloss: 2.56838 |  0:00:21s
epoch 45 | loss: 0.34114 | eval_custom_logloss: 2.07769 |  0:00:22s
epoch 46 | loss: 0.32927 | eval_custom_logloss: 2.03046 |  0:00:22s
epoch 47 | loss: 0.31992 | eval_custom_logloss: 1.96242 |  0:00:23s
epoch 48 | loss: 0.30932 | eval_custom_logloss: 2.14322 |  0:00:23s
epoch 49 | loss: 0.32284 | eval_custom_logloss: 1.95815 |  0:00:23s
epoch 50 | loss: 0.34143 | eval_custom_logloss: 1.75338 |  0:00:24s
epoch 51 | loss: 0.31165 | eval_custom_logloss: 2.4805  |  0:00:25s
epoch 52 | loss: 0.32654 | eval_custom_logloss: 2.58467 |  0:00:25s
epoch 53 | loss: 0.3515  | eval_custom_logloss: 1.96294 |  0:00:26s
epoch 54 | loss: 0.30899 | eval_custom_logloss: 1.77784 |  0:00:26s
epoch 55 | loss: 0.28256 | eval_custom_logloss: 1.82458 |  0:00:27s
epoch 56 | loss: 0.28461 | eval_custom_logloss: 1.77152 |  0:00:27s
epoch 57 | loss: 0.26762 | eval_custom_logloss: 1.67626 |  0:00:28s
epoch 58 | loss: 0.27015 | eval_custom_logloss: 1.63928 |  0:00:28s
epoch 59 | loss: 0.29011 | eval_custom_logloss: 1.53824 |  0:00:29s
epoch 60 | loss: 0.27128 | eval_custom_logloss: 1.64976 |  0:00:30s
epoch 61 | loss: 0.2633  | eval_custom_logloss: 1.5647  |  0:00:30s
epoch 62 | loss: 0.24808 | eval_custom_logloss: 1.77115 |  0:00:31s
epoch 63 | loss: 0.27658 | eval_custom_logloss: 1.38661 |  0:00:31s
epoch 64 | loss: 0.27009 | eval_custom_logloss: 1.72659 |  0:00:32s
epoch 65 | loss: 0.23384 | eval_custom_logloss: 1.80703 |  0:00:33s
epoch 66 | loss: 0.29691 | eval_custom_logloss: 1.35832 |  0:00:33s
epoch 67 | loss: 0.27359 | eval_custom_logloss: 1.42022 |  0:00:34s
epoch 68 | loss: 0.26994 | eval_custom_logloss: 1.3412  |  0:00:34s
epoch 69 | loss: 0.26924 | eval_custom_logloss: 1.32649 |  0:00:35s
epoch 70 | loss: 0.24126 | eval_custom_logloss: 1.79    |  0:00:35s
epoch 71 | loss: 0.21984 | eval_custom_logloss: 1.73784 |  0:00:36s
epoch 72 | loss: 0.25682 | eval_custom_logloss: 2.25235 |  0:00:37s
epoch 73 | loss: 0.25242 | eval_custom_logloss: 1.70006 |  0:00:37s
epoch 74 | loss: 0.22731 | eval_custom_logloss: 1.66905 |  0:00:38s
epoch 75 | loss: 0.20431 | eval_custom_logloss: 1.52498 |  0:00:38s
epoch 76 | loss: 0.23056 | eval_custom_logloss: 1.50715 |  0:00:39s
epoch 77 | loss: 0.21282 | eval_custom_logloss: 1.34801 |  0:00:39s
epoch 78 | loss: 0.20102 | eval_custom_logloss: 1.38786 |  0:00:40s
epoch 79 | loss: 0.19695 | eval_custom_logloss: 1.52909 |  0:00:41s
epoch 80 | loss: 0.19684 | eval_custom_logloss: 1.45308 |  0:00:41s
epoch 81 | loss: 0.21546 | eval_custom_logloss: 1.41739 |  0:00:42s
epoch 82 | loss: 0.21256 | eval_custom_logloss: 1.44082 |  0:00:42s
epoch 83 | loss: 0.20168 | eval_custom_logloss: 1.37779 |  0:00:43s
epoch 84 | loss: 0.16707 | eval_custom_logloss: 1.23264 |  0:00:43s
epoch 85 | loss: 0.19459 | eval_custom_logloss: 1.1958  |  0:00:44s
epoch 86 | loss: 0.1827  | eval_custom_logloss: 1.26379 |  0:00:45s
epoch 87 | loss: 0.18296 | eval_custom_logloss: 1.23401 |  0:00:45s
epoch 88 | loss: 0.17532 | eval_custom_logloss: 1.12322 |  0:00:46s
epoch 89 | loss: 0.15108 | eval_custom_logloss: 1.12521 |  0:00:46s
epoch 90 | loss: 0.21901 | eval_custom_logloss: 1.37157 |  0:00:47s
epoch 91 | loss: 0.24737 | eval_custom_logloss: 1.13725 |  0:00:47s
epoch 92 | loss: 0.24457 | eval_custom_logloss: 1.42168 |  0:00:48s
epoch 93 | loss: 0.23761 | eval_custom_logloss: 1.21741 |  0:00:49s
epoch 94 | loss: 0.20975 | eval_custom_logloss: 1.34047 |  0:00:49s
epoch 95 | loss: 0.17682 | eval_custom_logloss: 1.33144 |  0:00:50s
epoch 96 | loss: 0.2201  | eval_custom_logloss: 1.23971 |  0:00:50s
epoch 97 | loss: 0.17348 | eval_custom_logloss: 1.26679 |  0:00:51s
epoch 98 | loss: 0.20566 | eval_custom_logloss: 1.29199 |  0:00:51s
epoch 99 | loss: 0.1816  | eval_custom_logloss: 1.22722 |  0:00:52s
Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_eval_custom_logloss = 1.12322
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.988, 'Log Loss - std': 0.04069999999999996} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax', 'n_a': 23, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.49752 | eval_custom_logloss: 5.50834 |  0:00:00s
epoch 1  | loss: 1.11143 | eval_custom_logloss: 4.33782 |  0:00:00s
epoch 2  | loss: 1.01192 | eval_custom_logloss: 6.35557 |  0:00:01s
epoch 3  | loss: 0.8772  | eval_custom_logloss: 6.08547 |  0:00:01s
epoch 4  | loss: 0.84024 | eval_custom_logloss: 5.79559 |  0:00:02s
epoch 5  | loss: 0.76049 | eval_custom_logloss: 5.43576 |  0:00:02s
epoch 6  | loss: 0.76238 | eval_custom_logloss: 4.24745 |  0:00:03s
epoch 7  | loss: 0.76591 | eval_custom_logloss: 4.12466 |  0:00:03s
epoch 8  | loss: 0.74801 | eval_custom_logloss: 4.65824 |  0:00:04s
epoch 9  | loss: 0.69519 | eval_custom_logloss: 2.96151 |  0:00:04s
epoch 10 | loss: 0.68665 | eval_custom_logloss: 3.93187 |  0:00:05s
epoch 11 | loss: 0.6488  | eval_custom_logloss: 4.03838 |  0:00:05s
epoch 12 | loss: 0.63999 | eval_custom_logloss: 3.49536 |  0:00:06s
epoch 13 | loss: 0.63658 | eval_custom_logloss: 4.0769  |  0:00:06s
epoch 14 | loss: 0.58715 | eval_custom_logloss: 4.83938 |  0:00:07s
epoch 15 | loss: 0.58759 | eval_custom_logloss: 4.76872 |  0:00:07s
epoch 16 | loss: 0.58003 | eval_custom_logloss: 3.1543  |  0:00:08s
epoch 17 | loss: 0.56773 | eval_custom_logloss: 2.70459 |  0:00:08s
epoch 18 | loss: 0.56631 | eval_custom_logloss: 2.90747 |  0:00:09s
epoch 19 | loss: 0.56141 | eval_custom_logloss: 3.32672 |  0:00:09s
epoch 20 | loss: 0.54723 | eval_custom_logloss: 2.86137 |  0:00:10s
epoch 21 | loss: 0.52235 | eval_custom_logloss: 2.81468 |  0:00:10s
epoch 22 | loss: 0.52798 | eval_custom_logloss: 2.78036 |  0:00:11s
epoch 23 | loss: 0.5383  | eval_custom_logloss: 2.36763 |  0:00:12s
epoch 24 | loss: 0.52552 | eval_custom_logloss: 2.18305 |  0:00:12s
epoch 25 | loss: 0.51835 | eval_custom_logloss: 2.01792 |  0:00:13s
epoch 26 | loss: 0.51737 | eval_custom_logloss: 2.10535 |  0:00:13s
epoch 27 | loss: 0.47479 | eval_custom_logloss: 2.23    |  0:00:14s
epoch 28 | loss: 0.48818 | eval_custom_logloss: 2.04102 |  0:00:14s
epoch 29 | loss: 0.46479 | eval_custom_logloss: 1.98731 |  0:00:15s
epoch 30 | loss: 0.47719 | eval_custom_logloss: 1.96125 |  0:00:16s
epoch 31 | loss: 0.4882  | eval_custom_logloss: 2.14778 |  0:00:16s
epoch 32 | loss: 0.4737  | eval_custom_logloss: 2.13189 |  0:00:17s
epoch 33 | loss: 0.45514 | eval_custom_logloss: 2.12337 |  0:00:17s
epoch 34 | loss: 0.44432 | eval_custom_logloss: 2.10849 |  0:00:18s
epoch 35 | loss: 0.47357 | eval_custom_logloss: 1.65867 |  0:00:18s
epoch 36 | loss: 0.45053 | eval_custom_logloss: 1.89147 |  0:00:19s
epoch 37 | loss: 0.43361 | eval_custom_logloss: 1.47382 |  0:00:20s
epoch 38 | loss: 0.40511 | eval_custom_logloss: 1.30212 |  0:00:20s
epoch 39 | loss: 0.43607 | eval_custom_logloss: 1.35986 |  0:00:21s
epoch 40 | loss: 0.44208 | eval_custom_logloss: 1.2777  |  0:00:21s
epoch 41 | loss: 0.44791 | eval_custom_logloss: 0.97801 |  0:00:22s
epoch 42 | loss: 0.46695 | eval_custom_logloss: 1.08595 |  0:00:23s
epoch 43 | loss: 0.45434 | eval_custom_logloss: 1.20943 |  0:00:23s
epoch 44 | loss: 0.41559 | eval_custom_logloss: 1.15013 |  0:00:24s
epoch 45 | loss: 0.41629 | eval_custom_logloss: 1.17445 |  0:00:24s
epoch 46 | loss: 0.4087  | eval_custom_logloss: 0.89589 |  0:00:25s
epoch 47 | loss: 0.41932 | eval_custom_logloss: 1.05338 |  0:00:25s
epoch 48 | loss: 0.41439 | eval_custom_logloss: 0.91407 |  0:00:26s
epoch 49 | loss: 0.39113 | eval_custom_logloss: 0.87886 |  0:00:26s
epoch 50 | loss: 0.41099 | eval_custom_logloss: 1.02471 |  0:00:26s
epoch 51 | loss: 0.38599 | eval_custom_logloss: 1.25593 |  0:00:27s
epoch 52 | loss: 0.3723  | eval_custom_logloss: 1.35952 |  0:00:27s
epoch 53 | loss: 0.35937 | eval_custom_logloss: 1.33914 |  0:00:28s
epoch 54 | loss: 0.37575 | eval_custom_logloss: 1.35065 |  0:00:29s
epoch 55 | loss: 0.37067 | eval_custom_logloss: 1.234   |  0:00:29s
epoch 56 | loss: 0.3395  | eval_custom_logloss: 1.32676 |  0:00:30s
epoch 57 | loss: 0.33913 | eval_custom_logloss: 1.35754 |  0:00:30s
epoch 58 | loss: 0.32615 | eval_custom_logloss: 1.20504 |  0:00:31s
epoch 59 | loss: 0.34978 | eval_custom_logloss: 1.55518 |  0:00:31s
epoch 60 | loss: 0.33768 | eval_custom_logloss: 1.44742 |  0:00:32s
epoch 61 | loss: 0.33084 | eval_custom_logloss: 2.03303 |  0:00:33s
epoch 62 | loss: 0.3526  | eval_custom_logloss: 1.88804 |  0:00:33s
epoch 63 | loss: 0.33271 | eval_custom_logloss: 1.98339 |  0:00:34s
epoch 64 | loss: 0.34973 | eval_custom_logloss: 1.65697 |  0:00:34s
epoch 65 | loss: 0.32158 | eval_custom_logloss: 2.15023 |  0:00:35s
epoch 66 | loss: 0.30885 | eval_custom_logloss: 1.6413  |  0:00:35s
epoch 67 | loss: 0.32929 | eval_custom_logloss: 2.12899 |  0:00:36s
epoch 68 | loss: 0.31063 | eval_custom_logloss: 1.70065 |  0:00:37s
epoch 69 | loss: 0.31414 | eval_custom_logloss: 1.7145  |  0:00:37s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 0.87886
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9473333333333334, 'Log Loss - std': 0.06642200104048522} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax', 'n_a': 23, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.40212 | eval_custom_logloss: 5.45904 |  0:00:00s
epoch 1  | loss: 0.92758 | eval_custom_logloss: 4.32957 |  0:00:01s
epoch 2  | loss: 0.89499 | eval_custom_logloss: 4.41053 |  0:00:01s
epoch 3  | loss: 0.78709 | eval_custom_logloss: 3.96552 |  0:00:01s
epoch 4  | loss: 0.77917 | eval_custom_logloss: 4.16494 |  0:00:02s
epoch 5  | loss: 0.71955 | eval_custom_logloss: 4.36285 |  0:00:02s
epoch 6  | loss: 0.68844 | eval_custom_logloss: 4.37428 |  0:00:03s
epoch 7  | loss: 0.7025  | eval_custom_logloss: 3.41842 |  0:00:03s
epoch 8  | loss: 0.65002 | eval_custom_logloss: 4.03537 |  0:00:04s
epoch 9  | loss: 0.6338  | eval_custom_logloss: 6.55192 |  0:00:04s
epoch 10 | loss: 0.61766 | eval_custom_logloss: 6.17791 |  0:00:05s
epoch 11 | loss: 0.5933  | eval_custom_logloss: 5.59914 |  0:00:05s
epoch 12 | loss: 0.57945 | eval_custom_logloss: 5.49902 |  0:00:06s
epoch 13 | loss: 0.54156 | eval_custom_logloss: 5.7006  |  0:00:06s
epoch 14 | loss: 0.54633 | eval_custom_logloss: 4.89164 |  0:00:07s
epoch 15 | loss: 0.52093 | eval_custom_logloss: 3.01478 |  0:00:07s
epoch 16 | loss: 0.52386 | eval_custom_logloss: 2.02826 |  0:00:08s
epoch 17 | loss: 0.51349 | eval_custom_logloss: 2.02507 |  0:00:08s
epoch 18 | loss: 0.53558 | eval_custom_logloss: 2.03164 |  0:00:09s
epoch 19 | loss: 0.48889 | eval_custom_logloss: 2.08209 |  0:00:09s
epoch 20 | loss: 0.4918  | eval_custom_logloss: 2.01069 |  0:00:10s
epoch 21 | loss: 0.48633 | eval_custom_logloss: 1.66339 |  0:00:10s
epoch 22 | loss: 0.4686  | eval_custom_logloss: 1.70396 |  0:00:11s
epoch 23 | loss: 0.48634 | eval_custom_logloss: 2.37545 |  0:00:11s
epoch 24 | loss: 0.46219 | eval_custom_logloss: 2.17606 |  0:00:11s
epoch 25 | loss: 0.47852 | eval_custom_logloss: 2.37038 |  0:00:12s
epoch 26 | loss: 0.46177 | eval_custom_logloss: 3.16595 |  0:00:12s
epoch 27 | loss: 0.43269 | eval_custom_logloss: 2.62962 |  0:00:13s
epoch 28 | loss: 0.45697 | eval_custom_logloss: 2.62069 |  0:00:13s
epoch 29 | loss: 0.43465 | eval_custom_logloss: 2.00123 |  0:00:14s
epoch 30 | loss: 0.40085 | eval_custom_logloss: 2.54237 |  0:00:14s
epoch 31 | loss: 0.39355 | eval_custom_logloss: 2.86178 |  0:00:15s
epoch 32 | loss: 0.39633 | eval_custom_logloss: 3.00452 |  0:00:15s
epoch 33 | loss: 0.42156 | eval_custom_logloss: 2.25121 |  0:00:16s
epoch 34 | loss: 0.41487 | eval_custom_logloss: 2.69026 |  0:00:16s
epoch 35 | loss: 0.4169  | eval_custom_logloss: 2.6364  |  0:00:17s
epoch 36 | loss: 0.40217 | eval_custom_logloss: 2.33548 |  0:00:17s
epoch 37 | loss: 0.41682 | eval_custom_logloss: 2.30448 |  0:00:18s
epoch 38 | loss: 0.38479 | eval_custom_logloss: 2.41485 |  0:00:18s
epoch 39 | loss: 0.40342 | eval_custom_logloss: 1.81611 |  0:00:19s
epoch 40 | loss: 0.40448 | eval_custom_logloss: 1.87881 |  0:00:19s
epoch 41 | loss: 0.40774 | eval_custom_logloss: 1.4176  |  0:00:20s
epoch 42 | loss: 0.38589 | eval_custom_logloss: 1.73264 |  0:00:20s
epoch 43 | loss: 0.40435 | eval_custom_logloss: 1.77455 |  0:00:20s
epoch 44 | loss: 0.40014 | eval_custom_logloss: 1.81682 |  0:00:21s
epoch 45 | loss: 0.33728 | eval_custom_logloss: 2.03392 |  0:00:21s
epoch 46 | loss: 0.3741  | eval_custom_logloss: 2.35334 |  0:00:22s
epoch 47 | loss: 0.41178 | eval_custom_logloss: 2.59139 |  0:00:22s
epoch 48 | loss: 0.35691 | eval_custom_logloss: 2.56057 |  0:00:23s
epoch 49 | loss: 0.35298 | eval_custom_logloss: 2.34478 |  0:00:23s
epoch 50 | loss: 0.37321 | eval_custom_logloss: 2.43653 |  0:00:24s
epoch 51 | loss: 0.42254 | eval_custom_logloss: 2.46624 |  0:00:24s
epoch 52 | loss: 0.39277 | eval_custom_logloss: 1.73913 |  0:00:25s
epoch 53 | loss: 0.35662 | eval_custom_logloss: 1.87515 |  0:00:25s
epoch 54 | loss: 0.34673 | eval_custom_logloss: 1.94423 |  0:00:26s
epoch 55 | loss: 0.3928  | eval_custom_logloss: 1.93679 |  0:00:26s
epoch 56 | loss: 0.42386 | eval_custom_logloss: 1.68957 |  0:00:27s
epoch 57 | loss: 0.34884 | eval_custom_logloss: 1.99782 |  0:00:27s
epoch 58 | loss: 0.37317 | eval_custom_logloss: 2.07739 |  0:00:27s
epoch 59 | loss: 0.35985 | eval_custom_logloss: 1.94539 |  0:00:28s
epoch 60 | loss: 0.32209 | eval_custom_logloss: 2.08178 |  0:00:28s
epoch 61 | loss: 0.29828 | eval_custom_logloss: 2.68659 |  0:00:29s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 1.4176
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0447, 'Log Loss - std': 0.17818449708097503} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax', 'n_a': 23, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.42211 | eval_custom_logloss: 4.65999 |  0:00:00s
epoch 1  | loss: 1.0687  | eval_custom_logloss: 4.66564 |  0:00:01s
epoch 2  | loss: 1.08234 | eval_custom_logloss: 4.92223 |  0:00:01s
epoch 3  | loss: 0.90274 | eval_custom_logloss: 2.95205 |  0:00:02s
epoch 4  | loss: 0.78154 | eval_custom_logloss: 3.7778  |  0:00:02s
epoch 5  | loss: 0.82097 | eval_custom_logloss: 3.78594 |  0:00:03s
epoch 6  | loss: 0.73883 | eval_custom_logloss: 4.10708 |  0:00:03s
epoch 7  | loss: 0.74814 | eval_custom_logloss: 5.54023 |  0:00:04s
epoch 8  | loss: 0.72026 | eval_custom_logloss: 4.87124 |  0:00:04s
epoch 9  | loss: 0.69032 | eval_custom_logloss: 5.60444 |  0:00:05s
epoch 10 | loss: 0.67435 | eval_custom_logloss: 4.58448 |  0:00:05s
epoch 11 | loss: 0.65214 | eval_custom_logloss: 4.95975 |  0:00:06s
epoch 12 | loss: 0.61628 | eval_custom_logloss: 5.50171 |  0:00:07s
epoch 13 | loss: 0.65847 | eval_custom_logloss: 4.83434 |  0:00:07s
epoch 14 | loss: 0.68125 | eval_custom_logloss: 3.28796 |  0:00:07s
epoch 15 | loss: 0.63787 | eval_custom_logloss: 5.37493 |  0:00:08s
epoch 16 | loss: 0.58948 | eval_custom_logloss: 4.48949 |  0:00:08s
epoch 17 | loss: 0.59254 | eval_custom_logloss: 5.29398 |  0:00:09s
epoch 18 | loss: 0.58874 | eval_custom_logloss: 5.89557 |  0:00:09s
epoch 19 | loss: 0.61767 | eval_custom_logloss: 4.47179 |  0:00:10s
epoch 20 | loss: 0.61732 | eval_custom_logloss: 3.32222 |  0:00:10s
epoch 21 | loss: 0.5828  | eval_custom_logloss: 4.33486 |  0:00:10s
epoch 22 | loss: 0.56927 | eval_custom_logloss: 3.83594 |  0:00:11s
epoch 23 | loss: 0.63585 | eval_custom_logloss: 3.25134 |  0:00:11s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 2.95205
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.35452, 'Log Loss - std': 0.6398073941429561} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 1.35452 and parameters: {'n_d': 23, 'n_steps': 5, 'gamma': 1.5771579818956247, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0017197731300893776, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.34899 | eval_custom_logloss: 7.65866 |  0:00:00s
epoch 1  | loss: 1.01259 | eval_custom_logloss: 6.10596 |  0:00:01s
epoch 2  | loss: 1.05465 | eval_custom_logloss: 6.89514 |  0:00:01s
epoch 3  | loss: 0.90934 | eval_custom_logloss: 7.33747 |  0:00:02s
epoch 4  | loss: 0.87361 | eval_custom_logloss: 6.00159 |  0:00:02s
epoch 5  | loss: 0.85445 | eval_custom_logloss: 7.64673 |  0:00:03s
epoch 6  | loss: 0.80577 | eval_custom_logloss: 5.79939 |  0:00:03s
epoch 7  | loss: 0.79674 | eval_custom_logloss: 7.1932  |  0:00:04s
epoch 8  | loss: 0.86641 | eval_custom_logloss: 6.91824 |  0:00:04s
epoch 9  | loss: 0.83377 | eval_custom_logloss: 8.79691 |  0:00:05s
epoch 10 | loss: 0.7736  | eval_custom_logloss: 7.68417 |  0:00:05s
epoch 11 | loss: 0.73743 | eval_custom_logloss: 7.55168 |  0:00:06s
epoch 12 | loss: 0.75091 | eval_custom_logloss: 7.70672 |  0:00:07s
epoch 13 | loss: 0.7732  | eval_custom_logloss: 6.33515 |  0:00:07s
epoch 14 | loss: 0.85308 | eval_custom_logloss: 3.90665 |  0:00:08s
epoch 15 | loss: 0.76827 | eval_custom_logloss: 5.32081 |  0:00:08s
epoch 16 | loss: 0.72289 | eval_custom_logloss: 5.05517 |  0:00:09s
epoch 17 | loss: 0.6806  | eval_custom_logloss: 4.63018 |  0:00:09s
epoch 18 | loss: 0.67171 | eval_custom_logloss: 4.56072 |  0:00:10s
epoch 19 | loss: 0.68933 | eval_custom_logloss: 5.05333 |  0:00:10s
epoch 20 | loss: 0.67231 | eval_custom_logloss: 4.80621 |  0:00:11s
epoch 21 | loss: 0.67126 | eval_custom_logloss: 5.16324 |  0:00:11s
epoch 22 | loss: 0.68925 | eval_custom_logloss: 4.67913 |  0:00:12s
epoch 23 | loss: 0.67199 | eval_custom_logloss: 5.12772 |  0:00:13s
epoch 24 | loss: 0.66583 | eval_custom_logloss: 3.95437 |  0:00:13s
epoch 25 | loss: 0.62798 | eval_custom_logloss: 4.38822 |  0:00:14s
epoch 26 | loss: 0.61148 | eval_custom_logloss: 3.64766 |  0:00:14s
epoch 27 | loss: 0.60558 | eval_custom_logloss: 3.64064 |  0:00:15s
epoch 28 | loss: 0.60688 | eval_custom_logloss: 2.82776 |  0:00:15s
epoch 29 | loss: 0.58997 | eval_custom_logloss: 2.40747 |  0:00:16s
epoch 30 | loss: 0.60476 | eval_custom_logloss: 2.05666 |  0:00:16s
epoch 31 | loss: 0.57713 | eval_custom_logloss: 1.65208 |  0:00:17s
epoch 32 | loss: 0.58494 | eval_custom_logloss: 1.80098 |  0:00:17s
epoch 33 | loss: 0.59217 | eval_custom_logloss: 2.16566 |  0:00:18s
epoch 34 | loss: 0.62116 | eval_custom_logloss: 2.34736 |  0:00:19s
epoch 35 | loss: 0.58538 | eval_custom_logloss: 2.37533 |  0:00:19s
epoch 36 | loss: 0.61291 | eval_custom_logloss: 2.1164  |  0:00:20s
epoch 37 | loss: 0.6111  | eval_custom_logloss: 1.70066 |  0:00:20s
epoch 38 | loss: 0.59236 | eval_custom_logloss: 1.59803 |  0:00:21s
epoch 39 | loss: 0.60211 | eval_custom_logloss: 1.60036 |  0:00:21s
epoch 40 | loss: 0.58485 | eval_custom_logloss: 1.87689 |  0:00:22s
epoch 41 | loss: 0.57942 | eval_custom_logloss: 2.59145 |  0:00:22s
epoch 42 | loss: 0.59621 | eval_custom_logloss: 1.49197 |  0:00:23s
epoch 43 | loss: 0.63626 | eval_custom_logloss: 1.37668 |  0:00:23s
epoch 44 | loss: 0.61821 | eval_custom_logloss: 1.38193 |  0:00:24s
epoch 45 | loss: 0.58749 | eval_custom_logloss: 1.64161 |  0:00:24s
epoch 46 | loss: 0.60175 | eval_custom_logloss: 1.84424 |  0:00:25s
epoch 47 | loss: 0.61791 | eval_custom_logloss: 1.68046 |  0:00:25s
epoch 48 | loss: 0.60011 | eval_custom_logloss: 1.6704  |  0:00:26s
epoch 49 | loss: 0.67011 | eval_custom_logloss: 1.88204 |  0:00:26s
epoch 50 | loss: 0.61303 | eval_custom_logloss: 2.0206  |  0:00:27s
epoch 51 | loss: 0.61406 | eval_custom_logloss: 2.16457 |  0:00:28s
epoch 52 | loss: 0.61951 | eval_custom_logloss: 2.13617 |  0:00:28s
epoch 53 | loss: 0.59449 | eval_custom_logloss: 2.53207 |  0:00:29s
epoch 54 | loss: 0.60463 | eval_custom_logloss: 1.84015 |  0:00:29s
epoch 55 | loss: 0.63893 | eval_custom_logloss: 1.98523 |  0:00:30s
epoch 56 | loss: 0.57844 | eval_custom_logloss: 1.65217 |  0:00:30s
epoch 57 | loss: 0.57881 | eval_custom_logloss: 1.57481 |  0:00:31s
epoch 58 | loss: 0.56727 | eval_custom_logloss: 1.59409 |  0:00:31s
epoch 59 | loss: 0.58479 | eval_custom_logloss: 1.87423 |  0:00:32s
epoch 60 | loss: 0.57899 | eval_custom_logloss: 1.98998 |  0:00:32s
epoch 61 | loss: 0.60279 | eval_custom_logloss: 1.47432 |  0:00:33s
epoch 62 | loss: 0.57506 | eval_custom_logloss: 1.79514 |  0:00:33s
epoch 63 | loss: 0.58143 | eval_custom_logloss: 1.42648 |  0:00:34s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 1.37668
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.328, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.43744 | eval_custom_logloss: 6.96885 |  0:00:00s
epoch 1  | loss: 0.95322 | eval_custom_logloss: 7.58002 |  0:00:01s
epoch 2  | loss: 0.8626  | eval_custom_logloss: 6.77882 |  0:00:01s
epoch 3  | loss: 0.87949 | eval_custom_logloss: 8.19295 |  0:00:02s
epoch 4  | loss: 0.84514 | eval_custom_logloss: 5.98206 |  0:00:02s
epoch 5  | loss: 0.86037 | eval_custom_logloss: 7.62504 |  0:00:03s
epoch 6  | loss: 0.80048 | eval_custom_logloss: 6.18109 |  0:00:03s
epoch 7  | loss: 0.78943 | eval_custom_logloss: 5.22474 |  0:00:04s
epoch 8  | loss: 0.71818 | eval_custom_logloss: 6.08606 |  0:00:04s
epoch 9  | loss: 0.71114 | eval_custom_logloss: 6.62085 |  0:00:05s
epoch 10 | loss: 0.69113 | eval_custom_logloss: 6.32047 |  0:00:06s
epoch 11 | loss: 0.69471 | eval_custom_logloss: 5.34    |  0:00:06s
epoch 12 | loss: 0.64987 | eval_custom_logloss: 6.38459 |  0:00:07s
epoch 13 | loss: 0.67898 | eval_custom_logloss: 4.59674 |  0:00:07s
epoch 14 | loss: 0.66536 | eval_custom_logloss: 4.92047 |  0:00:08s
epoch 15 | loss: 0.69311 | eval_custom_logloss: 4.75054 |  0:00:08s
epoch 16 | loss: 0.6535  | eval_custom_logloss: 5.06132 |  0:00:09s
epoch 17 | loss: 0.63189 | eval_custom_logloss: 6.65311 |  0:00:09s
epoch 18 | loss: 0.6762  | eval_custom_logloss: 5.61571 |  0:00:10s
epoch 19 | loss: 0.68406 | eval_custom_logloss: 5.00119 |  0:00:10s
epoch 20 | loss: 0.64941 | eval_custom_logloss: 5.56533 |  0:00:11s
epoch 21 | loss: 0.63431 | eval_custom_logloss: 5.28549 |  0:00:11s
epoch 22 | loss: 0.62538 | eval_custom_logloss: 6.10546 |  0:00:12s
epoch 23 | loss: 0.60885 | eval_custom_logloss: 5.77643 |  0:00:12s
epoch 24 | loss: 0.60484 | eval_custom_logloss: 5.54728 |  0:00:13s
epoch 25 | loss: 0.60253 | eval_custom_logloss: 4.91039 |  0:00:14s
epoch 26 | loss: 0.67243 | eval_custom_logloss: 3.63109 |  0:00:14s
epoch 27 | loss: 0.62393 | eval_custom_logloss: 4.29882 |  0:00:15s
epoch 28 | loss: 0.56547 | eval_custom_logloss: 5.15463 |  0:00:15s
epoch 29 | loss: 0.5941  | eval_custom_logloss: 3.99141 |  0:00:16s
epoch 30 | loss: 0.59465 | eval_custom_logloss: 3.13512 |  0:00:16s
epoch 31 | loss: 0.61987 | eval_custom_logloss: 3.25722 |  0:00:17s
epoch 32 | loss: 0.58402 | eval_custom_logloss: 3.57513 |  0:00:17s
epoch 33 | loss: 0.58245 | eval_custom_logloss: 3.3006  |  0:00:18s
epoch 34 | loss: 0.56781 | eval_custom_logloss: 3.37427 |  0:00:18s
epoch 35 | loss: 0.58716 | eval_custom_logloss: 2.9932  |  0:00:19s
epoch 36 | loss: 0.54449 | eval_custom_logloss: 4.12987 |  0:00:19s
epoch 37 | loss: 0.52655 | eval_custom_logloss: 3.98172 |  0:00:20s
epoch 38 | loss: 0.53557 | eval_custom_logloss: 3.61464 |  0:00:20s
epoch 39 | loss: 0.52059 | eval_custom_logloss: 3.33494 |  0:00:21s
epoch 40 | loss: 0.59185 | eval_custom_logloss: 3.14912 |  0:00:22s
epoch 41 | loss: 0.62103 | eval_custom_logloss: 3.14383 |  0:00:22s
epoch 42 | loss: 0.59553 | eval_custom_logloss: 2.41339 |  0:00:23s
epoch 43 | loss: 0.57629 | eval_custom_logloss: 2.41542 |  0:00:23s
epoch 44 | loss: 0.55823 | eval_custom_logloss: 2.73234 |  0:00:24s
epoch 45 | loss: 0.55677 | eval_custom_logloss: 2.22206 |  0:00:24s
epoch 46 | loss: 0.57103 | eval_custom_logloss: 2.48331 |  0:00:25s
epoch 47 | loss: 0.5582  | eval_custom_logloss: 2.03716 |  0:00:25s
epoch 48 | loss: 0.5502  | eval_custom_logloss: 2.04237 |  0:00:26s
epoch 49 | loss: 0.53857 | eval_custom_logloss: 2.11543 |  0:00:27s
epoch 50 | loss: 0.52924 | eval_custom_logloss: 2.1565  |  0:00:27s
epoch 51 | loss: 0.54507 | eval_custom_logloss: 2.60888 |  0:00:28s
epoch 52 | loss: 0.54571 | eval_custom_logloss: 2.13981 |  0:00:28s
epoch 53 | loss: 0.54279 | eval_custom_logloss: 1.81962 |  0:00:29s
epoch 54 | loss: 0.54094 | eval_custom_logloss: 2.25068 |  0:00:29s
epoch 55 | loss: 0.57016 | eval_custom_logloss: 2.29685 |  0:00:30s
epoch 56 | loss: 0.52183 | eval_custom_logloss: 2.06408 |  0:00:30s
epoch 57 | loss: 0.53736 | eval_custom_logloss: 2.5954  |  0:00:31s
epoch 58 | loss: 0.53129 | eval_custom_logloss: 1.88832 |  0:00:31s
epoch 59 | loss: 0.54297 | eval_custom_logloss: 2.06973 |  0:00:32s
epoch 60 | loss: 0.5468  | eval_custom_logloss: 1.65249 |  0:00:32s
epoch 61 | loss: 0.54155 | eval_custom_logloss: 1.8683  |  0:00:33s
epoch 62 | loss: 0.5156  | eval_custom_logloss: 2.06476 |  0:00:34s
epoch 63 | loss: 0.54968 | eval_custom_logloss: 2.07542 |  0:00:34s
epoch 64 | loss: 0.50068 | eval_custom_logloss: 1.94057 |  0:00:35s
epoch 65 | loss: 0.54787 | eval_custom_logloss: 2.14218 |  0:00:35s
epoch 66 | loss: 0.53802 | eval_custom_logloss: 1.62747 |  0:00:36s
epoch 67 | loss: 0.54656 | eval_custom_logloss: 1.27128 |  0:00:36s
epoch 68 | loss: 0.55115 | eval_custom_logloss: 1.26695 |  0:00:37s
epoch 69 | loss: 0.55566 | eval_custom_logloss: 1.32375 |  0:00:37s
epoch 70 | loss: 0.53977 | eval_custom_logloss: 1.91211 |  0:00:38s
epoch 71 | loss: 0.53788 | eval_custom_logloss: 1.56731 |  0:00:38s
epoch 72 | loss: 0.53609 | eval_custom_logloss: 1.75618 |  0:00:39s
epoch 73 | loss: 0.54485 | eval_custom_logloss: 1.82583 |  0:00:39s
epoch 74 | loss: 0.52412 | eval_custom_logloss: 1.59405 |  0:00:40s
epoch 75 | loss: 0.50741 | eval_custom_logloss: 1.59569 |  0:00:40s
epoch 76 | loss: 0.50264 | eval_custom_logloss: 1.23075 |  0:00:41s
epoch 77 | loss: 0.4918  | eval_custom_logloss: 1.59256 |  0:00:42s
epoch 78 | loss: 0.50203 | eval_custom_logloss: 1.50047 |  0:00:42s
epoch 79 | loss: 0.5535  | eval_custom_logloss: 1.73324 |  0:00:43s
epoch 80 | loss: 0.55285 | eval_custom_logloss: 1.58731 |  0:00:43s
epoch 81 | loss: 0.55601 | eval_custom_logloss: 1.33823 |  0:00:44s
epoch 82 | loss: 0.5209  | eval_custom_logloss: 1.38071 |  0:00:44s
epoch 83 | loss: 0.51024 | eval_custom_logloss: 1.43697 |  0:00:45s
epoch 84 | loss: 0.49886 | eval_custom_logloss: 1.3744  |  0:00:45s
epoch 85 | loss: 0.46695 | eval_custom_logloss: 1.47145 |  0:00:46s
epoch 86 | loss: 0.46743 | eval_custom_logloss: 1.49757 |  0:00:46s
epoch 87 | loss: 0.45441 | eval_custom_logloss: 1.4444  |  0:00:47s
epoch 88 | loss: 0.45999 | eval_custom_logloss: 1.51019 |  0:00:48s
epoch 89 | loss: 0.41882 | eval_custom_logloss: 1.55918 |  0:00:48s
epoch 90 | loss: 0.44569 | eval_custom_logloss: 1.44939 |  0:00:49s
epoch 91 | loss: 0.45649 | eval_custom_logloss: 1.47595 |  0:00:49s
epoch 92 | loss: 0.49884 | eval_custom_logloss: 1.31263 |  0:00:50s
epoch 93 | loss: 0.49404 | eval_custom_logloss: 1.51757 |  0:00:50s
epoch 94 | loss: 0.47424 | eval_custom_logloss: 1.517   |  0:00:51s
epoch 95 | loss: 0.45498 | eval_custom_logloss: 1.52042 |  0:00:52s
epoch 96 | loss: 0.44581 | eval_custom_logloss: 1.55406 |  0:00:52s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 1.23075
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.26655, 'Log Loss - std': 0.061450000000000005} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.33134 | eval_custom_logloss: 7.95687 |  0:00:00s
epoch 1  | loss: 1.00394 | eval_custom_logloss: 7.61516 |  0:00:01s
epoch 2  | loss: 0.94425 | eval_custom_logloss: 9.37096 |  0:00:01s
epoch 3  | loss: 0.88963 | eval_custom_logloss: 7.81508 |  0:00:02s
epoch 4  | loss: 0.84124 | eval_custom_logloss: 9.04896 |  0:00:02s
epoch 5  | loss: 0.84627 | eval_custom_logloss: 10.02363|  0:00:03s
epoch 6  | loss: 0.84493 | eval_custom_logloss: 5.70716 |  0:00:03s
epoch 7  | loss: 0.84315 | eval_custom_logloss: 7.31364 |  0:00:04s
epoch 8  | loss: 0.82641 | eval_custom_logloss: 7.23745 |  0:00:04s
epoch 9  | loss: 0.796   | eval_custom_logloss: 7.56596 |  0:00:05s
epoch 10 | loss: 0.79199 | eval_custom_logloss: 8.84745 |  0:00:05s
epoch 11 | loss: 0.73162 | eval_custom_logloss: 7.56565 |  0:00:06s
epoch 12 | loss: 0.73768 | eval_custom_logloss: 4.95425 |  0:00:06s
epoch 13 | loss: 0.72718 | eval_custom_logloss: 4.10192 |  0:00:07s
epoch 14 | loss: 0.72969 | eval_custom_logloss: 6.6     |  0:00:08s
epoch 15 | loss: 0.69514 | eval_custom_logloss: 7.60466 |  0:00:08s
epoch 16 | loss: 0.67842 | eval_custom_logloss: 6.74832 |  0:00:09s
epoch 17 | loss: 0.67912 | eval_custom_logloss: 5.02846 |  0:00:09s
epoch 18 | loss: 0.6803  | eval_custom_logloss: 2.97548 |  0:00:10s
epoch 19 | loss: 0.65284 | eval_custom_logloss: 2.84048 |  0:00:10s
epoch 20 | loss: 0.63314 | eval_custom_logloss: 2.61218 |  0:00:11s
epoch 21 | loss: 0.64352 | eval_custom_logloss: 2.44493 |  0:00:11s
epoch 22 | loss: 0.6802  | eval_custom_logloss: 3.03593 |  0:00:12s
epoch 23 | loss: 0.6361  | eval_custom_logloss: 3.03512 |  0:00:12s
epoch 24 | loss: 0.59781 | eval_custom_logloss: 4.02107 |  0:00:13s
epoch 25 | loss: 0.62437 | eval_custom_logloss: 3.54883 |  0:00:13s
epoch 26 | loss: 0.61208 | eval_custom_logloss: 3.16498 |  0:00:14s
epoch 27 | loss: 0.60677 | eval_custom_logloss: 3.64991 |  0:00:15s
epoch 28 | loss: 0.59874 | eval_custom_logloss: 2.87463 |  0:00:15s
epoch 29 | loss: 0.58062 | eval_custom_logloss: 3.52064 |  0:00:16s
epoch 30 | loss: 0.57931 | eval_custom_logloss: 2.76571 |  0:00:16s
epoch 31 | loss: 0.61079 | eval_custom_logloss: 4.49454 |  0:00:17s
epoch 32 | loss: 0.59341 | eval_custom_logloss: 3.21993 |  0:00:17s
epoch 33 | loss: 0.58434 | eval_custom_logloss: 2.18199 |  0:00:18s
epoch 34 | loss: 0.57346 | eval_custom_logloss: 1.66692 |  0:00:18s
epoch 35 | loss: 0.55584 | eval_custom_logloss: 2.03144 |  0:00:19s
epoch 36 | loss: 0.54143 | eval_custom_logloss: 1.91547 |  0:00:19s
epoch 37 | loss: 0.56609 | eval_custom_logloss: 2.34925 |  0:00:20s
epoch 38 | loss: 0.53309 | eval_custom_logloss: 2.22761 |  0:00:20s
epoch 39 | loss: 0.552   | eval_custom_logloss: 2.68872 |  0:00:21s
epoch 40 | loss: 0.51759 | eval_custom_logloss: 2.62037 |  0:00:22s
epoch 41 | loss: 0.52187 | eval_custom_logloss: 2.40027 |  0:00:22s
epoch 42 | loss: 0.51833 | eval_custom_logloss: 2.00357 |  0:00:23s
epoch 43 | loss: 0.49422 | eval_custom_logloss: 2.175   |  0:00:23s
epoch 44 | loss: 0.50272 | eval_custom_logloss: 2.77138 |  0:00:24s
epoch 45 | loss: 0.53039 | eval_custom_logloss: 2.82105 |  0:00:24s
epoch 46 | loss: 0.56424 | eval_custom_logloss: 2.97446 |  0:00:25s
epoch 47 | loss: 0.60504 | eval_custom_logloss: 2.54687 |  0:00:25s
epoch 48 | loss: 0.58944 | eval_custom_logloss: 2.45154 |  0:00:26s
epoch 49 | loss: 0.60508 | eval_custom_logloss: 2.28549 |  0:00:26s
epoch 50 | loss: 0.60367 | eval_custom_logloss: 2.32952 |  0:00:27s
epoch 51 | loss: 0.6088  | eval_custom_logloss: 2.3793  |  0:00:28s
epoch 52 | loss: 0.63685 | eval_custom_logloss: 3.82525 |  0:00:28s
epoch 53 | loss: 0.59118 | eval_custom_logloss: 2.57877 |  0:00:29s
epoch 54 | loss: 0.597   | eval_custom_logloss: 2.68845 |  0:00:29s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 1.66692
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3617000000000001, 'Log Loss - std': 0.14361213969113706} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.3139  | eval_custom_logloss: 6.53358 |  0:00:00s
epoch 1  | loss: 1.02358 | eval_custom_logloss: 5.91221 |  0:00:01s
epoch 2  | loss: 0.91366 | eval_custom_logloss: 6.95891 |  0:00:01s
epoch 3  | loss: 0.86744 | eval_custom_logloss: 5.94504 |  0:00:02s
epoch 4  | loss: 0.80863 | eval_custom_logloss: 6.00309 |  0:00:02s
epoch 5  | loss: 0.82343 | eval_custom_logloss: 4.46242 |  0:00:03s
epoch 6  | loss: 0.82975 | eval_custom_logloss: 4.97251 |  0:00:03s
epoch 7  | loss: 0.78145 | eval_custom_logloss: 8.01856 |  0:00:04s
epoch 8  | loss: 0.77527 | eval_custom_logloss: 8.95162 |  0:00:04s
epoch 9  | loss: 0.77918 | eval_custom_logloss: 8.1135  |  0:00:05s
epoch 10 | loss: 0.7211  | eval_custom_logloss: 7.49722 |  0:00:05s
epoch 11 | loss: 0.70099 | eval_custom_logloss: 7.18225 |  0:00:06s
epoch 12 | loss: 0.66176 | eval_custom_logloss: 7.53369 |  0:00:06s
epoch 13 | loss: 0.74464 | eval_custom_logloss: 5.49024 |  0:00:07s
epoch 14 | loss: 0.73749 | eval_custom_logloss: 5.34329 |  0:00:08s
epoch 15 | loss: 0.78008 | eval_custom_logloss: 3.36657 |  0:00:08s
epoch 16 | loss: 0.73288 | eval_custom_logloss: 3.25095 |  0:00:09s
epoch 17 | loss: 0.63473 | eval_custom_logloss: 4.53185 |  0:00:09s
epoch 18 | loss: 0.64881 | eval_custom_logloss: 4.01977 |  0:00:10s
epoch 19 | loss: 0.64123 | eval_custom_logloss: 4.2255  |  0:00:10s
epoch 20 | loss: 0.58849 | eval_custom_logloss: 4.69511 |  0:00:11s
epoch 21 | loss: 0.60697 | eval_custom_logloss: 4.45105 |  0:00:11s
epoch 22 | loss: 0.59827 | eval_custom_logloss: 3.41603 |  0:00:12s
epoch 23 | loss: 0.5988  | eval_custom_logloss: 2.72315 |  0:00:12s
epoch 24 | loss: 0.5788  | eval_custom_logloss: 4.08786 |  0:00:13s
epoch 25 | loss: 0.56517 | eval_custom_logloss: 3.45771 |  0:00:13s
epoch 26 | loss: 0.57512 | eval_custom_logloss: 3.43267 |  0:00:14s
epoch 27 | loss: 0.56776 | eval_custom_logloss: 3.09931 |  0:00:14s
epoch 28 | loss: 0.56011 | eval_custom_logloss: 3.57087 |  0:00:15s
epoch 29 | loss: 0.55412 | eval_custom_logloss: 3.32701 |  0:00:16s
epoch 30 | loss: 0.56425 | eval_custom_logloss: 3.25463 |  0:00:16s
epoch 31 | loss: 0.54462 | eval_custom_logloss: 4.03241 |  0:00:17s
epoch 32 | loss: 0.55856 | eval_custom_logloss: 3.72114 |  0:00:17s
epoch 33 | loss: 0.53523 | eval_custom_logloss: 3.86178 |  0:00:18s
epoch 34 | loss: 0.55495 | eval_custom_logloss: 3.86121 |  0:00:18s
epoch 35 | loss: 0.53157 | eval_custom_logloss: 3.49348 |  0:00:19s
epoch 36 | loss: 0.54807 | eval_custom_logloss: 3.42365 |  0:00:19s
epoch 37 | loss: 0.56047 | eval_custom_logloss: 2.73758 |  0:00:20s
epoch 38 | loss: 0.54954 | eval_custom_logloss: 1.98971 |  0:00:20s
epoch 39 | loss: 0.57448 | eval_custom_logloss: 2.34646 |  0:00:21s
epoch 40 | loss: 0.53072 | eval_custom_logloss: 1.58247 |  0:00:21s
epoch 41 | loss: 0.5165  | eval_custom_logloss: 2.18819 |  0:00:22s
epoch 42 | loss: 0.48403 | eval_custom_logloss: 1.79469 |  0:00:23s
epoch 43 | loss: 0.48114 | eval_custom_logloss: 1.58813 |  0:00:23s
epoch 44 | loss: 0.50845 | eval_custom_logloss: 1.49412 |  0:00:24s
epoch 45 | loss: 0.467   | eval_custom_logloss: 1.33995 |  0:00:24s
epoch 46 | loss: 0.47576 | eval_custom_logloss: 1.35876 |  0:00:25s
epoch 47 | loss: 0.49135 | eval_custom_logloss: 1.44149 |  0:00:25s
epoch 48 | loss: 0.47963 | eval_custom_logloss: 1.25251 |  0:00:26s
epoch 49 | loss: 0.50598 | eval_custom_logloss: 1.58566 |  0:00:26s
epoch 50 | loss: 0.50437 | eval_custom_logloss: 1.72114 |  0:00:27s
epoch 51 | loss: 0.4994  | eval_custom_logloss: 1.67154 |  0:00:27s
epoch 52 | loss: 0.46161 | eval_custom_logloss: 1.5599  |  0:00:28s
epoch 53 | loss: 0.47592 | eval_custom_logloss: 1.43464 |  0:00:29s
epoch 54 | loss: 0.45287 | eval_custom_logloss: 1.55409 |  0:00:29s
epoch 55 | loss: 0.46316 | eval_custom_logloss: 1.57199 |  0:00:30s
epoch 56 | loss: 0.42217 | eval_custom_logloss: 1.42597 |  0:00:30s
epoch 57 | loss: 0.41548 | eval_custom_logloss: 1.5039  |  0:00:31s
epoch 58 | loss: 0.39007 | eval_custom_logloss: 1.82556 |  0:00:31s
epoch 59 | loss: 0.3973  | eval_custom_logloss: 1.77606 |  0:00:32s
epoch 60 | loss: 0.43873 | eval_custom_logloss: 1.40805 |  0:00:32s
epoch 61 | loss: 0.39998 | eval_custom_logloss: 1.25604 |  0:00:33s
epoch 62 | loss: 0.41941 | eval_custom_logloss: 1.3218  |  0:00:33s
epoch 63 | loss: 0.38291 | eval_custom_logloss: 1.42797 |  0:00:34s
epoch 64 | loss: 0.37977 | eval_custom_logloss: 1.36235 |  0:00:34s
epoch 65 | loss: 0.37867 | eval_custom_logloss: 1.20603 |  0:00:35s
epoch 66 | loss: 0.39485 | eval_custom_logloss: 1.18409 |  0:00:35s
epoch 67 | loss: 0.38047 | eval_custom_logloss: 1.40998 |  0:00:36s
epoch 68 | loss: 0.36962 | eval_custom_logloss: 1.18247 |  0:00:37s
epoch 69 | loss: 0.38845 | eval_custom_logloss: 1.38549 |  0:00:37s
epoch 70 | loss: 0.39653 | eval_custom_logloss: 1.11171 |  0:00:38s
epoch 71 | loss: 0.39944 | eval_custom_logloss: 1.18772 |  0:00:38s
epoch 72 | loss: 0.38776 | eval_custom_logloss: 1.27968 |  0:00:39s
epoch 73 | loss: 0.37853 | eval_custom_logloss: 1.37888 |  0:00:39s
epoch 74 | loss: 0.39029 | eval_custom_logloss: 1.14723 |  0:00:40s
epoch 75 | loss: 0.36797 | eval_custom_logloss: 1.30632 |  0:00:40s
epoch 76 | loss: 0.38316 | eval_custom_logloss: 0.86553 |  0:00:41s
epoch 77 | loss: 0.41357 | eval_custom_logloss: 1.52262 |  0:00:41s
epoch 78 | loss: 0.39272 | eval_custom_logloss: 1.40043 |  0:00:42s
epoch 79 | loss: 0.40253 | eval_custom_logloss: 1.2245  |  0:00:42s
epoch 80 | loss: 0.37495 | eval_custom_logloss: 1.32471 |  0:00:43s
epoch 81 | loss: 0.35608 | eval_custom_logloss: 1.45038 |  0:00:44s
epoch 82 | loss: 0.3601  | eval_custom_logloss: 1.43955 |  0:00:44s
epoch 83 | loss: 0.37439 | eval_custom_logloss: 1.41633 |  0:00:45s
epoch 84 | loss: 0.36179 | eval_custom_logloss: 1.2507  |  0:00:45s
epoch 85 | loss: 0.35215 | eval_custom_logloss: 1.31563 |  0:00:46s
epoch 86 | loss: 0.34285 | eval_custom_logloss: 1.29278 |  0:00:46s
epoch 87 | loss: 0.32092 | eval_custom_logloss: 1.40421 |  0:00:47s
epoch 88 | loss: 0.35111 | eval_custom_logloss: 1.33466 |  0:00:47s
epoch 89 | loss: 0.35404 | eval_custom_logloss: 1.25248 |  0:00:48s
epoch 90 | loss: 0.37636 | eval_custom_logloss: 1.11468 |  0:00:48s
epoch 91 | loss: 0.36599 | eval_custom_logloss: 1.05896 |  0:00:49s
epoch 92 | loss: 0.33399 | eval_custom_logloss: 1.25135 |  0:00:49s
epoch 93 | loss: 0.35188 | eval_custom_logloss: 0.99698 |  0:00:50s
epoch 94 | loss: 0.33504 | eval_custom_logloss: 0.9123  |  0:00:51s
epoch 95 | loss: 0.40323 | eval_custom_logloss: 1.22186 |  0:00:51s
epoch 96 | loss: 0.39865 | eval_custom_logloss: 0.88692 |  0:00:52s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 0.86553
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.2314000000000003, 'Log Loss - std': 0.25768702916522596} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.29415 | eval_custom_logloss: 7.36073 |  0:00:00s
epoch 1  | loss: 1.01594 | eval_custom_logloss: 6.11458 |  0:00:01s
epoch 2  | loss: 0.92608 | eval_custom_logloss: 7.19063 |  0:00:01s
epoch 3  | loss: 0.83819 | eval_custom_logloss: 5.49261 |  0:00:02s
epoch 4  | loss: 0.85343 | eval_custom_logloss: 5.81971 |  0:00:02s
epoch 5  | loss: 0.87366 | eval_custom_logloss: 5.62546 |  0:00:03s
epoch 6  | loss: 0.81058 | eval_custom_logloss: 6.72135 |  0:00:03s
epoch 7  | loss: 0.83323 | eval_custom_logloss: 6.60027 |  0:00:04s
epoch 8  | loss: 0.81125 | eval_custom_logloss: 5.49365 |  0:00:04s
epoch 9  | loss: 0.76407 | eval_custom_logloss: 4.65568 |  0:00:05s
epoch 10 | loss: 0.74999 | eval_custom_logloss: 6.95226 |  0:00:05s
epoch 11 | loss: 0.714   | eval_custom_logloss: 8.31486 |  0:00:06s
epoch 12 | loss: 0.72623 | eval_custom_logloss: 8.37734 |  0:00:07s
epoch 13 | loss: 0.76972 | eval_custom_logloss: 7.95875 |  0:00:07s
epoch 14 | loss: 0.72533 | eval_custom_logloss: 8.34952 |  0:00:08s
epoch 15 | loss: 0.76423 | eval_custom_logloss: 6.86893 |  0:00:08s
epoch 16 | loss: 0.70729 | eval_custom_logloss: 6.09061 |  0:00:09s
epoch 17 | loss: 0.70491 | eval_custom_logloss: 6.46723 |  0:00:09s
epoch 18 | loss: 0.69455 | eval_custom_logloss: 5.29386 |  0:00:10s
epoch 19 | loss: 0.7333  | eval_custom_logloss: 7.27732 |  0:00:10s
epoch 20 | loss: 0.72217 | eval_custom_logloss: 7.42819 |  0:00:11s
epoch 21 | loss: 0.69654 | eval_custom_logloss: 6.72727 |  0:00:11s
epoch 22 | loss: 0.69797 | eval_custom_logloss: 6.14278 |  0:00:12s
epoch 23 | loss: 0.72138 | eval_custom_logloss: 4.92076 |  0:00:12s
epoch 24 | loss: 0.68827 | eval_custom_logloss: 3.07481 |  0:00:13s
epoch 25 | loss: 0.67523 | eval_custom_logloss: 4.2989  |  0:00:13s
epoch 26 | loss: 0.63799 | eval_custom_logloss: 3.34491 |  0:00:14s
epoch 27 | loss: 0.62404 | eval_custom_logloss: 4.02758 |  0:00:14s
epoch 28 | loss: 0.62899 | eval_custom_logloss: 3.19048 |  0:00:15s
epoch 29 | loss: 0.59962 | eval_custom_logloss: 4.4297  |  0:00:16s
epoch 30 | loss: 0.57658 | eval_custom_logloss: 4.06396 |  0:00:16s
epoch 31 | loss: 0.57424 | eval_custom_logloss: 4.54201 |  0:00:17s
epoch 32 | loss: 0.56244 | eval_custom_logloss: 2.85556 |  0:00:17s
epoch 33 | loss: 0.57508 | eval_custom_logloss: 3.02196 |  0:00:18s
epoch 34 | loss: 0.58218 | eval_custom_logloss: 2.72112 |  0:00:18s
epoch 35 | loss: 0.56664 | eval_custom_logloss: 2.67069 |  0:00:19s
epoch 36 | loss: 0.57397 | eval_custom_logloss: 2.25701 |  0:00:19s
epoch 37 | loss: 0.57687 | eval_custom_logloss: 2.20745 |  0:00:20s
epoch 38 | loss: 0.54958 | eval_custom_logloss: 2.62405 |  0:00:20s
epoch 39 | loss: 0.55198 | eval_custom_logloss: 3.25834 |  0:00:21s
epoch 40 | loss: 0.53872 | eval_custom_logloss: 2.45542 |  0:00:21s
epoch 41 | loss: 0.56175 | eval_custom_logloss: 1.96193 |  0:00:22s
epoch 42 | loss: 0.54699 | eval_custom_logloss: 2.26662 |  0:00:23s
epoch 43 | loss: 0.56771 | eval_custom_logloss: 2.1498  |  0:00:23s
epoch 44 | loss: 0.58018 | eval_custom_logloss: 1.88784 |  0:00:24s
epoch 45 | loss: 0.57742 | eval_custom_logloss: 1.78566 |  0:00:24s
epoch 46 | loss: 0.55234 | eval_custom_logloss: 1.52568 |  0:00:25s
epoch 47 | loss: 0.5681  | eval_custom_logloss: 2.00789 |  0:00:25s
epoch 48 | loss: 0.53393 | eval_custom_logloss: 1.6692  |  0:00:26s
epoch 49 | loss: 0.55101 | eval_custom_logloss: 1.60177 |  0:00:26s
epoch 50 | loss: 0.57979 | eval_custom_logloss: 1.50155 |  0:00:27s
epoch 51 | loss: 0.57073 | eval_custom_logloss: 1.34876 |  0:00:28s
epoch 52 | loss: 0.5511  | eval_custom_logloss: 1.2998  |  0:00:28s
epoch 53 | loss: 0.53006 | eval_custom_logloss: 1.94003 |  0:00:29s
epoch 54 | loss: 0.53617 | eval_custom_logloss: 2.29956 |  0:00:29s
epoch 55 | loss: 0.54239 | eval_custom_logloss: 2.34372 |  0:00:30s
epoch 56 | loss: 0.52696 | eval_custom_logloss: 2.87733 |  0:00:30s
epoch 57 | loss: 0.48599 | eval_custom_logloss: 2.27026 |  0:00:31s
epoch 58 | loss: 0.51067 | eval_custom_logloss: 2.71826 |  0:00:31s
epoch 59 | loss: 0.49432 | eval_custom_logloss: 2.3029  |  0:00:32s
epoch 60 | loss: 0.53099 | eval_custom_logloss: 2.2247  |  0:00:32s
epoch 61 | loss: 0.51028 | eval_custom_logloss: 1.91534 |  0:00:33s
epoch 62 | loss: 0.48627 | eval_custom_logloss: 2.32858 |  0:00:34s
epoch 63 | loss: 0.49036 | eval_custom_logloss: 2.54039 |  0:00:34s
epoch 64 | loss: 0.52482 | eval_custom_logloss: 2.45597 |  0:00:35s
epoch 65 | loss: 0.48687 | eval_custom_logloss: 2.24382 |  0:00:35s
epoch 66 | loss: 0.45878 | eval_custom_logloss: 2.25875 |  0:00:36s
epoch 67 | loss: 0.43352 | eval_custom_logloss: 1.92518 |  0:00:36s
epoch 68 | loss: 0.46202 | eval_custom_logloss: 1.70613 |  0:00:37s
epoch 69 | loss: 0.45667 | eval_custom_logloss: 1.85262 |  0:00:37s
epoch 70 | loss: 0.43154 | eval_custom_logloss: 1.52137 |  0:00:38s
epoch 71 | loss: 0.42932 | eval_custom_logloss: 1.68181 |  0:00:38s
epoch 72 | loss: 0.41448 | eval_custom_logloss: 1.67326 |  0:00:39s

Early stopping occurred at epoch 72 with best_epoch = 52 and best_eval_custom_logloss = 1.2998
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.2325000000000004, 'Log Loss - std': 0.23049278513654176} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 1.2325000000000004 and parameters: {'n_d': 17, 'n_steps': 5, 'gamma': 1.423533646348479, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.001941101249741954, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.39665 | eval_custom_logloss: 4.69607 |  0:00:00s
epoch 1  | loss: 0.9886  | eval_custom_logloss: 2.65165 |  0:00:01s
epoch 2  | loss: 0.86013 | eval_custom_logloss: 2.36087 |  0:00:01s
epoch 3  | loss: 0.90159 | eval_custom_logloss: 2.91585 |  0:00:02s
epoch 4  | loss: 0.84646 | eval_custom_logloss: 2.30787 |  0:00:02s
epoch 5  | loss: 0.8005  | eval_custom_logloss: 1.62201 |  0:00:02s
epoch 6  | loss: 0.78879 | eval_custom_logloss: 1.21079 |  0:00:03s
epoch 7  | loss: 0.78767 | eval_custom_logloss: 1.915   |  0:00:03s
epoch 8  | loss: 0.75502 | eval_custom_logloss: 1.40374 |  0:00:04s
epoch 9  | loss: 0.72846 | eval_custom_logloss: 1.70735 |  0:00:04s
epoch 10 | loss: 0.71871 | eval_custom_logloss: 1.28213 |  0:00:05s
epoch 11 | loss: 0.77404 | eval_custom_logloss: 1.31692 |  0:00:05s
epoch 12 | loss: 0.72806 | eval_custom_logloss: 1.62976 |  0:00:06s
epoch 13 | loss: 0.67582 | eval_custom_logloss: 2.27306 |  0:00:06s
epoch 14 | loss: 0.64974 | eval_custom_logloss: 2.12828 |  0:00:07s
epoch 15 | loss: 0.66687 | eval_custom_logloss: 1.87733 |  0:00:07s
epoch 16 | loss: 0.66198 | eval_custom_logloss: 1.25944 |  0:00:07s
epoch 17 | loss: 0.63439 | eval_custom_logloss: 1.14308 |  0:00:08s
epoch 18 | loss: 0.63254 | eval_custom_logloss: 0.81265 |  0:00:08s
epoch 19 | loss: 0.6091  | eval_custom_logloss: 0.86485 |  0:00:09s
epoch 20 | loss: 0.67523 | eval_custom_logloss: 1.0484  |  0:00:09s
epoch 21 | loss: 0.65676 | eval_custom_logloss: 1.16953 |  0:00:10s
epoch 22 | loss: 0.61666 | eval_custom_logloss: 1.08072 |  0:00:10s
epoch 23 | loss: 0.63141 | eval_custom_logloss: 1.03383 |  0:00:11s
epoch 24 | loss: 0.6028  | eval_custom_logloss: 0.8648  |  0:00:11s
epoch 25 | loss: 0.61687 | eval_custom_logloss: 0.79117 |  0:00:12s
epoch 26 | loss: 0.60942 | eval_custom_logloss: 0.69966 |  0:00:12s
epoch 27 | loss: 0.57794 | eval_custom_logloss: 0.62621 |  0:00:13s
epoch 28 | loss: 0.52364 | eval_custom_logloss: 0.63761 |  0:00:13s
epoch 29 | loss: 0.56879 | eval_custom_logloss: 0.67572 |  0:00:13s
epoch 30 | loss: 0.58624 | eval_custom_logloss: 0.59796 |  0:00:14s
epoch 31 | loss: 0.61112 | eval_custom_logloss: 0.56864 |  0:00:14s
epoch 32 | loss: 0.59408 | eval_custom_logloss: 0.69359 |  0:00:15s
epoch 33 | loss: 0.60381 | eval_custom_logloss: 0.61635 |  0:00:15s
epoch 34 | loss: 0.58191 | eval_custom_logloss: 0.6781  |  0:00:16s
epoch 35 | loss: 0.58979 | eval_custom_logloss: 0.78605 |  0:00:16s
epoch 36 | loss: 0.55176 | eval_custom_logloss: 0.74478 |  0:00:17s
epoch 37 | loss: 0.54896 | eval_custom_logloss: 0.72661 |  0:00:17s
epoch 38 | loss: 0.58129 | eval_custom_logloss: 0.79239 |  0:00:18s
epoch 39 | loss: 0.53821 | eval_custom_logloss: 0.87411 |  0:00:18s
epoch 40 | loss: 0.51021 | eval_custom_logloss: 0.75325 |  0:00:18s
epoch 41 | loss: 0.52628 | eval_custom_logloss: 0.72506 |  0:00:19s
epoch 42 | loss: 0.54337 | eval_custom_logloss: 0.74735 |  0:00:19s
epoch 43 | loss: 0.51619 | eval_custom_logloss: 0.70326 |  0:00:20s
epoch 44 | loss: 0.5175  | eval_custom_logloss: 0.73764 |  0:00:20s
epoch 45 | loss: 0.48577 | eval_custom_logloss: 0.63973 |  0:00:21s
epoch 46 | loss: 0.52262 | eval_custom_logloss: 0.83847 |  0:00:21s
epoch 47 | loss: 0.55115 | eval_custom_logloss: 0.84113 |  0:00:22s
epoch 48 | loss: 0.54379 | eval_custom_logloss: 0.69878 |  0:00:22s
epoch 49 | loss: 0.54028 | eval_custom_logloss: 0.77825 |  0:00:23s
epoch 50 | loss: 0.54933 | eval_custom_logloss: 0.61349 |  0:00:24s
epoch 51 | loss: 0.50903 | eval_custom_logloss: 0.72202 |  0:00:24s

Early stopping occurred at epoch 51 with best_epoch = 31 and best_eval_custom_logloss = 0.56864
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5686, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.42142 | eval_custom_logloss: 5.15482 |  0:00:00s
epoch 1  | loss: 1.08528 | eval_custom_logloss: 3.10441 |  0:00:00s
epoch 2  | loss: 0.9423  | eval_custom_logloss: 2.69348 |  0:00:01s
epoch 3  | loss: 0.86683 | eval_custom_logloss: 2.15391 |  0:00:01s
epoch 4  | loss: 0.79053 | eval_custom_logloss: 2.59231 |  0:00:02s
epoch 5  | loss: 0.81162 | eval_custom_logloss: 1.34305 |  0:00:02s
epoch 6  | loss: 0.79489 | eval_custom_logloss: 1.18754 |  0:00:03s
epoch 7  | loss: 0.73822 | eval_custom_logloss: 2.23392 |  0:00:03s
epoch 8  | loss: 0.65899 | eval_custom_logloss: 2.25484 |  0:00:04s
epoch 9  | loss: 0.6497  | eval_custom_logloss: 2.20956 |  0:00:04s
epoch 10 | loss: 0.7499  | eval_custom_logloss: 1.38527 |  0:00:05s
epoch 11 | loss: 0.68911 | eval_custom_logloss: 1.18194 |  0:00:05s
epoch 12 | loss: 0.64133 | eval_custom_logloss: 1.56443 |  0:00:06s
epoch 13 | loss: 0.61021 | eval_custom_logloss: 1.03903 |  0:00:06s
epoch 14 | loss: 0.62053 | eval_custom_logloss: 1.2112  |  0:00:07s
epoch 15 | loss: 0.64627 | eval_custom_logloss: 0.99069 |  0:00:08s
epoch 16 | loss: 0.67342 | eval_custom_logloss: 1.34059 |  0:00:08s
epoch 17 | loss: 0.62639 | eval_custom_logloss: 1.18063 |  0:00:09s
epoch 18 | loss: 0.60789 | eval_custom_logloss: 0.98401 |  0:00:09s
epoch 19 | loss: 0.59487 | eval_custom_logloss: 1.49692 |  0:00:10s
epoch 20 | loss: 0.5982  | eval_custom_logloss: 1.17347 |  0:00:10s
epoch 21 | loss: 0.58848 | eval_custom_logloss: 1.19055 |  0:00:11s
epoch 22 | loss: 0.57886 | eval_custom_logloss: 1.12463 |  0:00:11s
epoch 23 | loss: 0.55966 | eval_custom_logloss: 1.10999 |  0:00:12s
epoch 24 | loss: 0.53497 | eval_custom_logloss: 0.97416 |  0:00:12s
epoch 25 | loss: 0.53915 | eval_custom_logloss: 0.87123 |  0:00:13s
epoch 26 | loss: 0.53963 | eval_custom_logloss: 1.06167 |  0:00:14s
epoch 27 | loss: 0.53692 | eval_custom_logloss: 0.95439 |  0:00:14s
epoch 28 | loss: 0.52741 | eval_custom_logloss: 1.24194 |  0:00:15s
epoch 29 | loss: 0.51872 | eval_custom_logloss: 1.03037 |  0:00:15s
epoch 30 | loss: 0.56437 | eval_custom_logloss: 0.92296 |  0:00:16s
epoch 31 | loss: 0.53106 | eval_custom_logloss: 0.82495 |  0:00:16s
epoch 32 | loss: 0.51495 | eval_custom_logloss: 0.88793 |  0:00:17s
epoch 33 | loss: 0.48263 | eval_custom_logloss: 0.85144 |  0:00:17s
epoch 34 | loss: 0.49913 | eval_custom_logloss: 1.25544 |  0:00:18s
epoch 35 | loss: 0.53911 | eval_custom_logloss: 1.03531 |  0:00:18s
epoch 36 | loss: 0.52512 | eval_custom_logloss: 0.89148 |  0:00:19s
epoch 37 | loss: 0.5091  | eval_custom_logloss: 0.80633 |  0:00:19s
epoch 38 | loss: 0.4832  | eval_custom_logloss: 0.77216 |  0:00:20s
epoch 39 | loss: 0.47447 | eval_custom_logloss: 0.62603 |  0:00:20s
epoch 40 | loss: 0.47172 | eval_custom_logloss: 0.65689 |  0:00:20s
epoch 41 | loss: 0.46268 | eval_custom_logloss: 0.60712 |  0:00:21s
epoch 42 | loss: 0.4218  | eval_custom_logloss: 0.71333 |  0:00:21s
epoch 43 | loss: 0.44836 | eval_custom_logloss: 0.7234  |  0:00:22s
epoch 44 | loss: 0.42725 | eval_custom_logloss: 0.63236 |  0:00:22s
epoch 45 | loss: 0.43403 | eval_custom_logloss: 0.69787 |  0:00:23s
epoch 46 | loss: 0.41056 | eval_custom_logloss: 0.68925 |  0:00:23s
epoch 47 | loss: 0.38124 | eval_custom_logloss: 0.75165 |  0:00:24s
epoch 48 | loss: 0.40285 | eval_custom_logloss: 0.70488 |  0:00:24s
epoch 49 | loss: 0.41899 | eval_custom_logloss: 0.73258 |  0:00:25s
epoch 50 | loss: 0.42216 | eval_custom_logloss: 0.83909 |  0:00:25s
epoch 51 | loss: 0.42288 | eval_custom_logloss: 0.75814 |  0:00:26s
epoch 52 | loss: 0.43661 | eval_custom_logloss: 0.62358 |  0:00:27s
epoch 53 | loss: 0.40332 | eval_custom_logloss: 0.61105 |  0:00:27s
epoch 54 | loss: 0.41522 | eval_custom_logloss: 0.7009  |  0:00:28s
epoch 55 | loss: 0.38873 | eval_custom_logloss: 0.61995 |  0:00:28s
epoch 56 | loss: 0.38332 | eval_custom_logloss: 0.70419 |  0:00:29s
epoch 57 | loss: 0.40656 | eval_custom_logloss: 0.9043  |  0:00:29s
epoch 58 | loss: 0.43483 | eval_custom_logloss: 0.79596 |  0:00:30s
epoch 59 | loss: 0.41875 | eval_custom_logloss: 0.7431  |  0:00:30s
epoch 60 | loss: 0.36856 | eval_custom_logloss: 0.73982 |  0:00:31s
epoch 61 | loss: 0.36019 | eval_custom_logloss: 0.70653 |  0:00:31s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 0.60712
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.58785, 'Log Loss - std': 0.01924999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.30003 | eval_custom_logloss: 5.94094 |  0:00:00s
epoch 1  | loss: 1.02214 | eval_custom_logloss: 4.05318 |  0:00:00s
epoch 2  | loss: 0.93247 | eval_custom_logloss: 2.7208  |  0:00:01s
epoch 3  | loss: 0.82997 | eval_custom_logloss: 2.07197 |  0:00:01s
epoch 4  | loss: 0.85058 | eval_custom_logloss: 1.75585 |  0:00:02s
epoch 5  | loss: 0.77883 | eval_custom_logloss: 2.2654  |  0:00:02s
epoch 6  | loss: 0.7761  | eval_custom_logloss: 3.89823 |  0:00:03s
epoch 7  | loss: 0.7603  | eval_custom_logloss: 1.86951 |  0:00:03s
epoch 8  | loss: 0.7423  | eval_custom_logloss: 2.16604 |  0:00:04s
epoch 9  | loss: 0.76561 | eval_custom_logloss: 1.30114 |  0:00:04s
epoch 10 | loss: 0.717   | eval_custom_logloss: 1.09907 |  0:00:05s
epoch 11 | loss: 0.72311 | eval_custom_logloss: 1.21387 |  0:00:05s
epoch 12 | loss: 0.67174 | eval_custom_logloss: 1.46878 |  0:00:06s
epoch 13 | loss: 0.67408 | eval_custom_logloss: 1.35875 |  0:00:06s
epoch 14 | loss: 0.74146 | eval_custom_logloss: 1.05344 |  0:00:07s
epoch 15 | loss: 0.70442 | eval_custom_logloss: 0.88004 |  0:00:08s
epoch 16 | loss: 0.67793 | eval_custom_logloss: 0.96564 |  0:00:08s
epoch 17 | loss: 0.64236 | eval_custom_logloss: 1.28697 |  0:00:09s
epoch 18 | loss: 0.62989 | eval_custom_logloss: 0.82482 |  0:00:09s
epoch 19 | loss: 0.64203 | eval_custom_logloss: 0.82308 |  0:00:10s
epoch 20 | loss: 0.60048 | eval_custom_logloss: 0.93177 |  0:00:10s
epoch 21 | loss: 0.61631 | eval_custom_logloss: 0.70122 |  0:00:11s
epoch 22 | loss: 0.61091 | eval_custom_logloss: 0.70217 |  0:00:11s
epoch 23 | loss: 0.60645 | eval_custom_logloss: 0.61063 |  0:00:12s
epoch 24 | loss: 0.5877  | eval_custom_logloss: 0.73581 |  0:00:12s
epoch 25 | loss: 0.57816 | eval_custom_logloss: 0.80672 |  0:00:13s
epoch 26 | loss: 0.58246 | eval_custom_logloss: 0.70899 |  0:00:13s
epoch 27 | loss: 0.57932 | eval_custom_logloss: 0.79568 |  0:00:14s
epoch 28 | loss: 0.55456 | eval_custom_logloss: 0.67467 |  0:00:14s
epoch 29 | loss: 0.55115 | eval_custom_logloss: 0.63326 |  0:00:15s
epoch 30 | loss: 0.54075 | eval_custom_logloss: 0.68634 |  0:00:16s
epoch 31 | loss: 0.56601 | eval_custom_logloss: 0.74689 |  0:00:16s
epoch 32 | loss: 0.55058 | eval_custom_logloss: 0.82647 |  0:00:17s
epoch 33 | loss: 0.56769 | eval_custom_logloss: 1.04635 |  0:00:17s
epoch 34 | loss: 0.5401  | eval_custom_logloss: 0.99766 |  0:00:18s
epoch 35 | loss: 0.52953 | eval_custom_logloss: 0.94662 |  0:00:18s
epoch 36 | loss: 0.51051 | eval_custom_logloss: 0.86408 |  0:00:19s
epoch 37 | loss: 0.50167 | eval_custom_logloss: 0.6965  |  0:00:19s
epoch 38 | loss: 0.51263 | eval_custom_logloss: 0.74455 |  0:00:20s
epoch 39 | loss: 0.53929 | eval_custom_logloss: 0.56921 |  0:00:20s
epoch 40 | loss: 0.51697 | eval_custom_logloss: 0.62165 |  0:00:21s
epoch 41 | loss: 0.49707 | eval_custom_logloss: 0.98797 |  0:00:21s
epoch 42 | loss: 0.5163  | eval_custom_logloss: 0.78716 |  0:00:22s
epoch 43 | loss: 0.48109 | eval_custom_logloss: 0.79341 |  0:00:22s
epoch 44 | loss: 0.47839 | eval_custom_logloss: 0.58636 |  0:00:23s
epoch 45 | loss: 0.45527 | eval_custom_logloss: 0.64518 |  0:00:23s
epoch 46 | loss: 0.48402 | eval_custom_logloss: 0.7433  |  0:00:24s
epoch 47 | loss: 0.44341 | eval_custom_logloss: 0.84429 |  0:00:24s
epoch 48 | loss: 0.45078 | eval_custom_logloss: 0.83456 |  0:00:25s
epoch 49 | loss: 0.44245 | eval_custom_logloss: 0.72395 |  0:00:26s
epoch 50 | loss: 0.44533 | eval_custom_logloss: 0.6759  |  0:00:26s
epoch 51 | loss: 0.45438 | eval_custom_logloss: 0.78187 |  0:00:27s
epoch 52 | loss: 0.44519 | eval_custom_logloss: 0.70039 |  0:00:27s
epoch 53 | loss: 0.43245 | eval_custom_logloss: 0.90283 |  0:00:28s
epoch 54 | loss: 0.41909 | eval_custom_logloss: 0.70657 |  0:00:28s
epoch 55 | loss: 0.40515 | eval_custom_logloss: 0.70459 |  0:00:29s
epoch 56 | loss: 0.43914 | eval_custom_logloss: 0.59763 |  0:00:29s
epoch 57 | loss: 0.44338 | eval_custom_logloss: 0.68732 |  0:00:30s
epoch 58 | loss: 0.43932 | eval_custom_logloss: 0.66651 |  0:00:30s
epoch 59 | loss: 0.40462 | eval_custom_logloss: 0.69381 |  0:00:31s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.56921
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5816333333333333, 'Log Loss - std': 0.01800931857554735} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.24583 | eval_custom_logloss: 5.50639 |  0:00:00s
epoch 1  | loss: 1.03841 | eval_custom_logloss: 3.76518 |  0:00:00s
epoch 2  | loss: 0.85332 | eval_custom_logloss: 3.20264 |  0:00:01s
epoch 3  | loss: 0.83494 | eval_custom_logloss: 2.20509 |  0:00:01s
epoch 4  | loss: 0.81556 | eval_custom_logloss: 2.62719 |  0:00:02s
epoch 5  | loss: 0.84338 | eval_custom_logloss: 4.60592 |  0:00:02s
epoch 6  | loss: 0.72995 | eval_custom_logloss: 4.34396 |  0:00:03s
epoch 7  | loss: 0.70878 | eval_custom_logloss: 4.76692 |  0:00:03s
epoch 8  | loss: 0.72088 | eval_custom_logloss: 2.95983 |  0:00:04s
epoch 9  | loss: 0.74965 | eval_custom_logloss: 1.97884 |  0:00:04s
epoch 10 | loss: 0.76357 | eval_custom_logloss: 1.88282 |  0:00:04s
epoch 11 | loss: 0.75974 | eval_custom_logloss: 1.81824 |  0:00:05s
epoch 12 | loss: 0.73348 | eval_custom_logloss: 1.24521 |  0:00:05s
epoch 13 | loss: 0.66007 | eval_custom_logloss: 1.50345 |  0:00:06s
epoch 14 | loss: 0.60767 | eval_custom_logloss: 1.45213 |  0:00:06s
epoch 15 | loss: 0.62503 | eval_custom_logloss: 1.05871 |  0:00:07s
epoch 16 | loss: 0.63294 | eval_custom_logloss: 0.91163 |  0:00:07s
epoch 17 | loss: 0.67513 | eval_custom_logloss: 1.16246 |  0:00:08s
epoch 18 | loss: 0.64024 | eval_custom_logloss: 1.45009 |  0:00:08s
epoch 19 | loss: 0.62208 | eval_custom_logloss: 1.61497 |  0:00:09s
epoch 20 | loss: 0.60857 | eval_custom_logloss: 1.23727 |  0:00:09s
epoch 21 | loss: 0.62213 | eval_custom_logloss: 1.20764 |  0:00:10s
epoch 22 | loss: 0.5844  | eval_custom_logloss: 1.11299 |  0:00:10s
epoch 23 | loss: 0.58018 | eval_custom_logloss: 1.11589 |  0:00:10s
epoch 24 | loss: 0.59122 | eval_custom_logloss: 1.2888  |  0:00:11s
epoch 25 | loss: 0.61256 | eval_custom_logloss: 0.95474 |  0:00:11s
epoch 26 | loss: 0.60339 | eval_custom_logloss: 1.01221 |  0:00:12s
epoch 27 | loss: 0.62754 | eval_custom_logloss: 0.98143 |  0:00:12s
epoch 28 | loss: 0.61715 | eval_custom_logloss: 0.90393 |  0:00:13s
epoch 29 | loss: 0.61586 | eval_custom_logloss: 1.09103 |  0:00:13s
epoch 30 | loss: 0.5972  | eval_custom_logloss: 1.19855 |  0:00:14s
epoch 31 | loss: 0.56354 | eval_custom_logloss: 1.29688 |  0:00:14s
epoch 32 | loss: 0.58171 | eval_custom_logloss: 0.92077 |  0:00:14s
epoch 33 | loss: 0.56803 | eval_custom_logloss: 0.87732 |  0:00:15s
epoch 34 | loss: 0.53738 | eval_custom_logloss: 0.82268 |  0:00:15s
epoch 35 | loss: 0.54069 | eval_custom_logloss: 0.85261 |  0:00:16s
epoch 36 | loss: 0.51896 | eval_custom_logloss: 0.81238 |  0:00:16s
epoch 37 | loss: 0.52211 | eval_custom_logloss: 0.80713 |  0:00:17s
epoch 38 | loss: 0.51662 | eval_custom_logloss: 0.85178 |  0:00:17s
epoch 39 | loss: 0.53961 | eval_custom_logloss: 0.68879 |  0:00:18s
epoch 40 | loss: 0.51399 | eval_custom_logloss: 0.73553 |  0:00:18s
epoch 41 | loss: 0.51015 | eval_custom_logloss: 0.91042 |  0:00:18s
epoch 42 | loss: 0.51144 | eval_custom_logloss: 0.70752 |  0:00:19s
epoch 43 | loss: 0.50145 | eval_custom_logloss: 0.8493  |  0:00:19s
epoch 44 | loss: 0.50734 | eval_custom_logloss: 0.77613 |  0:00:20s
epoch 45 | loss: 0.49644 | eval_custom_logloss: 0.856   |  0:00:20s
epoch 46 | loss: 0.50798 | eval_custom_logloss: 0.71105 |  0:00:21s
epoch 47 | loss: 0.49842 | eval_custom_logloss: 0.76556 |  0:00:21s
epoch 48 | loss: 0.49198 | eval_custom_logloss: 0.63159 |  0:00:22s
epoch 49 | loss: 0.45713 | eval_custom_logloss: 0.67224 |  0:00:22s
epoch 50 | loss: 0.48107 | eval_custom_logloss: 0.61698 |  0:00:22s
epoch 51 | loss: 0.52869 | eval_custom_logloss: 0.7038  |  0:00:23s
epoch 52 | loss: 0.50028 | eval_custom_logloss: 0.766   |  0:00:23s
epoch 53 | loss: 0.48205 | eval_custom_logloss: 0.69668 |  0:00:24s
epoch 54 | loss: 0.4727  | eval_custom_logloss: 0.71541 |  0:00:24s
epoch 55 | loss: 0.47694 | eval_custom_logloss: 0.72136 |  0:00:25s
epoch 56 | loss: 0.46312 | eval_custom_logloss: 0.63922 |  0:00:25s
epoch 57 | loss: 0.49525 | eval_custom_logloss: 0.64299 |  0:00:26s
epoch 58 | loss: 0.48906 | eval_custom_logloss: 0.85338 |  0:00:26s
epoch 59 | loss: 0.47506 | eval_custom_logloss: 0.64619 |  0:00:26s
epoch 60 | loss: 0.46132 | eval_custom_logloss: 0.63676 |  0:00:27s
epoch 61 | loss: 0.43301 | eval_custom_logloss: 0.60572 |  0:00:27s
epoch 62 | loss: 0.4254  | eval_custom_logloss: 0.66761 |  0:00:28s
epoch 63 | loss: 0.42686 | eval_custom_logloss: 0.80269 |  0:00:28s
epoch 64 | loss: 0.42942 | eval_custom_logloss: 0.7176  |  0:00:29s
epoch 65 | loss: 0.41939 | eval_custom_logloss: 0.70845 |  0:00:29s
epoch 66 | loss: 0.40531 | eval_custom_logloss: 0.76568 |  0:00:30s
epoch 67 | loss: 0.41533 | eval_custom_logloss: 0.62014 |  0:00:30s
epoch 68 | loss: 0.40514 | eval_custom_logloss: 0.62641 |  0:00:30s
epoch 69 | loss: 0.38593 | eval_custom_logloss: 0.63751 |  0:00:31s
epoch 70 | loss: 0.38645 | eval_custom_logloss: 0.65776 |  0:00:31s
epoch 71 | loss: 0.35818 | eval_custom_logloss: 0.6764  |  0:00:32s
epoch 72 | loss: 0.39927 | eval_custom_logloss: 0.80819 |  0:00:32s
epoch 73 | loss: 0.40484 | eval_custom_logloss: 0.69963 |  0:00:33s
epoch 74 | loss: 0.35764 | eval_custom_logloss: 0.75158 |  0:00:33s
epoch 75 | loss: 0.39482 | eval_custom_logloss: 0.80323 |  0:00:33s
epoch 76 | loss: 0.38068 | eval_custom_logloss: 0.67318 |  0:00:34s
epoch 77 | loss: 0.34865 | eval_custom_logloss: 0.71038 |  0:00:34s
epoch 78 | loss: 0.34304 | eval_custom_logloss: 0.7458  |  0:00:35s
epoch 79 | loss: 0.36625 | eval_custom_logloss: 0.5243  |  0:00:35s
epoch 80 | loss: 0.37329 | eval_custom_logloss: 0.53887 |  0:00:36s
epoch 81 | loss: 0.38671 | eval_custom_logloss: 0.60709 |  0:00:36s
epoch 82 | loss: 0.37941 | eval_custom_logloss: 0.48372 |  0:00:37s
epoch 83 | loss: 0.34205 | eval_custom_logloss: 0.49452 |  0:00:37s
epoch 84 | loss: 0.33258 | eval_custom_logloss: 0.52758 |  0:00:38s
epoch 85 | loss: 0.3319  | eval_custom_logloss: 0.67968 |  0:00:38s
epoch 86 | loss: 0.37206 | eval_custom_logloss: 0.55092 |  0:00:38s
epoch 87 | loss: 0.39132 | eval_custom_logloss: 0.7252  |  0:00:39s
epoch 88 | loss: 0.38451 | eval_custom_logloss: 0.6006  |  0:00:39s
epoch 89 | loss: 0.36805 | eval_custom_logloss: 0.56253 |  0:00:40s
epoch 90 | loss: 0.32723 | eval_custom_logloss: 0.59999 |  0:00:40s
epoch 91 | loss: 0.32017 | eval_custom_logloss: 0.64547 |  0:00:41s
epoch 92 | loss: 0.3785  | eval_custom_logloss: 0.59496 |  0:00:41s
epoch 93 | loss: 0.33228 | eval_custom_logloss: 0.63592 |  0:00:42s
epoch 94 | loss: 0.35998 | eval_custom_logloss: 0.69766 |  0:00:42s
epoch 95 | loss: 0.3475  | eval_custom_logloss: 0.60825 |  0:00:42s
epoch 96 | loss: 0.30679 | eval_custom_logloss: 0.5878  |  0:00:43s
epoch 97 | loss: 0.30801 | eval_custom_logloss: 0.57952 |  0:00:43s
epoch 98 | loss: 0.32766 | eval_custom_logloss: 0.55843 |  0:00:44s
epoch 99 | loss: 0.3316  | eval_custom_logloss: 0.53398 |  0:00:44s
Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_eval_custom_logloss = 0.48372
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.55715, 'Log Loss - std': 0.04518354235780987} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax', 'n_a': 24, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.2806  | eval_custom_logloss: 4.63539 |  0:00:00s
epoch 1  | loss: 0.97362 | eval_custom_logloss: 4.41525 |  0:00:00s
epoch 2  | loss: 0.93264 | eval_custom_logloss: 2.91683 |  0:00:01s
epoch 3  | loss: 0.85682 | eval_custom_logloss: 2.94758 |  0:00:01s
epoch 4  | loss: 0.82895 | eval_custom_logloss: 2.44003 |  0:00:02s
epoch 5  | loss: 0.8028  | eval_custom_logloss: 1.86137 |  0:00:02s
epoch 6  | loss: 0.77264 | eval_custom_logloss: 1.65435 |  0:00:03s
epoch 7  | loss: 0.73252 | eval_custom_logloss: 2.54316 |  0:00:03s
epoch 8  | loss: 0.75068 | eval_custom_logloss: 2.1051  |  0:00:04s
epoch 9  | loss: 0.70711 | eval_custom_logloss: 1.34859 |  0:00:04s
epoch 10 | loss: 0.66249 | eval_custom_logloss: 1.20517 |  0:00:05s
epoch 11 | loss: 0.66448 | eval_custom_logloss: 1.17592 |  0:00:05s
epoch 12 | loss: 0.68111 | eval_custom_logloss: 1.17789 |  0:00:06s
epoch 13 | loss: 0.66405 | eval_custom_logloss: 1.11122 |  0:00:07s
epoch 14 | loss: 0.64453 | eval_custom_logloss: 1.36219 |  0:00:07s
epoch 15 | loss: 0.63946 | eval_custom_logloss: 0.95121 |  0:00:08s
epoch 16 | loss: 0.68819 | eval_custom_logloss: 0.89482 |  0:00:08s
epoch 17 | loss: 0.64389 | eval_custom_logloss: 1.02178 |  0:00:09s
epoch 18 | loss: 0.69222 | eval_custom_logloss: 0.88969 |  0:00:09s
epoch 19 | loss: 0.63196 | eval_custom_logloss: 0.94508 |  0:00:10s
epoch 20 | loss: 0.62862 | eval_custom_logloss: 1.00292 |  0:00:10s
epoch 21 | loss: 0.62144 | eval_custom_logloss: 1.11481 |  0:00:11s
epoch 22 | loss: 0.5793  | eval_custom_logloss: 0.93315 |  0:00:11s
epoch 23 | loss: 0.57722 | eval_custom_logloss: 0.78063 |  0:00:12s
epoch 24 | loss: 0.57235 | eval_custom_logloss: 0.75783 |  0:00:12s
epoch 25 | loss: 0.56677 | eval_custom_logloss: 0.73767 |  0:00:13s
epoch 26 | loss: 0.56692 | eval_custom_logloss: 0.7182  |  0:00:13s
epoch 27 | loss: 0.58927 | eval_custom_logloss: 0.62837 |  0:00:14s
epoch 28 | loss: 0.58994 | eval_custom_logloss: 0.58795 |  0:00:15s
epoch 29 | loss: 0.60011 | eval_custom_logloss: 0.73112 |  0:00:15s
epoch 30 | loss: 0.55748 | eval_custom_logloss: 0.64464 |  0:00:16s
epoch 31 | loss: 0.54755 | eval_custom_logloss: 0.64427 |  0:00:16s
epoch 32 | loss: 0.52049 | eval_custom_logloss: 0.65697 |  0:00:17s
epoch 33 | loss: 0.51882 | eval_custom_logloss: 0.83646 |  0:00:17s
epoch 34 | loss: 0.55057 | eval_custom_logloss: 0.63244 |  0:00:18s
epoch 35 | loss: 0.50922 | eval_custom_logloss: 0.764   |  0:00:18s
epoch 36 | loss: 0.487   | eval_custom_logloss: 0.5872  |  0:00:19s
epoch 37 | loss: 0.48333 | eval_custom_logloss: 0.52031 |  0:00:19s
epoch 38 | loss: 0.48228 | eval_custom_logloss: 0.49496 |  0:00:20s
epoch 39 | loss: 0.46959 | eval_custom_logloss: 0.52777 |  0:00:20s
epoch 40 | loss: 0.46095 | eval_custom_logloss: 0.54301 |  0:00:21s
epoch 41 | loss: 0.45995 | eval_custom_logloss: 0.59072 |  0:00:21s
epoch 42 | loss: 0.49029 | eval_custom_logloss: 0.71876 |  0:00:22s
epoch 43 | loss: 0.49851 | eval_custom_logloss: 0.56848 |  0:00:23s
epoch 44 | loss: 0.47073 | eval_custom_logloss: 0.48927 |  0:00:23s
epoch 45 | loss: 0.40119 | eval_custom_logloss: 0.47791 |  0:00:24s
epoch 46 | loss: 0.41494 | eval_custom_logloss: 0.51994 |  0:00:24s
epoch 47 | loss: 0.39966 | eval_custom_logloss: 0.47464 |  0:00:25s
epoch 48 | loss: 0.41085 | eval_custom_logloss: 0.43888 |  0:00:25s
epoch 49 | loss: 0.41482 | eval_custom_logloss: 0.44758 |  0:00:26s
epoch 50 | loss: 0.39739 | eval_custom_logloss: 0.47966 |  0:00:26s
epoch 51 | loss: 0.36523 | eval_custom_logloss: 0.45506 |  0:00:27s
epoch 52 | loss: 0.33652 | eval_custom_logloss: 0.37896 |  0:00:27s
epoch 53 | loss: 0.37038 | eval_custom_logloss: 0.36409 |  0:00:28s
epoch 54 | loss: 0.43659 | eval_custom_logloss: 0.44613 |  0:00:28s
epoch 55 | loss: 0.43565 | eval_custom_logloss: 0.43302 |  0:00:29s
epoch 56 | loss: 0.39815 | eval_custom_logloss: 0.43606 |  0:00:29s
epoch 57 | loss: 0.38296 | eval_custom_logloss: 0.44214 |  0:00:30s
epoch 58 | loss: 0.40847 | eval_custom_logloss: 0.54401 |  0:00:31s
epoch 59 | loss: 0.4156  | eval_custom_logloss: 0.45534 |  0:00:31s
epoch 60 | loss: 0.38447 | eval_custom_logloss: 0.44711 |  0:00:32s
epoch 61 | loss: 0.34994 | eval_custom_logloss: 0.47834 |  0:00:32s
epoch 62 | loss: 0.36551 | eval_custom_logloss: 0.38019 |  0:00:33s
epoch 63 | loss: 0.32992 | eval_custom_logloss: 0.33726 |  0:00:33s
epoch 64 | loss: 0.31928 | eval_custom_logloss: 0.37137 |  0:00:34s
epoch 65 | loss: 0.31418 | eval_custom_logloss: 0.43138 |  0:00:34s
epoch 66 | loss: 0.29072 | eval_custom_logloss: 0.43544 |  0:00:35s
epoch 67 | loss: 0.32256 | eval_custom_logloss: 0.41905 |  0:00:35s
epoch 68 | loss: 0.2861  | eval_custom_logloss: 0.40759 |  0:00:36s
epoch 69 | loss: 0.30388 | eval_custom_logloss: 0.40585 |  0:00:36s
epoch 70 | loss: 0.27646 | eval_custom_logloss: 0.30777 |  0:00:37s
epoch 71 | loss: 0.28886 | eval_custom_logloss: 0.34478 |  0:00:37s
epoch 72 | loss: 0.32772 | eval_custom_logloss: 0.40574 |  0:00:38s
epoch 73 | loss: 0.34502 | eval_custom_logloss: 0.48681 |  0:00:38s
epoch 74 | loss: 0.44021 | eval_custom_logloss: 0.61304 |  0:00:39s
epoch 75 | loss: 0.38961 | eval_custom_logloss: 0.37211 |  0:00:40s
epoch 76 | loss: 0.37683 | eval_custom_logloss: 0.4382  |  0:00:40s
epoch 77 | loss: 0.41034 | eval_custom_logloss: 0.43287 |  0:00:41s
epoch 78 | loss: 0.45764 | eval_custom_logloss: 0.4797  |  0:00:41s
epoch 79 | loss: 0.38646 | eval_custom_logloss: 0.45252 |  0:00:42s
epoch 80 | loss: 0.34576 | eval_custom_logloss: 0.46885 |  0:00:42s
epoch 81 | loss: 0.3661  | eval_custom_logloss: 0.44915 |  0:00:43s
epoch 82 | loss: 0.3625  | eval_custom_logloss: 0.45743 |  0:00:43s
epoch 83 | loss: 0.35405 | eval_custom_logloss: 0.47423 |  0:00:44s
epoch 84 | loss: 0.33542 | eval_custom_logloss: 0.53571 |  0:00:44s
epoch 85 | loss: 0.34805 | eval_custom_logloss: 0.51154 |  0:00:45s
epoch 86 | loss: 0.36046 | eval_custom_logloss: 0.41007 |  0:00:45s
epoch 87 | loss: 0.37654 | eval_custom_logloss: 0.40968 |  0:00:46s
epoch 88 | loss: 0.32477 | eval_custom_logloss: 0.47099 |  0:00:46s
epoch 89 | loss: 0.31165 | eval_custom_logloss: 0.46142 |  0:00:47s
epoch 90 | loss: 0.32179 | eval_custom_logloss: 0.46812 |  0:00:47s

Early stopping occurred at epoch 90 with best_epoch = 70 and best_eval_custom_logloss = 0.30777
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.50728, 'Log Loss - std': 0.10761649315973829} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 0.50728 and parameters: {'n_d': 24, 'n_steps': 4, 'gamma': 1.7751281045495153, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.00972980661241431, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax', 'n_a': 44, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.07158 | eval_custom_logloss: 6.35818 |  0:00:00s
epoch 1  | loss: 1.62533 | eval_custom_logloss: 7.85059 |  0:00:01s
epoch 2  | loss: 1.59082 | eval_custom_logloss: 7.67048 |  0:00:02s
epoch 3  | loss: 2.47895 | eval_custom_logloss: 7.46695 |  0:00:03s
epoch 4  | loss: 3.0854  | eval_custom_logloss: 8.45884 |  0:00:03s
epoch 5  | loss: 2.23894 | eval_custom_logloss: 5.23427 |  0:00:04s
epoch 6  | loss: 1.57582 | eval_custom_logloss: 6.80632 |  0:00:05s
epoch 7  | loss: 1.0449  | eval_custom_logloss: 4.9272  |  0:00:06s
epoch 8  | loss: 0.90512 | eval_custom_logloss: 6.58193 |  0:00:06s
epoch 9  | loss: 1.78691 | eval_custom_logloss: 8.54882 |  0:00:07s
epoch 10 | loss: 2.26004 | eval_custom_logloss: 5.20088 |  0:00:08s
epoch 11 | loss: 0.88647 | eval_custom_logloss: 3.64478 |  0:00:09s
epoch 12 | loss: 0.72088 | eval_custom_logloss: 2.94432 |  0:00:09s
epoch 13 | loss: 0.72682 | eval_custom_logloss: 3.0914  |  0:00:10s
epoch 14 | loss: 0.70208 | eval_custom_logloss: 2.94444 |  0:00:11s
epoch 15 | loss: 0.70446 | eval_custom_logloss: 2.45643 |  0:00:12s
epoch 16 | loss: 0.67418 | eval_custom_logloss: 2.70562 |  0:00:12s
epoch 17 | loss: 0.65547 | eval_custom_logloss: 2.0165  |  0:00:13s
epoch 18 | loss: 0.6824  | eval_custom_logloss: 2.81572 |  0:00:14s
epoch 19 | loss: 0.66411 | eval_custom_logloss: 3.08328 |  0:00:15s
epoch 20 | loss: 0.64336 | eval_custom_logloss: 1.51793 |  0:00:15s
epoch 21 | loss: 0.6121  | eval_custom_logloss: 1.77028 |  0:00:16s
epoch 22 | loss: 0.6373  | eval_custom_logloss: 1.57384 |  0:00:17s
epoch 23 | loss: 0.56944 | eval_custom_logloss: 1.29783 |  0:00:18s
epoch 24 | loss: 0.5656  | eval_custom_logloss: 1.08329 |  0:00:18s
epoch 25 | loss: 0.58596 | eval_custom_logloss: 0.95015 |  0:00:19s
epoch 26 | loss: 0.57087 | eval_custom_logloss: 1.11334 |  0:00:20s
epoch 27 | loss: 0.54477 | eval_custom_logloss: 1.8404  |  0:00:21s
epoch 28 | loss: 0.58384 | eval_custom_logloss: 1.01441 |  0:00:21s
epoch 29 | loss: 0.55906 | eval_custom_logloss: 1.20042 |  0:00:22s
epoch 30 | loss: 0.55402 | eval_custom_logloss: 1.12471 |  0:00:23s
epoch 31 | loss: 0.54377 | eval_custom_logloss: 0.9854  |  0:00:24s
epoch 32 | loss: 0.54989 | eval_custom_logloss: 1.19225 |  0:00:25s
epoch 33 | loss: 0.52526 | eval_custom_logloss: 1.10687 |  0:00:25s
epoch 34 | loss: 0.52861 | eval_custom_logloss: 0.96714 |  0:00:26s
epoch 35 | loss: 0.48411 | eval_custom_logloss: 1.06534 |  0:00:27s
epoch 36 | loss: 0.46996 | eval_custom_logloss: 1.18184 |  0:00:28s
epoch 37 | loss: 0.46222 | eval_custom_logloss: 1.07125 |  0:00:28s
epoch 38 | loss: 0.46657 | eval_custom_logloss: 1.11144 |  0:00:29s
epoch 39 | loss: 0.49046 | eval_custom_logloss: 1.00584 |  0:00:30s
epoch 40 | loss: 0.46062 | eval_custom_logloss: 1.10341 |  0:00:31s
epoch 41 | loss: 0.42309 | eval_custom_logloss: 0.97918 |  0:00:31s
epoch 42 | loss: 0.42143 | eval_custom_logloss: 1.69576 |  0:00:32s
epoch 43 | loss: 0.44094 | eval_custom_logloss: 0.97689 |  0:00:33s
epoch 44 | loss: 0.42639 | eval_custom_logloss: 1.18971 |  0:00:34s
epoch 45 | loss: 0.45331 | eval_custom_logloss: 1.01412 |  0:00:34s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 0.95015
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9175, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax', 'n_a': 44, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.18008 | eval_custom_logloss: 8.02755 |  0:00:00s
epoch 1  | loss: 1.63626 | eval_custom_logloss: 6.61639 |  0:00:01s
epoch 2  | loss: 1.52263 | eval_custom_logloss: 7.54632 |  0:00:02s
epoch 3  | loss: 1.32914 | eval_custom_logloss: 6.58341 |  0:00:03s
epoch 4  | loss: 1.76995 | eval_custom_logloss: 6.24645 |  0:00:03s
epoch 5  | loss: 1.80874 | eval_custom_logloss: 7.25121 |  0:00:04s
epoch 6  | loss: 1.28905 | eval_custom_logloss: 5.20001 |  0:00:05s
epoch 7  | loss: 1.18294 | eval_custom_logloss: 5.92021 |  0:00:06s
epoch 8  | loss: 1.14682 | eval_custom_logloss: 5.27289 |  0:00:07s
epoch 9  | loss: 1.2266  | eval_custom_logloss: 4.6452  |  0:00:07s
epoch 10 | loss: 0.89504 | eval_custom_logloss: 6.00072 |  0:00:08s
epoch 11 | loss: 0.79536 | eval_custom_logloss: 2.78801 |  0:00:09s
epoch 12 | loss: 0.7452  | eval_custom_logloss: 2.35817 |  0:00:10s
epoch 13 | loss: 0.72116 | eval_custom_logloss: 2.51737 |  0:00:10s
epoch 14 | loss: 0.70545 | eval_custom_logloss: 2.92829 |  0:00:11s
epoch 15 | loss: 0.72506 | eval_custom_logloss: 2.90497 |  0:00:12s
epoch 16 | loss: 0.67838 | eval_custom_logloss: 2.58467 |  0:00:13s
epoch 17 | loss: 0.67512 | eval_custom_logloss: 2.80351 |  0:00:13s
epoch 18 | loss: 0.63005 | eval_custom_logloss: 3.46578 |  0:00:14s
epoch 19 | loss: 0.60702 | eval_custom_logloss: 3.22238 |  0:00:15s
epoch 20 | loss: 0.57404 | eval_custom_logloss: 3.31343 |  0:00:16s
epoch 21 | loss: 0.58015 | eval_custom_logloss: 2.68573 |  0:00:16s
epoch 22 | loss: 0.60644 | eval_custom_logloss: 2.33294 |  0:00:17s
epoch 23 | loss: 0.60523 | eval_custom_logloss: 2.52214 |  0:00:18s
epoch 24 | loss: 0.58578 | eval_custom_logloss: 2.45408 |  0:00:19s
epoch 25 | loss: 0.55476 | eval_custom_logloss: 2.75205 |  0:00:20s
epoch 26 | loss: 0.51026 | eval_custom_logloss: 2.38837 |  0:00:20s
epoch 27 | loss: 0.49003 | eval_custom_logloss: 2.19472 |  0:00:21s
epoch 28 | loss: 0.48935 | eval_custom_logloss: 1.26671 |  0:00:22s
epoch 29 | loss: 0.5323  | eval_custom_logloss: 1.90585 |  0:00:23s
epoch 30 | loss: 0.50073 | eval_custom_logloss: 2.02442 |  0:00:23s
epoch 31 | loss: 0.49072 | eval_custom_logloss: 1.61726 |  0:00:24s
epoch 32 | loss: 0.47401 | eval_custom_logloss: 1.42027 |  0:00:25s
epoch 33 | loss: 0.41961 | eval_custom_logloss: 1.22979 |  0:00:26s
epoch 34 | loss: 0.42942 | eval_custom_logloss: 1.14657 |  0:00:26s
epoch 35 | loss: 0.40152 | eval_custom_logloss: 1.24474 |  0:00:27s
epoch 36 | loss: 0.41167 | eval_custom_logloss: 1.1887  |  0:00:28s
epoch 37 | loss: 0.41419 | eval_custom_logloss: 0.98788 |  0:00:29s
epoch 38 | loss: 0.40567 | eval_custom_logloss: 0.89375 |  0:00:30s
epoch 39 | loss: 0.42228 | eval_custom_logloss: 0.80327 |  0:00:30s
epoch 40 | loss: 0.40167 | eval_custom_logloss: 0.79976 |  0:00:31s
epoch 41 | loss: 0.38052 | eval_custom_logloss: 0.93336 |  0:00:32s
epoch 42 | loss: 0.32042 | eval_custom_logloss: 0.92819 |  0:00:33s
epoch 43 | loss: 0.3809  | eval_custom_logloss: 0.78475 |  0:00:34s
epoch 44 | loss: 0.39453 | eval_custom_logloss: 0.80625 |  0:00:34s
epoch 45 | loss: 0.39092 | eval_custom_logloss: 0.91282 |  0:00:35s
epoch 46 | loss: 0.3939  | eval_custom_logloss: 0.95899 |  0:00:36s
epoch 47 | loss: 0.37073 | eval_custom_logloss: 0.78443 |  0:00:37s
epoch 48 | loss: 0.33564 | eval_custom_logloss: 1.18787 |  0:00:37s
epoch 49 | loss: 0.35819 | eval_custom_logloss: 1.09    |  0:00:38s
epoch 50 | loss: 0.31249 | eval_custom_logloss: 1.30011 |  0:00:39s
epoch 51 | loss: 0.33054 | eval_custom_logloss: 1.06335 |  0:00:40s
epoch 52 | loss: 0.30634 | eval_custom_logloss: 1.23083 |  0:00:40s
epoch 53 | loss: 0.30431 | eval_custom_logloss: 1.31281 |  0:00:41s
epoch 54 | loss: 0.28034 | eval_custom_logloss: 1.06593 |  0:00:42s
epoch 55 | loss: 0.28974 | eval_custom_logloss: 0.94626 |  0:00:43s
epoch 56 | loss: 0.31016 | eval_custom_logloss: 0.91486 |  0:00:44s
epoch 57 | loss: 0.29682 | eval_custom_logloss: 0.84392 |  0:00:44s
epoch 58 | loss: 0.31326 | eval_custom_logloss: 0.93293 |  0:00:45s
epoch 59 | loss: 0.29753 | eval_custom_logloss: 0.91943 |  0:00:46s
epoch 60 | loss: 0.32043 | eval_custom_logloss: 0.9213  |  0:00:47s
epoch 61 | loss: 0.29425 | eval_custom_logloss: 0.93736 |  0:00:48s
epoch 62 | loss: 0.28328 | eval_custom_logloss: 1.16322 |  0:00:48s
epoch 63 | loss: 0.23139 | eval_custom_logloss: 1.0639  |  0:00:49s
epoch 64 | loss: 0.2241  | eval_custom_logloss: 1.12962 |  0:00:50s
epoch 65 | loss: 0.25672 | eval_custom_logloss: 1.16106 |  0:00:51s
epoch 66 | loss: 0.24712 | eval_custom_logloss: 1.08926 |  0:00:52s
epoch 67 | loss: 0.24701 | eval_custom_logloss: 1.06173 |  0:00:52s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.78443
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.84895, 'Log Loss - std': 0.06855} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax', 'n_a': 44, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.90226 | eval_custom_logloss: 8.41086 |  0:00:00s
epoch 1  | loss: 1.58398 | eval_custom_logloss: 6.91695 |  0:00:01s
epoch 2  | loss: 1.51955 | eval_custom_logloss: 7.77874 |  0:00:02s
epoch 3  | loss: 1.92546 | eval_custom_logloss: 5.17129 |  0:00:03s
epoch 4  | loss: 1.16269 | eval_custom_logloss: 4.76929 |  0:00:03s
epoch 5  | loss: 1.24575 | eval_custom_logloss: 7.12336 |  0:00:04s
epoch 6  | loss: 1.37653 | eval_custom_logloss: 5.38233 |  0:00:05s
epoch 7  | loss: 0.95998 | eval_custom_logloss: 4.81453 |  0:00:06s
epoch 8  | loss: 0.88297 | eval_custom_logloss: 5.55302 |  0:00:06s
epoch 9  | loss: 0.99417 | eval_custom_logloss: 4.01367 |  0:00:07s
epoch 10 | loss: 0.80355 | eval_custom_logloss: 3.20674 |  0:00:08s
epoch 11 | loss: 0.76205 | eval_custom_logloss: 2.55605 |  0:00:09s
epoch 12 | loss: 0.71438 | eval_custom_logloss: 2.27253 |  0:00:09s
epoch 13 | loss: 0.68907 | eval_custom_logloss: 1.77108 |  0:00:10s
epoch 14 | loss: 0.64565 | eval_custom_logloss: 2.34443 |  0:00:11s
epoch 15 | loss: 0.6686  | eval_custom_logloss: 1.35275 |  0:00:12s
epoch 16 | loss: 0.63387 | eval_custom_logloss: 1.50776 |  0:00:13s
epoch 17 | loss: 0.58843 | eval_custom_logloss: 1.18597 |  0:00:13s
epoch 18 | loss: 0.53063 | eval_custom_logloss: 0.99996 |  0:00:14s
epoch 19 | loss: 0.5415  | eval_custom_logloss: 1.01904 |  0:00:15s
epoch 20 | loss: 0.55326 | eval_custom_logloss: 1.68886 |  0:00:16s
epoch 21 | loss: 0.56094 | eval_custom_logloss: 1.25792 |  0:00:16s
epoch 22 | loss: 0.53949 | eval_custom_logloss: 0.96533 |  0:00:17s
epoch 23 | loss: 0.56884 | eval_custom_logloss: 0.88398 |  0:00:18s
epoch 24 | loss: 0.51249 | eval_custom_logloss: 0.95101 |  0:00:19s
epoch 25 | loss: 0.53305 | eval_custom_logloss: 0.90081 |  0:00:19s
epoch 26 | loss: 0.51856 | eval_custom_logloss: 0.81106 |  0:00:20s
epoch 27 | loss: 0.50589 | eval_custom_logloss: 0.82309 |  0:00:21s
epoch 28 | loss: 0.47951 | eval_custom_logloss: 1.08214 |  0:00:22s
epoch 29 | loss: 0.46466 | eval_custom_logloss: 0.94437 |  0:00:22s
epoch 30 | loss: 0.48609 | eval_custom_logloss: 0.82337 |  0:00:23s
epoch 31 | loss: 0.47638 | eval_custom_logloss: 1.06372 |  0:00:24s
epoch 32 | loss: 0.49758 | eval_custom_logloss: 0.89678 |  0:00:24s
epoch 33 | loss: 0.45951 | eval_custom_logloss: 1.29683 |  0:00:25s
epoch 34 | loss: 0.4698  | eval_custom_logloss: 0.99187 |  0:00:26s
epoch 35 | loss: 0.43101 | eval_custom_logloss: 1.21389 |  0:00:27s
epoch 36 | loss: 0.44284 | eval_custom_logloss: 1.23644 |  0:00:27s
epoch 37 | loss: 0.41631 | eval_custom_logloss: 1.08338 |  0:00:28s
epoch 38 | loss: 0.42103 | eval_custom_logloss: 1.14424 |  0:00:29s
epoch 39 | loss: 0.43179 | eval_custom_logloss: 0.95552 |  0:00:30s
epoch 40 | loss: 0.42351 | eval_custom_logloss: 0.99123 |  0:00:30s
epoch 41 | loss: 0.41306 | eval_custom_logloss: 0.90006 |  0:00:31s
epoch 42 | loss: 0.38246 | eval_custom_logloss: 0.87362 |  0:00:32s
epoch 43 | loss: 0.39173 | eval_custom_logloss: 0.93923 |  0:00:33s
epoch 44 | loss: 0.32315 | eval_custom_logloss: 1.00968 |  0:00:34s
epoch 45 | loss: 0.37938 | eval_custom_logloss: 0.90319 |  0:00:34s
epoch 46 | loss: 0.37212 | eval_custom_logloss: 0.88339 |  0:00:35s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 0.81106
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8347000000000001, 'Log Loss - std': 0.05948831818096725} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax', 'n_a': 44, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.15384 | eval_custom_logloss: 7.37772 |  0:00:00s
epoch 1  | loss: 1.84127 | eval_custom_logloss: 9.09781 |  0:00:01s
epoch 2  | loss: 2.1763  | eval_custom_logloss: 7.39693 |  0:00:02s
epoch 3  | loss: 2.37261 | eval_custom_logloss: 7.00457 |  0:00:03s
epoch 4  | loss: 1.5971  | eval_custom_logloss: 5.22374 |  0:00:03s
epoch 5  | loss: 1.34755 | eval_custom_logloss: 6.44842 |  0:00:04s
epoch 6  | loss: 1.29576 | eval_custom_logloss: 8.32362 |  0:00:05s
epoch 7  | loss: 1.57897 | eval_custom_logloss: 5.8286  |  0:00:06s
epoch 8  | loss: 1.32865 | eval_custom_logloss: 5.46758 |  0:00:06s
epoch 9  | loss: 1.1215  | eval_custom_logloss: 6.48078 |  0:00:07s
epoch 10 | loss: 0.96279 | eval_custom_logloss: 4.71951 |  0:00:08s
epoch 11 | loss: 1.04442 | eval_custom_logloss: 3.11279 |  0:00:09s
epoch 12 | loss: 0.90061 | eval_custom_logloss: 5.05555 |  0:00:10s
epoch 13 | loss: 0.9862  | eval_custom_logloss: 4.76558 |  0:00:10s
epoch 14 | loss: 0.80643 | eval_custom_logloss: 2.64282 |  0:00:11s
epoch 15 | loss: 0.74193 | eval_custom_logloss: 2.53251 |  0:00:12s
epoch 16 | loss: 0.71323 | eval_custom_logloss: 2.07268 |  0:00:13s
epoch 17 | loss: 0.74235 | eval_custom_logloss: 3.63079 |  0:00:14s
epoch 18 | loss: 0.69499 | eval_custom_logloss: 2.7846  |  0:00:14s
epoch 19 | loss: 0.69273 | eval_custom_logloss: 1.87987 |  0:00:15s
epoch 20 | loss: 0.63933 | eval_custom_logloss: 2.31348 |  0:00:16s
epoch 21 | loss: 0.63361 | eval_custom_logloss: 1.62319 |  0:00:17s
epoch 22 | loss: 0.65545 | eval_custom_logloss: 1.79777 |  0:00:17s
epoch 23 | loss: 0.6194  | eval_custom_logloss: 2.03892 |  0:00:18s
epoch 24 | loss: 0.62213 | eval_custom_logloss: 2.02011 |  0:00:19s
epoch 25 | loss: 0.62589 | eval_custom_logloss: 1.65374 |  0:00:20s
epoch 26 | loss: 0.59351 | eval_custom_logloss: 1.81405 |  0:00:20s
epoch 27 | loss: 0.57557 | eval_custom_logloss: 2.47815 |  0:00:21s
epoch 28 | loss: 0.55988 | eval_custom_logloss: 2.00312 |  0:00:22s
epoch 29 | loss: 0.56186 | eval_custom_logloss: 1.5968  |  0:00:23s
epoch 30 | loss: 0.5491  | eval_custom_logloss: 1.52701 |  0:00:23s
epoch 31 | loss: 0.55422 | eval_custom_logloss: 1.46126 |  0:00:24s
epoch 32 | loss: 0.54209 | eval_custom_logloss: 1.37655 |  0:00:25s
epoch 33 | loss: 0.52413 | eval_custom_logloss: 1.03527 |  0:00:26s
epoch 34 | loss: 0.51821 | eval_custom_logloss: 1.15094 |  0:00:26s
epoch 35 | loss: 0.5584  | eval_custom_logloss: 1.3998  |  0:00:27s
epoch 36 | loss: 0.53988 | eval_custom_logloss: 1.2673  |  0:00:28s
epoch 37 | loss: 0.51328 | eval_custom_logloss: 1.10802 |  0:00:29s
epoch 38 | loss: 0.55423 | eval_custom_logloss: 1.06277 |  0:00:30s
epoch 39 | loss: 0.52401 | eval_custom_logloss: 0.92801 |  0:00:30s
epoch 40 | loss: 0.5323  | eval_custom_logloss: 0.80109 |  0:00:31s
epoch 41 | loss: 0.53829 | eval_custom_logloss: 0.8611  |  0:00:32s
epoch 42 | loss: 0.52656 | eval_custom_logloss: 0.94915 |  0:00:33s
epoch 43 | loss: 0.48764 | eval_custom_logloss: 0.91432 |  0:00:34s
epoch 44 | loss: 0.47826 | eval_custom_logloss: 0.93823 |  0:00:34s
epoch 45 | loss: 0.46539 | eval_custom_logloss: 0.93836 |  0:00:35s
epoch 46 | loss: 0.46178 | eval_custom_logloss: 0.84954 |  0:00:36s
epoch 47 | loss: 0.43765 | eval_custom_logloss: 0.84723 |  0:00:37s
epoch 48 | loss: 0.44355 | eval_custom_logloss: 0.79701 |  0:00:37s
epoch 49 | loss: 0.43275 | eval_custom_logloss: 0.92334 |  0:00:38s
epoch 50 | loss: 0.42274 | eval_custom_logloss: 0.69353 |  0:00:39s
epoch 51 | loss: 0.43108 | eval_custom_logloss: 0.71796 |  0:00:40s
epoch 52 | loss: 0.40071 | eval_custom_logloss: 1.05253 |  0:00:40s
epoch 53 | loss: 0.47659 | eval_custom_logloss: 0.83373 |  0:00:41s
epoch 54 | loss: 0.43076 | eval_custom_logloss: 0.90438 |  0:00:42s
epoch 55 | loss: 0.46741 | eval_custom_logloss: 1.07655 |  0:00:43s
epoch 56 | loss: 0.43956 | eval_custom_logloss: 1.01876 |  0:00:44s
epoch 57 | loss: 0.41831 | eval_custom_logloss: 1.06584 |  0:00:44s
epoch 58 | loss: 0.38613 | eval_custom_logloss: 1.23372 |  0:00:45s
epoch 59 | loss: 0.41005 | eval_custom_logloss: 1.41613 |  0:00:46s
epoch 60 | loss: 0.42362 | eval_custom_logloss: 0.92366 |  0:00:47s
epoch 61 | loss: 0.44418 | eval_custom_logloss: 1.01973 |  0:00:48s
epoch 62 | loss: 0.38704 | eval_custom_logloss: 1.20886 |  0:00:48s
epoch 63 | loss: 0.40405 | eval_custom_logloss: 1.0262  |  0:00:49s
epoch 64 | loss: 0.36324 | eval_custom_logloss: 1.14364 |  0:00:50s
epoch 65 | loss: 0.36619 | eval_custom_logloss: 1.0263  |  0:00:51s
epoch 66 | loss: 0.35622 | eval_custom_logloss: 1.23343 |  0:00:51s
epoch 67 | loss: 0.36638 | eval_custom_logloss: 1.20915 |  0:00:52s
epoch 68 | loss: 0.35384 | eval_custom_logloss: 1.02115 |  0:00:53s
epoch 69 | loss: 0.31081 | eval_custom_logloss: 1.85587 |  0:00:54s
epoch 70 | loss: 0.31679 | eval_custom_logloss: 1.19254 |  0:00:55s

Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_custom_logloss = 0.69353
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7948000000000001, 'Log Loss - std': 0.08619846286332487} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax', 'n_a': 44, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.82386 | eval_custom_logloss: 6.6026  |  0:00:01s
epoch 1  | loss: 1.67377 | eval_custom_logloss: 7.63001 |  0:00:02s
epoch 2  | loss: 1.74589 | eval_custom_logloss: 6.87468 |  0:00:02s
epoch 3  | loss: 1.16874 | eval_custom_logloss: 7.74126 |  0:00:03s
epoch 4  | loss: 1.19833 | eval_custom_logloss: 7.02436 |  0:00:04s
epoch 5  | loss: 1.58713 | eval_custom_logloss: 5.27764 |  0:00:05s
epoch 6  | loss: 2.35102 | eval_custom_logloss: 7.72247 |  0:00:06s
epoch 7  | loss: 1.75024 | eval_custom_logloss: 4.26173 |  0:00:06s
epoch 8  | loss: 1.43421 | eval_custom_logloss: 4.51531 |  0:00:07s
epoch 9  | loss: 0.98286 | eval_custom_logloss: 2.49991 |  0:00:08s
epoch 10 | loss: 0.83174 | eval_custom_logloss: 1.90225 |  0:00:09s
epoch 11 | loss: 0.76582 | eval_custom_logloss: 2.11435 |  0:00:10s
epoch 12 | loss: 0.77974 | eval_custom_logloss: 4.27484 |  0:00:10s
epoch 13 | loss: 0.85382 | eval_custom_logloss: 3.88321 |  0:00:11s
epoch 14 | loss: 0.81391 | eval_custom_logloss: 3.41265 |  0:00:12s
epoch 15 | loss: 0.76499 | eval_custom_logloss: 3.06952 |  0:00:13s
epoch 16 | loss: 0.72172 | eval_custom_logloss: 1.44237 |  0:00:14s
epoch 17 | loss: 0.70718 | eval_custom_logloss: 1.92706 |  0:00:14s
epoch 18 | loss: 0.74302 | eval_custom_logloss: 1.90997 |  0:00:15s
epoch 19 | loss: 0.71688 | eval_custom_logloss: 1.97536 |  0:00:16s
epoch 20 | loss: 0.70491 | eval_custom_logloss: 1.48786 |  0:00:17s
epoch 21 | loss: 0.73064 | eval_custom_logloss: 2.32337 |  0:00:18s
epoch 22 | loss: 0.64986 | eval_custom_logloss: 2.12836 |  0:00:18s
epoch 23 | loss: 0.61651 | eval_custom_logloss: 1.76979 |  0:00:19s
epoch 24 | loss: 0.6007  | eval_custom_logloss: 2.17708 |  0:00:20s
epoch 25 | loss: 0.6184  | eval_custom_logloss: 1.80859 |  0:00:21s
epoch 26 | loss: 0.62703 | eval_custom_logloss: 1.93351 |  0:00:22s
epoch 27 | loss: 0.62287 | eval_custom_logloss: 1.62384 |  0:00:22s
epoch 28 | loss: 0.58902 | eval_custom_logloss: 1.53092 |  0:00:23s
epoch 29 | loss: 0.56643 | eval_custom_logloss: 0.95722 |  0:00:24s
epoch 30 | loss: 0.5631  | eval_custom_logloss: 1.06606 |  0:00:25s
epoch 31 | loss: 0.58018 | eval_custom_logloss: 1.24274 |  0:00:26s
epoch 32 | loss: 0.55526 | eval_custom_logloss: 1.35963 |  0:00:26s
epoch 33 | loss: 0.55028 | eval_custom_logloss: 1.26541 |  0:00:27s
epoch 34 | loss: 0.55793 | eval_custom_logloss: 0.97544 |  0:00:28s
epoch 35 | loss: 0.5537  | eval_custom_logloss: 1.12708 |  0:00:29s
epoch 36 | loss: 0.56561 | eval_custom_logloss: 0.80963 |  0:00:30s
epoch 37 | loss: 0.52899 | eval_custom_logloss: 0.89581 |  0:00:30s
epoch 38 | loss: 0.54338 | eval_custom_logloss: 0.76182 |  0:00:31s
epoch 39 | loss: 0.50671 | eval_custom_logloss: 0.83448 |  0:00:32s
epoch 40 | loss: 0.46891 | eval_custom_logloss: 0.95412 |  0:00:33s
epoch 41 | loss: 0.51054 | eval_custom_logloss: 0.88194 |  0:00:33s
epoch 42 | loss: 0.47549 | eval_custom_logloss: 0.82491 |  0:00:34s
epoch 43 | loss: 0.51781 | eval_custom_logloss: 0.67097 |  0:00:35s
epoch 44 | loss: 0.47748 | eval_custom_logloss: 0.67456 |  0:00:36s
epoch 45 | loss: 0.51049 | eval_custom_logloss: 1.0699  |  0:00:36s
epoch 46 | loss: 0.50383 | eval_custom_logloss: 0.7042  |  0:00:37s
epoch 47 | loss: 0.46434 | eval_custom_logloss: 0.78896 |  0:00:38s
epoch 48 | loss: 0.43737 | eval_custom_logloss: 0.91414 |  0:00:39s
epoch 49 | loss: 0.45908 | eval_custom_logloss: 0.59711 |  0:00:40s
epoch 50 | loss: 0.4321  | eval_custom_logloss: 0.65717 |  0:00:40s
epoch 51 | loss: 0.42499 | eval_custom_logloss: 0.75786 |  0:00:41s
epoch 52 | loss: 0.40087 | eval_custom_logloss: 0.98026 |  0:00:42s
epoch 53 | loss: 0.46295 | eval_custom_logloss: 0.87175 |  0:00:43s
epoch 54 | loss: 0.45256 | eval_custom_logloss: 0.77269 |  0:00:43s
epoch 55 | loss: 0.42558 | eval_custom_logloss: 0.70658 |  0:00:44s
epoch 56 | loss: 0.44598 | eval_custom_logloss: 1.00597 |  0:00:45s
epoch 57 | loss: 0.47017 | eval_custom_logloss: 1.20131 |  0:00:46s
epoch 58 | loss: 0.44524 | eval_custom_logloss: 1.13508 |  0:00:46s
epoch 59 | loss: 0.45408 | eval_custom_logloss: 1.06665 |  0:00:47s
epoch 60 | loss: 0.42572 | eval_custom_logloss: 0.75739 |  0:00:48s
epoch 61 | loss: 0.45293 | eval_custom_logloss: 0.86808 |  0:00:49s
epoch 62 | loss: 0.44679 | eval_custom_logloss: 0.87746 |  0:00:49s
epoch 63 | loss: 0.41948 | eval_custom_logloss: 0.76047 |  0:00:50s
epoch 64 | loss: 0.40144 | eval_custom_logloss: 0.97869 |  0:00:51s
epoch 65 | loss: 0.41978 | eval_custom_logloss: 1.08969 |  0:00:52s
epoch 66 | loss: 0.3915  | eval_custom_logloss: 0.72752 |  0:00:53s
epoch 67 | loss: 0.3839  | eval_custom_logloss: 0.77122 |  0:00:53s
epoch 68 | loss: 0.40991 | eval_custom_logloss: 0.83295 |  0:00:54s
epoch 69 | loss: 0.39458 | eval_custom_logloss: 0.92991 |  0:00:55s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 0.59711
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.75526, 'Log Loss - std': 0.11044358922092311} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 0.75526 and parameters: {'n_d': 44, 'n_steps': 10, 'gamma': 1.9045378842268228, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0035916222476219703, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax', 'n_a': 55, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.3703  | eval_custom_logloss: 8.41093 |  0:00:00s
epoch 1  | loss: 0.98822 | eval_custom_logloss: 6.97102 |  0:00:00s
epoch 2  | loss: 0.82216 | eval_custom_logloss: 7.66948 |  0:00:01s
epoch 3  | loss: 0.80244 | eval_custom_logloss: 6.96981 |  0:00:01s
epoch 4  | loss: 0.80493 | eval_custom_logloss: 6.45093 |  0:00:02s
epoch 5  | loss: 0.75763 | eval_custom_logloss: 8.14068 |  0:00:02s
epoch 6  | loss: 0.70856 | eval_custom_logloss: 5.82612 |  0:00:03s
epoch 7  | loss: 0.64167 | eval_custom_logloss: 5.02284 |  0:00:03s
epoch 8  | loss: 0.60261 | eval_custom_logloss: 5.42238 |  0:00:03s
epoch 9  | loss: 0.65224 | eval_custom_logloss: 3.70677 |  0:00:04s
epoch 10 | loss: 0.59237 | eval_custom_logloss: 4.61921 |  0:00:04s
epoch 11 | loss: 0.54731 | eval_custom_logloss: 5.80544 |  0:00:05s
epoch 12 | loss: 0.55325 | eval_custom_logloss: 3.25383 |  0:00:05s
epoch 13 | loss: 0.55218 | eval_custom_logloss: 3.06772 |  0:00:06s
epoch 14 | loss: 0.56206 | eval_custom_logloss: 3.7674  |  0:00:06s
epoch 15 | loss: 0.61632 | eval_custom_logloss: 3.33134 |  0:00:07s
epoch 16 | loss: 0.61126 | eval_custom_logloss: 4.07832 |  0:00:07s
epoch 17 | loss: 0.601   | eval_custom_logloss: 3.70896 |  0:00:07s
epoch 18 | loss: 0.54895 | eval_custom_logloss: 1.51611 |  0:00:08s
epoch 19 | loss: 0.53313 | eval_custom_logloss: 3.95252 |  0:00:08s
epoch 20 | loss: 0.51781 | eval_custom_logloss: 2.1189  |  0:00:09s
epoch 21 | loss: 0.51106 | eval_custom_logloss: 2.1413  |  0:00:09s
epoch 22 | loss: 0.56285 | eval_custom_logloss: 2.02332 |  0:00:10s
epoch 23 | loss: 0.51467 | eval_custom_logloss: 2.21427 |  0:00:10s
epoch 24 | loss: 0.49916 | eval_custom_logloss: 1.45843 |  0:00:10s
epoch 25 | loss: 0.47555 | eval_custom_logloss: 1.81661 |  0:00:11s
epoch 26 | loss: 0.48576 | eval_custom_logloss: 1.69938 |  0:00:11s
epoch 27 | loss: 0.47468 | eval_custom_logloss: 1.56667 |  0:00:12s
epoch 28 | loss: 0.46006 | eval_custom_logloss: 1.53493 |  0:00:12s
epoch 29 | loss: 0.41834 | eval_custom_logloss: 1.74747 |  0:00:13s
epoch 30 | loss: 0.43036 | eval_custom_logloss: 1.47154 |  0:00:13s
epoch 31 | loss: 0.44579 | eval_custom_logloss: 1.62553 |  0:00:13s
epoch 32 | loss: 0.44131 | eval_custom_logloss: 1.55302 |  0:00:14s
epoch 33 | loss: 0.44219 | eval_custom_logloss: 1.88494 |  0:00:14s
epoch 34 | loss: 0.42906 | eval_custom_logloss: 1.76212 |  0:00:15s
epoch 35 | loss: 0.40653 | eval_custom_logloss: 1.44046 |  0:00:15s
epoch 36 | loss: 0.42192 | eval_custom_logloss: 1.40822 |  0:00:15s
epoch 37 | loss: 0.44284 | eval_custom_logloss: 1.5898  |  0:00:16s
epoch 38 | loss: 0.43576 | eval_custom_logloss: 1.60999 |  0:00:16s
epoch 39 | loss: 0.44188 | eval_custom_logloss: 1.96326 |  0:00:17s
epoch 40 | loss: 0.44122 | eval_custom_logloss: 1.40726 |  0:00:17s
epoch 41 | loss: 0.43486 | eval_custom_logloss: 1.55717 |  0:00:18s
epoch 42 | loss: 0.38594 | eval_custom_logloss: 1.79296 |  0:00:18s
epoch 43 | loss: 0.42247 | eval_custom_logloss: 1.85232 |  0:00:18s
epoch 44 | loss: 0.3955  | eval_custom_logloss: 1.52726 |  0:00:19s
epoch 45 | loss: 0.37527 | eval_custom_logloss: 1.72765 |  0:00:19s
epoch 46 | loss: 0.37167 | eval_custom_logloss: 2.07376 |  0:00:20s
epoch 47 | loss: 0.39164 | eval_custom_logloss: 2.41371 |  0:00:20s
epoch 48 | loss: 0.40274 | eval_custom_logloss: 2.30171 |  0:00:21s
epoch 49 | loss: 0.38451 | eval_custom_logloss: 1.75688 |  0:00:21s
epoch 50 | loss: 0.39351 | eval_custom_logloss: 1.26033 |  0:00:21s
epoch 51 | loss: 0.38618 | eval_custom_logloss: 1.25066 |  0:00:22s
epoch 52 | loss: 0.35148 | eval_custom_logloss: 1.60215 |  0:00:22s
epoch 53 | loss: 0.39516 | eval_custom_logloss: 1.55232 |  0:00:23s
epoch 54 | loss: 0.38554 | eval_custom_logloss: 1.27856 |  0:00:23s
epoch 55 | loss: 0.36119 | eval_custom_logloss: 1.21376 |  0:00:23s
epoch 56 | loss: 0.3447  | eval_custom_logloss: 1.8299  |  0:00:24s
epoch 57 | loss: 0.33686 | eval_custom_logloss: 2.2534  |  0:00:24s
epoch 58 | loss: 0.40084 | eval_custom_logloss: 1.20632 |  0:00:25s
epoch 59 | loss: 0.37095 | eval_custom_logloss: 1.55807 |  0:00:25s
epoch 60 | loss: 0.40325 | eval_custom_logloss: 1.09031 |  0:00:26s
epoch 61 | loss: 0.43825 | eval_custom_logloss: 1.01719 |  0:00:26s
epoch 62 | loss: 0.39486 | eval_custom_logloss: 1.28185 |  0:00:26s
epoch 63 | loss: 0.40227 | eval_custom_logloss: 1.15371 |  0:00:27s
epoch 64 | loss: 0.39398 | eval_custom_logloss: 1.16092 |  0:00:27s
epoch 65 | loss: 0.36207 | eval_custom_logloss: 1.30306 |  0:00:28s
epoch 66 | loss: 0.37669 | eval_custom_logloss: 1.30059 |  0:00:28s
epoch 67 | loss: 0.3391  | eval_custom_logloss: 1.33538 |  0:00:29s
epoch 68 | loss: 0.31691 | eval_custom_logloss: 1.22946 |  0:00:29s
epoch 69 | loss: 0.31693 | eval_custom_logloss: 1.63369 |  0:00:29s
epoch 70 | loss: 0.30846 | eval_custom_logloss: 1.48836 |  0:00:30s
epoch 71 | loss: 0.30332 | eval_custom_logloss: 1.23659 |  0:00:30s
epoch 72 | loss: 0.32395 | eval_custom_logloss: 1.31881 |  0:00:31s
epoch 73 | loss: 0.3011  | eval_custom_logloss: 1.09254 |  0:00:31s
epoch 74 | loss: 0.35223 | eval_custom_logloss: 1.17542 |  0:00:31s
epoch 75 | loss: 0.34008 | eval_custom_logloss: 1.10025 |  0:00:32s
epoch 76 | loss: 0.28255 | eval_custom_logloss: 1.27259 |  0:00:32s
epoch 77 | loss: 0.32634 | eval_custom_logloss: 1.21277 |  0:00:33s
epoch 78 | loss: 0.29395 | eval_custom_logloss: 1.09343 |  0:00:33s
epoch 79 | loss: 0.28481 | eval_custom_logloss: 1.1249  |  0:00:34s
epoch 80 | loss: 0.25299 | eval_custom_logloss: 1.16494 |  0:00:34s
epoch 81 | loss: 0.25819 | eval_custom_logloss: 1.19734 |  0:00:34s

Early stopping occurred at epoch 81 with best_epoch = 61 and best_eval_custom_logloss = 1.01719
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9889, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax', 'n_a': 55, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.27996 | eval_custom_logloss: 7.4522  |  0:00:00s
epoch 1  | loss: 1.02143 | eval_custom_logloss: 7.56048 |  0:00:01s
epoch 2  | loss: 0.87412 | eval_custom_logloss: 6.32806 |  0:00:01s
epoch 3  | loss: 0.78827 | eval_custom_logloss: 8.30958 |  0:00:01s
epoch 4  | loss: 0.82385 | eval_custom_logloss: 8.03069 |  0:00:02s
epoch 5  | loss: 0.74411 | eval_custom_logloss: 6.03412 |  0:00:02s
epoch 6  | loss: 0.74871 | eval_custom_logloss: 6.39592 |  0:00:03s
epoch 7  | loss: 0.66019 | eval_custom_logloss: 5.14258 |  0:00:03s
epoch 8  | loss: 0.63962 | eval_custom_logloss: 6.94929 |  0:00:04s
epoch 9  | loss: 0.62266 | eval_custom_logloss: 5.96578 |  0:00:04s
epoch 10 | loss: 0.64828 | eval_custom_logloss: 5.74267 |  0:00:05s
epoch 11 | loss: 0.60474 | eval_custom_logloss: 5.98403 |  0:00:05s
epoch 12 | loss: 0.61378 | eval_custom_logloss: 5.51189 |  0:00:06s
epoch 13 | loss: 0.69791 | eval_custom_logloss: 6.11318 |  0:00:06s
epoch 14 | loss: 0.63608 | eval_custom_logloss: 5.90567 |  0:00:06s
epoch 15 | loss: 0.60805 | eval_custom_logloss: 4.93108 |  0:00:07s
epoch 16 | loss: 0.59201 | eval_custom_logloss: 4.90871 |  0:00:07s
epoch 17 | loss: 0.58195 | eval_custom_logloss: 4.91798 |  0:00:08s
epoch 18 | loss: 0.54095 | eval_custom_logloss: 4.25346 |  0:00:08s
epoch 19 | loss: 0.55268 | eval_custom_logloss: 5.39141 |  0:00:09s
epoch 20 | loss: 0.49945 | eval_custom_logloss: 5.07575 |  0:00:09s
epoch 21 | loss: 0.50813 | eval_custom_logloss: 4.99344 |  0:00:09s
epoch 22 | loss: 0.52743 | eval_custom_logloss: 3.95195 |  0:00:10s
epoch 23 | loss: 0.48423 | eval_custom_logloss: 4.08432 |  0:00:10s
epoch 24 | loss: 0.48306 | eval_custom_logloss: 3.39063 |  0:00:11s
epoch 25 | loss: 0.47168 | eval_custom_logloss: 3.62245 |  0:00:11s
epoch 26 | loss: 0.47684 | eval_custom_logloss: 2.93566 |  0:00:12s
epoch 27 | loss: 0.52356 | eval_custom_logloss: 2.84408 |  0:00:12s
epoch 28 | loss: 0.53499 | eval_custom_logloss: 2.80095 |  0:00:13s
epoch 29 | loss: 0.50804 | eval_custom_logloss: 3.15167 |  0:00:13s
epoch 30 | loss: 0.49374 | eval_custom_logloss: 3.19911 |  0:00:14s
epoch 31 | loss: 0.55301 | eval_custom_logloss: 3.61353 |  0:00:14s
epoch 32 | loss: 0.56672 | eval_custom_logloss: 1.86158 |  0:00:15s
epoch 33 | loss: 0.51794 | eval_custom_logloss: 2.43355 |  0:00:15s
epoch 34 | loss: 0.50622 | eval_custom_logloss: 3.78192 |  0:00:16s
epoch 35 | loss: 0.52699 | eval_custom_logloss: 2.59685 |  0:00:16s
epoch 36 | loss: 0.50167 | eval_custom_logloss: 4.36374 |  0:00:17s
epoch 37 | loss: 0.47627 | eval_custom_logloss: 4.06877 |  0:00:17s
epoch 38 | loss: 0.47719 | eval_custom_logloss: 2.68847 |  0:00:17s
epoch 39 | loss: 0.46034 | eval_custom_logloss: 2.93773 |  0:00:18s
epoch 40 | loss: 0.46401 | eval_custom_logloss: 2.58192 |  0:00:18s
epoch 41 | loss: 0.47983 | eval_custom_logloss: 2.11304 |  0:00:19s
epoch 42 | loss: 0.44546 | eval_custom_logloss: 2.45431 |  0:00:19s
epoch 43 | loss: 0.4427  | eval_custom_logloss: 3.41817 |  0:00:20s
epoch 44 | loss: 0.52766 | eval_custom_logloss: 2.92807 |  0:00:20s
epoch 45 | loss: 0.58935 | eval_custom_logloss: 2.7942  |  0:00:21s
epoch 46 | loss: 0.58468 | eval_custom_logloss: 2.3075  |  0:00:21s
epoch 47 | loss: 0.56216 | eval_custom_logloss: 3.64948 |  0:00:22s
epoch 48 | loss: 0.51822 | eval_custom_logloss: 2.88167 |  0:00:22s
epoch 49 | loss: 0.51865 | eval_custom_logloss: 2.68764 |  0:00:23s
epoch 50 | loss: 0.46967 | eval_custom_logloss: 2.41774 |  0:00:23s
epoch 51 | loss: 0.43307 | eval_custom_logloss: 3.41158 |  0:00:24s
epoch 52 | loss: 0.42373 | eval_custom_logloss: 4.54111 |  0:00:24s

Early stopping occurred at epoch 52 with best_epoch = 32 and best_eval_custom_logloss = 1.86158
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3574, 'Log Loss - std': 0.3685} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax', 'n_a': 55, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.39649 | eval_custom_logloss: 7.66614 |  0:00:00s
epoch 1  | loss: 1.17272 | eval_custom_logloss: 7.38663 |  0:00:00s
epoch 2  | loss: 0.92966 | eval_custom_logloss: 8.36963 |  0:00:01s
epoch 3  | loss: 0.89568 | eval_custom_logloss: 7.97673 |  0:00:01s
epoch 4  | loss: 0.81289 | eval_custom_logloss: 8.63719 |  0:00:02s
epoch 5  | loss: 0.75944 | eval_custom_logloss: 8.14918 |  0:00:02s
epoch 6  | loss: 0.72753 | eval_custom_logloss: 9.6232  |  0:00:02s
epoch 7  | loss: 0.71461 | eval_custom_logloss: 9.45881 |  0:00:03s
epoch 8  | loss: 0.71632 | eval_custom_logloss: 8.05646 |  0:00:03s
epoch 9  | loss: 0.67311 | eval_custom_logloss: 6.75824 |  0:00:04s
epoch 10 | loss: 0.61109 | eval_custom_logloss: 5.50363 |  0:00:04s
epoch 11 | loss: 0.60601 | eval_custom_logloss: 6.46975 |  0:00:05s
epoch 12 | loss: 0.62076 | eval_custom_logloss: 4.72258 |  0:00:05s
epoch 13 | loss: 0.63114 | eval_custom_logloss: 4.69944 |  0:00:05s
epoch 14 | loss: 0.6067  | eval_custom_logloss: 4.10273 |  0:00:06s
epoch 15 | loss: 0.57516 | eval_custom_logloss: 3.77736 |  0:00:06s
epoch 16 | loss: 0.56791 | eval_custom_logloss: 4.11802 |  0:00:07s
epoch 17 | loss: 0.55787 | eval_custom_logloss: 5.41258 |  0:00:07s
epoch 18 | loss: 0.57845 | eval_custom_logloss: 4.55054 |  0:00:08s
epoch 19 | loss: 0.61966 | eval_custom_logloss: 3.70035 |  0:00:08s
epoch 20 | loss: 0.58241 | eval_custom_logloss: 4.19019 |  0:00:08s
epoch 21 | loss: 0.56636 | eval_custom_logloss: 3.73629 |  0:00:09s
epoch 22 | loss: 0.52383 | eval_custom_logloss: 3.96201 |  0:00:09s
epoch 23 | loss: 0.53246 | eval_custom_logloss: 2.96481 |  0:00:10s
epoch 24 | loss: 0.55017 | eval_custom_logloss: 3.61581 |  0:00:10s
epoch 25 | loss: 0.50448 | eval_custom_logloss: 2.87517 |  0:00:10s
epoch 26 | loss: 0.50658 | eval_custom_logloss: 3.33729 |  0:00:11s
epoch 27 | loss: 0.49519 | eval_custom_logloss: 3.04841 |  0:00:11s
epoch 28 | loss: 0.47265 | eval_custom_logloss: 3.17524 |  0:00:12s
epoch 29 | loss: 0.46214 | eval_custom_logloss: 2.83187 |  0:00:12s
epoch 30 | loss: 0.44128 | eval_custom_logloss: 2.44906 |  0:00:13s
epoch 31 | loss: 0.45584 | eval_custom_logloss: 2.82429 |  0:00:13s
epoch 32 | loss: 0.43171 | eval_custom_logloss: 3.20001 |  0:00:13s
epoch 33 | loss: 0.46013 | eval_custom_logloss: 2.54941 |  0:00:14s
epoch 34 | loss: 0.46458 | eval_custom_logloss: 2.22326 |  0:00:14s
epoch 35 | loss: 0.45412 | eval_custom_logloss: 1.86553 |  0:00:15s
epoch 36 | loss: 0.41414 | eval_custom_logloss: 2.05367 |  0:00:15s
epoch 37 | loss: 0.42064 | eval_custom_logloss: 2.12238 |  0:00:15s
epoch 38 | loss: 0.46165 | eval_custom_logloss: 1.84535 |  0:00:16s
epoch 39 | loss: 0.44698 | eval_custom_logloss: 2.01344 |  0:00:16s
epoch 40 | loss: 0.40947 | eval_custom_logloss: 1.8041  |  0:00:17s
epoch 41 | loss: 0.40543 | eval_custom_logloss: 2.33913 |  0:00:17s
epoch 42 | loss: 0.40437 | eval_custom_logloss: 2.56939 |  0:00:18s
epoch 43 | loss: 0.38795 | eval_custom_logloss: 2.68499 |  0:00:18s
epoch 44 | loss: 0.40827 | eval_custom_logloss: 2.71091 |  0:00:18s
epoch 45 | loss: 0.41429 | eval_custom_logloss: 2.37026 |  0:00:19s
epoch 46 | loss: 0.44593 | eval_custom_logloss: 2.13428 |  0:00:19s
epoch 47 | loss: 0.43818 | eval_custom_logloss: 1.90205 |  0:00:20s
epoch 48 | loss: 0.42433 | eval_custom_logloss: 1.6786  |  0:00:20s
epoch 49 | loss: 0.40611 | eval_custom_logloss: 1.91049 |  0:00:21s
epoch 50 | loss: 0.38571 | eval_custom_logloss: 2.17496 |  0:00:21s
epoch 51 | loss: 0.34187 | eval_custom_logloss: 2.52546 |  0:00:21s
epoch 52 | loss: 0.37801 | eval_custom_logloss: 2.1919  |  0:00:22s
epoch 53 | loss: 0.37614 | eval_custom_logloss: 1.78784 |  0:00:22s
epoch 54 | loss: 0.36649 | eval_custom_logloss: 1.764   |  0:00:23s
epoch 55 | loss: 0.32647 | eval_custom_logloss: 1.39196 |  0:00:23s
epoch 56 | loss: 0.3561  | eval_custom_logloss: 1.91677 |  0:00:23s
epoch 57 | loss: 0.3529  | eval_custom_logloss: 1.92308 |  0:00:24s
epoch 58 | loss: 0.35521 | eval_custom_logloss: 1.84121 |  0:00:24s
epoch 59 | loss: 0.37777 | eval_custom_logloss: 1.7697  |  0:00:25s
epoch 60 | loss: 0.35875 | eval_custom_logloss: 2.0062  |  0:00:25s
epoch 61 | loss: 0.341   | eval_custom_logloss: 1.9791  |  0:00:25s
epoch 62 | loss: 0.4016  | eval_custom_logloss: 1.91033 |  0:00:26s
epoch 63 | loss: 0.40011 | eval_custom_logloss: 1.89658 |  0:00:26s
epoch 64 | loss: 0.37304 | eval_custom_logloss: 1.84713 |  0:00:27s
epoch 65 | loss: 0.35435 | eval_custom_logloss: 2.17887 |  0:00:27s
epoch 66 | loss: 0.31815 | eval_custom_logloss: 1.97322 |  0:00:28s
epoch 67 | loss: 0.35918 | eval_custom_logloss: 2.1631  |  0:00:28s
epoch 68 | loss: 0.30908 | eval_custom_logloss: 2.10294 |  0:00:28s
epoch 69 | loss: 0.32514 | eval_custom_logloss: 2.35061 |  0:00:29s
epoch 70 | loss: 0.32119 | eval_custom_logloss: 1.81558 |  0:00:29s
epoch 71 | loss: 0.29699 | eval_custom_logloss: 1.84689 |  0:00:30s
epoch 72 | loss: 0.29422 | eval_custom_logloss: 2.2231  |  0:00:30s
epoch 73 | loss: 0.29622 | eval_custom_logloss: 1.5346  |  0:00:30s
epoch 74 | loss: 0.3154  | eval_custom_logloss: 1.54634 |  0:00:31s
epoch 75 | loss: 0.28312 | eval_custom_logloss: 1.56126 |  0:00:31s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 1.39196
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3060333333333334, 'Log Loss - std': 0.3095242083513914} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax', 'n_a': 55, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.28576 | eval_custom_logloss: 8.74508 |  0:00:00s
epoch 1  | loss: 1.01787 | eval_custom_logloss: 8.59835 |  0:00:00s
epoch 2  | loss: 0.887   | eval_custom_logloss: 7.42775 |  0:00:01s
epoch 3  | loss: 0.84551 | eval_custom_logloss: 6.1629  |  0:00:01s
epoch 4  | loss: 0.79721 | eval_custom_logloss: 7.39869 |  0:00:02s
epoch 5  | loss: 0.78864 | eval_custom_logloss: 8.34278 |  0:00:02s
epoch 6  | loss: 0.712   | eval_custom_logloss: 8.47642 |  0:00:02s
epoch 7  | loss: 0.71132 | eval_custom_logloss: 7.3461  |  0:00:03s
epoch 8  | loss: 0.72125 | eval_custom_logloss: 5.30884 |  0:00:03s
epoch 9  | loss: 0.65885 | eval_custom_logloss: 6.31273 |  0:00:04s
epoch 10 | loss: 0.70197 | eval_custom_logloss: 6.557   |  0:00:04s
epoch 11 | loss: 0.60988 | eval_custom_logloss: 6.2106  |  0:00:05s
epoch 12 | loss: 0.64395 | eval_custom_logloss: 5.20084 |  0:00:05s
epoch 13 | loss: 0.6424  | eval_custom_logloss: 6.036   |  0:00:06s
epoch 14 | loss: 0.67016 | eval_custom_logloss: 6.52665 |  0:00:06s
epoch 15 | loss: 0.63321 | eval_custom_logloss: 6.78369 |  0:00:07s
epoch 16 | loss: 0.5819  | eval_custom_logloss: 6.51521 |  0:00:07s
epoch 17 | loss: 0.56431 | eval_custom_logloss: 8.33623 |  0:00:08s
epoch 18 | loss: 0.62053 | eval_custom_logloss: 6.29911 |  0:00:08s
epoch 19 | loss: 0.61779 | eval_custom_logloss: 5.76995 |  0:00:09s
epoch 20 | loss: 0.59227 | eval_custom_logloss: 4.66954 |  0:00:09s
epoch 21 | loss: 0.57759 | eval_custom_logloss: 4.32388 |  0:00:10s
epoch 22 | loss: 0.58662 | eval_custom_logloss: 3.57352 |  0:00:10s
epoch 23 | loss: 0.56449 | eval_custom_logloss: 2.74531 |  0:00:11s
epoch 24 | loss: 0.58893 | eval_custom_logloss: 2.49666 |  0:00:11s
epoch 25 | loss: 0.58634 | eval_custom_logloss: 2.27043 |  0:00:12s
epoch 26 | loss: 0.57819 | eval_custom_logloss: 2.06531 |  0:00:12s
epoch 27 | loss: 0.56447 | eval_custom_logloss: 2.0041  |  0:00:13s
epoch 28 | loss: 0.56596 | eval_custom_logloss: 4.47417 |  0:00:13s
epoch 29 | loss: 0.57457 | eval_custom_logloss: 3.58496 |  0:00:14s
epoch 30 | loss: 0.55543 | eval_custom_logloss: 2.83059 |  0:00:14s
epoch 31 | loss: 0.52414 | eval_custom_logloss: 3.3552  |  0:00:15s
epoch 32 | loss: 0.53985 | eval_custom_logloss: 3.11788 |  0:00:15s
epoch 33 | loss: 0.55553 | eval_custom_logloss: 2.77108 |  0:00:16s
epoch 34 | loss: 0.52051 | eval_custom_logloss: 2.1258  |  0:00:16s
epoch 35 | loss: 0.51714 | eval_custom_logloss: 2.42652 |  0:00:17s
epoch 36 | loss: 0.52272 | eval_custom_logloss: 1.76198 |  0:00:17s
epoch 37 | loss: 0.52308 | eval_custom_logloss: 2.4513  |  0:00:18s
epoch 38 | loss: 0.50586 | eval_custom_logloss: 1.76943 |  0:00:18s
epoch 39 | loss: 0.49248 | eval_custom_logloss: 1.84621 |  0:00:19s
epoch 40 | loss: 0.51224 | eval_custom_logloss: 2.0339  |  0:00:19s
epoch 41 | loss: 0.45346 | eval_custom_logloss: 1.465   |  0:00:20s
epoch 42 | loss: 0.46328 | eval_custom_logloss: 1.62256 |  0:00:20s
epoch 43 | loss: 0.46448 | eval_custom_logloss: 2.12752 |  0:00:21s
epoch 44 | loss: 0.48495 | eval_custom_logloss: 2.44982 |  0:00:21s
epoch 45 | loss: 0.42238 | eval_custom_logloss: 2.8294  |  0:00:22s
epoch 46 | loss: 0.41337 | eval_custom_logloss: 2.14404 |  0:00:22s
epoch 47 | loss: 0.41133 | eval_custom_logloss: 2.32632 |  0:00:23s
epoch 48 | loss: 0.44875 | eval_custom_logloss: 1.89349 |  0:00:23s
epoch 49 | loss: 0.52436 | eval_custom_logloss: 1.75696 |  0:00:24s
epoch 50 | loss: 0.4652  | eval_custom_logloss: 1.9364  |  0:00:24s
epoch 51 | loss: 0.44589 | eval_custom_logloss: 2.31042 |  0:00:24s
epoch 52 | loss: 0.4297  | eval_custom_logloss: 2.11035 |  0:00:25s
epoch 53 | loss: 0.41211 | eval_custom_logloss: 2.64386 |  0:00:25s
epoch 54 | loss: 0.3907  | eval_custom_logloss: 2.76365 |  0:00:26s
epoch 55 | loss: 0.37794 | eval_custom_logloss: 2.52731 |  0:00:26s
epoch 56 | loss: 0.4058  | eval_custom_logloss: 2.20427 |  0:00:27s
epoch 57 | loss: 0.38291 | eval_custom_logloss: 2.26464 |  0:00:27s
epoch 58 | loss: 0.4251  | eval_custom_logloss: 2.45087 |  0:00:28s
epoch 59 | loss: 0.43064 | eval_custom_logloss: 3.72614 |  0:00:28s
epoch 60 | loss: 0.41278 | eval_custom_logloss: 2.60727 |  0:00:29s
epoch 61 | loss: 0.40008 | eval_custom_logloss: 2.6595  |  0:00:29s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 1.465
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.32455, 'Log Loss - std': 0.26996764157950487} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax', 'n_a': 55, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.36432 | eval_custom_logloss: 9.09905 |  0:00:00s
epoch 1  | loss: 1.18418 | eval_custom_logloss: 8.30994 |  0:00:00s
epoch 2  | loss: 0.94128 | eval_custom_logloss: 8.06719 |  0:00:01s
epoch 3  | loss: 0.88011 | eval_custom_logloss: 8.45587 |  0:00:01s
epoch 4  | loss: 0.79751 | eval_custom_logloss: 6.44994 |  0:00:02s
epoch 5  | loss: 0.77555 | eval_custom_logloss: 6.95956 |  0:00:02s
epoch 6  | loss: 0.74305 | eval_custom_logloss: 7.57379 |  0:00:03s
epoch 7  | loss: 0.76159 | eval_custom_logloss: 6.76785 |  0:00:03s
epoch 8  | loss: 0.76255 | eval_custom_logloss: 6.14493 |  0:00:03s
epoch 9  | loss: 0.74881 | eval_custom_logloss: 6.94801 |  0:00:04s
epoch 10 | loss: 0.68705 | eval_custom_logloss: 6.7485  |  0:00:04s
epoch 11 | loss: 0.66936 | eval_custom_logloss: 6.98277 |  0:00:05s
epoch 12 | loss: 0.64304 | eval_custom_logloss: 5.3331  |  0:00:05s
epoch 13 | loss: 0.61221 | eval_custom_logloss: 3.76454 |  0:00:06s
epoch 14 | loss: 0.61327 | eval_custom_logloss: 4.50125 |  0:00:06s
epoch 15 | loss: 0.59022 | eval_custom_logloss: 4.37613 |  0:00:06s
epoch 16 | loss: 0.59597 | eval_custom_logloss: 4.2127  |  0:00:07s
epoch 17 | loss: 0.58049 | eval_custom_logloss: 2.81211 |  0:00:07s
epoch 18 | loss: 0.58516 | eval_custom_logloss: 3.68606 |  0:00:08s
epoch 19 | loss: 0.56555 | eval_custom_logloss: 3.66613 |  0:00:08s
epoch 20 | loss: 0.56876 | eval_custom_logloss: 2.50249 |  0:00:09s
epoch 21 | loss: 0.55548 | eval_custom_logloss: 2.50606 |  0:00:09s
epoch 22 | loss: 0.53919 | eval_custom_logloss: 3.89582 |  0:00:09s
epoch 23 | loss: 0.53915 | eval_custom_logloss: 2.77918 |  0:00:10s
epoch 24 | loss: 0.52276 | eval_custom_logloss: 3.65092 |  0:00:10s
epoch 25 | loss: 0.55482 | eval_custom_logloss: 4.09346 |  0:00:11s
epoch 26 | loss: 0.51254 | eval_custom_logloss: 3.67793 |  0:00:11s
epoch 27 | loss: 0.55199 | eval_custom_logloss: 4.2428  |  0:00:12s
epoch 28 | loss: 0.53157 | eval_custom_logloss: 3.68124 |  0:00:12s
epoch 29 | loss: 0.51866 | eval_custom_logloss: 1.91273 |  0:00:13s
epoch 30 | loss: 0.51961 | eval_custom_logloss: 1.99088 |  0:00:13s
epoch 31 | loss: 0.52471 | eval_custom_logloss: 1.78082 |  0:00:14s
epoch 32 | loss: 0.55588 | eval_custom_logloss: 1.71583 |  0:00:14s
epoch 33 | loss: 0.52473 | eval_custom_logloss: 1.59404 |  0:00:15s
epoch 34 | loss: 0.51875 | eval_custom_logloss: 2.54244 |  0:00:15s
epoch 35 | loss: 0.50668 | eval_custom_logloss: 1.81952 |  0:00:16s
epoch 36 | loss: 0.50896 | eval_custom_logloss: 1.7416  |  0:00:16s
epoch 37 | loss: 0.4901  | eval_custom_logloss: 2.14388 |  0:00:17s
epoch 38 | loss: 0.46626 | eval_custom_logloss: 1.76128 |  0:00:17s
epoch 39 | loss: 0.49251 | eval_custom_logloss: 1.86945 |  0:00:18s
epoch 40 | loss: 0.46606 | eval_custom_logloss: 2.36592 |  0:00:18s
epoch 41 | loss: 0.46213 | eval_custom_logloss: 3.73793 |  0:00:19s
epoch 42 | loss: 0.43511 | eval_custom_logloss: 3.01726 |  0:00:19s
epoch 43 | loss: 0.46752 | eval_custom_logloss: 2.84909 |  0:00:20s
epoch 44 | loss: 0.47663 | eval_custom_logloss: 2.36869 |  0:00:20s
epoch 45 | loss: 0.44826 | eval_custom_logloss: 2.14817 |  0:00:21s
epoch 46 | loss: 0.43825 | eval_custom_logloss: 1.47886 |  0:00:21s
epoch 47 | loss: 0.41019 | eval_custom_logloss: 2.8076  |  0:00:22s
epoch 48 | loss: 0.42178 | eval_custom_logloss: 2.1516  |  0:00:22s
epoch 49 | loss: 0.41163 | eval_custom_logloss: 2.23219 |  0:00:23s
epoch 50 | loss: 0.44154 | eval_custom_logloss: 1.39921 |  0:00:23s
epoch 51 | loss: 0.45339 | eval_custom_logloss: 1.2343  |  0:00:23s
epoch 52 | loss: 0.42687 | eval_custom_logloss: 1.31589 |  0:00:24s
epoch 53 | loss: 0.41195 | eval_custom_logloss: 1.71531 |  0:00:24s
epoch 54 | loss: 0.41173 | eval_custom_logloss: 1.88062 |  0:00:25s
epoch 55 | loss: 0.39867 | eval_custom_logloss: 1.10011 |  0:00:25s
epoch 56 | loss: 0.38243 | eval_custom_logloss: 1.561   |  0:00:26s
epoch 57 | loss: 0.37995 | eval_custom_logloss: 1.55339 |  0:00:26s
epoch 58 | loss: 0.44487 | eval_custom_logloss: 1.59855 |  0:00:27s
epoch 59 | loss: 0.40627 | eval_custom_logloss: 2.56586 |  0:00:27s
epoch 60 | loss: 0.38571 | eval_custom_logloss: 3.19378 |  0:00:27s
epoch 61 | loss: 0.39303 | eval_custom_logloss: 2.49266 |  0:00:28s
epoch 62 | loss: 0.36517 | eval_custom_logloss: 3.17461 |  0:00:28s
epoch 63 | loss: 0.36051 | eval_custom_logloss: 3.10294 |  0:00:29s
epoch 64 | loss: 0.37922 | eval_custom_logloss: 3.62357 |  0:00:29s
epoch 65 | loss: 0.38453 | eval_custom_logloss: 2.48234 |  0:00:29s
epoch 66 | loss: 0.3718  | eval_custom_logloss: 2.83022 |  0:00:30s
epoch 67 | loss: 0.40953 | eval_custom_logloss: 2.40268 |  0:00:30s
epoch 68 | loss: 0.35703 | eval_custom_logloss: 3.05303 |  0:00:31s
epoch 69 | loss: 0.3611  | eval_custom_logloss: 2.45825 |  0:00:31s
epoch 70 | loss: 0.35557 | eval_custom_logloss: 1.60966 |  0:00:32s
epoch 71 | loss: 0.35614 | eval_custom_logloss: 2.29212 |  0:00:32s
epoch 72 | loss: 0.32922 | eval_custom_logloss: 2.23135 |  0:00:32s
epoch 73 | loss: 0.34043 | eval_custom_logloss: 2.30766 |  0:00:33s
epoch 74 | loss: 0.35033 | eval_custom_logloss: 2.03944 |  0:00:33s
epoch 75 | loss: 0.34011 | eval_custom_logloss: 1.64079 |  0:00:34s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 1.10011
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.2777999999999998, 'Log Loss - std': 0.25893681082457165} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 1.2777999999999998 and parameters: {'n_d': 55, 'n_steps': 4, 'gamma': 1.6097727441398082, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.0015812830896621236, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax', 'n_a': 27, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.23008 | eval_custom_logloss: 7.60593 |  0:00:00s
epoch 1  | loss: 1.08094 | eval_custom_logloss: 5.67218 |  0:00:01s
epoch 2  | loss: 1.03966 | eval_custom_logloss: 6.79045 |  0:00:02s
epoch 3  | loss: 0.95367 | eval_custom_logloss: 6.47969 |  0:00:03s
epoch 4  | loss: 0.90046 | eval_custom_logloss: 8.43025 |  0:00:03s
epoch 5  | loss: 0.91595 | eval_custom_logloss: 6.62971 |  0:00:04s
epoch 6  | loss: 0.88519 | eval_custom_logloss: 4.57285 |  0:00:05s
epoch 7  | loss: 0.81365 | eval_custom_logloss: 4.34356 |  0:00:05s
epoch 8  | loss: 0.74289 | eval_custom_logloss: 5.38533 |  0:00:06s
epoch 9  | loss: 0.8018  | eval_custom_logloss: 7.70429 |  0:00:07s
epoch 10 | loss: 0.77006 | eval_custom_logloss: 5.68049 |  0:00:07s
epoch 11 | loss: 0.76844 | eval_custom_logloss: 5.3478  |  0:00:08s
epoch 12 | loss: 0.7715  | eval_custom_logloss: 5.84186 |  0:00:09s
epoch 13 | loss: 0.7484  | eval_custom_logloss: 2.74514 |  0:00:09s
epoch 14 | loss: 0.75386 | eval_custom_logloss: 3.07365 |  0:00:10s
epoch 15 | loss: 0.70414 | eval_custom_logloss: 4.2875  |  0:00:11s
epoch 16 | loss: 0.73326 | eval_custom_logloss: 3.6911  |  0:00:11s
epoch 17 | loss: 0.68956 | eval_custom_logloss: 2.93361 |  0:00:12s
epoch 18 | loss: 0.7441  | eval_custom_logloss: 2.80812 |  0:00:12s
epoch 19 | loss: 0.70516 | eval_custom_logloss: 1.85549 |  0:00:13s
epoch 20 | loss: 0.67626 | eval_custom_logloss: 1.85545 |  0:00:14s
epoch 21 | loss: 0.68094 | eval_custom_logloss: 1.07844 |  0:00:15s
epoch 22 | loss: 0.67686 | eval_custom_logloss: 1.26589 |  0:00:15s
epoch 23 | loss: 0.6047  | eval_custom_logloss: 1.75384 |  0:00:16s
epoch 24 | loss: 0.61331 | eval_custom_logloss: 1.30975 |  0:00:17s
epoch 25 | loss: 0.69158 | eval_custom_logloss: 1.08368 |  0:00:17s
epoch 26 | loss: 0.651   | eval_custom_logloss: 1.44933 |  0:00:18s
epoch 27 | loss: 0.65246 | eval_custom_logloss: 1.03794 |  0:00:19s
epoch 28 | loss: 0.63751 | eval_custom_logloss: 1.6874  |  0:00:19s
epoch 29 | loss: 0.6385  | eval_custom_logloss: 1.87877 |  0:00:20s
epoch 30 | loss: 0.61581 | eval_custom_logloss: 1.48229 |  0:00:21s
epoch 31 | loss: 0.62302 | eval_custom_logloss: 1.04733 |  0:00:21s
epoch 32 | loss: 0.60777 | eval_custom_logloss: 1.57132 |  0:00:22s
epoch 33 | loss: 0.6121  | eval_custom_logloss: 2.00706 |  0:00:22s
epoch 34 | loss: 0.55593 | eval_custom_logloss: 1.68337 |  0:00:23s
epoch 35 | loss: 0.50757 | eval_custom_logloss: 1.78506 |  0:00:24s
epoch 36 | loss: 0.57367 | eval_custom_logloss: 1.40489 |  0:00:24s
epoch 37 | loss: 0.54837 | eval_custom_logloss: 1.21593 |  0:00:25s
epoch 38 | loss: 0.60579 | eval_custom_logloss: 1.37258 |  0:00:25s
epoch 39 | loss: 0.61301 | eval_custom_logloss: 1.72822 |  0:00:26s
epoch 40 | loss: 0.56769 | eval_custom_logloss: 1.04098 |  0:00:27s
epoch 41 | loss: 0.51888 | eval_custom_logloss: 1.32668 |  0:00:27s
epoch 42 | loss: 0.50708 | eval_custom_logloss: 1.16796 |  0:00:28s
epoch 43 | loss: 0.48104 | eval_custom_logloss: 0.936   |  0:00:29s
epoch 44 | loss: 0.48292 | eval_custom_logloss: 0.89079 |  0:00:29s
epoch 45 | loss: 0.47785 | eval_custom_logloss: 1.01687 |  0:00:30s
epoch 46 | loss: 0.47028 | eval_custom_logloss: 0.97485 |  0:00:31s
epoch 47 | loss: 0.45799 | eval_custom_logloss: 0.8236  |  0:00:31s
epoch 48 | loss: 0.43374 | eval_custom_logloss: 0.7357  |  0:00:32s
epoch 49 | loss: 0.4437  | eval_custom_logloss: 0.71854 |  0:00:32s
epoch 50 | loss: 0.45843 | eval_custom_logloss: 0.7938  |  0:00:33s
epoch 51 | loss: 0.49483 | eval_custom_logloss: 0.73247 |  0:00:34s
epoch 52 | loss: 0.49998 | eval_custom_logloss: 0.86276 |  0:00:34s
epoch 53 | loss: 0.47043 | eval_custom_logloss: 0.71948 |  0:00:35s
epoch 54 | loss: 0.42622 | eval_custom_logloss: 0.71399 |  0:00:36s
epoch 55 | loss: 0.43813 | eval_custom_logloss: 0.79407 |  0:00:36s
epoch 56 | loss: 0.42921 | eval_custom_logloss: 0.6544  |  0:00:37s
epoch 57 | loss: 0.42239 | eval_custom_logloss: 0.72365 |  0:00:37s
epoch 58 | loss: 0.4176  | eval_custom_logloss: 0.62404 |  0:00:38s
epoch 59 | loss: 0.39931 | eval_custom_logloss: 0.92356 |  0:00:39s
epoch 60 | loss: 0.41728 | eval_custom_logloss: 0.61558 |  0:00:39s
epoch 61 | loss: 0.43324 | eval_custom_logloss: 0.72226 |  0:00:40s
epoch 62 | loss: 0.48409 | eval_custom_logloss: 0.97271 |  0:00:41s
epoch 63 | loss: 0.46262 | eval_custom_logloss: 0.78226 |  0:00:41s
epoch 64 | loss: 0.45687 | eval_custom_logloss: 0.81477 |  0:00:42s
epoch 65 | loss: 0.40325 | eval_custom_logloss: 0.81921 |  0:00:43s
epoch 66 | loss: 0.35921 | eval_custom_logloss: 1.2229  |  0:00:43s
epoch 67 | loss: 0.40229 | eval_custom_logloss: 0.81079 |  0:00:44s
epoch 68 | loss: 0.38279 | eval_custom_logloss: 0.85024 |  0:00:45s
epoch 69 | loss: 0.39689 | eval_custom_logloss: 0.72133 |  0:00:46s
epoch 70 | loss: 0.34965 | eval_custom_logloss: 0.77824 |  0:00:46s
epoch 71 | loss: 0.37188 | eval_custom_logloss: 0.67494 |  0:00:47s
epoch 72 | loss: 0.37727 | eval_custom_logloss: 0.71734 |  0:00:48s
epoch 73 | loss: 0.42555 | eval_custom_logloss: 0.82989 |  0:00:49s
epoch 74 | loss: 0.36298 | eval_custom_logloss: 0.65404 |  0:00:49s
epoch 75 | loss: 0.35422 | eval_custom_logloss: 0.71215 |  0:00:50s
epoch 76 | loss: 0.34019 | eval_custom_logloss: 0.78725 |  0:00:50s
epoch 77 | loss: 0.31717 | eval_custom_logloss: 0.94408 |  0:00:51s
epoch 78 | loss: 0.33688 | eval_custom_logloss: 1.17475 |  0:00:52s
epoch 79 | loss: 0.35894 | eval_custom_logloss: 0.77703 |  0:00:52s
epoch 80 | loss: 0.35651 | eval_custom_logloss: 1.48078 |  0:00:53s

Early stopping occurred at epoch 80 with best_epoch = 60 and best_eval_custom_logloss = 0.61558
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6156, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax', 'n_a': 27, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.29059 | eval_custom_logloss: 8.16546 |  0:00:00s
epoch 1  | loss: 1.08686 | eval_custom_logloss: 8.07677 |  0:00:01s
epoch 2  | loss: 1.04729 | eval_custom_logloss: 6.94394 |  0:00:02s
epoch 3  | loss: 0.96345 | eval_custom_logloss: 5.67198 |  0:00:02s
epoch 4  | loss: 0.95586 | eval_custom_logloss: 4.68224 |  0:00:03s
epoch 5  | loss: 0.88421 | eval_custom_logloss: 6.33652 |  0:00:04s
epoch 6  | loss: 0.83249 | eval_custom_logloss: 5.82014 |  0:00:04s
epoch 7  | loss: 0.83101 | eval_custom_logloss: 6.57294 |  0:00:05s
epoch 8  | loss: 0.7426  | eval_custom_logloss: 6.04877 |  0:00:05s
epoch 9  | loss: 0.72113 | eval_custom_logloss: 4.6371  |  0:00:06s
epoch 10 | loss: 0.69483 | eval_custom_logloss: 5.65985 |  0:00:07s
epoch 11 | loss: 0.73757 | eval_custom_logloss: 3.80871 |  0:00:07s
epoch 12 | loss: 0.71601 | eval_custom_logloss: 3.62481 |  0:00:08s
epoch 13 | loss: 0.6872  | eval_custom_logloss: 2.82916 |  0:00:09s
epoch 14 | loss: 0.63416 | eval_custom_logloss: 1.90264 |  0:00:09s
epoch 15 | loss: 0.64225 | eval_custom_logloss: 2.1214  |  0:00:10s
epoch 16 | loss: 0.69779 | eval_custom_logloss: 1.94369 |  0:00:11s
epoch 17 | loss: 0.66389 | eval_custom_logloss: 2.37418 |  0:00:11s
epoch 18 | loss: 0.64263 | eval_custom_logloss: 2.17844 |  0:00:12s
epoch 19 | loss: 0.60675 | eval_custom_logloss: 1.87248 |  0:00:13s
epoch 20 | loss: 0.5679  | eval_custom_logloss: 1.34433 |  0:00:13s
epoch 21 | loss: 0.5785  | eval_custom_logloss: 1.87823 |  0:00:14s
epoch 22 | loss: 0.58082 | eval_custom_logloss: 1.45459 |  0:00:15s
epoch 23 | loss: 0.57989 | eval_custom_logloss: 1.18671 |  0:00:15s
epoch 24 | loss: 0.59082 | eval_custom_logloss: 1.11594 |  0:00:16s
epoch 25 | loss: 0.62504 | eval_custom_logloss: 1.04748 |  0:00:17s
epoch 26 | loss: 0.57155 | eval_custom_logloss: 1.07519 |  0:00:17s
epoch 27 | loss: 0.53848 | eval_custom_logloss: 1.14056 |  0:00:18s
epoch 28 | loss: 0.55581 | eval_custom_logloss: 1.01123 |  0:00:18s
epoch 29 | loss: 0.51095 | eval_custom_logloss: 0.96694 |  0:00:19s
epoch 30 | loss: 0.52531 | eval_custom_logloss: 1.05374 |  0:00:20s
epoch 31 | loss: 0.52944 | eval_custom_logloss: 1.48696 |  0:00:20s
epoch 32 | loss: 0.54641 | eval_custom_logloss: 0.98386 |  0:00:21s
epoch 33 | loss: 0.53965 | eval_custom_logloss: 0.9136  |  0:00:22s
epoch 34 | loss: 0.53526 | eval_custom_logloss: 0.85782 |  0:00:22s
epoch 35 | loss: 0.51511 | eval_custom_logloss: 0.94832 |  0:00:23s
epoch 36 | loss: 0.5419  | eval_custom_logloss: 1.46924 |  0:00:23s
epoch 37 | loss: 0.59607 | eval_custom_logloss: 1.54933 |  0:00:24s
epoch 38 | loss: 0.59259 | eval_custom_logloss: 1.29741 |  0:00:25s
epoch 39 | loss: 0.64323 | eval_custom_logloss: 1.60404 |  0:00:25s
epoch 40 | loss: 0.60032 | eval_custom_logloss: 1.4693  |  0:00:26s
epoch 41 | loss: 0.59846 | eval_custom_logloss: 1.10059 |  0:00:27s
epoch 42 | loss: 0.56816 | eval_custom_logloss: 1.13147 |  0:00:27s
epoch 43 | loss: 0.52703 | eval_custom_logloss: 1.2225  |  0:00:28s
epoch 44 | loss: 0.5269  | eval_custom_logloss: 1.27377 |  0:00:28s
epoch 45 | loss: 0.5007  | eval_custom_logloss: 0.85944 |  0:00:29s
epoch 46 | loss: 0.49417 | eval_custom_logloss: 1.10612 |  0:00:30s
epoch 47 | loss: 0.51325 | eval_custom_logloss: 0.89279 |  0:00:30s
epoch 48 | loss: 0.50298 | eval_custom_logloss: 0.85164 |  0:00:31s
epoch 49 | loss: 0.4951  | eval_custom_logloss: 0.82012 |  0:00:32s
epoch 50 | loss: 0.46494 | eval_custom_logloss: 0.83032 |  0:00:32s
epoch 51 | loss: 0.4515  | eval_custom_logloss: 0.81689 |  0:00:33s
epoch 52 | loss: 0.45697 | eval_custom_logloss: 0.90924 |  0:00:33s
epoch 53 | loss: 0.47817 | eval_custom_logloss: 1.06877 |  0:00:34s
epoch 54 | loss: 0.4504  | eval_custom_logloss: 0.88909 |  0:00:35s
epoch 55 | loss: 0.45662 | eval_custom_logloss: 0.81742 |  0:00:35s
epoch 56 | loss: 0.48396 | eval_custom_logloss: 0.6597  |  0:00:36s
epoch 57 | loss: 0.48102 | eval_custom_logloss: 0.73942 |  0:00:37s
epoch 58 | loss: 0.48404 | eval_custom_logloss: 0.83382 |  0:00:37s
epoch 59 | loss: 0.46524 | eval_custom_logloss: 0.72939 |  0:00:38s
epoch 60 | loss: 0.44099 | eval_custom_logloss: 0.85292 |  0:00:38s
epoch 61 | loss: 0.44045 | eval_custom_logloss: 1.074   |  0:00:39s
epoch 62 | loss: 0.47652 | eval_custom_logloss: 0.86839 |  0:00:40s
epoch 63 | loss: 0.43978 | eval_custom_logloss: 0.92787 |  0:00:40s
epoch 64 | loss: 0.44148 | eval_custom_logloss: 0.92385 |  0:00:41s
epoch 65 | loss: 0.44103 | eval_custom_logloss: 0.87876 |  0:00:42s
epoch 66 | loss: 0.43065 | eval_custom_logloss: 0.96346 |  0:00:42s
epoch 67 | loss: 0.43623 | eval_custom_logloss: 0.81553 |  0:00:43s
epoch 68 | loss: 0.41432 | eval_custom_logloss: 0.76698 |  0:00:43s
epoch 69 | loss: 0.42257 | eval_custom_logloss: 0.72027 |  0:00:44s
epoch 70 | loss: 0.42396 | eval_custom_logloss: 0.84294 |  0:00:45s
epoch 71 | loss: 0.42016 | eval_custom_logloss: 0.72304 |  0:00:45s
epoch 72 | loss: 0.42214 | eval_custom_logloss: 0.70498 |  0:00:46s
epoch 73 | loss: 0.4034  | eval_custom_logloss: 0.74146 |  0:00:47s
epoch 74 | loss: 0.41558 | eval_custom_logloss: 0.76397 |  0:00:47s
epoch 75 | loss: 0.40892 | eval_custom_logloss: 0.63781 |  0:00:48s
epoch 76 | loss: 0.38157 | eval_custom_logloss: 0.66404 |  0:00:48s
epoch 77 | loss: 0.38266 | eval_custom_logloss: 0.75317 |  0:00:49s
epoch 78 | loss: 0.37657 | eval_custom_logloss: 0.8108  |  0:00:50s
epoch 79 | loss: 0.3744  | eval_custom_logloss: 0.70171 |  0:00:50s
epoch 80 | loss: 0.36102 | eval_custom_logloss: 0.67981 |  0:00:51s
epoch 81 | loss: 0.35005 | eval_custom_logloss: 0.72913 |  0:00:52s
epoch 82 | loss: 0.35012 | eval_custom_logloss: 0.80967 |  0:00:52s
epoch 83 | loss: 0.3298  | eval_custom_logloss: 0.67856 |  0:00:53s
epoch 84 | loss: 0.35283 | eval_custom_logloss: 0.70469 |  0:00:53s
epoch 85 | loss: 0.33934 | eval_custom_logloss: 0.7848  |  0:00:54s
epoch 86 | loss: 0.3339  | eval_custom_logloss: 0.76389 |  0:00:55s
epoch 87 | loss: 0.33095 | eval_custom_logloss: 0.7362  |  0:00:55s
epoch 88 | loss: 0.30677 | eval_custom_logloss: 0.72303 |  0:00:56s
epoch 89 | loss: 0.32426 | eval_custom_logloss: 0.68787 |  0:00:57s
epoch 90 | loss: 0.3178  | eval_custom_logloss: 0.74239 |  0:00:57s
epoch 91 | loss: 0.32899 | eval_custom_logloss: 0.71558 |  0:00:58s
epoch 92 | loss: 0.33901 | eval_custom_logloss: 0.67216 |  0:00:59s
epoch 93 | loss: 0.33404 | eval_custom_logloss: 0.79937 |  0:00:59s
epoch 94 | loss: 0.33964 | eval_custom_logloss: 0.74937 |  0:01:00s
epoch 95 | loss: 0.31243 | eval_custom_logloss: 0.71959 |  0:01:00s

Early stopping occurred at epoch 95 with best_epoch = 75 and best_eval_custom_logloss = 0.63781
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6267, 'Log Loss - std': 0.011099999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax', 'n_a': 27, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.249   | eval_custom_logloss: 8.09727 |  0:00:00s
epoch 1  | loss: 1.10588 | eval_custom_logloss: 7.47783 |  0:00:01s
epoch 2  | loss: 1.08129 | eval_custom_logloss: 7.78008 |  0:00:01s
epoch 3  | loss: 1.00568 | eval_custom_logloss: 7.29907 |  0:00:02s
epoch 4  | loss: 0.89154 | eval_custom_logloss: 5.47613 |  0:00:03s
epoch 5  | loss: 0.79867 | eval_custom_logloss: 5.22298 |  0:00:03s
epoch 6  | loss: 0.81065 | eval_custom_logloss: 3.75349 |  0:00:04s
epoch 7  | loss: 0.89854 | eval_custom_logloss: 5.36408 |  0:00:05s
epoch 8  | loss: 0.82482 | eval_custom_logloss: 2.66982 |  0:00:05s
epoch 9  | loss: 0.76818 | eval_custom_logloss: 3.39378 |  0:00:06s
epoch 10 | loss: 0.80082 | eval_custom_logloss: 6.98144 |  0:00:07s
epoch 11 | loss: 0.78236 | eval_custom_logloss: 3.41632 |  0:00:07s
epoch 12 | loss: 0.74783 | eval_custom_logloss: 3.45491 |  0:00:08s
epoch 13 | loss: 0.71508 | eval_custom_logloss: 3.16251 |  0:00:08s
epoch 14 | loss: 0.7452  | eval_custom_logloss: 4.52307 |  0:00:09s
epoch 15 | loss: 0.70411 | eval_custom_logloss: 5.47028 |  0:00:10s
epoch 16 | loss: 0.68443 | eval_custom_logloss: 3.27092 |  0:00:10s
epoch 17 | loss: 0.68434 | eval_custom_logloss: 1.91653 |  0:00:11s
epoch 18 | loss: 0.70002 | eval_custom_logloss: 2.34192 |  0:00:12s
epoch 19 | loss: 0.70079 | eval_custom_logloss: 2.2215  |  0:00:12s
epoch 20 | loss: 0.68907 | eval_custom_logloss: 2.05339 |  0:00:13s
epoch 21 | loss: 0.7055  | eval_custom_logloss: 2.7297  |  0:00:13s
epoch 22 | loss: 0.65778 | eval_custom_logloss: 2.96504 |  0:00:14s
epoch 23 | loss: 0.64006 | eval_custom_logloss: 2.7554  |  0:00:15s
epoch 24 | loss: 0.62189 | eval_custom_logloss: 2.46722 |  0:00:15s
epoch 25 | loss: 0.60902 | eval_custom_logloss: 1.54399 |  0:00:16s
epoch 26 | loss: 0.64239 | eval_custom_logloss: 1.17811 |  0:00:17s
epoch 27 | loss: 0.62663 | eval_custom_logloss: 1.41205 |  0:00:17s
epoch 28 | loss: 0.61954 | eval_custom_logloss: 1.30869 |  0:00:18s
epoch 29 | loss: 0.63692 | eval_custom_logloss: 1.30522 |  0:00:19s
epoch 30 | loss: 0.65467 | eval_custom_logloss: 0.81154 |  0:00:19s
epoch 31 | loss: 0.64714 | eval_custom_logloss: 1.02093 |  0:00:20s
epoch 32 | loss: 0.63213 | eval_custom_logloss: 1.10526 |  0:00:20s
epoch 33 | loss: 0.61927 | eval_custom_logloss: 1.04629 |  0:00:21s
epoch 34 | loss: 0.63597 | eval_custom_logloss: 1.08559 |  0:00:22s
epoch 35 | loss: 0.63654 | eval_custom_logloss: 1.18291 |  0:00:22s
epoch 36 | loss: 0.64296 | eval_custom_logloss: 1.13537 |  0:00:23s
epoch 37 | loss: 0.61923 | eval_custom_logloss: 1.11944 |  0:00:24s
epoch 38 | loss: 0.6312  | eval_custom_logloss: 0.98292 |  0:00:24s
epoch 39 | loss: 0.67519 | eval_custom_logloss: 1.00954 |  0:00:25s
epoch 40 | loss: 0.68182 | eval_custom_logloss: 1.32666 |  0:00:25s
epoch 41 | loss: 0.67882 | eval_custom_logloss: 1.27721 |  0:00:26s
epoch 42 | loss: 0.66598 | eval_custom_logloss: 1.44111 |  0:00:27s
epoch 43 | loss: 0.65201 | eval_custom_logloss: 1.27881 |  0:00:27s
epoch 44 | loss: 0.685   | eval_custom_logloss: 1.27348 |  0:00:28s
epoch 45 | loss: 0.64847 | eval_custom_logloss: 1.57206 |  0:00:28s
epoch 46 | loss: 0.65364 | eval_custom_logloss: 1.05676 |  0:00:29s
epoch 47 | loss: 0.6508  | eval_custom_logloss: 1.15733 |  0:00:30s
epoch 48 | loss: 0.61574 | eval_custom_logloss: 1.14982 |  0:00:30s
epoch 49 | loss: 0.58917 | eval_custom_logloss: 1.09774 |  0:00:31s
epoch 50 | loss: 0.57683 | eval_custom_logloss: 1.06068 |  0:00:32s

Early stopping occurred at epoch 50 with best_epoch = 30 and best_eval_custom_logloss = 0.81154
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6883, 'Log Loss - std': 0.08758572943122639} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax', 'n_a': 27, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.22275 | eval_custom_logloss: 8.53934 |  0:00:00s
epoch 1  | loss: 1.04464 | eval_custom_logloss: 7.18195 |  0:00:01s
epoch 2  | loss: 1.03336 | eval_custom_logloss: 6.36417 |  0:00:02s
epoch 3  | loss: 0.88858 | eval_custom_logloss: 6.0485  |  0:00:02s
epoch 4  | loss: 0.84337 | eval_custom_logloss: 4.35265 |  0:00:03s
epoch 5  | loss: 0.81874 | eval_custom_logloss: 3.79723 |  0:00:04s
epoch 6  | loss: 0.79345 | eval_custom_logloss: 4.94569 |  0:00:04s
epoch 7  | loss: 0.80544 | eval_custom_logloss: 4.16881 |  0:00:05s
epoch 8  | loss: 0.85934 | eval_custom_logloss: 5.10329 |  0:00:06s
epoch 9  | loss: 0.75106 | eval_custom_logloss: 2.74134 |  0:00:06s
epoch 10 | loss: 0.71562 | eval_custom_logloss: 4.66794 |  0:00:07s
epoch 11 | loss: 0.72029 | eval_custom_logloss: 3.77713 |  0:00:08s
epoch 12 | loss: 0.64637 | eval_custom_logloss: 3.95515 |  0:00:08s
epoch 13 | loss: 0.67199 | eval_custom_logloss: 2.94687 |  0:00:09s
epoch 14 | loss: 0.62508 | eval_custom_logloss: 3.94346 |  0:00:10s
epoch 15 | loss: 0.64759 | eval_custom_logloss: 3.61736 |  0:00:10s
epoch 16 | loss: 0.62254 | eval_custom_logloss: 3.22276 |  0:00:11s
epoch 17 | loss: 0.61054 | eval_custom_logloss: 2.47159 |  0:00:11s
epoch 18 | loss: 0.58662 | eval_custom_logloss: 2.99278 |  0:00:12s
epoch 19 | loss: 0.58251 | eval_custom_logloss: 2.81978 |  0:00:13s
epoch 20 | loss: 0.58619 | eval_custom_logloss: 1.72821 |  0:00:13s
epoch 21 | loss: 0.5531  | eval_custom_logloss: 1.80125 |  0:00:14s
epoch 22 | loss: 0.55619 | eval_custom_logloss: 1.92814 |  0:00:15s
epoch 23 | loss: 0.60431 | eval_custom_logloss: 1.51452 |  0:00:15s
epoch 24 | loss: 0.57193 | eval_custom_logloss: 1.43362 |  0:00:16s
epoch 25 | loss: 0.58799 | eval_custom_logloss: 2.19593 |  0:00:16s
epoch 26 | loss: 0.57832 | eval_custom_logloss: 2.27705 |  0:00:17s
epoch 27 | loss: 0.54881 | eval_custom_logloss: 1.90854 |  0:00:18s
epoch 28 | loss: 0.50758 | eval_custom_logloss: 1.86868 |  0:00:18s
epoch 29 | loss: 0.52573 | eval_custom_logloss: 1.69768 |  0:00:19s
epoch 30 | loss: 0.49808 | eval_custom_logloss: 2.0001  |  0:00:20s
epoch 31 | loss: 0.49897 | eval_custom_logloss: 1.57022 |  0:00:20s
epoch 32 | loss: 0.47715 | eval_custom_logloss: 1.16496 |  0:00:21s
epoch 33 | loss: 0.49505 | eval_custom_logloss: 1.11483 |  0:00:21s
epoch 34 | loss: 0.51627 | eval_custom_logloss: 1.32525 |  0:00:22s
epoch 35 | loss: 0.53975 | eval_custom_logloss: 1.13569 |  0:00:23s
epoch 36 | loss: 0.47409 | eval_custom_logloss: 1.44172 |  0:00:23s
epoch 37 | loss: 0.45657 | eval_custom_logloss: 1.10531 |  0:00:24s
epoch 38 | loss: 0.46909 | eval_custom_logloss: 1.01833 |  0:00:25s
epoch 39 | loss: 0.48534 | eval_custom_logloss: 1.05232 |  0:00:25s
epoch 40 | loss: 0.47848 | eval_custom_logloss: 1.14818 |  0:00:26s
epoch 41 | loss: 0.4358  | eval_custom_logloss: 1.0767  |  0:00:26s
epoch 42 | loss: 0.45551 | eval_custom_logloss: 0.80488 |  0:00:27s
epoch 43 | loss: 0.45724 | eval_custom_logloss: 1.03624 |  0:00:28s
epoch 44 | loss: 0.43204 | eval_custom_logloss: 0.91554 |  0:00:28s
epoch 45 | loss: 0.44137 | eval_custom_logloss: 0.9272  |  0:00:29s
epoch 46 | loss: 0.4147  | eval_custom_logloss: 0.73759 |  0:00:30s
epoch 47 | loss: 0.45501 | eval_custom_logloss: 0.86546 |  0:00:30s
epoch 48 | loss: 0.43742 | eval_custom_logloss: 1.02601 |  0:00:31s
epoch 49 | loss: 0.43424 | eval_custom_logloss: 0.9614  |  0:00:31s
epoch 50 | loss: 0.39471 | eval_custom_logloss: 0.97283 |  0:00:32s
epoch 51 | loss: 0.40159 | eval_custom_logloss: 0.78365 |  0:00:33s
epoch 52 | loss: 0.38931 | eval_custom_logloss: 0.92091 |  0:00:33s
epoch 53 | loss: 0.4053  | eval_custom_logloss: 1.00546 |  0:00:34s
epoch 54 | loss: 0.41196 | eval_custom_logloss: 0.73675 |  0:00:35s
epoch 55 | loss: 0.39269 | eval_custom_logloss: 0.87245 |  0:00:35s
epoch 56 | loss: 0.38681 | eval_custom_logloss: 0.64279 |  0:00:36s
epoch 57 | loss: 0.34136 | eval_custom_logloss: 0.68615 |  0:00:36s
epoch 58 | loss: 0.37316 | eval_custom_logloss: 0.78664 |  0:00:37s
epoch 59 | loss: 0.3411  | eval_custom_logloss: 0.80389 |  0:00:38s
epoch 60 | loss: 0.37546 | eval_custom_logloss: 0.84146 |  0:00:38s
epoch 61 | loss: 0.36899 | eval_custom_logloss: 0.64802 |  0:00:39s
epoch 62 | loss: 0.3607  | eval_custom_logloss: 0.73594 |  0:00:40s
epoch 63 | loss: 0.32433 | eval_custom_logloss: 0.69737 |  0:00:40s
epoch 64 | loss: 0.33251 | eval_custom_logloss: 0.76218 |  0:00:41s
epoch 65 | loss: 0.33251 | eval_custom_logloss: 0.70643 |  0:00:41s
epoch 66 | loss: 0.3592  | eval_custom_logloss: 0.69994 |  0:00:42s
epoch 67 | loss: 0.33333 | eval_custom_logloss: 0.7076  |  0:00:43s
epoch 68 | loss: 0.34146 | eval_custom_logloss: 0.6739  |  0:00:43s
epoch 69 | loss: 0.2992  | eval_custom_logloss: 0.6369  |  0:00:44s
epoch 70 | loss: 0.28202 | eval_custom_logloss: 0.66847 |  0:00:45s
epoch 71 | loss: 0.28063 | eval_custom_logloss: 0.60589 |  0:00:45s
epoch 72 | loss: 0.30987 | eval_custom_logloss: 0.75239 |  0:00:46s
epoch 73 | loss: 0.35209 | eval_custom_logloss: 0.62503 |  0:00:46s
epoch 74 | loss: 0.36203 | eval_custom_logloss: 0.7413  |  0:00:47s
epoch 75 | loss: 0.28897 | eval_custom_logloss: 0.66347 |  0:00:48s
epoch 76 | loss: 0.28672 | eval_custom_logloss: 0.58953 |  0:00:48s
epoch 77 | loss: 0.31124 | eval_custom_logloss: 0.72232 |  0:00:49s
epoch 78 | loss: 0.26417 | eval_custom_logloss: 0.70848 |  0:00:50s
epoch 79 | loss: 0.27024 | eval_custom_logloss: 0.70007 |  0:00:50s
epoch 80 | loss: 0.24885 | eval_custom_logloss: 0.69956 |  0:00:51s
epoch 81 | loss: 0.26232 | eval_custom_logloss: 0.61931 |  0:00:51s
epoch 82 | loss: 0.23974 | eval_custom_logloss: 0.57486 |  0:00:52s
epoch 83 | loss: 0.25128 | eval_custom_logloss: 0.6581  |  0:00:53s
epoch 84 | loss: 0.26127 | eval_custom_logloss: 0.74364 |  0:00:53s
epoch 85 | loss: 0.2381  | eval_custom_logloss: 0.54176 |  0:00:54s
epoch 86 | loss: 0.27769 | eval_custom_logloss: 0.61856 |  0:00:55s
epoch 87 | loss: 0.25222 | eval_custom_logloss: 0.70006 |  0:00:55s
epoch 88 | loss: 0.27609 | eval_custom_logloss: 0.65495 |  0:00:56s
epoch 89 | loss: 0.22738 | eval_custom_logloss: 0.65769 |  0:00:56s
epoch 90 | loss: 0.25723 | eval_custom_logloss: 0.77048 |  0:00:57s
epoch 91 | loss: 0.32955 | eval_custom_logloss: 0.965   |  0:00:58s
epoch 92 | loss: 0.28919 | eval_custom_logloss: 1.13326 |  0:00:58s
epoch 93 | loss: 0.31958 | eval_custom_logloss: 1.4362  |  0:00:59s
epoch 94 | loss: 0.29961 | eval_custom_logloss: 1.1881  |  0:00:59s
epoch 95 | loss: 0.28013 | eval_custom_logloss: 1.14994 |  0:01:00s
epoch 96 | loss: 0.29158 | eval_custom_logloss: 0.89403 |  0:01:01s
epoch 97 | loss: 0.26663 | eval_custom_logloss: 0.91962 |  0:01:01s
epoch 98 | loss: 0.29171 | eval_custom_logloss: 0.926   |  0:01:02s
epoch 99 | loss: 0.29509 | eval_custom_logloss: 0.84706 |  0:01:03s
Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_eval_custom_logloss = 0.54176
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.651675, 'Log Loss - std': 0.09888183288653181} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax', 'n_a': 27, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.26606 | eval_custom_logloss: 6.4308  |  0:00:00s
epoch 1  | loss: 1.01145 | eval_custom_logloss: 7.2806  |  0:00:01s
epoch 2  | loss: 0.95854 | eval_custom_logloss: 5.41128 |  0:00:02s
epoch 3  | loss: 0.85108 | eval_custom_logloss: 5.90202 |  0:00:02s
epoch 4  | loss: 0.81852 | eval_custom_logloss: 5.65802 |  0:00:03s
epoch 5  | loss: 0.82506 | eval_custom_logloss: 6.06589 |  0:00:04s
epoch 6  | loss: 0.77313 | eval_custom_logloss: 4.29597 |  0:00:04s
epoch 7  | loss: 0.7429  | eval_custom_logloss: 5.18754 |  0:00:05s
epoch 8  | loss: 0.70564 | eval_custom_logloss: 4.52992 |  0:00:05s
epoch 9  | loss: 0.66313 | eval_custom_logloss: 2.1629  |  0:00:06s
epoch 10 | loss: 0.67019 | eval_custom_logloss: 3.99424 |  0:00:07s
epoch 11 | loss: 0.65417 | eval_custom_logloss: 3.37568 |  0:00:07s
epoch 12 | loss: 0.64714 | eval_custom_logloss: 3.18412 |  0:00:08s
epoch 13 | loss: 0.61511 | eval_custom_logloss: 3.5647  |  0:00:09s
epoch 14 | loss: 0.59553 | eval_custom_logloss: 3.6562  |  0:00:09s
epoch 15 | loss: 0.5611  | eval_custom_logloss: 1.84578 |  0:00:10s
epoch 16 | loss: 0.56538 | eval_custom_logloss: 2.21769 |  0:00:11s
epoch 17 | loss: 0.57282 | eval_custom_logloss: 1.64327 |  0:00:11s
epoch 18 | loss: 0.56999 | eval_custom_logloss: 1.12465 |  0:00:12s
epoch 19 | loss: 0.56435 | eval_custom_logloss: 1.15515 |  0:00:13s
epoch 20 | loss: 0.55865 | eval_custom_logloss: 1.04041 |  0:00:13s
epoch 21 | loss: 0.53805 | eval_custom_logloss: 1.06303 |  0:00:14s
epoch 22 | loss: 0.49385 | eval_custom_logloss: 1.00325 |  0:00:14s
epoch 23 | loss: 0.44067 | eval_custom_logloss: 0.79367 |  0:00:15s
epoch 24 | loss: 0.45279 | eval_custom_logloss: 0.88444 |  0:00:16s
epoch 25 | loss: 0.45638 | eval_custom_logloss: 0.64176 |  0:00:16s
epoch 26 | loss: 0.44359 | eval_custom_logloss: 0.67009 |  0:00:17s
epoch 27 | loss: 0.40678 | eval_custom_logloss: 0.70369 |  0:00:18s
epoch 28 | loss: 0.47348 | eval_custom_logloss: 0.6324  |  0:00:18s
epoch 29 | loss: 0.42206 | eval_custom_logloss: 1.13391 |  0:00:19s
epoch 30 | loss: 0.43255 | eval_custom_logloss: 0.69403 |  0:00:19s
epoch 31 | loss: 0.42554 | eval_custom_logloss: 0.72569 |  0:00:20s
epoch 32 | loss: 0.37462 | eval_custom_logloss: 0.79167 |  0:00:21s
epoch 33 | loss: 0.35458 | eval_custom_logloss: 0.78468 |  0:00:21s
epoch 34 | loss: 0.36989 | eval_custom_logloss: 0.66317 |  0:00:22s
epoch 35 | loss: 0.34707 | eval_custom_logloss: 0.79492 |  0:00:22s
epoch 36 | loss: 0.3402  | eval_custom_logloss: 0.68493 |  0:00:23s
epoch 37 | loss: 0.31281 | eval_custom_logloss: 0.72926 |  0:00:24s
epoch 38 | loss: 0.33532 | eval_custom_logloss: 0.72314 |  0:00:24s
epoch 39 | loss: 0.36389 | eval_custom_logloss: 0.77344 |  0:00:25s
epoch 40 | loss: 0.32903 | eval_custom_logloss: 0.51809 |  0:00:26s
epoch 41 | loss: 0.34975 | eval_custom_logloss: 0.67739 |  0:00:26s
epoch 42 | loss: 0.33656 | eval_custom_logloss: 0.62893 |  0:00:27s
epoch 43 | loss: 0.31895 | eval_custom_logloss: 0.88753 |  0:00:27s
epoch 44 | loss: 0.30373 | eval_custom_logloss: 0.96798 |  0:00:28s
epoch 45 | loss: 0.30941 | eval_custom_logloss: 0.93264 |  0:00:29s
epoch 46 | loss: 0.32175 | eval_custom_logloss: 1.00891 |  0:00:29s
epoch 47 | loss: 0.27965 | eval_custom_logloss: 0.54003 |  0:00:30s
epoch 48 | loss: 0.26278 | eval_custom_logloss: 0.62571 |  0:00:31s
epoch 49 | loss: 0.29274 | eval_custom_logloss: 0.7604  |  0:00:31s
epoch 50 | loss: 0.36545 | eval_custom_logloss: 0.97964 |  0:00:32s
epoch 51 | loss: 0.36507 | eval_custom_logloss: 0.67524 |  0:00:33s
epoch 52 | loss: 0.41133 | eval_custom_logloss: 0.84979 |  0:00:33s
epoch 53 | loss: 0.45651 | eval_custom_logloss: 0.62141 |  0:00:34s
epoch 54 | loss: 0.40028 | eval_custom_logloss: 0.62097 |  0:00:35s
epoch 55 | loss: 0.36538 | eval_custom_logloss: 0.8891  |  0:00:35s
epoch 56 | loss: 0.42988 | eval_custom_logloss: 0.96257 |  0:00:36s
epoch 57 | loss: 0.37439 | eval_custom_logloss: 1.03943 |  0:00:36s
epoch 58 | loss: 0.36232 | eval_custom_logloss: 0.8752  |  0:00:37s
epoch 59 | loss: 0.31327 | eval_custom_logloss: 0.92181 |  0:00:38s
epoch 60 | loss: 0.30723 | eval_custom_logloss: 0.78513 |  0:00:38s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.51809
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.62496, 'Log Loss - std': 0.10332888463542032} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 0.62496 and parameters: {'n_d': 27, 'n_steps': 6, 'gamma': 1.7411762190744702, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0053857392601423856, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax', 'n_a': 63, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.9969  | eval_custom_logloss: 4.05877 |  0:00:00s
epoch 1  | loss: 1.55366 | eval_custom_logloss: 1.87332 |  0:00:01s
epoch 2  | loss: 1.1722  | eval_custom_logloss: 2.05018 |  0:00:01s
epoch 3  | loss: 1.03236 | eval_custom_logloss: 1.72564 |  0:00:02s
epoch 4  | loss: 1.04109 | eval_custom_logloss: 1.39117 |  0:00:02s
epoch 5  | loss: 1.45452 | eval_custom_logloss: 2.2732  |  0:00:03s
epoch 6  | loss: 0.96356 | eval_custom_logloss: 0.94313 |  0:00:03s
epoch 7  | loss: 0.79356 | eval_custom_logloss: 1.26642 |  0:00:04s
epoch 8  | loss: 0.81302 | eval_custom_logloss: 0.97863 |  0:00:04s
epoch 9  | loss: 0.75058 | eval_custom_logloss: 1.00715 |  0:00:05s
epoch 10 | loss: 0.76077 | eval_custom_logloss: 1.47998 |  0:00:05s
epoch 11 | loss: 0.77167 | eval_custom_logloss: 1.0602  |  0:00:06s
epoch 12 | loss: 0.66544 | eval_custom_logloss: 0.84305 |  0:00:06s
epoch 13 | loss: 0.67777 | eval_custom_logloss: 0.94217 |  0:00:07s
epoch 14 | loss: 0.64975 | eval_custom_logloss: 0.7804  |  0:00:07s
epoch 15 | loss: 0.62173 | eval_custom_logloss: 0.69727 |  0:00:08s
epoch 16 | loss: 0.62934 | eval_custom_logloss: 0.82896 |  0:00:08s
epoch 17 | loss: 0.63414 | eval_custom_logloss: 0.64699 |  0:00:09s
epoch 18 | loss: 0.64484 | eval_custom_logloss: 0.6629  |  0:00:09s
epoch 19 | loss: 0.64399 | eval_custom_logloss: 0.70607 |  0:00:10s
epoch 20 | loss: 0.65608 | eval_custom_logloss: 0.67177 |  0:00:10s
epoch 21 | loss: 0.62183 | eval_custom_logloss: 0.68867 |  0:00:11s
epoch 22 | loss: 0.6233  | eval_custom_logloss: 0.57138 |  0:00:11s
epoch 23 | loss: 0.62039 | eval_custom_logloss: 0.60999 |  0:00:12s
epoch 24 | loss: 0.57089 | eval_custom_logloss: 0.59966 |  0:00:12s
epoch 25 | loss: 0.55584 | eval_custom_logloss: 0.59596 |  0:00:13s
epoch 26 | loss: 0.56375 | eval_custom_logloss: 0.59907 |  0:00:13s
epoch 27 | loss: 0.60945 | eval_custom_logloss: 0.71335 |  0:00:14s
epoch 28 | loss: 0.53185 | eval_custom_logloss: 0.58411 |  0:00:14s
epoch 29 | loss: 0.50779 | eval_custom_logloss: 0.57681 |  0:00:15s
epoch 30 | loss: 0.53957 | eval_custom_logloss: 0.52654 |  0:00:15s
epoch 31 | loss: 0.51344 | eval_custom_logloss: 0.58163 |  0:00:16s
epoch 32 | loss: 0.53809 | eval_custom_logloss: 0.71477 |  0:00:16s
epoch 33 | loss: 0.54109 | eval_custom_logloss: 0.57899 |  0:00:17s
epoch 34 | loss: 0.50799 | eval_custom_logloss: 0.54295 |  0:00:17s
epoch 35 | loss: 0.52873 | eval_custom_logloss: 0.53699 |  0:00:18s
epoch 36 | loss: 0.52949 | eval_custom_logloss: 0.54037 |  0:00:18s
epoch 37 | loss: 0.51527 | eval_custom_logloss: 0.53792 |  0:00:19s
epoch 38 | loss: 0.50756 | eval_custom_logloss: 0.51561 |  0:00:19s
epoch 39 | loss: 0.55313 | eval_custom_logloss: 0.70147 |  0:00:20s
epoch 40 | loss: 0.52682 | eval_custom_logloss: 0.59275 |  0:00:20s
epoch 41 | loss: 0.54068 | eval_custom_logloss: 0.53879 |  0:00:21s
epoch 42 | loss: 0.54597 | eval_custom_logloss: 0.52257 |  0:00:21s
epoch 43 | loss: 0.51526 | eval_custom_logloss: 0.50149 |  0:00:22s
epoch 44 | loss: 0.49435 | eval_custom_logloss: 0.53451 |  0:00:22s
epoch 45 | loss: 0.51195 | eval_custom_logloss: 0.53332 |  0:00:23s
epoch 46 | loss: 0.50093 | eval_custom_logloss: 0.54995 |  0:00:23s
epoch 47 | loss: 0.53495 | eval_custom_logloss: 0.5348  |  0:00:24s
epoch 48 | loss: 0.54833 | eval_custom_logloss: 0.57505 |  0:00:24s
epoch 49 | loss: 0.53926 | eval_custom_logloss: 0.53254 |  0:00:25s
epoch 50 | loss: 0.53198 | eval_custom_logloss: 0.52344 |  0:00:25s
epoch 51 | loss: 0.49653 | eval_custom_logloss: 0.54423 |  0:00:26s
epoch 52 | loss: 0.51057 | eval_custom_logloss: 0.48615 |  0:00:26s
epoch 53 | loss: 0.47084 | eval_custom_logloss: 0.4924  |  0:00:27s
epoch 54 | loss: 0.46891 | eval_custom_logloss: 0.47794 |  0:00:27s
epoch 55 | loss: 0.46674 | eval_custom_logloss: 0.47091 |  0:00:28s
epoch 56 | loss: 0.48791 | eval_custom_logloss: 0.47423 |  0:00:28s
epoch 57 | loss: 0.47772 | eval_custom_logloss: 0.50985 |  0:00:29s
epoch 58 | loss: 0.44636 | eval_custom_logloss: 0.45646 |  0:00:29s
epoch 59 | loss: 0.42952 | eval_custom_logloss: 0.43706 |  0:00:30s
epoch 60 | loss: 0.43657 | eval_custom_logloss: 0.45548 |  0:00:30s
epoch 61 | loss: 0.42229 | eval_custom_logloss: 0.50353 |  0:00:31s
epoch 62 | loss: 0.40767 | eval_custom_logloss: 0.44291 |  0:00:31s
epoch 63 | loss: 0.39386 | eval_custom_logloss: 0.46956 |  0:00:32s
epoch 64 | loss: 0.41099 | eval_custom_logloss: 0.46732 |  0:00:32s
epoch 65 | loss: 0.38319 | eval_custom_logloss: 0.46909 |  0:00:33s
epoch 66 | loss: 0.35908 | eval_custom_logloss: 0.46376 |  0:00:33s
epoch 67 | loss: 0.3894  | eval_custom_logloss: 0.45376 |  0:00:34s
epoch 68 | loss: 0.37381 | eval_custom_logloss: 0.42817 |  0:00:34s
epoch 69 | loss: 0.36769 | eval_custom_logloss: 0.46213 |  0:00:35s
epoch 70 | loss: 0.36191 | eval_custom_logloss: 0.45277 |  0:00:35s
epoch 71 | loss: 0.35383 | eval_custom_logloss: 0.41678 |  0:00:36s
epoch 72 | loss: 0.33766 | eval_custom_logloss: 0.43486 |  0:00:36s
epoch 73 | loss: 0.33563 | eval_custom_logloss: 0.41942 |  0:00:37s
epoch 74 | loss: 0.3253  | eval_custom_logloss: 0.40368 |  0:00:37s
epoch 75 | loss: 0.3328  | eval_custom_logloss: 0.40824 |  0:00:38s
epoch 76 | loss: 0.30727 | eval_custom_logloss: 0.39619 |  0:00:38s
epoch 77 | loss: 0.30504 | eval_custom_logloss: 0.43726 |  0:00:39s
epoch 78 | loss: 0.28978 | eval_custom_logloss: 0.44074 |  0:00:39s
epoch 79 | loss: 0.31046 | eval_custom_logloss: 0.41025 |  0:00:40s
epoch 80 | loss: 0.32759 | eval_custom_logloss: 0.41607 |  0:00:40s
epoch 81 | loss: 0.3206  | eval_custom_logloss: 0.39934 |  0:00:41s
epoch 82 | loss: 0.29007 | eval_custom_logloss: 0.4144  |  0:00:41s
epoch 83 | loss: 0.27736 | eval_custom_logloss: 0.42043 |  0:00:42s
epoch 84 | loss: 0.29519 | eval_custom_logloss: 0.42768 |  0:00:42s
epoch 85 | loss: 0.33279 | eval_custom_logloss: 0.46951 |  0:00:43s
epoch 86 | loss: 0.34527 | eval_custom_logloss: 0.46989 |  0:00:43s
epoch 87 | loss: 0.35567 | eval_custom_logloss: 0.5249  |  0:00:44s
epoch 88 | loss: 0.34693 | eval_custom_logloss: 0.52564 |  0:00:44s
epoch 89 | loss: 0.31524 | eval_custom_logloss: 0.462   |  0:00:45s
epoch 90 | loss: 0.2819  | eval_custom_logloss: 0.51846 |  0:00:45s
epoch 91 | loss: 0.29782 | eval_custom_logloss: 0.5074  |  0:00:46s
epoch 92 | loss: 0.30041 | eval_custom_logloss: 0.45228 |  0:00:46s
epoch 93 | loss: 0.30392 | eval_custom_logloss: 0.4702  |  0:00:47s
epoch 94 | loss: 0.26268 | eval_custom_logloss: 0.4603  |  0:00:47s
epoch 95 | loss: 0.30296 | eval_custom_logloss: 0.50954 |  0:00:48s
epoch 96 | loss: 0.343   | eval_custom_logloss: 0.4817  |  0:00:48s

Early stopping occurred at epoch 96 with best_epoch = 76 and best_eval_custom_logloss = 0.39619
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.3962, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax', 'n_a': 63, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.20854 | eval_custom_logloss: 2.87622 |  0:00:00s
epoch 1  | loss: 1.19528 | eval_custom_logloss: 1.60171 |  0:00:01s
epoch 2  | loss: 1.29601 | eval_custom_logloss: 2.90308 |  0:00:01s
epoch 3  | loss: 1.18683 | eval_custom_logloss: 1.50124 |  0:00:02s
epoch 4  | loss: 1.09577 | eval_custom_logloss: 2.03489 |  0:00:02s
epoch 5  | loss: 0.98125 | eval_custom_logloss: 1.63333 |  0:00:03s
epoch 6  | loss: 0.90685 | eval_custom_logloss: 1.73173 |  0:00:03s
epoch 7  | loss: 0.92791 | eval_custom_logloss: 1.37965 |  0:00:04s
epoch 8  | loss: 0.76888 | eval_custom_logloss: 1.28528 |  0:00:04s
epoch 9  | loss: 0.71024 | eval_custom_logloss: 1.46629 |  0:00:05s
epoch 10 | loss: 0.66659 | eval_custom_logloss: 1.23417 |  0:00:05s
epoch 11 | loss: 0.6368  | eval_custom_logloss: 0.92239 |  0:00:06s
epoch 12 | loss: 0.61993 | eval_custom_logloss: 0.75283 |  0:00:06s
epoch 13 | loss: 0.5857  | eval_custom_logloss: 0.70349 |  0:00:07s
epoch 14 | loss: 0.57232 | eval_custom_logloss: 0.77707 |  0:00:07s
epoch 15 | loss: 0.56403 | eval_custom_logloss: 0.74264 |  0:00:08s
epoch 16 | loss: 0.61549 | eval_custom_logloss: 0.7925  |  0:00:08s
epoch 17 | loss: 0.55522 | eval_custom_logloss: 0.76609 |  0:00:09s
epoch 18 | loss: 0.56773 | eval_custom_logloss: 0.94679 |  0:00:09s
epoch 19 | loss: 0.57133 | eval_custom_logloss: 0.90152 |  0:00:10s
epoch 20 | loss: 0.54034 | eval_custom_logloss: 0.77131 |  0:00:10s
epoch 21 | loss: 0.55803 | eval_custom_logloss: 0.73961 |  0:00:11s
epoch 22 | loss: 0.54757 | eval_custom_logloss: 0.69188 |  0:00:11s
epoch 23 | loss: 0.55795 | eval_custom_logloss: 0.84569 |  0:00:12s
epoch 24 | loss: 0.54577 | eval_custom_logloss: 0.87612 |  0:00:12s
epoch 25 | loss: 0.51323 | eval_custom_logloss: 0.83298 |  0:00:13s
epoch 26 | loss: 0.48721 | eval_custom_logloss: 0.69595 |  0:00:13s
epoch 27 | loss: 0.49082 | eval_custom_logloss: 0.68137 |  0:00:14s
epoch 28 | loss: 0.50012 | eval_custom_logloss: 0.79422 |  0:00:14s
epoch 29 | loss: 0.45538 | eval_custom_logloss: 0.74684 |  0:00:15s
epoch 30 | loss: 0.46876 | eval_custom_logloss: 0.75763 |  0:00:15s
epoch 31 | loss: 0.4345  | eval_custom_logloss: 0.69222 |  0:00:16s
epoch 32 | loss: 0.42203 | eval_custom_logloss: 0.8271  |  0:00:16s
epoch 33 | loss: 0.44907 | eval_custom_logloss: 0.68248 |  0:00:17s
epoch 34 | loss: 0.4634  | eval_custom_logloss: 0.61607 |  0:00:17s
epoch 35 | loss: 0.42038 | eval_custom_logloss: 0.63601 |  0:00:18s
epoch 36 | loss: 0.41311 | eval_custom_logloss: 0.68062 |  0:00:18s
epoch 37 | loss: 0.40242 | eval_custom_logloss: 0.57262 |  0:00:18s
epoch 38 | loss: 0.41106 | eval_custom_logloss: 0.54539 |  0:00:19s
epoch 39 | loss: 0.38054 | eval_custom_logloss: 0.53154 |  0:00:19s
epoch 40 | loss: 0.37622 | eval_custom_logloss: 0.64412 |  0:00:20s
epoch 41 | loss: 0.37358 | eval_custom_logloss: 0.53467 |  0:00:20s
epoch 42 | loss: 0.36827 | eval_custom_logloss: 0.5915  |  0:00:21s
epoch 43 | loss: 0.33915 | eval_custom_logloss: 0.62327 |  0:00:21s
epoch 44 | loss: 0.37376 | eval_custom_logloss: 0.6784  |  0:00:22s
epoch 45 | loss: 0.3446  | eval_custom_logloss: 0.62383 |  0:00:22s
epoch 46 | loss: 0.34258 | eval_custom_logloss: 0.66647 |  0:00:23s
epoch 47 | loss: 0.33047 | eval_custom_logloss: 0.58978 |  0:00:23s
epoch 48 | loss: 0.32506 | eval_custom_logloss: 0.58221 |  0:00:24s
epoch 49 | loss: 0.3264  | eval_custom_logloss: 0.58673 |  0:00:24s
epoch 50 | loss: 0.3138  | eval_custom_logloss: 0.57706 |  0:00:25s
epoch 51 | loss: 0.3021  | eval_custom_logloss: 0.61769 |  0:00:25s
epoch 52 | loss: 0.29881 | eval_custom_logloss: 0.61149 |  0:00:26s
epoch 53 | loss: 0.2722  | eval_custom_logloss: 0.57486 |  0:00:26s
epoch 54 | loss: 0.28309 | eval_custom_logloss: 0.64599 |  0:00:27s
epoch 55 | loss: 0.26029 | eval_custom_logloss: 0.63323 |  0:00:27s
epoch 56 | loss: 0.27021 | eval_custom_logloss: 0.60273 |  0:00:28s
epoch 57 | loss: 0.31544 | eval_custom_logloss: 0.66915 |  0:00:28s
epoch 58 | loss: 0.29264 | eval_custom_logloss: 0.54309 |  0:00:29s
epoch 59 | loss: 0.31584 | eval_custom_logloss: 0.58428 |  0:00:29s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.53154
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.46385, 'Log Loss - std': 0.06764999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax', 'n_a': 63, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.1856  | eval_custom_logloss: 3.70926 |  0:00:00s
epoch 1  | loss: 1.61363 | eval_custom_logloss: 1.8258  |  0:00:00s
epoch 2  | loss: 1.35035 | eval_custom_logloss: 1.28913 |  0:00:01s
epoch 3  | loss: 1.11819 | eval_custom_logloss: 1.85861 |  0:00:01s
epoch 4  | loss: 0.96423 | eval_custom_logloss: 1.23226 |  0:00:02s
epoch 5  | loss: 1.04109 | eval_custom_logloss: 1.36524 |  0:00:02s
epoch 6  | loss: 0.83351 | eval_custom_logloss: 1.50743 |  0:00:03s
epoch 7  | loss: 0.85608 | eval_custom_logloss: 1.16509 |  0:00:03s
epoch 8  | loss: 0.77906 | eval_custom_logloss: 0.96524 |  0:00:04s
epoch 9  | loss: 0.71947 | eval_custom_logloss: 0.99558 |  0:00:05s
epoch 10 | loss: 0.7481  | eval_custom_logloss: 0.85262 |  0:00:05s
epoch 11 | loss: 0.6672  | eval_custom_logloss: 0.84547 |  0:00:06s
epoch 12 | loss: 0.66323 | eval_custom_logloss: 0.8444  |  0:00:06s
epoch 13 | loss: 0.65903 | eval_custom_logloss: 0.76498 |  0:00:07s
epoch 14 | loss: 0.59234 | eval_custom_logloss: 0.69723 |  0:00:07s
epoch 15 | loss: 0.58231 | eval_custom_logloss: 0.59982 |  0:00:08s
epoch 16 | loss: 0.60843 | eval_custom_logloss: 0.7228  |  0:00:08s
epoch 17 | loss: 0.57469 | eval_custom_logloss: 0.67862 |  0:00:09s
epoch 18 | loss: 0.59378 | eval_custom_logloss: 0.76386 |  0:00:09s
epoch 19 | loss: 0.65386 | eval_custom_logloss: 0.76193 |  0:00:10s
epoch 20 | loss: 0.57728 | eval_custom_logloss: 0.66949 |  0:00:10s
epoch 21 | loss: 0.57236 | eval_custom_logloss: 0.81983 |  0:00:11s
epoch 22 | loss: 0.61839 | eval_custom_logloss: 0.90228 |  0:00:11s
epoch 23 | loss: 0.62432 | eval_custom_logloss: 0.87141 |  0:00:12s
epoch 24 | loss: 0.56612 | eval_custom_logloss: 0.7174  |  0:00:12s
epoch 25 | loss: 0.55562 | eval_custom_logloss: 0.74765 |  0:00:13s
epoch 26 | loss: 0.54542 | eval_custom_logloss: 0.5826  |  0:00:13s
epoch 27 | loss: 0.53212 | eval_custom_logloss: 0.57917 |  0:00:14s
epoch 28 | loss: 0.5429  | eval_custom_logloss: 0.58931 |  0:00:14s
epoch 29 | loss: 0.53146 | eval_custom_logloss: 0.56091 |  0:00:15s
epoch 30 | loss: 0.50945 | eval_custom_logloss: 0.51517 |  0:00:15s
epoch 31 | loss: 0.4866  | eval_custom_logloss: 0.56356 |  0:00:16s
epoch 32 | loss: 0.48788 | eval_custom_logloss: 0.55821 |  0:00:16s
epoch 33 | loss: 0.48932 | eval_custom_logloss: 0.6221  |  0:00:17s
epoch 34 | loss: 0.45911 | eval_custom_logloss: 0.67167 |  0:00:17s
epoch 35 | loss: 0.48084 | eval_custom_logloss: 0.63299 |  0:00:18s
epoch 36 | loss: 0.49868 | eval_custom_logloss: 0.70411 |  0:00:18s
epoch 37 | loss: 0.48923 | eval_custom_logloss: 0.63321 |  0:00:19s
epoch 38 | loss: 0.47018 | eval_custom_logloss: 0.51591 |  0:00:19s
epoch 39 | loss: 0.45833 | eval_custom_logloss: 0.47286 |  0:00:20s
epoch 40 | loss: 0.46293 | eval_custom_logloss: 0.56546 |  0:00:20s
epoch 41 | loss: 0.46224 | eval_custom_logloss: 0.66309 |  0:00:21s
epoch 42 | loss: 0.47119 | eval_custom_logloss: 0.63602 |  0:00:21s
epoch 43 | loss: 0.489   | eval_custom_logloss: 0.62997 |  0:00:22s
epoch 44 | loss: 0.45642 | eval_custom_logloss: 0.54355 |  0:00:22s
epoch 45 | loss: 0.47975 | eval_custom_logloss: 0.49964 |  0:00:23s
epoch 46 | loss: 0.44155 | eval_custom_logloss: 0.54948 |  0:00:23s
epoch 47 | loss: 0.51989 | eval_custom_logloss: 0.57773 |  0:00:24s
epoch 48 | loss: 0.49559 | eval_custom_logloss: 0.54086 |  0:00:24s
epoch 49 | loss: 0.46476 | eval_custom_logloss: 0.64629 |  0:00:25s
epoch 50 | loss: 0.48005 | eval_custom_logloss: 0.63328 |  0:00:25s
epoch 51 | loss: 0.46496 | eval_custom_logloss: 0.63774 |  0:00:26s
epoch 52 | loss: 0.49267 | eval_custom_logloss: 0.57927 |  0:00:26s
epoch 53 | loss: 0.48239 | eval_custom_logloss: 0.52635 |  0:00:27s
epoch 54 | loss: 0.46526 | eval_custom_logloss: 0.46586 |  0:00:27s
epoch 55 | loss: 0.42814 | eval_custom_logloss: 0.50421 |  0:00:28s
epoch 56 | loss: 0.42581 | eval_custom_logloss: 0.49249 |  0:00:28s
epoch 57 | loss: 0.41228 | eval_custom_logloss: 0.41616 |  0:00:29s
epoch 58 | loss: 0.40298 | eval_custom_logloss: 0.47825 |  0:00:29s
epoch 59 | loss: 0.36972 | eval_custom_logloss: 0.4829  |  0:00:30s
epoch 60 | loss: 0.38194 | eval_custom_logloss: 0.49784 |  0:00:30s
epoch 61 | loss: 0.40064 | eval_custom_logloss: 0.51736 |  0:00:31s
epoch 62 | loss: 0.38097 | eval_custom_logloss: 0.55699 |  0:00:31s
epoch 63 | loss: 0.38354 | eval_custom_logloss: 0.57488 |  0:00:32s
epoch 64 | loss: 0.38187 | eval_custom_logloss: 0.51299 |  0:00:32s
epoch 65 | loss: 0.37691 | eval_custom_logloss: 0.40911 |  0:00:33s
epoch 66 | loss: 0.37086 | eval_custom_logloss: 0.52152 |  0:00:33s
epoch 67 | loss: 0.34186 | eval_custom_logloss: 0.64849 |  0:00:34s
epoch 68 | loss: 0.32838 | eval_custom_logloss: 0.52304 |  0:00:34s
epoch 69 | loss: 0.33745 | eval_custom_logloss: 0.46659 |  0:00:35s
epoch 70 | loss: 0.3517  | eval_custom_logloss: 0.45824 |  0:00:35s
epoch 71 | loss: 0.35486 | eval_custom_logloss: 0.52455 |  0:00:36s
epoch 72 | loss: 0.32619 | eval_custom_logloss: 0.49702 |  0:00:36s
epoch 73 | loss: 0.33521 | eval_custom_logloss: 0.45104 |  0:00:37s
epoch 74 | loss: 0.3153  | eval_custom_logloss: 0.48166 |  0:00:37s
epoch 75 | loss: 0.342   | eval_custom_logloss: 0.59085 |  0:00:38s
epoch 76 | loss: 0.37084 | eval_custom_logloss: 0.47849 |  0:00:38s
epoch 77 | loss: 0.41627 | eval_custom_logloss: 0.47831 |  0:00:39s
epoch 78 | loss: 0.36617 | eval_custom_logloss: 0.51108 |  0:00:39s
epoch 79 | loss: 0.36793 | eval_custom_logloss: 0.57904 |  0:00:40s
epoch 80 | loss: 0.33641 | eval_custom_logloss: 0.42948 |  0:00:40s
epoch 81 | loss: 0.34621 | eval_custom_logloss: 0.55791 |  0:00:41s
epoch 82 | loss: 0.32967 | eval_custom_logloss: 0.43809 |  0:00:41s
epoch 83 | loss: 0.31415 | eval_custom_logloss: 0.40323 |  0:00:42s
epoch 84 | loss: 0.32579 | eval_custom_logloss: 0.41129 |  0:00:42s
epoch 85 | loss: 0.2784  | eval_custom_logloss: 0.39436 |  0:00:43s
epoch 86 | loss: 0.2996  | eval_custom_logloss: 0.46472 |  0:00:43s
epoch 87 | loss: 0.2702  | eval_custom_logloss: 0.42455 |  0:00:44s
epoch 88 | loss: 0.25713 | eval_custom_logloss: 0.42913 |  0:00:44s
epoch 89 | loss: 0.27456 | eval_custom_logloss: 0.50136 |  0:00:45s
epoch 90 | loss: 0.30346 | eval_custom_logloss: 0.44949 |  0:00:45s
epoch 91 | loss: 0.28992 | eval_custom_logloss: 0.40797 |  0:00:46s
epoch 92 | loss: 0.26924 | eval_custom_logloss: 0.4473  |  0:00:46s
epoch 93 | loss: 0.28982 | eval_custom_logloss: 0.46855 |  0:00:47s
epoch 94 | loss: 0.26528 | eval_custom_logloss: 0.43047 |  0:00:47s
epoch 95 | loss: 0.26713 | eval_custom_logloss: 0.43566 |  0:00:48s
epoch 96 | loss: 0.24485 | eval_custom_logloss: 0.41514 |  0:00:48s
epoch 97 | loss: 0.23501 | eval_custom_logloss: 0.35805 |  0:00:49s
epoch 98 | loss: 0.25714 | eval_custom_logloss: 0.39154 |  0:00:49s
epoch 99 | loss: 0.23248 | eval_custom_logloss: 0.41939 |  0:00:50s
Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_eval_custom_logloss = 0.35805
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.42860000000000004, 'Log Loss - std': 0.07440524175083366} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax', 'n_a': 63, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.0936  | eval_custom_logloss: 3.40914 |  0:00:00s
epoch 1  | loss: 1.28083 | eval_custom_logloss: 2.21721 |  0:00:01s
epoch 2  | loss: 1.4312  | eval_custom_logloss: 1.46687 |  0:00:01s
epoch 3  | loss: 1.20367 | eval_custom_logloss: 1.39333 |  0:00:02s
epoch 4  | loss: 1.06509 | eval_custom_logloss: 1.45006 |  0:00:02s
epoch 5  | loss: 0.84431 | eval_custom_logloss: 1.10151 |  0:00:03s
epoch 6  | loss: 0.8516  | eval_custom_logloss: 1.01163 |  0:00:03s
epoch 7  | loss: 0.85604 | eval_custom_logloss: 1.16585 |  0:00:04s
epoch 8  | loss: 0.85096 | eval_custom_logloss: 1.23504 |  0:00:04s
epoch 9  | loss: 0.8591  | eval_custom_logloss: 1.46085 |  0:00:05s
epoch 10 | loss: 0.76201 | eval_custom_logloss: 1.04981 |  0:00:05s
epoch 11 | loss: 0.72767 | eval_custom_logloss: 1.07517 |  0:00:06s
epoch 12 | loss: 0.70732 | eval_custom_logloss: 0.86982 |  0:00:06s
epoch 13 | loss: 0.60821 | eval_custom_logloss: 0.75558 |  0:00:07s
epoch 14 | loss: 0.56568 | eval_custom_logloss: 0.86905 |  0:00:07s
epoch 15 | loss: 0.54779 | eval_custom_logloss: 0.77168 |  0:00:08s
epoch 16 | loss: 0.57175 | eval_custom_logloss: 0.76636 |  0:00:08s
epoch 17 | loss: 0.52657 | eval_custom_logloss: 0.76968 |  0:00:09s
epoch 18 | loss: 0.52022 | eval_custom_logloss: 0.79946 |  0:00:09s
epoch 19 | loss: 0.55607 | eval_custom_logloss: 0.73055 |  0:00:10s
epoch 20 | loss: 0.51474 | eval_custom_logloss: 0.64824 |  0:00:10s
epoch 21 | loss: 0.49813 | eval_custom_logloss: 0.69479 |  0:00:11s
epoch 22 | loss: 0.49616 | eval_custom_logloss: 0.7336  |  0:00:11s
epoch 23 | loss: 0.50112 | eval_custom_logloss: 0.76384 |  0:00:12s
epoch 24 | loss: 0.51916 | eval_custom_logloss: 0.67564 |  0:00:12s
epoch 25 | loss: 0.53607 | eval_custom_logloss: 0.62383 |  0:00:13s
epoch 26 | loss: 0.5468  | eval_custom_logloss: 0.81844 |  0:00:13s
epoch 27 | loss: 0.53385 | eval_custom_logloss: 0.64958 |  0:00:14s
epoch 28 | loss: 0.51958 | eval_custom_logloss: 0.73681 |  0:00:14s
epoch 29 | loss: 0.51857 | eval_custom_logloss: 0.67676 |  0:00:14s
epoch 30 | loss: 0.48818 | eval_custom_logloss: 0.62038 |  0:00:15s
epoch 31 | loss: 0.46971 | eval_custom_logloss: 0.65887 |  0:00:15s
epoch 32 | loss: 0.46945 | eval_custom_logloss: 0.54954 |  0:00:16s
epoch 33 | loss: 0.46037 | eval_custom_logloss: 0.6965  |  0:00:16s
epoch 34 | loss: 0.47313 | eval_custom_logloss: 0.53558 |  0:00:17s
epoch 35 | loss: 0.44773 | eval_custom_logloss: 0.62288 |  0:00:17s
epoch 36 | loss: 0.41381 | eval_custom_logloss: 0.5681  |  0:00:18s
epoch 37 | loss: 0.41645 | eval_custom_logloss: 0.52743 |  0:00:18s
epoch 38 | loss: 0.40787 | eval_custom_logloss: 0.51107 |  0:00:19s
epoch 39 | loss: 0.37184 | eval_custom_logloss: 0.61185 |  0:00:19s
epoch 40 | loss: 0.35704 | eval_custom_logloss: 0.52807 |  0:00:20s
epoch 41 | loss: 0.36612 | eval_custom_logloss: 0.55287 |  0:00:20s
epoch 42 | loss: 0.35161 | eval_custom_logloss: 0.5425  |  0:00:21s
epoch 43 | loss: 0.32141 | eval_custom_logloss: 0.53095 |  0:00:21s
epoch 44 | loss: 0.33286 | eval_custom_logloss: 0.54911 |  0:00:22s
epoch 45 | loss: 0.33447 | eval_custom_logloss: 0.55978 |  0:00:22s
epoch 46 | loss: 0.31353 | eval_custom_logloss: 0.53059 |  0:00:23s
epoch 47 | loss: 0.3091  | eval_custom_logloss: 0.59125 |  0:00:23s
epoch 48 | loss: 0.31873 | eval_custom_logloss: 0.64978 |  0:00:24s
epoch 49 | loss: 0.3306  | eval_custom_logloss: 0.59643 |  0:00:24s
epoch 50 | loss: 0.30654 | eval_custom_logloss: 0.5136  |  0:00:25s
epoch 51 | loss: 0.25672 | eval_custom_logloss: 0.56205 |  0:00:25s
epoch 52 | loss: 0.28501 | eval_custom_logloss: 0.5166  |  0:00:26s
epoch 53 | loss: 0.27237 | eval_custom_logloss: 0.55189 |  0:00:26s
epoch 54 | loss: 0.29509 | eval_custom_logloss: 0.63798 |  0:00:27s
epoch 55 | loss: 0.27994 | eval_custom_logloss: 0.53411 |  0:00:27s
epoch 56 | loss: 0.25894 | eval_custom_logloss: 0.65962 |  0:00:28s
epoch 57 | loss: 0.28642 | eval_custom_logloss: 0.53345 |  0:00:28s
epoch 58 | loss: 0.29458 | eval_custom_logloss: 0.71217 |  0:00:29s

Early stopping occurred at epoch 58 with best_epoch = 38 and best_eval_custom_logloss = 0.51107
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.449225, 'Log Loss - std': 0.07367684083210951} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax', 'n_a': 63, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.32203 | eval_custom_logloss: 2.95709 |  0:00:00s
epoch 1  | loss: 1.52236 | eval_custom_logloss: 2.96572 |  0:00:01s
epoch 2  | loss: 1.63389 | eval_custom_logloss: 1.97517 |  0:00:01s
epoch 3  | loss: 1.17927 | eval_custom_logloss: 2.25388 |  0:00:02s
epoch 4  | loss: 1.14783 | eval_custom_logloss: 1.63516 |  0:00:02s
epoch 5  | loss: 1.02037 | eval_custom_logloss: 1.66184 |  0:00:03s
epoch 6  | loss: 1.36026 | eval_custom_logloss: 1.45336 |  0:00:03s
epoch 7  | loss: 0.83629 | eval_custom_logloss: 0.96939 |  0:00:03s
epoch 8  | loss: 0.78145 | eval_custom_logloss: 1.01882 |  0:00:04s
epoch 9  | loss: 0.78104 | eval_custom_logloss: 0.84818 |  0:00:04s
epoch 10 | loss: 0.75133 | eval_custom_logloss: 0.86544 |  0:00:05s
epoch 11 | loss: 0.70385 | eval_custom_logloss: 0.75348 |  0:00:05s
epoch 12 | loss: 0.67883 | eval_custom_logloss: 0.68378 |  0:00:06s
epoch 13 | loss: 0.64955 | eval_custom_logloss: 0.64825 |  0:00:06s
epoch 14 | loss: 0.64063 | eval_custom_logloss: 0.6567  |  0:00:07s
epoch 15 | loss: 0.6134  | eval_custom_logloss: 0.71376 |  0:00:07s
epoch 16 | loss: 0.6451  | eval_custom_logloss: 0.67222 |  0:00:08s
epoch 17 | loss: 0.60563 | eval_custom_logloss: 0.61347 |  0:00:08s
epoch 18 | loss: 0.62093 | eval_custom_logloss: 0.62424 |  0:00:09s
epoch 19 | loss: 0.65068 | eval_custom_logloss: 0.68742 |  0:00:09s
epoch 20 | loss: 0.61937 | eval_custom_logloss: 0.62192 |  0:00:10s
epoch 21 | loss: 0.63334 | eval_custom_logloss: 0.56545 |  0:00:10s
epoch 22 | loss: 0.64036 | eval_custom_logloss: 0.59685 |  0:00:11s
epoch 23 | loss: 0.62295 | eval_custom_logloss: 0.56643 |  0:00:11s
epoch 24 | loss: 0.5848  | eval_custom_logloss: 0.67347 |  0:00:12s
epoch 25 | loss: 0.56281 | eval_custom_logloss: 0.5478  |  0:00:12s
epoch 26 | loss: 0.58088 | eval_custom_logloss: 0.55303 |  0:00:13s
epoch 27 | loss: 0.55696 | eval_custom_logloss: 0.53768 |  0:00:13s
epoch 28 | loss: 0.54249 | eval_custom_logloss: 0.56475 |  0:00:14s
epoch 29 | loss: 0.52564 | eval_custom_logloss: 0.51974 |  0:00:14s
epoch 30 | loss: 0.5545  | eval_custom_logloss: 0.52742 |  0:00:15s
epoch 31 | loss: 0.51823 | eval_custom_logloss: 0.47205 |  0:00:15s
epoch 32 | loss: 0.48197 | eval_custom_logloss: 0.50841 |  0:00:16s
epoch 33 | loss: 0.4841  | eval_custom_logloss: 0.44939 |  0:00:16s
epoch 34 | loss: 0.48236 | eval_custom_logloss: 0.48095 |  0:00:17s
epoch 35 | loss: 0.49956 | eval_custom_logloss: 0.47496 |  0:00:17s
epoch 36 | loss: 0.53559 | eval_custom_logloss: 0.55668 |  0:00:18s
epoch 37 | loss: 0.50551 | eval_custom_logloss: 0.48857 |  0:00:18s
epoch 38 | loss: 0.50033 | eval_custom_logloss: 0.57289 |  0:00:19s
epoch 39 | loss: 0.50602 | eval_custom_logloss: 0.53532 |  0:00:19s
epoch 40 | loss: 0.48286 | eval_custom_logloss: 0.63024 |  0:00:20s
epoch 41 | loss: 0.48429 | eval_custom_logloss: 0.5011  |  0:00:20s
epoch 42 | loss: 0.48202 | eval_custom_logloss: 0.4904  |  0:00:21s
epoch 43 | loss: 0.48035 | eval_custom_logloss: 0.5067  |  0:00:22s
epoch 44 | loss: 0.44579 | eval_custom_logloss: 0.50303 |  0:00:22s
epoch 45 | loss: 0.5303  | eval_custom_logloss: 0.52117 |  0:00:23s
epoch 46 | loss: 0.48525 | eval_custom_logloss: 0.50458 |  0:00:23s
epoch 47 | loss: 0.44843 | eval_custom_logloss: 0.55361 |  0:00:24s
epoch 48 | loss: 0.4422  | eval_custom_logloss: 0.5634  |  0:00:24s
epoch 49 | loss: 0.45033 | eval_custom_logloss: 0.50722 |  0:00:25s
epoch 50 | loss: 0.43282 | eval_custom_logloss: 0.53403 |  0:00:25s
epoch 51 | loss: 0.42329 | eval_custom_logloss: 0.44146 |  0:00:25s
epoch 52 | loss: 0.38788 | eval_custom_logloss: 0.492   |  0:00:26s
epoch 53 | loss: 0.41006 | eval_custom_logloss: 0.36828 |  0:00:27s
epoch 54 | loss: 0.42315 | eval_custom_logloss: 0.43701 |  0:00:27s
epoch 55 | loss: 0.38797 | eval_custom_logloss: 0.40506 |  0:00:28s
epoch 56 | loss: 0.35645 | eval_custom_logloss: 0.35642 |  0:00:28s
epoch 57 | loss: 0.36584 | eval_custom_logloss: 0.3988  |  0:00:29s
epoch 58 | loss: 0.32186 | eval_custom_logloss: 0.39924 |  0:00:29s
epoch 59 | loss: 0.34665 | eval_custom_logloss: 0.41363 |  0:00:30s
epoch 60 | loss: 0.35747 | eval_custom_logloss: 0.35081 |  0:00:30s
epoch 61 | loss: 0.34505 | eval_custom_logloss: 0.38645 |  0:00:31s
epoch 62 | loss: 0.33636 | eval_custom_logloss: 0.38695 |  0:00:31s
epoch 63 | loss: 0.29387 | eval_custom_logloss: 0.44182 |  0:00:32s
epoch 64 | loss: 0.3002  | eval_custom_logloss: 0.37882 |  0:00:32s
epoch 65 | loss: 0.29669 | eval_custom_logloss: 0.35545 |  0:00:33s
epoch 66 | loss: 0.32335 | eval_custom_logloss: 0.43273 |  0:00:33s
epoch 67 | loss: 0.29871 | eval_custom_logloss: 0.3427  |  0:00:34s
epoch 68 | loss: 0.27389 | eval_custom_logloss: 0.4329  |  0:00:34s
epoch 69 | loss: 0.30768 | eval_custom_logloss: 0.34654 |  0:00:35s
epoch 70 | loss: 0.28242 | eval_custom_logloss: 0.41455 |  0:00:35s
epoch 71 | loss: 0.32781 | eval_custom_logloss: 0.35194 |  0:00:36s
epoch 72 | loss: 0.30431 | eval_custom_logloss: 0.3613  |  0:00:36s
epoch 73 | loss: 0.28998 | eval_custom_logloss: 0.37049 |  0:00:37s
epoch 74 | loss: 0.27713 | eval_custom_logloss: 0.33404 |  0:00:37s
epoch 75 | loss: 0.27945 | eval_custom_logloss: 0.33995 |  0:00:38s
epoch 76 | loss: 0.26658 | eval_custom_logloss: 0.29769 |  0:00:38s
epoch 77 | loss: 0.26831 | eval_custom_logloss: 0.33096 |  0:00:39s
epoch 78 | loss: 0.25299 | eval_custom_logloss: 0.3845  |  0:00:39s
epoch 79 | loss: 0.26448 | eval_custom_logloss: 0.34568 |  0:00:40s
epoch 80 | loss: 0.26128 | eval_custom_logloss: 0.27695 |  0:00:40s
epoch 81 | loss: 0.2546  | eval_custom_logloss: 0.28574 |  0:00:41s
epoch 82 | loss: 0.25918 | eval_custom_logloss: 0.34042 |  0:00:41s
epoch 83 | loss: 0.25417 | eval_custom_logloss: 0.33257 |  0:00:42s
epoch 84 | loss: 0.2567  | eval_custom_logloss: 0.35686 |  0:00:42s
epoch 85 | loss: 0.27599 | eval_custom_logloss: 0.50678 |  0:00:43s
epoch 86 | loss: 0.27186 | eval_custom_logloss: 0.47783 |  0:00:43s
epoch 87 | loss: 0.26145 | eval_custom_logloss: 0.43017 |  0:00:44s
epoch 88 | loss: 0.22703 | eval_custom_logloss: 0.32872 |  0:00:44s
epoch 89 | loss: 0.24061 | eval_custom_logloss: 0.38872 |  0:00:45s
epoch 90 | loss: 0.27227 | eval_custom_logloss: 0.3871  |  0:00:45s
epoch 91 | loss: 0.29074 | eval_custom_logloss: 0.32717 |  0:00:46s
epoch 92 | loss: 0.23324 | eval_custom_logloss: 0.41506 |  0:00:46s
epoch 93 | loss: 0.21278 | eval_custom_logloss: 0.3404  |  0:00:47s
epoch 94 | loss: 0.23641 | eval_custom_logloss: 0.3456  |  0:00:47s
epoch 95 | loss: 0.23095 | eval_custom_logloss: 0.27969 |  0:00:48s
epoch 96 | loss: 0.20877 | eval_custom_logloss: 0.28474 |  0:00:48s
epoch 97 | loss: 0.20551 | eval_custom_logloss: 0.33842 |  0:00:49s
epoch 98 | loss: 0.2029  | eval_custom_logloss: 0.25227 |  0:00:49s
epoch 99 | loss: 0.18886 | eval_custom_logloss: 0.37753 |  0:00:50s
Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_eval_custom_logloss = 0.25227
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40984, 'Log Loss - std': 0.10270021616335574} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 32 finished with value: 0.40984 and parameters: {'n_d': 63, 'n_steps': 8, 'gamma': 1.44371396360008, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 1, 'momentum': 0.016125280142687836, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax', 'n_a': 58, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.07167 | eval_custom_logloss: 6.43728 |  0:00:00s
epoch 1  | loss: 2.10515 | eval_custom_logloss: 7.62254 |  0:00:01s
epoch 2  | loss: 1.49447 | eval_custom_logloss: 7.49821 |  0:00:02s
epoch 3  | loss: 1.83528 | eval_custom_logloss: 10.50655|  0:00:02s
epoch 4  | loss: 1.75671 | eval_custom_logloss: 7.81094 |  0:00:03s
epoch 5  | loss: 1.80588 | eval_custom_logloss: 9.34228 |  0:00:04s
epoch 6  | loss: 1.33559 | eval_custom_logloss: 7.86223 |  0:00:04s
epoch 7  | loss: 1.89852 | eval_custom_logloss: 6.04267 |  0:00:05s
epoch 8  | loss: 1.1718  | eval_custom_logloss: 6.67511 |  0:00:06s
epoch 9  | loss: 0.89981 | eval_custom_logloss: 5.31825 |  0:00:06s
epoch 10 | loss: 0.84624 | eval_custom_logloss: 4.54573 |  0:00:07s
epoch 11 | loss: 0.71573 | eval_custom_logloss: 4.99959 |  0:00:07s
epoch 12 | loss: 0.67682 | eval_custom_logloss: 6.64457 |  0:00:08s
epoch 13 | loss: 0.62254 | eval_custom_logloss: 5.69992 |  0:00:09s
epoch 14 | loss: 0.61849 | eval_custom_logloss: 3.61249 |  0:00:09s
epoch 15 | loss: 0.61593 | eval_custom_logloss: 3.77647 |  0:00:10s
epoch 16 | loss: 0.57511 | eval_custom_logloss: 3.87904 |  0:00:11s
epoch 17 | loss: 0.52046 | eval_custom_logloss: 4.6612  |  0:00:11s
epoch 18 | loss: 0.55645 | eval_custom_logloss: 4.55306 |  0:00:12s
epoch 19 | loss: 0.53259 | eval_custom_logloss: 4.21238 |  0:00:12s
epoch 20 | loss: 0.56139 | eval_custom_logloss: 3.79024 |  0:00:13s
epoch 21 | loss: 0.56877 | eval_custom_logloss: 3.4911  |  0:00:14s
epoch 22 | loss: 0.53265 | eval_custom_logloss: 3.88752 |  0:00:14s
epoch 23 | loss: 0.53811 | eval_custom_logloss: 3.71239 |  0:00:15s
epoch 24 | loss: 0.55511 | eval_custom_logloss: 2.96949 |  0:00:15s
epoch 25 | loss: 0.51175 | eval_custom_logloss: 2.11396 |  0:00:16s
epoch 26 | loss: 0.50517 | eval_custom_logloss: 3.8747  |  0:00:17s
epoch 27 | loss: 0.5025  | eval_custom_logloss: 2.12619 |  0:00:17s
epoch 28 | loss: 0.47017 | eval_custom_logloss: 2.9181  |  0:00:18s
epoch 29 | loss: 0.47407 | eval_custom_logloss: 2.36995 |  0:00:19s
epoch 30 | loss: 0.43982 | eval_custom_logloss: 2.77051 |  0:00:19s
epoch 31 | loss: 0.4146  | eval_custom_logloss: 2.55757 |  0:00:20s
epoch 32 | loss: 0.45409 | eval_custom_logloss: 2.08498 |  0:00:20s
epoch 33 | loss: 0.45779 | eval_custom_logloss: 1.72471 |  0:00:21s
epoch 34 | loss: 0.47055 | eval_custom_logloss: 2.50704 |  0:00:22s
epoch 35 | loss: 0.48696 | eval_custom_logloss: 1.45209 |  0:00:22s
epoch 36 | loss: 0.44645 | eval_custom_logloss: 1.38732 |  0:00:23s
epoch 37 | loss: 0.41424 | eval_custom_logloss: 1.3677  |  0:00:23s
epoch 38 | loss: 0.42241 | eval_custom_logloss: 1.36069 |  0:00:24s
epoch 39 | loss: 0.41294 | eval_custom_logloss: 1.41509 |  0:00:25s
epoch 40 | loss: 0.44249 | eval_custom_logloss: 1.17197 |  0:00:25s
epoch 41 | loss: 0.41077 | eval_custom_logloss: 1.72522 |  0:00:26s
epoch 42 | loss: 0.39478 | eval_custom_logloss: 1.84203 |  0:00:27s
epoch 43 | loss: 0.42556 | eval_custom_logloss: 1.33023 |  0:00:27s
epoch 44 | loss: 0.40439 | eval_custom_logloss: 1.73425 |  0:00:28s
epoch 45 | loss: 0.42152 | eval_custom_logloss: 1.81063 |  0:00:28s
epoch 46 | loss: 0.37641 | eval_custom_logloss: 1.59603 |  0:00:29s
epoch 47 | loss: 0.37629 | eval_custom_logloss: 1.61356 |  0:00:30s
epoch 48 | loss: 0.38034 | eval_custom_logloss: 1.85483 |  0:00:30s
epoch 49 | loss: 0.35088 | eval_custom_logloss: 1.61296 |  0:00:31s
epoch 50 | loss: 0.38318 | eval_custom_logloss: 2.14609 |  0:00:32s
epoch 51 | loss: 0.38237 | eval_custom_logloss: 1.51774 |  0:00:32s
epoch 52 | loss: 0.38861 | eval_custom_logloss: 1.67597 |  0:00:33s
epoch 53 | loss: 0.41867 | eval_custom_logloss: 1.82951 |  0:00:34s
epoch 54 | loss: 0.41899 | eval_custom_logloss: 1.45761 |  0:00:34s
epoch 55 | loss: 0.48585 | eval_custom_logloss: 2.38454 |  0:00:35s
epoch 56 | loss: 0.4304  | eval_custom_logloss: 1.89079 |  0:00:35s
epoch 57 | loss: 0.48416 | eval_custom_logloss: 1.70118 |  0:00:36s
epoch 58 | loss: 0.48244 | eval_custom_logloss: 1.96425 |  0:00:37s
epoch 59 | loss: 0.43855 | eval_custom_logloss: 1.7364  |  0:00:37s
epoch 60 | loss: 0.44514 | eval_custom_logloss: 1.49602 |  0:00:38s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 1.17197
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0568, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax', 'n_a': 58, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.82325 | eval_custom_logloss: 7.55903 |  0:00:00s
epoch 1  | loss: 3.29722 | eval_custom_logloss: 8.85671 |  0:00:01s
epoch 2  | loss: 1.98845 | eval_custom_logloss: 7.79374 |  0:00:02s
epoch 3  | loss: 1.87385 | eval_custom_logloss: 8.19563 |  0:00:02s
epoch 4  | loss: 1.54174 | eval_custom_logloss: 7.17444 |  0:00:03s
epoch 5  | loss: 1.27562 | eval_custom_logloss: 8.39909 |  0:00:04s
epoch 6  | loss: 1.04935 | eval_custom_logloss: 8.43003 |  0:00:04s
epoch 7  | loss: 1.33863 | eval_custom_logloss: 6.58222 |  0:00:05s
epoch 8  | loss: 0.91373 | eval_custom_logloss: 7.54199 |  0:00:05s
epoch 9  | loss: 0.87013 | eval_custom_logloss: 7.33419 |  0:00:06s
epoch 10 | loss: 0.86877 | eval_custom_logloss: 5.07449 |  0:00:07s
epoch 11 | loss: 0.82425 | eval_custom_logloss: 4.52484 |  0:00:07s
epoch 12 | loss: 0.78482 | eval_custom_logloss: 4.87783 |  0:00:08s
epoch 13 | loss: 0.71302 | eval_custom_logloss: 5.08308 |  0:00:09s
epoch 14 | loss: 0.66885 | eval_custom_logloss: 6.719   |  0:00:09s
epoch 15 | loss: 0.68286 | eval_custom_logloss: 4.82317 |  0:00:10s
epoch 16 | loss: 0.60422 | eval_custom_logloss: 4.27483 |  0:00:10s
epoch 17 | loss: 0.62131 | eval_custom_logloss: 4.50331 |  0:00:11s
epoch 18 | loss: 0.63458 | eval_custom_logloss: 3.38889 |  0:00:12s
epoch 19 | loss: 0.59706 | eval_custom_logloss: 3.04454 |  0:00:12s
epoch 20 | loss: 0.61962 | eval_custom_logloss: 3.86954 |  0:00:13s
epoch 21 | loss: 0.61604 | eval_custom_logloss: 3.52652 |  0:00:14s
epoch 22 | loss: 0.55894 | eval_custom_logloss: 2.35572 |  0:00:14s
epoch 23 | loss: 0.55946 | eval_custom_logloss: 2.67153 |  0:00:15s
epoch 24 | loss: 0.55508 | eval_custom_logloss: 2.85955 |  0:00:16s
epoch 25 | loss: 0.50122 | eval_custom_logloss: 2.24525 |  0:00:16s
epoch 26 | loss: 0.5166  | eval_custom_logloss: 2.7784  |  0:00:17s
epoch 27 | loss: 0.48785 | eval_custom_logloss: 2.2489  |  0:00:17s
epoch 28 | loss: 0.56322 | eval_custom_logloss: 2.53727 |  0:00:18s
epoch 29 | loss: 0.51925 | eval_custom_logloss: 2.49931 |  0:00:19s
epoch 30 | loss: 0.4754  | eval_custom_logloss: 2.6095  |  0:00:19s
epoch 31 | loss: 0.47366 | eval_custom_logloss: 2.05061 |  0:00:20s
epoch 32 | loss: 0.47932 | eval_custom_logloss: 2.35987 |  0:00:21s
epoch 33 | loss: 0.44379 | eval_custom_logloss: 2.10663 |  0:00:21s
epoch 34 | loss: 0.461   | eval_custom_logloss: 1.75045 |  0:00:22s
epoch 35 | loss: 0.44034 | eval_custom_logloss: 2.21254 |  0:00:22s
epoch 36 | loss: 0.42716 | eval_custom_logloss: 2.67981 |  0:00:23s
epoch 37 | loss: 0.42181 | eval_custom_logloss: 2.20707 |  0:00:24s
epoch 38 | loss: 0.41888 | eval_custom_logloss: 1.98622 |  0:00:24s
epoch 39 | loss: 0.41558 | eval_custom_logloss: 1.81444 |  0:00:25s
epoch 40 | loss: 0.40383 | eval_custom_logloss: 1.77487 |  0:00:25s
epoch 41 | loss: 0.44782 | eval_custom_logloss: 1.87142 |  0:00:26s
epoch 42 | loss: 0.45454 | eval_custom_logloss: 1.97237 |  0:00:27s
epoch 43 | loss: 0.47275 | eval_custom_logloss: 1.76563 |  0:00:27s
epoch 44 | loss: 0.45442 | eval_custom_logloss: 1.82269 |  0:00:28s
epoch 45 | loss: 0.41864 | eval_custom_logloss: 1.89086 |  0:00:28s
epoch 46 | loss: 0.43544 | eval_custom_logloss: 1.79352 |  0:00:29s
epoch 47 | loss: 0.44711 | eval_custom_logloss: 1.64576 |  0:00:30s
epoch 48 | loss: 0.41793 | eval_custom_logloss: 1.93494 |  0:00:30s
epoch 49 | loss: 0.42195 | eval_custom_logloss: 1.64568 |  0:00:31s
epoch 50 | loss: 0.40573 | eval_custom_logloss: 1.7463  |  0:00:32s
epoch 51 | loss: 0.4478  | eval_custom_logloss: 1.21144 |  0:00:32s
epoch 52 | loss: 0.39869 | eval_custom_logloss: 1.56242 |  0:00:33s
epoch 53 | loss: 0.36494 | eval_custom_logloss: 1.61017 |  0:00:33s
epoch 54 | loss: 0.37653 | eval_custom_logloss: 1.4381  |  0:00:34s
epoch 55 | loss: 0.36508 | eval_custom_logloss: 1.56287 |  0:00:35s
epoch 56 | loss: 0.33028 | eval_custom_logloss: 1.30885 |  0:00:35s
epoch 57 | loss: 0.33258 | eval_custom_logloss: 1.69306 |  0:00:36s
epoch 58 | loss: 0.32783 | eval_custom_logloss: 1.76334 |  0:00:36s
epoch 59 | loss: 0.31052 | eval_custom_logloss: 1.99112 |  0:00:37s
epoch 60 | loss: 0.33643 | eval_custom_logloss: 1.43287 |  0:00:38s
epoch 61 | loss: 0.28622 | eval_custom_logloss: 1.64513 |  0:00:38s
epoch 62 | loss: 0.32465 | eval_custom_logloss: 1.61756 |  0:00:39s
epoch 63 | loss: 0.30317 | eval_custom_logloss: 1.31904 |  0:00:39s
epoch 64 | loss: 0.31702 | eval_custom_logloss: 1.41295 |  0:00:40s
epoch 65 | loss: 0.28609 | eval_custom_logloss: 1.61521 |  0:00:41s
epoch 66 | loss: 0.32491 | eval_custom_logloss: 1.90246 |  0:00:41s
epoch 67 | loss: 0.32688 | eval_custom_logloss: 1.56003 |  0:00:42s
epoch 68 | loss: 0.34467 | eval_custom_logloss: 1.78072 |  0:00:42s
epoch 69 | loss: 0.31154 | eval_custom_logloss: 1.60431 |  0:00:43s
epoch 70 | loss: 0.3238  | eval_custom_logloss: 1.62006 |  0:00:44s
epoch 71 | loss: 0.30132 | eval_custom_logloss: 1.70867 |  0:00:44s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 1.21144
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.08725, 'Log Loss - std': 0.030449999999999977} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax', 'n_a': 58, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.69544 | eval_custom_logloss: 8.28818 |  0:00:00s
epoch 1  | loss: 1.81316 | eval_custom_logloss: 7.82402 |  0:00:01s
epoch 2  | loss: 1.64499 | eval_custom_logloss: 6.98866 |  0:00:01s
epoch 3  | loss: 2.03736 | eval_custom_logloss: 9.02736 |  0:00:02s
epoch 4  | loss: 2.13366 | eval_custom_logloss: 6.46364 |  0:00:03s
epoch 5  | loss: 2.4938  | eval_custom_logloss: 6.99621 |  0:00:03s
epoch 6  | loss: 1.45841 | eval_custom_logloss: 4.9177  |  0:00:04s
epoch 7  | loss: 1.44105 | eval_custom_logloss: 6.23989 |  0:00:05s
epoch 8  | loss: 1.07395 | eval_custom_logloss: 8.6526  |  0:00:05s
epoch 9  | loss: 1.06304 | eval_custom_logloss: 7.08682 |  0:00:06s
epoch 10 | loss: 0.91362 | eval_custom_logloss: 5.60169 |  0:00:06s
epoch 11 | loss: 0.95033 | eval_custom_logloss: 6.99744 |  0:00:07s
epoch 12 | loss: 0.83671 | eval_custom_logloss: 6.30063 |  0:00:08s
epoch 13 | loss: 0.70873 | eval_custom_logloss: 4.89347 |  0:00:08s
epoch 14 | loss: 0.6981  | eval_custom_logloss: 5.68156 |  0:00:09s
epoch 15 | loss: 0.68804 | eval_custom_logloss: 6.37098 |  0:00:09s
epoch 16 | loss: 0.74496 | eval_custom_logloss: 5.44572 |  0:00:10s
epoch 17 | loss: 0.74582 | eval_custom_logloss: 3.31578 |  0:00:11s
epoch 18 | loss: 0.68216 | eval_custom_logloss: 2.49674 |  0:00:11s
epoch 19 | loss: 0.63827 | eval_custom_logloss: 3.22428 |  0:00:12s
epoch 20 | loss: 0.63923 | eval_custom_logloss: 3.88428 |  0:00:12s
epoch 21 | loss: 0.70296 | eval_custom_logloss: 3.70306 |  0:00:13s
epoch 22 | loss: 0.71067 | eval_custom_logloss: 2.34907 |  0:00:14s
epoch 23 | loss: 0.671   | eval_custom_logloss: 3.22963 |  0:00:14s
epoch 24 | loss: 0.65509 | eval_custom_logloss: 2.19858 |  0:00:15s
epoch 25 | loss: 0.61019 | eval_custom_logloss: 2.08734 |  0:00:15s
epoch 26 | loss: 0.6175  | eval_custom_logloss: 2.38151 |  0:00:16s
epoch 27 | loss: 0.63391 | eval_custom_logloss: 1.84498 |  0:00:17s
epoch 28 | loss: 0.62275 | eval_custom_logloss: 2.27036 |  0:00:17s
epoch 29 | loss: 0.60594 | eval_custom_logloss: 2.19999 |  0:00:18s
epoch 30 | loss: 0.59751 | eval_custom_logloss: 2.95866 |  0:00:19s
epoch 31 | loss: 0.59022 | eval_custom_logloss: 2.2628  |  0:00:19s
epoch 32 | loss: 0.60883 | eval_custom_logloss: 1.75965 |  0:00:20s
epoch 33 | loss: 0.61727 | eval_custom_logloss: 1.82468 |  0:00:20s
epoch 34 | loss: 0.61004 | eval_custom_logloss: 1.55416 |  0:00:21s
epoch 35 | loss: 0.61425 | eval_custom_logloss: 1.43737 |  0:00:22s
epoch 36 | loss: 0.59042 | eval_custom_logloss: 1.65781 |  0:00:22s
epoch 37 | loss: 0.58594 | eval_custom_logloss: 1.69224 |  0:00:23s
epoch 38 | loss: 0.55732 | eval_custom_logloss: 1.87652 |  0:00:23s
epoch 39 | loss: 0.58766 | eval_custom_logloss: 1.76889 |  0:00:24s
epoch 40 | loss: 0.60129 | eval_custom_logloss: 1.52796 |  0:00:25s
epoch 41 | loss: 0.56698 | eval_custom_logloss: 1.56667 |  0:00:25s
epoch 42 | loss: 0.61224 | eval_custom_logloss: 2.01194 |  0:00:26s
epoch 43 | loss: 0.55187 | eval_custom_logloss: 1.72491 |  0:00:26s
epoch 44 | loss: 0.5566  | eval_custom_logloss: 1.99003 |  0:00:27s
epoch 45 | loss: 0.53424 | eval_custom_logloss: 2.70323 |  0:00:28s
epoch 46 | loss: 0.578   | eval_custom_logloss: 2.85404 |  0:00:28s
epoch 47 | loss: 0.5997  | eval_custom_logloss: 2.70057 |  0:00:29s
epoch 48 | loss: 0.54599 | eval_custom_logloss: 1.83068 |  0:00:29s
epoch 49 | loss: 0.50785 | eval_custom_logloss: 1.90257 |  0:00:30s
epoch 50 | loss: 0.53866 | eval_custom_logloss: 1.79239 |  0:00:31s
epoch 51 | loss: 0.53677 | eval_custom_logloss: 1.74758 |  0:00:31s
epoch 52 | loss: 0.53361 | eval_custom_logloss: 2.03721 |  0:00:32s
epoch 53 | loss: 0.57092 | eval_custom_logloss: 1.61263 |  0:00:32s
epoch 54 | loss: 0.53674 | eval_custom_logloss: 1.72587 |  0:00:33s
epoch 55 | loss: 0.50491 | eval_custom_logloss: 1.52084 |  0:00:34s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 1.43737
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.2038666666666666, 'Log Loss - std': 0.1667843784318211} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax', 'n_a': 58, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.57613 | eval_custom_logloss: 8.5989  |  0:00:00s
epoch 1  | loss: 2.49426 | eval_custom_logloss: 8.29677 |  0:00:01s
epoch 2  | loss: 1.51016 | eval_custom_logloss: 8.291   |  0:00:01s
epoch 3  | loss: 1.82361 | eval_custom_logloss: 8.0568  |  0:00:02s
epoch 4  | loss: 1.43769 | eval_custom_logloss: 7.89193 |  0:00:03s
epoch 5  | loss: 1.8213  | eval_custom_logloss: 6.41374 |  0:00:03s
epoch 6  | loss: 1.47761 | eval_custom_logloss: 8.58279 |  0:00:04s
epoch 7  | loss: 1.38639 | eval_custom_logloss: 8.8986  |  0:00:04s
epoch 8  | loss: 1.45519 | eval_custom_logloss: 6.07817 |  0:00:05s
epoch 9  | loss: 0.8916  | eval_custom_logloss: 6.39362 |  0:00:06s
epoch 10 | loss: 0.92262 | eval_custom_logloss: 6.52031 |  0:00:06s
epoch 11 | loss: 0.79238 | eval_custom_logloss: 5.7534  |  0:00:07s
epoch 12 | loss: 0.73135 | eval_custom_logloss: 6.36263 |  0:00:08s
epoch 13 | loss: 0.81379 | eval_custom_logloss: 5.12185 |  0:00:08s
epoch 14 | loss: 0.79428 | eval_custom_logloss: 5.5803  |  0:00:09s
epoch 15 | loss: 0.75818 | eval_custom_logloss: 4.30483 |  0:00:09s
epoch 16 | loss: 0.65929 | eval_custom_logloss: 4.60933 |  0:00:10s
epoch 17 | loss: 0.64035 | eval_custom_logloss: 3.85461 |  0:00:11s
epoch 18 | loss: 0.61053 | eval_custom_logloss: 3.48909 |  0:00:11s
epoch 19 | loss: 0.65016 | eval_custom_logloss: 3.2046  |  0:00:12s
epoch 20 | loss: 0.62987 | eval_custom_logloss: 3.00167 |  0:00:13s
epoch 21 | loss: 0.6389  | eval_custom_logloss: 3.71082 |  0:00:13s
epoch 22 | loss: 0.62422 | eval_custom_logloss: 3.32983 |  0:00:14s
epoch 23 | loss: 0.64634 | eval_custom_logloss: 3.52427 |  0:00:14s
epoch 24 | loss: 0.61335 | eval_custom_logloss: 4.29956 |  0:00:15s
epoch 25 | loss: 0.61164 | eval_custom_logloss: 4.41992 |  0:00:16s
epoch 26 | loss: 0.58666 | eval_custom_logloss: 4.36139 |  0:00:16s
epoch 27 | loss: 0.57714 | eval_custom_logloss: 4.26627 |  0:00:17s
epoch 28 | loss: 0.62117 | eval_custom_logloss: 3.73368 |  0:00:17s
epoch 29 | loss: 0.58586 | eval_custom_logloss: 3.7005  |  0:00:18s
epoch 30 | loss: 0.55395 | eval_custom_logloss: 2.98461 |  0:00:19s
epoch 31 | loss: 0.55635 | eval_custom_logloss: 3.77653 |  0:00:19s
epoch 32 | loss: 0.57439 | eval_custom_logloss: 3.62655 |  0:00:20s
epoch 33 | loss: 0.55018 | eval_custom_logloss: 3.51378 |  0:00:20s
epoch 34 | loss: 0.5261  | eval_custom_logloss: 3.40145 |  0:00:21s
epoch 35 | loss: 0.52995 | eval_custom_logloss: 3.08448 |  0:00:22s
epoch 36 | loss: 0.50181 | eval_custom_logloss: 3.19367 |  0:00:22s
epoch 37 | loss: 0.49264 | eval_custom_logloss: 2.91563 |  0:00:23s
epoch 38 | loss: 0.50456 | eval_custom_logloss: 2.95291 |  0:00:23s
epoch 39 | loss: 0.51373 | eval_custom_logloss: 2.53422 |  0:00:24s
epoch 40 | loss: 0.534   | eval_custom_logloss: 2.79646 |  0:00:25s
epoch 41 | loss: 0.51898 | eval_custom_logloss: 2.22484 |  0:00:25s
epoch 42 | loss: 0.5159  | eval_custom_logloss: 2.46842 |  0:00:26s
epoch 43 | loss: 0.57107 | eval_custom_logloss: 1.81367 |  0:00:27s
epoch 44 | loss: 0.52369 | eval_custom_logloss: 2.31932 |  0:00:27s
epoch 45 | loss: 0.53224 | eval_custom_logloss: 1.66408 |  0:00:28s
epoch 46 | loss: 0.55503 | eval_custom_logloss: 1.30473 |  0:00:28s
epoch 47 | loss: 0.52033 | eval_custom_logloss: 1.32257 |  0:00:29s
epoch 48 | loss: 0.51593 | eval_custom_logloss: 1.48495 |  0:00:30s
epoch 49 | loss: 0.49765 | eval_custom_logloss: 1.20389 |  0:00:30s
epoch 50 | loss: 0.4864  | eval_custom_logloss: 1.97753 |  0:00:31s
epoch 51 | loss: 0.45027 | eval_custom_logloss: 1.83282 |  0:00:32s
epoch 52 | loss: 0.45653 | eval_custom_logloss: 1.20044 |  0:00:32s
epoch 53 | loss: 0.44265 | eval_custom_logloss: 1.74503 |  0:00:33s
epoch 54 | loss: 0.44427 | eval_custom_logloss: 2.15346 |  0:00:34s
epoch 55 | loss: 0.42283 | eval_custom_logloss: 2.53216 |  0:00:34s
epoch 56 | loss: 0.39708 | eval_custom_logloss: 2.11224 |  0:00:35s
epoch 57 | loss: 0.404   | eval_custom_logloss: 2.22686 |  0:00:35s
epoch 58 | loss: 0.37578 | eval_custom_logloss: 2.07895 |  0:00:36s
epoch 59 | loss: 0.36097 | eval_custom_logloss: 2.17025 |  0:00:37s
epoch 60 | loss: 0.42441 | eval_custom_logloss: 2.27104 |  0:00:37s
epoch 61 | loss: 0.38046 | eval_custom_logloss: 2.2533  |  0:00:38s
epoch 62 | loss: 0.37194 | eval_custom_logloss: 1.93475 |  0:00:38s
epoch 63 | loss: 0.41382 | eval_custom_logloss: 1.75613 |  0:00:39s
epoch 64 | loss: 0.43625 | eval_custom_logloss: 1.25676 |  0:00:40s
epoch 65 | loss: 0.38846 | eval_custom_logloss: 1.25621 |  0:00:40s
epoch 66 | loss: 0.37593 | eval_custom_logloss: 1.99634 |  0:00:41s
epoch 67 | loss: 0.38482 | eval_custom_logloss: 1.04869 |  0:00:41s
epoch 68 | loss: 0.36192 | eval_custom_logloss: 1.53494 |  0:00:42s
epoch 69 | loss: 0.31194 | eval_custom_logloss: 1.54365 |  0:00:43s
epoch 70 | loss: 0.31141 | eval_custom_logloss: 1.92778 |  0:00:43s
epoch 71 | loss: 0.34325 | eval_custom_logloss: 1.30369 |  0:00:44s
epoch 72 | loss: 0.33606 | eval_custom_logloss: 1.96374 |  0:00:44s
epoch 73 | loss: 0.2955  | eval_custom_logloss: 1.23211 |  0:00:45s
epoch 74 | loss: 0.29964 | eval_custom_logloss: 1.20099 |  0:00:46s
epoch 75 | loss: 0.29768 | eval_custom_logloss: 1.32063 |  0:00:46s
epoch 76 | loss: 0.29095 | eval_custom_logloss: 1.47412 |  0:00:47s
epoch 77 | loss: 0.31532 | eval_custom_logloss: 1.2136  |  0:00:47s
epoch 78 | loss: 0.28283 | eval_custom_logloss: 1.1267  |  0:00:48s
epoch 79 | loss: 0.29909 | eval_custom_logloss: 1.15331 |  0:00:49s
epoch 80 | loss: 0.2775  | eval_custom_logloss: 1.29886 |  0:00:49s
epoch 81 | loss: 0.28086 | eval_custom_logloss: 1.17691 |  0:00:50s
epoch 82 | loss: 0.29395 | eval_custom_logloss: 1.18784 |  0:00:50s
epoch 83 | loss: 0.31985 | eval_custom_logloss: 1.18678 |  0:00:51s
epoch 84 | loss: 0.29109 | eval_custom_logloss: 1.18856 |  0:00:52s
epoch 85 | loss: 0.28827 | eval_custom_logloss: 1.1126  |  0:00:52s
epoch 86 | loss: 0.27824 | eval_custom_logloss: 1.25992 |  0:00:53s
epoch 87 | loss: 0.29738 | eval_custom_logloss: 1.0922  |  0:00:54s

Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_custom_logloss = 1.04869
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.155975, 'Log Loss - std': 0.1665641224123611} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax', 'n_a': 58, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.64008 | eval_custom_logloss: 8.52864 |  0:00:00s
epoch 1  | loss: 1.65662 | eval_custom_logloss: 7.01801 |  0:00:01s
epoch 2  | loss: 1.78723 | eval_custom_logloss: 7.51083 |  0:00:01s
epoch 3  | loss: 3.51431 | eval_custom_logloss: 8.93083 |  0:00:02s
epoch 4  | loss: 2.73727 | eval_custom_logloss: 6.22809 |  0:00:03s
epoch 5  | loss: 1.35518 | eval_custom_logloss: 7.55273 |  0:00:03s
epoch 6  | loss: 1.5056  | eval_custom_logloss: 7.63294 |  0:00:04s
epoch 7  | loss: 1.52849 | eval_custom_logloss: 4.99144 |  0:00:04s
epoch 8  | loss: 1.20819 | eval_custom_logloss: 5.05209 |  0:00:05s
epoch 9  | loss: 0.94726 | eval_custom_logloss: 6.53547 |  0:00:06s
epoch 10 | loss: 0.84544 | eval_custom_logloss: 5.71099 |  0:00:06s
epoch 11 | loss: 0.77262 | eval_custom_logloss: 7.04852 |  0:00:07s
epoch 12 | loss: 0.69884 | eval_custom_logloss: 4.31936 |  0:00:07s
epoch 13 | loss: 0.69249 | eval_custom_logloss: 3.41355 |  0:00:08s
epoch 14 | loss: 0.65256 | eval_custom_logloss: 3.49469 |  0:00:09s
epoch 15 | loss: 0.63901 | eval_custom_logloss: 5.53861 |  0:00:09s
epoch 16 | loss: 0.67672 | eval_custom_logloss: 3.67762 |  0:00:10s
epoch 17 | loss: 0.66971 | eval_custom_logloss: 3.26192 |  0:00:11s
epoch 18 | loss: 0.7304  | eval_custom_logloss: 2.95902 |  0:00:11s
epoch 19 | loss: 0.69489 | eval_custom_logloss: 2.09928 |  0:00:12s
epoch 20 | loss: 0.64724 | eval_custom_logloss: 2.22753 |  0:00:12s
epoch 21 | loss: 0.66307 | eval_custom_logloss: 2.10135 |  0:00:13s
epoch 22 | loss: 0.63855 | eval_custom_logloss: 2.12442 |  0:00:14s
epoch 23 | loss: 0.61053 | eval_custom_logloss: 1.75611 |  0:00:14s
epoch 24 | loss: 0.6141  | eval_custom_logloss: 2.49755 |  0:00:15s
epoch 25 | loss: 0.62303 | eval_custom_logloss: 2.13635 |  0:00:15s
epoch 26 | loss: 0.59579 | eval_custom_logloss: 2.17469 |  0:00:16s
epoch 27 | loss: 0.56281 | eval_custom_logloss: 2.59797 |  0:00:17s
epoch 28 | loss: 0.55297 | eval_custom_logloss: 2.17455 |  0:00:17s
epoch 29 | loss: 0.57997 | eval_custom_logloss: 1.79951 |  0:00:18s
epoch 30 | loss: 0.53919 | eval_custom_logloss: 1.96676 |  0:00:18s
epoch 31 | loss: 0.50525 | eval_custom_logloss: 2.0697  |  0:00:19s
epoch 32 | loss: 0.51037 | eval_custom_logloss: 2.14087 |  0:00:20s
epoch 33 | loss: 0.52775 | eval_custom_logloss: 2.39533 |  0:00:20s
epoch 34 | loss: 0.54382 | eval_custom_logloss: 2.31591 |  0:00:21s
epoch 35 | loss: 0.52366 | eval_custom_logloss: 1.45062 |  0:00:21s
epoch 36 | loss: 0.59762 | eval_custom_logloss: 1.44658 |  0:00:22s
epoch 37 | loss: 0.55391 | eval_custom_logloss: 1.22582 |  0:00:23s
epoch 38 | loss: 0.54588 | eval_custom_logloss: 1.37967 |  0:00:23s
epoch 39 | loss: 0.53302 | eval_custom_logloss: 2.31217 |  0:00:24s
epoch 40 | loss: 0.54223 | eval_custom_logloss: 1.54631 |  0:00:24s
epoch 41 | loss: 0.486   | eval_custom_logloss: 1.42789 |  0:00:25s
epoch 42 | loss: 0.49048 | eval_custom_logloss: 1.59587 |  0:00:26s
epoch 43 | loss: 0.47558 | eval_custom_logloss: 1.36757 |  0:00:26s
epoch 44 | loss: 0.49277 | eval_custom_logloss: 1.59114 |  0:00:27s
epoch 45 | loss: 0.49695 | eval_custom_logloss: 1.01101 |  0:00:28s
epoch 46 | loss: 0.58925 | eval_custom_logloss: 1.04468 |  0:00:28s
epoch 47 | loss: 0.51    | eval_custom_logloss: 1.58822 |  0:00:29s
epoch 48 | loss: 0.52636 | eval_custom_logloss: 1.90462 |  0:00:29s
epoch 49 | loss: 0.53164 | eval_custom_logloss: 1.55612 |  0:00:30s
epoch 50 | loss: 0.4859  | eval_custom_logloss: 1.38253 |  0:00:31s
epoch 51 | loss: 0.5183  | eval_custom_logloss: 1.1637  |  0:00:31s
epoch 52 | loss: 0.5262  | eval_custom_logloss: 1.0898  |  0:00:32s
epoch 53 | loss: 0.50418 | eval_custom_logloss: 1.35878 |  0:00:32s
epoch 54 | loss: 0.4839  | eval_custom_logloss: 1.20528 |  0:00:33s
epoch 55 | loss: 0.49947 | eval_custom_logloss: 1.7439  |  0:00:34s
epoch 56 | loss: 0.46284 | eval_custom_logloss: 1.20539 |  0:00:34s
epoch 57 | loss: 0.45994 | eval_custom_logloss: 1.30289 |  0:00:35s
epoch 58 | loss: 0.51244 | eval_custom_logloss: 1.13509 |  0:00:35s
epoch 59 | loss: 0.49176 | eval_custom_logloss: 1.42832 |  0:00:36s
epoch 60 | loss: 0.46905 | eval_custom_logloss: 1.16584 |  0:00:37s
epoch 61 | loss: 0.45607 | eval_custom_logloss: 1.44309 |  0:00:37s
epoch 62 | loss: 0.50672 | eval_custom_logloss: 1.32779 |  0:00:38s
epoch 63 | loss: 0.54366 | eval_custom_logloss: 1.16639 |  0:00:38s
epoch 64 | loss: 0.49591 | eval_custom_logloss: 0.87607 |  0:00:39s
epoch 65 | loss: 0.46915 | eval_custom_logloss: 1.00909 |  0:00:40s
epoch 66 | loss: 0.45785 | eval_custom_logloss: 1.07296 |  0:00:40s
epoch 67 | loss: 0.44984 | eval_custom_logloss: 0.91764 |  0:00:41s
epoch 68 | loss: 0.45661 | eval_custom_logloss: 1.10202 |  0:00:41s
epoch 69 | loss: 0.47709 | eval_custom_logloss: 1.23803 |  0:00:42s
epoch 70 | loss: 0.46176 | eval_custom_logloss: 1.24888 |  0:00:43s
epoch 71 | loss: 0.44949 | eval_custom_logloss: 1.34333 |  0:00:43s
epoch 72 | loss: 0.44679 | eval_custom_logloss: 1.48345 |  0:00:44s
epoch 73 | loss: 0.43716 | eval_custom_logloss: 1.58192 |  0:00:45s
epoch 74 | loss: 0.4049  | eval_custom_logloss: 1.06809 |  0:00:45s
epoch 75 | loss: 0.45558 | eval_custom_logloss: 1.15506 |  0:00:46s
epoch 76 | loss: 0.45809 | eval_custom_logloss: 1.09332 |  0:00:46s
epoch 77 | loss: 0.45961 | eval_custom_logloss: 0.99078 |  0:00:47s
epoch 78 | loss: 0.44288 | eval_custom_logloss: 1.11676 |  0:00:48s
epoch 79 | loss: 0.40472 | eval_custom_logloss: 0.88079 |  0:00:48s
epoch 80 | loss: 0.38292 | eval_custom_logloss: 1.13896 |  0:00:49s
epoch 81 | loss: 0.40816 | eval_custom_logloss: 1.27891 |  0:00:49s
epoch 82 | loss: 0.40791 | eval_custom_logloss: 1.05643 |  0:00:50s
epoch 83 | loss: 0.38279 | eval_custom_logloss: 0.9517  |  0:00:51s
epoch 84 | loss: 0.39795 | eval_custom_logloss: 0.82913 |  0:00:51s
epoch 85 | loss: 0.35294 | eval_custom_logloss: 1.07324 |  0:00:52s
epoch 86 | loss: 0.35426 | eval_custom_logloss: 0.92571 |  0:00:52s
epoch 87 | loss: 0.38196 | eval_custom_logloss: 0.79507 |  0:00:53s
epoch 88 | loss: 0.3446  | eval_custom_logloss: 0.83099 |  0:00:54s
epoch 89 | loss: 0.3454  | eval_custom_logloss: 0.81853 |  0:00:54s
epoch 90 | loss: 0.34232 | eval_custom_logloss: 0.7189  |  0:00:55s
epoch 91 | loss: 0.35931 | eval_custom_logloss: 0.67304 |  0:00:56s
epoch 92 | loss: 0.33755 | eval_custom_logloss: 0.99648 |  0:00:56s
epoch 93 | loss: 0.3874  | eval_custom_logloss: 0.58771 |  0:00:57s
epoch 94 | loss: 0.35159 | eval_custom_logloss: 0.77426 |  0:00:57s
epoch 95 | loss: 0.33972 | eval_custom_logloss: 0.7207  |  0:00:58s
epoch 96 | loss: 0.32652 | eval_custom_logloss: 0.79285 |  0:00:59s
epoch 97 | loss: 0.3635  | eval_custom_logloss: 0.6531  |  0:00:59s
epoch 98 | loss: 0.36772 | eval_custom_logloss: 1.04885 |  0:01:00s
epoch 99 | loss: 0.35248 | eval_custom_logloss: 1.19436 |  0:01:00s
Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_eval_custom_logloss = 0.58771
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0388199999999999, 'Log Loss - std': 0.2776617755471574} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 33 finished with value: 1.0388199999999999 and parameters: {'n_d': 58, 'n_steps': 9, 'gamma': 1.8951967129788259, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 3, 'momentum': 0.0014634213152679206, 'mask_type': 'sparsemax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.14004 | eval_custom_logloss: 3.70147 |  0:00:00s
epoch 1  | loss: 0.95454 | eval_custom_logloss: 2.36828 |  0:00:00s
epoch 2  | loss: 0.88281 | eval_custom_logloss: 3.1146  |  0:00:01s
epoch 3  | loss: 0.85737 | eval_custom_logloss: 3.161   |  0:00:01s
epoch 4  | loss: 0.81328 | eval_custom_logloss: 2.40464 |  0:00:02s
epoch 5  | loss: 0.80059 | eval_custom_logloss: 3.07072 |  0:00:02s
epoch 6  | loss: 0.74565 | eval_custom_logloss: 2.59278 |  0:00:02s
epoch 7  | loss: 0.71265 | eval_custom_logloss: 3.98503 |  0:00:03s
epoch 8  | loss: 0.69481 | eval_custom_logloss: 4.03623 |  0:00:03s
epoch 9  | loss: 0.67828 | eval_custom_logloss: 6.58374 |  0:00:03s
epoch 10 | loss: 0.62722 | eval_custom_logloss: 6.03368 |  0:00:04s
epoch 11 | loss: 0.62779 | eval_custom_logloss: 7.24931 |  0:00:04s
epoch 12 | loss: 0.61201 | eval_custom_logloss: 5.06839 |  0:00:05s
epoch 13 | loss: 0.58425 | eval_custom_logloss: 5.34646 |  0:00:05s
epoch 14 | loss: 0.5952  | eval_custom_logloss: 5.49185 |  0:00:05s
epoch 15 | loss: 0.58392 | eval_custom_logloss: 4.00029 |  0:00:06s
epoch 16 | loss: 0.53064 | eval_custom_logloss: 6.14903 |  0:00:06s
epoch 17 | loss: 0.51827 | eval_custom_logloss: 5.48202 |  0:00:06s
epoch 18 | loss: 0.50313 | eval_custom_logloss: 5.47674 |  0:00:07s
epoch 19 | loss: 0.48291 | eval_custom_logloss: 4.99511 |  0:00:07s
epoch 20 | loss: 0.50144 | eval_custom_logloss: 4.66904 |  0:00:08s
epoch 21 | loss: 0.49078 | eval_custom_logloss: 5.31471 |  0:00:08s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.36828
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0897, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.15677 | eval_custom_logloss: 3.08258 |  0:00:00s
epoch 1  | loss: 0.92844 | eval_custom_logloss: 2.84862 |  0:00:00s
epoch 2  | loss: 0.85662 | eval_custom_logloss: 3.10854 |  0:00:01s
epoch 3  | loss: 0.79178 | eval_custom_logloss: 2.91152 |  0:00:01s
epoch 4  | loss: 0.72691 | eval_custom_logloss: 3.18162 |  0:00:02s
epoch 5  | loss: 0.67416 | eval_custom_logloss: 3.74064 |  0:00:02s
epoch 6  | loss: 0.62649 | eval_custom_logloss: 4.29683 |  0:00:02s
epoch 7  | loss: 0.62975 | eval_custom_logloss: 5.91452 |  0:00:03s
epoch 8  | loss: 0.61812 | eval_custom_logloss: 5.35164 |  0:00:03s
epoch 9  | loss: 0.5656  | eval_custom_logloss: 3.91216 |  0:00:03s
epoch 10 | loss: 0.54426 | eval_custom_logloss: 4.36923 |  0:00:04s
epoch 11 | loss: 0.56966 | eval_custom_logloss: 3.32498 |  0:00:04s
epoch 12 | loss: 0.51604 | eval_custom_logloss: 3.18724 |  0:00:05s
epoch 13 | loss: 0.50669 | eval_custom_logloss: 4.15457 |  0:00:05s
epoch 14 | loss: 0.53288 | eval_custom_logloss: 2.79085 |  0:00:05s
epoch 15 | loss: 0.59219 | eval_custom_logloss: 2.78656 |  0:00:06s
epoch 16 | loss: 0.54038 | eval_custom_logloss: 2.87678 |  0:00:06s
epoch 17 | loss: 0.50575 | eval_custom_logloss: 3.18578 |  0:00:06s
epoch 18 | loss: 0.51284 | eval_custom_logloss: 3.92071 |  0:00:07s
epoch 19 | loss: 0.47976 | eval_custom_logloss: 3.85235 |  0:00:07s
epoch 20 | loss: 0.4785  | eval_custom_logloss: 3.81175 |  0:00:08s
epoch 21 | loss: 0.44207 | eval_custom_logloss: 3.76302 |  0:00:08s
epoch 22 | loss: 0.42848 | eval_custom_logloss: 4.62521 |  0:00:09s
epoch 23 | loss: 0.40764 | eval_custom_logloss: 3.53421 |  0:00:09s
epoch 24 | loss: 0.40414 | eval_custom_logloss: 3.69045 |  0:00:10s
epoch 25 | loss: 0.41655 | eval_custom_logloss: 3.48969 |  0:00:10s
epoch 26 | loss: 0.39034 | eval_custom_logloss: 3.7583  |  0:00:10s
epoch 27 | loss: 0.44205 | eval_custom_logloss: 3.56253 |  0:00:11s
epoch 28 | loss: 0.39093 | eval_custom_logloss: 3.17063 |  0:00:11s
epoch 29 | loss: 0.39569 | eval_custom_logloss: 3.35574 |  0:00:12s
epoch 30 | loss: 0.36416 | eval_custom_logloss: 2.71003 |  0:00:12s
epoch 31 | loss: 0.3786  | eval_custom_logloss: 2.42456 |  0:00:13s
epoch 32 | loss: 0.3526  | eval_custom_logloss: 2.52474 |  0:00:13s
epoch 33 | loss: 0.35538 | eval_custom_logloss: 2.08129 |  0:00:14s
epoch 34 | loss: 0.33215 | eval_custom_logloss: 2.95514 |  0:00:14s
epoch 35 | loss: 0.31151 | eval_custom_logloss: 2.83794 |  0:00:15s
epoch 36 | loss: 0.31634 | eval_custom_logloss: 2.1541  |  0:00:15s
epoch 37 | loss: 0.30224 | eval_custom_logloss: 1.95744 |  0:00:16s
epoch 38 | loss: 0.32994 | eval_custom_logloss: 3.28216 |  0:00:16s
epoch 39 | loss: 0.32921 | eval_custom_logloss: 2.14491 |  0:00:16s
epoch 40 | loss: 0.34538 | eval_custom_logloss: 2.63885 |  0:00:17s
epoch 41 | loss: 0.35817 | eval_custom_logloss: 2.51999 |  0:00:17s
epoch 42 | loss: 0.34583 | eval_custom_logloss: 1.71283 |  0:00:18s
epoch 43 | loss: 0.3327  | eval_custom_logloss: 1.62115 |  0:00:18s
epoch 44 | loss: 0.34326 | eval_custom_logloss: 1.5497  |  0:00:19s
epoch 45 | loss: 0.34982 | eval_custom_logloss: 2.36748 |  0:00:19s
epoch 46 | loss: 0.33673 | eval_custom_logloss: 1.83387 |  0:00:20s
epoch 47 | loss: 0.33394 | eval_custom_logloss: 2.18348 |  0:00:20s
epoch 48 | loss: 0.33066 | eval_custom_logloss: 1.62316 |  0:00:20s
epoch 49 | loss: 0.31138 | eval_custom_logloss: 1.40697 |  0:00:21s
epoch 50 | loss: 0.29326 | eval_custom_logloss: 1.69682 |  0:00:21s
epoch 51 | loss: 0.30227 | eval_custom_logloss: 1.52798 |  0:00:22s
epoch 52 | loss: 0.28776 | eval_custom_logloss: 2.06453 |  0:00:22s
epoch 53 | loss: 0.28263 | eval_custom_logloss: 1.95228 |  0:00:23s
epoch 54 | loss: 0.28126 | eval_custom_logloss: 2.59837 |  0:00:23s
epoch 55 | loss: 0.26558 | eval_custom_logloss: 2.204   |  0:00:24s
epoch 56 | loss: 0.2889  | eval_custom_logloss: 2.19993 |  0:00:24s
epoch 57 | loss: 0.28655 | eval_custom_logloss: 1.89906 |  0:00:24s
epoch 58 | loss: 0.2725  | eval_custom_logloss: 2.19048 |  0:00:25s
epoch 59 | loss: 0.29764 | eval_custom_logloss: 2.52731 |  0:00:25s
epoch 60 | loss: 0.26707 | eval_custom_logloss: 2.71402 |  0:00:26s
epoch 61 | loss: 0.27044 | eval_custom_logloss: 2.86095 |  0:00:26s
epoch 62 | loss: 0.28002 | eval_custom_logloss: 2.79805 |  0:00:27s
epoch 63 | loss: 0.26759 | eval_custom_logloss: 3.08516 |  0:00:27s
epoch 64 | loss: 0.27985 | eval_custom_logloss: 2.62542 |  0:00:27s
epoch 65 | loss: 0.27057 | eval_custom_logloss: 2.64434 |  0:00:28s
epoch 66 | loss: 0.26828 | eval_custom_logloss: 2.46157 |  0:00:28s
epoch 67 | loss: 0.23861 | eval_custom_logloss: 2.564   |  0:00:29s
epoch 68 | loss: 0.24862 | eval_custom_logloss: 2.59443 |  0:00:29s
epoch 69 | loss: 0.26816 | eval_custom_logloss: 2.52804 |  0:00:30s

Early stopping occurred at epoch 69 with best_epoch = 49 and best_eval_custom_logloss = 1.40697
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.7297500000000001, 'Log Loss - std': 0.3599500000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.16283 | eval_custom_logloss: 3.07918 |  0:00:00s
epoch 1  | loss: 0.9118  | eval_custom_logloss: 3.17003 |  0:00:00s
epoch 2  | loss: 0.80471 | eval_custom_logloss: 3.04744 |  0:00:01s
epoch 3  | loss: 0.83809 | eval_custom_logloss: 4.5742  |  0:00:01s
epoch 4  | loss: 0.75146 | eval_custom_logloss: 5.71806 |  0:00:01s
epoch 5  | loss: 0.69461 | eval_custom_logloss: 4.99495 |  0:00:02s
epoch 6  | loss: 0.69718 | eval_custom_logloss: 5.63325 |  0:00:02s
epoch 7  | loss: 0.6424  | eval_custom_logloss: 4.8059  |  0:00:03s
epoch 8  | loss: 0.5843  | eval_custom_logloss: 5.18872 |  0:00:03s
epoch 9  | loss: 0.62854 | eval_custom_logloss: 4.33182 |  0:00:03s
epoch 10 | loss: 0.57506 | eval_custom_logloss: 4.92299 |  0:00:04s
epoch 11 | loss: 0.55666 | eval_custom_logloss: 6.34097 |  0:00:04s
epoch 12 | loss: 0.52393 | eval_custom_logloss: 6.2514  |  0:00:04s
epoch 13 | loss: 0.51129 | eval_custom_logloss: 7.20109 |  0:00:05s
epoch 14 | loss: 0.51238 | eval_custom_logloss: 6.80209 |  0:00:05s
epoch 15 | loss: 0.51007 | eval_custom_logloss: 5.22162 |  0:00:05s
epoch 16 | loss: 0.52413 | eval_custom_logloss: 3.78256 |  0:00:06s
epoch 17 | loss: 0.52758 | eval_custom_logloss: 3.03283 |  0:00:06s
epoch 18 | loss: 0.50296 | eval_custom_logloss: 3.04405 |  0:00:07s
epoch 19 | loss: 0.52528 | eval_custom_logloss: 3.36826 |  0:00:07s
epoch 20 | loss: 0.53705 | eval_custom_logloss: 2.79915 |  0:00:07s
epoch 21 | loss: 0.53087 | eval_custom_logloss: 2.45974 |  0:00:08s
epoch 22 | loss: 0.49925 | eval_custom_logloss: 2.26154 |  0:00:08s
epoch 23 | loss: 0.50271 | eval_custom_logloss: 3.5716  |  0:00:08s
epoch 24 | loss: 0.47454 | eval_custom_logloss: 3.18015 |  0:00:09s
epoch 25 | loss: 0.48554 | eval_custom_logloss: 4.40693 |  0:00:09s
epoch 26 | loss: 0.4817  | eval_custom_logloss: 2.97083 |  0:00:10s
epoch 27 | loss: 0.48176 | eval_custom_logloss: 3.23971 |  0:00:10s
epoch 28 | loss: 0.47858 | eval_custom_logloss: 3.2235  |  0:00:10s
epoch 29 | loss: 0.46166 | eval_custom_logloss: 3.35726 |  0:00:11s
epoch 30 | loss: 0.48616 | eval_custom_logloss: 2.83065 |  0:00:11s
epoch 31 | loss: 0.45681 | eval_custom_logloss: 2.81318 |  0:00:11s
epoch 32 | loss: 0.48393 | eval_custom_logloss: 2.72612 |  0:00:12s
epoch 33 | loss: 0.53076 | eval_custom_logloss: 1.85701 |  0:00:12s
epoch 34 | loss: 0.50903 | eval_custom_logloss: 1.53749 |  0:00:13s
epoch 35 | loss: 0.493   | eval_custom_logloss: 1.5851  |  0:00:13s
epoch 36 | loss: 0.48034 | eval_custom_logloss: 1.96697 |  0:00:13s
epoch 37 | loss: 0.47221 | eval_custom_logloss: 1.9952  |  0:00:14s
epoch 38 | loss: 0.46549 | eval_custom_logloss: 1.63209 |  0:00:14s
epoch 39 | loss: 0.46178 | eval_custom_logloss: 1.72444 |  0:00:14s
epoch 40 | loss: 0.50134 | eval_custom_logloss: 1.66423 |  0:00:15s
epoch 41 | loss: 0.47932 | eval_custom_logloss: 2.33593 |  0:00:15s
epoch 42 | loss: 0.48337 | eval_custom_logloss: 1.87864 |  0:00:16s
epoch 43 | loss: 0.4685  | eval_custom_logloss: 1.77686 |  0:00:16s
epoch 44 | loss: 0.44708 | eval_custom_logloss: 1.43594 |  0:00:16s
epoch 45 | loss: 0.44585 | eval_custom_logloss: 2.16052 |  0:00:17s
epoch 46 | loss: 0.47815 | eval_custom_logloss: 1.75434 |  0:00:17s
epoch 47 | loss: 0.44724 | eval_custom_logloss: 1.53124 |  0:00:17s
epoch 48 | loss: 0.44951 | eval_custom_logloss: 1.97537 |  0:00:18s
epoch 49 | loss: 0.42306 | eval_custom_logloss: 1.89915 |  0:00:18s
epoch 50 | loss: 0.4464  | eval_custom_logloss: 2.82077 |  0:00:18s
epoch 51 | loss: 0.44452 | eval_custom_logloss: 2.42426 |  0:00:19s
epoch 52 | loss: 0.43881 | eval_custom_logloss: 3.11355 |  0:00:19s
epoch 53 | loss: 0.4143  | eval_custom_logloss: 2.16298 |  0:00:20s
epoch 54 | loss: 0.40561 | eval_custom_logloss: 2.10462 |  0:00:20s
epoch 55 | loss: 0.44331 | eval_custom_logloss: 2.20116 |  0:00:20s
epoch 56 | loss: 0.4255  | eval_custom_logloss: 1.61724 |  0:00:21s
epoch 57 | loss: 0.43485 | eval_custom_logloss: 1.55494 |  0:00:21s
epoch 58 | loss: 0.41929 | eval_custom_logloss: 1.59933 |  0:00:21s
epoch 59 | loss: 0.40488 | eval_custom_logloss: 2.37491 |  0:00:22s
epoch 60 | loss: 0.37516 | eval_custom_logloss: 2.34809 |  0:00:22s
epoch 61 | loss: 0.36011 | eval_custom_logloss: 2.28239 |  0:00:23s
epoch 62 | loss: 0.34547 | eval_custom_logloss: 1.84382 |  0:00:23s
epoch 63 | loss: 0.38215 | eval_custom_logloss: 1.62232 |  0:00:23s
epoch 64 | loss: 0.37424 | eval_custom_logloss: 1.81614 |  0:00:24s

Early stopping occurred at epoch 64 with best_epoch = 44 and best_eval_custom_logloss = 1.43594
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.616766666666667, 'Log Loss - std': 0.3345242426026684} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13964 | eval_custom_logloss: 3.25519 |  0:00:00s
epoch 1  | loss: 0.90957 | eval_custom_logloss: 2.64799 |  0:00:00s
epoch 2  | loss: 0.87885 | eval_custom_logloss: 3.45657 |  0:00:01s
epoch 3  | loss: 0.754   | eval_custom_logloss: 3.31036 |  0:00:01s
epoch 4  | loss: 0.69818 | eval_custom_logloss: 3.64609 |  0:00:02s
epoch 5  | loss: 0.69451 | eval_custom_logloss: 2.65966 |  0:00:02s
epoch 6  | loss: 0.64395 | eval_custom_logloss: 3.88543 |  0:00:03s
epoch 7  | loss: 0.61447 | eval_custom_logloss: 5.34006 |  0:00:03s
epoch 8  | loss: 0.58589 | eval_custom_logloss: 6.15451 |  0:00:04s
epoch 9  | loss: 0.56897 | eval_custom_logloss: 6.19773 |  0:00:04s
epoch 10 | loss: 0.56745 | eval_custom_logloss: 6.70582 |  0:00:04s
epoch 11 | loss: 0.58854 | eval_custom_logloss: 6.83529 |  0:00:05s
epoch 12 | loss: 0.5506  | eval_custom_logloss: 7.10567 |  0:00:05s
epoch 13 | loss: 0.54643 | eval_custom_logloss: 7.09466 |  0:00:06s
epoch 14 | loss: 0.50696 | eval_custom_logloss: 5.44476 |  0:00:06s
epoch 15 | loss: 0.50908 | eval_custom_logloss: 4.39709 |  0:00:07s
epoch 16 | loss: 0.51813 | eval_custom_logloss: 4.45206 |  0:00:07s
epoch 17 | loss: 0.50762 | eval_custom_logloss: 4.77118 |  0:00:07s
epoch 18 | loss: 0.48597 | eval_custom_logloss: 4.43847 |  0:00:08s
epoch 19 | loss: 0.51892 | eval_custom_logloss: 3.04132 |  0:00:08s
epoch 20 | loss: 0.51521 | eval_custom_logloss: 3.27255 |  0:00:09s
epoch 21 | loss: 0.53959 | eval_custom_logloss: 3.97276 |  0:00:09s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.64799
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.804675, 'Log Loss - std': 0.4357275259551546} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.14963 | eval_custom_logloss: 2.77884 |  0:00:00s
epoch 1  | loss: 0.89694 | eval_custom_logloss: 2.94809 |  0:00:00s
epoch 2  | loss: 0.82863 | eval_custom_logloss: 3.41158 |  0:00:01s
epoch 3  | loss: 0.83563 | eval_custom_logloss: 5.78239 |  0:00:01s
epoch 4  | loss: 0.76376 | eval_custom_logloss: 4.50427 |  0:00:02s
epoch 5  | loss: 0.72023 | eval_custom_logloss: 5.06735 |  0:00:02s
epoch 6  | loss: 0.68486 | eval_custom_logloss: 6.05165 |  0:00:02s
epoch 7  | loss: 0.71074 | eval_custom_logloss: 7.97273 |  0:00:03s
epoch 8  | loss: 0.68743 | eval_custom_logloss: 5.58856 |  0:00:03s
epoch 9  | loss: 0.67189 | eval_custom_logloss: 5.60444 |  0:00:04s
epoch 10 | loss: 0.63166 | eval_custom_logloss: 5.36935 |  0:00:04s
epoch 11 | loss: 0.58728 | eval_custom_logloss: 5.92204 |  0:00:04s
epoch 12 | loss: 0.57992 | eval_custom_logloss: 4.79344 |  0:00:05s
epoch 13 | loss: 0.57208 | eval_custom_logloss: 4.75605 |  0:00:05s
epoch 14 | loss: 0.59181 | eval_custom_logloss: 3.86729 |  0:00:06s
epoch 15 | loss: 0.58491 | eval_custom_logloss: 3.19452 |  0:00:06s
epoch 16 | loss: 0.56843 | eval_custom_logloss: 4.48096 |  0:00:07s
epoch 17 | loss: 0.56168 | eval_custom_logloss: 4.34963 |  0:00:07s
epoch 18 | loss: 0.53094 | eval_custom_logloss: 4.70826 |  0:00:07s
epoch 19 | loss: 0.51831 | eval_custom_logloss: 4.60479 |  0:00:08s
epoch 20 | loss: 0.51708 | eval_custom_logloss: 4.00715 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.77884
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.9339400000000002, 'Log Loss - std': 0.4676799572357148} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 34 finished with value: 1.9339400000000002 and parameters: {'n_d': 11, 'n_steps': 3, 'gamma': 1.6258684540059365, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0011624597688195692, 'mask_type': 'entmax'}. Best is trial 15 with value: 2.28466.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.19943 | eval_custom_logloss: 4.63706 |  0:00:00s
epoch 1  | loss: 0.96054 | eval_custom_logloss: 4.99531 |  0:00:01s
epoch 2  | loss: 0.89836 | eval_custom_logloss: 5.59452 |  0:00:01s
epoch 3  | loss: 0.87613 | eval_custom_logloss: 5.39331 |  0:00:02s
epoch 4  | loss: 0.8236  | eval_custom_logloss: 6.17528 |  0:00:02s
epoch 5  | loss: 0.79366 | eval_custom_logloss: 5.41303 |  0:00:03s
epoch 6  | loss: 0.7526  | eval_custom_logloss: 5.29601 |  0:00:03s
epoch 7  | loss: 0.74912 | eval_custom_logloss: 5.78259 |  0:00:04s
epoch 8  | loss: 0.71176 | eval_custom_logloss: 6.08602 |  0:00:04s
epoch 9  | loss: 0.71953 | eval_custom_logloss: 6.86256 |  0:00:05s
epoch 10 | loss: 0.67394 | eval_custom_logloss: 7.99013 |  0:00:05s
epoch 11 | loss: 0.64965 | eval_custom_logloss: 3.82201 |  0:00:06s
epoch 12 | loss: 0.62494 | eval_custom_logloss: 4.65257 |  0:00:06s
epoch 13 | loss: 0.61058 | eval_custom_logloss: 4.89167 |  0:00:07s
epoch 14 | loss: 0.60337 | eval_custom_logloss: 6.23384 |  0:00:07s
epoch 15 | loss: 0.59038 | eval_custom_logloss: 5.61681 |  0:00:08s
epoch 16 | loss: 0.57077 | eval_custom_logloss: 6.65354 |  0:00:08s
epoch 17 | loss: 0.58657 | eval_custom_logloss: 5.46922 |  0:00:08s
epoch 18 | loss: 0.59053 | eval_custom_logloss: 6.09676 |  0:00:09s
epoch 19 | loss: 0.55106 | eval_custom_logloss: 6.68136 |  0:00:09s
epoch 20 | loss: 0.56351 | eval_custom_logloss: 5.25418 |  0:00:10s
epoch 21 | loss: 0.53289 | eval_custom_logloss: 5.35931 |  0:00:10s
epoch 22 | loss: 0.54961 | eval_custom_logloss: 5.11559 |  0:00:11s
epoch 23 | loss: 0.52793 | eval_custom_logloss: 3.71211 |  0:00:11s
epoch 24 | loss: 0.5223  | eval_custom_logloss: 4.49462 |  0:00:12s
epoch 25 | loss: 0.51855 | eval_custom_logloss: 4.84632 |  0:00:12s
epoch 26 | loss: 0.52494 | eval_custom_logloss: 5.06034 |  0:00:13s
epoch 27 | loss: 0.5225  | eval_custom_logloss: 4.35829 |  0:00:13s
epoch 28 | loss: 0.50709 | eval_custom_logloss: 3.61764 |  0:00:14s
epoch 29 | loss: 0.4855  | eval_custom_logloss: 2.87175 |  0:00:14s
epoch 30 | loss: 0.50597 | eval_custom_logloss: 3.25121 |  0:00:15s
epoch 31 | loss: 0.48979 | eval_custom_logloss: 3.02836 |  0:00:16s
epoch 32 | loss: 0.49187 | eval_custom_logloss: 3.29038 |  0:00:16s
epoch 33 | loss: 0.4965  | eval_custom_logloss: 3.50843 |  0:00:17s
epoch 34 | loss: 0.47779 | eval_custom_logloss: 2.75006 |  0:00:17s
epoch 35 | loss: 0.49447 | eval_custom_logloss: 2.84359 |  0:00:18s
epoch 36 | loss: 0.46978 | eval_custom_logloss: 1.97484 |  0:00:18s
epoch 37 | loss: 0.45279 | eval_custom_logloss: 2.23443 |  0:00:19s
epoch 38 | loss: 0.44393 | eval_custom_logloss: 2.20896 |  0:00:19s
epoch 39 | loss: 0.42811 | eval_custom_logloss: 2.50516 |  0:00:19s
epoch 40 | loss: 0.416   | eval_custom_logloss: 2.64859 |  0:00:20s
epoch 41 | loss: 0.43591 | eval_custom_logloss: 2.40333 |  0:00:20s
epoch 42 | loss: 0.43846 | eval_custom_logloss: 2.67102 |  0:00:21s
epoch 43 | loss: 0.43415 | eval_custom_logloss: 3.6881  |  0:00:21s
epoch 44 | loss: 0.44257 | eval_custom_logloss: 3.43224 |  0:00:22s
epoch 45 | loss: 0.43682 | eval_custom_logloss: 2.43768 |  0:00:22s
epoch 46 | loss: 0.43131 | eval_custom_logloss: 2.57701 |  0:00:23s
epoch 47 | loss: 0.44965 | eval_custom_logloss: 3.09355 |  0:00:23s
epoch 48 | loss: 0.40588 | eval_custom_logloss: 2.69792 |  0:00:24s
epoch 49 | loss: 0.41204 | eval_custom_logloss: 3.89908 |  0:00:24s
epoch 50 | loss: 0.41806 | eval_custom_logloss: 4.04069 |  0:00:25s
epoch 51 | loss: 0.39684 | eval_custom_logloss: 3.31366 |  0:00:25s
epoch 52 | loss: 0.41496 | eval_custom_logloss: 2.82266 |  0:00:26s
epoch 53 | loss: 0.3835  | eval_custom_logloss: 3.21982 |  0:00:26s
epoch 54 | loss: 0.35968 | eval_custom_logloss: 3.65976 |  0:00:27s
epoch 55 | loss: 0.36529 | eval_custom_logloss: 4.1094  |  0:00:27s
epoch 56 | loss: 0.35329 | eval_custom_logloss: 3.49721 |  0:00:28s

Early stopping occurred at epoch 56 with best_epoch = 36 and best_eval_custom_logloss = 1.97484
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.7158, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13783 | eval_custom_logloss: 3.96256 |  0:00:00s
epoch 1  | loss: 0.98959 | eval_custom_logloss: 4.85576 |  0:00:00s
epoch 2  | loss: 0.85702 | eval_custom_logloss: 5.43661 |  0:00:01s
epoch 3  | loss: 0.77517 | eval_custom_logloss: 5.62105 |  0:00:01s
epoch 4  | loss: 0.80595 | eval_custom_logloss: 6.34881 |  0:00:02s
epoch 5  | loss: 0.75521 | eval_custom_logloss: 7.35604 |  0:00:02s
epoch 6  | loss: 0.73891 | eval_custom_logloss: 5.8963  |  0:00:03s
epoch 7  | loss: 0.67639 | eval_custom_logloss: 6.73922 |  0:00:03s
epoch 8  | loss: 0.68504 | eval_custom_logloss: 7.8791  |  0:00:04s
epoch 9  | loss: 0.65674 | eval_custom_logloss: 5.97165 |  0:00:04s
epoch 10 | loss: 0.67376 | eval_custom_logloss: 5.74305 |  0:00:05s
epoch 11 | loss: 0.64934 | eval_custom_logloss: 6.12188 |  0:00:05s
epoch 12 | loss: 0.62445 | eval_custom_logloss: 6.8097  |  0:00:06s
epoch 13 | loss: 0.61884 | eval_custom_logloss: 5.98128 |  0:00:06s
epoch 14 | loss: 0.61351 | eval_custom_logloss: 7.04496 |  0:00:07s
epoch 15 | loss: 0.58727 | eval_custom_logloss: 7.5507  |  0:00:07s
epoch 16 | loss: 0.569   | eval_custom_logloss: 7.47698 |  0:00:08s
epoch 17 | loss: 0.5769  | eval_custom_logloss: 8.21629 |  0:00:08s
epoch 18 | loss: 0.5863  | eval_custom_logloss: 8.2817  |  0:00:09s
epoch 19 | loss: 0.5474  | eval_custom_logloss: 7.81179 |  0:00:09s
epoch 20 | loss: 0.52665 | eval_custom_logloss: 6.65238 |  0:00:10s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 3.96256
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.55025, 'Log Loss - std': 0.83445} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.17295 | eval_custom_logloss: 5.34274 |  0:00:00s
epoch 1  | loss: 0.92171 | eval_custom_logloss: 4.13464 |  0:00:00s
epoch 2  | loss: 0.90029 | eval_custom_logloss: 4.31886 |  0:00:01s
epoch 3  | loss: 0.82288 | eval_custom_logloss: 4.61578 |  0:00:01s
epoch 4  | loss: 0.76206 | eval_custom_logloss: 5.28063 |  0:00:02s
epoch 5  | loss: 0.75717 | eval_custom_logloss: 5.02678 |  0:00:02s
epoch 6  | loss: 0.76106 | eval_custom_logloss: 6.51327 |  0:00:03s
epoch 7  | loss: 0.69462 | eval_custom_logloss: 7.66407 |  0:00:03s
epoch 8  | loss: 0.67409 | eval_custom_logloss: 7.16152 |  0:00:04s
epoch 9  | loss: 0.62984 | eval_custom_logloss: 7.82026 |  0:00:04s
epoch 10 | loss: 0.63234 | eval_custom_logloss: 5.75541 |  0:00:05s
epoch 11 | loss: 0.65847 | eval_custom_logloss: 6.02851 |  0:00:05s
epoch 12 | loss: 0.63705 | eval_custom_logloss: 6.74432 |  0:00:06s
epoch 13 | loss: 0.66261 | eval_custom_logloss: 5.99841 |  0:00:06s
epoch 14 | loss: 0.63476 | eval_custom_logloss: 8.18276 |  0:00:07s
epoch 15 | loss: 0.60905 | eval_custom_logloss: 7.29814 |  0:00:07s
epoch 16 | loss: 0.59859 | eval_custom_logloss: 6.34123 |  0:00:08s
epoch 17 | loss: 0.57602 | eval_custom_logloss: 6.22935 |  0:00:08s
epoch 18 | loss: 0.56975 | eval_custom_logloss: 7.96504 |  0:00:09s
epoch 19 | loss: 0.55578 | eval_custom_logloss: 7.55582 |  0:00:09s
epoch 20 | loss: 0.54043 | eval_custom_logloss: 4.21752 |  0:00:10s
epoch 21 | loss: 0.5115  | eval_custom_logloss: 4.09712 |  0:00:10s
epoch 22 | loss: 0.52749 | eval_custom_logloss: 3.32653 |  0:00:11s
epoch 23 | loss: 0.52126 | eval_custom_logloss: 3.46338 |  0:00:11s
epoch 24 | loss: 0.49544 | eval_custom_logloss: 4.72544 |  0:00:12s
epoch 25 | loss: 0.51967 | eval_custom_logloss: 5.68263 |  0:00:12s
epoch 26 | loss: 0.54859 | eval_custom_logloss: 6.66071 |  0:00:13s
epoch 27 | loss: 0.53013 | eval_custom_logloss: 7.18197 |  0:00:13s
epoch 28 | loss: 0.52293 | eval_custom_logloss: 4.88947 |  0:00:14s
epoch 29 | loss: 0.49349 | eval_custom_logloss: 4.99938 |  0:00:14s
epoch 30 | loss: 0.51538 | eval_custom_logloss: 4.35311 |  0:00:15s
epoch 31 | loss: 0.50438 | eval_custom_logloss: 4.55052 |  0:00:15s
epoch 32 | loss: 0.47919 | eval_custom_logloss: 4.52215 |  0:00:16s
epoch 33 | loss: 0.48605 | eval_custom_logloss: 4.44892 |  0:00:16s
epoch 34 | loss: 0.47527 | eval_custom_logloss: 4.61196 |  0:00:17s
epoch 35 | loss: 0.4636  | eval_custom_logloss: 4.67759 |  0:00:17s
epoch 36 | loss: 0.45327 | eval_custom_logloss: 4.50941 |  0:00:18s
epoch 37 | loss: 0.45399 | eval_custom_logloss: 4.40174 |  0:00:18s
epoch 38 | loss: 0.45267 | eval_custom_logloss: 3.98633 |  0:00:18s
epoch 39 | loss: 0.42167 | eval_custom_logloss: 4.04782 |  0:00:19s
epoch 40 | loss: 0.42356 | eval_custom_logloss: 3.77247 |  0:00:19s
epoch 41 | loss: 0.45351 | eval_custom_logloss: 3.30367 |  0:00:20s
epoch 42 | loss: 0.43103 | eval_custom_logloss: 3.71492 |  0:00:20s
epoch 43 | loss: 0.46396 | eval_custom_logloss: 3.51954 |  0:00:21s
epoch 44 | loss: 0.44948 | eval_custom_logloss: 3.36713 |  0:00:21s
epoch 45 | loss: 0.42337 | eval_custom_logloss: 3.94724 |  0:00:22s
epoch 46 | loss: 0.42991 | eval_custom_logloss: 3.79072 |  0:00:22s
epoch 47 | loss: 0.43119 | eval_custom_logloss: 3.4088  |  0:00:23s
epoch 48 | loss: 0.41142 | eval_custom_logloss: 2.59472 |  0:00:23s
epoch 49 | loss: 0.42139 | eval_custom_logloss: 2.76102 |  0:00:24s
epoch 50 | loss: 0.44747 | eval_custom_logloss: 3.44054 |  0:00:24s
epoch 51 | loss: 0.39909 | eval_custom_logloss: 3.92298 |  0:00:25s
epoch 52 | loss: 0.41205 | eval_custom_logloss: 3.77076 |  0:00:25s
epoch 53 | loss: 0.38622 | eval_custom_logloss: 3.22125 |  0:00:26s
epoch 54 | loss: 0.39459 | eval_custom_logloss: 3.20773 |  0:00:26s
epoch 55 | loss: 0.37307 | eval_custom_logloss: 3.21603 |  0:00:27s
epoch 56 | loss: 0.38334 | eval_custom_logloss: 2.98612 |  0:00:27s
epoch 57 | loss: 0.39173 | eval_custom_logloss: 2.57965 |  0:00:28s
epoch 58 | loss: 0.37772 | eval_custom_logloss: 2.61812 |  0:00:28s
epoch 59 | loss: 0.37199 | eval_custom_logloss: 2.93343 |  0:00:29s
epoch 60 | loss: 0.36412 | eval_custom_logloss: 2.40007 |  0:00:29s
epoch 61 | loss: 0.37222 | eval_custom_logloss: 2.40699 |  0:00:30s
epoch 62 | loss: 0.35372 | eval_custom_logloss: 2.32003 |  0:00:30s
epoch 63 | loss: 0.33676 | eval_custom_logloss: 2.62048 |  0:00:31s
epoch 64 | loss: 0.33341 | eval_custom_logloss: 2.52408 |  0:00:31s
epoch 65 | loss: 0.33409 | eval_custom_logloss: 2.50266 |  0:00:32s
epoch 66 | loss: 0.34857 | eval_custom_logloss: 2.01539 |  0:00:32s
epoch 67 | loss: 0.35085 | eval_custom_logloss: 2.1088  |  0:00:33s
epoch 68 | loss: 0.34816 | eval_custom_logloss: 1.92135 |  0:00:33s
epoch 69 | loss: 0.33182 | eval_custom_logloss: 2.28697 |  0:00:34s
epoch 70 | loss: 0.32394 | eval_custom_logloss: 1.73786 |  0:00:34s
epoch 71 | loss: 0.31769 | eval_custom_logloss: 1.59691 |  0:00:35s
epoch 72 | loss: 0.33662 | eval_custom_logloss: 1.73321 |  0:00:35s
epoch 73 | loss: 0.32577 | eval_custom_logloss: 1.32933 |  0:00:36s
epoch 74 | loss: 0.32158 | eval_custom_logloss: 1.65618 |  0:00:36s
epoch 75 | loss: 0.34402 | eval_custom_logloss: 1.89684 |  0:00:37s
epoch 76 | loss: 0.32953 | eval_custom_logloss: 1.69584 |  0:00:37s
epoch 77 | loss: 0.31548 | eval_custom_logloss: 1.65688 |  0:00:38s
epoch 78 | loss: 0.31784 | eval_custom_logloss: 1.63212 |  0:00:38s
epoch 79 | loss: 0.30109 | eval_custom_logloss: 2.107   |  0:00:39s
epoch 80 | loss: 0.2988  | eval_custom_logloss: 2.03411 |  0:00:39s
epoch 81 | loss: 0.32849 | eval_custom_logloss: 2.34439 |  0:00:40s
epoch 82 | loss: 0.3133  | eval_custom_logloss: 1.88861 |  0:00:40s
epoch 83 | loss: 0.3327  | eval_custom_logloss: 1.96607 |  0:00:40s
epoch 84 | loss: 0.31767 | eval_custom_logloss: 1.94832 |  0:00:41s
epoch 85 | loss: 0.31956 | eval_custom_logloss: 2.03691 |  0:00:41s
epoch 86 | loss: 0.3192  | eval_custom_logloss: 2.34695 |  0:00:42s
epoch 87 | loss: 0.31451 | eval_custom_logloss: 1.84274 |  0:00:42s
epoch 88 | loss: 0.31961 | eval_custom_logloss: 1.87608 |  0:00:43s
epoch 89 | loss: 0.33926 | eval_custom_logloss: 2.32134 |  0:00:43s
epoch 90 | loss: 0.32554 | eval_custom_logloss: 1.88092 |  0:00:44s
epoch 91 | loss: 0.30962 | eval_custom_logloss: 2.31636 |  0:00:44s
epoch 92 | loss: 0.29188 | eval_custom_logloss: 2.72032 |  0:00:45s
epoch 93 | loss: 0.30354 | eval_custom_logloss: 2.25293 |  0:00:45s

Early stopping occurred at epoch 93 with best_epoch = 73 and best_eval_custom_logloss = 1.32933
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.1089333333333333, 'Log Loss - std': 0.9239725837683473} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13242 | eval_custom_logloss: 5.49773 |  0:00:00s
epoch 1  | loss: 0.96801 | eval_custom_logloss: 5.20854 |  0:00:01s
epoch 2  | loss: 0.86915 | eval_custom_logloss: 6.65319 |  0:00:01s
epoch 3  | loss: 0.81    | eval_custom_logloss: 6.52872 |  0:00:02s
epoch 4  | loss: 0.77896 | eval_custom_logloss: 5.81272 |  0:00:02s
epoch 5  | loss: 0.77191 | eval_custom_logloss: 7.19189 |  0:00:03s
epoch 6  | loss: 0.74473 | eval_custom_logloss: 7.63276 |  0:00:03s
epoch 7  | loss: 0.70954 | eval_custom_logloss: 6.10105 |  0:00:04s
epoch 8  | loss: 0.65715 | eval_custom_logloss: 7.57521 |  0:00:04s
epoch 9  | loss: 0.63006 | eval_custom_logloss: 7.53926 |  0:00:05s
epoch 10 | loss: 0.6201  | eval_custom_logloss: 7.15699 |  0:00:05s
epoch 11 | loss: 0.5835  | eval_custom_logloss: 7.85609 |  0:00:06s
epoch 12 | loss: 0.57037 | eval_custom_logloss: 7.25592 |  0:00:06s
epoch 13 | loss: 0.56918 | eval_custom_logloss: 6.12163 |  0:00:07s
epoch 14 | loss: 0.54932 | eval_custom_logloss: 5.58843 |  0:00:07s
epoch 15 | loss: 0.54422 | eval_custom_logloss: 4.29063 |  0:00:08s
epoch 16 | loss: 0.54794 | eval_custom_logloss: 6.06014 |  0:00:08s
epoch 17 | loss: 0.54833 | eval_custom_logloss: 7.2363  |  0:00:09s
epoch 18 | loss: 0.58148 | eval_custom_logloss: 6.2195  |  0:00:09s
epoch 19 | loss: 0.56102 | eval_custom_logloss: 5.38172 |  0:00:10s
epoch 20 | loss: 0.54129 | eval_custom_logloss: 4.78617 |  0:00:10s
epoch 21 | loss: 0.55691 | eval_custom_logloss: 5.61521 |  0:00:11s
epoch 22 | loss: 0.54397 | eval_custom_logloss: 6.24802 |  0:00:11s
epoch 23 | loss: 0.51177 | eval_custom_logloss: 5.00319 |  0:00:12s
epoch 24 | loss: 0.51224 | eval_custom_logloss: 5.26506 |  0:00:12s
epoch 25 | loss: 0.49909 | eval_custom_logloss: 6.39316 |  0:00:13s
epoch 26 | loss: 0.52499 | eval_custom_logloss: 5.2605  |  0:00:13s
epoch 27 | loss: 0.5837  | eval_custom_logloss: 4.77101 |  0:00:13s
epoch 28 | loss: 0.51571 | eval_custom_logloss: 4.03242 |  0:00:14s
epoch 29 | loss: 0.55286 | eval_custom_logloss: 3.67431 |  0:00:14s
epoch 30 | loss: 0.53574 | eval_custom_logloss: 3.41838 |  0:00:15s
epoch 31 | loss: 0.52978 | eval_custom_logloss: 4.57907 |  0:00:15s
epoch 32 | loss: 0.52115 | eval_custom_logloss: 4.70662 |  0:00:16s
epoch 33 | loss: 0.48842 | eval_custom_logloss: 4.65778 |  0:00:16s
epoch 34 | loss: 0.4877  | eval_custom_logloss: 4.1621  |  0:00:17s
epoch 35 | loss: 0.49531 | eval_custom_logloss: 3.62382 |  0:00:17s
epoch 36 | loss: 0.4548  | eval_custom_logloss: 3.73636 |  0:00:18s
epoch 37 | loss: 0.44414 | eval_custom_logloss: 4.91733 |  0:00:18s
epoch 38 | loss: 0.47059 | eval_custom_logloss: 4.50131 |  0:00:19s
epoch 39 | loss: 0.4732  | eval_custom_logloss: 4.69926 |  0:00:19s
epoch 40 | loss: 0.45055 | eval_custom_logloss: 4.65361 |  0:00:20s
epoch 41 | loss: 0.42973 | eval_custom_logloss: 4.61599 |  0:00:20s
epoch 42 | loss: 0.45245 | eval_custom_logloss: 4.25911 |  0:00:21s
epoch 43 | loss: 0.48146 | eval_custom_logloss: 3.11913 |  0:00:21s
epoch 44 | loss: 0.44373 | eval_custom_logloss: 4.2977  |  0:00:22s
epoch 45 | loss: 0.47903 | eval_custom_logloss: 4.33309 |  0:00:22s
epoch 46 | loss: 0.43467 | eval_custom_logloss: 5.04316 |  0:00:23s
epoch 47 | loss: 0.4349  | eval_custom_logloss: 5.35095 |  0:00:23s
epoch 48 | loss: 0.47549 | eval_custom_logloss: 4.8852  |  0:00:24s
epoch 49 | loss: 0.43984 | eval_custom_logloss: 5.50931 |  0:00:24s
epoch 50 | loss: 0.461   | eval_custom_logloss: 4.5009  |  0:00:25s
epoch 51 | loss: 0.44448 | eval_custom_logloss: 3.47641 |  0:00:25s
epoch 52 | loss: 0.45389 | eval_custom_logloss: 3.39293 |  0:00:26s
epoch 53 | loss: 0.43093 | eval_custom_logloss: 4.20668 |  0:00:26s
epoch 54 | loss: 0.43577 | eval_custom_logloss: 3.65958 |  0:00:27s
epoch 55 | loss: 0.41308 | eval_custom_logloss: 3.1935  |  0:00:27s
epoch 56 | loss: 0.43171 | eval_custom_logloss: 3.25497 |  0:00:28s
epoch 57 | loss: 0.46717 | eval_custom_logloss: 3.22819 |  0:00:28s
epoch 58 | loss: 0.43806 | eval_custom_logloss: 3.45617 |  0:00:29s
epoch 59 | loss: 0.43481 | eval_custom_logloss: 5.08136 |  0:00:29s
epoch 60 | loss: 0.4338  | eval_custom_logloss: 4.39759 |  0:00:30s
epoch 61 | loss: 0.41514 | eval_custom_logloss: 4.61753 |  0:00:30s
epoch 62 | loss: 0.41684 | eval_custom_logloss: 5.43709 |  0:00:31s
epoch 63 | loss: 0.39713 | eval_custom_logloss: 5.91027 |  0:00:31s

Early stopping occurred at epoch 63 with best_epoch = 43 and best_eval_custom_logloss = 3.11913
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.298925, 'Log Loss - std': 0.8652077795969013} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.16573 | eval_custom_logloss: 5.03148 |  0:00:00s
epoch 1  | loss: 0.95403 | eval_custom_logloss: 4.42523 |  0:00:00s
epoch 2  | loss: 0.89909 | eval_custom_logloss: 4.31881 |  0:00:01s
epoch 3  | loss: 0.84236 | eval_custom_logloss: 5.12486 |  0:00:02s
epoch 4  | loss: 0.85536 | eval_custom_logloss: 4.05994 |  0:00:02s
epoch 5  | loss: 0.76031 | eval_custom_logloss: 4.33204 |  0:00:02s
epoch 6  | loss: 0.7149  | eval_custom_logloss: 5.6562  |  0:00:03s
epoch 7  | loss: 0.74424 | eval_custom_logloss: 6.53435 |  0:00:03s
epoch 8  | loss: 0.70218 | eval_custom_logloss: 5.72634 |  0:00:04s
epoch 9  | loss: 0.68353 | eval_custom_logloss: 5.178   |  0:00:04s
epoch 10 | loss: 0.64601 | eval_custom_logloss: 6.87773 |  0:00:05s
epoch 11 | loss: 0.63679 | eval_custom_logloss: 8.29605 |  0:00:05s
epoch 12 | loss: 0.60471 | eval_custom_logloss: 6.02019 |  0:00:06s
epoch 13 | loss: 0.61995 | eval_custom_logloss: 6.68038 |  0:00:06s
epoch 14 | loss: 0.63863 | eval_custom_logloss: 7.73812 |  0:00:07s
epoch 15 | loss: 0.61957 | eval_custom_logloss: 6.30143 |  0:00:07s
epoch 16 | loss: 0.60515 | eval_custom_logloss: 5.83518 |  0:00:08s
epoch 17 | loss: 0.59089 | eval_custom_logloss: 5.56227 |  0:00:08s
epoch 18 | loss: 0.58791 | eval_custom_logloss: 5.95938 |  0:00:09s
epoch 19 | loss: 0.58298 | eval_custom_logloss: 6.51243 |  0:00:09s
epoch 20 | loss: 0.55947 | eval_custom_logloss: 5.44168 |  0:00:10s
epoch 21 | loss: 0.5517  | eval_custom_logloss: 5.52752 |  0:00:10s
epoch 22 | loss: 0.55757 | eval_custom_logloss: 6.1649  |  0:00:11s
epoch 23 | loss: 0.55761 | eval_custom_logloss: 3.64701 |  0:00:11s
epoch 24 | loss: 0.53144 | eval_custom_logloss: 3.82713 |  0:00:12s
epoch 25 | loss: 0.54437 | eval_custom_logloss: 3.19115 |  0:00:12s
epoch 26 | loss: 0.53271 | eval_custom_logloss: 2.98253 |  0:00:13s
epoch 27 | loss: 0.53267 | eval_custom_logloss: 4.55371 |  0:00:13s
epoch 28 | loss: 0.52182 | eval_custom_logloss: 4.57213 |  0:00:14s
epoch 29 | loss: 0.50342 | eval_custom_logloss: 4.73148 |  0:00:14s
epoch 30 | loss: 0.50961 | eval_custom_logloss: 4.6749  |  0:00:15s
epoch 31 | loss: 0.50541 | eval_custom_logloss: 4.89513 |  0:00:15s
epoch 32 | loss: 0.47947 | eval_custom_logloss: 5.84519 |  0:00:16s
epoch 33 | loss: 0.47914 | eval_custom_logloss: 5.60898 |  0:00:16s
epoch 34 | loss: 0.46993 | eval_custom_logloss: 5.41399 |  0:00:17s
epoch 35 | loss: 0.50343 | eval_custom_logloss: 4.68224 |  0:00:17s
epoch 36 | loss: 0.45628 | eval_custom_logloss: 4.30163 |  0:00:18s
epoch 37 | loss: 0.46361 | eval_custom_logloss: 5.70763 |  0:00:18s
epoch 38 | loss: 0.49853 | eval_custom_logloss: 5.1195  |  0:00:19s
epoch 39 | loss: 0.45091 | eval_custom_logloss: 4.60479 |  0:00:19s
epoch 40 | loss: 0.45499 | eval_custom_logloss: 4.21433 |  0:00:20s
epoch 41 | loss: 0.4348  | eval_custom_logloss: 4.68911 |  0:00:20s
epoch 42 | loss: 0.46011 | eval_custom_logloss: 4.32416 |  0:00:21s
epoch 43 | loss: 0.44304 | eval_custom_logloss: 4.47401 |  0:00:21s
epoch 44 | loss: 0.46119 | eval_custom_logloss: 4.06173 |  0:00:21s
epoch 45 | loss: 0.41741 | eval_custom_logloss: 3.68844 |  0:00:22s
epoch 46 | loss: 0.43326 | eval_custom_logloss: 3.48906 |  0:00:22s

Early stopping occurred at epoch 46 with best_epoch = 26 and best_eval_custom_logloss = 2.98253
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.35452, 'Log Loss - std': 0.7818125207490605} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 35 finished with value: 2.35452 and parameters: {'n_d': 14, 'n_steps': 5, 'gamma': 1.149436011787179, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010297812053822362, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13345 | eval_custom_logloss: 2.80861 |  0:00:00s
epoch 1  | loss: 0.91487 | eval_custom_logloss: 2.20154 |  0:00:00s
epoch 2  | loss: 0.85243 | eval_custom_logloss: 3.73135 |  0:00:01s
epoch 3  | loss: 0.82674 | eval_custom_logloss: 3.2116  |  0:00:01s
epoch 4  | loss: 0.80458 | eval_custom_logloss: 3.24514 |  0:00:02s
epoch 5  | loss: 0.76271 | eval_custom_logloss: 3.20657 |  0:00:02s
epoch 6  | loss: 0.72405 | eval_custom_logloss: 3.72713 |  0:00:03s
epoch 7  | loss: 0.70046 | eval_custom_logloss: 4.02931 |  0:00:03s
epoch 8  | loss: 0.65858 | eval_custom_logloss: 5.45294 |  0:00:04s
epoch 9  | loss: 0.68564 | eval_custom_logloss: 5.41342 |  0:00:04s
epoch 10 | loss: 0.61795 | eval_custom_logloss: 4.64676 |  0:00:05s
epoch 11 | loss: 0.60257 | eval_custom_logloss: 4.8619  |  0:00:05s
epoch 12 | loss: 0.56629 | eval_custom_logloss: 3.86547 |  0:00:06s
epoch 13 | loss: 0.56908 | eval_custom_logloss: 4.32001 |  0:00:06s
epoch 14 | loss: 0.54743 | eval_custom_logloss: 4.17497 |  0:00:07s
epoch 15 | loss: 0.56075 | eval_custom_logloss: 4.58231 |  0:00:07s
epoch 16 | loss: 0.55699 | eval_custom_logloss: 4.55683 |  0:00:08s
epoch 17 | loss: 0.55589 | eval_custom_logloss: 4.1547  |  0:00:08s
epoch 18 | loss: 0.51654 | eval_custom_logloss: 3.82881 |  0:00:09s
epoch 19 | loss: 0.50639 | eval_custom_logloss: 3.95906 |  0:00:09s
epoch 20 | loss: 0.54312 | eval_custom_logloss: 4.80002 |  0:00:10s
epoch 21 | loss: 0.5774  | eval_custom_logloss: 3.58407 |  0:00:10s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.20154
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0192, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11612 | eval_custom_logloss: 2.39477 |  0:00:00s
epoch 1  | loss: 0.89743 | eval_custom_logloss: 3.64006 |  0:00:00s
epoch 2  | loss: 0.89342 | eval_custom_logloss: 3.9739  |  0:00:01s
epoch 3  | loss: 0.91382 | eval_custom_logloss: 4.80449 |  0:00:01s
epoch 4  | loss: 0.8026  | eval_custom_logloss: 4.19592 |  0:00:02s
epoch 5  | loss: 0.73765 | eval_custom_logloss: 3.55127 |  0:00:02s
epoch 6  | loss: 0.69832 | eval_custom_logloss: 4.07809 |  0:00:03s
epoch 7  | loss: 0.64849 | eval_custom_logloss: 6.36443 |  0:00:03s
epoch 8  | loss: 0.61628 | eval_custom_logloss: 7.88551 |  0:00:04s
epoch 9  | loss: 0.62164 | eval_custom_logloss: 7.87871 |  0:00:04s
epoch 10 | loss: 0.58796 | eval_custom_logloss: 6.44909 |  0:00:04s
epoch 11 | loss: 0.59242 | eval_custom_logloss: 6.54138 |  0:00:05s
epoch 12 | loss: 0.59585 | eval_custom_logloss: 6.45111 |  0:00:05s
epoch 13 | loss: 0.5859  | eval_custom_logloss: 7.40793 |  0:00:06s
epoch 14 | loss: 0.58439 | eval_custom_logloss: 6.95881 |  0:00:06s
epoch 15 | loss: 0.63004 | eval_custom_logloss: 4.95846 |  0:00:07s
epoch 16 | loss: 0.58649 | eval_custom_logloss: 5.28863 |  0:00:07s
epoch 17 | loss: 0.55686 | eval_custom_logloss: 4.17118 |  0:00:08s
epoch 18 | loss: 0.55044 | eval_custom_logloss: 5.24702 |  0:00:08s
epoch 19 | loss: 0.54348 | eval_custom_logloss: 4.70988 |  0:00:09s
epoch 20 | loss: 0.57349 | eval_custom_logloss: 5.06051 |  0:00:09s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.39477
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.07095, 'Log Loss - std': 0.05174999999999996} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.15876 | eval_custom_logloss: 2.64812 |  0:00:00s
epoch 1  | loss: 0.93674 | eval_custom_logloss: 2.41626 |  0:00:00s
epoch 2  | loss: 0.93013 | eval_custom_logloss: 2.03574 |  0:00:01s
epoch 3  | loss: 0.83853 | eval_custom_logloss: 2.53063 |  0:00:01s
epoch 4  | loss: 0.78886 | eval_custom_logloss: 3.75101 |  0:00:02s
epoch 5  | loss: 0.73719 | eval_custom_logloss: 3.91105 |  0:00:02s
epoch 6  | loss: 0.77147 | eval_custom_logloss: 4.31754 |  0:00:03s
epoch 7  | loss: 0.70078 | eval_custom_logloss: 4.78531 |  0:00:03s
epoch 8  | loss: 0.71223 | eval_custom_logloss: 5.35639 |  0:00:04s
epoch 9  | loss: 0.63571 | eval_custom_logloss: 6.46071 |  0:00:04s
epoch 10 | loss: 0.62223 | eval_custom_logloss: 5.08266 |  0:00:05s
epoch 11 | loss: 0.60951 | eval_custom_logloss: 5.29869 |  0:00:05s
epoch 12 | loss: 0.59709 | eval_custom_logloss: 6.96818 |  0:00:06s
epoch 13 | loss: 0.59234 | eval_custom_logloss: 8.98726 |  0:00:06s
epoch 14 | loss: 0.56381 | eval_custom_logloss: 6.7295  |  0:00:07s
epoch 15 | loss: 0.56236 | eval_custom_logloss: 6.47096 |  0:00:07s
epoch 16 | loss: 0.57147 | eval_custom_logloss: 7.02554 |  0:00:08s
epoch 17 | loss: 0.5734  | eval_custom_logloss: 6.06284 |  0:00:08s
epoch 18 | loss: 0.52338 | eval_custom_logloss: 5.11906 |  0:00:09s
epoch 19 | loss: 0.5337  | eval_custom_logloss: 5.80341 |  0:00:09s
epoch 20 | loss: 0.52779 | eval_custom_logloss: 5.18671 |  0:00:10s
epoch 21 | loss: 0.49712 | eval_custom_logloss: 5.37599 |  0:00:10s
epoch 22 | loss: 0.49526 | eval_custom_logloss: 5.81337 |  0:00:11s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 2.03574
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0197333333333334, 'Log Loss - std': 0.08385504688979006} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13361 | eval_custom_logloss: 2.70479 |  0:00:00s
epoch 1  | loss: 0.90295 | eval_custom_logloss: 2.31122 |  0:00:00s
epoch 2  | loss: 0.89302 | eval_custom_logloss: 2.42145 |  0:00:01s
epoch 3  | loss: 0.81184 | eval_custom_logloss: 2.6905  |  0:00:01s
epoch 4  | loss: 0.82493 | eval_custom_logloss: 4.06085 |  0:00:02s
epoch 5  | loss: 0.74087 | eval_custom_logloss: 2.75842 |  0:00:02s
epoch 6  | loss: 0.73161 | eval_custom_logloss: 4.23426 |  0:00:03s
epoch 7  | loss: 0.70863 | eval_custom_logloss: 5.72736 |  0:00:03s
epoch 8  | loss: 0.64629 | eval_custom_logloss: 6.46857 |  0:00:04s
epoch 9  | loss: 0.6539  | eval_custom_logloss: 5.30508 |  0:00:04s
epoch 10 | loss: 0.60165 | eval_custom_logloss: 4.26994 |  0:00:05s
epoch 11 | loss: 0.59948 | eval_custom_logloss: 3.93431 |  0:00:05s
epoch 12 | loss: 0.60902 | eval_custom_logloss: 3.92669 |  0:00:06s
epoch 13 | loss: 0.62638 | eval_custom_logloss: 3.45386 |  0:00:06s
epoch 14 | loss: 0.59334 | eval_custom_logloss: 3.68113 |  0:00:07s
epoch 15 | loss: 0.55603 | eval_custom_logloss: 3.83891 |  0:00:07s
epoch 16 | loss: 0.56564 | eval_custom_logloss: 2.83295 |  0:00:08s
epoch 17 | loss: 0.53564 | eval_custom_logloss: 3.28215 |  0:00:08s
epoch 18 | loss: 0.53536 | eval_custom_logloss: 3.96434 |  0:00:09s
epoch 19 | loss: 0.53907 | eval_custom_logloss: 5.29755 |  0:00:09s
epoch 20 | loss: 0.51834 | eval_custom_logloss: 4.00353 |  0:00:10s
epoch 21 | loss: 0.50886 | eval_custom_logloss: 5.33181 |  0:00:10s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.31122
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0518, 'Log Loss - std': 0.09142518799543158} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.1715  | eval_custom_logloss: 2.5632  |  0:00:00s
epoch 1  | loss: 0.95328 | eval_custom_logloss: 3.84896 |  0:00:00s
epoch 2  | loss: 0.88182 | eval_custom_logloss: 3.30361 |  0:00:01s
epoch 3  | loss: 0.82095 | eval_custom_logloss: 2.72122 |  0:00:01s
epoch 4  | loss: 0.76527 | eval_custom_logloss: 3.7197  |  0:00:02s
epoch 5  | loss: 0.70987 | eval_custom_logloss: 4.16202 |  0:00:02s
epoch 6  | loss: 0.7055  | eval_custom_logloss: 3.41528 |  0:00:03s
epoch 7  | loss: 0.70023 | eval_custom_logloss: 4.04326 |  0:00:03s
epoch 8  | loss: 0.66603 | eval_custom_logloss: 4.12251 |  0:00:04s
epoch 9  | loss: 0.66916 | eval_custom_logloss: 4.70721 |  0:00:04s
epoch 10 | loss: 0.61035 | eval_custom_logloss: 3.91329 |  0:00:04s
epoch 11 | loss: 0.56821 | eval_custom_logloss: 4.34338 |  0:00:05s
epoch 12 | loss: 0.54991 | eval_custom_logloss: 5.83158 |  0:00:05s
epoch 13 | loss: 0.54544 | eval_custom_logloss: 6.24892 |  0:00:06s
epoch 14 | loss: 0.55144 | eval_custom_logloss: 7.15169 |  0:00:06s
epoch 15 | loss: 0.56578 | eval_custom_logloss: 4.99277 |  0:00:07s
epoch 16 | loss: 0.53485 | eval_custom_logloss: 4.47487 |  0:00:07s
epoch 17 | loss: 0.52641 | eval_custom_logloss: 5.75721 |  0:00:08s
epoch 18 | loss: 0.50392 | eval_custom_logloss: 5.24956 |  0:00:08s
epoch 19 | loss: 0.49034 | eval_custom_logloss: 5.41443 |  0:00:08s
epoch 20 | loss: 0.50381 | eval_custom_logloss: 6.67101 |  0:00:09s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.5632
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.09802, 'Log Loss - std': 0.12341801165146038} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 36 finished with value: 2.09802 and parameters: {'n_d': 11, 'n_steps': 4, 'gamma': 1.1484899267778907, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001004352752547957, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11176 | eval_custom_logloss: 1.99507 |  0:00:00s
epoch 1  | loss: 0.86614 | eval_custom_logloss: 2.02644 |  0:00:00s
epoch 2  | loss: 0.79881 | eval_custom_logloss: 2.30952 |  0:00:01s
epoch 3  | loss: 0.74082 | eval_custom_logloss: 3.16283 |  0:00:01s
epoch 4  | loss: 0.71892 | eval_custom_logloss: 2.84266 |  0:00:02s
epoch 5  | loss: 0.67731 | eval_custom_logloss: 2.11393 |  0:00:02s
epoch 6  | loss: 0.65117 | eval_custom_logloss: 2.79365 |  0:00:02s
epoch 7  | loss: 0.6436  | eval_custom_logloss: 3.9328  |  0:00:03s
epoch 8  | loss: 0.61061 | eval_custom_logloss: 2.39027 |  0:00:03s
epoch 9  | loss: 0.58865 | eval_custom_logloss: 2.38005 |  0:00:03s
epoch 10 | loss: 0.59329 | eval_custom_logloss: 2.49258 |  0:00:04s
epoch 11 | loss: 0.57365 | eval_custom_logloss: 3.2155  |  0:00:04s
epoch 12 | loss: 0.57021 | eval_custom_logloss: 3.00888 |  0:00:05s
epoch 13 | loss: 0.53185 | eval_custom_logloss: 2.84306 |  0:00:05s
epoch 14 | loss: 0.5213  | eval_custom_logloss: 2.35222 |  0:00:06s
epoch 15 | loss: 0.52902 | eval_custom_logloss: 1.97781 |  0:00:06s
epoch 16 | loss: 0.52652 | eval_custom_logloss: 1.87398 |  0:00:07s
epoch 17 | loss: 0.53861 | eval_custom_logloss: 1.5471  |  0:00:07s
epoch 18 | loss: 0.4872  | eval_custom_logloss: 1.47955 |  0:00:08s
epoch 19 | loss: 0.4905  | eval_custom_logloss: 1.67231 |  0:00:08s
epoch 20 | loss: 0.49004 | eval_custom_logloss: 1.79285 |  0:00:08s
epoch 21 | loss: 0.47567 | eval_custom_logloss: 1.50063 |  0:00:09s
epoch 22 | loss: 0.48876 | eval_custom_logloss: 1.40107 |  0:00:09s
epoch 23 | loss: 0.45837 | eval_custom_logloss: 1.49951 |  0:00:10s
epoch 24 | loss: 0.47252 | eval_custom_logloss: 1.22892 |  0:00:10s
epoch 25 | loss: 0.45658 | eval_custom_logloss: 1.16686 |  0:00:11s
epoch 26 | loss: 0.46825 | eval_custom_logloss: 1.34518 |  0:00:11s
epoch 27 | loss: 0.47006 | eval_custom_logloss: 1.1798  |  0:00:12s
epoch 28 | loss: 0.45074 | eval_custom_logloss: 1.34577 |  0:00:12s
epoch 29 | loss: 0.43264 | eval_custom_logloss: 1.22759 |  0:00:13s
epoch 30 | loss: 0.43225 | eval_custom_logloss: 0.98923 |  0:00:13s
epoch 31 | loss: 0.41495 | eval_custom_logloss: 1.09201 |  0:00:14s
epoch 32 | loss: 0.43017 | eval_custom_logloss: 0.97744 |  0:00:14s
epoch 33 | loss: 0.44528 | eval_custom_logloss: 0.80549 |  0:00:15s
epoch 34 | loss: 0.44612 | eval_custom_logloss: 0.73139 |  0:00:15s
epoch 35 | loss: 0.45412 | eval_custom_logloss: 0.80101 |  0:00:16s
epoch 36 | loss: 0.45909 | eval_custom_logloss: 0.93743 |  0:00:16s
epoch 37 | loss: 0.42935 | eval_custom_logloss: 0.87217 |  0:00:17s
epoch 38 | loss: 0.47437 | eval_custom_logloss: 0.78097 |  0:00:17s
epoch 39 | loss: 0.45705 | eval_custom_logloss: 0.96823 |  0:00:17s
epoch 40 | loss: 0.44883 | eval_custom_logloss: 1.5458  |  0:00:18s
epoch 41 | loss: 0.41779 | eval_custom_logloss: 1.54009 |  0:00:18s
epoch 42 | loss: 0.41005 | eval_custom_logloss: 1.48911 |  0:00:19s
epoch 43 | loss: 0.40519 | eval_custom_logloss: 1.50948 |  0:00:19s
epoch 44 | loss: 0.40338 | eval_custom_logloss: 1.18984 |  0:00:20s
epoch 45 | loss: 0.40226 | eval_custom_logloss: 1.30834 |  0:00:20s
epoch 46 | loss: 0.41547 | eval_custom_logloss: 1.17731 |  0:00:20s
epoch 47 | loss: 0.40081 | eval_custom_logloss: 0.96845 |  0:00:21s
epoch 48 | loss: 0.38824 | eval_custom_logloss: 1.12576 |  0:00:21s
epoch 49 | loss: 0.38843 | eval_custom_logloss: 1.06533 |  0:00:22s
epoch 50 | loss: 0.34657 | eval_custom_logloss: 1.14977 |  0:00:22s
epoch 51 | loss: 0.35677 | eval_custom_logloss: 0.94179 |  0:00:23s
epoch 52 | loss: 0.37391 | eval_custom_logloss: 0.73382 |  0:00:23s
epoch 53 | loss: 0.35041 | eval_custom_logloss: 0.85866 |  0:00:24s
epoch 54 | loss: 0.36622 | eval_custom_logloss: 0.85403 |  0:00:24s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 0.73139
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7312, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.07292 | eval_custom_logloss: 2.00793 |  0:00:00s
epoch 1  | loss: 0.93965 | eval_custom_logloss: 2.23389 |  0:00:00s
epoch 2  | loss: 0.84251 | eval_custom_logloss: 2.2659  |  0:00:01s
epoch 3  | loss: 0.75166 | eval_custom_logloss: 2.0489  |  0:00:01s
epoch 4  | loss: 0.69029 | eval_custom_logloss: 2.70928 |  0:00:02s
epoch 5  | loss: 0.66424 | eval_custom_logloss: 2.91119 |  0:00:02s
epoch 6  | loss: 0.64585 | eval_custom_logloss: 2.89554 |  0:00:03s
epoch 7  | loss: 0.61809 | eval_custom_logloss: 2.92902 |  0:00:03s
epoch 8  | loss: 0.5864  | eval_custom_logloss: 4.61792 |  0:00:03s
epoch 9  | loss: 0.56756 | eval_custom_logloss: 3.02601 |  0:00:04s
epoch 10 | loss: 0.53124 | eval_custom_logloss: 3.50733 |  0:00:04s
epoch 11 | loss: 0.5095  | eval_custom_logloss: 3.67181 |  0:00:05s
epoch 12 | loss: 0.49731 | eval_custom_logloss: 4.2048  |  0:00:05s
epoch 13 | loss: 0.4987  | eval_custom_logloss: 4.06393 |  0:00:06s
epoch 14 | loss: 0.47288 | eval_custom_logloss: 3.98858 |  0:00:06s
epoch 15 | loss: 0.45548 | eval_custom_logloss: 4.26866 |  0:00:06s
epoch 16 | loss: 0.47743 | eval_custom_logloss: 2.67715 |  0:00:07s
epoch 17 | loss: 0.48052 | eval_custom_logloss: 3.32109 |  0:00:07s
epoch 18 | loss: 0.46243 | eval_custom_logloss: 2.71738 |  0:00:08s
epoch 19 | loss: 0.48323 | eval_custom_logloss: 2.58443 |  0:00:08s
epoch 20 | loss: 0.45861 | eval_custom_logloss: 1.93565 |  0:00:09s
epoch 21 | loss: 0.42855 | eval_custom_logloss: 2.18541 |  0:00:09s
epoch 22 | loss: 0.43327 | eval_custom_logloss: 1.53799 |  0:00:10s
epoch 23 | loss: 0.45346 | eval_custom_logloss: 1.60527 |  0:00:10s
epoch 24 | loss: 0.45434 | eval_custom_logloss: 2.0063  |  0:00:11s
epoch 25 | loss: 0.49776 | eval_custom_logloss: 1.96548 |  0:00:11s
epoch 26 | loss: 0.48253 | eval_custom_logloss: 1.64118 |  0:00:11s
epoch 27 | loss: 0.46649 | eval_custom_logloss: 1.92968 |  0:00:12s
epoch 28 | loss: 0.43166 | eval_custom_logloss: 1.47696 |  0:00:12s
epoch 29 | loss: 0.46022 | eval_custom_logloss: 1.26829 |  0:00:13s
epoch 30 | loss: 0.42198 | eval_custom_logloss: 1.57236 |  0:00:13s
epoch 31 | loss: 0.38109 | eval_custom_logloss: 1.18361 |  0:00:14s
epoch 32 | loss: 0.3548  | eval_custom_logloss: 1.09649 |  0:00:14s
epoch 33 | loss: 0.36669 | eval_custom_logloss: 1.18631 |  0:00:15s
epoch 34 | loss: 0.3433  | eval_custom_logloss: 1.61406 |  0:00:15s
epoch 35 | loss: 0.38797 | eval_custom_logloss: 1.54408 |  0:00:15s
epoch 36 | loss: 0.4166  | eval_custom_logloss: 1.19051 |  0:00:16s
epoch 37 | loss: 0.37271 | eval_custom_logloss: 1.11824 |  0:00:16s
epoch 38 | loss: 0.35161 | eval_custom_logloss: 1.087   |  0:00:17s
epoch 39 | loss: 0.33526 | eval_custom_logloss: 1.50648 |  0:00:17s
epoch 40 | loss: 0.32086 | eval_custom_logloss: 1.17487 |  0:00:18s
epoch 41 | loss: 0.31037 | eval_custom_logloss: 0.9363  |  0:00:18s
epoch 42 | loss: 0.32063 | eval_custom_logloss: 1.03033 |  0:00:19s
epoch 43 | loss: 0.34028 | eval_custom_logloss: 1.06511 |  0:00:19s
epoch 44 | loss: 0.34102 | eval_custom_logloss: 0.93842 |  0:00:19s
epoch 45 | loss: 0.34182 | eval_custom_logloss: 0.94941 |  0:00:20s
epoch 46 | loss: 0.34716 | eval_custom_logloss: 1.09309 |  0:00:20s
epoch 47 | loss: 0.31446 | eval_custom_logloss: 0.97858 |  0:00:21s
epoch 48 | loss: 0.30733 | eval_custom_logloss: 1.0176  |  0:00:21s
epoch 49 | loss: 0.31267 | eval_custom_logloss: 1.02915 |  0:00:22s
epoch 50 | loss: 0.27152 | eval_custom_logloss: 1.11742 |  0:00:23s
epoch 51 | loss: 0.28492 | eval_custom_logloss: 1.33496 |  0:00:23s
epoch 52 | loss: 0.30679 | eval_custom_logloss: 1.21747 |  0:00:24s
epoch 53 | loss: 0.30916 | eval_custom_logloss: 1.09644 |  0:00:24s
epoch 54 | loss: 0.28959 | eval_custom_logloss: 1.03963 |  0:00:25s
epoch 55 | loss: 0.26825 | eval_custom_logloss: 1.04511 |  0:00:25s
epoch 56 | loss: 0.28997 | eval_custom_logloss: 0.97136 |  0:00:26s
epoch 57 | loss: 0.28296 | eval_custom_logloss: 1.10222 |  0:00:26s
epoch 58 | loss: 0.32719 | eval_custom_logloss: 1.11044 |  0:00:27s
epoch 59 | loss: 0.29539 | eval_custom_logloss: 0.86846 |  0:00:27s
epoch 60 | loss: 0.27036 | eval_custom_logloss: 0.83839 |  0:00:27s
epoch 61 | loss: 0.25589 | eval_custom_logloss: 0.84922 |  0:00:28s
epoch 62 | loss: 0.25363 | eval_custom_logloss: 0.91905 |  0:00:28s
epoch 63 | loss: 0.24623 | eval_custom_logloss: 0.97726 |  0:00:29s
epoch 64 | loss: 0.22254 | eval_custom_logloss: 0.75111 |  0:00:29s
epoch 65 | loss: 0.25085 | eval_custom_logloss: 1.04196 |  0:00:30s
epoch 66 | loss: 0.25992 | eval_custom_logloss: 0.77104 |  0:00:30s
epoch 67 | loss: 0.25736 | eval_custom_logloss: 0.79631 |  0:00:30s
epoch 68 | loss: 0.21975 | eval_custom_logloss: 0.96853 |  0:00:31s
epoch 69 | loss: 0.21895 | eval_custom_logloss: 1.17568 |  0:00:31s
epoch 70 | loss: 0.2408  | eval_custom_logloss: 0.91524 |  0:00:32s
epoch 71 | loss: 0.24329 | eval_custom_logloss: 0.85399 |  0:00:32s
epoch 72 | loss: 0.20786 | eval_custom_logloss: 0.86986 |  0:00:33s
epoch 73 | loss: 0.23706 | eval_custom_logloss: 1.10076 |  0:00:33s
epoch 74 | loss: 0.24027 | eval_custom_logloss: 1.11249 |  0:00:33s
epoch 75 | loss: 0.24559 | eval_custom_logloss: 1.0126  |  0:00:34s
epoch 76 | loss: 0.27001 | eval_custom_logloss: 1.0024  |  0:00:34s
epoch 77 | loss: 0.23479 | eval_custom_logloss: 0.92398 |  0:00:35s
epoch 78 | loss: 0.21365 | eval_custom_logloss: 0.89397 |  0:00:35s
epoch 79 | loss: 0.19464 | eval_custom_logloss: 0.86667 |  0:00:36s
epoch 80 | loss: 0.19051 | eval_custom_logloss: 0.82759 |  0:00:36s
epoch 81 | loss: 0.20183 | eval_custom_logloss: 0.86446 |  0:00:36s
epoch 82 | loss: 0.19647 | eval_custom_logloss: 0.75251 |  0:00:37s
epoch 83 | loss: 0.1912  | eval_custom_logloss: 0.77972 |  0:00:37s
epoch 84 | loss: 0.1753  | eval_custom_logloss: 0.95486 |  0:00:38s

Early stopping occurred at epoch 84 with best_epoch = 64 and best_eval_custom_logloss = 0.75111
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7293499999999999, 'Log Loss - std': 0.0018499999999999628} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.04554 | eval_custom_logloss: 2.33884 |  0:00:00s
epoch 1  | loss: 0.936   | eval_custom_logloss: 1.84038 |  0:00:00s
epoch 2  | loss: 0.87178 | eval_custom_logloss: 2.05313 |  0:00:01s
epoch 3  | loss: 0.78929 | eval_custom_logloss: 2.53831 |  0:00:01s
epoch 4  | loss: 0.79069 | eval_custom_logloss: 2.94377 |  0:00:02s
epoch 5  | loss: 0.77334 | eval_custom_logloss: 2.70959 |  0:00:02s
epoch 6  | loss: 0.72991 | eval_custom_logloss: 2.39729 |  0:00:03s
epoch 7  | loss: 0.71166 | eval_custom_logloss: 3.10873 |  0:00:03s
epoch 8  | loss: 0.65682 | eval_custom_logloss: 3.36697 |  0:00:04s
epoch 9  | loss: 0.66772 | eval_custom_logloss: 2.84433 |  0:00:04s
epoch 10 | loss: 0.64837 | eval_custom_logloss: 2.11813 |  0:00:04s
epoch 11 | loss: 0.62229 | eval_custom_logloss: 2.613   |  0:00:05s
epoch 12 | loss: 0.59856 | eval_custom_logloss: 3.17965 |  0:00:05s
epoch 13 | loss: 0.58595 | eval_custom_logloss: 2.06506 |  0:00:06s
epoch 14 | loss: 0.546   | eval_custom_logloss: 2.13283 |  0:00:06s
epoch 15 | loss: 0.53962 | eval_custom_logloss: 2.05015 |  0:00:07s
epoch 16 | loss: 0.59709 | eval_custom_logloss: 1.36115 |  0:00:07s
epoch 17 | loss: 0.53891 | eval_custom_logloss: 1.45492 |  0:00:07s
epoch 18 | loss: 0.49321 | eval_custom_logloss: 1.39458 |  0:00:08s
epoch 19 | loss: 0.50537 | eval_custom_logloss: 1.46384 |  0:00:08s
epoch 20 | loss: 0.50141 | eval_custom_logloss: 1.32631 |  0:00:09s
epoch 21 | loss: 0.50709 | eval_custom_logloss: 1.1236  |  0:00:09s
epoch 22 | loss: 0.5129  | eval_custom_logloss: 1.11424 |  0:00:10s
epoch 23 | loss: 0.50491 | eval_custom_logloss: 1.05475 |  0:00:10s
epoch 24 | loss: 0.48859 | eval_custom_logloss: 1.26408 |  0:00:11s
epoch 25 | loss: 0.50476 | eval_custom_logloss: 1.42868 |  0:00:11s
epoch 26 | loss: 0.49712 | eval_custom_logloss: 1.12435 |  0:00:11s
epoch 27 | loss: 0.52096 | eval_custom_logloss: 1.14399 |  0:00:12s
epoch 28 | loss: 0.51626 | eval_custom_logloss: 1.13417 |  0:00:12s
epoch 29 | loss: 0.51998 | eval_custom_logloss: 1.08963 |  0:00:13s
epoch 30 | loss: 0.48676 | eval_custom_logloss: 1.24766 |  0:00:13s
epoch 31 | loss: 0.48032 | eval_custom_logloss: 0.99118 |  0:00:14s
epoch 32 | loss: 0.46713 | eval_custom_logloss: 1.00502 |  0:00:14s
epoch 33 | loss: 0.45246 | eval_custom_logloss: 0.77822 |  0:00:14s
epoch 34 | loss: 0.45134 | eval_custom_logloss: 0.83931 |  0:00:15s
epoch 35 | loss: 0.4532  | eval_custom_logloss: 0.82765 |  0:00:15s
epoch 36 | loss: 0.4563  | eval_custom_logloss: 1.00856 |  0:00:16s
epoch 37 | loss: 0.48985 | eval_custom_logloss: 1.18249 |  0:00:16s
epoch 38 | loss: 0.46988 | eval_custom_logloss: 1.0832  |  0:00:17s
epoch 39 | loss: 0.4748  | eval_custom_logloss: 0.90412 |  0:00:17s
epoch 40 | loss: 0.44049 | eval_custom_logloss: 1.03494 |  0:00:18s
epoch 41 | loss: 0.42713 | eval_custom_logloss: 0.97311 |  0:00:18s
epoch 42 | loss: 0.42614 | eval_custom_logloss: 0.85715 |  0:00:18s
epoch 43 | loss: 0.43192 | eval_custom_logloss: 0.7848  |  0:00:19s
epoch 44 | loss: 0.42371 | eval_custom_logloss: 0.86165 |  0:00:19s
epoch 45 | loss: 0.37235 | eval_custom_logloss: 0.81942 |  0:00:20s
epoch 46 | loss: 0.38088 | eval_custom_logloss: 0.87955 |  0:00:20s
epoch 47 | loss: 0.36508 | eval_custom_logloss: 0.89696 |  0:00:21s
epoch 48 | loss: 0.35393 | eval_custom_logloss: 0.83604 |  0:00:21s
epoch 49 | loss: 0.36658 | eval_custom_logloss: 0.89469 |  0:00:21s
epoch 50 | loss: 0.35977 | eval_custom_logloss: 0.80802 |  0:00:22s
epoch 51 | loss: 0.36671 | eval_custom_logloss: 0.86475 |  0:00:22s
epoch 52 | loss: 0.3416  | eval_custom_logloss: 0.92719 |  0:00:23s
epoch 53 | loss: 0.3435  | eval_custom_logloss: 0.83276 |  0:00:23s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.77822
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7383666666666665, 'Log Loss - std': 0.012840647266482392} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.07845 | eval_custom_logloss: 2.71312 |  0:00:00s
epoch 1  | loss: 0.9112  | eval_custom_logloss: 1.7236  |  0:00:00s
epoch 2  | loss: 0.80731 | eval_custom_logloss: 2.13592 |  0:00:01s
epoch 3  | loss: 0.76995 | eval_custom_logloss: 1.51072 |  0:00:01s
epoch 4  | loss: 0.71253 | eval_custom_logloss: 2.13576 |  0:00:02s
epoch 5  | loss: 0.67843 | eval_custom_logloss: 3.20938 |  0:00:02s
epoch 6  | loss: 0.63434 | eval_custom_logloss: 3.2889  |  0:00:03s
epoch 7  | loss: 0.65325 | eval_custom_logloss: 3.04794 |  0:00:03s
epoch 8  | loss: 0.6101  | eval_custom_logloss: 3.03585 |  0:00:03s
epoch 9  | loss: 0.63834 | eval_custom_logloss: 2.99673 |  0:00:04s
epoch 10 | loss: 0.56779 | eval_custom_logloss: 2.08779 |  0:00:04s
epoch 11 | loss: 0.57069 | eval_custom_logloss: 2.22501 |  0:00:05s
epoch 12 | loss: 0.57002 | eval_custom_logloss: 3.25034 |  0:00:05s
epoch 13 | loss: 0.56204 | eval_custom_logloss: 3.03404 |  0:00:06s
epoch 14 | loss: 0.53258 | eval_custom_logloss: 3.03642 |  0:00:06s
epoch 15 | loss: 0.56196 | eval_custom_logloss: 3.11209 |  0:00:06s
epoch 16 | loss: 0.55802 | eval_custom_logloss: 3.85771 |  0:00:07s
epoch 17 | loss: 0.55279 | eval_custom_logloss: 3.20602 |  0:00:07s
epoch 18 | loss: 0.53725 | eval_custom_logloss: 2.97139 |  0:00:08s
epoch 19 | loss: 0.52308 | eval_custom_logloss: 2.29658 |  0:00:08s
epoch 20 | loss: 0.50673 | eval_custom_logloss: 3.11511 |  0:00:09s
epoch 21 | loss: 0.52289 | eval_custom_logloss: 2.46034 |  0:00:09s
epoch 22 | loss: 0.48498 | eval_custom_logloss: 2.58742 |  0:00:09s
epoch 23 | loss: 0.45984 | eval_custom_logloss: 3.13736 |  0:00:10s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 1.51072
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9099249999999999, 'Log Loss - std': 0.2973557580996205} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.08568 | eval_custom_logloss: 2.53302 |  0:00:00s
epoch 1  | loss: 0.91568 | eval_custom_logloss: 2.30502 |  0:00:00s
epoch 2  | loss: 0.84213 | eval_custom_logloss: 2.28759 |  0:00:01s
epoch 3  | loss: 0.77067 | eval_custom_logloss: 1.6719  |  0:00:01s
epoch 4  | loss: 0.69014 | eval_custom_logloss: 2.0689  |  0:00:02s
epoch 5  | loss: 0.68176 | eval_custom_logloss: 2.75961 |  0:00:02s
epoch 6  | loss: 0.61639 | eval_custom_logloss: 1.46771 |  0:00:03s
epoch 7  | loss: 0.60432 | eval_custom_logloss: 1.58525 |  0:00:03s
epoch 8  | loss: 0.58953 | eval_custom_logloss: 1.05971 |  0:00:03s
epoch 9  | loss: 0.55535 | eval_custom_logloss: 1.59587 |  0:00:04s
epoch 10 | loss: 0.57172 | eval_custom_logloss: 1.71444 |  0:00:04s
epoch 11 | loss: 0.55392 | eval_custom_logloss: 1.93943 |  0:00:05s
epoch 12 | loss: 0.56097 | eval_custom_logloss: 1.96271 |  0:00:05s
epoch 13 | loss: 0.50703 | eval_custom_logloss: 1.91693 |  0:00:05s
epoch 14 | loss: 0.49111 | eval_custom_logloss: 1.82083 |  0:00:06s
epoch 15 | loss: 0.49229 | eval_custom_logloss: 1.59303 |  0:00:06s
epoch 16 | loss: 0.47347 | eval_custom_logloss: 1.76176 |  0:00:07s
epoch 17 | loss: 0.46646 | eval_custom_logloss: 1.72259 |  0:00:07s
epoch 18 | loss: 0.4713  | eval_custom_logloss: 1.51529 |  0:00:08s
epoch 19 | loss: 0.45766 | eval_custom_logloss: 1.50961 |  0:00:08s
epoch 20 | loss: 0.47078 | eval_custom_logloss: 1.55034 |  0:00:08s
epoch 21 | loss: 0.43368 | eval_custom_logloss: 1.62419 |  0:00:09s
epoch 22 | loss: 0.42501 | eval_custom_logloss: 1.5339  |  0:00:09s
epoch 23 | loss: 0.42583 | eval_custom_logloss: 1.26221 |  0:00:10s
epoch 24 | loss: 0.38926 | eval_custom_logloss: 1.30742 |  0:00:10s
epoch 25 | loss: 0.40404 | eval_custom_logloss: 1.11958 |  0:00:11s
epoch 26 | loss: 0.41712 | eval_custom_logloss: 0.84157 |  0:00:11s
epoch 27 | loss: 0.45423 | eval_custom_logloss: 1.52237 |  0:00:12s
epoch 28 | loss: 0.42175 | eval_custom_logloss: 1.4576  |  0:00:12s
epoch 29 | loss: 0.40839 | eval_custom_logloss: 1.73595 |  0:00:13s
epoch 30 | loss: 0.41814 | eval_custom_logloss: 1.28868 |  0:00:13s
epoch 31 | loss: 0.41139 | eval_custom_logloss: 1.2161  |  0:00:14s
epoch 32 | loss: 0.38066 | eval_custom_logloss: 0.8647  |  0:00:14s
epoch 33 | loss: 0.39385 | eval_custom_logloss: 0.94729 |  0:00:15s
epoch 34 | loss: 0.36698 | eval_custom_logloss: 0.8354  |  0:00:15s
epoch 35 | loss: 0.36944 | eval_custom_logloss: 0.7152  |  0:00:16s
epoch 36 | loss: 0.37983 | eval_custom_logloss: 1.25662 |  0:00:16s
epoch 37 | loss: 0.38075 | eval_custom_logloss: 0.76486 |  0:00:17s
epoch 38 | loss: 0.38458 | eval_custom_logloss: 0.9486  |  0:00:17s
epoch 39 | loss: 0.36712 | eval_custom_logloss: 1.40745 |  0:00:18s
epoch 40 | loss: 0.35223 | eval_custom_logloss: 1.45485 |  0:00:18s
epoch 41 | loss: 0.37542 | eval_custom_logloss: 1.43413 |  0:00:19s
epoch 42 | loss: 0.37036 | eval_custom_logloss: 1.0144  |  0:00:19s
epoch 43 | loss: 0.36301 | eval_custom_logloss: 1.04832 |  0:00:20s
epoch 44 | loss: 0.33274 | eval_custom_logloss: 0.92387 |  0:00:20s
epoch 45 | loss: 0.35281 | eval_custom_logloss: 0.85541 |  0:00:21s
epoch 46 | loss: 0.31381 | eval_custom_logloss: 0.93593 |  0:00:21s
epoch 47 | loss: 0.35144 | eval_custom_logloss: 0.73707 |  0:00:21s
epoch 48 | loss: 0.32153 | eval_custom_logloss: 0.60454 |  0:00:22s
epoch 49 | loss: 0.33777 | eval_custom_logloss: 0.66412 |  0:00:22s
epoch 50 | loss: 0.32013 | eval_custom_logloss: 0.65594 |  0:00:23s
epoch 51 | loss: 0.31536 | eval_custom_logloss: 0.85295 |  0:00:23s
epoch 52 | loss: 0.28547 | eval_custom_logloss: 0.73712 |  0:00:24s
epoch 53 | loss: 0.29791 | eval_custom_logloss: 0.61508 |  0:00:24s
epoch 54 | loss: 0.34384 | eval_custom_logloss: 0.77815 |  0:00:24s
epoch 55 | loss: 0.34909 | eval_custom_logloss: 0.72693 |  0:00:25s
epoch 56 | loss: 0.29178 | eval_custom_logloss: 0.62108 |  0:00:25s
epoch 57 | loss: 0.30955 | eval_custom_logloss: 0.76436 |  0:00:26s
epoch 58 | loss: 0.33825 | eval_custom_logloss: 0.68125 |  0:00:26s
epoch 59 | loss: 0.30611 | eval_custom_logloss: 0.62105 |  0:00:27s
epoch 60 | loss: 0.27249 | eval_custom_logloss: 0.6897  |  0:00:27s
epoch 61 | loss: 0.26296 | eval_custom_logloss: 0.64776 |  0:00:27s
epoch 62 | loss: 0.25082 | eval_custom_logloss: 0.6996  |  0:00:28s
epoch 63 | loss: 0.25033 | eval_custom_logloss: 0.56437 |  0:00:28s
epoch 64 | loss: 0.25669 | eval_custom_logloss: 0.6326  |  0:00:29s
epoch 65 | loss: 0.2687  | eval_custom_logloss: 0.62682 |  0:00:29s
epoch 66 | loss: 0.29442 | eval_custom_logloss: 0.59032 |  0:00:29s
epoch 67 | loss: 0.285   | eval_custom_logloss: 0.57152 |  0:00:30s
epoch 68 | loss: 0.26243 | eval_custom_logloss: 0.74781 |  0:00:30s
epoch 69 | loss: 0.24895 | eval_custom_logloss: 0.66324 |  0:00:31s
epoch 70 | loss: 0.23687 | eval_custom_logloss: 0.53575 |  0:00:31s
epoch 71 | loss: 0.24323 | eval_custom_logloss: 0.63634 |  0:00:32s
epoch 72 | loss: 0.24248 | eval_custom_logloss: 0.5818  |  0:00:32s
epoch 73 | loss: 0.25006 | eval_custom_logloss: 0.51502 |  0:00:32s
epoch 74 | loss: 0.24154 | eval_custom_logloss: 0.53648 |  0:00:33s
epoch 75 | loss: 0.22272 | eval_custom_logloss: 0.49412 |  0:00:33s
epoch 76 | loss: 0.24267 | eval_custom_logloss: 0.59755 |  0:00:34s
epoch 77 | loss: 0.23572 | eval_custom_logloss: 0.57412 |  0:00:34s
epoch 78 | loss: 0.26128 | eval_custom_logloss: 0.5406  |  0:00:35s
epoch 79 | loss: 0.21787 | eval_custom_logloss: 0.47373 |  0:00:35s
epoch 80 | loss: 0.2067  | eval_custom_logloss: 0.55044 |  0:00:36s
epoch 81 | loss: 0.21501 | eval_custom_logloss: 0.64311 |  0:00:36s
epoch 82 | loss: 0.2162  | eval_custom_logloss: 0.52657 |  0:00:37s
epoch 83 | loss: 0.21117 | eval_custom_logloss: 0.56325 |  0:00:37s
epoch 84 | loss: 0.21384 | eval_custom_logloss: 0.5381  |  0:00:38s
epoch 85 | loss: 0.20278 | eval_custom_logloss: 0.47823 |  0:00:38s
epoch 86 | loss: 0.2048  | eval_custom_logloss: 0.5063  |  0:00:38s
epoch 87 | loss: 0.22264 | eval_custom_logloss: 0.59935 |  0:00:39s
epoch 88 | loss: 0.20277 | eval_custom_logloss: 0.54973 |  0:00:39s
epoch 89 | loss: 0.19557 | eval_custom_logloss: 0.48817 |  0:00:40s
epoch 90 | loss: 0.18077 | eval_custom_logloss: 0.52916 |  0:00:40s
epoch 91 | loss: 0.1909  | eval_custom_logloss: 0.56846 |  0:00:40s
epoch 92 | loss: 0.22096 | eval_custom_logloss: 0.51238 |  0:00:41s
epoch 93 | loss: 0.23439 | eval_custom_logloss: 0.55465 |  0:00:41s
epoch 94 | loss: 0.24542 | eval_custom_logloss: 0.57282 |  0:00:42s
epoch 95 | loss: 0.21714 | eval_custom_logloss: 0.5238  |  0:00:42s
epoch 96 | loss: 0.17993 | eval_custom_logloss: 0.50886 |  0:00:42s
epoch 97 | loss: 0.19473 | eval_custom_logloss: 0.51406 |  0:00:43s
epoch 98 | loss: 0.1899  | eval_custom_logloss: 0.45871 |  0:00:43s
epoch 99 | loss: 0.19868 | eval_custom_logloss: 0.48078 |  0:00:44s
Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_eval_custom_logloss = 0.45871
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8161999999999999, 'Log Loss - std': 0.32538263629148995} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 37 finished with value: 0.8161999999999999 and parameters: {'n_d': 14, 'n_steps': 4, 'gamma': 1.1664226240575517, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.003063147368002127, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax', 'n_a': 21, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.1071  | eval_custom_logloss: 1.29679 |  0:00:00s
epoch 1  | loss: 0.86892 | eval_custom_logloss: 1.29963 |  0:00:00s
epoch 2  | loss: 0.86907 | eval_custom_logloss: 1.09324 |  0:00:01s
epoch 3  | loss: 0.69557 | eval_custom_logloss: 1.38848 |  0:00:01s
epoch 4  | loss: 0.65349 | eval_custom_logloss: 1.61632 |  0:00:02s
epoch 5  | loss: 0.63822 | eval_custom_logloss: 1.38755 |  0:00:02s
epoch 6  | loss: 0.60714 | eval_custom_logloss: 1.50114 |  0:00:02s
epoch 7  | loss: 0.60728 | eval_custom_logloss: 1.9316  |  0:00:03s
epoch 8  | loss: 0.57973 | eval_custom_logloss: 1.70206 |  0:00:03s
epoch 9  | loss: 0.57788 | eval_custom_logloss: 1.61741 |  0:00:04s
epoch 10 | loss: 0.54684 | eval_custom_logloss: 1.5166  |  0:00:04s
epoch 11 | loss: 0.51881 | eval_custom_logloss: 1.65837 |  0:00:04s
epoch 12 | loss: 0.50672 | eval_custom_logloss: 1.67026 |  0:00:05s
epoch 13 | loss: 0.51736 | eval_custom_logloss: 1.25009 |  0:00:05s
epoch 14 | loss: 0.49615 | eval_custom_logloss: 1.4535  |  0:00:05s
epoch 15 | loss: 0.49457 | eval_custom_logloss: 1.31912 |  0:00:06s
epoch 16 | loss: 0.48554 | eval_custom_logloss: 1.45958 |  0:00:06s
epoch 17 | loss: 0.46046 | eval_custom_logloss: 1.26695 |  0:00:07s
epoch 18 | loss: 0.44686 | eval_custom_logloss: 1.24212 |  0:00:07s
epoch 19 | loss: 0.42747 | eval_custom_logloss: 1.37867 |  0:00:07s
epoch 20 | loss: 0.44155 | eval_custom_logloss: 1.22032 |  0:00:08s
epoch 21 | loss: 0.41041 | eval_custom_logloss: 1.17905 |  0:00:08s
epoch 22 | loss: 0.37859 | eval_custom_logloss: 1.27269 |  0:00:09s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 1.09324
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0559, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax', 'n_a': 21, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.02838 | eval_custom_logloss: 1.39924 |  0:00:00s
epoch 1  | loss: 0.88843 | eval_custom_logloss: 1.1712  |  0:00:00s
epoch 2  | loss: 0.716   | eval_custom_logloss: 1.15342 |  0:00:01s
epoch 3  | loss: 0.71487 | eval_custom_logloss: 1.27613 |  0:00:01s
epoch 4  | loss: 0.67313 | eval_custom_logloss: 1.27615 |  0:00:01s
epoch 5  | loss: 0.63949 | eval_custom_logloss: 1.4688  |  0:00:02s
epoch 6  | loss: 0.57489 | eval_custom_logloss: 1.84775 |  0:00:02s
epoch 7  | loss: 0.5672  | eval_custom_logloss: 2.3616  |  0:00:03s
epoch 8  | loss: 0.53474 | eval_custom_logloss: 2.58059 |  0:00:03s
epoch 9  | loss: 0.51068 | eval_custom_logloss: 2.4381  |  0:00:03s
epoch 10 | loss: 0.55243 | eval_custom_logloss: 1.71385 |  0:00:04s
epoch 11 | loss: 0.56459 | eval_custom_logloss: 1.55209 |  0:00:04s
epoch 12 | loss: 0.48631 | eval_custom_logloss: 1.44614 |  0:00:04s
epoch 13 | loss: 0.49641 | eval_custom_logloss: 1.61459 |  0:00:05s
epoch 14 | loss: 0.43463 | eval_custom_logloss: 1.30527 |  0:00:05s
epoch 15 | loss: 0.45126 | eval_custom_logloss: 1.46394 |  0:00:06s
epoch 16 | loss: 0.44312 | eval_custom_logloss: 1.42257 |  0:00:06s
epoch 17 | loss: 0.44334 | eval_custom_logloss: 1.35137 |  0:00:06s
epoch 18 | loss: 0.45826 | eval_custom_logloss: 1.38369 |  0:00:07s
epoch 19 | loss: 0.43232 | eval_custom_logloss: 1.48429 |  0:00:07s
epoch 20 | loss: 0.43109 | eval_custom_logloss: 1.57887 |  0:00:07s
epoch 21 | loss: 0.41277 | eval_custom_logloss: 1.3104  |  0:00:08s
epoch 22 | loss: 0.39277 | eval_custom_logloss: 1.21597 |  0:00:08s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 1.15342
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.10465, 'Log Loss - std': 0.04874999999999996} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax', 'n_a': 21, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.19194 | eval_custom_logloss: 1.36926 |  0:00:00s
epoch 1  | loss: 0.8831  | eval_custom_logloss: 1.00336 |  0:00:00s
epoch 2  | loss: 0.78599 | eval_custom_logloss: 1.28118 |  0:00:01s
epoch 3  | loss: 0.79083 | eval_custom_logloss: 1.37935 |  0:00:01s
epoch 4  | loss: 0.69375 | eval_custom_logloss: 1.25737 |  0:00:01s
epoch 5  | loss: 0.65099 | eval_custom_logloss: 1.2103  |  0:00:02s
epoch 6  | loss: 0.62601 | eval_custom_logloss: 1.17457 |  0:00:02s
epoch 7  | loss: 0.62216 | eval_custom_logloss: 1.09368 |  0:00:03s
epoch 8  | loss: 0.56211 | eval_custom_logloss: 1.04563 |  0:00:03s
epoch 9  | loss: 0.55023 | eval_custom_logloss: 1.39584 |  0:00:03s
epoch 10 | loss: 0.53103 | eval_custom_logloss: 1.58614 |  0:00:04s
epoch 11 | loss: 0.53911 | eval_custom_logloss: 1.21314 |  0:00:04s
epoch 12 | loss: 0.51669 | eval_custom_logloss: 1.29754 |  0:00:04s
epoch 13 | loss: 0.49449 | eval_custom_logloss: 1.12706 |  0:00:05s
epoch 14 | loss: 0.47967 | eval_custom_logloss: 1.07364 |  0:00:05s
epoch 15 | loss: 0.48295 | eval_custom_logloss: 0.93011 |  0:00:05s
epoch 16 | loss: 0.47831 | eval_custom_logloss: 1.13671 |  0:00:06s
epoch 17 | loss: 0.49328 | eval_custom_logloss: 0.97529 |  0:00:06s
epoch 18 | loss: 0.4762  | eval_custom_logloss: 0.98401 |  0:00:07s
epoch 19 | loss: 0.45979 | eval_custom_logloss: 0.88983 |  0:00:07s
epoch 20 | loss: 0.44281 | eval_custom_logloss: 0.79225 |  0:00:07s
epoch 21 | loss: 0.43944 | eval_custom_logloss: 0.90136 |  0:00:08s
epoch 22 | loss: 0.39528 | eval_custom_logloss: 0.82267 |  0:00:08s
epoch 23 | loss: 0.42814 | eval_custom_logloss: 0.83615 |  0:00:08s
epoch 24 | loss: 0.40817 | eval_custom_logloss: 0.80375 |  0:00:09s
epoch 25 | loss: 0.42411 | eval_custom_logloss: 0.86228 |  0:00:09s
epoch 26 | loss: 0.39726 | eval_custom_logloss: 0.76845 |  0:00:10s
epoch 27 | loss: 0.39034 | eval_custom_logloss: 0.74586 |  0:00:10s
epoch 28 | loss: 0.3752  | eval_custom_logloss: 0.74865 |  0:00:10s
epoch 29 | loss: 0.35787 | eval_custom_logloss: 0.7861  |  0:00:11s
epoch 30 | loss: 0.37129 | eval_custom_logloss: 0.83009 |  0:00:11s
epoch 31 | loss: 0.38594 | eval_custom_logloss: 0.77368 |  0:00:11s
epoch 32 | loss: 0.3567  | eval_custom_logloss: 0.83596 |  0:00:12s
epoch 33 | loss: 0.35451 | eval_custom_logloss: 0.87342 |  0:00:12s
epoch 34 | loss: 0.3308  | eval_custom_logloss: 0.84202 |  0:00:13s
epoch 35 | loss: 0.36735 | eval_custom_logloss: 0.89256 |  0:00:13s
epoch 36 | loss: 0.33424 | eval_custom_logloss: 1.00017 |  0:00:13s
epoch 37 | loss: 0.34424 | eval_custom_logloss: 0.82782 |  0:00:14s
epoch 38 | loss: 0.31824 | eval_custom_logloss: 0.84087 |  0:00:14s
epoch 39 | loss: 0.31928 | eval_custom_logloss: 0.7647  |  0:00:14s
epoch 40 | loss: 0.30423 | eval_custom_logloss: 0.67003 |  0:00:15s
epoch 41 | loss: 0.29607 | eval_custom_logloss: 0.70597 |  0:00:15s
epoch 42 | loss: 0.29305 | eval_custom_logloss: 0.86528 |  0:00:15s
epoch 43 | loss: 0.27517 | eval_custom_logloss: 0.70762 |  0:00:16s
epoch 44 | loss: 0.28628 | eval_custom_logloss: 0.81361 |  0:00:16s
epoch 45 | loss: 0.31854 | eval_custom_logloss: 0.79199 |  0:00:17s
epoch 46 | loss: 0.30016 | eval_custom_logloss: 0.80038 |  0:00:17s
epoch 47 | loss: 0.30641 | eval_custom_logloss: 0.75375 |  0:00:17s
epoch 48 | loss: 0.26831 | eval_custom_logloss: 0.93418 |  0:00:18s
epoch 49 | loss: 0.25941 | eval_custom_logloss: 0.71958 |  0:00:18s
epoch 50 | loss: 0.23214 | eval_custom_logloss: 0.82643 |  0:00:18s
epoch 51 | loss: 0.24566 | eval_custom_logloss: 0.85654 |  0:00:19s
epoch 52 | loss: 0.2417  | eval_custom_logloss: 0.79075 |  0:00:19s
epoch 53 | loss: 0.23908 | eval_custom_logloss: 0.90938 |  0:00:19s
epoch 54 | loss: 0.24676 | eval_custom_logloss: 0.76967 |  0:00:20s
epoch 55 | loss: 0.26444 | eval_custom_logloss: 0.72484 |  0:00:20s
epoch 56 | loss: 0.24359 | eval_custom_logloss: 0.75852 |  0:00:21s
epoch 57 | loss: 0.19532 | eval_custom_logloss: 1.01966 |  0:00:21s
epoch 58 | loss: 0.24919 | eval_custom_logloss: 1.01428 |  0:00:21s
epoch 59 | loss: 0.26115 | eval_custom_logloss: 0.67801 |  0:00:22s
epoch 60 | loss: 0.25206 | eval_custom_logloss: 0.89133 |  0:00:22s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.67003
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9597666666666665, 'Log Loss - std': 0.2087264610813769} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax', 'n_a': 21, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.02691 | eval_custom_logloss: 1.1288  |  0:00:00s
epoch 1  | loss: 0.86268 | eval_custom_logloss: 1.24179 |  0:00:00s
epoch 2  | loss: 0.77606 | eval_custom_logloss: 1.25797 |  0:00:00s
epoch 3  | loss: 0.73785 | eval_custom_logloss: 1.30268 |  0:00:01s
epoch 4  | loss: 0.64576 | eval_custom_logloss: 1.60868 |  0:00:01s
epoch 5  | loss: 0.64787 | eval_custom_logloss: 2.15482 |  0:00:01s
epoch 6  | loss: 0.61252 | eval_custom_logloss: 1.86692 |  0:00:02s
epoch 7  | loss: 0.57155 | eval_custom_logloss: 2.27003 |  0:00:02s
epoch 8  | loss: 0.5754  | eval_custom_logloss: 1.86773 |  0:00:03s
epoch 9  | loss: 0.5776  | eval_custom_logloss: 1.47403 |  0:00:03s
epoch 10 | loss: 0.55345 | eval_custom_logloss: 1.91899 |  0:00:03s
epoch 11 | loss: 0.56286 | eval_custom_logloss: 2.89762 |  0:00:04s
epoch 12 | loss: 0.50966 | eval_custom_logloss: 2.17956 |  0:00:04s
epoch 13 | loss: 0.49864 | eval_custom_logloss: 1.77819 |  0:00:04s
epoch 14 | loss: 0.48166 | eval_custom_logloss: 1.29427 |  0:00:05s
epoch 15 | loss: 0.48017 | eval_custom_logloss: 1.35067 |  0:00:05s
epoch 16 | loss: 0.46656 | eval_custom_logloss: 1.48366 |  0:00:06s
epoch 17 | loss: 0.46749 | eval_custom_logloss: 1.54118 |  0:00:06s
epoch 18 | loss: 0.47661 | eval_custom_logloss: 1.76231 |  0:00:06s
epoch 19 | loss: 0.48632 | eval_custom_logloss: 1.68836 |  0:00:07s
epoch 20 | loss: 0.43903 | eval_custom_logloss: 1.43611 |  0:00:07s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.1288
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9962749999999999, 'Log Loss - std': 0.19150359494014724} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax', 'n_a': 21, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.00694 | eval_custom_logloss: 1.20133 |  0:00:00s
epoch 1  | loss: 0.86556 | eval_custom_logloss: 1.31078 |  0:00:00s
epoch 2  | loss: 0.79188 | eval_custom_logloss: 1.24556 |  0:00:01s
epoch 3  | loss: 0.71892 | eval_custom_logloss: 1.13795 |  0:00:01s
epoch 4  | loss: 0.684   | eval_custom_logloss: 1.3981  |  0:00:02s
epoch 5  | loss: 0.70707 | eval_custom_logloss: 1.49648 |  0:00:02s
epoch 6  | loss: 0.64208 | eval_custom_logloss: 1.07824 |  0:00:03s
epoch 7  | loss: 0.61666 | eval_custom_logloss: 1.29349 |  0:00:03s
epoch 8  | loss: 0.60317 | eval_custom_logloss: 1.5231  |  0:00:03s
epoch 9  | loss: 0.57522 | eval_custom_logloss: 0.8233  |  0:00:04s
epoch 10 | loss: 0.61467 | eval_custom_logloss: 1.32169 |  0:00:04s
epoch 11 | loss: 0.55496 | eval_custom_logloss: 1.48215 |  0:00:05s
epoch 12 | loss: 0.55982 | eval_custom_logloss: 0.94844 |  0:00:05s
epoch 13 | loss: 0.58675 | eval_custom_logloss: 0.96833 |  0:00:06s
epoch 14 | loss: 0.5728  | eval_custom_logloss: 1.05434 |  0:00:06s
epoch 15 | loss: 0.5465  | eval_custom_logloss: 0.96157 |  0:00:07s
epoch 16 | loss: 0.49799 | eval_custom_logloss: 1.14745 |  0:00:07s
epoch 17 | loss: 0.4973  | eval_custom_logloss: 1.44406 |  0:00:07s
epoch 18 | loss: 0.48001 | eval_custom_logloss: 1.29859 |  0:00:08s
epoch 19 | loss: 0.47035 | eval_custom_logloss: 1.2136  |  0:00:08s
epoch 20 | loss: 0.46008 | eval_custom_logloss: 1.15447 |  0:00:09s
epoch 21 | loss: 0.43978 | eval_custom_logloss: 1.02671 |  0:00:09s
epoch 22 | loss: 0.45545 | eval_custom_logloss: 1.05587 |  0:00:10s
epoch 23 | loss: 0.45233 | eval_custom_logloss: 0.82797 |  0:00:10s
epoch 24 | loss: 0.45167 | eval_custom_logloss: 0.98554 |  0:00:10s
epoch 25 | loss: 0.42079 | eval_custom_logloss: 1.12034 |  0:00:11s
epoch 26 | loss: 0.43595 | eval_custom_logloss: 0.92559 |  0:00:11s
epoch 27 | loss: 0.40735 | eval_custom_logloss: 0.94732 |  0:00:12s
epoch 28 | loss: 0.38522 | eval_custom_logloss: 0.89189 |  0:00:12s
epoch 29 | loss: 0.36436 | eval_custom_logloss: 1.10219 |  0:00:13s

Early stopping occurred at epoch 29 with best_epoch = 9 and best_eval_custom_logloss = 0.8233
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.96168, 'Log Loss - std': 0.18473266522193627} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 38 finished with value: 0.96168 and parameters: {'n_d': 21, 'n_steps': 5, 'gamma': 1.0144413783086994, 'cat_emb_dim': 3, 'n_independent': 1, 'n_shared': 4, 'momentum': 0.0022358795589646225, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax', 'n_a': 34, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.37966 | eval_custom_logloss: 7.91568 |  0:00:00s
epoch 1  | loss: 1.12181 | eval_custom_logloss: 7.24688 |  0:00:01s
epoch 2  | loss: 1.03482 | eval_custom_logloss: 8.9018  |  0:00:02s
epoch 3  | loss: 0.98689 | eval_custom_logloss: 9.08966 |  0:00:02s
epoch 4  | loss: 0.85435 | eval_custom_logloss: 7.38389 |  0:00:03s
epoch 5  | loss: 0.80319 | eval_custom_logloss: 9.30792 |  0:00:03s
epoch 6  | loss: 0.77095 | eval_custom_logloss: 8.71344 |  0:00:04s
epoch 7  | loss: 0.71135 | eval_custom_logloss: 7.86898 |  0:00:05s
epoch 8  | loss: 0.68275 | eval_custom_logloss: 6.19212 |  0:00:05s
epoch 9  | loss: 0.70545 | eval_custom_logloss: 8.43129 |  0:00:06s
epoch 10 | loss: 0.68244 | eval_custom_logloss: 8.71331 |  0:00:06s
epoch 11 | loss: 0.61964 | eval_custom_logloss: 6.84536 |  0:00:07s
epoch 12 | loss: 0.60247 | eval_custom_logloss: 6.84452 |  0:00:08s
epoch 13 | loss: 0.5796  | eval_custom_logloss: 5.4604  |  0:00:08s
epoch 14 | loss: 0.589   | eval_custom_logloss: 5.97445 |  0:00:09s
epoch 15 | loss: 0.55958 | eval_custom_logloss: 5.95749 |  0:00:09s
epoch 16 | loss: 0.61292 | eval_custom_logloss: 6.36745 |  0:00:10s
epoch 17 | loss: 0.55594 | eval_custom_logloss: 6.23569 |  0:00:11s
epoch 18 | loss: 0.58488 | eval_custom_logloss: 5.46841 |  0:00:11s
epoch 19 | loss: 0.60149 | eval_custom_logloss: 5.81131 |  0:00:12s
epoch 20 | loss: 0.58003 | eval_custom_logloss: 5.83835 |  0:00:13s
epoch 21 | loss: 0.56465 | eval_custom_logloss: 6.01892 |  0:00:13s
epoch 22 | loss: 0.55564 | eval_custom_logloss: 4.67327 |  0:00:14s
epoch 23 | loss: 0.56144 | eval_custom_logloss: 5.57763 |  0:00:14s
epoch 24 | loss: 0.56689 | eval_custom_logloss: 5.62637 |  0:00:15s
epoch 25 | loss: 0.53004 | eval_custom_logloss: 4.76662 |  0:00:16s
epoch 26 | loss: 0.56276 | eval_custom_logloss: 6.05712 |  0:00:16s
epoch 27 | loss: 0.57401 | eval_custom_logloss: 5.51111 |  0:00:17s
epoch 28 | loss: 0.55782 | eval_custom_logloss: 6.49249 |  0:00:17s
epoch 29 | loss: 0.55157 | eval_custom_logloss: 7.12359 |  0:00:18s
epoch 30 | loss: 0.52804 | eval_custom_logloss: 5.83305 |  0:00:19s
epoch 31 | loss: 0.50216 | eval_custom_logloss: 5.57604 |  0:00:19s
epoch 32 | loss: 0.49763 | eval_custom_logloss: 5.56934 |  0:00:20s
epoch 33 | loss: 0.50199 | eval_custom_logloss: 4.51422 |  0:00:21s
epoch 34 | loss: 0.4759  | eval_custom_logloss: 4.92872 |  0:00:21s
epoch 35 | loss: 0.48924 | eval_custom_logloss: 3.99782 |  0:00:22s
epoch 36 | loss: 0.47577 | eval_custom_logloss: 4.6272  |  0:00:22s
epoch 37 | loss: 0.45391 | eval_custom_logloss: 4.44555 |  0:00:23s
epoch 38 | loss: 0.45547 | eval_custom_logloss: 4.06551 |  0:00:24s
epoch 39 | loss: 0.48051 | eval_custom_logloss: 4.44749 |  0:00:24s
epoch 40 | loss: 0.4676  | eval_custom_logloss: 3.60795 |  0:00:25s
epoch 41 | loss: 0.49822 | eval_custom_logloss: 4.17464 |  0:00:25s
epoch 42 | loss: 0.46273 | eval_custom_logloss: 2.47033 |  0:00:26s
epoch 43 | loss: 0.45075 | eval_custom_logloss: 3.52402 |  0:00:27s
epoch 44 | loss: 0.45407 | eval_custom_logloss: 2.61898 |  0:00:27s
epoch 45 | loss: 0.44987 | eval_custom_logloss: 2.23154 |  0:00:28s
epoch 46 | loss: 0.45073 | eval_custom_logloss: 2.29913 |  0:00:29s
epoch 47 | loss: 0.43669 | eval_custom_logloss: 1.94794 |  0:00:29s
epoch 48 | loss: 0.42212 | eval_custom_logloss: 2.76795 |  0:00:30s
epoch 49 | loss: 0.42591 | eval_custom_logloss: 2.53743 |  0:00:30s
epoch 50 | loss: 0.40563 | eval_custom_logloss: 2.26966 |  0:00:31s
epoch 51 | loss: 0.40493 | eval_custom_logloss: 2.56732 |  0:00:32s
epoch 52 | loss: 0.38073 | eval_custom_logloss: 2.74878 |  0:00:32s
epoch 53 | loss: 0.39249 | eval_custom_logloss: 2.48334 |  0:00:33s
epoch 54 | loss: 0.40607 | eval_custom_logloss: 2.67328 |  0:00:33s
epoch 55 | loss: 0.37401 | eval_custom_logloss: 2.27293 |  0:00:34s
epoch 56 | loss: 0.34953 | eval_custom_logloss: 1.88316 |  0:00:35s
epoch 57 | loss: 0.36377 | eval_custom_logloss: 2.04403 |  0:00:35s
epoch 58 | loss: 0.36053 | eval_custom_logloss: 2.1156  |  0:00:36s
epoch 59 | loss: 0.33933 | eval_custom_logloss: 2.00736 |  0:00:37s
epoch 60 | loss: 0.32753 | eval_custom_logloss: 2.47864 |  0:00:37s
epoch 61 | loss: 0.31014 | eval_custom_logloss: 2.43511 |  0:00:38s
epoch 62 | loss: 0.29831 | eval_custom_logloss: 2.70252 |  0:00:38s
epoch 63 | loss: 0.28923 | eval_custom_logloss: 3.23275 |  0:00:39s
epoch 64 | loss: 0.30298 | eval_custom_logloss: 2.77353 |  0:00:40s
epoch 65 | loss: 0.31184 | eval_custom_logloss: 2.61842 |  0:00:40s
epoch 66 | loss: 0.335   | eval_custom_logloss: 2.36188 |  0:00:41s
epoch 67 | loss: 0.30307 | eval_custom_logloss: 2.69372 |  0:00:41s
epoch 68 | loss: 0.27648 | eval_custom_logloss: 2.98649 |  0:00:42s
epoch 69 | loss: 0.26886 | eval_custom_logloss: 2.77893 |  0:00:43s
epoch 70 | loss: 0.28911 | eval_custom_logloss: 2.226   |  0:00:43s
epoch 71 | loss: 0.26689 | eval_custom_logloss: 2.49903 |  0:00:44s
epoch 72 | loss: 0.26569 | eval_custom_logloss: 2.01832 |  0:00:45s
epoch 73 | loss: 0.2425  | eval_custom_logloss: 2.04366 |  0:00:45s
epoch 74 | loss: 0.25059 | eval_custom_logloss: 1.68781 |  0:00:46s
epoch 75 | loss: 0.25353 | eval_custom_logloss: 2.26266 |  0:00:46s
epoch 76 | loss: 0.24625 | eval_custom_logloss: 2.02751 |  0:00:47s
epoch 77 | loss: 0.23384 | eval_custom_logloss: 2.46322 |  0:00:48s
epoch 78 | loss: 0.23053 | eval_custom_logloss: 3.20436 |  0:00:48s
epoch 79 | loss: 0.26819 | eval_custom_logloss: 2.13072 |  0:00:49s
epoch 80 | loss: 0.22267 | eval_custom_logloss: 3.00447 |  0:00:49s
epoch 81 | loss: 0.27065 | eval_custom_logloss: 2.86499 |  0:00:50s
epoch 82 | loss: 0.27236 | eval_custom_logloss: 3.76381 |  0:00:51s
epoch 83 | loss: 0.2527  | eval_custom_logloss: 3.9625  |  0:00:51s
epoch 84 | loss: 0.2804  | eval_custom_logloss: 4.05784 |  0:00:52s
epoch 85 | loss: 0.29047 | eval_custom_logloss: 3.24135 |  0:00:52s
epoch 86 | loss: 0.30786 | eval_custom_logloss: 3.87644 |  0:00:53s
epoch 87 | loss: 0.28089 | eval_custom_logloss: 3.72362 |  0:00:54s
epoch 88 | loss: 0.25427 | eval_custom_logloss: 3.12307 |  0:00:54s
epoch 89 | loss: 0.24654 | eval_custom_logloss: 2.80447 |  0:00:55s
epoch 90 | loss: 0.21163 | eval_custom_logloss: 3.80398 |  0:00:56s
epoch 91 | loss: 0.22619 | eval_custom_logloss: 3.31458 |  0:00:56s
epoch 92 | loss: 0.23771 | eval_custom_logloss: 3.16142 |  0:00:57s
epoch 93 | loss: 0.23    | eval_custom_logloss: 1.77516 |  0:00:57s
epoch 94 | loss: 0.21301 | eval_custom_logloss: 1.91373 |  0:00:58s

Early stopping occurred at epoch 94 with best_epoch = 74 and best_eval_custom_logloss = 1.68781
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.4294, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax', 'n_a': 34, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.3965  | eval_custom_logloss: 8.52797 |  0:00:00s
epoch 1  | loss: 1.07534 | eval_custom_logloss: 8.94049 |  0:00:01s
epoch 2  | loss: 0.98535 | eval_custom_logloss: 8.8757  |  0:00:01s
epoch 3  | loss: 0.88827 | eval_custom_logloss: 8.98323 |  0:00:02s
epoch 4  | loss: 0.89978 | eval_custom_logloss: 8.73365 |  0:00:03s
epoch 5  | loss: 0.87579 | eval_custom_logloss: 8.74042 |  0:00:03s
epoch 6  | loss: 0.77133 | eval_custom_logloss: 8.33825 |  0:00:04s
epoch 7  | loss: 0.74015 | eval_custom_logloss: 7.85431 |  0:00:05s
epoch 8  | loss: 0.68839 | eval_custom_logloss: 7.34634 |  0:00:05s
epoch 9  | loss: 0.6355  | eval_custom_logloss: 8.60646 |  0:00:06s
epoch 10 | loss: 0.6841  | eval_custom_logloss: 7.91043 |  0:00:06s
epoch 11 | loss: 0.65945 | eval_custom_logloss: 9.40094 |  0:00:07s
epoch 12 | loss: 0.62273 | eval_custom_logloss: 7.71581 |  0:00:08s
epoch 13 | loss: 0.57899 | eval_custom_logloss: 7.98707 |  0:00:08s
epoch 14 | loss: 0.573   | eval_custom_logloss: 7.02608 |  0:00:09s
epoch 15 | loss: 0.58839 | eval_custom_logloss: 6.86927 |  0:00:09s
epoch 16 | loss: 0.56746 | eval_custom_logloss: 6.46748 |  0:00:10s
epoch 17 | loss: 0.54611 | eval_custom_logloss: 6.20185 |  0:00:11s
epoch 18 | loss: 0.52629 | eval_custom_logloss: 6.13149 |  0:00:11s
epoch 19 | loss: 0.51397 | eval_custom_logloss: 6.37647 |  0:00:12s
epoch 20 | loss: 0.49353 | eval_custom_logloss: 5.03515 |  0:00:13s
epoch 21 | loss: 0.4993  | eval_custom_logloss: 4.99311 |  0:00:13s
epoch 22 | loss: 0.51351 | eval_custom_logloss: 4.96464 |  0:00:14s
epoch 23 | loss: 0.51849 | eval_custom_logloss: 5.5299  |  0:00:15s
epoch 24 | loss: 0.52559 | eval_custom_logloss: 5.58333 |  0:00:15s
epoch 25 | loss: 0.51562 | eval_custom_logloss: 6.13734 |  0:00:16s
epoch 26 | loss: 0.52988 | eval_custom_logloss: 4.52232 |  0:00:17s
epoch 27 | loss: 0.55338 | eval_custom_logloss: 5.65947 |  0:00:17s
epoch 28 | loss: 0.54321 | eval_custom_logloss: 6.61933 |  0:00:18s
epoch 29 | loss: 0.54528 | eval_custom_logloss: 4.97543 |  0:00:19s
epoch 30 | loss: 0.50171 | eval_custom_logloss: 5.18704 |  0:00:19s
epoch 31 | loss: 0.49454 | eval_custom_logloss: 6.45961 |  0:00:20s
epoch 32 | loss: 0.50469 | eval_custom_logloss: 5.63667 |  0:00:21s
epoch 33 | loss: 0.50292 | eval_custom_logloss: 4.57658 |  0:00:21s
epoch 34 | loss: 0.4921  | eval_custom_logloss: 4.78044 |  0:00:22s
epoch 35 | loss: 0.49005 | eval_custom_logloss: 3.96211 |  0:00:22s
epoch 36 | loss: 0.48383 | eval_custom_logloss: 4.37844 |  0:00:23s
epoch 37 | loss: 0.46238 | eval_custom_logloss: 4.61385 |  0:00:24s
epoch 38 | loss: 0.46217 | eval_custom_logloss: 3.87891 |  0:00:24s
epoch 39 | loss: 0.43966 | eval_custom_logloss: 3.86014 |  0:00:25s
epoch 40 | loss: 0.44864 | eval_custom_logloss: 4.24308 |  0:00:25s
epoch 41 | loss: 0.44466 | eval_custom_logloss: 4.47046 |  0:00:26s
epoch 42 | loss: 0.44129 | eval_custom_logloss: 4.55895 |  0:00:27s
epoch 43 | loss: 0.42179 | eval_custom_logloss: 4.51218 |  0:00:27s
epoch 44 | loss: 0.41622 | eval_custom_logloss: 5.19865 |  0:00:28s
epoch 45 | loss: 0.4429  | eval_custom_logloss: 3.82551 |  0:00:29s
epoch 46 | loss: 0.44576 | eval_custom_logloss: 4.25683 |  0:00:29s
epoch 47 | loss: 0.42729 | eval_custom_logloss: 4.19584 |  0:00:30s
epoch 48 | loss: 0.45765 | eval_custom_logloss: 5.34457 |  0:00:30s
epoch 49 | loss: 0.44777 | eval_custom_logloss: 4.60229 |  0:00:31s
epoch 50 | loss: 0.47354 | eval_custom_logloss: 4.13874 |  0:00:32s
epoch 51 | loss: 0.44197 | eval_custom_logloss: 2.79805 |  0:00:32s
epoch 52 | loss: 0.43326 | eval_custom_logloss: 2.99972 |  0:00:33s
epoch 53 | loss: 0.43059 | eval_custom_logloss: 2.85366 |  0:00:33s
epoch 54 | loss: 0.43341 | eval_custom_logloss: 3.19611 |  0:00:34s
epoch 55 | loss: 0.46243 | eval_custom_logloss: 4.14188 |  0:00:35s
epoch 56 | loss: 0.47386 | eval_custom_logloss: 3.96106 |  0:00:35s
epoch 57 | loss: 0.47604 | eval_custom_logloss: 4.36756 |  0:00:36s
epoch 58 | loss: 0.44997 | eval_custom_logloss: 3.72856 |  0:00:37s
epoch 59 | loss: 0.43448 | eval_custom_logloss: 4.11981 |  0:00:37s
epoch 60 | loss: 0.42015 | eval_custom_logloss: 4.49363 |  0:00:38s
epoch 61 | loss: 0.42509 | eval_custom_logloss: 4.72412 |  0:00:38s
epoch 62 | loss: 0.41126 | eval_custom_logloss: 5.19565 |  0:00:39s
epoch 63 | loss: 0.42551 | eval_custom_logloss: 4.06551 |  0:00:40s
epoch 64 | loss: 0.45012 | eval_custom_logloss: 3.98178 |  0:00:40s
epoch 65 | loss: 0.41929 | eval_custom_logloss: 3.67234 |  0:00:41s
epoch 66 | loss: 0.4349  | eval_custom_logloss: 3.36466 |  0:00:41s
epoch 67 | loss: 0.42175 | eval_custom_logloss: 3.58971 |  0:00:42s
epoch 68 | loss: 0.38458 | eval_custom_logloss: 3.19802 |  0:00:43s
epoch 69 | loss: 0.38697 | eval_custom_logloss: 3.20228 |  0:00:43s
epoch 70 | loss: 0.37026 | eval_custom_logloss: 3.18485 |  0:00:44s
epoch 71 | loss: 0.38776 | eval_custom_logloss: 2.86688 |  0:00:44s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 2.79805
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0156, 'Log Loss - std': 0.5861999999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax', 'n_a': 34, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.37357 | eval_custom_logloss: 8.46996 |  0:00:00s
epoch 1  | loss: 1.16809 | eval_custom_logloss: 8.20491 |  0:00:01s
epoch 2  | loss: 0.92713 | eval_custom_logloss: 9.21658 |  0:00:01s
epoch 3  | loss: 0.92195 | eval_custom_logloss: 9.01411 |  0:00:02s
epoch 4  | loss: 0.82555 | eval_custom_logloss: 8.27164 |  0:00:03s
epoch 5  | loss: 0.81482 | eval_custom_logloss: 9.53651 |  0:00:03s
epoch 6  | loss: 0.70048 | eval_custom_logloss: 7.42044 |  0:00:04s
epoch 7  | loss: 0.69826 | eval_custom_logloss: 8.27156 |  0:00:05s
epoch 8  | loss: 0.68107 | eval_custom_logloss: 7.71408 |  0:00:05s
epoch 9  | loss: 0.61405 | eval_custom_logloss: 8.31596 |  0:00:06s
epoch 10 | loss: 0.6078  | eval_custom_logloss: 8.12602 |  0:00:06s
epoch 11 | loss: 0.60156 | eval_custom_logloss: 6.29762 |  0:00:07s
epoch 12 | loss: 0.5818  | eval_custom_logloss: 6.22157 |  0:00:08s
epoch 13 | loss: 0.57267 | eval_custom_logloss: 5.63851 |  0:00:08s
epoch 14 | loss: 0.6081  | eval_custom_logloss: 6.41178 |  0:00:09s
epoch 15 | loss: 0.56582 | eval_custom_logloss: 4.92548 |  0:00:10s
epoch 16 | loss: 0.56311 | eval_custom_logloss: 5.60109 |  0:00:10s
epoch 17 | loss: 0.55374 | eval_custom_logloss: 7.26888 |  0:00:11s
epoch 18 | loss: 0.55673 | eval_custom_logloss: 5.58884 |  0:00:12s
epoch 19 | loss: 0.5874  | eval_custom_logloss: 6.82928 |  0:00:12s
epoch 20 | loss: 0.60923 | eval_custom_logloss: 6.60443 |  0:00:13s
epoch 21 | loss: 0.53134 | eval_custom_logloss: 5.25185 |  0:00:13s
epoch 22 | loss: 0.54545 | eval_custom_logloss: 5.17797 |  0:00:14s
epoch 23 | loss: 0.54043 | eval_custom_logloss: 4.98271 |  0:00:15s
epoch 24 | loss: 0.57153 | eval_custom_logloss: 5.54665 |  0:00:15s
epoch 25 | loss: 0.54993 | eval_custom_logloss: 6.1896  |  0:00:16s
epoch 26 | loss: 0.50772 | eval_custom_logloss: 4.69283 |  0:00:17s
epoch 27 | loss: 0.49865 | eval_custom_logloss: 5.09711 |  0:00:17s
epoch 28 | loss: 0.53976 | eval_custom_logloss: 4.3123  |  0:00:18s
epoch 29 | loss: 0.54402 | eval_custom_logloss: 5.14856 |  0:00:19s
epoch 30 | loss: 0.53511 | eval_custom_logloss: 5.07184 |  0:00:19s
epoch 31 | loss: 0.53372 | eval_custom_logloss: 4.98008 |  0:00:20s
epoch 32 | loss: 0.51465 | eval_custom_logloss: 5.22956 |  0:00:20s
epoch 33 | loss: 0.52573 | eval_custom_logloss: 6.14977 |  0:00:21s
epoch 34 | loss: 0.51413 | eval_custom_logloss: 4.98556 |  0:00:22s
epoch 35 | loss: 0.50402 | eval_custom_logloss: 3.89492 |  0:00:22s
epoch 36 | loss: 0.49445 | eval_custom_logloss: 4.98155 |  0:00:23s
epoch 37 | loss: 0.49472 | eval_custom_logloss: 4.6625  |  0:00:24s
epoch 38 | loss: 0.53971 | eval_custom_logloss: 4.19972 |  0:00:24s
epoch 39 | loss: 0.51045 | eval_custom_logloss: 4.56365 |  0:00:25s
epoch 40 | loss: 0.48596 | eval_custom_logloss: 3.76731 |  0:00:26s
epoch 41 | loss: 0.47528 | eval_custom_logloss: 4.23281 |  0:00:26s
epoch 42 | loss: 0.44759 | eval_custom_logloss: 3.26828 |  0:00:27s
epoch 43 | loss: 0.45185 | eval_custom_logloss: 3.66041 |  0:00:27s
epoch 44 | loss: 0.46127 | eval_custom_logloss: 4.32581 |  0:00:28s
epoch 45 | loss: 0.45779 | eval_custom_logloss: 3.79451 |  0:00:29s
epoch 46 | loss: 0.47043 | eval_custom_logloss: 4.53474 |  0:00:29s
epoch 47 | loss: 0.45696 | eval_custom_logloss: 5.61502 |  0:00:30s
epoch 48 | loss: 0.45582 | eval_custom_logloss: 5.71901 |  0:00:31s
epoch 49 | loss: 0.44038 | eval_custom_logloss: 4.02905 |  0:00:31s
epoch 50 | loss: 0.45519 | eval_custom_logloss: 4.62257 |  0:00:32s
epoch 51 | loss: 0.44056 | eval_custom_logloss: 4.09758 |  0:00:32s
epoch 52 | loss: 0.44412 | eval_custom_logloss: 4.11285 |  0:00:33s
epoch 53 | loss: 0.44856 | eval_custom_logloss: 4.05991 |  0:00:34s
epoch 54 | loss: 0.51064 | eval_custom_logloss: 3.15122 |  0:00:34s
epoch 55 | loss: 0.49636 | eval_custom_logloss: 2.11418 |  0:00:35s
epoch 56 | loss: 0.48973 | eval_custom_logloss: 2.39019 |  0:00:36s
epoch 57 | loss: 0.46921 | eval_custom_logloss: 3.0978  |  0:00:36s
epoch 58 | loss: 0.47903 | eval_custom_logloss: 3.78153 |  0:00:37s
epoch 59 | loss: 0.46352 | eval_custom_logloss: 3.37591 |  0:00:38s
epoch 60 | loss: 0.45137 | eval_custom_logloss: 3.64195 |  0:00:38s
epoch 61 | loss: 0.50247 | eval_custom_logloss: 3.50115 |  0:00:39s
epoch 62 | loss: 0.4844  | eval_custom_logloss: 4.0441  |  0:00:39s
epoch 63 | loss: 0.51563 | eval_custom_logloss: 4.081   |  0:00:40s
epoch 64 | loss: 0.49555 | eval_custom_logloss: 5.051   |  0:00:41s
epoch 65 | loss: 0.45592 | eval_custom_logloss: 3.78302 |  0:00:41s
epoch 66 | loss: 0.48202 | eval_custom_logloss: 5.20763 |  0:00:42s
epoch 67 | loss: 0.4899  | eval_custom_logloss: 4.83521 |  0:00:43s
epoch 68 | loss: 0.48009 | eval_custom_logloss: 4.14854 |  0:00:43s
epoch 69 | loss: 0.48284 | eval_custom_logloss: 3.36818 |  0:00:44s
epoch 70 | loss: 0.45704 | eval_custom_logloss: 3.4619  |  0:00:45s
epoch 71 | loss: 0.44332 | eval_custom_logloss: 2.69818 |  0:00:45s
epoch 72 | loss: 0.45952 | eval_custom_logloss: 3.02189 |  0:00:46s
epoch 73 | loss: 0.43944 | eval_custom_logloss: 3.25802 |  0:00:47s
epoch 74 | loss: 0.4311  | eval_custom_logloss: 3.72169 |  0:00:47s
epoch 75 | loss: 0.43029 | eval_custom_logloss: 3.72287 |  0:00:48s

Early stopping occurred at epoch 75 with best_epoch = 55 and best_eval_custom_logloss = 2.11418
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.9572666666666667, 'Log Loss - std': 0.4856876728470217} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax', 'n_a': 34, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.27961 | eval_custom_logloss: 9.01624 |  0:00:00s
epoch 1  | loss: 1.09495 | eval_custom_logloss: 7.69926 |  0:00:01s
epoch 2  | loss: 1.01654 | eval_custom_logloss: 8.74085 |  0:00:01s
epoch 3  | loss: 0.88862 | eval_custom_logloss: 8.02077 |  0:00:02s
epoch 4  | loss: 0.95037 | eval_custom_logloss: 7.31343 |  0:00:03s
epoch 5  | loss: 0.88343 | eval_custom_logloss: 8.75907 |  0:00:03s
epoch 6  | loss: 0.77498 | eval_custom_logloss: 7.70712 |  0:00:04s
epoch 7  | loss: 0.74744 | eval_custom_logloss: 10.24745|  0:00:04s
epoch 8  | loss: 0.81153 | eval_custom_logloss: 7.64413 |  0:00:05s
epoch 9  | loss: 0.74605 | eval_custom_logloss: 8.39133 |  0:00:06s
epoch 10 | loss: 0.76423 | eval_custom_logloss: 8.26608 |  0:00:06s
epoch 11 | loss: 0.63637 | eval_custom_logloss: 7.82907 |  0:00:07s
epoch 12 | loss: 0.58911 | eval_custom_logloss: 6.02541 |  0:00:07s
epoch 13 | loss: 0.59947 | eval_custom_logloss: 6.81287 |  0:00:08s
epoch 14 | loss: 0.60153 | eval_custom_logloss: 7.24462 |  0:00:09s
epoch 15 | loss: 0.57226 | eval_custom_logloss: 8.10171 |  0:00:09s
epoch 16 | loss: 0.61142 | eval_custom_logloss: 8.53368 |  0:00:10s
epoch 17 | loss: 0.62717 | eval_custom_logloss: 8.51935 |  0:00:11s
epoch 18 | loss: 0.6022  | eval_custom_logloss: 6.24817 |  0:00:11s
epoch 19 | loss: 0.56225 | eval_custom_logloss: 7.3535  |  0:00:12s
epoch 20 | loss: 0.54729 | eval_custom_logloss: 7.52086 |  0:00:13s
epoch 21 | loss: 0.55172 | eval_custom_logloss: 6.58073 |  0:00:13s
epoch 22 | loss: 0.5435  | eval_custom_logloss: 7.10154 |  0:00:14s
epoch 23 | loss: 0.54808 | eval_custom_logloss: 6.59378 |  0:00:14s
epoch 24 | loss: 0.5146  | eval_custom_logloss: 5.44728 |  0:00:15s
epoch 25 | loss: 0.52781 | eval_custom_logloss: 5.33956 |  0:00:16s
epoch 26 | loss: 0.52195 | eval_custom_logloss: 6.11826 |  0:00:16s
epoch 27 | loss: 0.52467 | eval_custom_logloss: 7.79144 |  0:00:17s
epoch 28 | loss: 0.51953 | eval_custom_logloss: 6.98915 |  0:00:18s
epoch 29 | loss: 0.52024 | eval_custom_logloss: 6.53698 |  0:00:18s
epoch 30 | loss: 0.50646 | eval_custom_logloss: 6.56283 |  0:00:19s
epoch 31 | loss: 0.47325 | eval_custom_logloss: 6.05382 |  0:00:20s
epoch 32 | loss: 0.46344 | eval_custom_logloss: 4.07306 |  0:00:20s
epoch 33 | loss: 0.43899 | eval_custom_logloss: 5.23799 |  0:00:21s
epoch 34 | loss: 0.45864 | eval_custom_logloss: 5.49076 |  0:00:21s
epoch 35 | loss: 0.455   | eval_custom_logloss: 5.00454 |  0:00:22s
epoch 36 | loss: 0.44905 | eval_custom_logloss: 4.45472 |  0:00:23s
epoch 37 | loss: 0.43405 | eval_custom_logloss: 4.7127  |  0:00:23s
epoch 38 | loss: 0.40472 | eval_custom_logloss: 5.52329 |  0:00:24s
epoch 39 | loss: 0.41713 | eval_custom_logloss: 4.20247 |  0:00:24s
epoch 40 | loss: 0.40622 | eval_custom_logloss: 4.7261  |  0:00:25s
epoch 41 | loss: 0.39669 | eval_custom_logloss: 5.20327 |  0:00:26s
epoch 42 | loss: 0.42596 | eval_custom_logloss: 3.65503 |  0:00:26s
epoch 43 | loss: 0.42004 | eval_custom_logloss: 4.09583 |  0:00:27s
epoch 44 | loss: 0.41321 | eval_custom_logloss: 3.16421 |  0:00:28s
epoch 45 | loss: 0.42261 | eval_custom_logloss: 3.74808 |  0:00:28s
epoch 46 | loss: 0.39851 | eval_custom_logloss: 3.5589  |  0:00:29s
epoch 47 | loss: 0.39953 | eval_custom_logloss: 2.98262 |  0:00:29s
epoch 48 | loss: 0.3857  | eval_custom_logloss: 2.90209 |  0:00:30s
epoch 49 | loss: 0.38733 | eval_custom_logloss: 3.38816 |  0:00:31s
epoch 50 | loss: 0.3417  | eval_custom_logloss: 3.10524 |  0:00:31s
epoch 51 | loss: 0.37641 | eval_custom_logloss: 2.86753 |  0:00:32s
epoch 52 | loss: 0.36028 | eval_custom_logloss: 3.59753 |  0:00:33s
epoch 53 | loss: 0.32795 | eval_custom_logloss: 3.86918 |  0:00:33s
epoch 54 | loss: 0.34456 | eval_custom_logloss: 3.8035  |  0:00:34s
epoch 55 | loss: 0.31959 | eval_custom_logloss: 3.61792 |  0:00:34s
epoch 56 | loss: 0.30815 | eval_custom_logloss: 3.6502  |  0:00:35s
epoch 57 | loss: 0.33742 | eval_custom_logloss: 4.01475 |  0:00:36s
epoch 58 | loss: 0.30183 | eval_custom_logloss: 4.49087 |  0:00:36s
epoch 59 | loss: 0.2813  | eval_custom_logloss: 3.84333 |  0:00:37s
epoch 60 | loss: 0.28804 | eval_custom_logloss: 4.12714 |  0:00:37s
epoch 61 | loss: 0.33156 | eval_custom_logloss: 3.62086 |  0:00:38s
epoch 62 | loss: 0.28789 | eval_custom_logloss: 3.5493  |  0:00:39s
epoch 63 | loss: 0.28451 | eval_custom_logloss: 3.22243 |  0:00:39s
epoch 64 | loss: 0.2929  | eval_custom_logloss: 3.26266 |  0:00:40s
epoch 65 | loss: 0.30601 | eval_custom_logloss: 3.79535 |  0:00:41s
epoch 66 | loss: 0.28998 | eval_custom_logloss: 3.02081 |  0:00:41s
epoch 67 | loss: 0.28336 | eval_custom_logloss: 3.17411 |  0:00:42s
epoch 68 | loss: 0.25525 | eval_custom_logloss: 3.90007 |  0:00:42s
epoch 69 | loss: 0.23071 | eval_custom_logloss: 3.86423 |  0:00:43s
epoch 70 | loss: 0.23085 | eval_custom_logloss: 3.86534 |  0:00:44s
epoch 71 | loss: 0.27732 | eval_custom_logloss: 4.15711 |  0:00:44s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 2.86753
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.0791500000000003, 'Log Loss - std': 0.47062302058016664} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax', 'n_a': 34, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.48085 | eval_custom_logloss: 8.14146 |  0:00:00s
epoch 1  | loss: 1.02109 | eval_custom_logloss: 8.08191 |  0:00:01s
epoch 2  | loss: 0.89607 | eval_custom_logloss: 9.96912 |  0:00:01s
epoch 3  | loss: 0.89228 | eval_custom_logloss: 9.61056 |  0:00:02s
epoch 4  | loss: 0.80972 | eval_custom_logloss: 8.60921 |  0:00:03s
epoch 5  | loss: 0.73975 | eval_custom_logloss: 7.27094 |  0:00:03s
epoch 6  | loss: 0.73614 | eval_custom_logloss: 7.17833 |  0:00:04s
epoch 7  | loss: 0.71985 | eval_custom_logloss: 8.09325 |  0:00:05s
epoch 8  | loss: 0.70078 | eval_custom_logloss: 7.8162  |  0:00:05s
epoch 9  | loss: 0.74331 | eval_custom_logloss: 8.4577  |  0:00:06s
epoch 10 | loss: 0.67535 | eval_custom_logloss: 7.87284 |  0:00:06s
epoch 11 | loss: 0.64528 | eval_custom_logloss: 7.2513  |  0:00:07s
epoch 12 | loss: 0.65123 | eval_custom_logloss: 7.38627 |  0:00:08s
epoch 13 | loss: 0.66261 | eval_custom_logloss: 8.59661 |  0:00:08s
epoch 14 | loss: 0.61884 | eval_custom_logloss: 5.84457 |  0:00:09s
epoch 15 | loss: 0.64472 | eval_custom_logloss: 5.34783 |  0:00:10s
epoch 16 | loss: 0.60109 | eval_custom_logloss: 5.64435 |  0:00:10s
epoch 17 | loss: 0.5941  | eval_custom_logloss: 6.18081 |  0:00:11s
epoch 18 | loss: 0.56679 | eval_custom_logloss: 7.4736  |  0:00:11s
epoch 19 | loss: 0.5545  | eval_custom_logloss: 7.03062 |  0:00:12s
epoch 20 | loss: 0.57908 | eval_custom_logloss: 7.87981 |  0:00:13s
epoch 21 | loss: 0.54961 | eval_custom_logloss: 7.38435 |  0:00:13s
epoch 22 | loss: 0.54864 | eval_custom_logloss: 8.24456 |  0:00:14s
epoch 23 | loss: 0.54858 | eval_custom_logloss: 7.82195 |  0:00:14s
epoch 24 | loss: 0.58649 | eval_custom_logloss: 4.90933 |  0:00:15s
epoch 25 | loss: 0.58488 | eval_custom_logloss: 5.10155 |  0:00:16s
epoch 26 | loss: 0.53968 | eval_custom_logloss: 3.8444  |  0:00:16s
epoch 27 | loss: 0.52258 | eval_custom_logloss: 6.1388  |  0:00:17s
epoch 28 | loss: 0.5419  | eval_custom_logloss: 7.40129 |  0:00:18s
epoch 29 | loss: 0.58016 | eval_custom_logloss: 4.21118 |  0:00:18s
epoch 30 | loss: 0.5453  | eval_custom_logloss: 5.01334 |  0:00:19s
epoch 31 | loss: 0.51799 | eval_custom_logloss: 5.52882 |  0:00:19s
epoch 32 | loss: 0.50814 | eval_custom_logloss: 5.38036 |  0:00:20s
epoch 33 | loss: 0.49615 | eval_custom_logloss: 5.4199  |  0:00:21s
epoch 34 | loss: 0.50557 | eval_custom_logloss: 5.81677 |  0:00:21s
epoch 35 | loss: 0.50399 | eval_custom_logloss: 6.08476 |  0:00:22s
epoch 36 | loss: 0.54261 | eval_custom_logloss: 4.74684 |  0:00:22s
epoch 37 | loss: 0.47723 | eval_custom_logloss: 5.47409 |  0:00:23s
epoch 38 | loss: 0.50172 | eval_custom_logloss: 4.91031 |  0:00:24s
epoch 39 | loss: 0.48451 | eval_custom_logloss: 3.9478  |  0:00:24s
epoch 40 | loss: 0.50887 | eval_custom_logloss: 3.50729 |  0:00:25s
epoch 41 | loss: 0.52013 | eval_custom_logloss: 3.97316 |  0:00:26s
epoch 42 | loss: 0.52349 | eval_custom_logloss: 3.49609 |  0:00:26s
epoch 43 | loss: 0.47925 | eval_custom_logloss: 2.67835 |  0:00:27s
epoch 44 | loss: 0.4871  | eval_custom_logloss: 3.11415 |  0:00:28s
epoch 45 | loss: 0.49583 | eval_custom_logloss: 2.53961 |  0:00:28s
epoch 46 | loss: 0.48286 | eval_custom_logloss: 2.28846 |  0:00:29s
epoch 47 | loss: 0.49204 | eval_custom_logloss: 2.19443 |  0:00:30s
epoch 48 | loss: 0.49736 | eval_custom_logloss: 1.79628 |  0:00:30s
epoch 49 | loss: 0.48044 | eval_custom_logloss: 2.69219 |  0:00:31s
epoch 50 | loss: 0.47537 | eval_custom_logloss: 2.40439 |  0:00:31s
epoch 51 | loss: 0.48249 | eval_custom_logloss: 2.08891 |  0:00:32s
epoch 52 | loss: 0.47824 | eval_custom_logloss: 3.30411 |  0:00:33s
epoch 53 | loss: 0.48526 | eval_custom_logloss: 3.16489 |  0:00:33s
epoch 54 | loss: 0.47414 | eval_custom_logloss: 2.80828 |  0:00:34s
epoch 55 | loss: 0.42429 | eval_custom_logloss: 2.45211 |  0:00:35s
epoch 56 | loss: 0.43626 | eval_custom_logloss: 2.26167 |  0:00:35s
epoch 57 | loss: 0.44852 | eval_custom_logloss: 2.09955 |  0:00:36s
epoch 58 | loss: 0.42965 | eval_custom_logloss: 2.1642  |  0:00:36s
epoch 59 | loss: 0.47488 | eval_custom_logloss: 2.10807 |  0:00:37s
epoch 60 | loss: 0.46835 | eval_custom_logloss: 2.20127 |  0:00:38s
epoch 61 | loss: 0.45977 | eval_custom_logloss: 2.05822 |  0:00:38s
epoch 62 | loss: 0.44237 | eval_custom_logloss: 1.87345 |  0:00:39s
epoch 63 | loss: 0.45072 | eval_custom_logloss: 1.65672 |  0:00:39s
epoch 64 | loss: 0.46142 | eval_custom_logloss: 1.72956 |  0:00:40s
epoch 65 | loss: 0.45631 | eval_custom_logloss: 2.44313 |  0:00:41s
epoch 66 | loss: 0.46212 | eval_custom_logloss: 2.49216 |  0:00:41s
epoch 67 | loss: 0.4253  | eval_custom_logloss: 2.74165 |  0:00:42s
epoch 68 | loss: 0.44975 | eval_custom_logloss: 2.80627 |  0:00:43s
epoch 69 | loss: 0.42879 | eval_custom_logloss: 3.05562 |  0:00:43s
epoch 70 | loss: 0.4324  | eval_custom_logloss: 3.06587 |  0:00:44s
epoch 71 | loss: 0.43062 | eval_custom_logloss: 2.68901 |  0:00:44s
epoch 72 | loss: 0.43312 | eval_custom_logloss: 2.23241 |  0:00:45s
epoch 73 | loss: 0.40991 | eval_custom_logloss: 2.29532 |  0:00:46s
epoch 74 | loss: 0.40668 | eval_custom_logloss: 2.35917 |  0:00:46s
epoch 75 | loss: 0.41376 | eval_custom_logloss: 2.66413 |  0:00:47s
epoch 76 | loss: 0.4297  | eval_custom_logloss: 2.38285 |  0:00:47s
epoch 77 | loss: 0.419   | eval_custom_logloss: 1.48019 |  0:00:48s
epoch 78 | loss: 0.39788 | eval_custom_logloss: 1.28437 |  0:00:49s
epoch 79 | loss: 0.40796 | eval_custom_logloss: 1.46524 |  0:00:49s
epoch 80 | loss: 0.40308 | eval_custom_logloss: 2.16616 |  0:00:50s
epoch 81 | loss: 0.40508 | eval_custom_logloss: 2.6111  |  0:00:51s
epoch 82 | loss: 0.43464 | eval_custom_logloss: 2.05015 |  0:00:51s
epoch 83 | loss: 0.40349 | eval_custom_logloss: 1.76027 |  0:00:52s
epoch 84 | loss: 0.41162 | eval_custom_logloss: 1.81852 |  0:00:52s
epoch 85 | loss: 0.40193 | eval_custom_logloss: 2.06183 |  0:00:53s
epoch 86 | loss: 0.3873  | eval_custom_logloss: 1.97835 |  0:00:54s
epoch 87 | loss: 0.41598 | eval_custom_logloss: 2.01514 |  0:00:54s
epoch 88 | loss: 0.41376 | eval_custom_logloss: 1.96991 |  0:00:55s
epoch 89 | loss: 0.4445  | eval_custom_logloss: 2.30787 |  0:00:55s
epoch 90 | loss: 0.44306 | eval_custom_logloss: 2.34043 |  0:00:56s
epoch 91 | loss: 0.43872 | eval_custom_logloss: 1.353   |  0:00:57s
epoch 92 | loss: 0.46777 | eval_custom_logloss: 1.21022 |  0:00:57s
epoch 93 | loss: 0.44732 | eval_custom_logloss: 2.10213 |  0:00:58s
epoch 94 | loss: 0.44619 | eval_custom_logloss: 1.403   |  0:00:59s
epoch 95 | loss: 0.45331 | eval_custom_logloss: 1.50151 |  0:00:59s
epoch 96 | loss: 0.44677 | eval_custom_logloss: 1.17995 |  0:01:00s
epoch 97 | loss: 0.45816 | eval_custom_logloss: 2.34737 |  0:01:00s
epoch 98 | loss: 0.45672 | eval_custom_logloss: 1.78865 |  0:01:01s
epoch 99 | loss: 0.46277 | eval_custom_logloss: 1.14893 |  0:01:02s
Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_eval_custom_logloss = 1.14893
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.88898, 'Log Loss - std': 0.5673159063520077} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 39 finished with value: 1.88898 and parameters: {'n_d': 34, 'n_steps': 6, 'gamma': 1.1203956888376525, 'cat_emb_dim': 2, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0013144128711803827, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.14418 | eval_custom_logloss: 1.76466 |  0:00:00s
epoch 1  | loss: 0.94871 | eval_custom_logloss: 2.23389 |  0:00:00s
epoch 2  | loss: 0.82147 | eval_custom_logloss: 2.27026 |  0:00:01s
epoch 3  | loss: 0.78791 | eval_custom_logloss: 5.27475 |  0:00:01s
epoch 4  | loss: 0.76341 | eval_custom_logloss: 3.39114 |  0:00:02s
epoch 5  | loss: 0.7527  | eval_custom_logloss: 2.36027 |  0:00:02s
epoch 6  | loss: 0.71071 | eval_custom_logloss: 3.40227 |  0:00:02s
epoch 7  | loss: 0.66677 | eval_custom_logloss: 3.78131 |  0:00:03s
epoch 8  | loss: 0.69555 | eval_custom_logloss: 3.99088 |  0:00:03s
epoch 9  | loss: 0.65514 | eval_custom_logloss: 2.58661 |  0:00:04s
epoch 10 | loss: 0.6529  | eval_custom_logloss: 2.71644 |  0:00:04s
epoch 11 | loss: 0.60535 | eval_custom_logloss: 2.92446 |  0:00:04s
epoch 12 | loss: 0.57752 | eval_custom_logloss: 3.59143 |  0:00:05s
epoch 13 | loss: 0.57627 | eval_custom_logloss: 4.58704 |  0:00:05s
epoch 14 | loss: 0.57021 | eval_custom_logloss: 3.06018 |  0:00:05s
epoch 15 | loss: 0.5537  | eval_custom_logloss: 2.78251 |  0:00:06s
epoch 16 | loss: 0.53746 | eval_custom_logloss: 2.90937 |  0:00:06s
epoch 17 | loss: 0.53374 | eval_custom_logloss: 3.44078 |  0:00:07s
epoch 18 | loss: 0.54488 | eval_custom_logloss: 3.55323 |  0:00:07s
epoch 19 | loss: 0.51202 | eval_custom_logloss: 2.64454 |  0:00:07s
epoch 20 | loss: 0.50032 | eval_custom_logloss: 2.98654 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.76466
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.6117, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13412 | eval_custom_logloss: 2.52772 |  0:00:00s
epoch 1  | loss: 0.92466 | eval_custom_logloss: 2.55581 |  0:00:00s
epoch 2  | loss: 0.79768 | eval_custom_logloss: 3.09656 |  0:00:01s
epoch 3  | loss: 0.76655 | eval_custom_logloss: 3.79789 |  0:00:01s
epoch 4  | loss: 0.69998 | eval_custom_logloss: 2.91316 |  0:00:02s
epoch 5  | loss: 0.70443 | eval_custom_logloss: 4.56847 |  0:00:02s
epoch 6  | loss: 0.61205 | eval_custom_logloss: 4.02531 |  0:00:02s
epoch 7  | loss: 0.61374 | eval_custom_logloss: 3.74391 |  0:00:03s
epoch 8  | loss: 0.61981 | eval_custom_logloss: 3.66776 |  0:00:03s
epoch 9  | loss: 0.59949 | eval_custom_logloss: 3.42344 |  0:00:04s
epoch 10 | loss: 0.5587  | eval_custom_logloss: 4.08733 |  0:00:04s
epoch 11 | loss: 0.52647 | eval_custom_logloss: 4.13506 |  0:00:04s
epoch 12 | loss: 0.48086 | eval_custom_logloss: 3.65724 |  0:00:05s
epoch 13 | loss: 0.50153 | eval_custom_logloss: 4.33286 |  0:00:05s
epoch 14 | loss: 0.4866  | eval_custom_logloss: 4.85533 |  0:00:06s
epoch 15 | loss: 0.49628 | eval_custom_logloss: 4.10119 |  0:00:06s
epoch 16 | loss: 0.46207 | eval_custom_logloss: 3.95375 |  0:00:07s
epoch 17 | loss: 0.4302  | eval_custom_logloss: 3.42695 |  0:00:07s
epoch 18 | loss: 0.47052 | eval_custom_logloss: 3.40115 |  0:00:07s
epoch 19 | loss: 0.45009 | eval_custom_logloss: 2.90159 |  0:00:08s
epoch 20 | loss: 0.44014 | eval_custom_logloss: 2.81656 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.52772
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.9351, 'Log Loss - std': 0.32340000000000013} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.12563 | eval_custom_logloss: 1.97847 |  0:00:00s
epoch 1  | loss: 0.89429 | eval_custom_logloss: 1.98083 |  0:00:00s
epoch 2  | loss: 0.81262 | eval_custom_logloss: 2.60933 |  0:00:01s
epoch 3  | loss: 0.7953  | eval_custom_logloss: 2.98177 |  0:00:01s
epoch 4  | loss: 0.74531 | eval_custom_logloss: 3.60463 |  0:00:02s
epoch 5  | loss: 0.7136  | eval_custom_logloss: 2.70263 |  0:00:02s
epoch 6  | loss: 0.6403  | eval_custom_logloss: 1.55124 |  0:00:02s
epoch 7  | loss: 0.61652 | eval_custom_logloss: 2.90453 |  0:00:03s
epoch 8  | loss: 0.60486 | eval_custom_logloss: 3.32674 |  0:00:03s
epoch 9  | loss: 0.62261 | eval_custom_logloss: 2.7855  |  0:00:04s
epoch 10 | loss: 0.58282 | eval_custom_logloss: 2.23361 |  0:00:04s
epoch 11 | loss: 0.59007 | eval_custom_logloss: 2.31252 |  0:00:04s
epoch 12 | loss: 0.57782 | eval_custom_logloss: 2.73899 |  0:00:05s
epoch 13 | loss: 0.56593 | eval_custom_logloss: 3.2247  |  0:00:05s
epoch 14 | loss: 0.55585 | eval_custom_logloss: 3.45851 |  0:00:06s
epoch 15 | loss: 0.59401 | eval_custom_logloss: 2.88021 |  0:00:06s
epoch 16 | loss: 0.59061 | eval_custom_logloss: 2.35702 |  0:00:07s
epoch 17 | loss: 0.55122 | eval_custom_logloss: 3.73097 |  0:00:07s
epoch 18 | loss: 0.56222 | eval_custom_logloss: 3.50106 |  0:00:07s
epoch 19 | loss: 0.54184 | eval_custom_logloss: 2.94174 |  0:00:08s
epoch 20 | loss: 0.50028 | eval_custom_logloss: 3.48261 |  0:00:08s
epoch 21 | loss: 0.51536 | eval_custom_logloss: 3.08005 |  0:00:09s
epoch 22 | loss: 0.49842 | eval_custom_logloss: 4.21879 |  0:00:09s
epoch 23 | loss: 0.50183 | eval_custom_logloss: 3.87041 |  0:00:09s
epoch 24 | loss: 0.48157 | eval_custom_logloss: 3.82523 |  0:00:10s
epoch 25 | loss: 0.4831  | eval_custom_logloss: 3.64067 |  0:00:10s
epoch 26 | loss: 0.47228 | eval_custom_logloss: 3.09518 |  0:00:11s

Early stopping occurred at epoch 26 with best_epoch = 6 and best_eval_custom_logloss = 1.55124
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.793, 'Log Loss - std': 0.3318280578854056} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.1032  | eval_custom_logloss: 2.45344 |  0:00:00s
epoch 1  | loss: 0.97074 | eval_custom_logloss: 2.87805 |  0:00:00s
epoch 2  | loss: 0.83667 | eval_custom_logloss: 2.80478 |  0:00:01s
epoch 3  | loss: 0.79949 | eval_custom_logloss: 3.02936 |  0:00:01s
epoch 4  | loss: 0.80202 | eval_custom_logloss: 2.81947 |  0:00:01s
epoch 5  | loss: 0.74254 | eval_custom_logloss: 4.39488 |  0:00:02s
epoch 6  | loss: 0.70948 | eval_custom_logloss: 4.77034 |  0:00:02s
epoch 7  | loss: 0.69075 | eval_custom_logloss: 5.46586 |  0:00:03s
epoch 8  | loss: 0.64874 | eval_custom_logloss: 5.13658 |  0:00:03s
epoch 9  | loss: 0.61226 | eval_custom_logloss: 4.82348 |  0:00:03s
epoch 10 | loss: 0.59554 | eval_custom_logloss: 4.5241  |  0:00:04s
epoch 11 | loss: 0.54674 | eval_custom_logloss: 4.57596 |  0:00:04s
epoch 12 | loss: 0.58621 | eval_custom_logloss: 5.19936 |  0:00:05s
epoch 13 | loss: 0.56246 | eval_custom_logloss: 4.60561 |  0:00:05s
epoch 14 | loss: 0.53908 | eval_custom_logloss: 3.35038 |  0:00:06s
epoch 15 | loss: 0.51898 | eval_custom_logloss: 3.56349 |  0:00:06s
epoch 16 | loss: 0.53858 | eval_custom_logloss: 2.61047 |  0:00:06s
epoch 17 | loss: 0.51826 | eval_custom_logloss: 2.34783 |  0:00:07s
epoch 18 | loss: 0.51139 | eval_custom_logloss: 2.18129 |  0:00:07s
epoch 19 | loss: 0.50339 | eval_custom_logloss: 2.37504 |  0:00:08s
epoch 20 | loss: 0.49553 | eval_custom_logloss: 2.58517 |  0:00:08s
epoch 21 | loss: 0.4809  | eval_custom_logloss: 2.96995 |  0:00:09s
epoch 22 | loss: 0.47017 | eval_custom_logloss: 2.67903 |  0:00:09s
epoch 23 | loss: 0.46879 | eval_custom_logloss: 2.38157 |  0:00:09s
epoch 24 | loss: 0.43396 | eval_custom_logloss: 2.3261  |  0:00:10s
epoch 25 | loss: 0.42096 | eval_custom_logloss: 2.1529  |  0:00:10s
epoch 26 | loss: 0.41078 | eval_custom_logloss: 2.54009 |  0:00:11s
epoch 27 | loss: 0.42329 | eval_custom_logloss: 2.97143 |  0:00:11s
epoch 28 | loss: 0.39532 | eval_custom_logloss: 2.51949 |  0:00:11s
epoch 29 | loss: 0.39238 | eval_custom_logloss: 2.16703 |  0:00:12s
epoch 30 | loss: 0.37893 | eval_custom_logloss: 2.46726 |  0:00:12s
epoch 31 | loss: 0.39747 | eval_custom_logloss: 2.11811 |  0:00:13s
epoch 32 | loss: 0.37203 | eval_custom_logloss: 2.18472 |  0:00:13s
epoch 33 | loss: 0.3872  | eval_custom_logloss: 2.20616 |  0:00:13s
epoch 34 | loss: 0.38461 | eval_custom_logloss: 2.42077 |  0:00:14s
epoch 35 | loss: 0.37058 | eval_custom_logloss: 2.06389 |  0:00:14s
epoch 36 | loss: 0.34726 | eval_custom_logloss: 2.84627 |  0:00:15s
epoch 37 | loss: 0.33522 | eval_custom_logloss: 2.2801  |  0:00:15s
epoch 38 | loss: 0.32393 | eval_custom_logloss: 2.39341 |  0:00:15s
epoch 39 | loss: 0.33107 | eval_custom_logloss: 2.21371 |  0:00:16s
epoch 40 | loss: 0.31351 | eval_custom_logloss: 2.48311 |  0:00:16s
epoch 41 | loss: 0.28356 | eval_custom_logloss: 2.79296 |  0:00:17s
epoch 42 | loss: 0.31562 | eval_custom_logloss: 2.33097 |  0:00:17s
epoch 43 | loss: 0.29105 | eval_custom_logloss: 2.55665 |  0:00:17s
epoch 44 | loss: 0.29438 | eval_custom_logloss: 2.56636 |  0:00:18s
epoch 45 | loss: 0.30043 | eval_custom_logloss: 3.29015 |  0:00:18s
epoch 46 | loss: 0.30046 | eval_custom_logloss: 2.59113 |  0:00:19s
epoch 47 | loss: 0.26802 | eval_custom_logloss: 2.87107 |  0:00:19s
epoch 48 | loss: 0.26417 | eval_custom_logloss: 2.74729 |  0:00:19s
epoch 49 | loss: 0.25926 | eval_custom_logloss: 2.38785 |  0:00:20s
epoch 50 | loss: 0.24938 | eval_custom_logloss: 2.53926 |  0:00:20s
epoch 51 | loss: 0.26583 | eval_custom_logloss: 2.81263 |  0:00:21s
epoch 52 | loss: 0.25259 | eval_custom_logloss: 3.11908 |  0:00:21s
epoch 53 | loss: 0.2405  | eval_custom_logloss: 2.63419 |  0:00:21s
epoch 54 | loss: 0.22142 | eval_custom_logloss: 2.95506 |  0:00:22s
epoch 55 | loss: 0.23358 | eval_custom_logloss: 2.96597 |  0:00:22s

Early stopping occurred at epoch 55 with best_epoch = 35 and best_eval_custom_logloss = 2.06389
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8117249999999998, 'Log Loss - std': 0.28919590570234577} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.15768 | eval_custom_logloss: 2.20566 |  0:00:00s
epoch 1  | loss: 0.88581 | eval_custom_logloss: 3.55419 |  0:00:00s
epoch 2  | loss: 0.86072 | eval_custom_logloss: 3.49474 |  0:00:01s
epoch 3  | loss: 0.81117 | eval_custom_logloss: 2.55113 |  0:00:01s
epoch 4  | loss: 0.75063 | eval_custom_logloss: 4.05296 |  0:00:02s
epoch 5  | loss: 0.72624 | eval_custom_logloss: 4.08636 |  0:00:02s
epoch 6  | loss: 0.69823 | eval_custom_logloss: 3.24668 |  0:00:02s
epoch 7  | loss: 0.64197 | eval_custom_logloss: 3.54886 |  0:00:03s
epoch 8  | loss: 0.61536 | eval_custom_logloss: 3.48365 |  0:00:03s
epoch 9  | loss: 0.65056 | eval_custom_logloss: 4.48878 |  0:00:03s
epoch 10 | loss: 0.59588 | eval_custom_logloss: 2.8949  |  0:00:04s
epoch 11 | loss: 0.54273 | eval_custom_logloss: 2.98535 |  0:00:04s
epoch 12 | loss: 0.55026 | eval_custom_logloss: 3.08943 |  0:00:05s
epoch 13 | loss: 0.53585 | eval_custom_logloss: 4.17223 |  0:00:05s
epoch 14 | loss: 0.51909 | eval_custom_logloss: 3.20083 |  0:00:05s
epoch 15 | loss: 0.51413 | eval_custom_logloss: 2.58488 |  0:00:06s
epoch 16 | loss: 0.50393 | eval_custom_logloss: 2.26369 |  0:00:06s
epoch 17 | loss: 0.52996 | eval_custom_logloss: 2.22304 |  0:00:07s
epoch 18 | loss: 0.54693 | eval_custom_logloss: 2.90344 |  0:00:07s
epoch 19 | loss: 0.51535 | eval_custom_logloss: 3.4665  |  0:00:07s
epoch 20 | loss: 0.49554 | eval_custom_logloss: 2.82937 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 2.20566
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8622999999999998, 'Log Loss - std': 0.2777386181286284} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 40 finished with value: 1.8622999999999998 and parameters: {'n_d': 14, 'n_steps': 4, 'gamma': 1.2073568362910976, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.0010819404168705266, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.622   | eval_custom_logloss: 1.08225 |  0:00:00s
epoch 1  | loss: 1.03704 | eval_custom_logloss: 1.18157 |  0:00:00s
epoch 2  | loss: 0.89721 | eval_custom_logloss: 0.69656 |  0:00:01s
epoch 3  | loss: 0.82104 | eval_custom_logloss: 0.65709 |  0:00:01s
epoch 4  | loss: 0.72091 | eval_custom_logloss: 0.73823 |  0:00:02s
epoch 5  | loss: 0.73898 | eval_custom_logloss: 0.64891 |  0:00:02s
epoch 6  | loss: 0.6827  | eval_custom_logloss: 0.64531 |  0:00:02s
epoch 7  | loss: 0.64909 | eval_custom_logloss: 0.87408 |  0:00:03s
epoch 8  | loss: 0.65945 | eval_custom_logloss: 0.68229 |  0:00:03s
epoch 9  | loss: 0.60553 | eval_custom_logloss: 0.64318 |  0:00:03s
epoch 10 | loss: 0.70342 | eval_custom_logloss: 0.70038 |  0:00:04s
epoch 11 | loss: 0.62277 | eval_custom_logloss: 0.59596 |  0:00:04s
epoch 12 | loss: 0.62153 | eval_custom_logloss: 0.58102 |  0:00:05s
epoch 13 | loss: 0.56871 | eval_custom_logloss: 0.50127 |  0:00:05s
epoch 14 | loss: 0.5449  | eval_custom_logloss: 0.52057 |  0:00:05s
epoch 15 | loss: 0.56042 | eval_custom_logloss: 0.50343 |  0:00:06s
epoch 16 | loss: 0.54771 | eval_custom_logloss: 0.49785 |  0:00:06s
epoch 17 | loss: 0.56649 | eval_custom_logloss: 0.57166 |  0:00:07s
epoch 18 | loss: 0.53599 | eval_custom_logloss: 0.55316 |  0:00:07s
epoch 19 | loss: 0.5736  | eval_custom_logloss: 0.61377 |  0:00:07s
epoch 20 | loss: 0.602   | eval_custom_logloss: 0.579   |  0:00:08s
epoch 21 | loss: 0.54982 | eval_custom_logloss: 0.53853 |  0:00:08s
epoch 22 | loss: 0.55063 | eval_custom_logloss: 0.53477 |  0:00:09s
epoch 23 | loss: 0.52314 | eval_custom_logloss: 0.53271 |  0:00:09s
epoch 24 | loss: 0.50451 | eval_custom_logloss: 0.56888 |  0:00:09s
epoch 25 | loss: 0.5213  | eval_custom_logloss: 0.51887 |  0:00:10s
epoch 26 | loss: 0.51088 | eval_custom_logloss: 0.5312  |  0:00:10s
epoch 27 | loss: 0.57017 | eval_custom_logloss: 0.53688 |  0:00:11s
epoch 28 | loss: 0.57603 | eval_custom_logloss: 0.55675 |  0:00:11s
epoch 29 | loss: 0.53914 | eval_custom_logloss: 0.52389 |  0:00:12s
epoch 30 | loss: 0.51027 | eval_custom_logloss: 0.56967 |  0:00:12s
epoch 31 | loss: 0.5628  | eval_custom_logloss: 0.54137 |  0:00:13s
epoch 32 | loss: 0.56818 | eval_custom_logloss: 0.52448 |  0:00:13s
epoch 33 | loss: 0.53569 | eval_custom_logloss: 0.53624 |  0:00:13s
epoch 34 | loss: 0.54594 | eval_custom_logloss: 0.53032 |  0:00:14s
epoch 35 | loss: 0.49509 | eval_custom_logloss: 0.48733 |  0:00:14s
epoch 36 | loss: 0.50579 | eval_custom_logloss: 0.50865 |  0:00:15s
epoch 37 | loss: 0.53734 | eval_custom_logloss: 0.52863 |  0:00:15s
epoch 38 | loss: 0.51274 | eval_custom_logloss: 0.52839 |  0:00:16s
epoch 39 | loss: 0.49794 | eval_custom_logloss: 0.48629 |  0:00:16s
epoch 40 | loss: 0.48179 | eval_custom_logloss: 0.47431 |  0:00:17s
epoch 41 | loss: 0.49146 | eval_custom_logloss: 0.49761 |  0:00:17s
epoch 42 | loss: 0.45028 | eval_custom_logloss: 0.49729 |  0:00:18s
epoch 43 | loss: 0.44581 | eval_custom_logloss: 0.53667 |  0:00:18s
epoch 44 | loss: 0.42711 | eval_custom_logloss: 0.50454 |  0:00:18s
epoch 45 | loss: 0.447   | eval_custom_logloss: 0.5394  |  0:00:19s
epoch 46 | loss: 0.48819 | eval_custom_logloss: 0.6864  |  0:00:19s
epoch 47 | loss: 0.55664 | eval_custom_logloss: 0.57044 |  0:00:20s
epoch 48 | loss: 0.49153 | eval_custom_logloss: 0.53542 |  0:00:20s
epoch 49 | loss: 0.50969 | eval_custom_logloss: 0.53834 |  0:00:21s
epoch 50 | loss: 0.48196 | eval_custom_logloss: 0.50398 |  0:00:21s
epoch 51 | loss: 0.45451 | eval_custom_logloss: 0.48381 |  0:00:22s
epoch 52 | loss: 0.44982 | eval_custom_logloss: 0.53436 |  0:00:22s
epoch 53 | loss: 0.46857 | eval_custom_logloss: 0.47713 |  0:00:23s
epoch 54 | loss: 0.45114 | eval_custom_logloss: 0.47048 |  0:00:23s
epoch 55 | loss: 0.4064  | eval_custom_logloss: 0.4559  |  0:00:23s
epoch 56 | loss: 0.41766 | eval_custom_logloss: 0.50363 |  0:00:24s
epoch 57 | loss: 0.44482 | eval_custom_logloss: 0.46845 |  0:00:24s
epoch 58 | loss: 0.4816  | eval_custom_logloss: 0.47283 |  0:00:25s
epoch 59 | loss: 0.48431 | eval_custom_logloss: 0.44569 |  0:00:25s
epoch 60 | loss: 0.42234 | eval_custom_logloss: 0.44033 |  0:00:26s
epoch 61 | loss: 0.41027 | eval_custom_logloss: 0.444   |  0:00:26s
epoch 62 | loss: 0.43542 | eval_custom_logloss: 0.47113 |  0:00:27s
epoch 63 | loss: 0.43291 | eval_custom_logloss: 0.56469 |  0:00:27s
epoch 64 | loss: 0.49661 | eval_custom_logloss: 0.50977 |  0:00:27s
epoch 65 | loss: 0.45397 | eval_custom_logloss: 0.46567 |  0:00:28s
epoch 66 | loss: 0.42152 | eval_custom_logloss: 0.4574  |  0:00:28s
epoch 67 | loss: 0.41547 | eval_custom_logloss: 0.46033 |  0:00:29s
epoch 68 | loss: 0.42441 | eval_custom_logloss: 0.49034 |  0:00:29s
epoch 69 | loss: 0.41815 | eval_custom_logloss: 0.44964 |  0:00:29s
epoch 70 | loss: 0.44463 | eval_custom_logloss: 0.4763  |  0:00:30s
epoch 71 | loss: 0.42566 | eval_custom_logloss: 0.49366 |  0:00:30s
epoch 72 | loss: 0.39812 | eval_custom_logloss: 0.50043 |  0:00:31s
epoch 73 | loss: 0.4055  | eval_custom_logloss: 0.49079 |  0:00:31s
epoch 74 | loss: 0.37603 | eval_custom_logloss: 0.45757 |  0:00:31s
epoch 75 | loss: 0.45425 | eval_custom_logloss: 0.46651 |  0:00:32s
epoch 76 | loss: 0.41136 | eval_custom_logloss: 0.52101 |  0:00:32s
epoch 77 | loss: 0.419   | eval_custom_logloss: 0.47724 |  0:00:33s
epoch 78 | loss: 0.42755 | eval_custom_logloss: 0.48909 |  0:00:33s
epoch 79 | loss: 0.40174 | eval_custom_logloss: 0.48731 |  0:00:33s
epoch 80 | loss: 0.38595 | eval_custom_logloss: 0.48519 |  0:00:34s

Early stopping occurred at epoch 80 with best_epoch = 60 and best_eval_custom_logloss = 0.44033
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4275, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.7022  | eval_custom_logloss: 1.19789 |  0:00:00s
epoch 1  | loss: 1.07841 | eval_custom_logloss: 1.08894 |  0:00:00s
epoch 2  | loss: 0.83906 | eval_custom_logloss: 0.86295 |  0:00:01s
epoch 3  | loss: 0.87857 | eval_custom_logloss: 0.90022 |  0:00:01s
epoch 4  | loss: 0.81468 | eval_custom_logloss: 0.78801 |  0:00:01s
epoch 5  | loss: 0.70076 | eval_custom_logloss: 0.77355 |  0:00:02s
epoch 6  | loss: 0.6108  | eval_custom_logloss: 0.71268 |  0:00:02s
epoch 7  | loss: 0.56631 | eval_custom_logloss: 0.73643 |  0:00:03s
epoch 8  | loss: 0.63977 | eval_custom_logloss: 0.73818 |  0:00:03s
epoch 9  | loss: 0.57354 | eval_custom_logloss: 0.65137 |  0:00:03s
epoch 10 | loss: 0.58487 | eval_custom_logloss: 0.68013 |  0:00:04s
epoch 11 | loss: 0.56331 | eval_custom_logloss: 0.59538 |  0:00:04s
epoch 12 | loss: 0.56871 | eval_custom_logloss: 0.64989 |  0:00:05s
epoch 13 | loss: 0.53816 | eval_custom_logloss: 0.6323  |  0:00:05s
epoch 14 | loss: 0.53622 | eval_custom_logloss: 0.66878 |  0:00:05s
epoch 15 | loss: 0.50908 | eval_custom_logloss: 0.6284  |  0:00:06s
epoch 16 | loss: 0.51097 | eval_custom_logloss: 0.62053 |  0:00:06s
epoch 17 | loss: 0.51869 | eval_custom_logloss: 0.60858 |  0:00:07s
epoch 18 | loss: 0.47475 | eval_custom_logloss: 0.57782 |  0:00:07s
epoch 19 | loss: 0.49479 | eval_custom_logloss: 0.60716 |  0:00:07s
epoch 20 | loss: 0.49016 | eval_custom_logloss: 0.65548 |  0:00:08s
epoch 21 | loss: 0.52185 | eval_custom_logloss: 0.61238 |  0:00:08s
epoch 22 | loss: 0.53142 | eval_custom_logloss: 0.65153 |  0:00:08s
epoch 23 | loss: 0.51587 | eval_custom_logloss: 0.64388 |  0:00:09s
epoch 24 | loss: 0.49474 | eval_custom_logloss: 0.60726 |  0:00:09s
epoch 25 | loss: 0.51989 | eval_custom_logloss: 0.60655 |  0:00:10s
epoch 26 | loss: 0.49937 | eval_custom_logloss: 0.61474 |  0:00:10s
epoch 27 | loss: 0.48362 | eval_custom_logloss: 0.63118 |  0:00:10s
epoch 28 | loss: 0.48388 | eval_custom_logloss: 0.62924 |  0:00:11s
epoch 29 | loss: 0.53325 | eval_custom_logloss: 0.64056 |  0:00:11s
epoch 30 | loss: 0.50166 | eval_custom_logloss: 0.59828 |  0:00:12s
epoch 31 | loss: 0.4583  | eval_custom_logloss: 0.58855 |  0:00:12s
epoch 32 | loss: 0.42202 | eval_custom_logloss: 0.55573 |  0:00:12s
epoch 33 | loss: 0.40472 | eval_custom_logloss: 0.58695 |  0:00:13s
epoch 34 | loss: 0.40736 | eval_custom_logloss: 0.58779 |  0:00:13s
epoch 35 | loss: 0.46985 | eval_custom_logloss: 0.61818 |  0:00:13s
epoch 36 | loss: 0.4607  | eval_custom_logloss: 0.58106 |  0:00:14s
epoch 37 | loss: 0.47922 | eval_custom_logloss: 0.70527 |  0:00:14s
epoch 38 | loss: 0.48783 | eval_custom_logloss: 0.55696 |  0:00:15s
epoch 39 | loss: 0.43889 | eval_custom_logloss: 0.5869  |  0:00:15s
epoch 40 | loss: 0.40826 | eval_custom_logloss: 0.619   |  0:00:15s
epoch 41 | loss: 0.43285 | eval_custom_logloss: 0.56576 |  0:00:16s
epoch 42 | loss: 0.4189  | eval_custom_logloss: 0.56267 |  0:00:16s
epoch 43 | loss: 0.36855 | eval_custom_logloss: 0.5317  |  0:00:17s
epoch 44 | loss: 0.356   | eval_custom_logloss: 0.52841 |  0:00:17s
epoch 45 | loss: 0.35322 | eval_custom_logloss: 0.50367 |  0:00:17s
epoch 46 | loss: 0.37136 | eval_custom_logloss: 0.50441 |  0:00:18s
epoch 47 | loss: 0.36524 | eval_custom_logloss: 0.45698 |  0:00:18s
epoch 48 | loss: 0.34572 | eval_custom_logloss: 0.50282 |  0:00:18s
epoch 49 | loss: 0.31963 | eval_custom_logloss: 0.45812 |  0:00:19s
epoch 50 | loss: 0.33712 | eval_custom_logloss: 0.5995  |  0:00:19s
epoch 51 | loss: 0.35782 | eval_custom_logloss: 0.55383 |  0:00:20s
epoch 52 | loss: 0.36014 | eval_custom_logloss: 0.50174 |  0:00:20s
epoch 53 | loss: 0.35347 | eval_custom_logloss: 0.43471 |  0:00:20s
epoch 54 | loss: 0.31626 | eval_custom_logloss: 0.47862 |  0:00:21s
epoch 55 | loss: 0.31746 | eval_custom_logloss: 0.49488 |  0:00:21s
epoch 56 | loss: 0.30887 | eval_custom_logloss: 0.46298 |  0:00:22s
epoch 57 | loss: 0.28803 | eval_custom_logloss: 0.49311 |  0:00:22s
epoch 58 | loss: 0.29416 | eval_custom_logloss: 0.49094 |  0:00:22s
epoch 59 | loss: 0.28599 | eval_custom_logloss: 0.50263 |  0:00:23s
epoch 60 | loss: 0.29641 | eval_custom_logloss: 0.48409 |  0:00:23s
epoch 61 | loss: 0.24901 | eval_custom_logloss: 0.51936 |  0:00:24s
epoch 62 | loss: 0.26665 | eval_custom_logloss: 0.48131 |  0:00:24s
epoch 63 | loss: 0.2444  | eval_custom_logloss: 0.47151 |  0:00:24s
epoch 64 | loss: 0.26678 | eval_custom_logloss: 0.4956  |  0:00:25s
epoch 65 | loss: 0.24763 | eval_custom_logloss: 0.48838 |  0:00:25s
epoch 66 | loss: 0.27133 | eval_custom_logloss: 0.54083 |  0:00:26s
epoch 67 | loss: 0.27863 | eval_custom_logloss: 0.51902 |  0:00:26s
epoch 68 | loss: 0.26343 | eval_custom_logloss: 0.42012 |  0:00:26s
epoch 69 | loss: 0.26567 | eval_custom_logloss: 0.48221 |  0:00:27s
epoch 70 | loss: 0.2624  | eval_custom_logloss: 0.50422 |  0:00:27s
epoch 71 | loss: 0.28734 | eval_custom_logloss: 0.44842 |  0:00:28s
epoch 72 | loss: 0.23464 | eval_custom_logloss: 0.40996 |  0:00:28s
epoch 73 | loss: 0.24758 | eval_custom_logloss: 0.48584 |  0:00:28s
epoch 74 | loss: 0.26178 | eval_custom_logloss: 0.46169 |  0:00:29s
epoch 75 | loss: 0.25092 | eval_custom_logloss: 0.49908 |  0:00:29s
epoch 76 | loss: 0.25849 | eval_custom_logloss: 0.545   |  0:00:30s
epoch 77 | loss: 0.24841 | eval_custom_logloss: 0.50765 |  0:00:30s
epoch 78 | loss: 0.24284 | eval_custom_logloss: 0.46407 |  0:00:31s
epoch 79 | loss: 0.21976 | eval_custom_logloss: 0.49884 |  0:00:31s
epoch 80 | loss: 0.24319 | eval_custom_logloss: 0.46923 |  0:00:31s
epoch 81 | loss: 0.22473 | eval_custom_logloss: 0.46334 |  0:00:32s
epoch 82 | loss: 0.22296 | eval_custom_logloss: 0.49196 |  0:00:32s
epoch 83 | loss: 0.20669 | eval_custom_logloss: 0.50874 |  0:00:32s
epoch 84 | loss: 0.22108 | eval_custom_logloss: 0.50963 |  0:00:33s
epoch 85 | loss: 0.23152 | eval_custom_logloss: 0.4792  |  0:00:33s
epoch 86 | loss: 0.24738 | eval_custom_logloss: 0.49288 |  0:00:34s
epoch 87 | loss: 0.23897 | eval_custom_logloss: 0.48432 |  0:00:34s
epoch 88 | loss: 0.2435  | eval_custom_logloss: 0.50233 |  0:00:34s
epoch 89 | loss: 0.2349  | eval_custom_logloss: 0.5639  |  0:00:35s
epoch 90 | loss: 0.2175  | eval_custom_logloss: 0.47925 |  0:00:35s
epoch 91 | loss: 0.22351 | eval_custom_logloss: 0.52045 |  0:00:35s
epoch 92 | loss: 0.21165 | eval_custom_logloss: 0.61733 |  0:00:36s

Early stopping occurred at epoch 92 with best_epoch = 72 and best_eval_custom_logloss = 0.40996
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.41874999999999996, 'Log Loss - std': 0.008750000000000008} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.65871 | eval_custom_logloss: 0.96405 |  0:00:00s
epoch 1  | loss: 1.10191 | eval_custom_logloss: 1.01118 |  0:00:00s
epoch 2  | loss: 0.9521  | eval_custom_logloss: 0.9705  |  0:00:01s
epoch 3  | loss: 0.88811 | eval_custom_logloss: 0.79902 |  0:00:01s
epoch 4  | loss: 0.83452 | eval_custom_logloss: 0.79879 |  0:00:01s
epoch 5  | loss: 0.76977 | eval_custom_logloss: 1.00274 |  0:00:02s
epoch 6  | loss: 0.78225 | eval_custom_logloss: 0.76446 |  0:00:02s
epoch 7  | loss: 0.67121 | eval_custom_logloss: 0.69223 |  0:00:03s
epoch 8  | loss: 0.63575 | eval_custom_logloss: 0.61967 |  0:00:03s
epoch 9  | loss: 0.59046 | eval_custom_logloss: 0.5996  |  0:00:03s
epoch 10 | loss: 0.63275 | eval_custom_logloss: 0.67396 |  0:00:04s
epoch 11 | loss: 0.64609 | eval_custom_logloss: 0.58855 |  0:00:04s
epoch 12 | loss: 0.56376 | eval_custom_logloss: 0.57457 |  0:00:05s
epoch 13 | loss: 0.53438 | eval_custom_logloss: 0.52438 |  0:00:05s
epoch 14 | loss: 0.57928 | eval_custom_logloss: 0.55352 |  0:00:06s
epoch 15 | loss: 0.5983  | eval_custom_logloss: 0.54003 |  0:00:06s
epoch 16 | loss: 0.5766  | eval_custom_logloss: 0.53287 |  0:00:06s
epoch 17 | loss: 0.53046 | eval_custom_logloss: 0.51766 |  0:00:07s
epoch 18 | loss: 0.47787 | eval_custom_logloss: 0.53238 |  0:00:07s
epoch 19 | loss: 0.48023 | eval_custom_logloss: 0.53591 |  0:00:08s
epoch 20 | loss: 0.48627 | eval_custom_logloss: 0.56202 |  0:00:08s
epoch 21 | loss: 0.47709 | eval_custom_logloss: 0.45839 |  0:00:09s
epoch 22 | loss: 0.45706 | eval_custom_logloss: 0.46984 |  0:00:09s
epoch 23 | loss: 0.45562 | eval_custom_logloss: 0.4592  |  0:00:09s
epoch 24 | loss: 0.40033 | eval_custom_logloss: 0.44758 |  0:00:10s
epoch 25 | loss: 0.42143 | eval_custom_logloss: 0.41847 |  0:00:10s
epoch 26 | loss: 0.39692 | eval_custom_logloss: 0.47822 |  0:00:11s
epoch 27 | loss: 0.39972 | eval_custom_logloss: 0.46451 |  0:00:11s
epoch 28 | loss: 0.36925 | eval_custom_logloss: 0.4272  |  0:00:11s
epoch 29 | loss: 0.39082 | eval_custom_logloss: 0.39701 |  0:00:12s
epoch 30 | loss: 0.3852  | eval_custom_logloss: 0.47423 |  0:00:12s
epoch 31 | loss: 0.36997 | eval_custom_logloss: 0.4553  |  0:00:13s
epoch 32 | loss: 0.36777 | eval_custom_logloss: 0.4577  |  0:00:13s
epoch 33 | loss: 0.38505 | eval_custom_logloss: 0.44348 |  0:00:14s
epoch 34 | loss: 0.43147 | eval_custom_logloss: 0.49504 |  0:00:14s
epoch 35 | loss: 0.3996  | eval_custom_logloss: 0.44021 |  0:00:14s
epoch 36 | loss: 0.38263 | eval_custom_logloss: 0.38364 |  0:00:15s
epoch 37 | loss: 0.389   | eval_custom_logloss: 0.46731 |  0:00:15s
epoch 38 | loss: 0.44531 | eval_custom_logloss: 0.46371 |  0:00:16s
epoch 39 | loss: 0.44741 | eval_custom_logloss: 0.48758 |  0:00:16s
epoch 40 | loss: 0.38412 | eval_custom_logloss: 0.43226 |  0:00:16s
epoch 41 | loss: 0.38882 | eval_custom_logloss: 0.44054 |  0:00:17s
epoch 42 | loss: 0.35761 | eval_custom_logloss: 0.4127  |  0:00:17s
epoch 43 | loss: 0.37676 | eval_custom_logloss: 0.43618 |  0:00:18s
epoch 44 | loss: 0.35246 | eval_custom_logloss: 0.4135  |  0:00:18s
epoch 45 | loss: 0.32874 | eval_custom_logloss: 0.40058 |  0:00:18s
epoch 46 | loss: 0.32018 | eval_custom_logloss: 0.4046  |  0:00:19s
epoch 47 | loss: 0.3184  | eval_custom_logloss: 0.37302 |  0:00:19s
epoch 48 | loss: 0.34153 | eval_custom_logloss: 0.38061 |  0:00:20s
epoch 49 | loss: 0.35706 | eval_custom_logloss: 0.4503  |  0:00:20s
epoch 50 | loss: 0.41105 | eval_custom_logloss: 0.42839 |  0:00:21s
epoch 51 | loss: 0.3757  | eval_custom_logloss: 0.45006 |  0:00:21s
epoch 52 | loss: 0.35832 | eval_custom_logloss: 0.39864 |  0:00:21s
epoch 53 | loss: 0.37948 | eval_custom_logloss: 0.4707  |  0:00:22s
epoch 54 | loss: 0.37537 | eval_custom_logloss: 0.42383 |  0:00:22s
epoch 55 | loss: 0.37055 | eval_custom_logloss: 0.44595 |  0:00:23s
epoch 56 | loss: 0.37517 | eval_custom_logloss: 0.4472  |  0:00:23s
epoch 57 | loss: 0.35358 | eval_custom_logloss: 0.4566  |  0:00:24s
epoch 58 | loss: 0.38393 | eval_custom_logloss: 0.43664 |  0:00:24s
epoch 59 | loss: 0.35603 | eval_custom_logloss: 0.41703 |  0:00:25s
epoch 60 | loss: 0.34561 | eval_custom_logloss: 0.4427  |  0:00:25s
epoch 61 | loss: 0.32666 | eval_custom_logloss: 0.4286  |  0:00:26s
epoch 62 | loss: 0.33821 | eval_custom_logloss: 0.4179  |  0:00:26s
epoch 63 | loss: 0.31166 | eval_custom_logloss: 0.4121  |  0:00:26s
epoch 64 | loss: 0.33265 | eval_custom_logloss: 0.44231 |  0:00:27s
epoch 65 | loss: 0.32438 | eval_custom_logloss: 0.38698 |  0:00:27s
epoch 66 | loss: 0.30748 | eval_custom_logloss: 0.3963  |  0:00:28s
epoch 67 | loss: 0.32536 | eval_custom_logloss: 0.40281 |  0:00:28s

Early stopping occurred at epoch 67 with best_epoch = 47 and best_eval_custom_logloss = 0.37302
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.40349999999999997, 'Log Loss - std': 0.02271930163245927} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.72055 | eval_custom_logloss: 1.07009 |  0:00:00s
epoch 1  | loss: 0.9148  | eval_custom_logloss: 0.97435 |  0:00:00s
epoch 2  | loss: 0.87062 | eval_custom_logloss: 0.75378 |  0:00:01s
epoch 3  | loss: 0.75005 | eval_custom_logloss: 0.72082 |  0:00:01s
epoch 4  | loss: 0.685   | eval_custom_logloss: 0.66214 |  0:00:02s
epoch 5  | loss: 0.67211 | eval_custom_logloss: 0.61589 |  0:00:02s
epoch 6  | loss: 0.63522 | eval_custom_logloss: 0.67787 |  0:00:03s
epoch 7  | loss: 0.61331 | eval_custom_logloss: 0.60907 |  0:00:03s
epoch 8  | loss: 0.61693 | eval_custom_logloss: 0.63734 |  0:00:03s
epoch 9  | loss: 0.59599 | eval_custom_logloss: 0.63584 |  0:00:04s
epoch 10 | loss: 0.58178 | eval_custom_logloss: 0.59123 |  0:00:04s
epoch 11 | loss: 0.57628 | eval_custom_logloss: 0.55458 |  0:00:04s
epoch 12 | loss: 0.51972 | eval_custom_logloss: 0.59873 |  0:00:05s
epoch 13 | loss: 0.54728 | eval_custom_logloss: 0.59574 |  0:00:05s
epoch 14 | loss: 0.51841 | eval_custom_logloss: 0.56637 |  0:00:06s
epoch 15 | loss: 0.48909 | eval_custom_logloss: 0.57667 |  0:00:06s
epoch 16 | loss: 0.48313 | eval_custom_logloss: 0.56408 |  0:00:06s
epoch 17 | loss: 0.49935 | eval_custom_logloss: 0.57169 |  0:00:07s
epoch 18 | loss: 0.47478 | eval_custom_logloss: 0.54792 |  0:00:07s
epoch 19 | loss: 0.45273 | eval_custom_logloss: 0.59593 |  0:00:08s
epoch 20 | loss: 0.4377  | eval_custom_logloss: 0.50623 |  0:00:08s
epoch 21 | loss: 0.4126  | eval_custom_logloss: 0.53172 |  0:00:08s
epoch 22 | loss: 0.43136 | eval_custom_logloss: 0.56653 |  0:00:09s
epoch 23 | loss: 0.42426 | eval_custom_logloss: 0.53552 |  0:00:09s
epoch 24 | loss: 0.41633 | eval_custom_logloss: 0.5475  |  0:00:10s
epoch 25 | loss: 0.40751 | eval_custom_logloss: 0.50819 |  0:00:10s
epoch 26 | loss: 0.44497 | eval_custom_logloss: 0.56948 |  0:00:10s
epoch 27 | loss: 0.49647 | eval_custom_logloss: 0.59483 |  0:00:11s
epoch 28 | loss: 0.45653 | eval_custom_logloss: 0.55449 |  0:00:11s
epoch 29 | loss: 0.44661 | eval_custom_logloss: 0.50271 |  0:00:12s
epoch 30 | loss: 0.46334 | eval_custom_logloss: 0.52774 |  0:00:12s
epoch 31 | loss: 0.42331 | eval_custom_logloss: 0.49526 |  0:00:12s
epoch 32 | loss: 0.42233 | eval_custom_logloss: 0.53854 |  0:00:13s
epoch 33 | loss: 0.40365 | eval_custom_logloss: 0.50753 |  0:00:13s
epoch 34 | loss: 0.40744 | eval_custom_logloss: 0.49836 |  0:00:14s
epoch 35 | loss: 0.37859 | eval_custom_logloss: 0.52434 |  0:00:14s
epoch 36 | loss: 0.38191 | eval_custom_logloss: 0.51342 |  0:00:14s
epoch 37 | loss: 0.40849 | eval_custom_logloss: 0.49477 |  0:00:15s
epoch 38 | loss: 0.36998 | eval_custom_logloss: 0.47446 |  0:00:15s
epoch 39 | loss: 0.36695 | eval_custom_logloss: 0.48146 |  0:00:16s
epoch 40 | loss: 0.37271 | eval_custom_logloss: 0.46846 |  0:00:16s
epoch 41 | loss: 0.34717 | eval_custom_logloss: 0.46531 |  0:00:16s
epoch 42 | loss: 0.32087 | eval_custom_logloss: 0.48265 |  0:00:17s
epoch 43 | loss: 0.31308 | eval_custom_logloss: 0.46897 |  0:00:17s
epoch 44 | loss: 0.34088 | eval_custom_logloss: 0.51485 |  0:00:18s
epoch 45 | loss: 0.35704 | eval_custom_logloss: 0.51012 |  0:00:18s
epoch 46 | loss: 0.3535  | eval_custom_logloss: 0.48823 |  0:00:18s
epoch 47 | loss: 0.37438 | eval_custom_logloss: 0.47558 |  0:00:19s
epoch 48 | loss: 0.34792 | eval_custom_logloss: 0.46299 |  0:00:19s
epoch 49 | loss: 0.35862 | eval_custom_logloss: 0.48737 |  0:00:20s
epoch 50 | loss: 0.34049 | eval_custom_logloss: 0.51809 |  0:00:20s
epoch 51 | loss: 0.38854 | eval_custom_logloss: 0.45396 |  0:00:20s
epoch 52 | loss: 0.36788 | eval_custom_logloss: 0.44658 |  0:00:21s
epoch 53 | loss: 0.35014 | eval_custom_logloss: 0.47338 |  0:00:21s
epoch 54 | loss: 0.34709 | eval_custom_logloss: 0.46357 |  0:00:22s
epoch 55 | loss: 0.33204 | eval_custom_logloss: 0.49826 |  0:00:22s
epoch 56 | loss: 0.31606 | eval_custom_logloss: 0.48673 |  0:00:23s
epoch 57 | loss: 0.31766 | eval_custom_logloss: 0.48154 |  0:00:23s
epoch 58 | loss: 0.34235 | eval_custom_logloss: 0.48387 |  0:00:24s
epoch 59 | loss: 0.33306 | eval_custom_logloss: 0.45507 |  0:00:24s
epoch 60 | loss: 0.30491 | eval_custom_logloss: 0.51982 |  0:00:24s
epoch 61 | loss: 0.31859 | eval_custom_logloss: 0.45513 |  0:00:25s
epoch 62 | loss: 0.30174 | eval_custom_logloss: 0.43749 |  0:00:25s
epoch 63 | loss: 0.29197 | eval_custom_logloss: 0.46649 |  0:00:26s
epoch 64 | loss: 0.27029 | eval_custom_logloss: 0.4698  |  0:00:26s
epoch 65 | loss: 0.28762 | eval_custom_logloss: 0.48224 |  0:00:26s
epoch 66 | loss: 0.29422 | eval_custom_logloss: 0.44921 |  0:00:27s
epoch 67 | loss: 0.31205 | eval_custom_logloss: 0.45535 |  0:00:27s
epoch 68 | loss: 0.28964 | eval_custom_logloss: 0.4602  |  0:00:27s
epoch 69 | loss: 0.29944 | eval_custom_logloss: 0.5076  |  0:00:28s
epoch 70 | loss: 0.30591 | eval_custom_logloss: 0.46698 |  0:00:28s
epoch 71 | loss: 0.28729 | eval_custom_logloss: 0.47455 |  0:00:29s
epoch 72 | loss: 0.27787 | eval_custom_logloss: 0.47543 |  0:00:29s
epoch 73 | loss: 0.28215 | eval_custom_logloss: 0.48742 |  0:00:29s
epoch 74 | loss: 0.28757 | eval_custom_logloss: 0.46096 |  0:00:30s
epoch 75 | loss: 0.26033 | eval_custom_logloss: 0.50191 |  0:00:30s
epoch 76 | loss: 0.28087 | eval_custom_logloss: 0.4938  |  0:00:31s
epoch 77 | loss: 0.28975 | eval_custom_logloss: 0.44894 |  0:00:31s
epoch 78 | loss: 0.27429 | eval_custom_logloss: 0.45051 |  0:00:31s
epoch 79 | loss: 0.26072 | eval_custom_logloss: 0.4088  |  0:00:32s
epoch 80 | loss: 0.2692  | eval_custom_logloss: 0.44862 |  0:00:32s
epoch 81 | loss: 0.25644 | eval_custom_logloss: 0.4842  |  0:00:33s
epoch 82 | loss: 0.29325 | eval_custom_logloss: 0.44214 |  0:00:33s
epoch 83 | loss: 0.27768 | eval_custom_logloss: 0.50089 |  0:00:34s
epoch 84 | loss: 0.27143 | eval_custom_logloss: 0.47737 |  0:00:34s
epoch 85 | loss: 0.2608  | eval_custom_logloss: 0.4964  |  0:00:34s
epoch 86 | loss: 0.26304 | eval_custom_logloss: 0.43395 |  0:00:35s
epoch 87 | loss: 0.27052 | eval_custom_logloss: 0.46339 |  0:00:35s
epoch 88 | loss: 0.28203 | eval_custom_logloss: 0.44457 |  0:00:36s
epoch 89 | loss: 0.24792 | eval_custom_logloss: 0.46273 |  0:00:36s
epoch 90 | loss: 0.26508 | eval_custom_logloss: 0.443   |  0:00:37s
epoch 91 | loss: 0.23166 | eval_custom_logloss: 0.42391 |  0:00:37s
epoch 92 | loss: 0.2265  | eval_custom_logloss: 0.4398  |  0:00:38s
epoch 93 | loss: 0.21694 | eval_custom_logloss: 0.4855  |  0:00:38s
epoch 94 | loss: 0.21039 | eval_custom_logloss: 0.48273 |  0:00:38s
epoch 95 | loss: 0.21768 | eval_custom_logloss: 0.45743 |  0:00:39s
epoch 96 | loss: 0.20814 | eval_custom_logloss: 0.44413 |  0:00:39s
epoch 97 | loss: 0.24246 | eval_custom_logloss: 0.54511 |  0:00:40s
epoch 98 | loss: 0.20175 | eval_custom_logloss: 0.48512 |  0:00:40s
epoch 99 | loss: 0.2268  | eval_custom_logloss: 0.52161 |  0:00:41s

Early stopping occurred at epoch 99 with best_epoch = 79 and best_eval_custom_logloss = 0.4088
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.404825, 'Log Loss - std': 0.019808883739373097} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax', 'n_a': 64, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.50406 | eval_custom_logloss: 1.11358 |  0:00:00s
epoch 1  | loss: 1.00741 | eval_custom_logloss: 0.648   |  0:00:00s
epoch 2  | loss: 0.86106 | eval_custom_logloss: 0.85889 |  0:00:01s
epoch 3  | loss: 0.75938 | eval_custom_logloss: 0.7013  |  0:00:01s
epoch 4  | loss: 0.77473 | eval_custom_logloss: 0.88838 |  0:00:01s
epoch 5  | loss: 0.73017 | eval_custom_logloss: 0.67151 |  0:00:02s
epoch 6  | loss: 0.67691 | eval_custom_logloss: 0.63324 |  0:00:02s
epoch 7  | loss: 0.64594 | eval_custom_logloss: 0.61539 |  0:00:03s
epoch 8  | loss: 0.60764 | eval_custom_logloss: 0.56363 |  0:00:03s
epoch 9  | loss: 0.59697 | eval_custom_logloss: 0.57578 |  0:00:04s
epoch 10 | loss: 0.58098 | eval_custom_logloss: 0.49677 |  0:00:04s
epoch 11 | loss: 0.57028 | eval_custom_logloss: 0.51956 |  0:00:04s
epoch 12 | loss: 0.54513 | eval_custom_logloss: 0.45182 |  0:00:05s
epoch 13 | loss: 0.46522 | eval_custom_logloss: 0.46306 |  0:00:05s
epoch 14 | loss: 0.48876 | eval_custom_logloss: 0.43498 |  0:00:06s
epoch 15 | loss: 0.49302 | eval_custom_logloss: 0.43521 |  0:00:06s
epoch 16 | loss: 0.504   | eval_custom_logloss: 0.45805 |  0:00:06s
epoch 17 | loss: 0.48226 | eval_custom_logloss: 0.4234  |  0:00:07s
epoch 18 | loss: 0.44914 | eval_custom_logloss: 0.35919 |  0:00:07s
epoch 19 | loss: 0.40956 | eval_custom_logloss: 0.38046 |  0:00:07s
epoch 20 | loss: 0.411   | eval_custom_logloss: 0.42354 |  0:00:08s
epoch 21 | loss: 0.40896 | eval_custom_logloss: 0.43857 |  0:00:08s
epoch 22 | loss: 0.42164 | eval_custom_logloss: 0.41055 |  0:00:09s
epoch 23 | loss: 0.42517 | eval_custom_logloss: 0.35983 |  0:00:09s
epoch 24 | loss: 0.41108 | eval_custom_logloss: 0.40816 |  0:00:09s
epoch 25 | loss: 0.38385 | eval_custom_logloss: 0.41131 |  0:00:10s
epoch 26 | loss: 0.39245 | eval_custom_logloss: 0.38584 |  0:00:10s
epoch 27 | loss: 0.38677 | eval_custom_logloss: 0.34531 |  0:00:11s
epoch 28 | loss: 0.36261 | eval_custom_logloss: 0.35476 |  0:00:11s
epoch 29 | loss: 0.41298 | eval_custom_logloss: 0.36567 |  0:00:11s
epoch 30 | loss: 0.41233 | eval_custom_logloss: 0.33528 |  0:00:12s
epoch 31 | loss: 0.38433 | eval_custom_logloss: 0.42829 |  0:00:12s
epoch 32 | loss: 0.38009 | eval_custom_logloss: 0.3421  |  0:00:12s
epoch 33 | loss: 0.38215 | eval_custom_logloss: 0.33371 |  0:00:13s
epoch 34 | loss: 0.36516 | eval_custom_logloss: 0.32396 |  0:00:13s
epoch 35 | loss: 0.36944 | eval_custom_logloss: 0.33488 |  0:00:14s
epoch 36 | loss: 0.38229 | eval_custom_logloss: 0.29759 |  0:00:14s
epoch 37 | loss: 0.36392 | eval_custom_logloss: 0.34985 |  0:00:14s
epoch 38 | loss: 0.36313 | eval_custom_logloss: 0.35465 |  0:00:15s
epoch 39 | loss: 0.37697 | eval_custom_logloss: 0.30118 |  0:00:15s
epoch 40 | loss: 0.35807 | eval_custom_logloss: 0.31468 |  0:00:16s
epoch 41 | loss: 0.30796 | eval_custom_logloss: 0.27078 |  0:00:16s
epoch 42 | loss: 0.32408 | eval_custom_logloss: 0.31365 |  0:00:16s
epoch 43 | loss: 0.31839 | eval_custom_logloss: 0.30804 |  0:00:17s
epoch 44 | loss: 0.296   | eval_custom_logloss: 0.2888  |  0:00:17s
epoch 45 | loss: 0.30359 | eval_custom_logloss: 0.32053 |  0:00:17s
epoch 46 | loss: 0.32059 | eval_custom_logloss: 0.31615 |  0:00:18s
epoch 47 | loss: 0.30056 | eval_custom_logloss: 0.26448 |  0:00:18s
epoch 48 | loss: 0.29204 | eval_custom_logloss: 0.25389 |  0:00:19s
epoch 49 | loss: 0.27157 | eval_custom_logloss: 0.24654 |  0:00:19s
epoch 50 | loss: 0.25364 | eval_custom_logloss: 0.27591 |  0:00:19s
epoch 51 | loss: 0.27304 | eval_custom_logloss: 0.24218 |  0:00:20s
epoch 52 | loss: 0.2739  | eval_custom_logloss: 0.25529 |  0:00:20s
epoch 53 | loss: 0.31616 | eval_custom_logloss: 0.35953 |  0:00:21s
epoch 54 | loss: 0.31135 | eval_custom_logloss: 0.31766 |  0:00:21s
epoch 55 | loss: 0.31617 | eval_custom_logloss: 0.32951 |  0:00:21s
epoch 56 | loss: 0.2834  | eval_custom_logloss: 0.31849 |  0:00:22s
epoch 57 | loss: 0.25484 | eval_custom_logloss: 0.30766 |  0:00:22s
epoch 58 | loss: 0.29189 | eval_custom_logloss: 0.30395 |  0:00:22s
epoch 59 | loss: 0.281   | eval_custom_logloss: 0.2734  |  0:00:23s
epoch 60 | loss: 0.27368 | eval_custom_logloss: 0.28037 |  0:00:23s
epoch 61 | loss: 0.25395 | eval_custom_logloss: 0.26146 |  0:00:24s
epoch 62 | loss: 0.27617 | eval_custom_logloss: 0.26891 |  0:00:24s
epoch 63 | loss: 0.2714  | eval_custom_logloss: 0.33454 |  0:00:24s
epoch 64 | loss: 0.25705 | eval_custom_logloss: 0.31556 |  0:00:25s
epoch 65 | loss: 0.2578  | eval_custom_logloss: 0.32879 |  0:00:25s
epoch 66 | loss: 0.28037 | eval_custom_logloss: 0.27661 |  0:00:26s
epoch 67 | loss: 0.22206 | eval_custom_logloss: 0.28486 |  0:00:26s
epoch 68 | loss: 0.23447 | eval_custom_logloss: 0.33997 |  0:00:26s
epoch 69 | loss: 0.2755  | eval_custom_logloss: 0.31237 |  0:00:27s
epoch 70 | loss: 0.28452 | eval_custom_logloss: 0.2858  |  0:00:27s
epoch 71 | loss: 0.29333 | eval_custom_logloss: 0.2733  |  0:00:28s

Early stopping occurred at epoch 71 with best_epoch = 51 and best_eval_custom_logloss = 0.24218
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.37229999999999996, 'Log Loss - std': 0.0674197003849765} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 41 finished with value: 0.37229999999999996 and parameters: {'n_d': 64, 'n_steps': 4, 'gamma': 1.3574114215529465, 'cat_emb_dim': 3, 'n_independent': 3, 'n_shared': 4, 'momentum': 0.22747395654666205, 'mask_type': 'sparsemax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.42177 | eval_custom_logloss: 1.30253 |  0:00:00s
epoch 1  | loss: 1.11285 | eval_custom_logloss: 1.18176 |  0:00:01s
epoch 2  | loss: 1.15825 | eval_custom_logloss: 0.94964 |  0:00:02s
epoch 3  | loss: 0.97417 | eval_custom_logloss: 0.86649 |  0:00:03s
epoch 4  | loss: 0.8967  | eval_custom_logloss: 0.98597 |  0:00:03s
epoch 5  | loss: 0.86182 | eval_custom_logloss: 1.08477 |  0:00:04s
epoch 6  | loss: 0.74984 | eval_custom_logloss: 1.14868 |  0:00:05s
epoch 7  | loss: 0.77215 | eval_custom_logloss: 0.65994 |  0:00:05s
epoch 8  | loss: 0.72518 | eval_custom_logloss: 1.03046 |  0:00:06s
epoch 9  | loss: 0.70784 | eval_custom_logloss: 0.96106 |  0:00:07s
epoch 10 | loss: 0.68217 | eval_custom_logloss: 1.07489 |  0:00:08s
epoch 11 | loss: 0.77316 | eval_custom_logloss: 1.98868 |  0:00:08s
epoch 12 | loss: 0.73883 | eval_custom_logloss: 0.95656 |  0:00:09s
epoch 13 | loss: 0.63705 | eval_custom_logloss: 0.68163 |  0:00:10s
epoch 14 | loss: 0.61636 | eval_custom_logloss: 0.57195 |  0:00:10s
epoch 15 | loss: 0.6023  | eval_custom_logloss: 0.59247 |  0:00:11s
epoch 16 | loss: 0.58423 | eval_custom_logloss: 0.57559 |  0:00:12s
epoch 17 | loss: 0.54608 | eval_custom_logloss: 0.59538 |  0:00:13s
epoch 18 | loss: 0.55849 | eval_custom_logloss: 0.77102 |  0:00:13s
epoch 19 | loss: 0.62771 | eval_custom_logloss: 0.83258 |  0:00:14s
epoch 20 | loss: 0.60858 | eval_custom_logloss: 0.64731 |  0:00:15s
epoch 21 | loss: 0.58447 | eval_custom_logloss: 0.57935 |  0:00:15s
epoch 22 | loss: 0.57966 | eval_custom_logloss: 0.56364 |  0:00:16s
epoch 23 | loss: 0.57555 | eval_custom_logloss: 0.54937 |  0:00:17s
epoch 24 | loss: 0.54757 | eval_custom_logloss: 0.53018 |  0:00:18s
epoch 25 | loss: 0.53668 | eval_custom_logloss: 0.50445 |  0:00:18s
epoch 26 | loss: 0.51639 | eval_custom_logloss: 0.52685 |  0:00:19s
epoch 27 | loss: 0.54265 | eval_custom_logloss: 0.58911 |  0:00:20s
epoch 28 | loss: 0.52597 | eval_custom_logloss: 0.58942 |  0:00:21s
epoch 29 | loss: 0.52603 | eval_custom_logloss: 0.57331 |  0:00:21s
epoch 30 | loss: 0.5249  | eval_custom_logloss: 0.57174 |  0:00:22s
epoch 31 | loss: 0.53773 | eval_custom_logloss: 0.51476 |  0:00:23s
epoch 32 | loss: 0.51989 | eval_custom_logloss: 0.54467 |  0:00:23s
epoch 33 | loss: 0.51708 | eval_custom_logloss: 0.54806 |  0:00:24s
epoch 34 | loss: 0.52976 | eval_custom_logloss: 0.53286 |  0:00:25s
epoch 35 | loss: 0.52801 | eval_custom_logloss: 0.58387 |  0:00:25s
epoch 36 | loss: 0.53168 | eval_custom_logloss: 0.54409 |  0:00:26s
epoch 37 | loss: 0.4954  | eval_custom_logloss: 0.59446 |  0:00:27s
epoch 38 | loss: 0.49516 | eval_custom_logloss: 0.54597 |  0:00:28s
epoch 39 | loss: 0.4828  | eval_custom_logloss: 0.59003 |  0:00:28s
epoch 40 | loss: 0.48805 | eval_custom_logloss: 0.53829 |  0:00:29s
epoch 41 | loss: 0.49582 | eval_custom_logloss: 0.532   |  0:00:30s
epoch 42 | loss: 0.50454 | eval_custom_logloss: 0.54162 |  0:00:30s
epoch 43 | loss: 0.53744 | eval_custom_logloss: 0.53656 |  0:00:31s
epoch 44 | loss: 0.4984  | eval_custom_logloss: 0.51745 |  0:00:32s
epoch 45 | loss: 0.48192 | eval_custom_logloss: 0.54274 |  0:00:33s

Early stopping occurred at epoch 45 with best_epoch = 25 and best_eval_custom_logloss = 0.50445
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5045, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.14332 | eval_custom_logloss: 1.6584  |  0:00:00s
epoch 1  | loss: 1.16502 | eval_custom_logloss: 1.06724 |  0:00:01s
epoch 2  | loss: 0.99545 | eval_custom_logloss: 0.91316 |  0:00:02s
epoch 3  | loss: 0.9035  | eval_custom_logloss: 0.96542 |  0:00:02s
epoch 4  | loss: 0.85414 | eval_custom_logloss: 1.06051 |  0:00:03s
epoch 5  | loss: 0.83099 | eval_custom_logloss: 1.23642 |  0:00:04s
epoch 6  | loss: 0.808   | eval_custom_logloss: 1.27207 |  0:00:05s
epoch 7  | loss: 0.74316 | eval_custom_logloss: 1.10946 |  0:00:05s
epoch 8  | loss: 0.72993 | eval_custom_logloss: 1.06051 |  0:00:06s
epoch 9  | loss: 0.7073  | eval_custom_logloss: 1.05065 |  0:00:07s
epoch 10 | loss: 0.66552 | eval_custom_logloss: 0.96999 |  0:00:08s
epoch 11 | loss: 0.66644 | eval_custom_logloss: 0.97166 |  0:00:08s
epoch 12 | loss: 0.68834 | eval_custom_logloss: 0.95894 |  0:00:09s
epoch 13 | loss: 0.63926 | eval_custom_logloss: 0.84077 |  0:00:10s
epoch 14 | loss: 0.59114 | eval_custom_logloss: 0.93739 |  0:00:11s
epoch 15 | loss: 0.59302 | eval_custom_logloss: 0.86851 |  0:00:11s
epoch 16 | loss: 0.59245 | eval_custom_logloss: 0.76773 |  0:00:12s
epoch 17 | loss: 0.56395 | eval_custom_logloss: 0.64311 |  0:00:13s
epoch 18 | loss: 0.55735 | eval_custom_logloss: 0.77455 |  0:00:13s
epoch 19 | loss: 0.54863 | eval_custom_logloss: 0.72232 |  0:00:14s
epoch 20 | loss: 0.54823 | eval_custom_logloss: 0.64153 |  0:00:15s
epoch 21 | loss: 0.52023 | eval_custom_logloss: 0.65694 |  0:00:16s
epoch 22 | loss: 0.50926 | eval_custom_logloss: 0.69835 |  0:00:16s
epoch 23 | loss: 0.4908  | eval_custom_logloss: 0.65901 |  0:00:17s
epoch 24 | loss: 0.50739 | eval_custom_logloss: 0.62956 |  0:00:18s
epoch 25 | loss: 0.49659 | eval_custom_logloss: 0.60848 |  0:00:18s
epoch 26 | loss: 0.48606 | eval_custom_logloss: 0.71894 |  0:00:19s
epoch 27 | loss: 0.49461 | eval_custom_logloss: 0.70374 |  0:00:20s
epoch 28 | loss: 0.46379 | eval_custom_logloss: 0.65258 |  0:00:21s
epoch 29 | loss: 0.46981 | eval_custom_logloss: 0.59763 |  0:00:21s
epoch 30 | loss: 0.47594 | eval_custom_logloss: 0.60149 |  0:00:22s
epoch 31 | loss: 0.5047  | eval_custom_logloss: 0.65354 |  0:00:23s
epoch 32 | loss: 0.50535 | eval_custom_logloss: 0.58333 |  0:00:23s
epoch 33 | loss: 0.48577 | eval_custom_logloss: 0.59156 |  0:00:24s
epoch 34 | loss: 0.47899 | eval_custom_logloss: 0.58067 |  0:00:25s
epoch 35 | loss: 0.49794 | eval_custom_logloss: 0.59379 |  0:00:25s
epoch 36 | loss: 0.4592  | eval_custom_logloss: 0.62335 |  0:00:26s
epoch 37 | loss: 0.43534 | eval_custom_logloss: 0.68657 |  0:00:27s
epoch 38 | loss: 0.45826 | eval_custom_logloss: 0.57574 |  0:00:28s
epoch 39 | loss: 0.49569 | eval_custom_logloss: 0.65564 |  0:00:28s
epoch 40 | loss: 0.47262 | eval_custom_logloss: 0.58693 |  0:00:29s
epoch 41 | loss: 0.44425 | eval_custom_logloss: 0.55398 |  0:00:30s
epoch 42 | loss: 0.42999 | eval_custom_logloss: 0.58133 |  0:00:30s
epoch 43 | loss: 0.43884 | eval_custom_logloss: 0.61724 |  0:00:31s
epoch 44 | loss: 0.41996 | eval_custom_logloss: 0.57981 |  0:00:32s
epoch 45 | loss: 0.40333 | eval_custom_logloss: 0.57716 |  0:00:33s
epoch 46 | loss: 0.38963 | eval_custom_logloss: 0.53404 |  0:00:33s
epoch 47 | loss: 0.39417 | eval_custom_logloss: 0.56214 |  0:00:34s
epoch 48 | loss: 0.37583 | eval_custom_logloss: 0.54833 |  0:00:35s
epoch 49 | loss: 0.37679 | eval_custom_logloss: 0.5703  |  0:00:35s
epoch 50 | loss: 0.40691 | eval_custom_logloss: 0.57474 |  0:00:36s
epoch 51 | loss: 0.37394 | eval_custom_logloss: 0.539   |  0:00:37s
epoch 52 | loss: 0.36646 | eval_custom_logloss: 0.58956 |  0:00:38s
epoch 53 | loss: 0.37063 | eval_custom_logloss: 0.58388 |  0:00:38s
epoch 54 | loss: 0.37787 | eval_custom_logloss: 0.63564 |  0:00:39s
epoch 55 | loss: 0.36457 | eval_custom_logloss: 0.68752 |  0:00:40s
epoch 56 | loss: 0.37355 | eval_custom_logloss: 0.60015 |  0:00:40s
epoch 57 | loss: 0.35642 | eval_custom_logloss: 0.56377 |  0:00:41s
epoch 58 | loss: 0.33167 | eval_custom_logloss: 0.5738  |  0:00:42s
epoch 59 | loss: 0.3362  | eval_custom_logloss: 0.66581 |  0:00:42s
epoch 60 | loss: 0.3783  | eval_custom_logloss: 0.60428 |  0:00:43s
epoch 61 | loss: 0.36019 | eval_custom_logloss: 0.60395 |  0:00:44s
epoch 62 | loss: 0.33307 | eval_custom_logloss: 0.67004 |  0:00:45s
epoch 63 | loss: 0.37962 | eval_custom_logloss: 0.66126 |  0:00:45s
epoch 64 | loss: 0.39876 | eval_custom_logloss: 0.75282 |  0:00:46s
epoch 65 | loss: 0.37363 | eval_custom_logloss: 0.63225 |  0:00:47s
epoch 66 | loss: 0.35358 | eval_custom_logloss: 0.5569  |  0:00:47s

Early stopping occurred at epoch 66 with best_epoch = 46 and best_eval_custom_logloss = 0.53404
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.51925, 'Log Loss - std': 0.01475000000000004} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.11752 | eval_custom_logloss: 1.17014 |  0:00:00s
epoch 1  | loss: 1.17086 | eval_custom_logloss: 0.83166 |  0:00:01s
epoch 2  | loss: 1.02592 | eval_custom_logloss: 0.89755 |  0:00:02s
epoch 3  | loss: 0.93324 | eval_custom_logloss: 0.85177 |  0:00:02s
epoch 4  | loss: 0.81978 | eval_custom_logloss: 0.82535 |  0:00:03s
epoch 5  | loss: 0.75016 | eval_custom_logloss: 1.01502 |  0:00:04s
epoch 6  | loss: 0.80832 | eval_custom_logloss: 0.74384 |  0:00:05s
epoch 7  | loss: 0.73716 | eval_custom_logloss: 0.75117 |  0:00:05s
epoch 8  | loss: 0.74285 | eval_custom_logloss: 0.95236 |  0:00:06s
epoch 9  | loss: 0.67447 | eval_custom_logloss: 1.27196 |  0:00:07s
epoch 10 | loss: 0.72488 | eval_custom_logloss: 0.7985  |  0:00:07s
epoch 11 | loss: 0.71597 | eval_custom_logloss: 0.81326 |  0:00:08s
epoch 12 | loss: 0.66003 | eval_custom_logloss: 0.7223  |  0:00:09s
epoch 13 | loss: 0.64989 | eval_custom_logloss: 0.71126 |  0:00:09s
epoch 14 | loss: 0.61791 | eval_custom_logloss: 0.90407 |  0:00:10s
epoch 15 | loss: 0.6111  | eval_custom_logloss: 0.95521 |  0:00:11s
epoch 16 | loss: 0.64078 | eval_custom_logloss: 0.62714 |  0:00:12s
epoch 17 | loss: 0.62742 | eval_custom_logloss: 0.64246 |  0:00:12s
epoch 18 | loss: 0.63308 | eval_custom_logloss: 0.62062 |  0:00:13s
epoch 19 | loss: 0.59084 | eval_custom_logloss: 0.62639 |  0:00:14s
epoch 20 | loss: 0.58982 | eval_custom_logloss: 0.65803 |  0:00:15s
epoch 21 | loss: 0.62143 | eval_custom_logloss: 0.61038 |  0:00:15s
epoch 22 | loss: 0.586   | eval_custom_logloss: 0.66327 |  0:00:16s
epoch 23 | loss: 0.60132 | eval_custom_logloss: 0.65792 |  0:00:17s
epoch 24 | loss: 0.618   | eval_custom_logloss: 0.69937 |  0:00:17s
epoch 25 | loss: 0.5827  | eval_custom_logloss: 0.68466 |  0:00:18s
epoch 26 | loss: 0.5913  | eval_custom_logloss: 0.61986 |  0:00:19s
epoch 27 | loss: 0.59807 | eval_custom_logloss: 0.60562 |  0:00:19s
epoch 28 | loss: 0.58427 | eval_custom_logloss: 0.66474 |  0:00:20s
epoch 29 | loss: 0.59767 | eval_custom_logloss: 0.61392 |  0:00:21s
epoch 30 | loss: 0.62034 | eval_custom_logloss: 0.69424 |  0:00:22s
epoch 31 | loss: 0.62243 | eval_custom_logloss: 0.64782 |  0:00:22s
epoch 32 | loss: 0.59985 | eval_custom_logloss: 0.57293 |  0:00:23s
epoch 33 | loss: 0.56995 | eval_custom_logloss: 0.59913 |  0:00:24s
epoch 34 | loss: 0.54235 | eval_custom_logloss: 0.53986 |  0:00:24s
epoch 35 | loss: 0.56236 | eval_custom_logloss: 0.53849 |  0:00:25s
epoch 36 | loss: 0.53383 | eval_custom_logloss: 0.51962 |  0:00:26s
epoch 37 | loss: 0.52782 | eval_custom_logloss: 0.53068 |  0:00:27s
epoch 38 | loss: 0.53104 | eval_custom_logloss: 0.50348 |  0:00:27s
epoch 39 | loss: 0.52624 | eval_custom_logloss: 0.51286 |  0:00:28s
epoch 40 | loss: 0.53215 | eval_custom_logloss: 0.52862 |  0:00:29s
epoch 41 | loss: 0.53487 | eval_custom_logloss: 0.5277  |  0:00:30s
epoch 42 | loss: 0.55205 | eval_custom_logloss: 0.50958 |  0:00:30s
epoch 43 | loss: 0.54479 | eval_custom_logloss: 0.59707 |  0:00:31s
epoch 44 | loss: 0.52525 | eval_custom_logloss: 0.53379 |  0:00:32s
epoch 45 | loss: 0.52506 | eval_custom_logloss: 0.4991  |  0:00:32s
epoch 46 | loss: 0.51578 | eval_custom_logloss: 0.52676 |  0:00:33s
epoch 47 | loss: 0.51722 | eval_custom_logloss: 0.53247 |  0:00:34s
epoch 48 | loss: 0.51446 | eval_custom_logloss: 0.54027 |  0:00:34s
epoch 49 | loss: 0.52122 | eval_custom_logloss: 0.54926 |  0:00:35s
epoch 50 | loss: 0.5173  | eval_custom_logloss: 0.49804 |  0:00:36s
epoch 51 | loss: 0.51865 | eval_custom_logloss: 0.49016 |  0:00:37s
epoch 52 | loss: 0.5068  | eval_custom_logloss: 0.49436 |  0:00:37s
epoch 53 | loss: 0.4981  | eval_custom_logloss: 0.51684 |  0:00:38s
epoch 54 | loss: 0.48381 | eval_custom_logloss: 0.48941 |  0:00:39s
epoch 55 | loss: 0.47766 | eval_custom_logloss: 0.51075 |  0:00:39s
epoch 56 | loss: 0.47614 | eval_custom_logloss: 0.58001 |  0:00:40s
epoch 57 | loss: 0.51085 | eval_custom_logloss: 0.54637 |  0:00:41s
epoch 58 | loss: 0.52266 | eval_custom_logloss: 0.50209 |  0:00:42s
epoch 59 | loss: 0.48706 | eval_custom_logloss: 0.55044 |  0:00:42s
epoch 60 | loss: 0.49125 | eval_custom_logloss: 0.54885 |  0:00:43s
epoch 61 | loss: 0.45895 | eval_custom_logloss: 0.50058 |  0:00:44s
epoch 62 | loss: 0.44392 | eval_custom_logloss: 0.48199 |  0:00:44s
epoch 63 | loss: 0.44127 | eval_custom_logloss: 0.54576 |  0:00:45s
epoch 64 | loss: 0.48561 | eval_custom_logloss: 0.59668 |  0:00:46s
epoch 65 | loss: 0.44054 | eval_custom_logloss: 0.55568 |  0:00:47s
epoch 66 | loss: 0.45925 | eval_custom_logloss: 0.55545 |  0:00:47s
epoch 67 | loss: 0.46693 | eval_custom_logloss: 0.5811  |  0:00:48s
epoch 68 | loss: 0.46013 | eval_custom_logloss: 0.56034 |  0:00:49s
epoch 69 | loss: 0.46329 | eval_custom_logloss: 0.63282 |  0:00:50s
epoch 70 | loss: 0.48681 | eval_custom_logloss: 0.58135 |  0:00:50s
epoch 71 | loss: 0.46912 | eval_custom_logloss: 0.60795 |  0:00:51s
epoch 72 | loss: 0.46842 | eval_custom_logloss: 0.5252  |  0:00:52s
epoch 73 | loss: 0.45907 | eval_custom_logloss: 0.5028  |  0:00:53s
epoch 74 | loss: 0.43841 | eval_custom_logloss: 0.50558 |  0:00:53s
epoch 75 | loss: 0.43672 | eval_custom_logloss: 0.51189 |  0:00:54s
epoch 76 | loss: 0.45681 | eval_custom_logloss: 0.49544 |  0:00:55s
epoch 77 | loss: 0.43962 | eval_custom_logloss: 0.53479 |  0:00:56s
epoch 78 | loss: 0.45786 | eval_custom_logloss: 0.54548 |  0:00:56s
epoch 79 | loss: 0.45873 | eval_custom_logloss: 0.52544 |  0:00:57s
epoch 80 | loss: 0.48276 | eval_custom_logloss: 0.47774 |  0:00:58s
epoch 81 | loss: 0.41536 | eval_custom_logloss: 0.49379 |  0:00:59s
epoch 82 | loss: 0.41846 | eval_custom_logloss: 0.49192 |  0:00:59s
epoch 83 | loss: 0.42117 | eval_custom_logloss: 0.49582 |  0:01:00s
epoch 84 | loss: 0.43695 | eval_custom_logloss: 0.5068  |  0:01:01s
epoch 85 | loss: 0.44015 | eval_custom_logloss: 0.56673 |  0:01:01s
epoch 86 | loss: 0.42697 | eval_custom_logloss: 0.49677 |  0:01:02s
epoch 87 | loss: 0.41266 | eval_custom_logloss: 0.52194 |  0:01:03s
epoch 88 | loss: 0.42368 | eval_custom_logloss: 0.55614 |  0:01:04s
epoch 89 | loss: 0.43685 | eval_custom_logloss: 0.50559 |  0:01:04s
epoch 90 | loss: 0.40298 | eval_custom_logloss: 0.49828 |  0:01:05s
epoch 91 | loss: 0.41827 | eval_custom_logloss: 0.51516 |  0:01:06s
epoch 92 | loss: 0.4342  | eval_custom_logloss: 0.49918 |  0:01:06s
epoch 93 | loss: 0.4427  | eval_custom_logloss: 0.44418 |  0:01:07s
epoch 94 | loss: 0.41127 | eval_custom_logloss: 0.48693 |  0:01:08s
epoch 95 | loss: 0.3926  | eval_custom_logloss: 0.50496 |  0:01:09s
epoch 96 | loss: 0.37631 | eval_custom_logloss: 0.51782 |  0:01:09s
epoch 97 | loss: 0.3887  | eval_custom_logloss: 0.51258 |  0:01:10s
epoch 98 | loss: 0.39165 | eval_custom_logloss: 0.60191 |  0:01:11s
epoch 99 | loss: 0.39292 | eval_custom_logloss: 0.50656 |  0:01:11s
Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_eval_custom_logloss = 0.44418
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4942333333333333, 'Log Loss - std': 0.03737256866859555} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.9537  | eval_custom_logloss: 1.2148  |  0:00:00s
epoch 1  | loss: 1.15452 | eval_custom_logloss: 0.90946 |  0:00:01s
epoch 2  | loss: 0.86235 | eval_custom_logloss: 0.79876 |  0:00:02s
epoch 3  | loss: 0.85017 | eval_custom_logloss: 0.76776 |  0:00:02s
epoch 4  | loss: 0.83627 | eval_custom_logloss: 0.81541 |  0:00:03s
epoch 5  | loss: 0.77196 | eval_custom_logloss: 0.86019 |  0:00:04s
epoch 6  | loss: 0.76296 | eval_custom_logloss: 1.01218 |  0:00:05s
epoch 7  | loss: 0.74571 | eval_custom_logloss: 0.86731 |  0:00:05s
epoch 8  | loss: 0.71451 | eval_custom_logloss: 0.77688 |  0:00:06s
epoch 9  | loss: 0.73083 | eval_custom_logloss: 0.78593 |  0:00:07s
epoch 10 | loss: 0.70537 | eval_custom_logloss: 1.02408 |  0:00:07s
epoch 11 | loss: 0.68298 | eval_custom_logloss: 1.03864 |  0:00:08s
epoch 12 | loss: 0.68845 | eval_custom_logloss: 0.92329 |  0:00:09s
epoch 13 | loss: 0.68783 | eval_custom_logloss: 0.84949 |  0:00:09s
epoch 14 | loss: 0.63726 | eval_custom_logloss: 0.72562 |  0:00:10s
epoch 15 | loss: 0.6379  | eval_custom_logloss: 0.72337 |  0:00:11s
epoch 16 | loss: 0.64871 | eval_custom_logloss: 0.84021 |  0:00:12s
epoch 17 | loss: 0.61537 | eval_custom_logloss: 1.01133 |  0:00:12s
epoch 18 | loss: 0.57836 | eval_custom_logloss: 0.95285 |  0:00:13s
epoch 19 | loss: 0.57414 | eval_custom_logloss: 0.73578 |  0:00:14s
epoch 20 | loss: 0.56095 | eval_custom_logloss: 0.69405 |  0:00:14s
epoch 21 | loss: 0.53112 | eval_custom_logloss: 0.69654 |  0:00:15s
epoch 22 | loss: 0.56854 | eval_custom_logloss: 0.61632 |  0:00:16s
epoch 23 | loss: 0.55206 | eval_custom_logloss: 0.56881 |  0:00:17s
epoch 24 | loss: 0.59606 | eval_custom_logloss: 0.65937 |  0:00:17s
epoch 25 | loss: 0.56996 | eval_custom_logloss: 0.69715 |  0:00:18s
epoch 26 | loss: 0.54177 | eval_custom_logloss: 0.61211 |  0:00:19s
epoch 27 | loss: 0.50085 | eval_custom_logloss: 0.56168 |  0:00:19s
epoch 28 | loss: 0.50645 | eval_custom_logloss: 0.5924  |  0:00:20s
epoch 29 | loss: 0.51159 | eval_custom_logloss: 0.62239 |  0:00:21s
epoch 30 | loss: 0.52843 | eval_custom_logloss: 0.58323 |  0:00:22s
epoch 31 | loss: 0.51449 | eval_custom_logloss: 0.56748 |  0:00:22s
epoch 32 | loss: 0.51239 | eval_custom_logloss: 0.57967 |  0:00:23s
epoch 33 | loss: 0.50329 | eval_custom_logloss: 0.58311 |  0:00:24s
epoch 34 | loss: 0.4927  | eval_custom_logloss: 0.55211 |  0:00:24s
epoch 35 | loss: 0.5072  | eval_custom_logloss: 0.55301 |  0:00:25s
epoch 36 | loss: 0.53941 | eval_custom_logloss: 0.56941 |  0:00:26s
epoch 37 | loss: 0.54598 | eval_custom_logloss: 0.59204 |  0:00:26s
epoch 38 | loss: 0.53428 | eval_custom_logloss: 0.61563 |  0:00:27s
epoch 39 | loss: 0.51058 | eval_custom_logloss: 0.59218 |  0:00:28s
epoch 40 | loss: 0.51558 | eval_custom_logloss: 0.5914  |  0:00:28s
epoch 41 | loss: 0.51003 | eval_custom_logloss: 0.61455 |  0:00:29s
epoch 42 | loss: 0.53471 | eval_custom_logloss: 0.63765 |  0:00:30s
epoch 43 | loss: 0.50918 | eval_custom_logloss: 0.54972 |  0:00:31s
epoch 44 | loss: 0.5075  | eval_custom_logloss: 0.58025 |  0:00:31s
epoch 45 | loss: 0.48975 | eval_custom_logloss: 0.53459 |  0:00:32s
epoch 46 | loss: 0.51163 | eval_custom_logloss: 0.55608 |  0:00:33s
epoch 47 | loss: 0.48539 | eval_custom_logloss: 0.56924 |  0:00:33s
epoch 48 | loss: 0.47916 | eval_custom_logloss: 0.58829 |  0:00:34s
epoch 49 | loss: 0.49985 | eval_custom_logloss: 0.64468 |  0:00:35s
epoch 50 | loss: 0.49227 | eval_custom_logloss: 0.60887 |  0:00:36s
epoch 51 | loss: 0.46568 | eval_custom_logloss: 0.60946 |  0:00:36s
epoch 52 | loss: 0.46985 | eval_custom_logloss: 0.57271 |  0:00:37s
epoch 53 | loss: 0.45666 | eval_custom_logloss: 0.53927 |  0:00:38s
epoch 54 | loss: 0.45049 | eval_custom_logloss: 0.53139 |  0:00:38s
epoch 55 | loss: 0.43296 | eval_custom_logloss: 0.55941 |  0:00:39s
epoch 56 | loss: 0.44566 | eval_custom_logloss: 0.54385 |  0:00:40s
epoch 57 | loss: 0.43318 | eval_custom_logloss: 0.53975 |  0:00:40s
epoch 58 | loss: 0.4281  | eval_custom_logloss: 0.52819 |  0:00:41s
epoch 59 | loss: 0.41566 | eval_custom_logloss: 0.69542 |  0:00:42s
epoch 60 | loss: 0.42437 | eval_custom_logloss: 0.53887 |  0:00:43s
epoch 61 | loss: 0.41881 | eval_custom_logloss: 0.50801 |  0:00:43s
epoch 62 | loss: 0.4068  | eval_custom_logloss: 0.49591 |  0:00:44s
epoch 63 | loss: 0.4065  | eval_custom_logloss: 0.52433 |  0:00:45s
epoch 64 | loss: 0.41298 | eval_custom_logloss: 0.54183 |  0:00:45s
epoch 65 | loss: 0.40012 | eval_custom_logloss: 0.49694 |  0:00:46s
epoch 66 | loss: 0.39901 | eval_custom_logloss: 0.49858 |  0:00:47s
epoch 67 | loss: 0.38141 | eval_custom_logloss: 0.46781 |  0:00:48s
epoch 68 | loss: 0.37446 | eval_custom_logloss: 0.47701 |  0:00:48s
epoch 69 | loss: 0.37059 | eval_custom_logloss: 0.44834 |  0:00:49s
epoch 70 | loss: 0.35946 | eval_custom_logloss: 0.4452  |  0:00:50s
epoch 71 | loss: 0.3824  | eval_custom_logloss: 0.4862  |  0:00:50s
epoch 72 | loss: 0.39532 | eval_custom_logloss: 0.55337 |  0:00:51s
epoch 73 | loss: 0.36936 | eval_custom_logloss: 0.43509 |  0:00:52s
epoch 74 | loss: 0.35899 | eval_custom_logloss: 0.49659 |  0:00:53s
epoch 75 | loss: 0.35074 | eval_custom_logloss: 0.45298 |  0:00:53s
epoch 76 | loss: 0.35291 | eval_custom_logloss: 0.43922 |  0:00:54s
epoch 77 | loss: 0.33045 | eval_custom_logloss: 0.44653 |  0:00:55s
epoch 78 | loss: 0.33979 | eval_custom_logloss: 0.43538 |  0:00:55s
epoch 79 | loss: 0.3359  | eval_custom_logloss: 0.48526 |  0:00:56s
epoch 80 | loss: 0.35739 | eval_custom_logloss: 0.51893 |  0:00:57s
epoch 81 | loss: 0.34236 | eval_custom_logloss: 0.59114 |  0:00:57s
epoch 82 | loss: 0.30819 | eval_custom_logloss: 0.44793 |  0:00:58s
epoch 83 | loss: 0.31524 | eval_custom_logloss: 0.57524 |  0:00:59s
epoch 84 | loss: 0.34465 | eval_custom_logloss: 0.46363 |  0:01:00s
epoch 85 | loss: 0.29819 | eval_custom_logloss: 0.52988 |  0:01:00s
epoch 86 | loss: 0.32787 | eval_custom_logloss: 0.52874 |  0:01:01s
epoch 87 | loss: 0.31564 | eval_custom_logloss: 0.48301 |  0:01:02s
epoch 88 | loss: 0.33915 | eval_custom_logloss: 0.48368 |  0:01:02s
epoch 89 | loss: 0.33014 | eval_custom_logloss: 0.53438 |  0:01:03s
epoch 90 | loss: 0.31175 | eval_custom_logloss: 0.48298 |  0:01:04s
epoch 91 | loss: 0.29682 | eval_custom_logloss: 0.47711 |  0:01:04s
epoch 92 | loss: 0.29809 | eval_custom_logloss: 0.50155 |  0:01:05s
epoch 93 | loss: 0.28608 | eval_custom_logloss: 0.47283 |  0:01:06s

Early stopping occurred at epoch 93 with best_epoch = 73 and best_eval_custom_logloss = 0.43509
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.47945, 'Log Loss - std': 0.041269510537441566} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax', 'n_a': 17, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 3.02624 | eval_custom_logloss: 1.19019 |  0:00:00s
epoch 1  | loss: 1.13735 | eval_custom_logloss: 1.07866 |  0:00:01s
epoch 2  | loss: 1.00258 | eval_custom_logloss: 0.89424 |  0:00:02s
epoch 3  | loss: 0.96917 | eval_custom_logloss: 0.81079 |  0:00:03s
epoch 4  | loss: 0.91959 | eval_custom_logloss: 0.88924 |  0:00:04s
epoch 5  | loss: 0.84565 | eval_custom_logloss: 0.9413  |  0:00:04s
epoch 6  | loss: 0.83998 | eval_custom_logloss: 0.67929 |  0:00:05s
epoch 7  | loss: 0.80901 | eval_custom_logloss: 1.04229 |  0:00:06s
epoch 8  | loss: 0.77496 | eval_custom_logloss: 0.67786 |  0:00:07s
epoch 9  | loss: 0.73478 | eval_custom_logloss: 0.77992 |  0:00:07s
epoch 10 | loss: 0.77767 | eval_custom_logloss: 1.24572 |  0:00:08s
epoch 11 | loss: 0.82701 | eval_custom_logloss: 1.1773  |  0:00:09s
epoch 12 | loss: 0.76754 | eval_custom_logloss: 0.88577 |  0:00:10s
epoch 13 | loss: 0.74154 | eval_custom_logloss: 0.73912 |  0:00:10s
epoch 14 | loss: 0.70292 | eval_custom_logloss: 0.74027 |  0:00:11s
epoch 15 | loss: 0.70903 | eval_custom_logloss: 0.71652 |  0:00:12s
epoch 16 | loss: 0.66198 | eval_custom_logloss: 0.60771 |  0:00:13s
epoch 17 | loss: 0.65317 | eval_custom_logloss: 0.69447 |  0:00:13s
epoch 18 | loss: 0.65659 | eval_custom_logloss: 0.66366 |  0:00:14s
epoch 19 | loss: 0.63455 | eval_custom_logloss: 0.67623 |  0:00:15s
epoch 20 | loss: 0.63293 | eval_custom_logloss: 0.6685  |  0:00:16s
epoch 21 | loss: 0.58772 | eval_custom_logloss: 0.66687 |  0:00:16s
epoch 22 | loss: 0.60562 | eval_custom_logloss: 0.6173  |  0:00:17s
epoch 23 | loss: 0.59102 | eval_custom_logloss: 0.58982 |  0:00:18s
epoch 24 | loss: 0.62749 | eval_custom_logloss: 0.598   |  0:00:19s
epoch 25 | loss: 0.59646 | eval_custom_logloss: 0.52859 |  0:00:19s
epoch 26 | loss: 0.55452 | eval_custom_logloss: 0.48357 |  0:00:20s
epoch 27 | loss: 0.52769 | eval_custom_logloss: 0.46551 |  0:00:21s
epoch 28 | loss: 0.50553 | eval_custom_logloss: 0.47023 |  0:00:22s
epoch 29 | loss: 0.52134 | eval_custom_logloss: 0.43544 |  0:00:22s
epoch 30 | loss: 0.51959 | eval_custom_logloss: 0.46566 |  0:00:23s
epoch 31 | loss: 0.51662 | eval_custom_logloss: 0.4502  |  0:00:24s
epoch 32 | loss: 0.52509 | eval_custom_logloss: 0.47061 |  0:00:25s
epoch 33 | loss: 0.51059 | eval_custom_logloss: 0.45242 |  0:00:25s
epoch 34 | loss: 0.50102 | eval_custom_logloss: 0.45139 |  0:00:26s
epoch 35 | loss: 0.5353  | eval_custom_logloss: 0.47294 |  0:00:27s
epoch 36 | loss: 0.50337 | eval_custom_logloss: 0.44438 |  0:00:27s
epoch 37 | loss: 0.53374 | eval_custom_logloss: 0.45911 |  0:00:28s
epoch 38 | loss: 0.48853 | eval_custom_logloss: 0.46074 |  0:00:29s
epoch 39 | loss: 0.52616 | eval_custom_logloss: 0.45037 |  0:00:30s
epoch 40 | loss: 0.51144 | eval_custom_logloss: 0.47599 |  0:00:31s
epoch 41 | loss: 0.51346 | eval_custom_logloss: 0.44142 |  0:00:31s
epoch 42 | loss: 0.50768 | eval_custom_logloss: 0.44742 |  0:00:32s
epoch 43 | loss: 0.49884 | eval_custom_logloss: 0.44729 |  0:00:33s
epoch 44 | loss: 0.50611 | eval_custom_logloss: 0.44147 |  0:00:34s
epoch 45 | loss: 0.53043 | eval_custom_logloss: 0.46431 |  0:00:34s
epoch 46 | loss: 0.55106 | eval_custom_logloss: 0.48978 |  0:00:35s
epoch 47 | loss: 0.52519 | eval_custom_logloss: 0.44834 |  0:00:36s
epoch 48 | loss: 0.50275 | eval_custom_logloss: 0.42394 |  0:00:37s
epoch 49 | loss: 0.49135 | eval_custom_logloss: 0.45666 |  0:00:37s
epoch 50 | loss: 0.50444 | eval_custom_logloss: 0.45002 |  0:00:38s
epoch 51 | loss: 0.498   | eval_custom_logloss: 0.4374  |  0:00:39s
epoch 52 | loss: 0.50244 | eval_custom_logloss: 0.43766 |  0:00:39s
epoch 53 | loss: 0.48761 | eval_custom_logloss: 0.43151 |  0:00:40s
epoch 54 | loss: 0.48396 | eval_custom_logloss: 0.44991 |  0:00:41s
epoch 55 | loss: 0.47511 | eval_custom_logloss: 0.41679 |  0:00:42s
epoch 56 | loss: 0.51062 | eval_custom_logloss: 0.43775 |  0:00:42s
epoch 57 | loss: 0.49737 | eval_custom_logloss: 0.40024 |  0:00:43s
epoch 58 | loss: 0.50188 | eval_custom_logloss: 0.43211 |  0:00:44s
epoch 59 | loss: 0.50912 | eval_custom_logloss: 0.45048 |  0:00:45s
epoch 60 | loss: 0.50673 | eval_custom_logloss: 0.43079 |  0:00:45s
epoch 61 | loss: 0.48763 | eval_custom_logloss: 0.47971 |  0:00:46s
epoch 62 | loss: 0.47584 | eval_custom_logloss: 0.44001 |  0:00:47s
epoch 63 | loss: 0.48479 | eval_custom_logloss: 0.43661 |  0:00:48s
epoch 64 | loss: 0.45923 | eval_custom_logloss: 0.41904 |  0:00:48s
epoch 65 | loss: 0.45822 | eval_custom_logloss: 0.39232 |  0:00:49s
epoch 66 | loss: 0.46192 | eval_custom_logloss: 0.40496 |  0:00:50s
epoch 67 | loss: 0.4533  | eval_custom_logloss: 0.44212 |  0:00:50s
epoch 68 | loss: 0.45008 | eval_custom_logloss: 0.40226 |  0:00:51s
epoch 69 | loss: 0.45065 | eval_custom_logloss: 0.38633 |  0:00:52s
epoch 70 | loss: 0.44308 | eval_custom_logloss: 0.37228 |  0:00:53s
epoch 71 | loss: 0.4453  | eval_custom_logloss: 0.38025 |  0:00:54s
epoch 72 | loss: 0.44615 | eval_custom_logloss: 0.39468 |  0:00:54s
epoch 73 | loss: 0.44824 | eval_custom_logloss: 0.38379 |  0:00:55s
epoch 74 | loss: 0.43748 | eval_custom_logloss: 0.38539 |  0:00:56s
epoch 75 | loss: 0.44777 | eval_custom_logloss: 0.40628 |  0:00:57s
epoch 76 | loss: 0.42914 | eval_custom_logloss: 0.4151  |  0:00:57s
epoch 77 | loss: 0.43581 | eval_custom_logloss: 0.3975  |  0:00:58s
epoch 78 | loss: 0.44602 | eval_custom_logloss: 0.3743  |  0:00:59s
epoch 79 | loss: 0.41929 | eval_custom_logloss: 0.42409 |  0:00:59s
epoch 80 | loss: 0.45408 | eval_custom_logloss: 0.3913  |  0:01:00s
epoch 81 | loss: 0.42705 | eval_custom_logloss: 0.38688 |  0:01:01s
epoch 82 | loss: 0.40926 | eval_custom_logloss: 0.41703 |  0:01:02s
epoch 83 | loss: 0.41261 | eval_custom_logloss: 0.38473 |  0:01:02s
epoch 84 | loss: 0.41118 | eval_custom_logloss: 0.38477 |  0:01:03s
epoch 85 | loss: 0.3995  | eval_custom_logloss: 0.398   |  0:01:04s
epoch 86 | loss: 0.44195 | eval_custom_logloss: 0.37976 |  0:01:04s
epoch 87 | loss: 0.42428 | eval_custom_logloss: 0.38101 |  0:01:05s
epoch 88 | loss: 0.42535 | eval_custom_logloss: 0.39177 |  0:01:06s
epoch 89 | loss: 0.41543 | eval_custom_logloss: 0.39487 |  0:01:07s
epoch 90 | loss: 0.41154 | eval_custom_logloss: 0.35435 |  0:01:07s
epoch 91 | loss: 0.41934 | eval_custom_logloss: 0.39459 |  0:01:08s
epoch 92 | loss: 0.38032 | eval_custom_logloss: 0.35566 |  0:01:09s
epoch 93 | loss: 0.38773 | eval_custom_logloss: 0.35627 |  0:01:10s
epoch 94 | loss: 0.38399 | eval_custom_logloss: 0.39089 |  0:01:10s
epoch 95 | loss: 0.36149 | eval_custom_logloss: 0.35652 |  0:01:11s
epoch 96 | loss: 0.38628 | eval_custom_logloss: 0.33194 |  0:01:12s
epoch 97 | loss: 0.37729 | eval_custom_logloss: 0.37436 |  0:01:13s
epoch 98 | loss: 0.39944 | eval_custom_logloss: 0.34326 |  0:01:13s
epoch 99 | loss: 0.39366 | eval_custom_logloss: 0.3542  |  0:01:14s
Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_eval_custom_logloss = 0.33194
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.44993999999999995, 'Log Loss - std': 0.06961248738552589} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 42 finished with value: 0.44993999999999995 and parameters: {'n_d': 17, 'n_steps': 7, 'gamma': 1.077967194176817, 'cat_emb_dim': 1, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.04687172742750824, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.12142 | eval_custom_logloss: 8.23377 |  0:00:00s
epoch 1  | loss: 1.80453 | eval_custom_logloss: 8.28068 |  0:00:01s
epoch 2  | loss: 2.10394 | eval_custom_logloss: 8.12177 |  0:00:02s
epoch 3  | loss: 1.50059 | eval_custom_logloss: 4.92593 |  0:00:03s
epoch 4  | loss: 1.30932 | eval_custom_logloss: 5.81077 |  0:00:04s
epoch 5  | loss: 1.00674 | eval_custom_logloss: 6.36804 |  0:00:05s
epoch 6  | loss: 1.10166 | eval_custom_logloss: 7.24214 |  0:00:05s
epoch 7  | loss: 1.07572 | eval_custom_logloss: 5.0373  |  0:00:06s
epoch 8  | loss: 0.83587 | eval_custom_logloss: 5.81543 |  0:00:07s
epoch 9  | loss: 0.98393 | eval_custom_logloss: 3.40056 |  0:00:08s
epoch 10 | loss: 1.85716 | eval_custom_logloss: 7.19    |  0:00:09s
epoch 11 | loss: 1.05739 | eval_custom_logloss: 3.20016 |  0:00:10s
epoch 12 | loss: 0.85387 | eval_custom_logloss: 4.73952 |  0:00:10s
epoch 13 | loss: 0.79877 | eval_custom_logloss: 5.34879 |  0:00:11s
epoch 14 | loss: 0.79686 | eval_custom_logloss: 2.54088 |  0:00:12s
epoch 15 | loss: 0.76381 | eval_custom_logloss: 3.00463 |  0:00:13s
epoch 16 | loss: 0.74672 | eval_custom_logloss: 4.06874 |  0:00:14s
epoch 17 | loss: 0.69411 | eval_custom_logloss: 4.00706 |  0:00:15s
epoch 18 | loss: 0.70458 | eval_custom_logloss: 4.56086 |  0:00:16s
epoch 19 | loss: 0.76892 | eval_custom_logloss: 3.80381 |  0:00:17s
epoch 20 | loss: 0.79165 | eval_custom_logloss: 4.77279 |  0:00:17s
epoch 21 | loss: 0.75777 | eval_custom_logloss: 2.5505  |  0:00:18s
epoch 22 | loss: 0.66266 | eval_custom_logloss: 2.40523 |  0:00:19s
epoch 23 | loss: 0.64516 | eval_custom_logloss: 2.69545 |  0:00:20s
epoch 24 | loss: 0.73812 | eval_custom_logloss: 1.58733 |  0:00:21s
epoch 25 | loss: 0.68603 | eval_custom_logloss: 1.4263  |  0:00:22s
epoch 26 | loss: 0.63588 | eval_custom_logloss: 1.75703 |  0:00:23s
epoch 27 | loss: 0.65166 | eval_custom_logloss: 1.0991  |  0:00:24s
epoch 28 | loss: 0.62501 | eval_custom_logloss: 1.6854  |  0:00:24s
epoch 29 | loss: 0.64138 | eval_custom_logloss: 1.68187 |  0:00:25s
epoch 30 | loss: 0.64078 | eval_custom_logloss: 2.44994 |  0:00:26s
epoch 31 | loss: 0.68573 | eval_custom_logloss: 2.14736 |  0:00:27s
epoch 32 | loss: 0.662   | eval_custom_logloss: 1.79775 |  0:00:28s
epoch 33 | loss: 0.6348  | eval_custom_logloss: 1.88571 |  0:00:28s
epoch 34 | loss: 0.66019 | eval_custom_logloss: 1.85561 |  0:00:29s
epoch 35 | loss: 0.68305 | eval_custom_logloss: 2.05578 |  0:00:30s
epoch 36 | loss: 0.67021 | eval_custom_logloss: 1.88177 |  0:00:31s
epoch 37 | loss: 0.64241 | eval_custom_logloss: 1.59724 |  0:00:32s
epoch 38 | loss: 0.65211 | eval_custom_logloss: 1.16251 |  0:00:33s
epoch 39 | loss: 0.64145 | eval_custom_logloss: 1.34213 |  0:00:33s
epoch 40 | loss: 0.63477 | eval_custom_logloss: 1.4018  |  0:00:34s
epoch 41 | loss: 0.64507 | eval_custom_logloss: 1.1412  |  0:00:35s
epoch 42 | loss: 0.67133 | eval_custom_logloss: 1.33797 |  0:00:36s
epoch 43 | loss: 0.70897 | eval_custom_logloss: 0.97894 |  0:00:37s
epoch 44 | loss: 0.63969 | eval_custom_logloss: 1.04901 |  0:00:38s
epoch 45 | loss: 0.61489 | eval_custom_logloss: 0.99273 |  0:00:39s
epoch 46 | loss: 0.62279 | eval_custom_logloss: 0.87113 |  0:00:39s
epoch 47 | loss: 0.5933  | eval_custom_logloss: 1.24574 |  0:00:40s
epoch 48 | loss: 0.62979 | eval_custom_logloss: 1.27776 |  0:00:41s
epoch 49 | loss: 0.60824 | eval_custom_logloss: 1.29159 |  0:00:42s
epoch 50 | loss: 0.60516 | eval_custom_logloss: 1.17141 |  0:00:43s
epoch 51 | loss: 0.60537 | eval_custom_logloss: 1.17089 |  0:00:44s
epoch 52 | loss: 0.62148 | eval_custom_logloss: 1.05939 |  0:00:45s
epoch 53 | loss: 0.62882 | eval_custom_logloss: 0.86555 |  0:00:45s
epoch 54 | loss: 0.57863 | eval_custom_logloss: 0.96477 |  0:00:46s
epoch 55 | loss: 0.56169 | eval_custom_logloss: 1.26723 |  0:00:47s
epoch 56 | loss: 0.59465 | eval_custom_logloss: 1.70645 |  0:00:48s
epoch 57 | loss: 0.62644 | eval_custom_logloss: 0.85796 |  0:00:49s
epoch 58 | loss: 0.597   | eval_custom_logloss: 0.97788 |  0:00:50s
epoch 59 | loss: 0.61191 | eval_custom_logloss: 1.0215  |  0:00:51s
epoch 60 | loss: 0.60102 | eval_custom_logloss: 0.89829 |  0:00:51s
epoch 61 | loss: 0.55573 | eval_custom_logloss: 0.79315 |  0:00:52s
epoch 62 | loss: 0.54944 | eval_custom_logloss: 1.10352 |  0:00:53s
epoch 63 | loss: 0.56873 | eval_custom_logloss: 0.83088 |  0:00:54s
epoch 64 | loss: 0.53458 | eval_custom_logloss: 0.78622 |  0:00:55s
epoch 65 | loss: 0.53281 | eval_custom_logloss: 0.74889 |  0:00:56s
epoch 66 | loss: 0.53541 | eval_custom_logloss: 0.69618 |  0:00:57s
epoch 67 | loss: 0.53313 | eval_custom_logloss: 0.68467 |  0:00:58s
epoch 68 | loss: 0.55626 | eval_custom_logloss: 0.747   |  0:00:59s
epoch 69 | loss: 0.59675 | eval_custom_logloss: 0.79572 |  0:00:59s
epoch 70 | loss: 0.56888 | eval_custom_logloss: 0.89755 |  0:01:00s
epoch 71 | loss: 0.53216 | eval_custom_logloss: 0.79559 |  0:01:01s
epoch 72 | loss: 0.53735 | eval_custom_logloss: 0.84888 |  0:01:02s
epoch 73 | loss: 0.52382 | eval_custom_logloss: 0.82245 |  0:01:03s
epoch 74 | loss: 0.50022 | eval_custom_logloss: 0.76206 |  0:01:04s
epoch 75 | loss: 0.52231 | eval_custom_logloss: 0.82706 |  0:01:04s
epoch 76 | loss: 0.51292 | eval_custom_logloss: 0.80865 |  0:01:05s
epoch 77 | loss: 0.48875 | eval_custom_logloss: 0.71256 |  0:01:06s
epoch 78 | loss: 0.5406  | eval_custom_logloss: 0.72524 |  0:01:07s
epoch 79 | loss: 0.50105 | eval_custom_logloss: 0.68987 |  0:01:08s
epoch 80 | loss: 0.50513 | eval_custom_logloss: 0.748   |  0:01:08s
epoch 81 | loss: 0.50753 | eval_custom_logloss: 0.75951 |  0:01:09s
epoch 82 | loss: 0.48419 | eval_custom_logloss: 0.80395 |  0:01:10s
epoch 83 | loss: 0.47173 | eval_custom_logloss: 0.69141 |  0:01:11s
epoch 84 | loss: 0.49063 | eval_custom_logloss: 0.69577 |  0:01:12s
epoch 85 | loss: 0.48334 | eval_custom_logloss: 0.79257 |  0:01:13s
epoch 86 | loss: 0.47793 | eval_custom_logloss: 0.84831 |  0:01:14s
epoch 87 | loss: 0.50605 | eval_custom_logloss: 0.8724  |  0:01:14s

Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_custom_logloss = 0.68467
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.6847, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.29314 | eval_custom_logloss: 7.97953 |  0:00:00s
epoch 1  | loss: 1.387   | eval_custom_logloss: 8.64304 |  0:00:01s
epoch 2  | loss: 1.11299 | eval_custom_logloss: 8.19984 |  0:00:02s
epoch 3  | loss: 1.46807 | eval_custom_logloss: 8.24057 |  0:00:03s
epoch 4  | loss: 1.39817 | eval_custom_logloss: 7.42598 |  0:00:04s
epoch 5  | loss: 1.60644 | eval_custom_logloss: 6.84348 |  0:00:05s
epoch 6  | loss: 1.45367 | eval_custom_logloss: 6.40097 |  0:00:06s
epoch 7  | loss: 1.01553 | eval_custom_logloss: 6.37802 |  0:00:07s
epoch 8  | loss: 0.89238 | eval_custom_logloss: 5.41425 |  0:00:08s
epoch 9  | loss: 1.18011 | eval_custom_logloss: 8.12946 |  0:00:08s
epoch 10 | loss: 1.03209 | eval_custom_logloss: 4.73802 |  0:00:09s
epoch 11 | loss: 0.77162 | eval_custom_logloss: 3.95376 |  0:00:10s
epoch 12 | loss: 0.79864 | eval_custom_logloss: 3.59248 |  0:00:11s
epoch 13 | loss: 0.75423 | eval_custom_logloss: 3.75268 |  0:00:12s
epoch 14 | loss: 0.69999 | eval_custom_logloss: 4.57648 |  0:00:13s
epoch 15 | loss: 0.66642 | eval_custom_logloss: 4.19299 |  0:00:14s
epoch 16 | loss: 0.69595 | eval_custom_logloss: 4.00882 |  0:00:15s
epoch 17 | loss: 0.72746 | eval_custom_logloss: 3.7014  |  0:00:15s
epoch 18 | loss: 0.69883 | eval_custom_logloss: 3.53275 |  0:00:16s
epoch 19 | loss: 0.69128 | eval_custom_logloss: 1.95584 |  0:00:17s
epoch 20 | loss: 0.64654 | eval_custom_logloss: 2.50908 |  0:00:18s
epoch 21 | loss: 0.65927 | eval_custom_logloss: 3.49591 |  0:00:19s
epoch 22 | loss: 0.67175 | eval_custom_logloss: 2.01429 |  0:00:20s
epoch 23 | loss: 0.69622 | eval_custom_logloss: 1.58384 |  0:00:21s
epoch 24 | loss: 0.7039  | eval_custom_logloss: 3.03947 |  0:00:21s
epoch 25 | loss: 0.65211 | eval_custom_logloss: 3.30295 |  0:00:22s
epoch 26 | loss: 0.66059 | eval_custom_logloss: 2.32787 |  0:00:23s
epoch 27 | loss: 0.66431 | eval_custom_logloss: 2.95273 |  0:00:24s
epoch 28 | loss: 0.70379 | eval_custom_logloss: 2.96723 |  0:00:25s
epoch 29 | loss: 0.65371 | eval_custom_logloss: 3.26842 |  0:00:26s
epoch 30 | loss: 0.60992 | eval_custom_logloss: 2.38182 |  0:00:27s
epoch 31 | loss: 0.60967 | eval_custom_logloss: 2.01568 |  0:00:27s
epoch 32 | loss: 0.60682 | eval_custom_logloss: 2.76279 |  0:00:28s
epoch 33 | loss: 0.61089 | eval_custom_logloss: 1.93015 |  0:00:29s
epoch 34 | loss: 0.60021 | eval_custom_logloss: 1.69119 |  0:00:30s
epoch 35 | loss: 0.59162 | eval_custom_logloss: 1.8171  |  0:00:31s
epoch 36 | loss: 0.56758 | eval_custom_logloss: 2.03395 |  0:00:32s
epoch 37 | loss: 0.60654 | eval_custom_logloss: 2.87186 |  0:00:32s
epoch 38 | loss: 0.65028 | eval_custom_logloss: 2.32281 |  0:00:33s
epoch 39 | loss: 0.58943 | eval_custom_logloss: 1.81912 |  0:00:34s
epoch 40 | loss: 0.59414 | eval_custom_logloss: 2.04368 |  0:00:35s
epoch 41 | loss: 0.60199 | eval_custom_logloss: 2.90283 |  0:00:36s
epoch 42 | loss: 0.61839 | eval_custom_logloss: 2.80336 |  0:00:37s
epoch 43 | loss: 0.60091 | eval_custom_logloss: 2.08722 |  0:00:38s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 1.58384
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1193, 'Log Loss - std': 0.43460000000000004} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.00666 | eval_custom_logloss: 8.81531 |  0:00:01s
epoch 1  | loss: 1.53808 | eval_custom_logloss: 8.91105 |  0:00:02s
epoch 2  | loss: 1.4956  | eval_custom_logloss: 7.60368 |  0:00:03s
epoch 3  | loss: 1.59523 | eval_custom_logloss: 8.23098 |  0:00:03s
epoch 4  | loss: 1.19787 | eval_custom_logloss: 6.20623 |  0:00:04s
epoch 5  | loss: 0.95551 | eval_custom_logloss: 6.10508 |  0:00:05s
epoch 6  | loss: 0.91171 | eval_custom_logloss: 4.79563 |  0:00:06s
epoch 7  | loss: 1.30926 | eval_custom_logloss: 4.19848 |  0:00:07s
epoch 8  | loss: 1.16768 | eval_custom_logloss: 6.21917 |  0:00:08s
epoch 9  | loss: 0.8866  | eval_custom_logloss: 4.58324 |  0:00:08s
epoch 10 | loss: 0.79561 | eval_custom_logloss: 3.71731 |  0:00:09s
epoch 11 | loss: 0.68396 | eval_custom_logloss: 5.47755 |  0:00:10s
epoch 12 | loss: 0.7437  | eval_custom_logloss: 3.96407 |  0:00:11s
epoch 13 | loss: 0.753   | eval_custom_logloss: 3.21754 |  0:00:12s
epoch 14 | loss: 0.72769 | eval_custom_logloss: 7.14139 |  0:00:13s
epoch 15 | loss: 0.76977 | eval_custom_logloss: 3.10969 |  0:00:14s
epoch 16 | loss: 0.74051 | eval_custom_logloss: 3.88279 |  0:00:14s
epoch 17 | loss: 0.71871 | eval_custom_logloss: 3.53463 |  0:00:15s
epoch 18 | loss: 0.70923 | eval_custom_logloss: 2.23263 |  0:00:16s
epoch 19 | loss: 0.81773 | eval_custom_logloss: 4.24954 |  0:00:17s
epoch 20 | loss: 0.72464 | eval_custom_logloss: 2.24128 |  0:00:18s
epoch 21 | loss: 0.71469 | eval_custom_logloss: 2.05664 |  0:00:19s
epoch 22 | loss: 0.7343  | eval_custom_logloss: 2.20474 |  0:00:20s
epoch 23 | loss: 0.71453 | eval_custom_logloss: 1.75577 |  0:00:20s
epoch 24 | loss: 0.64305 | eval_custom_logloss: 1.57762 |  0:00:21s
epoch 25 | loss: 0.75403 | eval_custom_logloss: 1.97095 |  0:00:22s
epoch 26 | loss: 0.71154 | eval_custom_logloss: 2.03842 |  0:00:23s
epoch 27 | loss: 0.69915 | eval_custom_logloss: 1.9263  |  0:00:24s
epoch 28 | loss: 0.66617 | eval_custom_logloss: 2.11114 |  0:00:25s
epoch 29 | loss: 0.64316 | eval_custom_logloss: 1.89341 |  0:00:26s
epoch 30 | loss: 0.64132 | eval_custom_logloss: 1.80145 |  0:00:26s
epoch 31 | loss: 0.6254  | eval_custom_logloss: 1.91912 |  0:00:27s
epoch 32 | loss: 0.65625 | eval_custom_logloss: 1.40775 |  0:00:28s
epoch 33 | loss: 0.67772 | eval_custom_logloss: 1.55194 |  0:00:29s
epoch 34 | loss: 0.66768 | eval_custom_logloss: 1.51672 |  0:00:30s
epoch 35 | loss: 0.6106  | eval_custom_logloss: 2.25114 |  0:00:31s
epoch 36 | loss: 0.61094 | eval_custom_logloss: 1.40519 |  0:00:31s
epoch 37 | loss: 0.58186 | eval_custom_logloss: 2.08932 |  0:00:32s
epoch 38 | loss: 0.61888 | eval_custom_logloss: 1.89039 |  0:00:33s
epoch 39 | loss: 0.57105 | eval_custom_logloss: 1.86548 |  0:00:34s
epoch 40 | loss: 0.57775 | eval_custom_logloss: 2.15063 |  0:00:35s
epoch 41 | loss: 0.55971 | eval_custom_logloss: 1.76592 |  0:00:36s
epoch 42 | loss: 0.54866 | eval_custom_logloss: 1.55033 |  0:00:36s
epoch 43 | loss: 0.55449 | eval_custom_logloss: 1.73169 |  0:00:37s
epoch 44 | loss: 0.52994 | eval_custom_logloss: 1.19552 |  0:00:38s
epoch 45 | loss: 0.54022 | eval_custom_logloss: 1.01726 |  0:00:39s
epoch 46 | loss: 0.52399 | eval_custom_logloss: 1.01973 |  0:00:40s
epoch 47 | loss: 0.53773 | eval_custom_logloss: 1.06516 |  0:00:41s
epoch 48 | loss: 0.52116 | eval_custom_logloss: 1.16654 |  0:00:42s
epoch 49 | loss: 0.50583 | eval_custom_logloss: 1.45334 |  0:00:43s
epoch 50 | loss: 0.52978 | eval_custom_logloss: 1.2986  |  0:00:43s
epoch 51 | loss: 0.49747 | eval_custom_logloss: 0.92249 |  0:00:44s
epoch 52 | loss: 0.50374 | eval_custom_logloss: 0.89064 |  0:00:45s
epoch 53 | loss: 0.502   | eval_custom_logloss: 0.83253 |  0:00:46s
epoch 54 | loss: 0.47847 | eval_custom_logloss: 0.74589 |  0:00:47s
epoch 55 | loss: 0.46733 | eval_custom_logloss: 0.63379 |  0:00:48s
epoch 56 | loss: 0.45553 | eval_custom_logloss: 0.67901 |  0:00:49s
epoch 57 | loss: 0.46602 | eval_custom_logloss: 0.60946 |  0:00:50s
epoch 58 | loss: 0.43938 | eval_custom_logloss: 0.56952 |  0:00:50s
epoch 59 | loss: 0.44796 | eval_custom_logloss: 0.72517 |  0:00:51s
epoch 60 | loss: 0.42715 | eval_custom_logloss: 0.53575 |  0:00:52s
epoch 61 | loss: 0.42614 | eval_custom_logloss: 0.5596  |  0:00:53s
epoch 62 | loss: 0.40131 | eval_custom_logloss: 0.55641 |  0:00:54s
epoch 63 | loss: 0.4173  | eval_custom_logloss: 0.63363 |  0:00:55s
epoch 64 | loss: 0.42815 | eval_custom_logloss: 0.70165 |  0:00:56s
epoch 65 | loss: 0.43683 | eval_custom_logloss: 0.54477 |  0:00:56s
epoch 66 | loss: 0.40888 | eval_custom_logloss: 0.64447 |  0:00:57s
epoch 67 | loss: 0.39497 | eval_custom_logloss: 0.77624 |  0:00:58s
epoch 68 | loss: 0.44609 | eval_custom_logloss: 0.96548 |  0:00:59s
epoch 69 | loss: 0.42704 | eval_custom_logloss: 0.82372 |  0:01:00s
epoch 70 | loss: 0.42361 | eval_custom_logloss: 0.84981 |  0:01:01s
epoch 71 | loss: 0.45513 | eval_custom_logloss: 0.6382  |  0:01:02s
epoch 72 | loss: 0.45402 | eval_custom_logloss: 0.70378 |  0:01:03s
epoch 73 | loss: 0.40682 | eval_custom_logloss: 0.70407 |  0:01:03s
epoch 74 | loss: 0.41731 | eval_custom_logloss: 0.69578 |  0:01:04s
epoch 75 | loss: 0.38627 | eval_custom_logloss: 0.71068 |  0:01:05s
epoch 76 | loss: 0.3924  | eval_custom_logloss: 0.72097 |  0:01:06s
epoch 77 | loss: 0.3828  | eval_custom_logloss: 0.76207 |  0:01:07s
epoch 78 | loss: 0.42983 | eval_custom_logloss: 0.6545  |  0:01:08s
epoch 79 | loss: 0.39975 | eval_custom_logloss: 0.67678 |  0:01:08s
epoch 80 | loss: 0.37863 | eval_custom_logloss: 0.65609 |  0:01:09s

Early stopping occurred at epoch 80 with best_epoch = 60 and best_eval_custom_logloss = 0.53575
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9247666666666666, 'Log Loss - std': 0.4490039445508494} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.26007 | eval_custom_logloss: 7.95956 |  0:00:01s
epoch 1  | loss: 1.42706 | eval_custom_logloss: 7.01797 |  0:00:02s
epoch 2  | loss: 1.15991 | eval_custom_logloss: 7.40082 |  0:00:03s
epoch 3  | loss: 1.3966  | eval_custom_logloss: 6.73101 |  0:00:04s
epoch 4  | loss: 1.33438 | eval_custom_logloss: 8.1529  |  0:00:04s
epoch 5  | loss: 1.08744 | eval_custom_logloss: 5.37669 |  0:00:05s
epoch 6  | loss: 1.65433 | eval_custom_logloss: 8.05582 |  0:00:06s
epoch 7  | loss: 2.43494 | eval_custom_logloss: 7.92997 |  0:00:07s
epoch 8  | loss: 1.93061 | eval_custom_logloss: 8.09015 |  0:00:08s
epoch 9  | loss: 1.01441 | eval_custom_logloss: 4.94394 |  0:00:09s
epoch 10 | loss: 0.8386  | eval_custom_logloss: 4.14625 |  0:00:10s
epoch 11 | loss: 1.04005 | eval_custom_logloss: 5.03695 |  0:00:11s
epoch 12 | loss: 0.84452 | eval_custom_logloss: 6.02749 |  0:00:11s
epoch 13 | loss: 0.7857  | eval_custom_logloss: 4.94998 |  0:00:12s
epoch 14 | loss: 0.80201 | eval_custom_logloss: 4.35013 |  0:00:13s
epoch 15 | loss: 0.72549 | eval_custom_logloss: 5.70877 |  0:00:14s
epoch 16 | loss: 0.70903 | eval_custom_logloss: 6.413   |  0:00:15s
epoch 17 | loss: 0.81179 | eval_custom_logloss: 6.08664 |  0:00:16s
epoch 18 | loss: 0.74439 | eval_custom_logloss: 5.16332 |  0:00:17s
epoch 19 | loss: 0.68686 | eval_custom_logloss: 4.74221 |  0:00:17s
epoch 20 | loss: 0.68751 | eval_custom_logloss: 5.44339 |  0:00:18s
epoch 21 | loss: 0.70411 | eval_custom_logloss: 4.5081  |  0:00:19s
epoch 22 | loss: 0.67704 | eval_custom_logloss: 2.90962 |  0:00:20s
epoch 23 | loss: 0.70473 | eval_custom_logloss: 3.09151 |  0:00:21s
epoch 24 | loss: 0.73093 | eval_custom_logloss: 4.35185 |  0:00:22s
epoch 25 | loss: 0.65763 | eval_custom_logloss: 3.60827 |  0:00:23s
epoch 26 | loss: 0.67438 | eval_custom_logloss: 3.34876 |  0:00:24s
epoch 27 | loss: 0.71262 | eval_custom_logloss: 1.97398 |  0:00:24s
epoch 28 | loss: 0.69759 | eval_custom_logloss: 1.861   |  0:00:25s
epoch 29 | loss: 0.64781 | eval_custom_logloss: 2.45652 |  0:00:26s
epoch 30 | loss: 0.61968 | eval_custom_logloss: 3.49629 |  0:00:27s
epoch 31 | loss: 0.64548 | eval_custom_logloss: 3.21047 |  0:00:28s
epoch 32 | loss: 0.6149  | eval_custom_logloss: 2.7914  |  0:00:29s
epoch 33 | loss: 0.64543 | eval_custom_logloss: 1.74146 |  0:00:30s
epoch 34 | loss: 0.6736  | eval_custom_logloss: 2.56241 |  0:00:30s
epoch 35 | loss: 0.60157 | eval_custom_logloss: 1.34819 |  0:00:31s
epoch 36 | loss: 0.64322 | eval_custom_logloss: 1.80632 |  0:00:32s
epoch 37 | loss: 0.71602 | eval_custom_logloss: 2.35574 |  0:00:33s
epoch 38 | loss: 0.66469 | eval_custom_logloss: 1.20543 |  0:00:34s
epoch 39 | loss: 0.59214 | eval_custom_logloss: 1.51843 |  0:00:35s
epoch 40 | loss: 0.59655 | eval_custom_logloss: 1.36983 |  0:00:36s
epoch 41 | loss: 0.56166 | eval_custom_logloss: 1.29518 |  0:00:36s
epoch 42 | loss: 0.53504 | eval_custom_logloss: 1.29949 |  0:00:37s
epoch 43 | loss: 0.5448  | eval_custom_logloss: 1.06453 |  0:00:38s
epoch 44 | loss: 0.55038 | eval_custom_logloss: 1.27878 |  0:00:39s
epoch 45 | loss: 0.58731 | eval_custom_logloss: 1.11323 |  0:00:40s
epoch 46 | loss: 0.59606 | eval_custom_logloss: 1.19447 |  0:00:41s
epoch 47 | loss: 0.59765 | eval_custom_logloss: 0.90452 |  0:00:42s
epoch 48 | loss: 0.61526 | eval_custom_logloss: 0.93915 |  0:00:42s
epoch 49 | loss: 0.58572 | eval_custom_logloss: 1.0486  |  0:00:43s
epoch 50 | loss: 0.58469 | eval_custom_logloss: 0.98896 |  0:00:44s
epoch 51 | loss: 0.57106 | eval_custom_logloss: 0.93854 |  0:00:45s
epoch 52 | loss: 0.57372 | eval_custom_logloss: 0.9178  |  0:00:46s
epoch 53 | loss: 0.56251 | eval_custom_logloss: 1.53369 |  0:00:47s
epoch 54 | loss: 0.56877 | eval_custom_logloss: 1.30813 |  0:00:48s
epoch 55 | loss: 0.54913 | eval_custom_logloss: 1.00454 |  0:00:48s
epoch 56 | loss: 0.56843 | eval_custom_logloss: 1.02006 |  0:00:49s
epoch 57 | loss: 0.60538 | eval_custom_logloss: 1.28361 |  0:00:50s
epoch 58 | loss: 0.59013 | eval_custom_logloss: 1.02509 |  0:00:51s
epoch 59 | loss: 0.58388 | eval_custom_logloss: 1.00367 |  0:00:52s
epoch 60 | loss: 0.57617 | eval_custom_logloss: 0.82349 |  0:00:53s
epoch 61 | loss: 0.5603  | eval_custom_logloss: 1.06355 |  0:00:53s
epoch 62 | loss: 0.5482  | eval_custom_logloss: 1.11959 |  0:00:54s
epoch 63 | loss: 0.56559 | eval_custom_logloss: 1.19201 |  0:00:55s
epoch 64 | loss: 0.55583 | eval_custom_logloss: 1.00797 |  0:00:56s
epoch 65 | loss: 0.57078 | eval_custom_logloss: 1.01222 |  0:00:57s
epoch 66 | loss: 0.54298 | eval_custom_logloss: 0.90535 |  0:00:58s
epoch 67 | loss: 0.55305 | eval_custom_logloss: 0.83799 |  0:00:59s
epoch 68 | loss: 0.51412 | eval_custom_logloss: 0.84462 |  0:00:59s
epoch 69 | loss: 0.51734 | eval_custom_logloss: 0.91723 |  0:01:00s
epoch 70 | loss: 0.53747 | eval_custom_logloss: 0.93965 |  0:01:01s
epoch 71 | loss: 0.52029 | eval_custom_logloss: 0.81413 |  0:01:02s
epoch 72 | loss: 0.53367 | eval_custom_logloss: 0.74609 |  0:01:03s
epoch 73 | loss: 0.54682 | eval_custom_logloss: 0.81851 |  0:01:04s
epoch 74 | loss: 0.54082 | eval_custom_logloss: 0.84685 |  0:01:05s
epoch 75 | loss: 0.52543 | eval_custom_logloss: 1.29133 |  0:01:06s
epoch 76 | loss: 0.49567 | eval_custom_logloss: 1.08922 |  0:01:06s
epoch 77 | loss: 0.56427 | eval_custom_logloss: 1.07289 |  0:01:07s
epoch 78 | loss: 0.49986 | eval_custom_logloss: 0.8452  |  0:01:08s
epoch 79 | loss: 0.47048 | eval_custom_logloss: 0.94933 |  0:01:09s
epoch 80 | loss: 0.49781 | eval_custom_logloss: 0.85607 |  0:01:10s
epoch 81 | loss: 0.50586 | eval_custom_logloss: 0.78898 |  0:01:11s
epoch 82 | loss: 0.47092 | eval_custom_logloss: 0.77426 |  0:01:11s
epoch 83 | loss: 0.47558 | eval_custom_logloss: 1.00038 |  0:01:12s
epoch 84 | loss: 0.48348 | eval_custom_logloss: 1.11366 |  0:01:13s
epoch 85 | loss: 0.49547 | eval_custom_logloss: 0.9023  |  0:01:14s
epoch 86 | loss: 0.47629 | eval_custom_logloss: 0.93187 |  0:01:15s
epoch 87 | loss: 0.46458 | eval_custom_logloss: 0.79292 |  0:01:16s
epoch 88 | loss: 0.48569 | eval_custom_logloss: 0.87575 |  0:01:17s
epoch 89 | loss: 0.47164 | eval_custom_logloss: 0.803   |  0:01:17s
epoch 90 | loss: 0.47219 | eval_custom_logloss: 0.71248 |  0:01:18s
epoch 91 | loss: 0.46291 | eval_custom_logloss: 0.78059 |  0:01:19s
epoch 92 | loss: 0.46778 | eval_custom_logloss: 0.76427 |  0:01:20s
epoch 93 | loss: 0.45755 | eval_custom_logloss: 0.93638 |  0:01:21s
epoch 94 | loss: 0.49893 | eval_custom_logloss: 0.72537 |  0:01:22s
epoch 95 | loss: 0.46506 | eval_custom_logloss: 0.70835 |  0:01:23s
epoch 96 | loss: 0.43051 | eval_custom_logloss: 0.81856 |  0:01:24s
epoch 97 | loss: 0.43166 | eval_custom_logloss: 0.71458 |  0:01:24s
epoch 98 | loss: 0.41951 | eval_custom_logloss: 0.74459 |  0:01:25s
epoch 99 | loss: 0.41384 | eval_custom_logloss: 0.87625 |  0:01:26s
Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_eval_custom_logloss = 0.70835
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8706499999999999, 'Log Loss - std': 0.39998655914917947} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax', 'n_a': 47, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.18324 | eval_custom_logloss: 8.21499 |  0:00:00s
epoch 1  | loss: 1.557   | eval_custom_logloss: 8.68876 |  0:00:01s
epoch 2  | loss: 1.38992 | eval_custom_logloss: 7.34964 |  0:00:02s
epoch 3  | loss: 1.16247 | eval_custom_logloss: 5.78057 |  0:00:03s
epoch 4  | loss: 1.11394 | eval_custom_logloss: 5.98834 |  0:00:04s
epoch 5  | loss: 1.10363 | eval_custom_logloss: 5.8961  |  0:00:05s
epoch 6  | loss: 0.97924 | eval_custom_logloss: 6.85505 |  0:00:06s
epoch 7  | loss: 0.92205 | eval_custom_logloss: 4.49388 |  0:00:06s
epoch 8  | loss: 0.95154 | eval_custom_logloss: 5.76846 |  0:00:07s
epoch 9  | loss: 0.99165 | eval_custom_logloss: 6.47216 |  0:00:08s
epoch 10 | loss: 1.05869 | eval_custom_logloss: 3.98112 |  0:00:09s
epoch 11 | loss: 0.83818 | eval_custom_logloss: 4.94932 |  0:00:10s
epoch 12 | loss: 0.74634 | eval_custom_logloss: 4.46978 |  0:00:11s
epoch 13 | loss: 0.78033 | eval_custom_logloss: 4.4226  |  0:00:12s
epoch 14 | loss: 0.72022 | eval_custom_logloss: 2.90465 |  0:00:12s
epoch 15 | loss: 0.70884 | eval_custom_logloss: 2.93675 |  0:00:13s
epoch 16 | loss: 0.75625 | eval_custom_logloss: 2.76582 |  0:00:14s
epoch 17 | loss: 0.89807 | eval_custom_logloss: 3.0572  |  0:00:15s
epoch 18 | loss: 0.68893 | eval_custom_logloss: 2.46444 |  0:00:16s
epoch 19 | loss: 0.71739 | eval_custom_logloss: 4.0986  |  0:00:17s
epoch 20 | loss: 0.70814 | eval_custom_logloss: 2.86532 |  0:00:18s
epoch 21 | loss: 0.64568 | eval_custom_logloss: 2.44703 |  0:00:19s
epoch 22 | loss: 0.64196 | eval_custom_logloss: 1.93024 |  0:00:19s
epoch 23 | loss: 0.68216 | eval_custom_logloss: 2.27029 |  0:00:20s
epoch 24 | loss: 0.70651 | eval_custom_logloss: 1.62704 |  0:00:21s
epoch 25 | loss: 0.66032 | eval_custom_logloss: 1.75566 |  0:00:22s
epoch 26 | loss: 0.70707 | eval_custom_logloss: 3.01476 |  0:00:23s
epoch 27 | loss: 0.69172 | eval_custom_logloss: 1.41733 |  0:00:24s
epoch 28 | loss: 0.67239 | eval_custom_logloss: 2.37452 |  0:00:25s
epoch 29 | loss: 0.64492 | eval_custom_logloss: 1.95562 |  0:00:26s
epoch 30 | loss: 0.68702 | eval_custom_logloss: 1.99664 |  0:00:26s
epoch 31 | loss: 0.71199 | eval_custom_logloss: 2.73014 |  0:00:27s
epoch 32 | loss: 0.67225 | eval_custom_logloss: 1.82747 |  0:00:28s
epoch 33 | loss: 0.65197 | eval_custom_logloss: 1.52297 |  0:00:29s
epoch 34 | loss: 0.62588 | eval_custom_logloss: 1.82162 |  0:00:30s
epoch 35 | loss: 0.58129 | eval_custom_logloss: 1.85515 |  0:00:31s
epoch 36 | loss: 0.60903 | eval_custom_logloss: 1.73782 |  0:00:32s
epoch 37 | loss: 0.59415 | eval_custom_logloss: 1.59121 |  0:00:33s
epoch 38 | loss: 0.59528 | eval_custom_logloss: 1.93903 |  0:00:33s
epoch 39 | loss: 0.56488 | eval_custom_logloss: 2.21198 |  0:00:34s
epoch 40 | loss: 0.5763  | eval_custom_logloss: 1.54904 |  0:00:35s
epoch 41 | loss: 0.54975 | eval_custom_logloss: 1.70281 |  0:00:36s
epoch 42 | loss: 0.54485 | eval_custom_logloss: 1.36845 |  0:00:37s
epoch 43 | loss: 0.55556 | eval_custom_logloss: 1.17602 |  0:00:38s
epoch 44 | loss: 0.54774 | eval_custom_logloss: 1.40081 |  0:00:38s
epoch 45 | loss: 0.57509 | eval_custom_logloss: 1.94677 |  0:00:39s
epoch 46 | loss: 0.57308 | eval_custom_logloss: 1.22801 |  0:00:40s
epoch 47 | loss: 0.57935 | eval_custom_logloss: 1.05102 |  0:00:41s
epoch 48 | loss: 0.56097 | eval_custom_logloss: 1.36639 |  0:00:42s
epoch 49 | loss: 0.56334 | eval_custom_logloss: 1.24928 |  0:00:43s
epoch 50 | loss: 0.55311 | eval_custom_logloss: 1.15603 |  0:00:44s
epoch 51 | loss: 0.55927 | eval_custom_logloss: 1.13314 |  0:00:44s
epoch 52 | loss: 0.55099 | eval_custom_logloss: 1.24869 |  0:00:45s
epoch 53 | loss: 0.56183 | eval_custom_logloss: 1.06017 |  0:00:46s
epoch 54 | loss: 0.55858 | eval_custom_logloss: 0.94776 |  0:00:47s
epoch 55 | loss: 0.52519 | eval_custom_logloss: 0.92696 |  0:00:48s
epoch 56 | loss: 0.51541 | eval_custom_logloss: 0.85393 |  0:00:49s
epoch 57 | loss: 0.5452  | eval_custom_logloss: 1.08337 |  0:00:50s
epoch 58 | loss: 0.5348  | eval_custom_logloss: 0.90619 |  0:00:50s
epoch 59 | loss: 0.54617 | eval_custom_logloss: 0.98596 |  0:00:51s
epoch 60 | loss: 0.51183 | eval_custom_logloss: 1.08713 |  0:00:52s
epoch 61 | loss: 0.50041 | eval_custom_logloss: 0.901   |  0:00:53s
epoch 62 | loss: 0.50313 | eval_custom_logloss: 0.86424 |  0:00:54s
epoch 63 | loss: 0.52775 | eval_custom_logloss: 0.87692 |  0:00:55s
epoch 64 | loss: 0.52216 | eval_custom_logloss: 0.75343 |  0:00:55s
epoch 65 | loss: 0.51396 | eval_custom_logloss: 0.75104 |  0:00:56s
epoch 66 | loss: 0.52205 | eval_custom_logloss: 0.6743  |  0:00:57s
epoch 67 | loss: 0.5371  | eval_custom_logloss: 0.89575 |  0:00:58s
epoch 68 | loss: 0.51345 | eval_custom_logloss: 0.96125 |  0:00:59s
epoch 69 | loss: 0.54053 | eval_custom_logloss: 0.93871 |  0:01:00s
epoch 70 | loss: 0.52529 | eval_custom_logloss: 0.77293 |  0:01:00s
epoch 71 | loss: 0.50791 | eval_custom_logloss: 0.88064 |  0:01:01s
epoch 72 | loss: 0.50954 | eval_custom_logloss: 0.88617 |  0:01:02s
epoch 73 | loss: 0.50997 | eval_custom_logloss: 0.76201 |  0:01:03s
epoch 74 | loss: 0.52823 | eval_custom_logloss: 0.86826 |  0:01:04s
epoch 75 | loss: 0.5498  | eval_custom_logloss: 0.77689 |  0:01:04s
epoch 76 | loss: 0.5234  | eval_custom_logloss: 0.68235 |  0:01:05s
epoch 77 | loss: 0.5107  | eval_custom_logloss: 0.68184 |  0:01:06s
epoch 78 | loss: 0.52929 | eval_custom_logloss: 0.67055 |  0:01:07s
epoch 79 | loss: 0.49654 | eval_custom_logloss: 0.56357 |  0:01:08s
epoch 80 | loss: 0.49138 | eval_custom_logloss: 0.55458 |  0:01:09s
epoch 81 | loss: 0.49233 | eval_custom_logloss: 0.53216 |  0:01:10s
epoch 82 | loss: 0.49761 | eval_custom_logloss: 0.54426 |  0:01:10s
epoch 83 | loss: 0.48285 | eval_custom_logloss: 0.51254 |  0:01:11s
epoch 84 | loss: 0.48669 | eval_custom_logloss: 0.7616  |  0:01:12s
epoch 85 | loss: 0.5184  | eval_custom_logloss: 0.6206  |  0:01:13s
epoch 86 | loss: 0.50943 | eval_custom_logloss: 0.63475 |  0:01:14s
epoch 87 | loss: 0.51528 | eval_custom_logloss: 0.61548 |  0:01:15s
epoch 88 | loss: 0.54128 | eval_custom_logloss: 0.94343 |  0:01:15s
epoch 89 | loss: 0.53621 | eval_custom_logloss: 0.88141 |  0:01:16s
epoch 90 | loss: 0.52948 | eval_custom_logloss: 0.95402 |  0:01:17s
epoch 91 | loss: 0.49467 | eval_custom_logloss: 0.73944 |  0:01:18s
epoch 92 | loss: 0.48793 | eval_custom_logloss: 0.68156 |  0:01:19s
epoch 93 | loss: 0.51983 | eval_custom_logloss: 0.95248 |  0:01:19s
epoch 94 | loss: 0.47714 | eval_custom_logloss: 1.12915 |  0:01:20s
epoch 95 | loss: 0.51413 | eval_custom_logloss: 1.13488 |  0:01:21s
epoch 96 | loss: 0.53572 | eval_custom_logloss: 0.75827 |  0:01:22s
epoch 97 | loss: 0.51899 | eval_custom_logloss: 0.85298 |  0:01:23s
epoch 98 | loss: 0.52054 | eval_custom_logloss: 1.01839 |  0:01:24s
epoch 99 | loss: 0.50876 | eval_custom_logloss: 0.88898 |  0:01:24s
Stop training because you reached max_epochs = 100 with best_epoch = 83 and best_eval_custom_logloss = 0.51254
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.79902, 'Log Loss - std': 0.3853762130697742} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 43 finished with value: 0.79902 and parameters: {'n_d': 47, 'n_steps': 10, 'gamma': 1.1593451384607762, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 4, 'momentum': 0.004019725742603097, 'mask_type': 'sparsemax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09797 | eval_custom_logloss: 3.03494 |  0:00:00s
epoch 1  | loss: 0.94288 | eval_custom_logloss: 2.22087 |  0:00:01s
epoch 2  | loss: 0.84483 | eval_custom_logloss: 4.06802 |  0:00:01s
epoch 3  | loss: 0.7947  | eval_custom_logloss: 5.22279 |  0:00:02s
epoch 4  | loss: 0.73723 | eval_custom_logloss: 4.11611 |  0:00:02s
epoch 5  | loss: 0.71834 | eval_custom_logloss: 4.615   |  0:00:03s
epoch 6  | loss: 0.69366 | eval_custom_logloss: 4.72445 |  0:00:03s
epoch 7  | loss: 0.66727 | eval_custom_logloss: 4.92814 |  0:00:04s
epoch 8  | loss: 0.64319 | eval_custom_logloss: 5.91027 |  0:00:04s
epoch 9  | loss: 0.59875 | eval_custom_logloss: 6.92854 |  0:00:04s
epoch 10 | loss: 0.56802 | eval_custom_logloss: 6.15723 |  0:00:05s
epoch 11 | loss: 0.54656 | eval_custom_logloss: 4.33666 |  0:00:05s
epoch 12 | loss: 0.5623  | eval_custom_logloss: 3.98105 |  0:00:05s
epoch 13 | loss: 0.53831 | eval_custom_logloss: 3.01731 |  0:00:06s
epoch 14 | loss: 0.56578 | eval_custom_logloss: 3.07776 |  0:00:06s
epoch 15 | loss: 0.54    | eval_custom_logloss: 3.99624 |  0:00:07s
epoch 16 | loss: 0.51137 | eval_custom_logloss: 2.69568 |  0:00:07s
epoch 17 | loss: 0.51842 | eval_custom_logloss: 2.21901 |  0:00:08s
epoch 18 | loss: 0.50321 | eval_custom_logloss: 2.42977 |  0:00:08s
epoch 19 | loss: 0.47037 | eval_custom_logloss: 3.01925 |  0:00:09s
epoch 20 | loss: 0.46383 | eval_custom_logloss: 2.85478 |  0:00:09s
epoch 21 | loss: 0.46737 | eval_custom_logloss: 3.36079 |  0:00:09s
epoch 22 | loss: 0.47586 | eval_custom_logloss: 3.21339 |  0:00:10s
epoch 23 | loss: 0.45874 | eval_custom_logloss: 2.84424 |  0:00:10s
epoch 24 | loss: 0.44677 | eval_custom_logloss: 2.52233 |  0:00:11s
epoch 25 | loss: 0.44025 | eval_custom_logloss: 2.83014 |  0:00:11s
epoch 26 | loss: 0.42371 | eval_custom_logloss: 3.07833 |  0:00:11s
epoch 27 | loss: 0.42829 | eval_custom_logloss: 3.11442 |  0:00:12s
epoch 28 | loss: 0.42256 | eval_custom_logloss: 2.66911 |  0:00:12s
epoch 29 | loss: 0.43123 | eval_custom_logloss: 2.54862 |  0:00:13s
epoch 30 | loss: 0.41302 | eval_custom_logloss: 2.44226 |  0:00:13s
epoch 31 | loss: 0.39172 | eval_custom_logloss: 2.84815 |  0:00:13s
epoch 32 | loss: 0.41546 | eval_custom_logloss: 2.29626 |  0:00:14s
epoch 33 | loss: 0.45343 | eval_custom_logloss: 1.93753 |  0:00:14s
epoch 34 | loss: 0.4299  | eval_custom_logloss: 2.13845 |  0:00:14s
epoch 35 | loss: 0.41632 | eval_custom_logloss: 1.69669 |  0:00:15s
epoch 36 | loss: 0.38289 | eval_custom_logloss: 1.66281 |  0:00:15s
epoch 37 | loss: 0.39317 | eval_custom_logloss: 1.88569 |  0:00:16s
epoch 38 | loss: 0.37988 | eval_custom_logloss: 1.96747 |  0:00:16s
epoch 39 | loss: 0.39723 | eval_custom_logloss: 1.92366 |  0:00:17s
epoch 40 | loss: 0.39754 | eval_custom_logloss: 1.51818 |  0:00:17s
epoch 41 | loss: 0.38038 | eval_custom_logloss: 1.50769 |  0:00:18s
epoch 42 | loss: 0.38652 | eval_custom_logloss: 1.74969 |  0:00:18s
epoch 43 | loss: 0.38147 | eval_custom_logloss: 1.70948 |  0:00:18s
epoch 44 | loss: 0.38826 | eval_custom_logloss: 1.35971 |  0:00:19s
epoch 45 | loss: 0.39402 | eval_custom_logloss: 1.24014 |  0:00:19s
epoch 46 | loss: 0.37444 | eval_custom_logloss: 1.28699 |  0:00:20s
epoch 47 | loss: 0.3702  | eval_custom_logloss: 1.67505 |  0:00:20s
epoch 48 | loss: 0.35184 | eval_custom_logloss: 1.58737 |  0:00:21s
epoch 49 | loss: 0.37843 | eval_custom_logloss: 1.75255 |  0:00:21s
epoch 50 | loss: 0.36531 | eval_custom_logloss: 1.75489 |  0:00:22s
epoch 51 | loss: 0.33845 | eval_custom_logloss: 2.04668 |  0:00:22s
epoch 52 | loss: 0.36353 | eval_custom_logloss: 1.79664 |  0:00:23s
epoch 53 | loss: 0.3596  | eval_custom_logloss: 1.93221 |  0:00:23s
epoch 54 | loss: 0.38027 | eval_custom_logloss: 1.4968  |  0:00:23s
epoch 55 | loss: 0.37093 | eval_custom_logloss: 1.2923  |  0:00:24s
epoch 56 | loss: 0.37024 | eval_custom_logloss: 1.55042 |  0:00:24s
epoch 57 | loss: 0.36823 | eval_custom_logloss: 1.6034  |  0:00:25s
epoch 58 | loss: 0.37307 | eval_custom_logloss: 1.82611 |  0:00:25s
epoch 59 | loss: 0.38535 | eval_custom_logloss: 1.81039 |  0:00:26s
epoch 60 | loss: 0.36563 | eval_custom_logloss: 1.84344 |  0:00:26s
epoch 61 | loss: 0.3499  | eval_custom_logloss: 1.66298 |  0:00:27s
epoch 62 | loss: 0.31285 | eval_custom_logloss: 1.42713 |  0:00:27s
epoch 63 | loss: 0.32152 | eval_custom_logloss: 1.80584 |  0:00:27s
epoch 64 | loss: 0.33505 | eval_custom_logloss: 1.64942 |  0:00:28s
epoch 65 | loss: 0.33899 | eval_custom_logloss: 1.56034 |  0:00:28s

Early stopping occurred at epoch 65 with best_epoch = 45 and best_eval_custom_logloss = 1.24014
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1881, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09658 | eval_custom_logloss: 3.05897 |  0:00:00s
epoch 1  | loss: 0.90197 | eval_custom_logloss: 2.57268 |  0:00:00s
epoch 2  | loss: 0.82882 | eval_custom_logloss: 5.7646  |  0:00:01s
epoch 3  | loss: 0.78175 | eval_custom_logloss: 4.15243 |  0:00:01s
epoch 4  | loss: 0.73347 | eval_custom_logloss: 6.44843 |  0:00:01s
epoch 5  | loss: 0.65178 | eval_custom_logloss: 5.69662 |  0:00:02s
epoch 6  | loss: 0.69456 | eval_custom_logloss: 5.77458 |  0:00:02s
epoch 7  | loss: 0.61877 | eval_custom_logloss: 4.4782  |  0:00:03s
epoch 8  | loss: 0.58565 | eval_custom_logloss: 6.69381 |  0:00:03s
epoch 9  | loss: 0.58228 | eval_custom_logloss: 7.04271 |  0:00:04s
epoch 10 | loss: 0.54785 | eval_custom_logloss: 7.44355 |  0:00:04s
epoch 11 | loss: 0.54548 | eval_custom_logloss: 7.48855 |  0:00:04s
epoch 12 | loss: 0.51965 | eval_custom_logloss: 7.68624 |  0:00:05s
epoch 13 | loss: 0.5177  | eval_custom_logloss: 7.96336 |  0:00:05s
epoch 14 | loss: 0.49684 | eval_custom_logloss: 7.67732 |  0:00:06s
epoch 15 | loss: 0.49781 | eval_custom_logloss: 7.34818 |  0:00:06s
epoch 16 | loss: 0.48503 | eval_custom_logloss: 7.26234 |  0:00:07s
epoch 17 | loss: 0.47841 | eval_custom_logloss: 7.66103 |  0:00:07s
epoch 18 | loss: 0.49467 | eval_custom_logloss: 6.5738  |  0:00:08s
epoch 19 | loss: 0.46382 | eval_custom_logloss: 5.56805 |  0:00:08s
epoch 20 | loss: 0.422   | eval_custom_logloss: 4.32299 |  0:00:08s
epoch 21 | loss: 0.42295 | eval_custom_logloss: 2.98056 |  0:00:09s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.57268
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.7738, 'Log Loss - std': 0.5857000000000001} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09648 | eval_custom_logloss: 2.77543 |  0:00:00s
epoch 1  | loss: 0.91791 | eval_custom_logloss: 3.33881 |  0:00:00s
epoch 2  | loss: 0.84053 | eval_custom_logloss: 3.25448 |  0:00:01s
epoch 3  | loss: 0.81218 | eval_custom_logloss: 3.88123 |  0:00:01s
epoch 4  | loss: 0.74853 | eval_custom_logloss: 2.92559 |  0:00:02s
epoch 5  | loss: 0.69474 | eval_custom_logloss: 3.57902 |  0:00:02s
epoch 6  | loss: 0.65027 | eval_custom_logloss: 2.85327 |  0:00:02s
epoch 7  | loss: 0.64382 | eval_custom_logloss: 2.90817 |  0:00:03s
epoch 8  | loss: 0.60617 | eval_custom_logloss: 3.52662 |  0:00:03s
epoch 9  | loss: 0.60566 | eval_custom_logloss: 3.29885 |  0:00:04s
epoch 10 | loss: 0.57867 | eval_custom_logloss: 4.31941 |  0:00:04s
epoch 11 | loss: 0.56641 | eval_custom_logloss: 3.57953 |  0:00:05s
epoch 12 | loss: 0.53405 | eval_custom_logloss: 2.25082 |  0:00:05s
epoch 13 | loss: 0.52408 | eval_custom_logloss: 2.38674 |  0:00:06s
epoch 14 | loss: 0.52826 | eval_custom_logloss: 2.95277 |  0:00:07s
epoch 15 | loss: 0.49383 | eval_custom_logloss: 2.53297 |  0:00:07s
epoch 16 | loss: 0.47818 | eval_custom_logloss: 3.34222 |  0:00:07s
epoch 17 | loss: 0.49179 | eval_custom_logloss: 4.11453 |  0:00:08s
epoch 18 | loss: 0.45138 | eval_custom_logloss: 5.5396  |  0:00:08s
epoch 19 | loss: 0.47013 | eval_custom_logloss: 4.69285 |  0:00:09s
epoch 20 | loss: 0.46342 | eval_custom_logloss: 4.1881  |  0:00:09s
epoch 21 | loss: 0.4754  | eval_custom_logloss: 4.15127 |  0:00:09s
epoch 22 | loss: 0.45288 | eval_custom_logloss: 3.51643 |  0:00:10s
epoch 23 | loss: 0.46288 | eval_custom_logloss: 3.35674 |  0:00:10s
epoch 24 | loss: 0.45105 | eval_custom_logloss: 4.28355 |  0:00:11s
epoch 25 | loss: 0.4313  | eval_custom_logloss: 3.52022 |  0:00:11s
epoch 26 | loss: 0.40735 | eval_custom_logloss: 4.13939 |  0:00:11s
epoch 27 | loss: 0.43095 | eval_custom_logloss: 3.58803 |  0:00:12s
epoch 28 | loss: 0.41371 | eval_custom_logloss: 4.02467 |  0:00:12s
epoch 29 | loss: 0.38877 | eval_custom_logloss: 3.38844 |  0:00:12s
epoch 30 | loss: 0.40028 | eval_custom_logloss: 4.14608 |  0:00:13s
epoch 31 | loss: 0.40092 | eval_custom_logloss: 3.54833 |  0:00:13s
epoch 32 | loss: 0.43486 | eval_custom_logloss: 3.57565 |  0:00:14s

Early stopping occurred at epoch 32 with best_epoch = 12 and best_eval_custom_logloss = 2.25082
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8767666666666667, 'Log Loss - std': 0.4999005856723471} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.18549 | eval_custom_logloss: 3.37961 |  0:00:00s
epoch 1  | loss: 0.92753 | eval_custom_logloss: 2.36834 |  0:00:00s
epoch 2  | loss: 0.88732 | eval_custom_logloss: 2.81655 |  0:00:01s
epoch 3  | loss: 0.79954 | eval_custom_logloss: 4.88933 |  0:00:01s
epoch 4  | loss: 0.82003 | eval_custom_logloss: 3.85632 |  0:00:02s
epoch 5  | loss: 0.76403 | eval_custom_logloss: 4.75709 |  0:00:02s
epoch 6  | loss: 0.65849 | eval_custom_logloss: 6.86124 |  0:00:02s
epoch 7  | loss: 0.58843 | eval_custom_logloss: 6.94116 |  0:00:03s
epoch 8  | loss: 0.57549 | eval_custom_logloss: 6.44454 |  0:00:03s
epoch 9  | loss: 0.54208 | eval_custom_logloss: 6.32857 |  0:00:04s
epoch 10 | loss: 0.53251 | eval_custom_logloss: 5.61628 |  0:00:04s
epoch 11 | loss: 0.50227 | eval_custom_logloss: 5.50291 |  0:00:05s
epoch 12 | loss: 0.50145 | eval_custom_logloss: 5.57703 |  0:00:05s
epoch 13 | loss: 0.47643 | eval_custom_logloss: 5.11953 |  0:00:06s
epoch 14 | loss: 0.48329 | eval_custom_logloss: 5.43056 |  0:00:06s
epoch 15 | loss: 0.47753 | eval_custom_logloss: 4.6354  |  0:00:06s
epoch 16 | loss: 0.47159 | eval_custom_logloss: 4.37187 |  0:00:07s
epoch 17 | loss: 0.44964 | eval_custom_logloss: 4.43643 |  0:00:07s
epoch 18 | loss: 0.42653 | eval_custom_logloss: 5.31505 |  0:00:08s
epoch 19 | loss: 0.4508  | eval_custom_logloss: 3.97697 |  0:00:08s
epoch 20 | loss: 0.41363 | eval_custom_logloss: 3.36848 |  0:00:08s
epoch 21 | loss: 0.42489 | eval_custom_logloss: 2.72389 |  0:00:09s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 2.36834
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.946475, 'Log Loss - std': 0.44944766310995554} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax', 'n_a': 11, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.13211 | eval_custom_logloss: 3.20186 |  0:00:00s
epoch 1  | loss: 0.97149 | eval_custom_logloss: 3.19423 |  0:00:00s
epoch 2  | loss: 0.83427 | eval_custom_logloss: 3.73971 |  0:00:01s
epoch 3  | loss: 0.81163 | eval_custom_logloss: 3.04914 |  0:00:01s
epoch 4  | loss: 0.73657 | eval_custom_logloss: 4.1147  |  0:00:02s
epoch 5  | loss: 0.73308 | eval_custom_logloss: 3.76064 |  0:00:02s
epoch 6  | loss: 0.68218 | eval_custom_logloss: 6.04573 |  0:00:02s
epoch 7  | loss: 0.61449 | eval_custom_logloss: 5.23083 |  0:00:03s
epoch 8  | loss: 0.60194 | eval_custom_logloss: 4.71469 |  0:00:03s
epoch 9  | loss: 0.58096 | eval_custom_logloss: 3.53871 |  0:00:04s
epoch 10 | loss: 0.56066 | eval_custom_logloss: 3.14095 |  0:00:04s
epoch 11 | loss: 0.56308 | eval_custom_logloss: 3.16989 |  0:00:04s
epoch 12 | loss: 0.56692 | eval_custom_logloss: 2.65764 |  0:00:05s
epoch 13 | loss: 0.54237 | eval_custom_logloss: 4.60493 |  0:00:05s
epoch 14 | loss: 0.56614 | eval_custom_logloss: 2.17114 |  0:00:06s
epoch 15 | loss: 0.54369 | eval_custom_logloss: 3.18641 |  0:00:06s
epoch 16 | loss: 0.56009 | eval_custom_logloss: 3.13616 |  0:00:06s
epoch 17 | loss: 0.55684 | eval_custom_logloss: 2.06506 |  0:00:07s
epoch 18 | loss: 0.53191 | eval_custom_logloss: 2.11416 |  0:00:07s
epoch 19 | loss: 0.51647 | eval_custom_logloss: 2.63693 |  0:00:08s
epoch 20 | loss: 0.52358 | eval_custom_logloss: 2.22429 |  0:00:08s
epoch 21 | loss: 0.51823 | eval_custom_logloss: 2.04796 |  0:00:09s
epoch 22 | loss: 0.50441 | eval_custom_logloss: 2.21047 |  0:00:09s
epoch 23 | loss: 0.48775 | eval_custom_logloss: 2.44877 |  0:00:09s
epoch 24 | loss: 0.47842 | eval_custom_logloss: 2.45912 |  0:00:10s
epoch 25 | loss: 0.47739 | eval_custom_logloss: 2.08684 |  0:00:10s
epoch 26 | loss: 0.47415 | eval_custom_logloss: 1.7052  |  0:00:11s
epoch 27 | loss: 0.46996 | eval_custom_logloss: 1.60383 |  0:00:11s
epoch 28 | loss: 0.4919  | eval_custom_logloss: 2.54907 |  0:00:11s
epoch 29 | loss: 0.47558 | eval_custom_logloss: 2.91508 |  0:00:12s
epoch 30 | loss: 0.48406 | eval_custom_logloss: 2.68631 |  0:00:12s
epoch 31 | loss: 0.46983 | eval_custom_logloss: 2.50812 |  0:00:13s
epoch 32 | loss: 0.47246 | eval_custom_logloss: 2.35978 |  0:00:13s
epoch 33 | loss: 0.44065 | eval_custom_logloss: 2.58506 |  0:00:14s
epoch 34 | loss: 0.45669 | eval_custom_logloss: 2.43198 |  0:00:14s
epoch 35 | loss: 0.43324 | eval_custom_logloss: 2.34789 |  0:00:14s
epoch 36 | loss: 0.41719 | eval_custom_logloss: 2.36565 |  0:00:15s
epoch 37 | loss: 0.41293 | eval_custom_logloss: 2.44237 |  0:00:15s
epoch 38 | loss: 0.40192 | eval_custom_logloss: 2.51646 |  0:00:16s
epoch 39 | loss: 0.38463 | eval_custom_logloss: 2.18986 |  0:00:16s
epoch 40 | loss: 0.41856 | eval_custom_logloss: 1.98812 |  0:00:16s
epoch 41 | loss: 0.39344 | eval_custom_logloss: 2.13753 |  0:00:17s
epoch 42 | loss: 0.43184 | eval_custom_logloss: 2.49254 |  0:00:17s
epoch 43 | loss: 0.37848 | eval_custom_logloss: 2.0116  |  0:00:17s
epoch 44 | loss: 0.39537 | eval_custom_logloss: 2.43467 |  0:00:18s
epoch 45 | loss: 0.40956 | eval_custom_logloss: 2.50034 |  0:00:18s
epoch 46 | loss: 0.38927 | eval_custom_logloss: 2.1775  |  0:00:19s
epoch 47 | loss: 0.3852  | eval_custom_logloss: 2.61204 |  0:00:19s

Early stopping occurred at epoch 47 with best_epoch = 27 and best_eval_custom_logloss = 1.60383
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.8584599999999998, 'Log Loss - std': 0.43884977201771463} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 44 finished with value: 1.8584599999999998 and parameters: {'n_d': 11, 'n_steps': 3, 'gamma': 1.309202690492535, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.001264770618134766, 'mask_type': 'entmax'}. Best is trial 35 with value: 2.35452.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.05047 | eval_custom_logloss: 4.03839 |  0:00:00s
epoch 1  | loss: 0.90643 | eval_custom_logloss: 3.96418 |  0:00:00s
epoch 2  | loss: 0.81705 | eval_custom_logloss: 3.48314 |  0:00:01s
epoch 3  | loss: 0.73675 | eval_custom_logloss: 6.0961  |  0:00:01s
epoch 4  | loss: 0.73868 | eval_custom_logloss: 6.39592 |  0:00:02s
epoch 5  | loss: 0.7055  | eval_custom_logloss: 9.83414 |  0:00:02s
epoch 6  | loss: 0.63627 | eval_custom_logloss: 8.94898 |  0:00:02s
epoch 7  | loss: 0.64953 | eval_custom_logloss: 7.6783  |  0:00:03s
epoch 8  | loss: 0.59164 | eval_custom_logloss: 9.79307 |  0:00:03s
epoch 9  | loss: 0.5689  | eval_custom_logloss: 5.80126 |  0:00:04s
epoch 10 | loss: 0.55361 | eval_custom_logloss: 5.08395 |  0:00:04s
epoch 11 | loss: 0.52162 | eval_custom_logloss: 6.08921 |  0:00:04s
epoch 12 | loss: 0.48966 | eval_custom_logloss: 7.50061 |  0:00:05s
epoch 13 | loss: 0.50344 | eval_custom_logloss: 7.3551  |  0:00:05s
epoch 14 | loss: 0.49235 | eval_custom_logloss: 6.64885 |  0:00:05s
epoch 15 | loss: 0.47523 | eval_custom_logloss: 6.87397 |  0:00:06s
epoch 16 | loss: 0.45768 | eval_custom_logloss: 7.47423 |  0:00:06s
epoch 17 | loss: 0.45076 | eval_custom_logloss: 7.53772 |  0:00:07s
epoch 18 | loss: 0.46316 | eval_custom_logloss: 7.35108 |  0:00:07s
epoch 19 | loss: 0.42296 | eval_custom_logloss: 7.54575 |  0:00:07s
epoch 20 | loss: 0.46589 | eval_custom_logloss: 5.89407 |  0:00:08s
epoch 21 | loss: 0.44406 | eval_custom_logloss: 5.61162 |  0:00:08s
epoch 22 | loss: 0.41221 | eval_custom_logloss: 5.40108 |  0:00:08s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 3.48314
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.0467, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.04816 | eval_custom_logloss: 3.98175 |  0:00:00s
epoch 1  | loss: 0.87666 | eval_custom_logloss: 3.26023 |  0:00:00s
epoch 2  | loss: 0.79239 | eval_custom_logloss: 3.57881 |  0:00:01s
epoch 3  | loss: 0.70852 | eval_custom_logloss: 4.8317  |  0:00:01s
epoch 4  | loss: 0.71432 | eval_custom_logloss: 6.46357 |  0:00:01s
epoch 5  | loss: 0.66109 | eval_custom_logloss: 5.90256 |  0:00:02s
epoch 6  | loss: 0.61462 | eval_custom_logloss: 6.62521 |  0:00:02s
epoch 7  | loss: 0.59296 | eval_custom_logloss: 7.03261 |  0:00:02s
epoch 8  | loss: 0.55939 | eval_custom_logloss: 6.54453 |  0:00:03s
epoch 9  | loss: 0.50201 | eval_custom_logloss: 7.12515 |  0:00:03s
epoch 10 | loss: 0.53014 | eval_custom_logloss: 5.98362 |  0:00:04s
epoch 11 | loss: 0.5225  | eval_custom_logloss: 5.11315 |  0:00:04s
epoch 12 | loss: 0.50317 | eval_custom_logloss: 5.78528 |  0:00:04s
epoch 13 | loss: 0.49529 | eval_custom_logloss: 4.98024 |  0:00:05s
epoch 14 | loss: 0.49445 | eval_custom_logloss: 4.95011 |  0:00:05s
epoch 15 | loss: 0.50992 | eval_custom_logloss: 4.86301 |  0:00:05s
epoch 16 | loss: 0.47258 | eval_custom_logloss: 4.75008 |  0:00:06s
epoch 17 | loss: 0.46181 | eval_custom_logloss: 3.16845 |  0:00:06s
epoch 18 | loss: 0.45728 | eval_custom_logloss: 5.14382 |  0:00:07s
epoch 19 | loss: 0.45532 | eval_custom_logloss: 6.11254 |  0:00:07s
epoch 20 | loss: 0.45321 | eval_custom_logloss: 5.48649 |  0:00:07s
epoch 21 | loss: 0.43108 | eval_custom_logloss: 5.78496 |  0:00:08s
epoch 22 | loss: 0.48609 | eval_custom_logloss: 4.48889 |  0:00:08s
epoch 23 | loss: 0.46094 | eval_custom_logloss: 4.36747 |  0:00:08s
epoch 24 | loss: 0.43445 | eval_custom_logloss: 6.1658  |  0:00:09s
epoch 25 | loss: 0.45048 | eval_custom_logloss: 4.37438 |  0:00:09s
epoch 26 | loss: 0.42046 | eval_custom_logloss: 5.26741 |  0:00:10s
epoch 27 | loss: 0.42776 | eval_custom_logloss: 5.43811 |  0:00:10s
epoch 28 | loss: 0.44416 | eval_custom_logloss: 5.57837 |  0:00:10s
epoch 29 | loss: 0.43294 | eval_custom_logloss: 4.28959 |  0:00:11s
epoch 30 | loss: 0.42169 | eval_custom_logloss: 4.29892 |  0:00:11s
epoch 31 | loss: 0.40611 | eval_custom_logloss: 4.17216 |  0:00:11s
epoch 32 | loss: 0.45505 | eval_custom_logloss: 3.80863 |  0:00:12s
epoch 33 | loss: 0.42021 | eval_custom_logloss: 3.21286 |  0:00:12s
epoch 34 | loss: 0.39713 | eval_custom_logloss: 4.11738 |  0:00:12s
epoch 35 | loss: 0.39224 | eval_custom_logloss: 4.38201 |  0:00:13s
epoch 36 | loss: 0.41126 | eval_custom_logloss: 4.07669 |  0:00:13s
epoch 37 | loss: 0.44741 | eval_custom_logloss: 3.82899 |  0:00:14s

Early stopping occurred at epoch 37 with best_epoch = 17 and best_eval_custom_logloss = 3.16845
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.8509, 'Log Loss - std': 0.19579999999999997} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.01341 | eval_custom_logloss: 3.93397 |  0:00:00s
epoch 1  | loss: 0.83523 | eval_custom_logloss: 4.57029 |  0:00:00s
epoch 2  | loss: 0.78357 | eval_custom_logloss: 4.44465 |  0:00:01s
epoch 3  | loss: 0.72272 | eval_custom_logloss: 4.83928 |  0:00:01s
epoch 4  | loss: 0.70817 | eval_custom_logloss: 5.39022 |  0:00:01s
epoch 5  | loss: 0.67171 | eval_custom_logloss: 4.94104 |  0:00:02s
epoch 6  | loss: 0.63778 | eval_custom_logloss: 5.84516 |  0:00:02s
epoch 7  | loss: 0.59508 | eval_custom_logloss: 4.88097 |  0:00:03s
epoch 8  | loss: 0.56264 | eval_custom_logloss: 5.97784 |  0:00:03s
epoch 9  | loss: 0.55005 | eval_custom_logloss: 5.08021 |  0:00:03s
epoch 10 | loss: 0.54745 | eval_custom_logloss: 6.26746 |  0:00:04s
epoch 11 | loss: 0.53229 | eval_custom_logloss: 4.7603  |  0:00:04s
epoch 12 | loss: 0.49774 | eval_custom_logloss: 3.97974 |  0:00:04s
epoch 13 | loss: 0.48687 | eval_custom_logloss: 5.66379 |  0:00:05s
epoch 14 | loss: 0.44993 | eval_custom_logloss: 7.35647 |  0:00:05s
epoch 15 | loss: 0.46648 | eval_custom_logloss: 6.11456 |  0:00:06s
epoch 16 | loss: 0.46908 | eval_custom_logloss: 7.74985 |  0:00:06s
epoch 17 | loss: 0.46148 | eval_custom_logloss: 4.10485 |  0:00:06s
epoch 18 | loss: 0.41968 | eval_custom_logloss: 5.48722 |  0:00:07s
epoch 19 | loss: 0.39401 | eval_custom_logloss: 4.14204 |  0:00:07s
epoch 20 | loss: 0.37862 | eval_custom_logloss: 3.94948 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 3.93397
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.0332000000000003, 'Log Loss - std': 0.3033562372305318} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.07275 | eval_custom_logloss: 3.38556 |  0:00:00s
epoch 1  | loss: 0.90802 | eval_custom_logloss: 3.40711 |  0:00:00s
epoch 2  | loss: 0.79005 | eval_custom_logloss: 4.21449 |  0:00:01s
epoch 3  | loss: 0.69496 | eval_custom_logloss: 6.10748 |  0:00:01s
epoch 4  | loss: 0.65263 | eval_custom_logloss: 6.45903 |  0:00:01s
epoch 5  | loss: 0.65371 | eval_custom_logloss: 5.23208 |  0:00:02s
epoch 6  | loss: 0.58757 | eval_custom_logloss: 4.62858 |  0:00:02s
epoch 7  | loss: 0.5567  | eval_custom_logloss: 5.49414 |  0:00:03s
epoch 8  | loss: 0.54641 | eval_custom_logloss: 5.95823 |  0:00:03s
epoch 9  | loss: 0.50338 | eval_custom_logloss: 6.08825 |  0:00:03s
epoch 10 | loss: 0.4854  | eval_custom_logloss: 6.57941 |  0:00:04s
epoch 11 | loss: 0.49603 | eval_custom_logloss: 5.91184 |  0:00:04s
epoch 12 | loss: 0.51966 | eval_custom_logloss: 5.41492 |  0:00:04s
epoch 13 | loss: 0.49756 | eval_custom_logloss: 6.02379 |  0:00:05s
epoch 14 | loss: 0.51088 | eval_custom_logloss: 6.9505  |  0:00:05s
epoch 15 | loss: 0.55855 | eval_custom_logloss: 6.08284 |  0:00:06s
epoch 16 | loss: 0.50227 | eval_custom_logloss: 5.17912 |  0:00:06s
epoch 17 | loss: 0.46305 | eval_custom_logloss: 4.79238 |  0:00:07s
epoch 18 | loss: 0.43321 | eval_custom_logloss: 4.31944 |  0:00:07s
epoch 19 | loss: 0.45077 | eval_custom_logloss: 4.82042 |  0:00:08s
epoch 20 | loss: 0.45418 | eval_custom_logloss: 4.59321 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 3.38556
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.003075, 'Log Loss - std': 0.26784566801611714} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11969 | eval_custom_logloss: 4.09163 |  0:00:00s
epoch 1  | loss: 0.87411 | eval_custom_logloss: 3.7555  |  0:00:00s
epoch 2  | loss: 0.78356 | eval_custom_logloss: 4.64136 |  0:00:01s
epoch 3  | loss: 0.71597 | eval_custom_logloss: 6.0853  |  0:00:01s
epoch 4  | loss: 0.66152 | eval_custom_logloss: 5.77437 |  0:00:01s
epoch 5  | loss: 0.63896 | eval_custom_logloss: 5.12903 |  0:00:02s
epoch 6  | loss: 0.57967 | eval_custom_logloss: 6.04289 |  0:00:02s
epoch 7  | loss: 0.54645 | eval_custom_logloss: 5.50244 |  0:00:03s
epoch 8  | loss: 0.52046 | eval_custom_logloss: 5.51066 |  0:00:03s
epoch 9  | loss: 0.53556 | eval_custom_logloss: 5.93506 |  0:00:03s
epoch 10 | loss: 0.53202 | eval_custom_logloss: 5.39585 |  0:00:04s
epoch 11 | loss: 0.52523 | eval_custom_logloss: 4.74756 |  0:00:04s
epoch 12 | loss: 0.50843 | eval_custom_logloss: 5.11053 |  0:00:04s
epoch 13 | loss: 0.49234 | eval_custom_logloss: 4.81993 |  0:00:05s
epoch 14 | loss: 0.49583 | eval_custom_logloss: 4.21543 |  0:00:05s
epoch 15 | loss: 0.4665  | eval_custom_logloss: 3.81897 |  0:00:06s
epoch 16 | loss: 0.44681 | eval_custom_logloss: 3.86648 |  0:00:06s
epoch 17 | loss: 0.44147 | eval_custom_logloss: 2.89364 |  0:00:06s
epoch 18 | loss: 0.43741 | eval_custom_logloss: 3.28084 |  0:00:07s
epoch 19 | loss: 0.4249  | eval_custom_logloss: 2.9091  |  0:00:07s
epoch 20 | loss: 0.41164 | eval_custom_logloss: 2.26066 |  0:00:07s
epoch 21 | loss: 0.41076 | eval_custom_logloss: 3.06583 |  0:00:08s
epoch 22 | loss: 0.41484 | eval_custom_logloss: 3.64915 |  0:00:08s
epoch 23 | loss: 0.41461 | eval_custom_logloss: 4.07977 |  0:00:09s
epoch 24 | loss: 0.38951 | eval_custom_logloss: 3.78217 |  0:00:09s
epoch 25 | loss: 0.42421 | eval_custom_logloss: 3.64338 |  0:00:09s
epoch 26 | loss: 0.37166 | eval_custom_logloss: 3.79812 |  0:00:10s
epoch 27 | loss: 0.36312 | eval_custom_logloss: 3.90533 |  0:00:10s
epoch 28 | loss: 0.36406 | eval_custom_logloss: 3.75519 |  0:00:10s
epoch 29 | loss: 0.34859 | eval_custom_logloss: 3.84762 |  0:00:11s
epoch 30 | loss: 0.32629 | eval_custom_logloss: 3.98002 |  0:00:11s
epoch 31 | loss: 0.37207 | eval_custom_logloss: 3.37651 |  0:00:12s
epoch 32 | loss: 0.36946 | eval_custom_logloss: 3.31166 |  0:00:12s
epoch 33 | loss: 0.34664 | eval_custom_logloss: 3.13519 |  0:00:12s
epoch 34 | loss: 0.33888 | eval_custom_logloss: 3.31521 |  0:00:13s
epoch 35 | loss: 0.31009 | eval_custom_logloss: 3.03957 |  0:00:13s
epoch 36 | loss: 0.32138 | eval_custom_logloss: 2.5571  |  0:00:13s
epoch 37 | loss: 0.30995 | eval_custom_logloss: 2.27321 |  0:00:14s
epoch 38 | loss: 0.3063  | eval_custom_logloss: 3.16778 |  0:00:14s
epoch 39 | loss: 0.30857 | eval_custom_logloss: 2.7409  |  0:00:14s
epoch 40 | loss: 0.28263 | eval_custom_logloss: 2.4899  |  0:00:15s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 2.26066
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.80386, 'Log Loss - std': 0.46490806230909787} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 45 finished with value: 2.80386 and parameters: {'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax', 'n_a': 18, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.10263 | eval_custom_logloss: 4.349   |  0:00:00s
epoch 1  | loss: 0.87656 | eval_custom_logloss: 4.08364 |  0:00:00s
epoch 2  | loss: 0.76358 | eval_custom_logloss: 3.48402 |  0:00:01s
epoch 3  | loss: 0.67403 | eval_custom_logloss: 4.48265 |  0:00:01s
epoch 4  | loss: 0.64973 | eval_custom_logloss: 4.51651 |  0:00:01s
epoch 5  | loss: 0.60792 | eval_custom_logloss: 3.61665 |  0:00:02s
epoch 6  | loss: 0.53294 | eval_custom_logloss: 4.00339 |  0:00:02s
epoch 7  | loss: 0.53699 | eval_custom_logloss: 3.54598 |  0:00:03s
epoch 8  | loss: 0.5328  | eval_custom_logloss: 3.15479 |  0:00:03s
epoch 9  | loss: 0.49787 | eval_custom_logloss: 3.87537 |  0:00:03s
epoch 10 | loss: 0.50119 | eval_custom_logloss: 4.0251  |  0:00:04s
epoch 11 | loss: 0.47952 | eval_custom_logloss: 4.5262  |  0:00:04s
epoch 12 | loss: 0.43924 | eval_custom_logloss: 4.13476 |  0:00:04s
epoch 13 | loss: 0.46031 | eval_custom_logloss: 3.14434 |  0:00:05s
epoch 14 | loss: 0.44837 | eval_custom_logloss: 3.1195  |  0:00:05s
epoch 15 | loss: 0.43479 | eval_custom_logloss: 2.81822 |  0:00:06s
epoch 16 | loss: 0.42902 | eval_custom_logloss: 2.28693 |  0:00:06s
epoch 17 | loss: 0.36471 | eval_custom_logloss: 3.04868 |  0:00:06s
epoch 18 | loss: 0.45411 | eval_custom_logloss: 2.42448 |  0:00:07s
epoch 19 | loss: 0.40874 | eval_custom_logloss: 2.72498 |  0:00:07s
epoch 20 | loss: 0.36146 | eval_custom_logloss: 3.17388 |  0:00:07s
epoch 21 | loss: 0.36425 | eval_custom_logloss: 2.99705 |  0:00:08s
epoch 22 | loss: 0.3623  | eval_custom_logloss: 2.65934 |  0:00:08s
epoch 23 | loss: 0.35382 | eval_custom_logloss: 2.6255  |  0:00:09s
epoch 24 | loss: 0.31524 | eval_custom_logloss: 2.61897 |  0:00:09s
epoch 25 | loss: 0.30789 | eval_custom_logloss: 3.0534  |  0:00:09s
epoch 26 | loss: 0.35513 | eval_custom_logloss: 2.28241 |  0:00:10s
epoch 27 | loss: 0.32746 | eval_custom_logloss: 2.03768 |  0:00:10s
epoch 28 | loss: 0.29733 | eval_custom_logloss: 2.13687 |  0:00:10s
epoch 29 | loss: 0.32659 | eval_custom_logloss: 1.54499 |  0:00:11s
epoch 30 | loss: 0.31327 | eval_custom_logloss: 2.00969 |  0:00:11s
epoch 31 | loss: 0.28798 | eval_custom_logloss: 1.93727 |  0:00:11s
epoch 32 | loss: 0.27951 | eval_custom_logloss: 2.14406 |  0:00:12s
epoch 33 | loss: 0.28506 | eval_custom_logloss: 2.12935 |  0:00:12s
epoch 34 | loss: 0.28189 | eval_custom_logloss: 1.81689 |  0:00:13s
epoch 35 | loss: 0.28821 | eval_custom_logloss: 1.61578 |  0:00:13s
epoch 36 | loss: 0.24569 | eval_custom_logloss: 1.94596 |  0:00:13s
epoch 37 | loss: 0.23841 | eval_custom_logloss: 2.47914 |  0:00:14s
epoch 38 | loss: 0.24565 | eval_custom_logloss: 2.28031 |  0:00:14s
epoch 39 | loss: 0.24144 | eval_custom_logloss: 1.68861 |  0:00:15s
epoch 40 | loss: 0.25799 | eval_custom_logloss: 2.11684 |  0:00:15s
epoch 41 | loss: 0.27318 | eval_custom_logloss: 1.83705 |  0:00:15s
epoch 42 | loss: 0.2499  | eval_custom_logloss: 1.45163 |  0:00:16s
epoch 43 | loss: 0.22349 | eval_custom_logloss: 1.43648 |  0:00:16s
epoch 44 | loss: 0.23849 | eval_custom_logloss: 1.30589 |  0:00:17s
epoch 45 | loss: 0.22964 | eval_custom_logloss: 1.47267 |  0:00:17s
epoch 46 | loss: 0.22443 | eval_custom_logloss: 1.35142 |  0:00:17s
epoch 47 | loss: 0.23305 | eval_custom_logloss: 1.35862 |  0:00:18s
epoch 48 | loss: 0.23887 | eval_custom_logloss: 1.40148 |  0:00:18s
epoch 49 | loss: 0.2499  | eval_custom_logloss: 1.30964 |  0:00:19s
epoch 50 | loss: 0.28821 | eval_custom_logloss: 1.04767 |  0:00:19s
epoch 51 | loss: 0.26307 | eval_custom_logloss: 1.07784 |  0:00:20s
epoch 52 | loss: 0.24387 | eval_custom_logloss: 1.13211 |  0:00:20s
epoch 53 | loss: 0.24261 | eval_custom_logloss: 1.17975 |  0:00:20s
epoch 54 | loss: 0.26291 | eval_custom_logloss: 1.25476 |  0:00:21s
epoch 55 | loss: 0.21998 | eval_custom_logloss: 1.38138 |  0:00:21s
epoch 56 | loss: 0.24498 | eval_custom_logloss: 1.35672 |  0:00:22s
epoch 57 | loss: 0.24994 | eval_custom_logloss: 1.41891 |  0:00:22s
epoch 58 | loss: 0.22074 | eval_custom_logloss: 1.32486 |  0:00:22s
epoch 59 | loss: 0.1954  | eval_custom_logloss: 1.32165 |  0:00:23s
epoch 60 | loss: 0.21445 | eval_custom_logloss: 1.29904 |  0:00:23s
epoch 61 | loss: 0.18597 | eval_custom_logloss: 1.6176  |  0:00:24s
epoch 62 | loss: 0.20251 | eval_custom_logloss: 1.42306 |  0:00:24s
epoch 63 | loss: 0.20526 | eval_custom_logloss: 1.35116 |  0:00:25s
epoch 64 | loss: 0.20856 | eval_custom_logloss: 1.30048 |  0:00:25s
epoch 65 | loss: 0.1956  | eval_custom_logloss: 1.29147 |  0:00:25s
epoch 66 | loss: 0.18822 | eval_custom_logloss: 1.30141 |  0:00:26s
epoch 67 | loss: 0.22206 | eval_custom_logloss: 1.45995 |  0:00:26s
epoch 68 | loss: 0.19701 | eval_custom_logloss: 1.18754 |  0:00:27s
epoch 69 | loss: 0.19519 | eval_custom_logloss: 1.31327 |  0:00:27s
epoch 70 | loss: 0.17884 | eval_custom_logloss: 1.14166 |  0:00:27s

Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_custom_logloss = 1.04767
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0236, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax', 'n_a': 18, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.06652 | eval_custom_logloss: 3.04869 |  0:00:00s
epoch 1  | loss: 0.84652 | eval_custom_logloss: 3.40548 |  0:00:00s
epoch 2  | loss: 0.73131 | eval_custom_logloss: 3.95528 |  0:00:01s
epoch 3  | loss: 0.66805 | eval_custom_logloss: 4.71171 |  0:00:01s
epoch 4  | loss: 0.65667 | eval_custom_logloss: 3.83214 |  0:00:01s
epoch 5  | loss: 0.63584 | eval_custom_logloss: 4.33464 |  0:00:02s
epoch 6  | loss: 0.57658 | eval_custom_logloss: 4.59172 |  0:00:02s
epoch 7  | loss: 0.52705 | eval_custom_logloss: 4.17339 |  0:00:02s
epoch 8  | loss: 0.52831 | eval_custom_logloss: 4.59502 |  0:00:03s
epoch 9  | loss: 0.51852 | eval_custom_logloss: 3.99194 |  0:00:03s
epoch 10 | loss: 0.49551 | eval_custom_logloss: 3.70397 |  0:00:03s
epoch 11 | loss: 0.47796 | eval_custom_logloss: 4.07966 |  0:00:04s
epoch 12 | loss: 0.47158 | eval_custom_logloss: 3.26962 |  0:00:04s
epoch 13 | loss: 0.41976 | eval_custom_logloss: 3.2431  |  0:00:05s
epoch 14 | loss: 0.42789 | eval_custom_logloss: 3.20463 |  0:00:05s
epoch 15 | loss: 0.41215 | eval_custom_logloss: 2.61402 |  0:00:05s
epoch 16 | loss: 0.40254 | eval_custom_logloss: 2.8158  |  0:00:06s
epoch 17 | loss: 0.36261 | eval_custom_logloss: 2.70893 |  0:00:06s
epoch 18 | loss: 0.36994 | eval_custom_logloss: 2.65892 |  0:00:07s
epoch 19 | loss: 0.36412 | eval_custom_logloss: 1.96066 |  0:00:07s
epoch 20 | loss: 0.34236 | eval_custom_logloss: 2.05764 |  0:00:08s
epoch 21 | loss: 0.35644 | eval_custom_logloss: 1.90085 |  0:00:08s
epoch 22 | loss: 0.35028 | eval_custom_logloss: 1.40493 |  0:00:08s
epoch 23 | loss: 0.33334 | eval_custom_logloss: 1.43178 |  0:00:09s
epoch 24 | loss: 0.34112 | eval_custom_logloss: 1.84744 |  0:00:09s
epoch 25 | loss: 0.31229 | eval_custom_logloss: 1.79763 |  0:00:10s
epoch 26 | loss: 0.32339 | eval_custom_logloss: 1.49971 |  0:00:10s
epoch 27 | loss: 0.30267 | eval_custom_logloss: 1.39635 |  0:00:10s
epoch 28 | loss: 0.29447 | eval_custom_logloss: 1.30543 |  0:00:11s
epoch 29 | loss: 0.31987 | eval_custom_logloss: 1.59479 |  0:00:11s
epoch 30 | loss: 0.2858  | eval_custom_logloss: 1.52236 |  0:00:12s
epoch 31 | loss: 0.29312 | eval_custom_logloss: 1.76591 |  0:00:12s
epoch 32 | loss: 0.3176  | eval_custom_logloss: 1.44875 |  0:00:13s
epoch 33 | loss: 0.29846 | eval_custom_logloss: 1.26321 |  0:00:13s
epoch 34 | loss: 0.27754 | eval_custom_logloss: 1.34553 |  0:00:13s
epoch 35 | loss: 0.29923 | eval_custom_logloss: 1.27812 |  0:00:14s
epoch 36 | loss: 0.2705  | eval_custom_logloss: 1.16391 |  0:00:14s
epoch 37 | loss: 0.28398 | eval_custom_logloss: 1.18667 |  0:00:15s
epoch 38 | loss: 0.2614  | eval_custom_logloss: 1.16787 |  0:00:15s
epoch 39 | loss: 0.26415 | eval_custom_logloss: 1.07906 |  0:00:15s
epoch 40 | loss: 0.23096 | eval_custom_logloss: 1.19047 |  0:00:16s
epoch 41 | loss: 0.21857 | eval_custom_logloss: 1.16122 |  0:00:16s
epoch 42 | loss: 0.23044 | eval_custom_logloss: 1.22591 |  0:00:17s
epoch 43 | loss: 0.22958 | eval_custom_logloss: 1.25168 |  0:00:17s
epoch 44 | loss: 0.25339 | eval_custom_logloss: 1.68259 |  0:00:17s
epoch 45 | loss: 0.26526 | eval_custom_logloss: 1.4862  |  0:00:18s
epoch 46 | loss: 0.27436 | eval_custom_logloss: 1.46888 |  0:00:18s
epoch 47 | loss: 0.24283 | eval_custom_logloss: 1.23979 |  0:00:19s
epoch 48 | loss: 0.26855 | eval_custom_logloss: 1.649   |  0:00:19s
epoch 49 | loss: 0.24424 | eval_custom_logloss: 1.20689 |  0:00:19s
epoch 50 | loss: 0.24042 | eval_custom_logloss: 1.71862 |  0:00:20s
epoch 51 | loss: 0.27012 | eval_custom_logloss: 1.69043 |  0:00:20s
epoch 52 | loss: 0.22478 | eval_custom_logloss: 1.55968 |  0:00:21s
epoch 53 | loss: 0.22315 | eval_custom_logloss: 1.8158  |  0:00:21s
epoch 54 | loss: 0.25102 | eval_custom_logloss: 1.51956 |  0:00:21s
epoch 55 | loss: 0.23237 | eval_custom_logloss: 1.45581 |  0:00:22s
epoch 56 | loss: 0.20979 | eval_custom_logloss: 1.5161  |  0:00:22s
epoch 57 | loss: 0.23859 | eval_custom_logloss: 1.45773 |  0:00:23s
epoch 58 | loss: 0.21284 | eval_custom_logloss: 1.57575 |  0:00:23s
epoch 59 | loss: 0.23644 | eval_custom_logloss: 1.51848 |  0:00:24s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 1.07906
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.03225, 'Log Loss - std': 0.008649999999999936} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax', 'n_a': 18, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11794 | eval_custom_logloss: 3.86709 |  0:00:00s
epoch 1  | loss: 0.87225 | eval_custom_logloss: 3.7921  |  0:00:00s
epoch 2  | loss: 0.79818 | eval_custom_logloss: 4.36287 |  0:00:01s
epoch 3  | loss: 0.73637 | eval_custom_logloss: 4.0731  |  0:00:01s
epoch 4  | loss: 0.69806 | eval_custom_logloss: 5.44386 |  0:00:01s
epoch 5  | loss: 0.6148  | eval_custom_logloss: 7.64912 |  0:00:02s
epoch 6  | loss: 0.5691  | eval_custom_logloss: 5.47488 |  0:00:02s
epoch 7  | loss: 0.53658 | eval_custom_logloss: 5.38864 |  0:00:02s
epoch 8  | loss: 0.53243 | eval_custom_logloss: 4.5013  |  0:00:03s
epoch 9  | loss: 0.51154 | eval_custom_logloss: 3.79782 |  0:00:03s
epoch 10 | loss: 0.51296 | eval_custom_logloss: 3.19274 |  0:00:03s
epoch 11 | loss: 0.48191 | eval_custom_logloss: 2.67751 |  0:00:04s
epoch 12 | loss: 0.48103 | eval_custom_logloss: 2.88393 |  0:00:04s
epoch 13 | loss: 0.47719 | eval_custom_logloss: 2.38218 |  0:00:04s
epoch 14 | loss: 0.45108 | eval_custom_logloss: 2.45914 |  0:00:05s
epoch 15 | loss: 0.45081 | eval_custom_logloss: 2.67035 |  0:00:05s
epoch 16 | loss: 0.45072 | eval_custom_logloss: 2.10772 |  0:00:06s
epoch 17 | loss: 0.4578  | eval_custom_logloss: 2.75987 |  0:00:06s
epoch 18 | loss: 0.47078 | eval_custom_logloss: 2.13455 |  0:00:06s
epoch 19 | loss: 0.44491 | eval_custom_logloss: 1.19493 |  0:00:07s
epoch 20 | loss: 0.41823 | eval_custom_logloss: 1.23824 |  0:00:07s
epoch 21 | loss: 0.37011 | eval_custom_logloss: 1.30657 |  0:00:07s
epoch 22 | loss: 0.40292 | eval_custom_logloss: 1.36055 |  0:00:08s
epoch 23 | loss: 0.36794 | eval_custom_logloss: 1.44854 |  0:00:08s
epoch 24 | loss: 0.36374 | eval_custom_logloss: 1.46114 |  0:00:08s
epoch 25 | loss: 0.36038 | eval_custom_logloss: 1.6751  |  0:00:09s
epoch 26 | loss: 0.36285 | eval_custom_logloss: 1.92549 |  0:00:09s
epoch 27 | loss: 0.38735 | eval_custom_logloss: 1.78186 |  0:00:09s
epoch 28 | loss: 0.41787 | eval_custom_logloss: 1.29383 |  0:00:10s
epoch 29 | loss: 0.40715 | eval_custom_logloss: 1.98564 |  0:00:10s
epoch 30 | loss: 0.34658 | eval_custom_logloss: 1.71458 |  0:00:11s
epoch 31 | loss: 0.38766 | eval_custom_logloss: 1.57091 |  0:00:11s
epoch 32 | loss: 0.34595 | eval_custom_logloss: 1.2959  |  0:00:11s
epoch 33 | loss: 0.33169 | eval_custom_logloss: 1.53752 |  0:00:12s
epoch 34 | loss: 0.3247  | eval_custom_logloss: 1.57611 |  0:00:12s
epoch 35 | loss: 0.34939 | eval_custom_logloss: 1.58291 |  0:00:12s
epoch 36 | loss: 0.36869 | eval_custom_logloss: 1.75511 |  0:00:13s
epoch 37 | loss: 0.32625 | eval_custom_logloss: 2.14951 |  0:00:13s
epoch 38 | loss: 0.32623 | eval_custom_logloss: 2.00031 |  0:00:13s
epoch 39 | loss: 0.3345  | eval_custom_logloss: 1.66248 |  0:00:14s

Early stopping occurred at epoch 39 with best_epoch = 19 and best_eval_custom_logloss = 1.19493
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0850666666666666, 'Log Loss - std': 0.07502720987896476} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax', 'n_a': 18, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.10476 | eval_custom_logloss: 3.29507 |  0:00:00s
epoch 1  | loss: 0.87947 | eval_custom_logloss: 4.3239  |  0:00:00s
epoch 2  | loss: 0.77636 | eval_custom_logloss: 3.42822 |  0:00:01s
epoch 3  | loss: 0.7075  | eval_custom_logloss: 6.9731  |  0:00:01s
epoch 4  | loss: 0.68198 | eval_custom_logloss: 5.1317  |  0:00:01s
epoch 5  | loss: 0.64842 | eval_custom_logloss: 3.77782 |  0:00:02s
epoch 6  | loss: 0.61951 | eval_custom_logloss: 4.98457 |  0:00:02s
epoch 7  | loss: 0.55358 | eval_custom_logloss: 3.39403 |  0:00:02s
epoch 8  | loss: 0.52101 | eval_custom_logloss: 3.29803 |  0:00:03s
epoch 9  | loss: 0.54919 | eval_custom_logloss: 2.83515 |  0:00:03s
epoch 10 | loss: 0.50194 | eval_custom_logloss: 3.19578 |  0:00:03s
epoch 11 | loss: 0.50109 | eval_custom_logloss: 1.91429 |  0:00:04s
epoch 12 | loss: 0.48087 | eval_custom_logloss: 2.12349 |  0:00:04s
epoch 13 | loss: 0.47421 | eval_custom_logloss: 2.77845 |  0:00:05s
epoch 14 | loss: 0.45325 | eval_custom_logloss: 2.28837 |  0:00:05s
epoch 15 | loss: 0.42359 | eval_custom_logloss: 1.64042 |  0:00:05s
epoch 16 | loss: 0.43111 | eval_custom_logloss: 1.19462 |  0:00:06s
epoch 17 | loss: 0.42513 | eval_custom_logloss: 1.16963 |  0:00:06s
epoch 18 | loss: 0.40535 | eval_custom_logloss: 1.33347 |  0:00:06s
epoch 19 | loss: 0.38796 | eval_custom_logloss: 1.61818 |  0:00:07s
epoch 20 | loss: 0.3918  | eval_custom_logloss: 1.69736 |  0:00:07s
epoch 21 | loss: 0.38328 | eval_custom_logloss: 2.60894 |  0:00:07s
epoch 22 | loss: 0.37168 | eval_custom_logloss: 2.23664 |  0:00:08s
epoch 23 | loss: 0.38025 | eval_custom_logloss: 2.42457 |  0:00:08s
epoch 24 | loss: 0.40515 | eval_custom_logloss: 1.70339 |  0:00:08s
epoch 25 | loss: 0.38274 | eval_custom_logloss: 1.65335 |  0:00:09s
epoch 26 | loss: 0.35133 | eval_custom_logloss: 1.81883 |  0:00:09s
epoch 27 | loss: 0.35514 | eval_custom_logloss: 2.26672 |  0:00:09s
epoch 28 | loss: 0.31544 | eval_custom_logloss: 2.3619  |  0:00:10s
epoch 29 | loss: 0.36893 | eval_custom_logloss: 1.73081 |  0:00:10s
epoch 30 | loss: 0.34803 | eval_custom_logloss: 1.44636 |  0:00:10s
epoch 31 | loss: 0.33843 | eval_custom_logloss: 1.37978 |  0:00:11s
epoch 32 | loss: 0.32572 | eval_custom_logloss: 1.68617 |  0:00:11s
epoch 33 | loss: 0.31851 | eval_custom_logloss: 1.73622 |  0:00:12s
epoch 34 | loss: 0.30366 | eval_custom_logloss: 1.27762 |  0:00:12s
epoch 35 | loss: 0.32029 | eval_custom_logloss: 1.29084 |  0:00:12s
epoch 36 | loss: 0.31822 | eval_custom_logloss: 1.04588 |  0:00:13s
epoch 37 | loss: 0.29571 | eval_custom_logloss: 1.12477 |  0:00:13s
epoch 38 | loss: 0.28519 | eval_custom_logloss: 1.05549 |  0:00:13s
epoch 39 | loss: 0.27789 | eval_custom_logloss: 0.9341  |  0:00:14s
epoch 40 | loss: 0.27828 | eval_custom_logloss: 0.95153 |  0:00:14s
epoch 41 | loss: 0.28329 | eval_custom_logloss: 0.9957  |  0:00:14s
epoch 42 | loss: 0.29372 | eval_custom_logloss: 1.44821 |  0:00:15s
epoch 43 | loss: 0.28636 | eval_custom_logloss: 1.40569 |  0:00:15s
epoch 44 | loss: 0.3003  | eval_custom_logloss: 1.1993  |  0:00:15s
epoch 45 | loss: 0.31815 | eval_custom_logloss: 1.80114 |  0:00:16s
epoch 46 | loss: 0.29843 | eval_custom_logloss: 1.02888 |  0:00:16s
epoch 47 | loss: 0.28311 | eval_custom_logloss: 1.27875 |  0:00:16s
epoch 48 | loss: 0.27796 | eval_custom_logloss: 0.84425 |  0:00:17s
epoch 49 | loss: 0.27958 | eval_custom_logloss: 0.91154 |  0:00:17s
epoch 50 | loss: 0.26648 | eval_custom_logloss: 0.96886 |  0:00:17s
epoch 51 | loss: 0.27792 | eval_custom_logloss: 0.97333 |  0:00:18s
epoch 52 | loss: 0.2876  | eval_custom_logloss: 1.16753 |  0:00:18s
epoch 53 | loss: 0.25132 | eval_custom_logloss: 1.34598 |  0:00:19s
epoch 54 | loss: 0.25536 | eval_custom_logloss: 1.67684 |  0:00:19s
epoch 55 | loss: 0.30077 | eval_custom_logloss: 1.05351 |  0:00:19s
epoch 56 | loss: 0.28632 | eval_custom_logloss: 1.16074 |  0:00:20s
epoch 57 | loss: 0.27203 | eval_custom_logloss: 1.42584 |  0:00:20s
epoch 58 | loss: 0.25542 | eval_custom_logloss: 1.14565 |  0:00:20s
epoch 59 | loss: 0.26237 | eval_custom_logloss: 1.04331 |  0:00:21s
epoch 60 | loss: 0.23215 | eval_custom_logloss: 0.95536 |  0:00:21s
epoch 61 | loss: 0.2096  | eval_custom_logloss: 0.99065 |  0:00:21s
epoch 62 | loss: 0.24311 | eval_custom_logloss: 0.93697 |  0:00:22s
epoch 63 | loss: 0.22662 | eval_custom_logloss: 0.99142 |  0:00:22s
epoch 64 | loss: 0.19156 | eval_custom_logloss: 0.93751 |  0:00:22s
epoch 65 | loss: 0.20429 | eval_custom_logloss: 0.96615 |  0:00:23s
epoch 66 | loss: 0.21318 | eval_custom_logloss: 1.12224 |  0:00:23s
epoch 67 | loss: 0.23828 | eval_custom_logloss: 1.16305 |  0:00:24s
epoch 68 | loss: 0.20605 | eval_custom_logloss: 1.13364 |  0:00:24s

Early stopping occurred at epoch 68 with best_epoch = 48 and best_eval_custom_logloss = 0.84425
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.023075, 'Log Loss - std': 0.12550184012595197} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax', 'n_a': 18, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.10564 | eval_custom_logloss: 2.69721 |  0:00:00s
epoch 1  | loss: 0.87011 | eval_custom_logloss: 3.56504 |  0:00:00s
epoch 2  | loss: 0.787   | eval_custom_logloss: 4.08513 |  0:00:01s
epoch 3  | loss: 0.70678 | eval_custom_logloss: 4.13172 |  0:00:01s
epoch 4  | loss: 0.63468 | eval_custom_logloss: 4.37177 |  0:00:01s
epoch 5  | loss: 0.63105 | eval_custom_logloss: 4.88378 |  0:00:02s
epoch 6  | loss: 0.59073 | eval_custom_logloss: 4.89023 |  0:00:02s
epoch 7  | loss: 0.56846 | eval_custom_logloss: 4.15821 |  0:00:02s
epoch 8  | loss: 0.5812  | eval_custom_logloss: 5.2716  |  0:00:03s
epoch 9  | loss: 0.55418 | eval_custom_logloss: 4.07939 |  0:00:03s
epoch 10 | loss: 0.54847 | eval_custom_logloss: 2.55075 |  0:00:03s
epoch 11 | loss: 0.54632 | eval_custom_logloss: 2.76595 |  0:00:04s
epoch 12 | loss: 0.55612 | eval_custom_logloss: 2.80201 |  0:00:04s
epoch 13 | loss: 0.54766 | eval_custom_logloss: 1.90281 |  0:00:04s
epoch 14 | loss: 0.52854 | eval_custom_logloss: 1.52132 |  0:00:05s
epoch 15 | loss: 0.49387 | eval_custom_logloss: 2.84205 |  0:00:05s
epoch 16 | loss: 0.49253 | eval_custom_logloss: 2.00813 |  0:00:05s
epoch 17 | loss: 0.49114 | eval_custom_logloss: 1.67598 |  0:00:06s
epoch 18 | loss: 0.47623 | eval_custom_logloss: 2.13147 |  0:00:06s
epoch 19 | loss: 0.46048 | eval_custom_logloss: 1.87415 |  0:00:06s
epoch 20 | loss: 0.45635 | eval_custom_logloss: 1.61539 |  0:00:07s
epoch 21 | loss: 0.46478 | eval_custom_logloss: 1.56139 |  0:00:07s
epoch 22 | loss: 0.46644 | eval_custom_logloss: 1.69996 |  0:00:07s
epoch 23 | loss: 0.47619 | eval_custom_logloss: 1.73935 |  0:00:08s
epoch 24 | loss: 0.44284 | eval_custom_logloss: 1.52527 |  0:00:08s
epoch 25 | loss: 0.43366 | eval_custom_logloss: 1.61928 |  0:00:09s
epoch 26 | loss: 0.43514 | eval_custom_logloss: 1.90115 |  0:00:09s
epoch 27 | loss: 0.41696 | eval_custom_logloss: 1.5027  |  0:00:09s
epoch 28 | loss: 0.38786 | eval_custom_logloss: 1.53069 |  0:00:10s
epoch 29 | loss: 0.40757 | eval_custom_logloss: 1.3286  |  0:00:10s
epoch 30 | loss: 0.37543 | eval_custom_logloss: 1.22477 |  0:00:10s
epoch 31 | loss: 0.36945 | eval_custom_logloss: 1.1268  |  0:00:11s
epoch 32 | loss: 0.36907 | eval_custom_logloss: 1.06248 |  0:00:11s
epoch 33 | loss: 0.36937 | eval_custom_logloss: 1.01133 |  0:00:11s
epoch 34 | loss: 0.37727 | eval_custom_logloss: 0.96122 |  0:00:12s
epoch 35 | loss: 0.38973 | eval_custom_logloss: 1.00832 |  0:00:12s
epoch 36 | loss: 0.37444 | eval_custom_logloss: 1.13006 |  0:00:12s
epoch 37 | loss: 0.41899 | eval_custom_logloss: 0.93213 |  0:00:13s
epoch 38 | loss: 0.39846 | eval_custom_logloss: 0.85908 |  0:00:13s
epoch 39 | loss: 0.35433 | eval_custom_logloss: 0.85437 |  0:00:13s
epoch 40 | loss: 0.36145 | eval_custom_logloss: 0.90978 |  0:00:14s
epoch 41 | loss: 0.37631 | eval_custom_logloss: 1.30714 |  0:00:14s
epoch 42 | loss: 0.38136 | eval_custom_logloss: 1.143   |  0:00:14s
epoch 43 | loss: 0.36694 | eval_custom_logloss: 0.98227 |  0:00:15s
epoch 44 | loss: 0.32536 | eval_custom_logloss: 1.03486 |  0:00:15s
epoch 45 | loss: 0.35779 | eval_custom_logloss: 1.02315 |  0:00:15s
epoch 46 | loss: 0.31639 | eval_custom_logloss: 1.11572 |  0:00:16s
epoch 47 | loss: 0.30444 | eval_custom_logloss: 1.15921 |  0:00:16s
epoch 48 | loss: 0.28632 | eval_custom_logloss: 1.19193 |  0:00:17s
epoch 49 | loss: 0.27791 | eval_custom_logloss: 1.19505 |  0:00:17s
epoch 50 | loss: 0.2908  | eval_custom_logloss: 1.27635 |  0:00:17s
epoch 51 | loss: 0.2794  | eval_custom_logloss: 1.57445 |  0:00:18s
epoch 52 | loss: 0.29133 | eval_custom_logloss: 1.31906 |  0:00:18s
epoch 53 | loss: 0.2679  | eval_custom_logloss: 1.51832 |  0:00:18s
epoch 54 | loss: 0.26014 | eval_custom_logloss: 1.034   |  0:00:19s
epoch 55 | loss: 0.2631  | eval_custom_logloss: 0.87492 |  0:00:19s
epoch 56 | loss: 0.24947 | eval_custom_logloss: 1.08688 |  0:00:20s
epoch 57 | loss: 0.25845 | eval_custom_logloss: 1.2443  |  0:00:20s
epoch 58 | loss: 0.23833 | eval_custom_logloss: 1.04308 |  0:00:20s
epoch 59 | loss: 0.23612 | eval_custom_logloss: 0.90699 |  0:00:21s

Early stopping occurred at epoch 59 with best_epoch = 39 and best_eval_custom_logloss = 0.85437
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.98934, 'Log Loss - std': 0.13096858554630575} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 46 finished with value: 0.98934 and parameters: {'n_d': 18, 'n_steps': 3, 'gamma': 1.0013153039866614, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 3, 'momentum': 0.0021825458689294073, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.32448 | eval_custom_logloss: 1.28861 |  0:00:00s
epoch 1  | loss: 1.01059 | eval_custom_logloss: 1.23573 |  0:00:00s
epoch 2  | loss: 0.8855  | eval_custom_logloss: 1.57134 |  0:00:01s
epoch 3  | loss: 0.82205 | eval_custom_logloss: 1.45771 |  0:00:01s
epoch 4  | loss: 0.76442 | eval_custom_logloss: 1.42004 |  0:00:02s
epoch 5  | loss: 0.72841 | eval_custom_logloss: 2.91593 |  0:00:02s
epoch 6  | loss: 0.7464  | eval_custom_logloss: 2.37267 |  0:00:03s
epoch 7  | loss: 0.71533 | eval_custom_logloss: 3.98755 |  0:00:03s
epoch 8  | loss: 0.72305 | eval_custom_logloss: 3.50473 |  0:00:04s
epoch 9  | loss: 0.67816 | eval_custom_logloss: 5.05314 |  0:00:04s
epoch 10 | loss: 0.67825 | eval_custom_logloss: 4.67177 |  0:00:05s
epoch 11 | loss: 0.67072 | eval_custom_logloss: 3.02109 |  0:00:05s
epoch 12 | loss: 0.64507 | eval_custom_logloss: 3.85251 |  0:00:06s
epoch 13 | loss: 0.62354 | eval_custom_logloss: 3.34437 |  0:00:06s
epoch 14 | loss: 0.58512 | eval_custom_logloss: 3.13087 |  0:00:07s
epoch 15 | loss: 0.57432 | eval_custom_logloss: 2.59598 |  0:00:07s
epoch 16 | loss: 0.51357 | eval_custom_logloss: 1.85341 |  0:00:08s
epoch 17 | loss: 0.55582 | eval_custom_logloss: 2.05954 |  0:00:08s
epoch 18 | loss: 0.53891 | eval_custom_logloss: 1.35725 |  0:00:09s
epoch 19 | loss: 0.5608  | eval_custom_logloss: 1.24332 |  0:00:10s
epoch 20 | loss: 0.51483 | eval_custom_logloss: 1.69535 |  0:00:10s
epoch 21 | loss: 0.502   | eval_custom_logloss: 1.5707  |  0:00:11s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 1.23573
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1786, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.21176 | eval_custom_logloss: 1.64363 |  0:00:00s
epoch 1  | loss: 1.04439 | eval_custom_logloss: 1.90219 |  0:00:00s
epoch 2  | loss: 0.85379 | eval_custom_logloss: 1.58811 |  0:00:01s
epoch 3  | loss: 0.83522 | eval_custom_logloss: 0.95461 |  0:00:01s
epoch 4  | loss: 0.80176 | eval_custom_logloss: 1.08318 |  0:00:02s
epoch 5  | loss: 0.76341 | eval_custom_logloss: 1.23252 |  0:00:02s
epoch 6  | loss: 0.74141 | eval_custom_logloss: 1.02794 |  0:00:03s
epoch 7  | loss: 0.72418 | eval_custom_logloss: 1.14127 |  0:00:03s
epoch 8  | loss: 0.71122 | eval_custom_logloss: 1.8373  |  0:00:04s
epoch 9  | loss: 0.68713 | eval_custom_logloss: 2.1367  |  0:00:04s
epoch 10 | loss: 0.66405 | eval_custom_logloss: 1.59845 |  0:00:05s
epoch 11 | loss: 0.64961 | eval_custom_logloss: 2.05369 |  0:00:05s
epoch 12 | loss: 0.63033 | eval_custom_logloss: 1.86559 |  0:00:05s
epoch 13 | loss: 0.62394 | eval_custom_logloss: 2.51332 |  0:00:06s
epoch 14 | loss: 0.61561 | eval_custom_logloss: 2.51023 |  0:00:06s
epoch 15 | loss: 0.59087 | eval_custom_logloss: 2.54625 |  0:00:07s
epoch 16 | loss: 0.55651 | eval_custom_logloss: 3.19585 |  0:00:07s
epoch 17 | loss: 0.54014 | eval_custom_logloss: 3.4408  |  0:00:08s
epoch 18 | loss: 0.54017 | eval_custom_logloss: 2.68207 |  0:00:08s
epoch 19 | loss: 0.53284 | eval_custom_logloss: 2.48383 |  0:00:09s
epoch 20 | loss: 0.54507 | eval_custom_logloss: 2.18594 |  0:00:09s
epoch 21 | loss: 0.50804 | eval_custom_logloss: 1.74906 |  0:00:09s
epoch 22 | loss: 0.49613 | eval_custom_logloss: 1.75528 |  0:00:10s
epoch 23 | loss: 0.541   | eval_custom_logloss: 1.41146 |  0:00:10s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 0.95461
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.0602, 'Log Loss - std': 0.11840000000000006} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.36794 | eval_custom_logloss: 1.27284 |  0:00:00s
epoch 1  | loss: 0.90092 | eval_custom_logloss: 1.2784  |  0:00:00s
epoch 2  | loss: 0.86026 | eval_custom_logloss: 1.45617 |  0:00:01s
epoch 3  | loss: 0.85142 | eval_custom_logloss: 1.72874 |  0:00:01s
epoch 4  | loss: 0.77609 | eval_custom_logloss: 1.87173 |  0:00:02s
epoch 5  | loss: 0.76927 | eval_custom_logloss: 2.10864 |  0:00:02s
epoch 6  | loss: 0.74384 | eval_custom_logloss: 2.57945 |  0:00:03s
epoch 7  | loss: 0.70755 | eval_custom_logloss: 3.20296 |  0:00:03s
epoch 8  | loss: 0.7034  | eval_custom_logloss: 1.78897 |  0:00:04s
epoch 9  | loss: 0.66626 | eval_custom_logloss: 2.51886 |  0:00:04s
epoch 10 | loss: 0.65559 | eval_custom_logloss: 2.87103 |  0:00:04s
epoch 11 | loss: 0.64996 | eval_custom_logloss: 2.41943 |  0:00:05s
epoch 12 | loss: 0.66903 | eval_custom_logloss: 2.94419 |  0:00:05s
epoch 13 | loss: 0.61513 | eval_custom_logloss: 2.27305 |  0:00:06s
epoch 14 | loss: 0.6475  | eval_custom_logloss: 3.05297 |  0:00:06s
epoch 15 | loss: 0.60622 | eval_custom_logloss: 2.59689 |  0:00:07s
epoch 16 | loss: 0.61434 | eval_custom_logloss: 1.34581 |  0:00:07s
epoch 17 | loss: 0.57824 | eval_custom_logloss: 1.83815 |  0:00:08s
epoch 18 | loss: 0.58217 | eval_custom_logloss: 1.79232 |  0:00:08s
epoch 19 | loss: 0.55656 | eval_custom_logloss: 1.38233 |  0:00:09s
epoch 20 | loss: 0.5805  | eval_custom_logloss: 1.53754 |  0:00:09s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.27284
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1182333333333334, 'Log Loss - std': 0.12681262643058153} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.31359 | eval_custom_logloss: 1.71281 |  0:00:00s
epoch 1  | loss: 0.9986  | eval_custom_logloss: 1.33412 |  0:00:00s
epoch 2  | loss: 0.81643 | eval_custom_logloss: 1.39062 |  0:00:01s
epoch 3  | loss: 0.79353 | eval_custom_logloss: 1.22147 |  0:00:01s
epoch 4  | loss: 0.81311 | eval_custom_logloss: 1.15332 |  0:00:02s
epoch 5  | loss: 0.73223 | eval_custom_logloss: 1.48521 |  0:00:02s
epoch 6  | loss: 0.73245 | eval_custom_logloss: 2.58199 |  0:00:03s
epoch 7  | loss: 0.6766  | eval_custom_logloss: 1.89192 |  0:00:03s
epoch 8  | loss: 0.68092 | eval_custom_logloss: 2.66506 |  0:00:04s
epoch 9  | loss: 0.68955 | eval_custom_logloss: 3.28829 |  0:00:04s
epoch 10 | loss: 0.64951 | eval_custom_logloss: 2.28238 |  0:00:05s
epoch 11 | loss: 0.61543 | eval_custom_logloss: 1.30842 |  0:00:05s
epoch 12 | loss: 0.62403 | eval_custom_logloss: 1.37383 |  0:00:05s
epoch 13 | loss: 0.58273 | eval_custom_logloss: 1.87451 |  0:00:06s
epoch 14 | loss: 0.54853 | eval_custom_logloss: 1.49321 |  0:00:06s
epoch 15 | loss: 0.5261  | eval_custom_logloss: 1.50883 |  0:00:07s
epoch 16 | loss: 0.53016 | eval_custom_logloss: 1.53933 |  0:00:07s
epoch 17 | loss: 0.56205 | eval_custom_logloss: 1.34481 |  0:00:08s
epoch 18 | loss: 0.54284 | eval_custom_logloss: 1.58714 |  0:00:08s
epoch 19 | loss: 0.53959 | eval_custom_logloss: 2.09963 |  0:00:09s
epoch 20 | loss: 0.50942 | eval_custom_logloss: 1.85072 |  0:00:09s
epoch 21 | loss: 0.50199 | eval_custom_logloss: 1.73929 |  0:00:09s
epoch 22 | loss: 0.49479 | eval_custom_logloss: 1.68755 |  0:00:10s
epoch 23 | loss: 0.52112 | eval_custom_logloss: 1.65536 |  0:00:10s
epoch 24 | loss: 0.46971 | eval_custom_logloss: 1.64048 |  0:00:11s

Early stopping occurred at epoch 24 with best_epoch = 4 and best_eval_custom_logloss = 1.15332
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1238000000000001, 'Log Loss - std': 0.11024538539095413} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax', 'n_a': 8, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.22054 | eval_custom_logloss: 1.28399 |  0:00:00s
epoch 1  | loss: 0.98664 | eval_custom_logloss: 1.52853 |  0:00:00s
epoch 2  | loss: 0.8957  | eval_custom_logloss: 1.4332  |  0:00:01s
epoch 3  | loss: 0.78555 | eval_custom_logloss: 1.37858 |  0:00:01s
epoch 4  | loss: 0.77856 | eval_custom_logloss: 1.5192  |  0:00:02s
epoch 5  | loss: 0.75867 | eval_custom_logloss: 1.81158 |  0:00:02s
epoch 6  | loss: 0.75332 | eval_custom_logloss: 1.86818 |  0:00:03s
epoch 7  | loss: 0.71126 | eval_custom_logloss: 1.5313  |  0:00:03s
epoch 8  | loss: 0.72197 | eval_custom_logloss: 1.24618 |  0:00:04s
epoch 9  | loss: 0.72902 | eval_custom_logloss: 1.85716 |  0:00:04s
epoch 10 | loss: 0.67038 | eval_custom_logloss: 1.60036 |  0:00:05s
epoch 11 | loss: 0.66014 | eval_custom_logloss: 2.1661  |  0:00:05s
epoch 12 | loss: 0.65865 | eval_custom_logloss: 2.1886  |  0:00:06s
epoch 13 | loss: 0.6363  | eval_custom_logloss: 2.93796 |  0:00:06s
epoch 14 | loss: 0.60513 | eval_custom_logloss: 1.94678 |  0:00:07s
epoch 15 | loss: 0.63369 | eval_custom_logloss: 1.62345 |  0:00:07s
epoch 16 | loss: 0.60385 | eval_custom_logloss: 1.99175 |  0:00:08s
epoch 17 | loss: 0.62163 | eval_custom_logloss: 1.97058 |  0:00:08s
epoch 18 | loss: 0.57469 | eval_custom_logloss: 2.11367 |  0:00:09s
epoch 19 | loss: 0.54251 | eval_custom_logloss: 1.71191 |  0:00:09s
epoch 20 | loss: 0.57236 | eval_custom_logloss: 2.04753 |  0:00:10s
epoch 21 | loss: 0.54617 | eval_custom_logloss: 1.65196 |  0:00:11s
epoch 22 | loss: 0.53016 | eval_custom_logloss: 1.60631 |  0:00:11s
epoch 23 | loss: 0.542   | eval_custom_logloss: 1.57143 |  0:00:12s
epoch 24 | loss: 0.52044 | eval_custom_logloss: 1.72436 |  0:00:12s
epoch 25 | loss: 0.54514 | eval_custom_logloss: 1.59975 |  0:00:13s
epoch 26 | loss: 0.52747 | eval_custom_logloss: 1.82283 |  0:00:13s
epoch 27 | loss: 0.5196  | eval_custom_logloss: 2.3527  |  0:00:14s
epoch 28 | loss: 0.51721 | eval_custom_logloss: 1.4592  |  0:00:14s

Early stopping occurred at epoch 28 with best_epoch = 8 and best_eval_custom_logloss = 1.24618
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.1400000000000001, 'Log Loss - std': 0.10379304408292496} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 47 finished with value: 1.1400000000000001 and parameters: {'n_d': 8, 'n_steps': 4, 'gamma': 1.2184697815573031, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.002692721154566793, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax', 'n_a': 13, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.20383 | eval_custom_logloss: 1.58445 |  0:00:00s
epoch 1  | loss: 0.89041 | eval_custom_logloss: 1.60173 |  0:00:00s
epoch 2  | loss: 0.82094 | eval_custom_logloss: 1.46052 |  0:00:01s
epoch 3  | loss: 0.7894  | eval_custom_logloss: 1.39416 |  0:00:01s
epoch 4  | loss: 0.77315 | eval_custom_logloss: 1.78949 |  0:00:02s
epoch 5  | loss: 0.72102 | eval_custom_logloss: 2.72728 |  0:00:02s
epoch 6  | loss: 0.67428 | eval_custom_logloss: 4.75795 |  0:00:02s
epoch 7  | loss: 0.68062 | eval_custom_logloss: 3.38526 |  0:00:03s
epoch 8  | loss: 0.63871 | eval_custom_logloss: 3.21211 |  0:00:03s
epoch 9  | loss: 0.66298 | eval_custom_logloss: 3.91167 |  0:00:04s
epoch 10 | loss: 0.62588 | eval_custom_logloss: 3.02859 |  0:00:04s
epoch 11 | loss: 0.60166 | eval_custom_logloss: 3.18951 |  0:00:05s
epoch 12 | loss: 0.57017 | eval_custom_logloss: 3.14616 |  0:00:05s
epoch 13 | loss: 0.58063 | eval_custom_logloss: 3.32042 |  0:00:06s
epoch 14 | loss: 0.5603  | eval_custom_logloss: 2.73933 |  0:00:06s
epoch 15 | loss: 0.55866 | eval_custom_logloss: 2.03273 |  0:00:07s
epoch 16 | loss: 0.5417  | eval_custom_logloss: 2.45983 |  0:00:07s
epoch 17 | loss: 0.52007 | eval_custom_logloss: 2.19062 |  0:00:08s
epoch 18 | loss: 0.51272 | eval_custom_logloss: 1.91203 |  0:00:08s
epoch 19 | loss: 0.54112 | eval_custom_logloss: 1.87696 |  0:00:09s
epoch 20 | loss: 0.51645 | eval_custom_logloss: 2.09277 |  0:00:09s
epoch 21 | loss: 0.53032 | eval_custom_logloss: 2.61941 |  0:00:10s
epoch 22 | loss: 0.49243 | eval_custom_logloss: 1.88519 |  0:00:10s
epoch 23 | loss: 0.49694 | eval_custom_logloss: 2.11768 |  0:00:11s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 1.39416
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3373, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax', 'n_a': 13, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09275 | eval_custom_logloss: 1.67908 |  0:00:00s
epoch 1  | loss: 0.88668 | eval_custom_logloss: 2.18064 |  0:00:00s
epoch 2  | loss: 0.85891 | eval_custom_logloss: 2.15617 |  0:00:01s
epoch 3  | loss: 0.7747  | eval_custom_logloss: 3.46635 |  0:00:01s
epoch 4  | loss: 0.69458 | eval_custom_logloss: 3.33921 |  0:00:02s
epoch 5  | loss: 0.66204 | eval_custom_logloss: 3.19823 |  0:00:02s
epoch 6  | loss: 0.65228 | eval_custom_logloss: 2.73028 |  0:00:02s
epoch 7  | loss: 0.61275 | eval_custom_logloss: 2.89866 |  0:00:03s
epoch 8  | loss: 0.58807 | eval_custom_logloss: 3.17325 |  0:00:03s
epoch 9  | loss: 0.55932 | eval_custom_logloss: 2.91227 |  0:00:04s
epoch 10 | loss: 0.56365 | eval_custom_logloss: 2.6553  |  0:00:04s
epoch 11 | loss: 0.52181 | eval_custom_logloss: 3.09516 |  0:00:04s
epoch 12 | loss: 0.51499 | eval_custom_logloss: 2.93827 |  0:00:05s
epoch 13 | loss: 0.50319 | eval_custom_logloss: 3.01982 |  0:00:05s
epoch 14 | loss: 0.50201 | eval_custom_logloss: 2.10659 |  0:00:06s
epoch 15 | loss: 0.49247 | eval_custom_logloss: 2.9123  |  0:00:06s
epoch 16 | loss: 0.47868 | eval_custom_logloss: 3.3862  |  0:00:06s
epoch 17 | loss: 0.45671 | eval_custom_logloss: 3.35155 |  0:00:07s
epoch 18 | loss: 0.44391 | eval_custom_logloss: 3.18267 |  0:00:07s
epoch 19 | loss: 0.44282 | eval_custom_logloss: 3.57466 |  0:00:08s
epoch 20 | loss: 0.44357 | eval_custom_logloss: 3.86733 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 1.67908
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.4815999999999998, 'Log Loss - std': 0.14429999999999998} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax', 'n_a': 13, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.08684 | eval_custom_logloss: 1.64838 |  0:00:00s
epoch 1  | loss: 0.93051 | eval_custom_logloss: 1.81336 |  0:00:00s
epoch 2  | loss: 0.84256 | eval_custom_logloss: 1.77227 |  0:00:01s
epoch 3  | loss: 0.76095 | eval_custom_logloss: 1.37198 |  0:00:01s
epoch 4  | loss: 0.72697 | eval_custom_logloss: 1.91329 |  0:00:01s
epoch 5  | loss: 0.73255 | eval_custom_logloss: 2.69925 |  0:00:02s
epoch 6  | loss: 0.73904 | eval_custom_logloss: 2.56444 |  0:00:02s
epoch 7  | loss: 0.69477 | eval_custom_logloss: 4.44471 |  0:00:02s
epoch 8  | loss: 0.67232 | eval_custom_logloss: 5.50541 |  0:00:03s
epoch 9  | loss: 0.6515  | eval_custom_logloss: 3.32962 |  0:00:03s
epoch 10 | loss: 0.65174 | eval_custom_logloss: 2.25904 |  0:00:03s
epoch 11 | loss: 0.61827 | eval_custom_logloss: 3.24811 |  0:00:04s
epoch 12 | loss: 0.57696 | eval_custom_logloss: 4.27192 |  0:00:04s
epoch 13 | loss: 0.56533 | eval_custom_logloss: 4.42152 |  0:00:05s
epoch 14 | loss: 0.59331 | eval_custom_logloss: 4.04852 |  0:00:05s
epoch 15 | loss: 0.59079 | eval_custom_logloss: 3.93782 |  0:00:05s
epoch 16 | loss: 0.5571  | eval_custom_logloss: 2.78538 |  0:00:06s
epoch 17 | loss: 0.55813 | eval_custom_logloss: 2.65573 |  0:00:06s
epoch 18 | loss: 0.5333  | eval_custom_logloss: 3.05685 |  0:00:06s
epoch 19 | loss: 0.54802 | eval_custom_logloss: 2.90104 |  0:00:07s
epoch 20 | loss: 0.54134 | eval_custom_logloss: 3.07059 |  0:00:07s
epoch 21 | loss: 0.52793 | eval_custom_logloss: 2.92393 |  0:00:07s
epoch 22 | loss: 0.52343 | eval_custom_logloss: 2.55606 |  0:00:08s
epoch 23 | loss: 0.52505 | eval_custom_logloss: 2.39709 |  0:00:08s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 1.37198
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.4422999999999997, 'Log Loss - std': 0.1302714089890794} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax', 'n_a': 13, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.10672 | eval_custom_logloss: 1.74429 |  0:00:00s
epoch 1  | loss: 0.91047 | eval_custom_logloss: 1.24606 |  0:00:00s
epoch 2  | loss: 0.77877 | eval_custom_logloss: 1.71589 |  0:00:01s
epoch 3  | loss: 0.76865 | eval_custom_logloss: 2.258   |  0:00:01s
epoch 4  | loss: 0.73567 | eval_custom_logloss: 1.97371 |  0:00:02s
epoch 5  | loss: 0.68625 | eval_custom_logloss: 1.72131 |  0:00:02s
epoch 6  | loss: 0.66115 | eval_custom_logloss: 2.66    |  0:00:02s
epoch 7  | loss: 0.62049 | eval_custom_logloss: 4.53978 |  0:00:03s
epoch 8  | loss: 0.59549 | eval_custom_logloss: 4.93322 |  0:00:03s
epoch 9  | loss: 0.6128  | eval_custom_logloss: 4.35783 |  0:00:04s
epoch 10 | loss: 0.60524 | eval_custom_logloss: 4.33296 |  0:00:04s
epoch 11 | loss: 0.58969 | eval_custom_logloss: 5.36623 |  0:00:04s
epoch 12 | loss: 0.57942 | eval_custom_logloss: 6.51095 |  0:00:05s
epoch 13 | loss: 0.58759 | eval_custom_logloss: 5.00536 |  0:00:05s
epoch 14 | loss: 0.55913 | eval_custom_logloss: 6.39893 |  0:00:06s
epoch 15 | loss: 0.54644 | eval_custom_logloss: 4.8503  |  0:00:06s
epoch 16 | loss: 0.53388 | eval_custom_logloss: 3.88438 |  0:00:07s
epoch 17 | loss: 0.50837 | eval_custom_logloss: 4.50136 |  0:00:07s
epoch 18 | loss: 0.48363 | eval_custom_logloss: 4.06187 |  0:00:08s
epoch 19 | loss: 0.50971 | eval_custom_logloss: 3.60869 |  0:00:08s
epoch 20 | loss: 0.48466 | eval_custom_logloss: 2.73638 |  0:00:09s
epoch 21 | loss: 0.47164 | eval_custom_logloss: 2.78578 |  0:00:09s

Early stopping occurred at epoch 21 with best_epoch = 1 and best_eval_custom_logloss = 1.24606
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3900249999999998, 'Log Loss - std': 0.14465824164215457} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax', 'n_a': 13, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09572 | eval_custom_logloss: 1.42362 |  0:00:00s
epoch 1  | loss: 0.97345 | eval_custom_logloss: 1.32239 |  0:00:00s
epoch 2  | loss: 0.88723 | eval_custom_logloss: 1.42137 |  0:00:01s
epoch 3  | loss: 0.78834 | eval_custom_logloss: 1.30925 |  0:00:01s
epoch 4  | loss: 0.7294  | eval_custom_logloss: 2.21026 |  0:00:02s
epoch 5  | loss: 0.72659 | eval_custom_logloss: 2.5661  |  0:00:02s
epoch 6  | loss: 0.70084 | eval_custom_logloss: 3.37252 |  0:00:02s
epoch 7  | loss: 0.68526 | eval_custom_logloss: 4.05055 |  0:00:03s
epoch 8  | loss: 0.66598 | eval_custom_logloss: 4.85599 |  0:00:03s
epoch 9  | loss: 0.63811 | eval_custom_logloss: 5.81139 |  0:00:04s
epoch 10 | loss: 0.61168 | eval_custom_logloss: 5.32861 |  0:00:04s
epoch 11 | loss: 0.57978 | eval_custom_logloss: 4.68735 |  0:00:05s
epoch 12 | loss: 0.55455 | eval_custom_logloss: 5.76432 |  0:00:05s
epoch 13 | loss: 0.55511 | eval_custom_logloss: 4.9236  |  0:00:06s
epoch 14 | loss: 0.56385 | eval_custom_logloss: 4.77748 |  0:00:06s
epoch 15 | loss: 0.53789 | eval_custom_logloss: 4.54164 |  0:00:07s
epoch 16 | loss: 0.5099  | eval_custom_logloss: 5.16984 |  0:00:07s
epoch 17 | loss: 0.53335 | eval_custom_logloss: 3.92571 |  0:00:08s
epoch 18 | loss: 0.4985  | eval_custom_logloss: 4.25274 |  0:00:08s
epoch 19 | loss: 0.47282 | eval_custom_logloss: 4.7641  |  0:00:08s
epoch 20 | loss: 0.50239 | eval_custom_logloss: 5.27146 |  0:00:09s
epoch 21 | loss: 0.50293 | eval_custom_logloss: 5.55604 |  0:00:09s
epoch 22 | loss: 0.496   | eval_custom_logloss: 4.86197 |  0:00:10s
epoch 23 | loss: 0.47487 | eval_custom_logloss: 4.09441 |  0:00:10s

Early stopping occurred at epoch 23 with best_epoch = 3 and best_eval_custom_logloss = 1.30925
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.3677599999999999, 'Log Loss - std': 0.13683466812178846} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 48 finished with value: 1.3677599999999999 and parameters: {'n_d': 13, 'n_steps': 5, 'gamma': 1.0476666342188001, 'cat_emb_dim': 3, 'n_independent': 2, 'n_shared': 4, 'momentum': 0.0010004054935576477, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.32293 | eval_custom_logloss: 8.01722 |  0:00:00s
epoch 1  | loss: 1.14816 | eval_custom_logloss: 7.19121 |  0:00:01s
epoch 2  | loss: 0.92092 | eval_custom_logloss: 8.5746  |  0:00:01s
epoch 3  | loss: 0.9072  | eval_custom_logloss: 7.99195 |  0:00:02s
epoch 4  | loss: 0.86171 | eval_custom_logloss: 9.52649 |  0:00:03s
epoch 5  | loss: 1.087   | eval_custom_logloss: 9.04723 |  0:00:03s
epoch 6  | loss: 0.79291 | eval_custom_logloss: 9.48515 |  0:00:04s
epoch 7  | loss: 0.76289 | eval_custom_logloss: 8.32664 |  0:00:05s
epoch 8  | loss: 0.69352 | eval_custom_logloss: 8.32292 |  0:00:05s
epoch 9  | loss: 0.68831 | eval_custom_logloss: 6.79952 |  0:00:06s
epoch 10 | loss: 0.68939 | eval_custom_logloss: 7.87544 |  0:00:06s
epoch 11 | loss: 0.65419 | eval_custom_logloss: 7.78155 |  0:00:07s
epoch 12 | loss: 0.70598 | eval_custom_logloss: 7.31322 |  0:00:08s
epoch 13 | loss: 0.63879 | eval_custom_logloss: 6.8247  |  0:00:08s
epoch 14 | loss: 0.65658 | eval_custom_logloss: 7.95581 |  0:00:09s
epoch 15 | loss: 0.63705 | eval_custom_logloss: 7.48207 |  0:00:10s
epoch 16 | loss: 0.58926 | eval_custom_logloss: 4.35717 |  0:00:10s
epoch 17 | loss: 0.58368 | eval_custom_logloss: 8.54451 |  0:00:11s
epoch 18 | loss: 0.558   | eval_custom_logloss: 7.12274 |  0:00:11s
epoch 19 | loss: 0.53085 | eval_custom_logloss: 6.36364 |  0:00:12s
epoch 20 | loss: 0.53021 | eval_custom_logloss: 5.92332 |  0:00:13s
epoch 21 | loss: 0.53587 | eval_custom_logloss: 3.124   |  0:00:13s
epoch 22 | loss: 0.51736 | eval_custom_logloss: 5.9237  |  0:00:14s
epoch 23 | loss: 0.50819 | eval_custom_logloss: 5.62986 |  0:00:15s
epoch 24 | loss: 0.54042 | eval_custom_logloss: 3.47811 |  0:00:15s
epoch 25 | loss: 0.56651 | eval_custom_logloss: 6.6437  |  0:00:16s
epoch 26 | loss: 0.536   | eval_custom_logloss: 4.82024 |  0:00:17s
epoch 27 | loss: 0.49674 | eval_custom_logloss: 4.36138 |  0:00:17s
epoch 28 | loss: 0.49293 | eval_custom_logloss: 5.55195 |  0:00:18s
epoch 29 | loss: 0.49038 | eval_custom_logloss: 5.95941 |  0:00:18s
epoch 30 | loss: 0.53751 | eval_custom_logloss: 6.06077 |  0:00:19s
epoch 31 | loss: 0.49063 | eval_custom_logloss: 4.585   |  0:00:20s
epoch 32 | loss: 0.49535 | eval_custom_logloss: 4.54604 |  0:00:20s
epoch 33 | loss: 0.46832 | eval_custom_logloss: 5.1834  |  0:00:21s
epoch 34 | loss: 0.48462 | eval_custom_logloss: 4.95771 |  0:00:22s
epoch 35 | loss: 0.45307 | eval_custom_logloss: 5.23086 |  0:00:22s
epoch 36 | loss: 0.4567  | eval_custom_logloss: 6.0101  |  0:00:23s
epoch 37 | loss: 0.44542 | eval_custom_logloss: 5.15023 |  0:00:23s
epoch 38 | loss: 0.4587  | eval_custom_logloss: 5.01216 |  0:00:24s
epoch 39 | loss: 0.46968 | eval_custom_logloss: 4.18682 |  0:00:25s
epoch 40 | loss: 0.45859 | eval_custom_logloss: 4.55262 |  0:00:25s
epoch 41 | loss: 0.45683 | eval_custom_logloss: 3.61134 |  0:00:26s

Early stopping occurred at epoch 41 with best_epoch = 21 and best_eval_custom_logloss = 3.124
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.5622, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.30609 | eval_custom_logloss: 8.94956 |  0:00:00s
epoch 1  | loss: 1.2754  | eval_custom_logloss: 9.02599 |  0:00:01s
epoch 2  | loss: 1.09975 | eval_custom_logloss: 8.36805 |  0:00:01s
epoch 3  | loss: 0.97436 | eval_custom_logloss: 7.29614 |  0:00:02s
epoch 4  | loss: 0.87281 | eval_custom_logloss: 7.75675 |  0:00:03s
epoch 5  | loss: 0.82876 | eval_custom_logloss: 8.32756 |  0:00:03s
epoch 6  | loss: 0.74044 | eval_custom_logloss: 7.6338  |  0:00:04s
epoch 7  | loss: 0.70207 | eval_custom_logloss: 8.76658 |  0:00:05s
epoch 8  | loss: 0.73526 | eval_custom_logloss: 6.98189 |  0:00:05s
epoch 9  | loss: 0.6829  | eval_custom_logloss: 7.10833 |  0:00:06s
epoch 10 | loss: 0.65614 | eval_custom_logloss: 7.02518 |  0:00:06s
epoch 11 | loss: 0.70556 | eval_custom_logloss: 8.20048 |  0:00:07s
epoch 12 | loss: 0.67494 | eval_custom_logloss: 7.25102 |  0:00:08s
epoch 13 | loss: 0.63365 | eval_custom_logloss: 7.69508 |  0:00:08s
epoch 14 | loss: 0.69958 | eval_custom_logloss: 9.37991 |  0:00:09s
epoch 15 | loss: 0.61056 | eval_custom_logloss: 7.46342 |  0:00:10s
epoch 16 | loss: 0.60881 | eval_custom_logloss: 7.73287 |  0:00:10s
epoch 17 | loss: 0.57597 | eval_custom_logloss: 6.23674 |  0:00:11s
epoch 18 | loss: 0.5754  | eval_custom_logloss: 6.28032 |  0:00:11s
epoch 19 | loss: 0.58436 | eval_custom_logloss: 6.906   |  0:00:12s
epoch 20 | loss: 0.53704 | eval_custom_logloss: 6.63829 |  0:00:13s
epoch 21 | loss: 0.51701 | eval_custom_logloss: 7.42875 |  0:00:13s
epoch 22 | loss: 0.54377 | eval_custom_logloss: 5.48883 |  0:00:14s
epoch 23 | loss: 0.55436 | eval_custom_logloss: 5.3161  |  0:00:14s
epoch 24 | loss: 0.54229 | eval_custom_logloss: 6.37567 |  0:00:15s
epoch 25 | loss: 0.50153 | eval_custom_logloss: 5.11875 |  0:00:16s
epoch 26 | loss: 0.48966 | eval_custom_logloss: 5.1997  |  0:00:16s
epoch 27 | loss: 0.48973 | eval_custom_logloss: 4.80147 |  0:00:17s
epoch 28 | loss: 0.46985 | eval_custom_logloss: 5.54887 |  0:00:18s
epoch 29 | loss: 0.48536 | eval_custom_logloss: 6.36345 |  0:00:18s
epoch 30 | loss: 0.47858 | eval_custom_logloss: 5.7079  |  0:00:19s
epoch 31 | loss: 0.46843 | eval_custom_logloss: 4.54079 |  0:00:19s
epoch 32 | loss: 0.46818 | eval_custom_logloss: 4.26058 |  0:00:20s
epoch 33 | loss: 0.45462 | eval_custom_logloss: 5.38239 |  0:00:21s
epoch 34 | loss: 0.47527 | eval_custom_logloss: 4.5111  |  0:00:21s
epoch 35 | loss: 0.46915 | eval_custom_logloss: 4.61049 |  0:00:22s
epoch 36 | loss: 0.45318 | eval_custom_logloss: 4.6901  |  0:00:22s
epoch 37 | loss: 0.43788 | eval_custom_logloss: 5.11739 |  0:00:23s
epoch 38 | loss: 0.45894 | eval_custom_logloss: 4.8371  |  0:00:24s
epoch 39 | loss: 0.45772 | eval_custom_logloss: 4.37852 |  0:00:24s
epoch 40 | loss: 0.48142 | eval_custom_logloss: 4.0799  |  0:00:25s
epoch 41 | loss: 0.48224 | eval_custom_logloss: 4.3293  |  0:00:25s
epoch 42 | loss: 0.4732  | eval_custom_logloss: 4.67967 |  0:00:26s
epoch 43 | loss: 0.45506 | eval_custom_logloss: 4.39353 |  0:00:27s
epoch 44 | loss: 0.45189 | eval_custom_logloss: 3.69976 |  0:00:27s
epoch 45 | loss: 0.48145 | eval_custom_logloss: 3.55027 |  0:00:28s
epoch 46 | loss: 0.48021 | eval_custom_logloss: 4.35172 |  0:00:29s
epoch 47 | loss: 0.4746  | eval_custom_logloss: 4.47708 |  0:00:29s
epoch 48 | loss: 0.45786 | eval_custom_logloss: 3.44489 |  0:00:30s
epoch 49 | loss: 0.43918 | eval_custom_logloss: 4.02482 |  0:00:30s
epoch 50 | loss: 0.44656 | eval_custom_logloss: 3.78777 |  0:00:31s
epoch 51 | loss: 0.44315 | eval_custom_logloss: 4.76345 |  0:00:32s
epoch 52 | loss: 0.45323 | eval_custom_logloss: 4.14504 |  0:00:32s
epoch 53 | loss: 0.45126 | eval_custom_logloss: 3.13694 |  0:00:33s
epoch 54 | loss: 0.46636 | eval_custom_logloss: 3.3817  |  0:00:33s
epoch 55 | loss: 0.44902 | eval_custom_logloss: 2.96014 |  0:00:34s
epoch 56 | loss: 0.46637 | eval_custom_logloss: 4.01946 |  0:00:35s
epoch 57 | loss: 0.46213 | eval_custom_logloss: 2.68881 |  0:00:35s
epoch 58 | loss: 0.44707 | eval_custom_logloss: 3.6946  |  0:00:36s
epoch 59 | loss: 0.44349 | eval_custom_logloss: 4.2254  |  0:00:37s
epoch 60 | loss: 0.43956 | eval_custom_logloss: 4.14057 |  0:00:37s
epoch 61 | loss: 0.4128  | eval_custom_logloss: 3.65842 |  0:00:38s
epoch 62 | loss: 0.43119 | eval_custom_logloss: 3.25517 |  0:00:38s
epoch 63 | loss: 0.41392 | eval_custom_logloss: 2.92619 |  0:00:39s
epoch 64 | loss: 0.40414 | eval_custom_logloss: 3.61703 |  0:00:40s
epoch 65 | loss: 0.41979 | eval_custom_logloss: 3.33528 |  0:00:40s
epoch 66 | loss: 0.44357 | eval_custom_logloss: 2.32012 |  0:00:41s
epoch 67 | loss: 0.45794 | eval_custom_logloss: 1.68361 |  0:00:41s
epoch 68 | loss: 0.45764 | eval_custom_logloss: 2.07738 |  0:00:42s
epoch 69 | loss: 0.48509 | eval_custom_logloss: 2.38079 |  0:00:43s
epoch 70 | loss: 0.49067 | eval_custom_logloss: 2.30339 |  0:00:43s
epoch 71 | loss: 0.44504 | eval_custom_logloss: 2.87201 |  0:00:44s
epoch 72 | loss: 0.42238 | eval_custom_logloss: 3.44312 |  0:00:44s
epoch 73 | loss: 0.41972 | eval_custom_logloss: 2.99074 |  0:00:45s
epoch 74 | loss: 0.42821 | eval_custom_logloss: 2.98265 |  0:00:46s
epoch 75 | loss: 0.40016 | eval_custom_logloss: 3.28388 |  0:00:46s
epoch 76 | loss: 0.37756 | eval_custom_logloss: 2.48388 |  0:00:47s
epoch 77 | loss: 0.38031 | eval_custom_logloss: 2.28115 |  0:00:48s
epoch 78 | loss: 0.38578 | eval_custom_logloss: 2.7666  |  0:00:48s
epoch 79 | loss: 0.40977 | eval_custom_logloss: 3.05249 |  0:00:49s
epoch 80 | loss: 0.38574 | eval_custom_logloss: 3.30262 |  0:00:49s
epoch 81 | loss: 0.36807 | eval_custom_logloss: 3.54674 |  0:00:50s
epoch 82 | loss: 0.32709 | eval_custom_logloss: 4.286   |  0:00:51s
epoch 83 | loss: 0.30876 | eval_custom_logloss: 4.60179 |  0:00:51s
epoch 84 | loss: 0.29874 | eval_custom_logloss: 4.87958 |  0:00:52s
epoch 85 | loss: 0.29587 | eval_custom_logloss: 4.49149 |  0:00:52s
epoch 86 | loss: 0.30614 | eval_custom_logloss: 4.90763 |  0:00:53s
epoch 87 | loss: 0.32986 | eval_custom_logloss: 3.36361 |  0:00:54s

Early stopping occurred at epoch 87 with best_epoch = 67 and best_eval_custom_logloss = 1.68361
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.04875, 'Log Loss - std': 0.5134499999999999} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.36617 | eval_custom_logloss: 8.01213 |  0:00:00s
epoch 1  | loss: 1.10289 | eval_custom_logloss: 8.20107 |  0:00:01s
epoch 2  | loss: 1.00774 | eval_custom_logloss: 8.30306 |  0:00:01s
epoch 3  | loss: 0.93777 | eval_custom_logloss: 8.9185  |  0:00:02s
epoch 4  | loss: 0.87501 | eval_custom_logloss: 8.81617 |  0:00:03s
epoch 5  | loss: 0.78092 | eval_custom_logloss: 7.92652 |  0:00:03s
epoch 6  | loss: 0.72739 | eval_custom_logloss: 8.20966 |  0:00:04s
epoch 7  | loss: 0.72176 | eval_custom_logloss: 9.12976 |  0:00:05s
epoch 8  | loss: 0.69228 | eval_custom_logloss: 7.23693 |  0:00:05s
epoch 9  | loss: 0.63719 | eval_custom_logloss: 6.93445 |  0:00:06s
epoch 10 | loss: 0.65241 | eval_custom_logloss: 6.12524 |  0:00:07s
epoch 11 | loss: 0.60589 | eval_custom_logloss: 6.0312  |  0:00:07s
epoch 12 | loss: 0.61103 | eval_custom_logloss: 7.25147 |  0:00:08s
epoch 13 | loss: 0.58109 | eval_custom_logloss: 6.39226 |  0:00:09s
epoch 14 | loss: 0.55433 | eval_custom_logloss: 5.28329 |  0:00:09s
epoch 15 | loss: 0.5574  | eval_custom_logloss: 5.41434 |  0:00:10s
epoch 16 | loss: 0.55964 | eval_custom_logloss: 5.63204 |  0:00:11s
epoch 17 | loss: 0.51476 | eval_custom_logloss: 5.92414 |  0:00:11s
epoch 18 | loss: 0.54666 | eval_custom_logloss: 6.69917 |  0:00:12s
epoch 19 | loss: 0.52448 | eval_custom_logloss: 4.25492 |  0:00:13s
epoch 20 | loss: 0.57659 | eval_custom_logloss: 4.95314 |  0:00:13s
epoch 21 | loss: 0.55811 | eval_custom_logloss: 5.27703 |  0:00:14s
epoch 22 | loss: 0.59382 | eval_custom_logloss: 5.94619 |  0:00:14s
epoch 23 | loss: 0.55469 | eval_custom_logloss: 6.25036 |  0:00:15s
epoch 24 | loss: 0.56878 | eval_custom_logloss: 5.59814 |  0:00:16s
epoch 25 | loss: 0.59746 | eval_custom_logloss: 6.27285 |  0:00:16s
epoch 26 | loss: 0.5803  | eval_custom_logloss: 5.68877 |  0:00:17s
epoch 27 | loss: 0.59987 | eval_custom_logloss: 4.03743 |  0:00:18s
epoch 28 | loss: 0.6054  | eval_custom_logloss: 4.02585 |  0:00:18s
epoch 29 | loss: 0.5531  | eval_custom_logloss: 5.56724 |  0:00:19s
epoch 30 | loss: 0.56509 | eval_custom_logloss: 5.36006 |  0:00:20s
epoch 31 | loss: 0.51809 | eval_custom_logloss: 4.76628 |  0:00:20s
epoch 32 | loss: 0.52537 | eval_custom_logloss: 4.77676 |  0:00:21s
epoch 33 | loss: 0.51342 | eval_custom_logloss: 4.95578 |  0:00:21s
epoch 34 | loss: 0.51094 | eval_custom_logloss: 5.19999 |  0:00:22s
epoch 35 | loss: 0.52997 | eval_custom_logloss: 5.15893 |  0:00:23s
epoch 36 | loss: 0.52335 | eval_custom_logloss: 3.76759 |  0:00:23s
epoch 37 | loss: 0.50753 | eval_custom_logloss: 4.75225 |  0:00:24s
epoch 38 | loss: 0.51258 | eval_custom_logloss: 4.87034 |  0:00:25s
epoch 39 | loss: 0.50185 | eval_custom_logloss: 4.56452 |  0:00:25s
epoch 40 | loss: 0.50601 | eval_custom_logloss: 4.72109 |  0:00:26s
epoch 41 | loss: 0.51423 | eval_custom_logloss: 4.61199 |  0:00:27s
epoch 42 | loss: 0.5128  | eval_custom_logloss: 5.38396 |  0:00:27s
epoch 43 | loss: 0.48774 | eval_custom_logloss: 5.58722 |  0:00:28s
epoch 44 | loss: 0.48132 | eval_custom_logloss: 5.20529 |  0:00:28s
epoch 45 | loss: 0.47214 | eval_custom_logloss: 4.44982 |  0:00:29s
epoch 46 | loss: 0.45891 | eval_custom_logloss: 4.70334 |  0:00:30s
epoch 47 | loss: 0.449   | eval_custom_logloss: 4.15712 |  0:00:30s
epoch 48 | loss: 0.46368 | eval_custom_logloss: 3.47457 |  0:00:31s
epoch 49 | loss: 0.4942  | eval_custom_logloss: 2.72949 |  0:00:32s
epoch 50 | loss: 0.4589  | eval_custom_logloss: 2.08255 |  0:00:32s
epoch 51 | loss: 0.43947 | eval_custom_logloss: 2.45423 |  0:00:33s
epoch 52 | loss: 0.41574 | eval_custom_logloss: 3.27585 |  0:00:34s
epoch 53 | loss: 0.42689 | eval_custom_logloss: 3.45237 |  0:00:34s
epoch 54 | loss: 0.43081 | eval_custom_logloss: 3.83704 |  0:00:35s
epoch 55 | loss: 0.41439 | eval_custom_logloss: 4.70596 |  0:00:35s
epoch 56 | loss: 0.42759 | eval_custom_logloss: 4.86596 |  0:00:36s
epoch 57 | loss: 0.45536 | eval_custom_logloss: 3.95273 |  0:00:37s
epoch 58 | loss: 0.42176 | eval_custom_logloss: 3.60625 |  0:00:37s
epoch 59 | loss: 0.40374 | eval_custom_logloss: 4.57484 |  0:00:38s
epoch 60 | loss: 0.39472 | eval_custom_logloss: 4.99557 |  0:00:39s
epoch 61 | loss: 0.38337 | eval_custom_logloss: 4.57739 |  0:00:39s
epoch 62 | loss: 0.3855  | eval_custom_logloss: 4.21247 |  0:00:40s
epoch 63 | loss: 0.36703 | eval_custom_logloss: 4.35983 |  0:00:41s
epoch 64 | loss: 0.35568 | eval_custom_logloss: 4.15369 |  0:00:41s
epoch 65 | loss: 0.35976 | eval_custom_logloss: 3.7041  |  0:00:42s
epoch 66 | loss: 0.3512  | eval_custom_logloss: 3.41127 |  0:00:43s
epoch 67 | loss: 0.33026 | eval_custom_logloss: 3.42213 |  0:00:43s
epoch 68 | loss: 0.31878 | eval_custom_logloss: 3.08449 |  0:00:44s
epoch 69 | loss: 0.33504 | eval_custom_logloss: 3.52317 |  0:00:45s
epoch 70 | loss: 0.3222  | eval_custom_logloss: 3.56521 |  0:00:45s

Early stopping occurred at epoch 70 with best_epoch = 50 and best_eval_custom_logloss = 2.08255
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.9992333333333334, 'Log Loss - std': 0.42503851067350995} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.39172 | eval_custom_logloss: 8.70525 |  0:00:00s
epoch 1  | loss: 1.12239 | eval_custom_logloss: 7.78863 |  0:00:01s
epoch 2  | loss: 0.95052 | eval_custom_logloss: 9.00287 |  0:00:01s
epoch 3  | loss: 0.84242 | eval_custom_logloss: 8.24023 |  0:00:02s
epoch 4  | loss: 0.86212 | eval_custom_logloss: 8.68964 |  0:00:03s
epoch 5  | loss: 0.81094 | eval_custom_logloss: 8.76273 |  0:00:03s
epoch 6  | loss: 0.69334 | eval_custom_logloss: 7.52133 |  0:00:04s
epoch 7  | loss: 0.63212 | eval_custom_logloss: 6.95035 |  0:00:05s
epoch 8  | loss: 0.71302 | eval_custom_logloss: 8.22061 |  0:00:05s
epoch 9  | loss: 0.61056 | eval_custom_logloss: 8.13944 |  0:00:06s
epoch 10 | loss: 0.59675 | eval_custom_logloss: 7.62908 |  0:00:07s
epoch 11 | loss: 0.63451 | eval_custom_logloss: 5.95915 |  0:00:07s
epoch 12 | loss: 0.60877 | eval_custom_logloss: 5.78127 |  0:00:08s
epoch 13 | loss: 0.5942  | eval_custom_logloss: 6.99282 |  0:00:09s
epoch 14 | loss: 0.58866 | eval_custom_logloss: 7.20442 |  0:00:09s
epoch 15 | loss: 0.59398 | eval_custom_logloss: 6.37698 |  0:00:10s
epoch 16 | loss: 0.5803  | eval_custom_logloss: 6.75697 |  0:00:10s
epoch 17 | loss: 0.56057 | eval_custom_logloss: 6.60502 |  0:00:11s
epoch 18 | loss: 0.57806 | eval_custom_logloss: 6.78052 |  0:00:12s
epoch 19 | loss: 0.55173 | eval_custom_logloss: 6.72835 |  0:00:12s
epoch 20 | loss: 0.54792 | eval_custom_logloss: 5.62869 |  0:00:13s
epoch 21 | loss: 0.52251 | eval_custom_logloss: 5.77341 |  0:00:14s
epoch 22 | loss: 0.55705 | eval_custom_logloss: 6.27779 |  0:00:14s
epoch 23 | loss: 0.51477 | eval_custom_logloss: 5.49249 |  0:00:15s
epoch 24 | loss: 0.53134 | eval_custom_logloss: 4.1187  |  0:00:15s
epoch 25 | loss: 0.51414 | eval_custom_logloss: 4.02974 |  0:00:16s
epoch 26 | loss: 0.53576 | eval_custom_logloss: 3.76603 |  0:00:17s
epoch 27 | loss: 0.5412  | eval_custom_logloss: 4.26856 |  0:00:17s
epoch 28 | loss: 0.51158 | eval_custom_logloss: 3.4115  |  0:00:18s
epoch 29 | loss: 0.5106  | eval_custom_logloss: 3.64005 |  0:00:19s
epoch 30 | loss: 0.50222 | eval_custom_logloss: 4.3235  |  0:00:19s
epoch 31 | loss: 0.48544 | eval_custom_logloss: 3.21442 |  0:00:20s
epoch 32 | loss: 0.51925 | eval_custom_logloss: 2.73782 |  0:00:21s
epoch 33 | loss: 0.4952  | eval_custom_logloss: 3.38309 |  0:00:21s
epoch 34 | loss: 0.50267 | eval_custom_logloss: 3.69066 |  0:00:22s
epoch 35 | loss: 0.51282 | eval_custom_logloss: 3.96842 |  0:00:22s
epoch 36 | loss: 0.47861 | eval_custom_logloss: 3.52878 |  0:00:23s
epoch 37 | loss: 0.47484 | eval_custom_logloss: 4.34469 |  0:00:24s
epoch 38 | loss: 0.48032 | eval_custom_logloss: 4.04218 |  0:00:24s
epoch 39 | loss: 0.49486 | eval_custom_logloss: 3.63382 |  0:00:25s
epoch 40 | loss: 0.49092 | eval_custom_logloss: 3.31759 |  0:00:25s
epoch 41 | loss: 0.47495 | eval_custom_logloss: 3.71569 |  0:00:26s
epoch 42 | loss: 0.48307 | eval_custom_logloss: 5.14895 |  0:00:27s
epoch 43 | loss: 0.45539 | eval_custom_logloss: 3.95572 |  0:00:27s
epoch 44 | loss: 0.48612 | eval_custom_logloss: 3.10169 |  0:00:28s
epoch 45 | loss: 0.4646  | eval_custom_logloss: 3.53082 |  0:00:29s
epoch 46 | loss: 0.46582 | eval_custom_logloss: 3.47602 |  0:00:29s
epoch 47 | loss: 0.4443  | eval_custom_logloss: 3.6877  |  0:00:30s
epoch 48 | loss: 0.43849 | eval_custom_logloss: 3.02579 |  0:00:30s
epoch 49 | loss: 0.44579 | eval_custom_logloss: 2.80233 |  0:00:31s
epoch 50 | loss: 0.44648 | eval_custom_logloss: 2.77799 |  0:00:32s
epoch 51 | loss: 0.43574 | eval_custom_logloss: 2.51445 |  0:00:32s
epoch 52 | loss: 0.40694 | eval_custom_logloss: 2.3718  |  0:00:33s
epoch 53 | loss: 0.42194 | eval_custom_logloss: 2.1809  |  0:00:34s
epoch 54 | loss: 0.42572 | eval_custom_logloss: 2.40682 |  0:00:34s
epoch 55 | loss: 0.39124 | eval_custom_logloss: 2.32586 |  0:00:35s
epoch 56 | loss: 0.40767 | eval_custom_logloss: 2.02827 |  0:00:35s
epoch 57 | loss: 0.46771 | eval_custom_logloss: 2.17514 |  0:00:36s
epoch 58 | loss: 0.51049 | eval_custom_logloss: 2.4321  |  0:00:37s
epoch 59 | loss: 0.51636 | eval_custom_logloss: 2.64952 |  0:00:37s
epoch 60 | loss: 0.47942 | eval_custom_logloss: 2.54084 |  0:00:38s
epoch 61 | loss: 0.46339 | eval_custom_logloss: 2.4544  |  0:00:39s
epoch 62 | loss: 0.47319 | eval_custom_logloss: 2.68944 |  0:00:39s
epoch 63 | loss: 0.50353 | eval_custom_logloss: 2.38564 |  0:00:40s
epoch 64 | loss: 0.49111 | eval_custom_logloss: 1.97404 |  0:00:40s
epoch 65 | loss: 0.48988 | eval_custom_logloss: 1.75581 |  0:00:41s
epoch 66 | loss: 0.47528 | eval_custom_logloss: 1.66958 |  0:00:42s
epoch 67 | loss: 0.46035 | eval_custom_logloss: 1.95592 |  0:00:42s
epoch 68 | loss: 0.47653 | eval_custom_logloss: 1.65186 |  0:00:43s
epoch 69 | loss: 0.44833 | eval_custom_logloss: 1.74369 |  0:00:44s
epoch 70 | loss: 0.42532 | eval_custom_logloss: 1.74649 |  0:00:44s
epoch 71 | loss: 0.43395 | eval_custom_logloss: 1.95368 |  0:00:45s
epoch 72 | loss: 0.41869 | eval_custom_logloss: 1.45507 |  0:00:45s
epoch 73 | loss: 0.41274 | eval_custom_logloss: 1.77037 |  0:00:46s
epoch 74 | loss: 0.42643 | eval_custom_logloss: 1.85964 |  0:00:47s
epoch 75 | loss: 0.40961 | eval_custom_logloss: 1.54768 |  0:00:47s
epoch 76 | loss: 0.41371 | eval_custom_logloss: 1.68441 |  0:00:48s
epoch 77 | loss: 0.41129 | eval_custom_logloss: 1.62239 |  0:00:49s
epoch 78 | loss: 0.37426 | eval_custom_logloss: 1.42134 |  0:00:49s
epoch 79 | loss: 0.3867  | eval_custom_logloss: 1.57998 |  0:00:50s
epoch 80 | loss: 0.37217 | eval_custom_logloss: 1.87006 |  0:00:50s
epoch 81 | loss: 0.35875 | eval_custom_logloss: 2.48952 |  0:00:51s
epoch 82 | loss: 0.34766 | eval_custom_logloss: 2.39119 |  0:00:52s
epoch 83 | loss: 0.36899 | eval_custom_logloss: 3.14301 |  0:00:52s
epoch 84 | loss: 0.36733 | eval_custom_logloss: 3.39442 |  0:00:53s
epoch 85 | loss: 0.36683 | eval_custom_logloss: 1.87936 |  0:00:53s
epoch 86 | loss: 0.36077 | eval_custom_logloss: 2.23336 |  0:00:54s
epoch 87 | loss: 0.37226 | eval_custom_logloss: 2.08516 |  0:00:55s
epoch 88 | loss: 0.32815 | eval_custom_logloss: 1.94476 |  0:00:55s
epoch 89 | loss: 0.32453 | eval_custom_logloss: 1.66701 |  0:00:56s
epoch 90 | loss: 0.31021 | eval_custom_logloss: 1.86013 |  0:00:57s
epoch 91 | loss: 0.34252 | eval_custom_logloss: 1.81232 |  0:00:57s
epoch 92 | loss: 0.3006  | eval_custom_logloss: 2.12379 |  0:00:58s
epoch 93 | loss: 0.32629 | eval_custom_logloss: 2.24347 |  0:00:58s
epoch 94 | loss: 0.34201 | eval_custom_logloss: 1.99955 |  0:00:59s
epoch 95 | loss: 0.28725 | eval_custom_logloss: 1.89178 |  0:01:00s
epoch 96 | loss: 0.2824  | eval_custom_logloss: 2.03783 |  0:01:00s
epoch 97 | loss: 0.30862 | eval_custom_logloss: 2.38954 |  0:01:01s
epoch 98 | loss: 0.31996 | eval_custom_logloss: 2.13101 |  0:01:02s

Early stopping occurred at epoch 98 with best_epoch = 78 and best_eval_custom_logloss = 1.42134
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.83555, 'Log Loss - std': 0.4646181254535814} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax', 'n_a': 38, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.42203 | eval_custom_logloss: 9.39875 |  0:00:00s
epoch 1  | loss: 1.29442 | eval_custom_logloss: 8.28243 |  0:00:01s
epoch 2  | loss: 1.04085 | eval_custom_logloss: 8.78405 |  0:00:01s
epoch 3  | loss: 1.08234 | eval_custom_logloss: 7.74775 |  0:00:02s
epoch 4  | loss: 0.85082 | eval_custom_logloss: 8.07943 |  0:00:03s
epoch 5  | loss: 0.89752 | eval_custom_logloss: 8.24514 |  0:00:03s
epoch 6  | loss: 0.83901 | eval_custom_logloss: 8.87243 |  0:00:04s
epoch 7  | loss: 0.79039 | eval_custom_logloss: 7.42924 |  0:00:04s
epoch 8  | loss: 0.76054 | eval_custom_logloss: 7.14463 |  0:00:05s
epoch 9  | loss: 0.7051  | eval_custom_logloss: 7.13128 |  0:00:06s
epoch 10 | loss: 0.6922  | eval_custom_logloss: 8.60345 |  0:00:06s
epoch 11 | loss: 0.67647 | eval_custom_logloss: 7.66106 |  0:00:07s
epoch 12 | loss: 0.68332 | eval_custom_logloss: 7.54455 |  0:00:08s
epoch 13 | loss: 0.67353 | eval_custom_logloss: 7.16958 |  0:00:08s
epoch 14 | loss: 0.72169 | eval_custom_logloss: 6.8206  |  0:00:09s
epoch 15 | loss: 0.6856  | eval_custom_logloss: 7.29287 |  0:00:09s
epoch 16 | loss: 0.64329 | eval_custom_logloss: 4.90736 |  0:00:10s
epoch 17 | loss: 0.58507 | eval_custom_logloss: 4.88727 |  0:00:11s
epoch 18 | loss: 0.5806  | eval_custom_logloss: 4.65507 |  0:00:11s
epoch 19 | loss: 0.56563 | eval_custom_logloss: 5.01995 |  0:00:12s
epoch 20 | loss: 0.53896 | eval_custom_logloss: 4.53476 |  0:00:13s
epoch 21 | loss: 0.55878 | eval_custom_logloss: 2.59697 |  0:00:13s
epoch 22 | loss: 0.61265 | eval_custom_logloss: 2.61481 |  0:00:14s
epoch 23 | loss: 0.57846 | eval_custom_logloss: 2.21434 |  0:00:15s
epoch 24 | loss: 0.55473 | eval_custom_logloss: 1.93563 |  0:00:15s
epoch 25 | loss: 0.52618 | eval_custom_logloss: 2.03768 |  0:00:16s
epoch 26 | loss: 0.54213 | eval_custom_logloss: 2.52462 |  0:00:17s
epoch 27 | loss: 0.56184 | eval_custom_logloss: 1.63929 |  0:00:17s
epoch 28 | loss: 0.53332 | eval_custom_logloss: 2.33008 |  0:00:18s
epoch 29 | loss: 0.55926 | eval_custom_logloss: 2.3534  |  0:00:19s
epoch 30 | loss: 0.55995 | eval_custom_logloss: 3.68969 |  0:00:19s
epoch 31 | loss: 0.50744 | eval_custom_logloss: 2.90016 |  0:00:20s
epoch 32 | loss: 0.52067 | eval_custom_logloss: 3.88588 |  0:00:20s
epoch 33 | loss: 0.52654 | eval_custom_logloss: 3.34639 |  0:00:21s
epoch 34 | loss: 0.51597 | eval_custom_logloss: 3.60147 |  0:00:22s
epoch 35 | loss: 0.50053 | eval_custom_logloss: 3.42374 |  0:00:23s
epoch 36 | loss: 0.52389 | eval_custom_logloss: 2.82446 |  0:00:23s
epoch 37 | loss: 0.51344 | eval_custom_logloss: 2.40633 |  0:00:24s
epoch 38 | loss: 0.51512 | eval_custom_logloss: 1.87238 |  0:00:25s
epoch 39 | loss: 0.50871 | eval_custom_logloss: 1.935   |  0:00:25s
epoch 40 | loss: 0.49996 | eval_custom_logloss: 1.96837 |  0:00:26s
epoch 41 | loss: 0.4903  | eval_custom_logloss: 1.66857 |  0:00:27s
epoch 42 | loss: 0.50865 | eval_custom_logloss: 1.62624 |  0:00:27s
epoch 43 | loss: 0.48443 | eval_custom_logloss: 1.9147  |  0:00:28s
epoch 44 | loss: 0.48517 | eval_custom_logloss: 1.86714 |  0:00:29s
epoch 45 | loss: 0.46903 | eval_custom_logloss: 1.58623 |  0:00:29s
epoch 46 | loss: 0.49464 | eval_custom_logloss: 1.68137 |  0:00:30s
epoch 47 | loss: 0.49008 | eval_custom_logloss: 1.86699 |  0:00:31s
epoch 48 | loss: 0.49289 | eval_custom_logloss: 1.62759 |  0:00:31s
epoch 49 | loss: 0.49235 | eval_custom_logloss: 2.06439 |  0:00:32s
epoch 50 | loss: 0.47382 | eval_custom_logloss: 1.67186 |  0:00:33s
epoch 51 | loss: 0.46554 | eval_custom_logloss: 1.88258 |  0:00:33s
epoch 52 | loss: 0.48229 | eval_custom_logloss: 2.4591  |  0:00:34s
epoch 53 | loss: 0.47875 | eval_custom_logloss: 2.52383 |  0:00:35s
epoch 54 | loss: 0.51075 | eval_custom_logloss: 2.18822 |  0:00:35s
epoch 55 | loss: 0.46117 | eval_custom_logloss: 2.64612 |  0:00:36s
epoch 56 | loss: 0.47136 | eval_custom_logloss: 3.00068 |  0:00:37s
epoch 57 | loss: 0.46219 | eval_custom_logloss: 2.23722 |  0:00:37s
epoch 58 | loss: 0.474   | eval_custom_logloss: 2.24894 |  0:00:38s
epoch 59 | loss: 0.4699  | eval_custom_logloss: 2.02343 |  0:00:39s
epoch 60 | loss: 0.4423  | eval_custom_logloss: 1.81567 |  0:00:39s
epoch 61 | loss: 0.45002 | eval_custom_logloss: 1.79696 |  0:00:40s
epoch 62 | loss: 0.43761 | eval_custom_logloss: 1.54358 |  0:00:40s
epoch 63 | loss: 0.44892 | eval_custom_logloss: 1.72153 |  0:00:41s
epoch 64 | loss: 0.43042 | eval_custom_logloss: 1.59118 |  0:00:42s
epoch 65 | loss: 0.45026 | eval_custom_logloss: 1.41959 |  0:00:42s
epoch 66 | loss: 0.46457 | eval_custom_logloss: 2.09581 |  0:00:43s
epoch 67 | loss: 0.45831 | eval_custom_logloss: 1.57195 |  0:00:44s
epoch 68 | loss: 0.45243 | eval_custom_logloss: 1.99527 |  0:00:44s
epoch 69 | loss: 0.43842 | eval_custom_logloss: 1.63456 |  0:00:45s
epoch 70 | loss: 0.44551 | eval_custom_logloss: 1.77429 |  0:00:46s
epoch 71 | loss: 0.44067 | eval_custom_logloss: 1.81206 |  0:00:46s
epoch 72 | loss: 0.43612 | eval_custom_logloss: 1.34511 |  0:00:47s
epoch 73 | loss: 0.42005 | eval_custom_logloss: 1.30992 |  0:00:48s
epoch 74 | loss: 0.38718 | eval_custom_logloss: 1.47068 |  0:00:48s
epoch 75 | loss: 0.3994  | eval_custom_logloss: 1.51339 |  0:00:49s
epoch 76 | loss: 0.42472 | eval_custom_logloss: 1.40055 |  0:00:49s
epoch 77 | loss: 0.42628 | eval_custom_logloss: 1.35851 |  0:00:50s
epoch 78 | loss: 0.40283 | eval_custom_logloss: 1.44044 |  0:00:51s
epoch 79 | loss: 0.42191 | eval_custom_logloss: 1.1817  |  0:00:51s
epoch 80 | loss: 0.43791 | eval_custom_logloss: 0.78956 |  0:00:52s
epoch 81 | loss: 0.42202 | eval_custom_logloss: 0.8634  |  0:00:53s
epoch 82 | loss: 0.41052 | eval_custom_logloss: 1.12256 |  0:00:53s
epoch 83 | loss: 0.44668 | eval_custom_logloss: 1.36041 |  0:00:54s
epoch 84 | loss: 0.43816 | eval_custom_logloss: 1.11683 |  0:00:55s
epoch 85 | loss: 0.41945 | eval_custom_logloss: 1.28163 |  0:00:55s
epoch 86 | loss: 0.40825 | eval_custom_logloss: 1.31154 |  0:00:56s
epoch 87 | loss: 0.41757 | eval_custom_logloss: 1.24828 |  0:00:57s
epoch 88 | loss: 0.41585 | eval_custom_logloss: 1.28555 |  0:00:57s
epoch 89 | loss: 0.41886 | eval_custom_logloss: 1.25691 |  0:00:58s
epoch 90 | loss: 0.41263 | eval_custom_logloss: 1.22191 |  0:00:59s
epoch 91 | loss: 0.39253 | eval_custom_logloss: 1.28795 |  0:00:59s
epoch 92 | loss: 0.37745 | eval_custom_logloss: 1.15713 |  0:01:00s
epoch 93 | loss: 0.35809 | eval_custom_logloss: 1.04543 |  0:01:00s
epoch 94 | loss: 0.39965 | eval_custom_logloss: 1.38028 |  0:01:01s
epoch 95 | loss: 0.3971  | eval_custom_logloss: 1.67959 |  0:01:02s
epoch 96 | loss: 0.3979  | eval_custom_logloss: 1.17928 |  0:01:02s
epoch 97 | loss: 0.41173 | eval_custom_logloss: 1.00398 |  0:01:03s
epoch 98 | loss: 0.4207  | eval_custom_logloss: 0.76273 |  0:01:04s
epoch 99 | loss: 0.37231 | eval_custom_logloss: 0.97989 |  0:01:04s
Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_eval_custom_logloss = 0.76273
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 1.61842, 'Log Loss - std': 0.6010638481891919} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 49 finished with value: 1.61842 and parameters: {'n_d': 38, 'n_steps': 6, 'gamma': 1.1063980440635983, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 5, 'momentum': 0.0014211980729350278, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.14822 | eval_custom_logloss: 2.22915 |  0:00:00s
epoch 1  | loss: 0.87676 | eval_custom_logloss: 2.02082 |  0:00:00s
epoch 2  | loss: 0.8059  | eval_custom_logloss: 1.8517  |  0:00:01s
epoch 3  | loss: 0.75209 | eval_custom_logloss: 2.14567 |  0:00:01s
epoch 4  | loss: 0.70177 | eval_custom_logloss: 1.47058 |  0:00:01s
epoch 5  | loss: 0.6807  | eval_custom_logloss: 2.08747 |  0:00:02s
epoch 6  | loss: 0.65541 | eval_custom_logloss: 2.87711 |  0:00:02s
epoch 7  | loss: 0.68035 | eval_custom_logloss: 3.05389 |  0:00:03s
epoch 8  | loss: 0.63188 | eval_custom_logloss: 3.25563 |  0:00:03s
epoch 9  | loss: 0.61049 | eval_custom_logloss: 2.74281 |  0:00:03s
epoch 10 | loss: 0.58196 | eval_custom_logloss: 2.89327 |  0:00:04s
epoch 11 | loss: 0.55414 | eval_custom_logloss: 2.34927 |  0:00:04s
epoch 12 | loss: 0.53839 | eval_custom_logloss: 2.30889 |  0:00:04s
epoch 13 | loss: 0.51398 | eval_custom_logloss: 2.29773 |  0:00:05s
epoch 14 | loss: 0.50989 | eval_custom_logloss: 1.50313 |  0:00:05s
epoch 15 | loss: 0.5134  | eval_custom_logloss: 1.29391 |  0:00:06s
epoch 16 | loss: 0.52991 | eval_custom_logloss: 1.48246 |  0:00:06s
epoch 17 | loss: 0.49108 | eval_custom_logloss: 1.47675 |  0:00:06s
epoch 18 | loss: 0.48254 | eval_custom_logloss: 2.25288 |  0:00:07s
epoch 19 | loss: 0.48828 | eval_custom_logloss: 1.14921 |  0:00:07s
epoch 20 | loss: 0.44902 | eval_custom_logloss: 0.99447 |  0:00:07s
epoch 21 | loss: 0.44155 | eval_custom_logloss: 0.92044 |  0:00:08s
epoch 22 | loss: 0.45598 | eval_custom_logloss: 0.79872 |  0:00:08s
epoch 23 | loss: 0.44884 | eval_custom_logloss: 1.2101  |  0:00:09s
epoch 24 | loss: 0.44943 | eval_custom_logloss: 1.30196 |  0:00:09s
epoch 25 | loss: 0.42705 | eval_custom_logloss: 1.02176 |  0:00:09s
epoch 26 | loss: 0.42511 | eval_custom_logloss: 1.02003 |  0:00:10s
epoch 27 | loss: 0.41617 | eval_custom_logloss: 0.98273 |  0:00:10s
epoch 28 | loss: 0.41596 | eval_custom_logloss: 1.07576 |  0:00:10s
epoch 29 | loss: 0.43403 | eval_custom_logloss: 0.98972 |  0:00:11s
epoch 30 | loss: 0.41501 | eval_custom_logloss: 1.29686 |  0:00:11s
epoch 31 | loss: 0.38047 | eval_custom_logloss: 1.24147 |  0:00:12s
epoch 32 | loss: 0.39426 | eval_custom_logloss: 1.16017 |  0:00:12s
epoch 33 | loss: 0.40069 | eval_custom_logloss: 1.45702 |  0:00:12s
epoch 34 | loss: 0.40857 | eval_custom_logloss: 1.23532 |  0:00:13s
epoch 35 | loss: 0.39249 | eval_custom_logloss: 1.13677 |  0:00:13s
epoch 36 | loss: 0.39263 | eval_custom_logloss: 1.15554 |  0:00:13s
epoch 37 | loss: 0.36536 | eval_custom_logloss: 0.96323 |  0:00:14s
epoch 38 | loss: 0.36214 | eval_custom_logloss: 1.25919 |  0:00:14s
epoch 39 | loss: 0.3349  | eval_custom_logloss: 1.06088 |  0:00:14s
epoch 40 | loss: 0.32209 | eval_custom_logloss: 0.95355 |  0:00:15s
epoch 41 | loss: 0.32027 | eval_custom_logloss: 0.97864 |  0:00:15s
epoch 42 | loss: 0.33461 | eval_custom_logloss: 0.92997 |  0:00:15s

Early stopping occurred at epoch 42 with best_epoch = 22 and best_eval_custom_logloss = 0.79872
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.7741, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11296 | eval_custom_logloss: 3.18148 |  0:00:00s
epoch 1  | loss: 0.83891 | eval_custom_logloss: 2.59806 |  0:00:00s
epoch 2  | loss: 0.77487 | eval_custom_logloss: 2.53252 |  0:00:01s
epoch 3  | loss: 0.70478 | eval_custom_logloss: 2.84438 |  0:00:01s
epoch 4  | loss: 0.69923 | eval_custom_logloss: 2.25252 |  0:00:01s
epoch 5  | loss: 0.67794 | eval_custom_logloss: 2.39022 |  0:00:02s
epoch 6  | loss: 0.59946 | eval_custom_logloss: 2.17011 |  0:00:02s
epoch 7  | loss: 0.58854 | eval_custom_logloss: 1.99801 |  0:00:02s
epoch 8  | loss: 0.5839  | eval_custom_logloss: 1.77189 |  0:00:03s
epoch 9  | loss: 0.54024 | eval_custom_logloss: 2.52641 |  0:00:03s
epoch 10 | loss: 0.50513 | eval_custom_logloss: 2.66679 |  0:00:03s
epoch 11 | loss: 0.49386 | eval_custom_logloss: 1.87014 |  0:00:04s
epoch 12 | loss: 0.5179  | eval_custom_logloss: 1.88757 |  0:00:04s
epoch 13 | loss: 0.50004 | eval_custom_logloss: 2.82436 |  0:00:04s
epoch 14 | loss: 0.49102 | eval_custom_logloss: 1.87616 |  0:00:05s
epoch 15 | loss: 0.48175 | eval_custom_logloss: 1.58212 |  0:00:05s
epoch 16 | loss: 0.46393 | eval_custom_logloss: 1.6661  |  0:00:06s
epoch 17 | loss: 0.42448 | eval_custom_logloss: 1.64814 |  0:00:06s
epoch 18 | loss: 0.44436 | eval_custom_logloss: 1.64809 |  0:00:06s
epoch 19 | loss: 0.42258 | eval_custom_logloss: 1.94986 |  0:00:07s
epoch 20 | loss: 0.43982 | eval_custom_logloss: 2.23162 |  0:00:07s
epoch 21 | loss: 0.42919 | eval_custom_logloss: 1.90726 |  0:00:07s
epoch 22 | loss: 0.42851 | eval_custom_logloss: 1.9468  |  0:00:08s
epoch 23 | loss: 0.40129 | eval_custom_logloss: 1.38823 |  0:00:08s
epoch 24 | loss: 0.37572 | eval_custom_logloss: 1.49582 |  0:00:08s
epoch 25 | loss: 0.38096 | eval_custom_logloss: 1.92063 |  0:00:09s
epoch 26 | loss: 0.36744 | eval_custom_logloss: 1.6171  |  0:00:09s
epoch 27 | loss: 0.39941 | eval_custom_logloss: 1.50121 |  0:00:09s
epoch 28 | loss: 0.37533 | eval_custom_logloss: 1.32198 |  0:00:10s
epoch 29 | loss: 0.35198 | eval_custom_logloss: 1.8694  |  0:00:10s
epoch 30 | loss: 0.32107 | eval_custom_logloss: 1.43938 |  0:00:10s
epoch 31 | loss: 0.3659  | eval_custom_logloss: 1.57603 |  0:00:11s
epoch 32 | loss: 0.31036 | eval_custom_logloss: 1.52864 |  0:00:11s
epoch 33 | loss: 0.33165 | eval_custom_logloss: 1.32162 |  0:00:11s
epoch 34 | loss: 0.34379 | eval_custom_logloss: 1.35324 |  0:00:12s
epoch 35 | loss: 0.35249 | eval_custom_logloss: 1.33969 |  0:00:12s
epoch 36 | loss: 0.35403 | eval_custom_logloss: 1.27813 |  0:00:12s
epoch 37 | loss: 0.3665  | eval_custom_logloss: 1.10499 |  0:00:13s
epoch 38 | loss: 0.36349 | eval_custom_logloss: 1.2286  |  0:00:13s
epoch 39 | loss: 0.32694 | eval_custom_logloss: 1.61957 |  0:00:13s
epoch 40 | loss: 0.33097 | eval_custom_logloss: 1.26456 |  0:00:14s
epoch 41 | loss: 0.35151 | eval_custom_logloss: 1.4099  |  0:00:14s
epoch 42 | loss: 0.32595 | eval_custom_logloss: 1.42257 |  0:00:14s
epoch 43 | loss: 0.30963 | eval_custom_logloss: 1.76025 |  0:00:15s
epoch 44 | loss: 0.33048 | eval_custom_logloss: 1.43631 |  0:00:15s
epoch 45 | loss: 0.30769 | eval_custom_logloss: 1.69376 |  0:00:15s
epoch 46 | loss: 0.32166 | eval_custom_logloss: 1.448   |  0:00:16s
epoch 47 | loss: 0.28414 | eval_custom_logloss: 1.36847 |  0:00:16s
epoch 48 | loss: 0.27013 | eval_custom_logloss: 1.57586 |  0:00:17s
epoch 49 | loss: 0.2908  | eval_custom_logloss: 1.16486 |  0:00:17s
epoch 50 | loss: 0.25498 | eval_custom_logloss: 1.36067 |  0:00:17s
epoch 51 | loss: 0.25848 | eval_custom_logloss: 1.5659  |  0:00:18s
epoch 52 | loss: 0.27097 | eval_custom_logloss: 1.35397 |  0:00:18s
epoch 53 | loss: 0.27322 | eval_custom_logloss: 1.34527 |  0:00:18s
epoch 54 | loss: 0.26347 | eval_custom_logloss: 1.15606 |  0:00:19s
epoch 55 | loss: 0.26389 | eval_custom_logloss: 1.15649 |  0:00:19s
epoch 56 | loss: 0.23782 | eval_custom_logloss: 1.11749 |  0:00:19s
epoch 57 | loss: 0.25866 | eval_custom_logloss: 1.39422 |  0:00:20s

Early stopping occurred at epoch 57 with best_epoch = 37 and best_eval_custom_logloss = 1.10499
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.9049, 'Log Loss - std': 0.13080000000000003} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.09187 | eval_custom_logloss: 2.06665 |  0:00:00s
epoch 1  | loss: 0.87878 | eval_custom_logloss: 3.01638 |  0:00:00s
epoch 2  | loss: 0.82028 | eval_custom_logloss: 2.94492 |  0:00:00s
epoch 3  | loss: 0.76872 | eval_custom_logloss: 2.20906 |  0:00:01s
epoch 4  | loss: 0.70164 | eval_custom_logloss: 2.27861 |  0:00:01s
epoch 5  | loss: 0.67922 | eval_custom_logloss: 2.5769  |  0:00:01s
epoch 6  | loss: 0.662   | eval_custom_logloss: 2.95026 |  0:00:02s
epoch 7  | loss: 0.63333 | eval_custom_logloss: 2.58061 |  0:00:02s
epoch 8  | loss: 0.63986 | eval_custom_logloss: 2.29098 |  0:00:02s
epoch 9  | loss: 0.58926 | eval_custom_logloss: 2.64539 |  0:00:03s
epoch 10 | loss: 0.56467 | eval_custom_logloss: 2.61754 |  0:00:03s
epoch 11 | loss: 0.55069 | eval_custom_logloss: 2.84557 |  0:00:03s
epoch 12 | loss: 0.55287 | eval_custom_logloss: 2.12888 |  0:00:04s
epoch 13 | loss: 0.54949 | eval_custom_logloss: 1.95538 |  0:00:04s
epoch 14 | loss: 0.6102  | eval_custom_logloss: 1.75207 |  0:00:04s
epoch 15 | loss: 0.56711 | eval_custom_logloss: 1.26992 |  0:00:05s
epoch 16 | loss: 0.52779 | eval_custom_logloss: 1.44504 |  0:00:05s
epoch 17 | loss: 0.52041 | eval_custom_logloss: 1.44457 |  0:00:05s
epoch 18 | loss: 0.54685 | eval_custom_logloss: 1.90582 |  0:00:06s
epoch 19 | loss: 0.51743 | eval_custom_logloss: 1.98122 |  0:00:06s
epoch 20 | loss: 0.53096 | eval_custom_logloss: 1.59484 |  0:00:06s
epoch 21 | loss: 0.55928 | eval_custom_logloss: 1.18936 |  0:00:06s
epoch 22 | loss: 0.54972 | eval_custom_logloss: 0.78499 |  0:00:07s
epoch 23 | loss: 0.53836 | eval_custom_logloss: 0.75659 |  0:00:07s
epoch 24 | loss: 0.51823 | eval_custom_logloss: 1.13284 |  0:00:08s
epoch 25 | loss: 0.4974  | eval_custom_logloss: 0.98812 |  0:00:08s
epoch 26 | loss: 0.54105 | eval_custom_logloss: 1.01454 |  0:00:08s
epoch 27 | loss: 0.51823 | eval_custom_logloss: 0.83914 |  0:00:08s
epoch 28 | loss: 0.50254 | eval_custom_logloss: 0.78568 |  0:00:09s
epoch 29 | loss: 0.4978  | eval_custom_logloss: 1.03243 |  0:00:09s
epoch 30 | loss: 0.48151 | eval_custom_logloss: 1.1959  |  0:00:09s
epoch 31 | loss: 0.46429 | eval_custom_logloss: 1.0999  |  0:00:10s
epoch 32 | loss: 0.46007 | eval_custom_logloss: 1.01194 |  0:00:10s
epoch 33 | loss: 0.44836 | eval_custom_logloss: 1.14878 |  0:00:10s
epoch 34 | loss: 0.41069 | eval_custom_logloss: 1.10603 |  0:00:11s
epoch 35 | loss: 0.4176  | eval_custom_logloss: 0.92473 |  0:00:11s
epoch 36 | loss: 0.43172 | eval_custom_logloss: 1.02295 |  0:00:11s
epoch 37 | loss: 0.41767 | eval_custom_logloss: 1.36866 |  0:00:12s
epoch 38 | loss: 0.43456 | eval_custom_logloss: 1.01825 |  0:00:12s
epoch 39 | loss: 0.39387 | eval_custom_logloss: 0.89925 |  0:00:12s
epoch 40 | loss: 0.41578 | eval_custom_logloss: 1.17366 |  0:00:13s
epoch 41 | loss: 0.40965 | eval_custom_logloss: 1.17681 |  0:00:13s
epoch 42 | loss: 0.3721  | eval_custom_logloss: 0.9649  |  0:00:13s
epoch 43 | loss: 0.42695 | eval_custom_logloss: 0.81448 |  0:00:14s

Early stopping occurred at epoch 43 with best_epoch = 23 and best_eval_custom_logloss = 0.75659
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8554666666666667, 'Log Loss - std': 0.12764430613579633} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.05295 | eval_custom_logloss: 2.96643 |  0:00:00s
epoch 1  | loss: 0.90386 | eval_custom_logloss: 1.82268 |  0:00:00s
epoch 2  | loss: 0.82248 | eval_custom_logloss: 1.96433 |  0:00:00s
epoch 3  | loss: 0.73771 | eval_custom_logloss: 1.64301 |  0:00:01s
epoch 4  | loss: 0.67638 | eval_custom_logloss: 3.49081 |  0:00:01s
epoch 5  | loss: 0.6491  | eval_custom_logloss: 1.81601 |  0:00:01s
epoch 6  | loss: 0.62437 | eval_custom_logloss: 1.74668 |  0:00:02s
epoch 7  | loss: 0.57604 | eval_custom_logloss: 1.63697 |  0:00:02s
epoch 8  | loss: 0.57686 | eval_custom_logloss: 2.52399 |  0:00:02s
epoch 9  | loss: 0.53315 | eval_custom_logloss: 2.32836 |  0:00:03s
epoch 10 | loss: 0.50864 | eval_custom_logloss: 2.52527 |  0:00:03s
epoch 11 | loss: 0.50794 | eval_custom_logloss: 2.16837 |  0:00:03s
epoch 12 | loss: 0.51733 | eval_custom_logloss: 2.59185 |  0:00:04s
epoch 13 | loss: 0.49616 | eval_custom_logloss: 2.81734 |  0:00:04s
epoch 14 | loss: 0.48583 | eval_custom_logloss: 2.05097 |  0:00:04s
epoch 15 | loss: 0.47823 | eval_custom_logloss: 1.72852 |  0:00:05s
epoch 16 | loss: 0.48574 | eval_custom_logloss: 1.87888 |  0:00:05s
epoch 17 | loss: 0.46042 | eval_custom_logloss: 2.05595 |  0:00:05s
epoch 18 | loss: 0.45946 | eval_custom_logloss: 2.64488 |  0:00:06s
epoch 19 | loss: 0.45376 | eval_custom_logloss: 2.42389 |  0:00:06s
epoch 20 | loss: 0.44244 | eval_custom_logloss: 2.00698 |  0:00:06s
epoch 21 | loss: 0.4282  | eval_custom_logloss: 1.59256 |  0:00:07s
epoch 22 | loss: 0.4388  | eval_custom_logloss: 1.93398 |  0:00:07s
epoch 23 | loss: 0.41834 | eval_custom_logloss: 2.2229  |  0:00:07s
epoch 24 | loss: 0.42475 | eval_custom_logloss: 2.04155 |  0:00:08s
epoch 25 | loss: 0.41321 | eval_custom_logloss: 1.69575 |  0:00:08s
epoch 26 | loss: 0.44235 | eval_custom_logloss: 1.40735 |  0:00:08s
epoch 27 | loss: 0.44485 | eval_custom_logloss: 1.40588 |  0:00:08s
epoch 28 | loss: 0.42226 | eval_custom_logloss: 1.92948 |  0:00:09s
epoch 29 | loss: 0.43462 | eval_custom_logloss: 1.80963 |  0:00:09s
epoch 30 | loss: 0.41548 | eval_custom_logloss: 1.60932 |  0:00:09s
epoch 31 | loss: 0.41474 | eval_custom_logloss: 1.65458 |  0:00:10s
epoch 32 | loss: 0.44478 | eval_custom_logloss: 1.25241 |  0:00:10s
epoch 33 | loss: 0.43221 | eval_custom_logloss: 0.86393 |  0:00:10s
epoch 34 | loss: 0.42194 | eval_custom_logloss: 1.56778 |  0:00:11s
epoch 35 | loss: 0.41385 | eval_custom_logloss: 1.67478 |  0:00:11s
epoch 36 | loss: 0.42894 | eval_custom_logloss: 1.45035 |  0:00:11s
epoch 37 | loss: 0.40448 | eval_custom_logloss: 1.68114 |  0:00:12s
epoch 38 | loss: 0.4303  | eval_custom_logloss: 1.002   |  0:00:12s
epoch 39 | loss: 0.42793 | eval_custom_logloss: 1.11451 |  0:00:12s
epoch 40 | loss: 0.40606 | eval_custom_logloss: 1.08092 |  0:00:13s
epoch 41 | loss: 0.40922 | eval_custom_logloss: 1.27413 |  0:00:13s
epoch 42 | loss: 0.41332 | eval_custom_logloss: 1.16648 |  0:00:13s
epoch 43 | loss: 0.40567 | eval_custom_logloss: 1.04548 |  0:00:14s
epoch 44 | loss: 0.39741 | eval_custom_logloss: 1.05232 |  0:00:14s
epoch 45 | loss: 0.37548 | eval_custom_logloss: 1.10264 |  0:00:14s
epoch 46 | loss: 0.38084 | eval_custom_logloss: 1.20545 |  0:00:15s
epoch 47 | loss: 0.36898 | eval_custom_logloss: 1.26355 |  0:00:15s
epoch 48 | loss: 0.40484 | eval_custom_logloss: 1.55185 |  0:00:15s
epoch 49 | loss: 0.39541 | eval_custom_logloss: 1.25276 |  0:00:16s
epoch 50 | loss: 0.38774 | eval_custom_logloss: 1.48766 |  0:00:16s
epoch 51 | loss: 0.38469 | eval_custom_logloss: 0.86358 |  0:00:16s
epoch 52 | loss: 0.36647 | eval_custom_logloss: 0.88716 |  0:00:17s
epoch 53 | loss: 0.39919 | eval_custom_logloss: 0.85559 |  0:00:17s
epoch 54 | loss: 0.35403 | eval_custom_logloss: 0.87027 |  0:00:17s
epoch 55 | loss: 0.36061 | eval_custom_logloss: 0.87258 |  0:00:18s
epoch 56 | loss: 0.35638 | eval_custom_logloss: 0.97968 |  0:00:18s
epoch 57 | loss: 0.38241 | eval_custom_logloss: 0.91272 |  0:00:18s
epoch 58 | loss: 0.37074 | eval_custom_logloss: 0.75406 |  0:00:19s
epoch 59 | loss: 0.36583 | eval_custom_logloss: 0.80725 |  0:00:19s
epoch 60 | loss: 0.34092 | eval_custom_logloss: 1.10226 |  0:00:19s
epoch 61 | loss: 0.36201 | eval_custom_logloss: 1.14669 |  0:00:20s
epoch 62 | loss: 0.37544 | eval_custom_logloss: 1.08348 |  0:00:20s
epoch 63 | loss: 0.35835 | eval_custom_logloss: 1.22056 |  0:00:20s
epoch 64 | loss: 0.3574  | eval_custom_logloss: 0.99238 |  0:00:21s
epoch 65 | loss: 0.37056 | eval_custom_logloss: 0.95044 |  0:00:21s
epoch 66 | loss: 0.36406 | eval_custom_logloss: 0.88012 |  0:00:21s
epoch 67 | loss: 0.35326 | eval_custom_logloss: 0.95078 |  0:00:22s
epoch 68 | loss: 0.33201 | eval_custom_logloss: 0.95265 |  0:00:22s
epoch 69 | loss: 0.34578 | eval_custom_logloss: 1.37741 |  0:00:22s
epoch 70 | loss: 0.35877 | eval_custom_logloss: 0.99475 |  0:00:23s
epoch 71 | loss: 0.32396 | eval_custom_logloss: 1.06461 |  0:00:23s
epoch 72 | loss: 0.31866 | eval_custom_logloss: 1.15933 |  0:00:23s
epoch 73 | loss: 0.33053 | eval_custom_logloss: 1.07581 |  0:00:24s
epoch 74 | loss: 0.31617 | eval_custom_logloss: 0.99903 |  0:00:24s
epoch 75 | loss: 0.35143 | eval_custom_logloss: 0.8942  |  0:00:24s
epoch 76 | loss: 0.32244 | eval_custom_logloss: 0.92868 |  0:00:25s
epoch 77 | loss: 0.30246 | eval_custom_logloss: 0.97482 |  0:00:25s
epoch 78 | loss: 0.28658 | eval_custom_logloss: 0.89645 |  0:00:25s

Early stopping occurred at epoch 78 with best_epoch = 58 and best_eval_custom_logloss = 0.75406
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8269000000000001, 'Log Loss - std': 0.12111137436260892} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax', 'n_a': 20, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.01736 | eval_custom_logloss: 3.66053 |  0:00:00s
epoch 1  | loss: 0.84761 | eval_custom_logloss: 3.08491 |  0:00:00s
epoch 2  | loss: 0.78901 | eval_custom_logloss: 2.50581 |  0:00:01s
epoch 3  | loss: 0.75475 | eval_custom_logloss: 1.90696 |  0:00:01s
epoch 4  | loss: 0.64485 | eval_custom_logloss: 1.80418 |  0:00:01s
epoch 5  | loss: 0.64604 | eval_custom_logloss: 1.65789 |  0:00:02s
epoch 6  | loss: 0.57638 | eval_custom_logloss: 1.55152 |  0:00:02s
epoch 7  | loss: 0.5724  | eval_custom_logloss: 1.84743 |  0:00:03s
epoch 8  | loss: 0.54112 | eval_custom_logloss: 1.713   |  0:00:03s
epoch 9  | loss: 0.55307 | eval_custom_logloss: 1.76716 |  0:00:03s
epoch 10 | loss: 0.55261 | eval_custom_logloss: 2.20411 |  0:00:04s
epoch 11 | loss: 0.52125 | eval_custom_logloss: 1.50039 |  0:00:04s
epoch 12 | loss: 0.509   | eval_custom_logloss: 1.22342 |  0:00:04s
epoch 13 | loss: 0.50219 | eval_custom_logloss: 0.93269 |  0:00:05s
epoch 14 | loss: 0.49609 | eval_custom_logloss: 1.11679 |  0:00:05s
epoch 15 | loss: 0.50965 | eval_custom_logloss: 1.07794 |  0:00:05s
epoch 16 | loss: 0.51666 | eval_custom_logloss: 1.4716  |  0:00:06s
epoch 17 | loss: 0.48796 | eval_custom_logloss: 1.58979 |  0:00:06s
epoch 18 | loss: 0.43812 | eval_custom_logloss: 1.31044 |  0:00:07s
epoch 19 | loss: 0.46414 | eval_custom_logloss: 1.17265 |  0:00:07s
epoch 20 | loss: 0.46585 | eval_custom_logloss: 1.73642 |  0:00:07s
epoch 21 | loss: 0.47223 | eval_custom_logloss: 1.86766 |  0:00:08s
epoch 22 | loss: 0.45987 | eval_custom_logloss: 1.92662 |  0:00:08s
epoch 23 | loss: 0.47937 | eval_custom_logloss: 2.02037 |  0:00:08s
epoch 24 | loss: 0.45061 | eval_custom_logloss: 1.80226 |  0:00:09s
epoch 25 | loss: 0.43169 | eval_custom_logloss: 2.09862 |  0:00:09s
epoch 26 | loss: 0.44643 | eval_custom_logloss: 1.98363 |  0:00:10s
epoch 27 | loss: 0.44503 | eval_custom_logloss: 1.94453 |  0:00:10s
epoch 28 | loss: 0.48671 | eval_custom_logloss: 1.60545 |  0:00:10s
epoch 29 | loss: 0.43051 | eval_custom_logloss: 1.52775 |  0:00:11s
epoch 30 | loss: 0.4493  | eval_custom_logloss: 1.1847  |  0:00:11s
epoch 31 | loss: 0.43665 | eval_custom_logloss: 1.27807 |  0:00:12s
epoch 32 | loss: 0.4202  | eval_custom_logloss: 1.37977 |  0:00:12s
epoch 33 | loss: 0.39138 | eval_custom_logloss: 1.39521 |  0:00:12s

Early stopping occurred at epoch 33 with best_epoch = 13 and best_eval_custom_logloss = 0.93269
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.8444200000000001, 'Log Loss - std': 0.11385154193070907} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 50 finished with value: 0.8444200000000001 and parameters: {'n_d': 20, 'n_steps': 3, 'gamma': 1.3583318934221527, 'cat_emb_dim': 2, 'n_independent': 4, 'n_shared': 2, 'momentum': 0.0019161936321641438, 'mask_type': 'sparsemax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.37937 | eval_custom_logloss: 7.51494 |  0:00:00s
epoch 1  | loss: 1.08118 | eval_custom_logloss: 6.32297 |  0:00:00s
epoch 2  | loss: 0.90227 | eval_custom_logloss: 7.26054 |  0:00:01s
epoch 3  | loss: 0.88777 | eval_custom_logloss: 4.68778 |  0:00:01s
epoch 4  | loss: 0.78571 | eval_custom_logloss: 5.60877 |  0:00:02s
epoch 5  | loss: 0.81442 | eval_custom_logloss: 4.22242 |  0:00:02s
epoch 6  | loss: 0.68126 | eval_custom_logloss: 2.64957 |  0:00:03s
epoch 7  | loss: 0.63724 | eval_custom_logloss: 2.97453 |  0:00:03s
epoch 8  | loss: 0.61998 | eval_custom_logloss: 1.70413 |  0:00:04s
epoch 9  | loss: 0.61872 | eval_custom_logloss: 1.96655 |  0:00:04s
epoch 10 | loss: 0.61105 | eval_custom_logloss: 2.10824 |  0:00:05s
epoch 11 | loss: 0.54268 | eval_custom_logloss: 2.5022  |  0:00:05s
epoch 12 | loss: 0.55918 | eval_custom_logloss: 1.63807 |  0:00:06s
epoch 13 | loss: 0.52349 | eval_custom_logloss: 1.26292 |  0:00:06s
epoch 14 | loss: 0.50681 | eval_custom_logloss: 1.19699 |  0:00:06s
epoch 15 | loss: 0.50966 | eval_custom_logloss: 1.24615 |  0:00:07s
epoch 16 | loss: 0.47206 | eval_custom_logloss: 1.3361  |  0:00:07s
epoch 17 | loss: 0.45516 | eval_custom_logloss: 1.23736 |  0:00:08s
epoch 18 | loss: 0.44006 | eval_custom_logloss: 1.25101 |  0:00:08s
epoch 19 | loss: 0.40276 | eval_custom_logloss: 1.11466 |  0:00:09s
epoch 20 | loss: 0.44434 | eval_custom_logloss: 1.21362 |  0:00:09s
epoch 21 | loss: 0.49907 | eval_custom_logloss: 0.92094 |  0:00:10s
epoch 22 | loss: 0.44532 | eval_custom_logloss: 0.85937 |  0:00:10s
epoch 23 | loss: 0.41085 | eval_custom_logloss: 1.01304 |  0:00:11s
epoch 24 | loss: 0.38707 | eval_custom_logloss: 1.06272 |  0:00:11s
epoch 25 | loss: 0.42454 | eval_custom_logloss: 1.25122 |  0:00:11s
epoch 26 | loss: 0.39634 | eval_custom_logloss: 0.98826 |  0:00:12s
epoch 27 | loss: 0.38688 | eval_custom_logloss: 1.19265 |  0:00:12s
epoch 28 | loss: 0.4249  | eval_custom_logloss: 0.9877  |  0:00:13s
epoch 29 | loss: 0.39485 | eval_custom_logloss: 0.70609 |  0:00:13s
epoch 30 | loss: 0.35715 | eval_custom_logloss: 0.90136 |  0:00:14s
epoch 31 | loss: 0.38032 | eval_custom_logloss: 0.76909 |  0:00:14s
epoch 32 | loss: 0.39995 | eval_custom_logloss: 0.78469 |  0:00:15s
epoch 33 | loss: 0.37561 | eval_custom_logloss: 0.72715 |  0:00:15s
epoch 34 | loss: 0.35385 | eval_custom_logloss: 0.71879 |  0:00:15s
epoch 35 | loss: 0.35494 | eval_custom_logloss: 0.75206 |  0:00:16s
epoch 36 | loss: 0.37233 | eval_custom_logloss: 0.6275  |  0:00:16s
epoch 37 | loss: 0.33285 | eval_custom_logloss: 0.83839 |  0:00:17s
epoch 38 | loss: 0.28437 | eval_custom_logloss: 0.74111 |  0:00:17s
epoch 39 | loss: 0.29698 | eval_custom_logloss: 0.78475 |  0:00:18s
epoch 40 | loss: 0.32396 | eval_custom_logloss: 0.58715 |  0:00:18s
epoch 41 | loss: 0.32146 | eval_custom_logloss: 1.3351  |  0:00:19s
epoch 42 | loss: 0.32977 | eval_custom_logloss: 0.65991 |  0:00:19s
epoch 43 | loss: 0.31096 | eval_custom_logloss: 0.63343 |  0:00:20s
epoch 44 | loss: 0.29782 | eval_custom_logloss: 1.18217 |  0:00:20s
epoch 45 | loss: 0.30023 | eval_custom_logloss: 1.04141 |  0:00:21s
epoch 46 | loss: 0.28562 | eval_custom_logloss: 0.72792 |  0:00:21s
epoch 47 | loss: 0.26017 | eval_custom_logloss: 0.80505 |  0:00:22s
epoch 48 | loss: 0.23917 | eval_custom_logloss: 1.1527  |  0:00:22s
epoch 49 | loss: 0.22935 | eval_custom_logloss: 0.71341 |  0:00:23s
epoch 50 | loss: 0.26473 | eval_custom_logloss: 0.76359 |  0:00:23s
epoch 51 | loss: 0.26692 | eval_custom_logloss: 0.88713 |  0:00:24s
epoch 52 | loss: 0.26047 | eval_custom_logloss: 1.13723 |  0:00:24s
epoch 53 | loss: 0.29411 | eval_custom_logloss: 1.0772  |  0:00:25s
epoch 54 | loss: 0.32064 | eval_custom_logloss: 0.93892 |  0:00:25s
epoch 55 | loss: 0.28581 | eval_custom_logloss: 0.80509 |  0:00:26s
epoch 56 | loss: 0.28544 | eval_custom_logloss: 1.08342 |  0:00:26s
epoch 57 | loss: 0.28929 | eval_custom_logloss: 0.72424 |  0:00:27s
epoch 58 | loss: 0.31391 | eval_custom_logloss: 0.87219 |  0:00:27s
epoch 59 | loss: 0.26793 | eval_custom_logloss: 0.77236 |  0:00:28s
epoch 60 | loss: 0.26685 | eval_custom_logloss: 0.88052 |  0:00:28s

Early stopping occurred at epoch 60 with best_epoch = 40 and best_eval_custom_logloss = 0.58715
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5721, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.44679 | eval_custom_logloss: 7.62226 |  0:00:00s
epoch 1  | loss: 1.18325 | eval_custom_logloss: 6.9428  |  0:00:01s
epoch 2  | loss: 0.92215 | eval_custom_logloss: 5.89973 |  0:00:01s
epoch 3  | loss: 0.8039  | eval_custom_logloss: 4.45181 |  0:00:01s
epoch 4  | loss: 0.83721 | eval_custom_logloss: 4.34281 |  0:00:02s
epoch 5  | loss: 0.72631 | eval_custom_logloss: 4.35305 |  0:00:02s
epoch 6  | loss: 0.67008 | eval_custom_logloss: 3.74978 |  0:00:03s
epoch 7  | loss: 0.61623 | eval_custom_logloss: 2.77097 |  0:00:03s
epoch 8  | loss: 0.58352 | eval_custom_logloss: 3.02531 |  0:00:04s
epoch 9  | loss: 0.53937 | eval_custom_logloss: 2.02523 |  0:00:04s
epoch 10 | loss: 0.53831 | eval_custom_logloss: 2.18094 |  0:00:05s
epoch 11 | loss: 0.51756 | eval_custom_logloss: 1.53331 |  0:00:05s
epoch 12 | loss: 0.52724 | eval_custom_logloss: 1.33647 |  0:00:06s
epoch 13 | loss: 0.49877 | eval_custom_logloss: 1.89544 |  0:00:07s
epoch 14 | loss: 0.48258 | eval_custom_logloss: 1.17129 |  0:00:07s
epoch 15 | loss: 0.46839 | eval_custom_logloss: 1.30479 |  0:00:08s
epoch 16 | loss: 0.43513 | eval_custom_logloss: 1.47547 |  0:00:08s
epoch 17 | loss: 0.41533 | eval_custom_logloss: 1.35433 |  0:00:09s
epoch 18 | loss: 0.43742 | eval_custom_logloss: 1.01169 |  0:00:09s
epoch 19 | loss: 0.40753 | eval_custom_logloss: 0.91353 |  0:00:10s
epoch 20 | loss: 0.42609 | eval_custom_logloss: 0.908   |  0:00:10s
epoch 21 | loss: 0.43022 | eval_custom_logloss: 1.29227 |  0:00:11s
epoch 22 | loss: 0.40224 | eval_custom_logloss: 1.03307 |  0:00:11s
epoch 23 | loss: 0.41295 | eval_custom_logloss: 0.8165  |  0:00:12s
epoch 24 | loss: 0.39111 | eval_custom_logloss: 0.89297 |  0:00:13s
epoch 25 | loss: 0.38826 | eval_custom_logloss: 0.6707  |  0:00:13s
epoch 26 | loss: 0.34254 | eval_custom_logloss: 0.67093 |  0:00:14s
epoch 27 | loss: 0.33593 | eval_custom_logloss: 0.66767 |  0:00:14s
epoch 28 | loss: 0.33855 | eval_custom_logloss: 0.65729 |  0:00:15s
epoch 29 | loss: 0.33764 | eval_custom_logloss: 0.74458 |  0:00:15s
epoch 30 | loss: 0.3134  | eval_custom_logloss: 0.68877 |  0:00:16s
epoch 31 | loss: 0.34694 | eval_custom_logloss: 0.79449 |  0:00:16s
epoch 32 | loss: 0.34496 | eval_custom_logloss: 1.00142 |  0:00:17s
epoch 33 | loss: 0.31574 | eval_custom_logloss: 0.72466 |  0:00:17s
epoch 34 | loss: 0.39418 | eval_custom_logloss: 0.55509 |  0:00:18s
epoch 35 | loss: 0.40507 | eval_custom_logloss: 0.62792 |  0:00:19s
epoch 36 | loss: 0.32314 | eval_custom_logloss: 0.95455 |  0:00:19s
epoch 37 | loss: 0.299   | eval_custom_logloss: 0.84758 |  0:00:20s
epoch 38 | loss: 0.28845 | eval_custom_logloss: 0.8952  |  0:00:20s
epoch 39 | loss: 0.28548 | eval_custom_logloss: 1.06515 |  0:00:21s
epoch 40 | loss: 0.29018 | eval_custom_logloss: 1.12195 |  0:00:21s
epoch 41 | loss: 0.27506 | eval_custom_logloss: 0.91651 |  0:00:22s
epoch 42 | loss: 0.29751 | eval_custom_logloss: 0.95895 |  0:00:22s
epoch 43 | loss: 0.31565 | eval_custom_logloss: 0.66482 |  0:00:23s
epoch 44 | loss: 0.26922 | eval_custom_logloss: 0.83577 |  0:00:23s
epoch 45 | loss: 0.32608 | eval_custom_logloss: 0.98346 |  0:00:24s
epoch 46 | loss: 0.31493 | eval_custom_logloss: 0.75228 |  0:00:24s
epoch 47 | loss: 0.28531 | eval_custom_logloss: 0.85482 |  0:00:25s
epoch 48 | loss: 0.24256 | eval_custom_logloss: 0.83025 |  0:00:25s
epoch 49 | loss: 0.23864 | eval_custom_logloss: 0.83848 |  0:00:26s
epoch 50 | loss: 0.22418 | eval_custom_logloss: 0.86146 |  0:00:27s
epoch 51 | loss: 0.19851 | eval_custom_logloss: 0.78381 |  0:00:27s
epoch 52 | loss: 0.22752 | eval_custom_logloss: 0.75872 |  0:00:28s
epoch 53 | loss: 0.21905 | eval_custom_logloss: 0.87198 |  0:00:28s
epoch 54 | loss: 0.20473 | eval_custom_logloss: 0.95923 |  0:00:29s

Early stopping occurred at epoch 54 with best_epoch = 34 and best_eval_custom_logloss = 0.55509
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5636000000000001, 'Log Loss - std': 0.008500000000000008} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.397   | eval_custom_logloss: 8.11805 |  0:00:00s
epoch 1  | loss: 1.05352 | eval_custom_logloss: 7.33103 |  0:00:00s
epoch 2  | loss: 0.92339 | eval_custom_logloss: 6.9329  |  0:00:01s
epoch 3  | loss: 0.88858 | eval_custom_logloss: 4.78508 |  0:00:01s
epoch 4  | loss: 0.80928 | eval_custom_logloss: 4.27211 |  0:00:02s
epoch 5  | loss: 0.66371 | eval_custom_logloss: 2.70291 |  0:00:02s
epoch 6  | loss: 0.68309 | eval_custom_logloss: 2.96278 |  0:00:02s
epoch 7  | loss: 0.61537 | eval_custom_logloss: 2.20934 |  0:00:03s
epoch 8  | loss: 0.59896 | eval_custom_logloss: 1.25333 |  0:00:03s
epoch 9  | loss: 0.61187 | eval_custom_logloss: 1.2784  |  0:00:04s
epoch 10 | loss: 0.59662 | eval_custom_logloss: 1.43175 |  0:00:04s
epoch 11 | loss: 0.61857 | eval_custom_logloss: 0.96665 |  0:00:04s
epoch 12 | loss: 0.57855 | eval_custom_logloss: 1.43818 |  0:00:05s
epoch 13 | loss: 0.54716 | eval_custom_logloss: 1.48216 |  0:00:05s
epoch 14 | loss: 0.53257 | eval_custom_logloss: 1.71552 |  0:00:06s
epoch 15 | loss: 0.56932 | eval_custom_logloss: 1.01114 |  0:00:06s
epoch 16 | loss: 0.53092 | eval_custom_logloss: 1.04192 |  0:00:06s
epoch 17 | loss: 0.52543 | eval_custom_logloss: 0.89836 |  0:00:07s
epoch 18 | loss: 0.52373 | eval_custom_logloss: 1.28137 |  0:00:07s
epoch 19 | loss: 0.50249 | eval_custom_logloss: 1.10751 |  0:00:08s
epoch 20 | loss: 0.48522 | eval_custom_logloss: 1.17739 |  0:00:08s
epoch 21 | loss: 0.48253 | eval_custom_logloss: 0.86821 |  0:00:08s
epoch 22 | loss: 0.4929  | eval_custom_logloss: 0.70533 |  0:00:09s
epoch 23 | loss: 0.50168 | eval_custom_logloss: 0.73409 |  0:00:10s
epoch 24 | loss: 0.49114 | eval_custom_logloss: 0.69963 |  0:00:10s
epoch 25 | loss: 0.49558 | eval_custom_logloss: 0.68199 |  0:00:11s
epoch 26 | loss: 0.47063 | eval_custom_logloss: 0.63796 |  0:00:11s
epoch 27 | loss: 0.45187 | eval_custom_logloss: 0.72279 |  0:00:12s
epoch 28 | loss: 0.45394 | eval_custom_logloss: 0.58546 |  0:00:12s
epoch 29 | loss: 0.46522 | eval_custom_logloss: 0.51926 |  0:00:13s
epoch 30 | loss: 0.44784 | eval_custom_logloss: 0.56008 |  0:00:13s
epoch 31 | loss: 0.45121 | eval_custom_logloss: 0.63932 |  0:00:14s
epoch 32 | loss: 0.44917 | eval_custom_logloss: 0.64721 |  0:00:15s
epoch 33 | loss: 0.43383 | eval_custom_logloss: 0.69011 |  0:00:15s
epoch 34 | loss: 0.43314 | eval_custom_logloss: 0.60886 |  0:00:16s
epoch 35 | loss: 0.43459 | eval_custom_logloss: 0.52501 |  0:00:16s
epoch 36 | loss: 0.40176 | eval_custom_logloss: 0.62199 |  0:00:17s
epoch 37 | loss: 0.39108 | eval_custom_logloss: 0.64322 |  0:00:17s
epoch 38 | loss: 0.40271 | eval_custom_logloss: 0.69384 |  0:00:18s
epoch 39 | loss: 0.38107 | eval_custom_logloss: 0.50831 |  0:00:18s
epoch 40 | loss: 0.38021 | eval_custom_logloss: 0.70754 |  0:00:19s
epoch 41 | loss: 0.37408 | eval_custom_logloss: 0.57859 |  0:00:19s
epoch 42 | loss: 0.36242 | eval_custom_logloss: 0.71651 |  0:00:20s
epoch 43 | loss: 0.39924 | eval_custom_logloss: 0.56817 |  0:00:20s
epoch 44 | loss: 0.37429 | eval_custom_logloss: 0.54999 |  0:00:21s
epoch 45 | loss: 0.37315 | eval_custom_logloss: 0.59193 |  0:00:22s
epoch 46 | loss: 0.38347 | eval_custom_logloss: 0.87929 |  0:00:22s
epoch 47 | loss: 0.37114 | eval_custom_logloss: 0.60431 |  0:00:23s
epoch 48 | loss: 0.35741 | eval_custom_logloss: 0.73894 |  0:00:23s
epoch 49 | loss: 0.33448 | eval_custom_logloss: 0.5758  |  0:00:24s
epoch 50 | loss: 0.33563 | eval_custom_logloss: 0.53321 |  0:00:24s
epoch 51 | loss: 0.34205 | eval_custom_logloss: 0.53976 |  0:00:25s
epoch 52 | loss: 0.32071 | eval_custom_logloss: 0.64193 |  0:00:25s
epoch 53 | loss: 0.28862 | eval_custom_logloss: 0.45286 |  0:00:26s
epoch 54 | loss: 0.28443 | eval_custom_logloss: 0.55524 |  0:00:26s
epoch 55 | loss: 0.30411 | eval_custom_logloss: 0.51492 |  0:00:27s
epoch 56 | loss: 0.32129 | eval_custom_logloss: 0.42566 |  0:00:27s
epoch 57 | loss: 0.30447 | eval_custom_logloss: 0.49685 |  0:00:28s
epoch 58 | loss: 0.28492 | eval_custom_logloss: 0.47835 |  0:00:29s
epoch 59 | loss: 0.28846 | eval_custom_logloss: 0.5228  |  0:00:29s
epoch 60 | loss: 0.29897 | eval_custom_logloss: 0.47396 |  0:00:30s
epoch 61 | loss: 0.31578 | eval_custom_logloss: 0.52139 |  0:00:30s
epoch 62 | loss: 0.32733 | eval_custom_logloss: 0.59773 |  0:00:31s
epoch 63 | loss: 0.35954 | eval_custom_logloss: 0.66143 |  0:00:31s
epoch 64 | loss: 0.34687 | eval_custom_logloss: 0.60223 |  0:00:32s
epoch 65 | loss: 0.34729 | eval_custom_logloss: 0.54711 |  0:00:32s
epoch 66 | loss: 0.29252 | eval_custom_logloss: 0.6939  |  0:00:33s
epoch 67 | loss: 0.27706 | eval_custom_logloss: 0.55895 |  0:00:33s
epoch 68 | loss: 0.28194 | eval_custom_logloss: 0.54528 |  0:00:34s
epoch 69 | loss: 0.30796 | eval_custom_logloss: 0.61479 |  0:00:34s
epoch 70 | loss: 0.28105 | eval_custom_logloss: 0.77144 |  0:00:35s
epoch 71 | loss: 0.29958 | eval_custom_logloss: 0.52712 |  0:00:35s
epoch 72 | loss: 0.27723 | eval_custom_logloss: 0.74824 |  0:00:36s
epoch 73 | loss: 0.23773 | eval_custom_logloss: 0.54028 |  0:00:37s
epoch 74 | loss: 0.2672  | eval_custom_logloss: 0.53099 |  0:00:37s
epoch 75 | loss: 0.24209 | eval_custom_logloss: 0.45647 |  0:00:38s
epoch 76 | loss: 0.23341 | eval_custom_logloss: 0.50524 |  0:00:38s

Early stopping occurred at epoch 76 with best_epoch = 56 and best_eval_custom_logloss = 0.42566
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5176333333333334, 'Log Loss - std': 0.06537610844609487} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.39574 | eval_custom_logloss: 7.78508 |  0:00:00s
epoch 1  | loss: 1.07867 | eval_custom_logloss: 6.99645 |  0:00:00s
epoch 2  | loss: 0.9862  | eval_custom_logloss: 7.36722 |  0:00:01s
epoch 3  | loss: 0.99196 | eval_custom_logloss: 5.77047 |  0:00:01s
epoch 4  | loss: 0.84656 | eval_custom_logloss: 4.67122 |  0:00:02s
epoch 5  | loss: 0.67553 | eval_custom_logloss: 3.34067 |  0:00:02s
epoch 6  | loss: 0.64738 | eval_custom_logloss: 4.4519  |  0:00:03s
epoch 7  | loss: 0.65877 | eval_custom_logloss: 3.05995 |  0:00:03s
epoch 8  | loss: 0.57515 | eval_custom_logloss: 1.81093 |  0:00:04s
epoch 9  | loss: 0.54383 | eval_custom_logloss: 1.98233 |  0:00:04s
epoch 10 | loss: 0.5463  | eval_custom_logloss: 1.67898 |  0:00:05s
epoch 11 | loss: 0.50752 | eval_custom_logloss: 1.44909 |  0:00:05s
epoch 12 | loss: 0.51442 | eval_custom_logloss: 1.05752 |  0:00:06s
epoch 13 | loss: 0.51106 | eval_custom_logloss: 1.07698 |  0:00:07s
epoch 14 | loss: 0.50169 | eval_custom_logloss: 0.94045 |  0:00:07s
epoch 15 | loss: 0.48921 | eval_custom_logloss: 0.93594 |  0:00:08s
epoch 16 | loss: 0.49551 | eval_custom_logloss: 0.93801 |  0:00:08s
epoch 17 | loss: 0.45708 | eval_custom_logloss: 1.06728 |  0:00:09s
epoch 18 | loss: 0.45149 | eval_custom_logloss: 1.04353 |  0:00:09s
epoch 19 | loss: 0.44776 | eval_custom_logloss: 0.90575 |  0:00:10s
epoch 20 | loss: 0.42918 | eval_custom_logloss: 0.75582 |  0:00:10s
epoch 21 | loss: 0.44983 | eval_custom_logloss: 0.79356 |  0:00:11s
epoch 22 | loss: 0.44661 | eval_custom_logloss: 0.83955 |  0:00:11s
epoch 23 | loss: 0.42789 | eval_custom_logloss: 0.86105 |  0:00:12s
epoch 24 | loss: 0.47946 | eval_custom_logloss: 0.62338 |  0:00:12s
epoch 25 | loss: 0.44145 | eval_custom_logloss: 0.84308 |  0:00:13s
epoch 26 | loss: 0.39741 | eval_custom_logloss: 0.81014 |  0:00:14s
epoch 27 | loss: 0.44144 | eval_custom_logloss: 0.58545 |  0:00:14s
epoch 28 | loss: 0.42407 | eval_custom_logloss: 0.74036 |  0:00:15s
epoch 29 | loss: 0.3845  | eval_custom_logloss: 0.61833 |  0:00:15s
epoch 30 | loss: 0.37014 | eval_custom_logloss: 0.71395 |  0:00:16s
epoch 31 | loss: 0.38394 | eval_custom_logloss: 0.65266 |  0:00:16s
epoch 32 | loss: 0.38331 | eval_custom_logloss: 0.73881 |  0:00:17s
epoch 33 | loss: 0.36426 | eval_custom_logloss: 0.65422 |  0:00:17s
epoch 34 | loss: 0.37836 | eval_custom_logloss: 0.66763 |  0:00:18s
epoch 35 | loss: 0.35683 | eval_custom_logloss: 0.81263 |  0:00:18s
epoch 36 | loss: 0.35162 | eval_custom_logloss: 0.68368 |  0:00:19s
epoch 37 | loss: 0.37344 | eval_custom_logloss: 0.6972  |  0:00:19s
epoch 38 | loss: 0.33997 | eval_custom_logloss: 0.81848 |  0:00:20s
epoch 39 | loss: 0.34153 | eval_custom_logloss: 0.72415 |  0:00:20s
epoch 40 | loss: 0.32502 | eval_custom_logloss: 0.77754 |  0:00:20s
epoch 41 | loss: 0.2984  | eval_custom_logloss: 0.76816 |  0:00:21s
epoch 42 | loss: 0.3014  | eval_custom_logloss: 0.85733 |  0:00:21s
epoch 43 | loss: 0.34526 | eval_custom_logloss: 0.78771 |  0:00:22s
epoch 44 | loss: 0.31497 | eval_custom_logloss: 0.72729 |  0:00:22s
epoch 45 | loss: 0.34304 | eval_custom_logloss: 0.70298 |  0:00:23s
epoch 46 | loss: 0.33412 | eval_custom_logloss: 0.70145 |  0:00:23s
epoch 47 | loss: 0.3322  | eval_custom_logloss: 0.60375 |  0:00:24s

Early stopping occurred at epoch 47 with best_epoch = 27 and best_eval_custom_logloss = 0.58545
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5336000000000001, 'Log Loss - std': 0.06301055467141993} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax', 'n_a': 50, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.46068 | eval_custom_logloss: 7.41518 |  0:00:00s
epoch 1  | loss: 1.10257 | eval_custom_logloss: 8.1249  |  0:00:00s
epoch 2  | loss: 0.98553 | eval_custom_logloss: 6.15976 |  0:00:01s
epoch 3  | loss: 0.86058 | eval_custom_logloss: 4.0599  |  0:00:01s
epoch 4  | loss: 0.76563 | eval_custom_logloss: 4.73988 |  0:00:02s
epoch 5  | loss: 0.77148 | eval_custom_logloss: 3.10067 |  0:00:02s
epoch 6  | loss: 0.71755 | eval_custom_logloss: 3.06585 |  0:00:02s
epoch 7  | loss: 0.65286 | eval_custom_logloss: 3.0439  |  0:00:03s
epoch 8  | loss: 0.67558 | eval_custom_logloss: 1.564   |  0:00:03s
epoch 9  | loss: 0.59349 | eval_custom_logloss: 1.95066 |  0:00:04s
epoch 10 | loss: 0.57887 | eval_custom_logloss: 1.7537  |  0:00:04s
epoch 11 | loss: 0.56213 | eval_custom_logloss: 1.07536 |  0:00:05s
epoch 12 | loss: 0.55211 | eval_custom_logloss: 1.01739 |  0:00:05s
epoch 13 | loss: 0.55572 | eval_custom_logloss: 1.35647 |  0:00:05s
epoch 14 | loss: 0.54127 | eval_custom_logloss: 1.25518 |  0:00:06s
epoch 15 | loss: 0.53181 | eval_custom_logloss: 0.87685 |  0:00:06s
epoch 16 | loss: 0.51045 | eval_custom_logloss: 0.92275 |  0:00:07s
epoch 17 | loss: 0.50376 | eval_custom_logloss: 0.85141 |  0:00:07s
epoch 18 | loss: 0.50037 | eval_custom_logloss: 0.74916 |  0:00:07s
epoch 19 | loss: 0.51038 | eval_custom_logloss: 0.6792  |  0:00:08s
epoch 20 | loss: 0.51445 | eval_custom_logloss: 0.59313 |  0:00:08s
epoch 21 | loss: 0.50163 | eval_custom_logloss: 0.59468 |  0:00:09s
epoch 22 | loss: 0.50055 | eval_custom_logloss: 0.57445 |  0:00:10s
epoch 23 | loss: 0.46824 | eval_custom_logloss: 0.79227 |  0:00:10s
epoch 24 | loss: 0.50149 | eval_custom_logloss: 0.78337 |  0:00:11s
epoch 25 | loss: 0.4728  | eval_custom_logloss: 0.59404 |  0:00:11s
epoch 26 | loss: 0.49604 | eval_custom_logloss: 0.48293 |  0:00:12s
epoch 27 | loss: 0.47462 | eval_custom_logloss: 0.62538 |  0:00:12s
epoch 28 | loss: 0.47115 | eval_custom_logloss: 0.59233 |  0:00:13s
epoch 29 | loss: 0.47587 | eval_custom_logloss: 0.54834 |  0:00:13s
epoch 30 | loss: 0.4547  | eval_custom_logloss: 0.72924 |  0:00:14s
epoch 31 | loss: 0.44037 | eval_custom_logloss: 0.66867 |  0:00:14s
epoch 32 | loss: 0.42896 | eval_custom_logloss: 0.70248 |  0:00:15s
epoch 33 | loss: 0.40766 | eval_custom_logloss: 0.50988 |  0:00:16s
epoch 34 | loss: 0.37532 | eval_custom_logloss: 0.47662 |  0:00:16s
epoch 35 | loss: 0.38    | eval_custom_logloss: 0.47613 |  0:00:17s
epoch 36 | loss: 0.39664 | eval_custom_logloss: 0.45578 |  0:00:17s
epoch 37 | loss: 0.40063 | eval_custom_logloss: 0.46126 |  0:00:18s
epoch 38 | loss: 0.39506 | eval_custom_logloss: 0.47592 |  0:00:18s
epoch 39 | loss: 0.36372 | eval_custom_logloss: 0.42803 |  0:00:19s
epoch 40 | loss: 0.38447 | eval_custom_logloss: 0.41851 |  0:00:19s
epoch 41 | loss: 0.37364 | eval_custom_logloss: 0.38281 |  0:00:20s
epoch 42 | loss: 0.35687 | eval_custom_logloss: 0.40349 |  0:00:21s
epoch 43 | loss: 0.42526 | eval_custom_logloss: 0.58296 |  0:00:21s
epoch 44 | loss: 0.41905 | eval_custom_logloss: 0.52729 |  0:00:22s
epoch 45 | loss: 0.40149 | eval_custom_logloss: 0.50122 |  0:00:22s
epoch 46 | loss: 0.39703 | eval_custom_logloss: 0.42743 |  0:00:23s
epoch 47 | loss: 0.35901 | eval_custom_logloss: 0.45079 |  0:00:23s
epoch 48 | loss: 0.33573 | eval_custom_logloss: 0.4598  |  0:00:24s
epoch 49 | loss: 0.35974 | eval_custom_logloss: 0.43768 |  0:00:24s
epoch 50 | loss: 0.34608 | eval_custom_logloss: 0.36427 |  0:00:25s
epoch 51 | loss: 0.32765 | eval_custom_logloss: 0.39778 |  0:00:26s
epoch 52 | loss: 0.36085 | eval_custom_logloss: 0.6386  |  0:00:26s
epoch 53 | loss: 0.37417 | eval_custom_logloss: 0.38599 |  0:00:27s
epoch 54 | loss: 0.33613 | eval_custom_logloss: 0.39849 |  0:00:27s
epoch 55 | loss: 0.34012 | eval_custom_logloss: 0.39262 |  0:00:27s
epoch 56 | loss: 0.30413 | eval_custom_logloss: 0.45025 |  0:00:28s
epoch 57 | loss: 0.37139 | eval_custom_logloss: 0.47922 |  0:00:28s
epoch 58 | loss: 0.37713 | eval_custom_logloss: 0.50177 |  0:00:29s
epoch 59 | loss: 0.40445 | eval_custom_logloss: 0.45704 |  0:00:29s
epoch 60 | loss: 0.39958 | eval_custom_logloss: 0.41123 |  0:00:30s
epoch 61 | loss: 0.36171 | eval_custom_logloss: 0.38645 |  0:00:30s
epoch 62 | loss: 0.32777 | eval_custom_logloss: 0.36525 |  0:00:31s
epoch 63 | loss: 0.31543 | eval_custom_logloss: 0.36062 |  0:00:31s
epoch 64 | loss: 0.33214 | eval_custom_logloss: 0.46521 |  0:00:32s
epoch 65 | loss: 0.31738 | eval_custom_logloss: 0.4268  |  0:00:32s
epoch 66 | loss: 0.30384 | eval_custom_logloss: 0.39989 |  0:00:32s
epoch 67 | loss: 0.30127 | eval_custom_logloss: 0.43004 |  0:00:33s
epoch 68 | loss: 0.2744  | eval_custom_logloss: 0.38907 |  0:00:33s
epoch 69 | loss: 0.25421 | eval_custom_logloss: 0.4816  |  0:00:34s
epoch 70 | loss: 0.27321 | eval_custom_logloss: 0.45306 |  0:00:34s
epoch 71 | loss: 0.27773 | eval_custom_logloss: 0.49063 |  0:00:35s
epoch 72 | loss: 0.28722 | eval_custom_logloss: 0.50115 |  0:00:35s
epoch 73 | loss: 0.26529 | eval_custom_logloss: 0.35424 |  0:00:36s
epoch 74 | loss: 0.30988 | eval_custom_logloss: 0.43952 |  0:00:36s
epoch 75 | loss: 0.27927 | eval_custom_logloss: 0.37249 |  0:00:37s
epoch 76 | loss: 0.28303 | eval_custom_logloss: 0.54137 |  0:00:37s
epoch 77 | loss: 0.26816 | eval_custom_logloss: 0.43564 |  0:00:37s
epoch 78 | loss: 0.28094 | eval_custom_logloss: 0.49362 |  0:00:38s
epoch 79 | loss: 0.24043 | eval_custom_logloss: 0.62289 |  0:00:38s
epoch 80 | loss: 0.2298  | eval_custom_logloss: 0.66417 |  0:00:39s
epoch 81 | loss: 0.22142 | eval_custom_logloss: 0.57916 |  0:00:39s
epoch 82 | loss: 0.21914 | eval_custom_logloss: 0.3547  |  0:00:40s
epoch 83 | loss: 0.22404 | eval_custom_logloss: 0.40626 |  0:00:40s
epoch 84 | loss: 0.21419 | eval_custom_logloss: 0.45753 |  0:00:41s
epoch 85 | loss: 0.23066 | eval_custom_logloss: 0.35147 |  0:00:41s
epoch 86 | loss: 0.22041 | eval_custom_logloss: 0.34    |  0:00:42s
epoch 87 | loss: 0.21446 | eval_custom_logloss: 0.42252 |  0:00:42s
epoch 88 | loss: 0.22159 | eval_custom_logloss: 0.52658 |  0:00:43s
epoch 89 | loss: 0.26071 | eval_custom_logloss: 0.38139 |  0:00:43s
epoch 90 | loss: 0.24242 | eval_custom_logloss: 0.41312 |  0:00:44s
epoch 91 | loss: 0.2111  | eval_custom_logloss: 0.38514 |  0:00:44s
epoch 92 | loss: 0.21608 | eval_custom_logloss: 0.38909 |  0:00:45s
epoch 93 | loss: 0.20871 | eval_custom_logloss: 0.37161 |  0:00:45s
epoch 94 | loss: 0.18562 | eval_custom_logloss: 0.2887  |  0:00:46s
epoch 95 | loss: 0.18265 | eval_custom_logloss: 0.37741 |  0:00:46s
epoch 96 | loss: 0.19019 | eval_custom_logloss: 0.39307 |  0:00:47s
epoch 97 | loss: 0.22416 | eval_custom_logloss: 0.37521 |  0:00:47s
epoch 98 | loss: 0.20701 | eval_custom_logloss: 0.45078 |  0:00:48s
epoch 99 | loss: 0.20338 | eval_custom_logloss: 0.357   |  0:00:49s
Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_eval_custom_logloss = 0.2887
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.48462000000000005, 'Log Loss - std': 0.11301515650566522} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 51 finished with value: 0.48462000000000005 and parameters: {'n_d': 50, 'n_steps': 4, 'gamma': 1.2598616209612414, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.007742068327150191, 'mask_type': 'entmax'}. Best is trial 45 with value: 2.80386.
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.82637 | eval_custom_logloss: 1.62775 |  0:00:00s
epoch 1  | loss: 1.62877 | eval_custom_logloss: 1.22909 |  0:00:01s
epoch 2  | loss: 1.3495  | eval_custom_logloss: 1.26914 |  0:00:02s
epoch 3  | loss: 1.21637 | eval_custom_logloss: 0.89959 |  0:00:03s
epoch 4  | loss: 1.30555 | eval_custom_logloss: 0.92416 |  0:00:03s
epoch 5  | loss: 1.07836 | eval_custom_logloss: 0.88118 |  0:00:04s
epoch 6  | loss: 1.02458 | eval_custom_logloss: 0.98487 |  0:00:05s
epoch 7  | loss: 0.84582 | eval_custom_logloss: 0.73502 |  0:00:06s
epoch 8  | loss: 0.84468 | eval_custom_logloss: 0.67888 |  0:00:06s
epoch 9  | loss: 0.802   | eval_custom_logloss: 0.77366 |  0:00:07s
epoch 10 | loss: 0.87341 | eval_custom_logloss: 1.23383 |  0:00:08s
epoch 11 | loss: 1.31726 | eval_custom_logloss: 1.06285 |  0:00:09s
epoch 12 | loss: 0.89736 | eval_custom_logloss: 0.7719  |  0:00:10s
epoch 13 | loss: 0.73423 | eval_custom_logloss: 0.84152 |  0:00:10s
epoch 14 | loss: 0.73923 | eval_custom_logloss: 0.73374 |  0:00:11s
epoch 15 | loss: 0.72917 | eval_custom_logloss: 0.60779 |  0:00:12s
epoch 16 | loss: 0.68253 | eval_custom_logloss: 0.61424 |  0:00:13s
epoch 17 | loss: 0.69478 | eval_custom_logloss: 0.63922 |  0:00:13s
epoch 18 | loss: 0.65336 | eval_custom_logloss: 0.59841 |  0:00:14s
epoch 19 | loss: 0.62369 | eval_custom_logloss: 0.59393 |  0:00:15s
epoch 20 | loss: 0.63909 | eval_custom_logloss: 0.58683 |  0:00:16s
epoch 21 | loss: 0.59926 | eval_custom_logloss: 0.59746 |  0:00:16s
epoch 22 | loss: 0.6049  | eval_custom_logloss: 0.59817 |  0:00:17s
epoch 23 | loss: 0.59773 | eval_custom_logloss: 0.61904 |  0:00:18s
epoch 24 | loss: 0.57019 | eval_custom_logloss: 0.56559 |  0:00:19s
epoch 25 | loss: 0.58951 | eval_custom_logloss: 0.55969 |  0:00:19s
epoch 26 | loss: 0.56016 | eval_custom_logloss: 0.5241  |  0:00:20s
epoch 27 | loss: 0.57682 | eval_custom_logloss: 0.54195 |  0:00:21s
epoch 28 | loss: 0.53686 | eval_custom_logloss: 0.53058 |  0:00:22s
epoch 29 | loss: 0.56555 | eval_custom_logloss: 0.5639  |  0:00:22s
epoch 30 | loss: 0.59468 | eval_custom_logloss: 0.57165 |  0:00:23s
epoch 31 | loss: 0.58568 | eval_custom_logloss: 0.56095 |  0:00:24s
epoch 32 | loss: 0.59113 | eval_custom_logloss: 0.52366 |  0:00:25s
epoch 33 | loss: 0.54101 | eval_custom_logloss: 0.53893 |  0:00:25s
epoch 34 | loss: 0.56528 | eval_custom_logloss: 0.57378 |  0:00:26s
epoch 35 | loss: 0.54828 | eval_custom_logloss: 0.52639 |  0:00:27s
epoch 36 | loss: 0.54593 | eval_custom_logloss: 0.55566 |  0:00:28s
epoch 37 | loss: 0.53604 | eval_custom_logloss: 0.51162 |  0:00:28s
epoch 38 | loss: 0.53335 | eval_custom_logloss: 0.51295 |  0:00:29s
epoch 39 | loss: 0.51861 | eval_custom_logloss: 0.53144 |  0:00:30s
epoch 40 | loss: 0.54059 | eval_custom_logloss: 0.53316 |  0:00:31s
epoch 41 | loss: 0.54828 | eval_custom_logloss: 0.52525 |  0:00:32s
epoch 42 | loss: 0.51035 | eval_custom_logloss: 0.49764 |  0:00:32s
epoch 43 | loss: 0.52878 | eval_custom_logloss: 0.51757 |  0:00:33s
epoch 44 | loss: 0.51433 | eval_custom_logloss: 0.51559 |  0:00:34s
epoch 45 | loss: 0.51417 | eval_custom_logloss: 0.53362 |  0:00:35s
epoch 46 | loss: 0.52238 | eval_custom_logloss: 0.50753 |  0:00:36s
epoch 47 | loss: 0.51634 | eval_custom_logloss: 0.54822 |  0:00:36s
epoch 48 | loss: 0.53029 | eval_custom_logloss: 0.53992 |  0:00:37s
epoch 49 | loss: 0.50999 | eval_custom_logloss: 0.49456 |  0:00:38s
epoch 50 | loss: 0.54063 | eval_custom_logloss: 0.49984 |  0:00:39s
epoch 51 | loss: 0.54288 | eval_custom_logloss: 0.49582 |  0:00:39s
epoch 52 | loss: 0.53488 | eval_custom_logloss: 0.49157 |  0:00:40s
epoch 53 | loss: 0.5217  | eval_custom_logloss: 0.47988 |  0:00:41s
epoch 54 | loss: 0.55811 | eval_custom_logloss: 0.49348 |  0:00:42s
epoch 55 | loss: 0.51031 | eval_custom_logloss: 0.46784 |  0:00:42s
epoch 56 | loss: 0.48002 | eval_custom_logloss: 0.47245 |  0:00:43s
epoch 57 | loss: 0.47827 | eval_custom_logloss: 0.46755 |  0:00:44s
epoch 58 | loss: 0.49406 | eval_custom_logloss: 0.4832  |  0:00:45s
epoch 59 | loss: 0.49661 | eval_custom_logloss: 0.45387 |  0:00:45s
epoch 60 | loss: 0.49209 | eval_custom_logloss: 0.4643  |  0:00:46s
epoch 61 | loss: 0.4885  | eval_custom_logloss: 0.44815 |  0:00:47s
epoch 62 | loss: 0.47496 | eval_custom_logloss: 0.45467 |  0:00:48s
epoch 63 | loss: 0.47145 | eval_custom_logloss: 0.46589 |  0:00:49s
epoch 64 | loss: 0.45036 | eval_custom_logloss: 0.44497 |  0:00:49s
epoch 65 | loss: 0.45364 | eval_custom_logloss: 0.43804 |  0:00:50s
epoch 66 | loss: 0.45777 | eval_custom_logloss: 0.45751 |  0:00:51s
epoch 67 | loss: 0.44231 | eval_custom_logloss: 0.4385  |  0:00:52s
epoch 68 | loss: 0.44233 | eval_custom_logloss: 0.4621  |  0:00:53s
epoch 69 | loss: 0.42426 | eval_custom_logloss: 0.44148 |  0:00:53s
epoch 70 | loss: 0.42577 | eval_custom_logloss: 0.42943 |  0:00:54s
epoch 71 | loss: 0.41223 | eval_custom_logloss: 0.43584 |  0:00:55s
epoch 72 | loss: 0.46696 | eval_custom_logloss: 0.47753 |  0:00:56s
epoch 73 | loss: 0.4575  | eval_custom_logloss: 0.45092 |  0:00:57s
epoch 74 | loss: 0.41137 | eval_custom_logloss: 0.47035 |  0:00:58s
epoch 75 | loss: 0.43176 | eval_custom_logloss: 0.45733 |  0:00:59s
epoch 76 | loss: 0.43218 | eval_custom_logloss: 0.44373 |  0:00:59s
epoch 77 | loss: 0.44226 | eval_custom_logloss: 0.43171 |  0:01:00s
epoch 78 | loss: 0.41213 | eval_custom_logloss: 0.47546 |  0:01:01s
epoch 79 | loss: 0.39055 | eval_custom_logloss: 0.43455 |  0:01:02s
epoch 80 | loss: 0.40148 | eval_custom_logloss: 0.41426 |  0:01:02s
epoch 81 | loss: 0.39879 | eval_custom_logloss: 0.43171 |  0:01:03s
epoch 82 | loss: 0.40429 | eval_custom_logloss: 0.439   |  0:01:04s
epoch 83 | loss: 0.38352 | eval_custom_logloss: 0.44622 |  0:01:05s
epoch 84 | loss: 0.40093 | eval_custom_logloss: 0.4627  |  0:01:05s
epoch 85 | loss: 0.37994 | eval_custom_logloss: 0.45428 |  0:01:06s
epoch 86 | loss: 0.3798  | eval_custom_logloss: 0.42607 |  0:01:07s
epoch 87 | loss: 0.41794 | eval_custom_logloss: 0.46365 |  0:01:08s
epoch 88 | loss: 0.39844 | eval_custom_logloss: 0.43844 |  0:01:09s
epoch 89 | loss: 0.3754  | eval_custom_logloss: 0.44209 |  0:01:09s
epoch 90 | loss: 0.36768 | eval_custom_logloss: 0.41819 |  0:01:10s
epoch 91 | loss: 0.37837 | eval_custom_logloss: 0.41419 |  0:01:11s
epoch 92 | loss: 0.37998 | eval_custom_logloss: 0.44106 |  0:01:12s
epoch 93 | loss: 0.41929 | eval_custom_logloss: 0.57159 |  0:01:12s
epoch 94 | loss: 0.42252 | eval_custom_logloss: 0.42893 |  0:01:13s
epoch 95 | loss: 0.39446 | eval_custom_logloss: 0.42781 |  0:01:14s
epoch 96 | loss: 0.37047 | eval_custom_logloss: 0.46631 |  0:01:15s
epoch 97 | loss: 0.38062 | eval_custom_logloss: 0.44849 |  0:01:15s
epoch 98 | loss: 0.37501 | eval_custom_logloss: 0.43113 |  0:01:16s
epoch 99 | loss: 0.35068 | eval_custom_logloss: 0.42528 |  0:01:17s
Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_eval_custom_logloss = 0.41419
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.4142, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.70284 | eval_custom_logloss: 1.27307 |  0:00:00s
epoch 1  | loss: 1.55222 | eval_custom_logloss: 1.50167 |  0:00:01s
epoch 2  | loss: 1.33792 | eval_custom_logloss: 1.20269 |  0:00:02s
epoch 3  | loss: 1.04466 | eval_custom_logloss: 1.19441 |  0:00:03s
epoch 4  | loss: 1.09561 | eval_custom_logloss: 1.22541 |  0:00:03s
epoch 5  | loss: 1.0335  | eval_custom_logloss: 1.52269 |  0:00:04s
epoch 6  | loss: 1.17778 | eval_custom_logloss: 1.43884 |  0:00:05s
epoch 7  | loss: 1.12266 | eval_custom_logloss: 1.38602 |  0:00:06s
epoch 8  | loss: 1.01551 | eval_custom_logloss: 0.80514 |  0:00:06s
epoch 9  | loss: 0.87275 | eval_custom_logloss: 0.8676  |  0:00:07s
epoch 10 | loss: 0.76996 | eval_custom_logloss: 0.85398 |  0:00:08s
epoch 11 | loss: 0.69705 | eval_custom_logloss: 0.7454  |  0:00:09s
epoch 12 | loss: 0.71773 | eval_custom_logloss: 0.81994 |  0:00:09s
epoch 13 | loss: 0.79339 | eval_custom_logloss: 0.74386 |  0:00:10s
epoch 14 | loss: 0.721   | eval_custom_logloss: 0.84298 |  0:00:11s
epoch 15 | loss: 0.64898 | eval_custom_logloss: 0.7507  |  0:00:12s
epoch 16 | loss: 0.67455 | eval_custom_logloss: 1.04481 |  0:00:12s
epoch 17 | loss: 0.71873 | eval_custom_logloss: 0.8356  |  0:00:13s
epoch 18 | loss: 0.79069 | eval_custom_logloss: 0.945   |  0:00:14s
epoch 19 | loss: 0.77974 | eval_custom_logloss: 0.77363 |  0:00:15s
epoch 20 | loss: 0.70691 | eval_custom_logloss: 0.78106 |  0:00:15s
epoch 21 | loss: 0.71001 | eval_custom_logloss: 0.71614 |  0:00:16s
epoch 22 | loss: 0.66561 | eval_custom_logloss: 0.73831 |  0:00:17s
epoch 23 | loss: 0.66799 | eval_custom_logloss: 0.77171 |  0:00:18s
epoch 24 | loss: 0.66788 | eval_custom_logloss: 0.76898 |  0:00:19s
epoch 25 | loss: 0.73061 | eval_custom_logloss: 0.7979  |  0:00:19s
epoch 26 | loss: 0.74127 | eval_custom_logloss: 0.74166 |  0:00:20s
epoch 27 | loss: 0.62127 | eval_custom_logloss: 0.66285 |  0:00:21s
epoch 28 | loss: 0.63445 | eval_custom_logloss: 0.81665 |  0:00:22s
epoch 29 | loss: 0.6598  | eval_custom_logloss: 0.78964 |  0:00:22s
epoch 30 | loss: 0.6569  | eval_custom_logloss: 0.93989 |  0:00:23s
epoch 31 | loss: 0.63815 | eval_custom_logloss: 0.77712 |  0:00:24s
epoch 32 | loss: 0.6132  | eval_custom_logloss: 0.74577 |  0:00:25s
epoch 33 | loss: 0.69868 | eval_custom_logloss: 0.84522 |  0:00:26s
epoch 34 | loss: 0.68762 | eval_custom_logloss: 0.75554 |  0:00:26s
epoch 35 | loss: 0.64249 | eval_custom_logloss: 0.76687 |  0:00:27s
epoch 36 | loss: 0.68488 | eval_custom_logloss: 0.84827 |  0:00:28s
epoch 37 | loss: 0.73107 | eval_custom_logloss: 0.81813 |  0:00:29s
epoch 38 | loss: 0.65481 | eval_custom_logloss: 0.78239 |  0:00:29s
epoch 39 | loss: 0.67964 | eval_custom_logloss: 0.72521 |  0:00:30s
epoch 40 | loss: 0.66335 | eval_custom_logloss: 0.78913 |  0:00:31s
epoch 41 | loss: 0.65929 | eval_custom_logloss: 0.80007 |  0:00:32s
epoch 42 | loss: 0.66748 | eval_custom_logloss: 0.84236 |  0:00:32s
epoch 43 | loss: 0.65046 | eval_custom_logloss: 0.67349 |  0:00:33s
epoch 44 | loss: 0.65284 | eval_custom_logloss: 0.70413 |  0:00:34s
epoch 45 | loss: 0.6515  | eval_custom_logloss: 0.71401 |  0:00:35s
epoch 46 | loss: 0.66725 | eval_custom_logloss: 0.76124 |  0:00:36s
epoch 47 | loss: 0.66711 | eval_custom_logloss: 0.71512 |  0:00:36s

Early stopping occurred at epoch 47 with best_epoch = 27 and best_eval_custom_logloss = 0.66285
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5385500000000001, 'Log Loss - std': 0.12435000000000002} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.73642 | eval_custom_logloss: 1.10672 |  0:00:00s
epoch 1  | loss: 1.61478 | eval_custom_logloss: 1.39733 |  0:00:01s
epoch 2  | loss: 1.4576  | eval_custom_logloss: 1.11701 |  0:00:02s
epoch 3  | loss: 1.26323 | eval_custom_logloss: 0.96426 |  0:00:03s
epoch 4  | loss: 1.18933 | eval_custom_logloss: 0.95901 |  0:00:03s
epoch 5  | loss: 1.08689 | eval_custom_logloss: 0.98685 |  0:00:04s
epoch 6  | loss: 0.91705 | eval_custom_logloss: 0.79086 |  0:00:05s
epoch 7  | loss: 0.90537 | eval_custom_logloss: 0.84958 |  0:00:06s
epoch 8  | loss: 1.10192 | eval_custom_logloss: 0.84617 |  0:00:07s
epoch 9  | loss: 0.8951  | eval_custom_logloss: 1.27172 |  0:00:07s
epoch 10 | loss: 2.01433 | eval_custom_logloss: 1.47482 |  0:00:08s
epoch 11 | loss: 1.09244 | eval_custom_logloss: 0.79727 |  0:00:09s
epoch 12 | loss: 0.82741 | eval_custom_logloss: 0.83238 |  0:00:10s
epoch 13 | loss: 0.84981 | eval_custom_logloss: 0.87798 |  0:00:10s
epoch 14 | loss: 0.82957 | eval_custom_logloss: 0.87995 |  0:00:11s
epoch 15 | loss: 0.75182 | eval_custom_logloss: 0.77491 |  0:00:12s
epoch 16 | loss: 0.73413 | eval_custom_logloss: 0.73812 |  0:00:13s
epoch 17 | loss: 0.70823 | eval_custom_logloss: 0.76462 |  0:00:13s
epoch 18 | loss: 0.71729 | eval_custom_logloss: 0.75387 |  0:00:14s
epoch 19 | loss: 0.75504 | eval_custom_logloss: 0.69153 |  0:00:15s
epoch 20 | loss: 0.74782 | eval_custom_logloss: 0.84331 |  0:00:16s
epoch 21 | loss: 0.97734 | eval_custom_logloss: 0.91539 |  0:00:16s
epoch 22 | loss: 0.83875 | eval_custom_logloss: 0.71824 |  0:00:17s
epoch 23 | loss: 0.78227 | eval_custom_logloss: 0.86134 |  0:00:18s
epoch 24 | loss: 0.74621 | eval_custom_logloss: 0.7169  |  0:00:19s
epoch 25 | loss: 0.77034 | eval_custom_logloss: 0.73681 |  0:00:19s
epoch 26 | loss: 0.69767 | eval_custom_logloss: 0.71871 |  0:00:20s
epoch 27 | loss: 0.68243 | eval_custom_logloss: 0.68557 |  0:00:21s
epoch 28 | loss: 0.68905 | eval_custom_logloss: 0.69185 |  0:00:22s
epoch 29 | loss: 0.68154 | eval_custom_logloss: 0.7059  |  0:00:22s
epoch 30 | loss: 0.70768 | eval_custom_logloss: 0.66349 |  0:00:23s
epoch 31 | loss: 0.68784 | eval_custom_logloss: 0.67987 |  0:00:24s
epoch 32 | loss: 0.68525 | eval_custom_logloss: 0.63562 |  0:00:25s
epoch 33 | loss: 0.67836 | eval_custom_logloss: 0.70009 |  0:00:25s
epoch 34 | loss: 0.67887 | eval_custom_logloss: 0.64205 |  0:00:26s
epoch 35 | loss: 0.65249 | eval_custom_logloss: 0.68057 |  0:00:27s
epoch 36 | loss: 0.6423  | eval_custom_logloss: 0.67196 |  0:00:28s
epoch 37 | loss: 0.63437 | eval_custom_logloss: 0.6355  |  0:00:28s
epoch 38 | loss: 0.60931 | eval_custom_logloss: 0.668   |  0:00:29s
epoch 39 | loss: 0.62194 | eval_custom_logloss: 0.69788 |  0:00:30s
epoch 40 | loss: 0.63299 | eval_custom_logloss: 0.66148 |  0:00:31s
epoch 41 | loss: 0.66595 | eval_custom_logloss: 0.6795  |  0:00:31s
epoch 42 | loss: 0.64459 | eval_custom_logloss: 0.62672 |  0:00:32s
epoch 43 | loss: 0.63946 | eval_custom_logloss: 0.62692 |  0:00:33s
epoch 44 | loss: 0.6141  | eval_custom_logloss: 0.64359 |  0:00:34s
epoch 45 | loss: 0.6247  | eval_custom_logloss: 0.61404 |  0:00:34s
epoch 46 | loss: 0.58509 | eval_custom_logloss: 0.64486 |  0:00:35s
epoch 47 | loss: 0.58606 | eval_custom_logloss: 0.56384 |  0:00:36s
epoch 48 | loss: 0.59678 | eval_custom_logloss: 0.62081 |  0:00:37s
epoch 49 | loss: 0.60682 | eval_custom_logloss: 0.64286 |  0:00:37s
epoch 50 | loss: 0.60868 | eval_custom_logloss: 0.63588 |  0:00:38s
epoch 51 | loss: 0.57592 | eval_custom_logloss: 0.64861 |  0:00:39s
epoch 52 | loss: 0.57224 | eval_custom_logloss: 0.57297 |  0:00:40s
epoch 53 | loss: 0.57154 | eval_custom_logloss: 0.59868 |  0:00:40s
epoch 54 | loss: 0.58403 | eval_custom_logloss: 0.56956 |  0:00:41s
epoch 55 | loss: 0.57233 | eval_custom_logloss: 0.65445 |  0:00:42s
epoch 56 | loss: 0.55788 | eval_custom_logloss: 0.57145 |  0:00:43s
epoch 57 | loss: 0.53164 | eval_custom_logloss: 0.55062 |  0:00:44s
epoch 58 | loss: 0.54504 | eval_custom_logloss: 0.61225 |  0:00:44s
epoch 59 | loss: 0.59561 | eval_custom_logloss: 0.58605 |  0:00:45s
epoch 60 | loss: 0.58021 | eval_custom_logloss: 0.58653 |  0:00:46s
epoch 61 | loss: 0.52869 | eval_custom_logloss: 0.56515 |  0:00:47s
epoch 62 | loss: 0.56273 | eval_custom_logloss: 0.599   |  0:00:47s
epoch 63 | loss: 0.55144 | eval_custom_logloss: 0.58327 |  0:00:48s
epoch 64 | loss: 0.54869 | eval_custom_logloss: 0.59007 |  0:00:49s
epoch 65 | loss: 0.53049 | eval_custom_logloss: 0.55225 |  0:00:50s
epoch 66 | loss: 0.52179 | eval_custom_logloss: 0.62839 |  0:00:50s
epoch 67 | loss: 0.5622  | eval_custom_logloss: 0.54582 |  0:00:51s
epoch 68 | loss: 0.56432 | eval_custom_logloss: 0.61737 |  0:00:52s
epoch 69 | loss: 0.54217 | eval_custom_logloss: 0.56202 |  0:00:53s
epoch 70 | loss: 0.53858 | eval_custom_logloss: 0.54925 |  0:00:53s
epoch 71 | loss: 0.53034 | eval_custom_logloss: 0.55161 |  0:00:54s
epoch 72 | loss: 0.54145 | eval_custom_logloss: 0.59413 |  0:00:55s
epoch 73 | loss: 0.5383  | eval_custom_logloss: 0.57386 |  0:00:56s
epoch 74 | loss: 0.52287 | eval_custom_logloss: 0.53676 |  0:00:56s
epoch 75 | loss: 0.51358 | eval_custom_logloss: 0.54871 |  0:00:57s
epoch 76 | loss: 0.53252 | eval_custom_logloss: 0.53593 |  0:00:58s
epoch 77 | loss: 0.50714 | eval_custom_logloss: 0.54431 |  0:00:59s
epoch 78 | loss: 0.50518 | eval_custom_logloss: 0.5514  |  0:00:59s
epoch 79 | loss: 0.52463 | eval_custom_logloss: 0.5239  |  0:01:00s
epoch 80 | loss: 0.50297 | eval_custom_logloss: 0.52753 |  0:01:01s
epoch 81 | loss: 0.51    | eval_custom_logloss: 0.52898 |  0:01:02s
epoch 82 | loss: 0.48163 | eval_custom_logloss: 0.52575 |  0:01:02s
epoch 83 | loss: 0.48512 | eval_custom_logloss: 0.53112 |  0:01:03s
epoch 84 | loss: 0.48486 | eval_custom_logloss: 0.53197 |  0:01:04s
epoch 85 | loss: 0.50596 | eval_custom_logloss: 0.54975 |  0:01:05s
epoch 86 | loss: 0.45498 | eval_custom_logloss: 0.50544 |  0:01:05s
epoch 87 | loss: 0.47851 | eval_custom_logloss: 0.57117 |  0:01:06s
epoch 88 | loss: 0.47809 | eval_custom_logloss: 0.4938  |  0:01:07s
epoch 89 | loss: 0.46562 | eval_custom_logloss: 0.5457  |  0:01:08s
epoch 90 | loss: 0.49179 | eval_custom_logloss: 0.55761 |  0:01:08s
epoch 91 | loss: 0.50728 | eval_custom_logloss: 0.51231 |  0:01:09s
epoch 92 | loss: 0.47962 | eval_custom_logloss: 0.49811 |  0:01:10s
epoch 93 | loss: 0.47883 | eval_custom_logloss: 0.51326 |  0:01:11s
epoch 94 | loss: 0.45583 | eval_custom_logloss: 0.48125 |  0:01:12s
epoch 95 | loss: 0.46583 | eval_custom_logloss: 0.48247 |  0:01:12s
epoch 96 | loss: 0.44327 | eval_custom_logloss: 0.53183 |  0:01:13s
epoch 97 | loss: 0.44412 | eval_custom_logloss: 0.51586 |  0:01:14s
epoch 98 | loss: 0.45393 | eval_custom_logloss: 0.50495 |  0:01:15s
epoch 99 | loss: 0.45593 | eval_custom_logloss: 0.50471 |  0:01:15s
Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_eval_custom_logloss = 0.48125
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5194333333333334, 'Log Loss - std': 0.10506906723145919} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.80108 | eval_custom_logloss: 1.3882  |  0:00:00s
epoch 1  | loss: 1.62013 | eval_custom_logloss: 1.30401 |  0:00:01s
epoch 2  | loss: 1.42127 | eval_custom_logloss: 1.14833 |  0:00:02s
epoch 3  | loss: 1.17537 | eval_custom_logloss: 1.22539 |  0:00:02s
epoch 4  | loss: 1.70449 | eval_custom_logloss: 1.7482  |  0:00:03s
epoch 5  | loss: 1.20434 | eval_custom_logloss: 0.9853  |  0:00:04s
epoch 6  | loss: 1.01544 | eval_custom_logloss: 1.1103  |  0:00:05s
epoch 7  | loss: 0.95105 | eval_custom_logloss: 0.99187 |  0:00:05s
epoch 8  | loss: 0.86172 | eval_custom_logloss: 0.86429 |  0:00:06s
epoch 9  | loss: 0.95994 | eval_custom_logloss: 0.89846 |  0:00:07s
epoch 10 | loss: 0.91665 | eval_custom_logloss: 0.89107 |  0:00:08s
epoch 11 | loss: 1.02639 | eval_custom_logloss: 0.79831 |  0:00:09s
epoch 12 | loss: 1.0331  | eval_custom_logloss: 0.7802  |  0:00:09s
epoch 13 | loss: 0.82804 | eval_custom_logloss: 0.76152 |  0:00:10s
epoch 14 | loss: 0.70889 | eval_custom_logloss: 0.71704 |  0:00:11s
epoch 15 | loss: 0.67014 | eval_custom_logloss: 0.71862 |  0:00:12s
epoch 16 | loss: 0.74117 | eval_custom_logloss: 0.74293 |  0:00:13s
epoch 17 | loss: 0.76207 | eval_custom_logloss: 0.82314 |  0:00:13s
epoch 18 | loss: 0.76009 | eval_custom_logloss: 0.87031 |  0:00:14s
epoch 19 | loss: 0.8062  | eval_custom_logloss: 0.76463 |  0:00:15s
epoch 20 | loss: 0.73395 | eval_custom_logloss: 0.78997 |  0:00:16s
epoch 21 | loss: 0.69445 | eval_custom_logloss: 0.73238 |  0:00:16s
epoch 22 | loss: 0.66344 | eval_custom_logloss: 0.68544 |  0:00:17s
epoch 23 | loss: 0.68785 | eval_custom_logloss: 0.72087 |  0:00:18s
epoch 24 | loss: 0.65358 | eval_custom_logloss: 0.67194 |  0:00:19s
epoch 25 | loss: 0.62989 | eval_custom_logloss: 0.6903  |  0:00:20s
epoch 26 | loss: 0.61319 | eval_custom_logloss: 0.69536 |  0:00:20s
epoch 27 | loss: 0.61235 | eval_custom_logloss: 0.71062 |  0:00:21s
epoch 28 | loss: 0.64471 | eval_custom_logloss: 0.79476 |  0:00:22s
epoch 29 | loss: 0.65503 | eval_custom_logloss: 0.74654 |  0:00:23s
epoch 30 | loss: 0.69529 | eval_custom_logloss: 0.7513  |  0:00:23s
epoch 31 | loss: 0.65649 | eval_custom_logloss: 0.73216 |  0:00:24s
epoch 32 | loss: 0.64579 | eval_custom_logloss: 0.74265 |  0:00:25s
epoch 33 | loss: 0.65463 | eval_custom_logloss: 0.73267 |  0:00:26s
epoch 34 | loss: 0.67118 | eval_custom_logloss: 0.70866 |  0:00:27s
epoch 35 | loss: 0.64839 | eval_custom_logloss: 0.76369 |  0:00:27s
epoch 36 | loss: 0.67865 | eval_custom_logloss: 0.74486 |  0:00:28s
epoch 37 | loss: 0.67614 | eval_custom_logloss: 0.71355 |  0:00:29s
epoch 38 | loss: 0.64292 | eval_custom_logloss: 0.65354 |  0:00:30s
epoch 39 | loss: 0.63124 | eval_custom_logloss: 0.65204 |  0:00:30s
epoch 40 | loss: 0.62775 | eval_custom_logloss: 0.6134  |  0:00:31s
epoch 41 | loss: 0.58294 | eval_custom_logloss: 0.58266 |  0:00:32s
epoch 42 | loss: 0.58986 | eval_custom_logloss: 0.63131 |  0:00:33s
epoch 43 | loss: 0.59204 | eval_custom_logloss: 0.63648 |  0:00:34s
epoch 44 | loss: 0.59337 | eval_custom_logloss: 0.66173 |  0:00:34s
epoch 45 | loss: 0.63725 | eval_custom_logloss: 0.59878 |  0:00:35s
epoch 46 | loss: 0.53996 | eval_custom_logloss: 0.58383 |  0:00:36s
epoch 47 | loss: 0.54776 | eval_custom_logloss: 0.60669 |  0:00:37s
epoch 48 | loss: 0.59081 | eval_custom_logloss: 0.61696 |  0:00:37s
epoch 49 | loss: 0.57822 | eval_custom_logloss: 0.58777 |  0:00:38s
epoch 50 | loss: 0.54852 | eval_custom_logloss: 0.62278 |  0:00:39s
epoch 51 | loss: 0.55991 | eval_custom_logloss: 0.62347 |  0:00:40s
epoch 52 | loss: 0.55961 | eval_custom_logloss: 0.64061 |  0:00:40s
epoch 53 | loss: 0.57096 | eval_custom_logloss: 0.65577 |  0:00:41s
epoch 54 | loss: 0.63496 | eval_custom_logloss: 0.63952 |  0:00:42s
epoch 55 | loss: 0.59487 | eval_custom_logloss: 0.62154 |  0:00:43s
epoch 56 | loss: 0.57593 | eval_custom_logloss: 0.61458 |  0:00:44s
epoch 57 | loss: 0.58559 | eval_custom_logloss: 0.60035 |  0:00:44s
epoch 58 | loss: 0.56532 | eval_custom_logloss: 0.62928 |  0:00:45s
epoch 59 | loss: 0.59185 | eval_custom_logloss: 0.62077 |  0:00:46s
epoch 60 | loss: 0.56698 | eval_custom_logloss: 0.61249 |  0:00:47s
epoch 61 | loss: 0.58706 | eval_custom_logloss: 0.66392 |  0:00:47s

Early stopping occurred at epoch 61 with best_epoch = 41 and best_eval_custom_logloss = 0.58266
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.53525, 'Log Loss - std': 0.0950270093184038} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax', 'n_a': 26, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 2.87257 | eval_custom_logloss: 1.44769 |  0:00:00s
epoch 1  | loss: 1.68666 | eval_custom_logloss: 1.54966 |  0:00:01s
epoch 2  | loss: 1.31492 | eval_custom_logloss: 1.30824 |  0:00:02s
epoch 3  | loss: 1.07377 | eval_custom_logloss: 0.8235  |  0:00:03s
epoch 4  | loss: 0.91564 | eval_custom_logloss: 0.72152 |  0:00:03s
epoch 5  | loss: 0.94355 | eval_custom_logloss: 0.95458 |  0:00:04s
epoch 6  | loss: 0.92588 | eval_custom_logloss: 0.67969 |  0:00:05s
epoch 7  | loss: 0.85208 | eval_custom_logloss: 0.75548 |  0:00:06s
epoch 8  | loss: 0.84067 | eval_custom_logloss: 0.8001  |  0:00:07s
epoch 9  | loss: 0.78313 | eval_custom_logloss: 0.67999 |  0:00:07s
epoch 10 | loss: 0.80699 | eval_custom_logloss: 0.81802 |  0:00:08s
epoch 11 | loss: 1.09727 | eval_custom_logloss: 1.43166 |  0:00:09s
epoch 12 | loss: 1.12667 | eval_custom_logloss: 0.88169 |  0:00:10s
epoch 13 | loss: 0.81838 | eval_custom_logloss: 0.73749 |  0:00:10s
epoch 14 | loss: 0.86519 | eval_custom_logloss: 0.79575 |  0:00:11s
epoch 15 | loss: 0.79446 | eval_custom_logloss: 0.73654 |  0:00:12s
epoch 16 | loss: 0.71405 | eval_custom_logloss: 0.6356  |  0:00:13s
epoch 17 | loss: 0.67852 | eval_custom_logloss: 0.61524 |  0:00:14s
epoch 18 | loss: 0.71689 | eval_custom_logloss: 0.66442 |  0:00:15s
epoch 19 | loss: 0.72176 | eval_custom_logloss: 0.61584 |  0:00:15s
epoch 20 | loss: 0.68122 | eval_custom_logloss: 0.63637 |  0:00:16s
epoch 21 | loss: 0.63527 | eval_custom_logloss: 0.58516 |  0:00:17s
epoch 22 | loss: 0.70655 | eval_custom_logloss: 0.59797 |  0:00:18s
epoch 23 | loss: 0.67057 | eval_custom_logloss: 0.64022 |  0:00:18s
epoch 24 | loss: 0.69287 | eval_custom_logloss: 0.55111 |  0:00:19s
epoch 25 | loss: 0.65092 | eval_custom_logloss: 0.552   |  0:00:20s
epoch 26 | loss: 0.6284  | eval_custom_logloss: 0.49462 |  0:00:21s
epoch 27 | loss: 0.59986 | eval_custom_logloss: 0.49898 |  0:00:22s
epoch 28 | loss: 0.59086 | eval_custom_logloss: 0.50727 |  0:00:22s
epoch 29 | loss: 0.58224 | eval_custom_logloss: 0.54109 |  0:00:23s
epoch 30 | loss: 0.58445 | eval_custom_logloss: 0.55405 |  0:00:24s
epoch 31 | loss: 0.5529  | eval_custom_logloss: 0.52931 |  0:00:25s
epoch 32 | loss: 0.58601 | eval_custom_logloss: 0.49704 |  0:00:25s
epoch 33 | loss: 0.58048 | eval_custom_logloss: 0.45354 |  0:00:26s
epoch 34 | loss: 0.55438 | eval_custom_logloss: 0.46769 |  0:00:27s
epoch 35 | loss: 0.58376 | eval_custom_logloss: 0.54893 |  0:00:28s
epoch 36 | loss: 0.59197 | eval_custom_logloss: 0.49618 |  0:00:29s
epoch 37 | loss: 0.57068 | eval_custom_logloss: 0.49936 |  0:00:29s
epoch 38 | loss: 0.58611 | eval_custom_logloss: 0.53203 |  0:00:30s
epoch 39 | loss: 0.59463 | eval_custom_logloss: 0.53271 |  0:00:31s
epoch 40 | loss: 0.57037 | eval_custom_logloss: 0.55718 |  0:00:32s
epoch 41 | loss: 0.61937 | eval_custom_logloss: 0.54376 |  0:00:32s
epoch 42 | loss: 0.58277 | eval_custom_logloss: 0.52085 |  0:00:33s
epoch 43 | loss: 0.63145 | eval_custom_logloss: 0.60021 |  0:00:34s
epoch 44 | loss: 0.64616 | eval_custom_logloss: 0.60806 |  0:00:35s
epoch 45 | loss: 0.61903 | eval_custom_logloss: 0.56991 |  0:00:36s
epoch 46 | loss: 0.60708 | eval_custom_logloss: 0.56421 |  0:00:36s
epoch 47 | loss: 0.60722 | eval_custom_logloss: 0.53021 |  0:00:37s
epoch 48 | loss: 0.58793 | eval_custom_logloss: 0.51959 |  0:00:38s
epoch 49 | loss: 0.58866 | eval_custom_logloss: 0.49203 |  0:00:39s
epoch 50 | loss: 0.55942 | eval_custom_logloss: 0.47336 |  0:00:39s
epoch 51 | loss: 0.54843 | eval_custom_logloss: 0.47321 |  0:00:40s
epoch 52 | loss: 0.53465 | eval_custom_logloss: 0.45746 |  0:00:41s
epoch 53 | loss: 0.54777 | eval_custom_logloss: 0.45397 |  0:00:42s

Early stopping occurred at epoch 53 with best_epoch = 33 and best_eval_custom_logloss = 0.45354
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is False b4 loss saving
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 0.5189, 'Log Loss - std': 0.09106808442039395} 
 

Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 52 finished with value: 0.5189 and parameters: {'n_d': 26, 'n_steps': 10, 'gamma': 1.1473192933159277, 'cat_emb_dim': 3, 'n_independent': 4, 'n_shared': 3, 'momentum': 0.3006544593279018, 'mask_type': 'sparsemax'}. Best is trial 45 with value: 2.80386.
Best parameters After Trials: {'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax'}
Parameters saved to YAML file!!!
In get_device
Fold 1
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [1.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 ...
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]
 [0.0 0.0 0.0 ... 2.579169386727153 1.2837928753303054 0.9948004955077808]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.05047 | eval_custom_logloss: 4.03839 |  0:00:00s
epoch 1  | loss: 0.90643 | eval_custom_logloss: 3.96418 |  0:00:00s
epoch 2  | loss: 0.81705 | eval_custom_logloss: 3.48314 |  0:00:01s
epoch 3  | loss: 0.73675 | eval_custom_logloss: 6.0961  |  0:00:01s
epoch 4  | loss: 0.73868 | eval_custom_logloss: 6.39592 |  0:00:02s
epoch 5  | loss: 0.7055  | eval_custom_logloss: 9.83414 |  0:00:02s
epoch 6  | loss: 0.63627 | eval_custom_logloss: 8.94898 |  0:00:03s
epoch 7  | loss: 0.64953 | eval_custom_logloss: 7.6783  |  0:00:03s
epoch 8  | loss: 0.59164 | eval_custom_logloss: 9.79307 |  0:00:04s
epoch 9  | loss: 0.5689  | eval_custom_logloss: 5.80126 |  0:00:04s
epoch 10 | loss: 0.55361 | eval_custom_logloss: 5.08395 |  0:00:04s
epoch 11 | loss: 0.52162 | eval_custom_logloss: 6.08921 |  0:00:05s
epoch 12 | loss: 0.48966 | eval_custom_logloss: 7.50061 |  0:00:05s
epoch 13 | loss: 0.50344 | eval_custom_logloss: 7.3551  |  0:00:06s
epoch 14 | loss: 0.49235 | eval_custom_logloss: 6.64885 |  0:00:06s
epoch 15 | loss: 0.47523 | eval_custom_logloss: 6.87397 |  0:00:06s
epoch 16 | loss: 0.45768 | eval_custom_logloss: 7.47423 |  0:00:07s
epoch 17 | loss: 0.45076 | eval_custom_logloss: 7.53772 |  0:00:07s
epoch 18 | loss: 0.46316 | eval_custom_logloss: 7.35108 |  0:00:08s
epoch 19 | loss: 0.42296 | eval_custom_logloss: 7.54575 |  0:00:08s
epoch 20 | loss: 0.46589 | eval_custom_logloss: 5.89407 |  0:00:08s
epoch 21 | loss: 0.44406 | eval_custom_logloss: 5.61162 |  0:00:09s
epoch 22 | loss: 0.41221 | eval_custom_logloss: 5.40108 |  0:00:09s

Early stopping occurred at epoch 22 with best_epoch = 2 and best_eval_custom_logloss = 3.48314
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is True b4 loss saving
Log file exists at: output/TabNet/SAT11/logging/loss_0.txt
File name : output/TabNet/SAT11/logging/loss_0.txt . The file was saved
Log file exists at: output/TabNet/SAT11/logging/val_loss_0.txt
File name : output/TabNet/SAT11/logging/val_loss_0.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.0467, 'Log Loss - std': 0.0} 
 

Fold 2
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 ...
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]
 [0.0 0.0 0.0 ... 2.5554593677083233 1.2724838377231176
  0.9898050177516838]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.04816 | eval_custom_logloss: 3.98175 |  0:00:00s
epoch 1  | loss: 0.87666 | eval_custom_logloss: 3.26023 |  0:00:00s
epoch 2  | loss: 0.79239 | eval_custom_logloss: 3.57881 |  0:00:01s
epoch 3  | loss: 0.70852 | eval_custom_logloss: 4.8317  |  0:00:01s
epoch 4  | loss: 0.71432 | eval_custom_logloss: 6.46357 |  0:00:01s
epoch 5  | loss: 0.66109 | eval_custom_logloss: 5.90256 |  0:00:02s
epoch 6  | loss: 0.61462 | eval_custom_logloss: 6.62521 |  0:00:02s
epoch 7  | loss: 0.59296 | eval_custom_logloss: 7.03261 |  0:00:03s
epoch 8  | loss: 0.55939 | eval_custom_logloss: 6.54453 |  0:00:03s
epoch 9  | loss: 0.50201 | eval_custom_logloss: 7.12515 |  0:00:03s
epoch 10 | loss: 0.53014 | eval_custom_logloss: 5.98362 |  0:00:04s
epoch 11 | loss: 0.5225  | eval_custom_logloss: 5.11315 |  0:00:04s
epoch 12 | loss: 0.50317 | eval_custom_logloss: 5.78528 |  0:00:05s
epoch 13 | loss: 0.49529 | eval_custom_logloss: 4.98024 |  0:00:05s
epoch 14 | loss: 0.49445 | eval_custom_logloss: 4.95011 |  0:00:06s
epoch 15 | loss: 0.50992 | eval_custom_logloss: 4.86301 |  0:00:06s
epoch 16 | loss: 0.47258 | eval_custom_logloss: 4.75008 |  0:00:06s
epoch 17 | loss: 0.46181 | eval_custom_logloss: 3.16845 |  0:00:07s
epoch 18 | loss: 0.45728 | eval_custom_logloss: 5.14382 |  0:00:07s
epoch 19 | loss: 0.45532 | eval_custom_logloss: 6.11254 |  0:00:08s
epoch 20 | loss: 0.45321 | eval_custom_logloss: 5.48649 |  0:00:08s
epoch 21 | loss: 0.43108 | eval_custom_logloss: 5.78496 |  0:00:09s
epoch 22 | loss: 0.48609 | eval_custom_logloss: 4.48889 |  0:00:09s
epoch 23 | loss: 0.46094 | eval_custom_logloss: 4.36747 |  0:00:10s
epoch 24 | loss: 0.43445 | eval_custom_logloss: 6.1658  |  0:00:10s
epoch 25 | loss: 0.45048 | eval_custom_logloss: 4.37438 |  0:00:10s
epoch 26 | loss: 0.42046 | eval_custom_logloss: 5.26741 |  0:00:11s
epoch 27 | loss: 0.42776 | eval_custom_logloss: 5.43811 |  0:00:11s
epoch 28 | loss: 0.44416 | eval_custom_logloss: 5.57837 |  0:00:11s
epoch 29 | loss: 0.43294 | eval_custom_logloss: 4.28959 |  0:00:12s
epoch 30 | loss: 0.42169 | eval_custom_logloss: 4.29892 |  0:00:12s
epoch 31 | loss: 0.40611 | eval_custom_logloss: 4.17216 |  0:00:13s
epoch 32 | loss: 0.45505 | eval_custom_logloss: 3.80863 |  0:00:13s
epoch 33 | loss: 0.42021 | eval_custom_logloss: 3.21286 |  0:00:13s
epoch 34 | loss: 0.39713 | eval_custom_logloss: 4.11738 |  0:00:14s
epoch 35 | loss: 0.39224 | eval_custom_logloss: 4.38201 |  0:00:14s
epoch 36 | loss: 0.41126 | eval_custom_logloss: 4.07669 |  0:00:15s
epoch 37 | loss: 0.44741 | eval_custom_logloss: 3.82899 |  0:00:15s

Early stopping occurred at epoch 37 with best_epoch = 17 and best_eval_custom_logloss = 3.16845
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is True b4 loss saving
Log file exists at: output/TabNet/SAT11/logging/loss_1.txt
File name : output/TabNet/SAT11/logging/loss_1.txt . The file was saved
Log file exists at: output/TabNet/SAT11/logging/val_loss_1.txt
File name : output/TabNet/SAT11/logging/val_loss_1.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.8509, 'Log Loss - std': 0.19579999999999997} 
 

Fold 3
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 ...
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]
 [0.0 0.0 0.0 ... 2.572681656950333 1.2836416330199947 0.9931480745530373]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.01341 | eval_custom_logloss: 3.93397 |  0:00:00s
epoch 1  | loss: 0.83523 | eval_custom_logloss: 4.57029 |  0:00:00s
epoch 2  | loss: 0.78357 | eval_custom_logloss: 4.44465 |  0:00:01s
epoch 3  | loss: 0.72272 | eval_custom_logloss: 4.83928 |  0:00:01s
epoch 4  | loss: 0.70817 | eval_custom_logloss: 5.39022 |  0:00:01s
epoch 5  | loss: 0.67171 | eval_custom_logloss: 4.94104 |  0:00:02s
epoch 6  | loss: 0.63778 | eval_custom_logloss: 5.84516 |  0:00:02s
epoch 7  | loss: 0.59508 | eval_custom_logloss: 4.88097 |  0:00:03s
epoch 8  | loss: 0.56264 | eval_custom_logloss: 5.97784 |  0:00:03s
epoch 9  | loss: 0.55005 | eval_custom_logloss: 5.08021 |  0:00:03s
epoch 10 | loss: 0.54745 | eval_custom_logloss: 6.26746 |  0:00:04s
epoch 11 | loss: 0.53229 | eval_custom_logloss: 4.7603  |  0:00:04s
epoch 12 | loss: 0.49774 | eval_custom_logloss: 3.97974 |  0:00:04s
epoch 13 | loss: 0.48687 | eval_custom_logloss: 5.66379 |  0:00:05s
epoch 14 | loss: 0.44993 | eval_custom_logloss: 7.35647 |  0:00:05s
epoch 15 | loss: 0.46648 | eval_custom_logloss: 6.11456 |  0:00:06s
epoch 16 | loss: 0.46908 | eval_custom_logloss: 7.74985 |  0:00:06s
epoch 17 | loss: 0.46148 | eval_custom_logloss: 4.10485 |  0:00:06s
epoch 18 | loss: 0.41968 | eval_custom_logloss: 5.48722 |  0:00:07s
epoch 19 | loss: 0.39401 | eval_custom_logloss: 4.14204 |  0:00:07s
epoch 20 | loss: 0.37862 | eval_custom_logloss: 3.94948 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 3.93397
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is True b4 loss saving
Log file exists at: output/TabNet/SAT11/logging/loss_2.txt
File name : output/TabNet/SAT11/logging/loss_2.txt . The file was saved
Log file exists at: output/TabNet/SAT11/logging/val_loss_2.txt
File name : output/TabNet/SAT11/logging/val_loss_2.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.0332000000000003, 'Log Loss - std': 0.3033562372305318} 
 

Fold 4
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 1.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 ...
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]
 [0.0 0.0 0.0 ... 2.5776931347039733 1.2593157823662984
  0.9810282088831183]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.07275 | eval_custom_logloss: 3.38556 |  0:00:00s
epoch 1  | loss: 0.90802 | eval_custom_logloss: 3.40711 |  0:00:00s
epoch 2  | loss: 0.79005 | eval_custom_logloss: 4.21449 |  0:00:01s
epoch 3  | loss: 0.69496 | eval_custom_logloss: 6.10748 |  0:00:01s
epoch 4  | loss: 0.65263 | eval_custom_logloss: 6.45903 |  0:00:01s
epoch 5  | loss: 0.65371 | eval_custom_logloss: 5.23208 |  0:00:02s
epoch 6  | loss: 0.58757 | eval_custom_logloss: 4.62858 |  0:00:02s
epoch 7  | loss: 0.5567  | eval_custom_logloss: 5.49414 |  0:00:03s
epoch 8  | loss: 0.54641 | eval_custom_logloss: 5.95823 |  0:00:03s
epoch 9  | loss: 0.50338 | eval_custom_logloss: 6.08825 |  0:00:03s
epoch 10 | loss: 0.4854  | eval_custom_logloss: 6.57941 |  0:00:04s
epoch 11 | loss: 0.49603 | eval_custom_logloss: 5.91184 |  0:00:04s
epoch 12 | loss: 0.51966 | eval_custom_logloss: 5.41492 |  0:00:05s
epoch 13 | loss: 0.49756 | eval_custom_logloss: 6.02379 |  0:00:05s
epoch 14 | loss: 0.51088 | eval_custom_logloss: 6.9505  |  0:00:05s
epoch 15 | loss: 0.55855 | eval_custom_logloss: 6.08284 |  0:00:06s
epoch 16 | loss: 0.50227 | eval_custom_logloss: 5.17912 |  0:00:06s
epoch 17 | loss: 0.46305 | eval_custom_logloss: 4.79238 |  0:00:07s
epoch 18 | loss: 0.43321 | eval_custom_logloss: 4.31944 |  0:00:07s
epoch 19 | loss: 0.45077 | eval_custom_logloss: 4.82042 |  0:00:08s
epoch 20 | loss: 0.45418 | eval_custom_logloss: 4.59321 |  0:00:08s

Early stopping occurred at epoch 20 with best_epoch = 0 and best_eval_custom_logloss = 3.38556
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is True b4 loss saving
Log file exists at: output/TabNet/SAT11/logging/loss_3.txt
File name : output/TabNet/SAT11/logging/loss_3.txt . The file was saved
Log file exists at: output/TabNet/SAT11/logging/val_loss_3.txt
File name : output/TabNet/SAT11/logging/val_loss_3.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 3.003075, 'Log Loss - std': 0.26784566801611714} 
 

Fold 5
num_features : 116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
BEFORE ANY ENCODING
num_features :116
num_classes : 1
cat_idx : [0]
nominal_idx : [115]
ordinal_idx : None
num_idx : None
cat_dims : [2]
bin_alt : None 


X shape : (1380, 116)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


AFTER SEPARATING CATEGORICALS AND NUMERICALS
Numerical Index V1 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]
Cat Dims V1 : []
Cat Idx V1 : [115] 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 


Scaling the data...
One Hot Encoding...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After OHE
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 1.0 0.0]
 [1.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 1.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 1.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]
 [0.0 0.0 0.0 0.0 0.0]] 
 
 
Val : (1380, 130) 
 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
After ORDINAL
Numerical Index V2 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114] 


OHE Idx : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]


Ordinal Idx V2: None


Cat Dims V2 : []
Cat Idx V2 : None 
 

Train: [[0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [1.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 1.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 ...
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]
 [0.0 0.0 0.0 ... 2.6560947038188796 1.2792309188622057
  0.9870369058550037]] 
 
 
Val : (1380, 130) 
 

FINISHED ENCODING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Number of Classes B4 Bin Verifier: 3
Unique values in y_train: (array([0., 1., 2.]), 3)
Unique values in y_test: (array([0., 1., 2.]), 3)
No need to shift labels.
VERIFY SHIFT
Train after shift : [0 1 2], Length : 3
Test after shift : [0 1 2], Length : 3
Number of Classes After Bin Verifier: 3
In get_device
{'n_d': 14, 'n_steps': 3, 'gamma': 1.0136194191205694, 'cat_emb_dim': 3, 'n_independent': 5, 'n_shared': 4, 'momentum': 0.0010133373717923582, 'mask_type': 'entmax', 'n_a': 14, 'cat_idxs': [], 'cat_dims': [], 'device_name': device(type='cuda')}
epoch 0  | loss: 1.11969 | eval_custom_logloss: 4.09163 |  0:00:00s
epoch 1  | loss: 0.87411 | eval_custom_logloss: 3.7555  |  0:00:00s
epoch 2  | loss: 0.78356 | eval_custom_logloss: 4.64136 |  0:00:01s
epoch 3  | loss: 0.71597 | eval_custom_logloss: 6.0853  |  0:00:01s
epoch 4  | loss: 0.66152 | eval_custom_logloss: 5.77437 |  0:00:02s
epoch 5  | loss: 0.63896 | eval_custom_logloss: 5.12903 |  0:00:02s
epoch 6  | loss: 0.57967 | eval_custom_logloss: 6.04289 |  0:00:02s
epoch 7  | loss: 0.54645 | eval_custom_logloss: 5.50244 |  0:00:03s
epoch 8  | loss: 0.52046 | eval_custom_logloss: 5.51066 |  0:00:03s
epoch 9  | loss: 0.53556 | eval_custom_logloss: 5.93506 |  0:00:04s
epoch 10 | loss: 0.53202 | eval_custom_logloss: 5.39585 |  0:00:04s
epoch 11 | loss: 0.52523 | eval_custom_logloss: 4.74756 |  0:00:04s
epoch 12 | loss: 0.50843 | eval_custom_logloss: 5.11053 |  0:00:05s
epoch 13 | loss: 0.49234 | eval_custom_logloss: 4.81993 |  0:00:05s
epoch 14 | loss: 0.49583 | eval_custom_logloss: 4.21543 |  0:00:06s
epoch 15 | loss: 0.4665  | eval_custom_logloss: 3.81897 |  0:00:06s
epoch 16 | loss: 0.44681 | eval_custom_logloss: 3.86648 |  0:00:07s
epoch 17 | loss: 0.44147 | eval_custom_logloss: 2.89364 |  0:00:07s
epoch 18 | loss: 0.43741 | eval_custom_logloss: 3.28084 |  0:00:08s
epoch 19 | loss: 0.4249  | eval_custom_logloss: 2.9091  |  0:00:08s
epoch 20 | loss: 0.41164 | eval_custom_logloss: 2.26066 |  0:00:08s
epoch 21 | loss: 0.41076 | eval_custom_logloss: 3.06583 |  0:00:09s
epoch 22 | loss: 0.41484 | eval_custom_logloss: 3.64915 |  0:00:09s
epoch 23 | loss: 0.41461 | eval_custom_logloss: 4.07977 |  0:00:10s
epoch 24 | loss: 0.38951 | eval_custom_logloss: 3.78217 |  0:00:10s
epoch 25 | loss: 0.42421 | eval_custom_logloss: 3.64338 |  0:00:10s
epoch 26 | loss: 0.37166 | eval_custom_logloss: 3.79812 |  0:00:11s
epoch 27 | loss: 0.36312 | eval_custom_logloss: 3.90533 |  0:00:11s
epoch 28 | loss: 0.36406 | eval_custom_logloss: 3.75519 |  0:00:12s
epoch 29 | loss: 0.34859 | eval_custom_logloss: 3.84762 |  0:00:12s
epoch 30 | loss: 0.32629 | eval_custom_logloss: 3.98002 |  0:00:12s
epoch 31 | loss: 0.37207 | eval_custom_logloss: 3.37651 |  0:00:13s
epoch 32 | loss: 0.36946 | eval_custom_logloss: 3.31166 |  0:00:13s
epoch 33 | loss: 0.34664 | eval_custom_logloss: 3.13519 |  0:00:13s
epoch 34 | loss: 0.33888 | eval_custom_logloss: 3.31521 |  0:00:14s
epoch 35 | loss: 0.31009 | eval_custom_logloss: 3.03957 |  0:00:14s
epoch 36 | loss: 0.32138 | eval_custom_logloss: 2.5571  |  0:00:15s
epoch 37 | loss: 0.30995 | eval_custom_logloss: 2.27321 |  0:00:15s
epoch 38 | loss: 0.3063  | eval_custom_logloss: 3.16778 |  0:00:15s
epoch 39 | loss: 0.30857 | eval_custom_logloss: 2.7409  |  0:00:16s
epoch 40 | loss: 0.28263 | eval_custom_logloss: 2.4899  |  0:00:16s

Early stopping occurred at epoch 40 with best_epoch = 20 and best_eval_custom_logloss = 2.26066
Self Metric: [<class 'models.tabnet.CustomLogLoss'>]
State of save is True b4 loss saving
Log file exists at: output/TabNet/SAT11/logging/loss_4.txt
File name : output/TabNet/SAT11/logging/loss_4.txt . The file was saved
Log file exists at: output/TabNet/SAT11/logging/val_loss_4.txt
File name : output/TabNet/SAT11/logging/val_loss_4.txt . The file was saved
Saved Losses and Regularization
±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

B4 Evaluation
Number of classes : 3
Class label len :3
Class labels : [0, 1, 2]
Unique y_true : [0 1 2] 

Prediction shape : (345,)
Probabilities shape : (345, 3) 

±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±± 

After Evaluation
{'Log Loss - mean': 2.80386, 'Log Loss - std': 0.46490806230909787} 
 

Saving model.....
Results After CV: {'Log Loss - mean': 2.80386, 'Log Loss - std': 0.46490806230909787}
Train time: 12.143839591800496
Inference time: 0.0517156215999421
Finished cross validation
Loss path :output/TabNet/SAT11/logging/
Plots saved successfully!
