 Panda Version: 2.0.3
Namespace(batch_size=128, cat_dims=None, cat_idx=None, config='config/house_sales.yml', data_parallel=False, dataset='House_Sales', direction='minimize', dropna_idx=[0], early_stopping_rounds=20, epochs=500, gpu_ids=[0, 1], logging_period=100, miss_cat_idx=None, miss_num_idx=None, model_name='DeepGBM', n_trials=100, nominal_idx=[14], num_classes=1, num_features=21, num_idx=None, num_splits=5, objective='regression', one_hot_encode=True, optimize_hyperparameters=True, ordinal_encode=False, ordinal_idx=None, scale=True, seed=221, shuffle=True, target_encode=False, use_gpu=True, val_batch_size=256)
Start hyperparameter optimization
 Panda Version: 2.0.3
Loading dataset House_Sales...
Dataset loaded! 

X b4 encoding : [ 3.00000e+00  1.00000e+00  1.18000e+03  5.65000e+03  1.00000e+00
  0.00000e+00  0.00000e+00  3.00000e+00  7.00000e+00  1.18000e+03
  0.00000e+00  1.95500e+03  0.00000e+00  9.81780e+04  4.75112e+01
 -1.22257e+02  1.34000e+03  5.65000e+03  2.01400e+03  1.00000e+01
  1.30000e+01] 

(21613, 21)
Data Type of X: <class 'numpy.ndarray'>
Nominal Idx: [13]
Ordinal Idx: None
Cat Dims: None 
 

Normonal Idx: [13]
Cat Idx Part II: [13] 
ENDE 
 

X after Nominal Encoding: [ 3.00000e+00  1.00000e+00  1.18000e+03  5.65000e+03  1.00000e+00
  0.00000e+00  0.00000e+00  3.00000e+00  7.00000e+00  1.18000e+03
  0.00000e+00  1.95500e+03  0.00000e+00  9.81780e+04  4.75112e+01
 -1.22257e+02  1.34000e+03  5.65000e+03  2.01400e+03  1.00000e+01
  1.30000e+01] 
 

Scaling the data...
X after Scaling: [-3.98737149e-01 -1.44746357e+00 -9.79835021e-01 -2.28321332e-01
 -9.15427004e-01 -8.71726310e-02 -3.05759464e-01 -6.29186873e-01
 -5.58835749e-01 -7.34707638e-01 -6.58681040e-01 -5.44897771e-01
 -2.10128386e-01  9.81780000e+04 -3.52571748e-01 -3.06078958e-01
 -9.43355198e-01 -2.60715408e-01 -6.90654785e-01  1.09962055e+00
 -3.11319009e-01] 
 

One Hot Encoding...
X after One Hot Encoding: [ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  1.          0.          0.          0.         -0.39873715 -1.44746357
 -0.97983502 -0.22832133 -0.915427   -0.08717263 -0.30575946 -0.62918687
 -0.55883575 -0.73470764 -0.65868104 -0.54489777 -0.21012839 -0.35257175
 -0.30607896 -0.9433552  -0.26071541 -0.69065478  1.09962055 -0.31131901] 
 

args.num_features: 90
args.cat_idx: None
Cat Dims: []
New Shape: (21613, 90)
False 
 

A new study created in RDB with name: DeepGBM_House_Sales
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003573 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524740517.647
Test Loss of 418110758887.365234, Test MSE of 418110756175.893799
Epoch 2: training loss 427503713942.588
Test Loss of 418092799673.589661, Test MSE of 418092803648.489258
Epoch 3: training loss 427475866925.176
Test Loss of 418068914393.922729, Test MSE of 418068915906.960205
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427489203019.294
Test Loss of 418075651132.402527, Test MSE of 418075654825.706970
Epoch 2: training loss 427481568316.235
Test Loss of 418077170103.161682, Test MSE of 418077172399.870239
Epoch 3: training loss 427481024993.882
Test Loss of 418076167628.243347, Test MSE of 418076169482.826599
Epoch 4: training loss 422090923550.118
Test Loss of 401145706319.293091, Test MSE of 401145703870.738220
Epoch 5: training loss 385700151536.941
Test Loss of 346527801201.876465, Test MSE of 346527803758.934265
Epoch 6: training loss 317708515328.000
Test Loss of 268697120395.636353, Test MSE of 268697123373.376068
Epoch 7: training loss 227570999356.235
Test Loss of 173786882550.169800, Test MSE of 173786883647.242310
Epoch 8: training loss 163215753728.000
Test Loss of 132842473053.683090, Test MSE of 132842472931.102386
Epoch 9: training loss 142331436965.647
Test Loss of 120927821393.839462, Test MSE of 120927819548.335358
Epoch 10: training loss 136649828111.059
Test Loss of 116410645267.838074, Test MSE of 116410644154.645554
Epoch 11: training loss 133885896192.000
Test Loss of 113656803995.980576, Test MSE of 113656804312.462677
Epoch 12: training loss 130011620623.059
Test Loss of 110808164032.222061, Test MSE of 110808166710.335938
Epoch 13: training loss 127750176993.882
Test Loss of 107611713832.564423, Test MSE of 107611712683.942551
Epoch 14: training loss 123627906319.059
Test Loss of 104662501564.076797, Test MSE of 104662502270.911865
Epoch 15: training loss 119455839397.647
Test Loss of 102088204204.620865, Test MSE of 102088202832.662796
Epoch 16: training loss 117015672455.529
Test Loss of 97769116081.003006, Test MSE of 97769115648.347763
Epoch 17: training loss 113808843655.529
Test Loss of 97313959216.854965, Test MSE of 97313958512.891052
Epoch 18: training loss 110266120026.353
Test Loss of 94080915428.996536, Test MSE of 94080916140.848618
Epoch 19: training loss 106711626255.059
Test Loss of 91015404345.737686, Test MSE of 91015404153.649063
Epoch 20: training loss 102294127992.471
Test Loss of 88103714707.986115, Test MSE of 88103714386.685150
Epoch 21: training loss 100523834503.529
Test Loss of 86108465861.196396, Test MSE of 86108465060.207520
Epoch 22: training loss 96891088414.118
Test Loss of 82854110669.664581, Test MSE of 82854110319.840317
Epoch 23: training loss 94153184346.353
Test Loss of 80035261079.716858, Test MSE of 80035261477.904556
Epoch 24: training loss 91215073641.412
Test Loss of 77325730985.837616, Test MSE of 77325730119.182175
Epoch 25: training loss 87670697020.235
Test Loss of 74890415054.493637, Test MSE of 74890415194.156754
Epoch 26: training loss 83643069816.471
Test Loss of 72610084683.503128, Test MSE of 72610085157.956039
Epoch 27: training loss 81617976184.471
Test Loss of 68875155400.571823, Test MSE of 68875155438.246292
Epoch 28: training loss 78114117360.941
Test Loss of 65615341972.104553, Test MSE of 65615341382.287895
Epoch 29: training loss 75827706454.588
Test Loss of 64478335683.775154, Test MSE of 64478334944.869957
Epoch 30: training loss 72545153325.176
Test Loss of 63217093652.607910, Test MSE of 63217095114.204025
Epoch 31: training loss 68919971463.529
Test Loss of 60454446420.385841, Test MSE of 60454446275.808662
Epoch 32: training loss 66997584368.941
Test Loss of 57914497004.576454, Test MSE of 57914498058.144890
Epoch 33: training loss 63579005236.706
Test Loss of 55814628500.992828, Test MSE of 55814628426.692253
Epoch 34: training loss 61645087932.235
Test Loss of 52445535624.024055, Test MSE of 52445535887.978325
Epoch 35: training loss 58693026435.765
Test Loss of 50709548604.994682, Test MSE of 50709548138.368118
Epoch 36: training loss 56612149910.588
Test Loss of 46065254056.061066, Test MSE of 46065254031.610626
Epoch 37: training loss 53392118490.353
Test Loss of 44666201868.021278, Test MSE of 44666201969.098740
Epoch 38: training loss 50890279988.706
Test Loss of 43718200289.443443, Test MSE of 43718201622.961098
Epoch 39: training loss 49502250488.471
Test Loss of 43033796842.977562, Test MSE of 43033796027.213089
Epoch 40: training loss 46883349285.647
Test Loss of 38588861004.865143, Test MSE of 38588861090.073189
Epoch 41: training loss 44370598704.941
Test Loss of 38832993065.156601, Test MSE of 38832993351.079636
Epoch 42: training loss 42453517763.765
Test Loss of 38319867724.450615, Test MSE of 38319867805.241119
Epoch 43: training loss 40345957135.059
Test Loss of 36078104939.836227, Test MSE of 36078104850.275681
Epoch 44: training loss 39039945946.353
Test Loss of 34404691858.091141, Test MSE of 34404692621.113579
Epoch 45: training loss 37348032858.353
Test Loss of 31810722975.888966, Test MSE of 31810723542.576828
Epoch 46: training loss 35502645376.000
Test Loss of 30398183741.882950, Test MSE of 30398183334.102108
Epoch 47: training loss 33855017984.000
Test Loss of 29143519638.473282, Test MSE of 29143519174.415176
Epoch 48: training loss 32473327616.000
Test Loss of 30307453491.282906, Test MSE of 30307453842.952896
Epoch 49: training loss 31234795482.353
Test Loss of 26440631336.978951, Test MSE of 26440630951.474705
Epoch 50: training loss 29134220822.588
Test Loss of 26121441594.566734, Test MSE of 26121441728.512619
Epoch 51: training loss 28331617739.294
Test Loss of 26076451624.209114, Test MSE of 26076451833.825012
Epoch 52: training loss 27147783265.882
Test Loss of 25415141153.339809, Test MSE of 25415141016.258038
Epoch 53: training loss 25916226748.235
Test Loss of 26592913417.474903, Test MSE of 26592913420.606392
Epoch 54: training loss 24936990622.118
Test Loss of 24344869173.355541, Test MSE of 24344869446.987545
Epoch 55: training loss 23758290127.059
Test Loss of 21716366188.428406, Test MSE of 21716366660.259037
Epoch 56: training loss 22914093594.353
Test Loss of 21012075112.816101, Test MSE of 21012075020.968796
Epoch 57: training loss 21845725391.059
Test Loss of 22421788178.357620, Test MSE of 22421787723.613617
Epoch 58: training loss 21286842247.529
Test Loss of 20510626386.076336, Test MSE of 20510626604.399799
Epoch 59: training loss 20434846938.353
Test Loss of 23570489927.653946, Test MSE of 23570490038.683605
Epoch 60: training loss 19592418176.000
Test Loss of 20807213436.417301, Test MSE of 20807213373.318058
Epoch 61: training loss 18921528948.706
Test Loss of 20429018754.161461, Test MSE of 20429018584.456779
Epoch 62: training loss 18570360677.647
Test Loss of 21713617141.399952, Test MSE of 21713617250.966125
Epoch 63: training loss 17953145460.706
Test Loss of 20157596184.279434, Test MSE of 20157596184.177505
Epoch 64: training loss 17325930680.471
Test Loss of 19860039102.030998, Test MSE of 19860039288.211327
Epoch 65: training loss 17192208342.588
Test Loss of 20652786144.377514, Test MSE of 20652786118.204082
Epoch 66: training loss 16292324879.059
Test Loss of 20049176019.823273, Test MSE of 20049176461.274189
Epoch 67: training loss 15777799472.941
Test Loss of 18150336491.628960, Test MSE of 18150336604.993240
Epoch 68: training loss 15434636724.706
Test Loss of 20834932650.725883, Test MSE of 20834932557.278542
Epoch 69: training loss 14837116698.353
Test Loss of 19404541948.210041, Test MSE of 19404542260.626251
Epoch 70: training loss 14557429716.706
Test Loss of 18399327861.370346, Test MSE of 18399327352.108910
Epoch 71: training loss 14117686799.059
Test Loss of 18910807896.294239, Test MSE of 18910807641.257950
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18910807641.25795, 'MSE - std': 0.0, 'R2 - mean': 0.8527394890905026, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005685 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918659824.941
Test Loss of 424557747634.661133, Test MSE of 424557753418.085388
Epoch 2: training loss 427899047213.176
Test Loss of 424542178051.730713, Test MSE of 424542182687.798706
Epoch 3: training loss 427872792335.059
Test Loss of 424520617117.046509, Test MSE of 424520623627.726013
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887137249.882
Test Loss of 424525488057.885742, Test MSE of 424525491761.698608
Epoch 2: training loss 427879736018.824
Test Loss of 424526555386.611145, Test MSE of 424526558480.602173
Epoch 3: training loss 427879328105.412
Test Loss of 424526035722.363159, Test MSE of 424526033265.862366
Epoch 4: training loss 422906568463.059
Test Loss of 408715767086.959961, Test MSE of 408715770181.346008
Epoch 5: training loss 387489078573.176
Test Loss of 355796749135.529968, Test MSE of 355796747356.156372
Epoch 6: training loss 319460977483.294
Test Loss of 279683450935.428162, Test MSE of 279683453101.999939
Epoch 7: training loss 229307407360.000
Test Loss of 185403014709.414764, Test MSE of 185403018266.925903
Epoch 8: training loss 163122612766.118
Test Loss of 144146196891.684479, Test MSE of 144146197537.441895
Epoch 9: training loss 141201010115.765
Test Loss of 132299762682.078186, Test MSE of 132299764712.966980
Epoch 10: training loss 134849739143.529
Test Loss of 128129612078.723114, Test MSE of 128129612549.489777
Epoch 11: training loss 131556214422.588
Test Loss of 125399384446.786026, Test MSE of 125399383495.069641
Epoch 12: training loss 129021791864.471
Test Loss of 122481520231.631744, Test MSE of 122481520028.770508
Epoch 13: training loss 127010832173.176
Test Loss of 119447396706.361328, Test MSE of 119447398003.976929
Epoch 14: training loss 122113464440.471
Test Loss of 117010135608.020355, Test MSE of 117010136839.704224
Epoch 15: training loss 119472584734.118
Test Loss of 113230469356.398804, Test MSE of 113230470498.146591
Epoch 16: training loss 114769860638.118
Test Loss of 110349433836.813324, Test MSE of 110349432082.968079
Epoch 17: training loss 111732628028.235
Test Loss of 108002395412.904007, Test MSE of 108002394045.951111
Epoch 18: training loss 108967469236.706
Test Loss of 104460185817.922745, Test MSE of 104460184566.971573
Epoch 19: training loss 105734207277.176
Test Loss of 100127186793.349060, Test MSE of 100127185483.984848
Epoch 20: training loss 103594028272.941
Test Loss of 96899098458.662964, Test MSE of 96899097757.016937
Epoch 21: training loss 99545373816.471
Test Loss of 95042761196.694885, Test MSE of 95042760283.311554
Epoch 22: training loss 97359771286.588
Test Loss of 91497702017.450851, Test MSE of 91497702184.853912
Epoch 23: training loss 92770894275.765
Test Loss of 89289939465.593338, Test MSE of 89289940040.312576
Epoch 24: training loss 88705934908.235
Test Loss of 85934262714.951660, Test MSE of 85934261741.192871
Epoch 25: training loss 86380812016.941
Test Loss of 81976855778.213272, Test MSE of 81976856487.671524
Epoch 26: training loss 83940644999.529
Test Loss of 81556115489.162155, Test MSE of 81556115907.636703
Epoch 27: training loss 80428961671.529
Test Loss of 79499289149.231552, Test MSE of 79499288482.389328
Epoch 28: training loss 78030391265.882
Test Loss of 73665732934.884109, Test MSE of 73665732384.787689
Epoch 29: training loss 74416035523.765
Test Loss of 72962064436.111954, Test MSE of 72962064320.410889
Epoch 30: training loss 71424490706.824
Test Loss of 68574525337.197319, Test MSE of 68574523185.086113
Epoch 31: training loss 68613026650.353
Test Loss of 65051840836.041641, Test MSE of 65051842706.689156
Epoch 32: training loss 65999987787.294
Test Loss of 60615095515.107101, Test MSE of 60615096368.383591
Epoch 33: training loss 63387045074.824
Test Loss of 61825517764.604210, Test MSE of 61825519496.554451
Epoch 34: training loss 60030848752.941
Test Loss of 60455911124.593109, Test MSE of 60455910817.649086
Epoch 35: training loss 57911950531.765
Test Loss of 56658543949.279663, Test MSE of 56658542672.961182
Epoch 36: training loss 55121953626.353
Test Loss of 54968538925.183441, Test MSE of 54968539423.309692
Epoch 37: training loss 52613828969.412
Test Loss of 50753149703.994446, Test MSE of 50753148012.374947
Epoch 38: training loss 50379138981.647
Test Loss of 50513738910.467728, Test MSE of 50513736579.000389
Epoch 39: training loss 48029060788.706
Test Loss of 48598081554.476059, Test MSE of 48598082146.275581
Epoch 40: training loss 46168976481.882
Test Loss of 48277789510.055054, Test MSE of 48277789173.882660
Epoch 41: training loss 43945967472.941
Test Loss of 45753760737.917191, Test MSE of 45753761645.046265
Epoch 42: training loss 41943326968.471
Test Loss of 39544210271.163544, Test MSE of 39544211140.016296
Epoch 43: training loss 39309595949.176
Test Loss of 41670456056.360863, Test MSE of 41670455159.230789
Epoch 44: training loss 37469605059.765
Test Loss of 39752819358.349297, Test MSE of 39752817977.426315
Epoch 45: training loss 35850576225.882
Test Loss of 39767589732.374741, Test MSE of 39767590931.360832
Epoch 46: training loss 34150466695.529
Test Loss of 38210645374.075409, Test MSE of 38210645782.441246
Epoch 47: training loss 32339190716.235
Test Loss of 36545993968.899376, Test MSE of 36545993875.265381
Epoch 48: training loss 30849725846.588
Test Loss of 33795307285.969929, Test MSE of 33795306091.719330
Epoch 49: training loss 29507450488.471
Test Loss of 35219350145.924591, Test MSE of 35219350886.688530
Epoch 50: training loss 28159721712.941
Test Loss of 34332767254.976635, Test MSE of 34332767460.210388
Epoch 51: training loss 26939944583.529
Test Loss of 32582944894.489937, Test MSE of 32582946169.264820
Epoch 52: training loss 25583853688.471
Test Loss of 33840737620.385841, Test MSE of 33840737128.523682
Epoch 53: training loss 24323701040.941
Test Loss of 34274544243.712238, Test MSE of 34274545335.751961
Epoch 54: training loss 23302707117.176
Test Loss of 31115640711.668747, Test MSE of 31115639555.368916
Epoch 55: training loss 22260822253.176
Test Loss of 28213009028.530186, Test MSE of 28213009020.481552
Epoch 56: training loss 21072090179.765
Test Loss of 29347582332.417301, Test MSE of 29347582248.559376
Epoch 57: training loss 20489655631.059
Test Loss of 30122083748.211891, Test MSE of 30122083441.983109
Epoch 58: training loss 19739053586.824
Test Loss of 30191408176.558872, Test MSE of 30191407963.650558
Epoch 59: training loss 19060863943.529
Test Loss of 30375937035.369881, Test MSE of 30375937371.561298
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24643372506.409622, 'MSE - std': 5732564865.151674, 'R2 - mean': 0.8179377318521104, 'R2 - std': 0.034801757238392184} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004373 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927551096.471
Test Loss of 447258524945.587769, Test MSE of 447258525077.731995
Epoch 2: training loss 421906261534.118
Test Loss of 447238736408.753174, Test MSE of 447238729799.492737
Epoch 3: training loss 421878385242.353
Test Loss of 447212773404.424683, Test MSE of 447212776696.715332
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895529291.294
Test Loss of 447221768209.054810, Test MSE of 447221769058.687134
Epoch 2: training loss 421885593840.941
Test Loss of 447222503961.700684, Test MSE of 447222503365.487305
Epoch 3: training loss 421885060517.647
Test Loss of 447221965287.009949, Test MSE of 447221970498.896362
Epoch 4: training loss 416725544598.588
Test Loss of 430553498448.951172, Test MSE of 430553499842.802551
Epoch 5: training loss 380891012758.588
Test Loss of 375661097645.035400, Test MSE of 375661096614.457092
Epoch 6: training loss 313167820438.588
Test Loss of 297425626526.290100, Test MSE of 297425631200.601440
Epoch 7: training loss 224086585464.471
Test Loss of 200880137217.184357, Test MSE of 200880140586.012512
Epoch 8: training loss 159353156427.294
Test Loss of 157055782367.193146, Test MSE of 157055789260.130066
Epoch 9: training loss 139265192417.882
Test Loss of 143502659592.527405, Test MSE of 143502662522.848053
Epoch 10: training loss 132685417231.059
Test Loss of 138397845169.062225, Test MSE of 138397844534.223969
Epoch 11: training loss 129492992662.588
Test Loss of 135403787575.013641, Test MSE of 135403789654.219925
Epoch 12: training loss 127749771866.353
Test Loss of 131874975860.541290, Test MSE of 131874974769.937256
Epoch 13: training loss 122918561430.588
Test Loss of 128392584372.733749, Test MSE of 128392583618.267120
Epoch 14: training loss 119890115553.882
Test Loss of 126011873848.020355, Test MSE of 126011877390.950211
Epoch 15: training loss 117880689633.882
Test Loss of 123809332895.533661, Test MSE of 123809331481.829025
Epoch 16: training loss 113241561328.941
Test Loss of 120534426803.312515, Test MSE of 120534427235.497818
Epoch 17: training loss 111564822799.059
Test Loss of 115974634280.682861, Test MSE of 115974634365.902115
Epoch 18: training loss 107464357376.000
Test Loss of 113413019727.115433, Test MSE of 113413019487.566238
Epoch 19: training loss 103547690255.059
Test Loss of 109876602418.098541, Test MSE of 109876603876.758301
Epoch 20: training loss 101463480545.882
Test Loss of 107093209701.026138, Test MSE of 107093211705.374573
Epoch 21: training loss 98224912805.647
Test Loss of 105055368069.063156, Test MSE of 105055368742.239151
Epoch 22: training loss 93593032448.000
Test Loss of 100077500432.581085, Test MSE of 100077501034.110855
Epoch 23: training loss 92213046392.471
Test Loss of 95687983352.242432, Test MSE of 95687983546.449921
Epoch 24: training loss 89150345200.941
Test Loss of 95215456084.504272, Test MSE of 95215456751.260391
Epoch 25: training loss 85936447849.412
Test Loss of 91432420057.567429, Test MSE of 91432418744.807434
Epoch 26: training loss 82342828965.647
Test Loss of 86716141106.572281, Test MSE of 86716142769.568726
Epoch 27: training loss 79550800173.176
Test Loss of 84919451416.812393, Test MSE of 84919450965.096237
Epoch 28: training loss 77289617392.941
Test Loss of 83016422305.487854, Test MSE of 83016420838.328140
Epoch 29: training loss 73435429933.176
Test Loss of 80366329833.260239, Test MSE of 80366329458.805801
Epoch 30: training loss 70749197146.353
Test Loss of 75395942222.345596, Test MSE of 75395941339.301361
Epoch 31: training loss 68654123685.647
Test Loss of 71136883932.054596, Test MSE of 71136884558.532532
Epoch 32: training loss 64945124472.471
Test Loss of 72542873625.108490, Test MSE of 72542874402.774567
Epoch 33: training loss 63150797793.882
Test Loss of 68210354409.556328, Test MSE of 68210353608.791496
Epoch 34: training loss 60185088572.235
Test Loss of 62293331752.919731, Test MSE of 62293331091.596184
Epoch 35: training loss 57782211915.294
Test Loss of 63490667783.639137, Test MSE of 63490667584.724648
Epoch 36: training loss 55539448500.706
Test Loss of 60876203636.422852, Test MSE of 60876204062.884254
Epoch 37: training loss 53001154349.176
Test Loss of 56889146224.692108, Test MSE of 56889146136.212845
Epoch 38: training loss 50170228449.882
Test Loss of 56543760074.170715, Test MSE of 56543760111.747063
Epoch 39: training loss 48809260739.765
Test Loss of 53692479231.703911, Test MSE of 53692479614.168571
Epoch 40: training loss 45902742076.235
Test Loss of 50566711385.537819, Test MSE of 50566711803.408020
Epoch 41: training loss 44071372574.118
Test Loss of 48327830463.333794, Test MSE of 48327830415.843987
Epoch 42: training loss 42509938544.941
Test Loss of 46294455660.073097, Test MSE of 46294456088.001999
Epoch 43: training loss 40574365342.118
Test Loss of 43106375485.290771, Test MSE of 43106374880.188736
Epoch 44: training loss 38494937705.412
Test Loss of 39931248161.517464, Test MSE of 39931248074.136017
Epoch 45: training loss 36915693643.294
Test Loss of 41249553980.047188, Test MSE of 41249554002.763428
Epoch 46: training loss 34565311728.941
Test Loss of 38759482087.779785, Test MSE of 38759482776.883110
Epoch 47: training loss 33903686038.588
Test Loss of 37328778952.749481, Test MSE of 37328779166.235786
Epoch 48: training loss 31805821342.118
Test Loss of 37911040372.363640, Test MSE of 37911040189.805260
Epoch 49: training loss 30592444912.941
Test Loss of 34642036480.651398, Test MSE of 34642036431.168747
Epoch 50: training loss 28696067365.647
Test Loss of 33937296927.622486, Test MSE of 33937297860.493214
Epoch 51: training loss 27682400587.294
Test Loss of 34563827528.660652, Test MSE of 34563827486.439545
Epoch 52: training loss 26806576041.412
Test Loss of 33125064196.145271, Test MSE of 33125064872.551426
Epoch 53: training loss 25783183469.176
Test Loss of 27769010796.606060, Test MSE of 27769010674.066837
Epoch 54: training loss 24475529524.706
Test Loss of 30908000387.227390, Test MSE of 30908001004.753689
Epoch 55: training loss 23355033999.059
Test Loss of 27314394574.612076, Test MSE of 27314394319.235615
Epoch 56: training loss 22536590524.235
Test Loss of 32797727947.473515, Test MSE of 32797727835.312160
Epoch 57: training loss 21457170635.294
Test Loss of 28158403733.940319, Test MSE of 28158403482.693882
Epoch 58: training loss 21030094599.529
Test Loss of 27446926151.476288, Test MSE of 27446926258.492462
Epoch 59: training loss 19987816914.824
Test Loss of 26598089845.251907, Test MSE of 26598089961.624268
Epoch 60: training loss 19464497776.941
Test Loss of 23154686993.291695, Test MSE of 23154687119.516804
Epoch 61: training loss 18833793253.647
Test Loss of 25290582104.116585, Test MSE of 25290581628.786442
Epoch 62: training loss 17776613187.765
Test Loss of 26443063257.626648, Test MSE of 26443063732.360600
Epoch 63: training loss 17600858605.176
Test Loss of 23734987711.333797, Test MSE of 23734987903.922874
Epoch 64: training loss 17196751073.882
Test Loss of 25802392121.441593, Test MSE of 25802392210.000435
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25029712407.606556, 'MSE - std': 4712400343.133141, 'R2 - mean': 0.8213702138565777, 'R2 - std': 0.02882716433057493} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005573 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110103913.412
Test Loss of 410765969854.371155, Test MSE of 410765966062.236511
Epoch 2: training loss 430088384030.118
Test Loss of 410747421270.952332, Test MSE of 410747414474.024719
Epoch 3: training loss 430060357752.471
Test Loss of 410724045874.702454, Test MSE of 410724043953.443054
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076790181.647
Test Loss of 410727764201.136536, Test MSE of 410727765091.680603
Epoch 2: training loss 430067371791.059
Test Loss of 410728246062.082397, Test MSE of 410728245276.484985
Epoch 3: training loss 430066941229.176
Test Loss of 410728898541.045837, Test MSE of 410728897243.609070
Epoch 4: training loss 425167988856.471
Test Loss of 395083976504.981018, Test MSE of 395083970397.209534
Epoch 5: training loss 389620557462.588
Test Loss of 341159927356.890320, Test MSE of 341159923209.393250
Epoch 6: training loss 322384168839.529
Test Loss of 264349031170.487732, Test MSE of 264349027916.203369
Epoch 7: training loss 231457821394.824
Test Loss of 169111596860.771851, Test MSE of 169111595598.881622
Epoch 8: training loss 166748072538.353
Test Loss of 127019253418.824615, Test MSE of 127019254661.453766
Epoch 9: training loss 145230009193.412
Test Loss of 114694302714.787598, Test MSE of 114694304167.828262
Epoch 10: training loss 140268703442.824
Test Loss of 110443659085.830627, Test MSE of 110443659435.874939
Epoch 11: training loss 135405544719.059
Test Loss of 108058049343.141144, Test MSE of 108058050074.166733
Epoch 12: training loss 132633292739.765
Test Loss of 105100678008.951416, Test MSE of 105100680803.399033
Epoch 13: training loss 130522832052.706
Test Loss of 102916932584.781113, Test MSE of 102916929820.410370
Epoch 14: training loss 125530722544.941
Test Loss of 100211245499.528000, Test MSE of 100211244914.420212
Epoch 15: training loss 123396596344.471
Test Loss of 96874017827.539108, Test MSE of 96874016592.311752
Epoch 16: training loss 118609876208.941
Test Loss of 93480896442.817215, Test MSE of 93480896314.273529
Epoch 17: training loss 116321451248.941
Test Loss of 91704972524.453491, Test MSE of 91704971674.324295
Epoch 18: training loss 112021959439.059
Test Loss of 87752580380.786667, Test MSE of 87752580291.648087
Epoch 19: training loss 108979723896.471
Test Loss of 85733399749.597412, Test MSE of 85733400745.145905
Epoch 20: training loss 105929618311.529
Test Loss of 82792302666.869049, Test MSE of 82792304500.481613
Epoch 21: training loss 102490499282.824
Test Loss of 80851218831.933365, Test MSE of 80851218049.508789
Epoch 22: training loss 99956434522.353
Test Loss of 78261906013.586304, Test MSE of 78261905746.107117
Epoch 23: training loss 95405801170.824
Test Loss of 75955095789.875061, Test MSE of 75955094277.418030
Epoch 24: training loss 93153961893.647
Test Loss of 72406505018.047195, Test MSE of 72406505110.590729
Epoch 25: training loss 89572725172.706
Test Loss of 71396931630.437759, Test MSE of 71396931386.764389
Epoch 26: training loss 86158148924.235
Test Loss of 68137731822.111984, Test MSE of 68137732849.220932
Epoch 27: training loss 82305987011.765
Test Loss of 66197012836.338730, Test MSE of 66197012817.576241
Epoch 28: training loss 80053461504.000
Test Loss of 64334047538.110138, Test MSE of 64334049060.468040
Epoch 29: training loss 77584130183.529
Test Loss of 61282506449.680702, Test MSE of 61282505908.325584
Epoch 30: training loss 74240448768.000
Test Loss of 59959276060.194351, Test MSE of 59959275442.693817
Epoch 31: training loss 70445761611.294
Test Loss of 56944527290.817215, Test MSE of 56944526732.541534
Epoch 32: training loss 68922556792.471
Test Loss of 55382503880.795929, Test MSE of 55382503522.655075
Epoch 33: training loss 65875438908.235
Test Loss of 51612445135.429893, Test MSE of 51612445166.289497
Epoch 34: training loss 63302570300.235
Test Loss of 50836673224.677467, Test MSE of 50836672998.723785
Epoch 35: training loss 60644407627.294
Test Loss of 49782929065.876907, Test MSE of 49782928767.207603
Epoch 36: training loss 57233771346.824
Test Loss of 46628643187.502083, Test MSE of 46628643025.591942
Epoch 37: training loss 55791088082.824
Test Loss of 45913924417.984268, Test MSE of 45913924063.190613
Epoch 38: training loss 52947409114.353
Test Loss of 42323657445.108749, Test MSE of 42323657591.672096
Epoch 39: training loss 50006747693.176
Test Loss of 40915322922.646919, Test MSE of 40915322709.131935
Epoch 40: training loss 48041344158.118
Test Loss of 41003235457.836189, Test MSE of 41003235553.488449
Epoch 41: training loss 45653717549.176
Test Loss of 35844685422.171219, Test MSE of 35844685760.978065
Epoch 42: training loss 43828790309.647
Test Loss of 37019847703.218880, Test MSE of 37019847942.746269
Epoch 43: training loss 42230996570.353
Test Loss of 34428763111.833412, Test MSE of 34428763394.220749
Epoch 44: training loss 39995337464.471
Test Loss of 34415993986.310043, Test MSE of 34415993949.698700
Epoch 45: training loss 37991203666.824
Test Loss of 32822428324.664509, Test MSE of 32822428908.167141
Epoch 46: training loss 36485710607.059
Test Loss of 31981586275.627949, Test MSE of 31981586356.407413
Epoch 47: training loss 34836593844.706
Test Loss of 30610884116.138824, Test MSE of 30610884347.658470
Epoch 48: training loss 33102679273.412
Test Loss of 29635181419.683479, Test MSE of 29635182381.008854
Epoch 49: training loss 31934327687.529
Test Loss of 28456814229.975010, Test MSE of 28456813955.979965
Epoch 50: training loss 30368318960.941
Test Loss of 28992154484.212864, Test MSE of 28992154261.735794
Epoch 51: training loss 28907374230.588
Test Loss of 26289996574.919018, Test MSE of 26289996358.627213
Epoch 52: training loss 27755990953.412
Test Loss of 27446214847.437298, Test MSE of 27446215331.363590
Epoch 53: training loss 26528890036.706
Test Loss of 26051120293.375290, Test MSE of 26051120282.580536
Epoch 54: training loss 25452649464.471
Test Loss of 24714551124.464600, Test MSE of 24714551069.758518
Epoch 55: training loss 24415854716.235
Test Loss of 20992750042.802406, Test MSE of 20992750370.546352
Epoch 56: training loss 23584752538.353
Test Loss of 23250158788.649700, Test MSE of 23250158850.627468
Epoch 57: training loss 22548386247.529
Test Loss of 23692299563.950024, Test MSE of 23692299646.816521
Epoch 58: training loss 21979190422.588
Test Loss of 22909008628.272095, Test MSE of 22909008278.560925
Epoch 59: training loss 21236580137.412
Test Loss of 21565908543.733456, Test MSE of 21565908448.985435
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24163761417.95128, 'MSE - std': 4347947917.687824, 'R2 - mean': 0.8215289899582034, 'R2 - std': 0.024966571291585495} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005525 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042504914.824
Test Loss of 431610432487.833435, Test MSE of 431610432034.943665
Epoch 2: training loss 424021939862.588
Test Loss of 431589422907.824158, Test MSE of 431589424367.243896
Epoch 3: training loss 423994545814.588
Test Loss of 431561948857.987976, Test MSE of 431561939924.188416
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010507926.588
Test Loss of 431562095987.975952, Test MSE of 431562103523.276428
Epoch 2: training loss 423996110727.529
Test Loss of 431565915105.199463, Test MSE of 431565924870.692017
Epoch 3: training loss 423995552948.706
Test Loss of 431566037579.579834, Test MSE of 431566025637.781921
Epoch 4: training loss 418751038162.824
Test Loss of 414633782099.990723, Test MSE of 414633774683.931213
Epoch 5: training loss 382695029217.882
Test Loss of 359108480536.877380, Test MSE of 359108482158.839294
Epoch 6: training loss 314662110208.000
Test Loss of 280298084644.842224, Test MSE of 280298087779.480347
Epoch 7: training loss 226908568274.824
Test Loss of 184389314305.066162, Test MSE of 184389316706.414062
Epoch 8: training loss 161720720986.353
Test Loss of 140100379958.374817, Test MSE of 140100380971.253326
Epoch 9: training loss 140820077387.294
Test Loss of 126647770658.354462, Test MSE of 126647768588.412827
Epoch 10: training loss 135480921569.882
Test Loss of 122025473347.642761, Test MSE of 122025475398.579254
Epoch 11: training loss 132144687932.235
Test Loss of 118475560688.007401, Test MSE of 118475560383.842804
Epoch 12: training loss 129424217178.353
Test Loss of 116175624553.077286, Test MSE of 116175625499.207703
Epoch 13: training loss 126039208146.824
Test Loss of 112549413387.135590, Test MSE of 112549411754.516693
Epoch 14: training loss 123410556355.765
Test Loss of 109790141762.695053, Test MSE of 109790144137.829712
Epoch 15: training loss 119737185099.294
Test Loss of 107020000525.623322, Test MSE of 107020002325.945938
Epoch 16: training loss 116774591533.176
Test Loss of 103148273224.262848, Test MSE of 103148271839.829529
Epoch 17: training loss 112724681758.118
Test Loss of 101540758986.217499, Test MSE of 101540761331.108490
Epoch 18: training loss 109658919243.294
Test Loss of 98082994339.953720, Test MSE of 98082993863.077362
Epoch 19: training loss 106273029609.412
Test Loss of 95694684588.364639, Test MSE of 95694684927.860168
Epoch 20: training loss 104557846603.294
Test Loss of 90245087804.416473, Test MSE of 90245089594.996674
Epoch 21: training loss 99253927243.294
Test Loss of 88277802917.019897, Test MSE of 88277800859.899750
Epoch 22: training loss 97052450063.059
Test Loss of 86937313458.169373, Test MSE of 86937312771.672165
Epoch 23: training loss 93561790192.941
Test Loss of 83558726015.822311, Test MSE of 83558725456.311890
Epoch 24: training loss 90385785569.882
Test Loss of 79116793557.471542, Test MSE of 79116793920.310150
Epoch 25: training loss 87215458620.235
Test Loss of 76323690400.281357, Test MSE of 76323690607.193222
Epoch 26: training loss 84403118817.882
Test Loss of 74635314838.922714, Test MSE of 74635313662.807297
Epoch 27: training loss 81490386612.706
Test Loss of 73392254701.164276, Test MSE of 73392255589.377380
Epoch 28: training loss 78323480079.059
Test Loss of 68693835362.798706, Test MSE of 68693835142.345627
Epoch 29: training loss 75271156329.412
Test Loss of 66712854474.928276, Test MSE of 66712855765.905571
Epoch 30: training loss 72297213244.235
Test Loss of 64508372526.674690, Test MSE of 64508373322.464676
Epoch 31: training loss 69613205443.765
Test Loss of 59695361602.576584, Test MSE of 59695360927.211479
Epoch 32: training loss 67145416666.353
Test Loss of 57605313992.322075, Test MSE of 57605313839.249901
Epoch 33: training loss 63814324675.765
Test Loss of 55504814968.951408, Test MSE of 55504815214.492073
Epoch 34: training loss 61786860152.471
Test Loss of 54923663160.033318, Test MSE of 54923664476.150894
Epoch 35: training loss 58387360210.824
Test Loss of 51021866516.138824, Test MSE of 51021866675.413681
Epoch 36: training loss 56349245696.000
Test Loss of 47809775973.286438, Test MSE of 47809776281.740669
Epoch 37: training loss 53892238689.882
Test Loss of 46724256650.010178, Test MSE of 46724256280.225739
Epoch 38: training loss 51191159205.647
Test Loss of 44257034969.736237, Test MSE of 44257034740.345520
Epoch 39: training loss 48998439363.765
Test Loss of 43272611314.021286, Test MSE of 43272610529.296524
Epoch 40: training loss 46746523459.765
Test Loss of 42811890002.332253, Test MSE of 42811890235.853394
Epoch 41: training loss 44526174825.412
Test Loss of 40298336568.744102, Test MSE of 40298335891.959648
Epoch 42: training loss 42476309466.353
Test Loss of 36985393002.261917, Test MSE of 36985393536.663002
Epoch 43: training loss 41146707335.529
Test Loss of 39423849867.668671, Test MSE of 39423849588.239548
Epoch 44: training loss 39449527529.412
Test Loss of 33001969879.130032, Test MSE of 33001970102.150631
Epoch 45: training loss 37722106496.000
Test Loss of 30854750942.474781, Test MSE of 30854751034.789593
Epoch 46: training loss 35752690266.353
Test Loss of 29184351118.748726, Test MSE of 29184351029.684055
Epoch 47: training loss 34537022817.882
Test Loss of 32806950843.764923, Test MSE of 32806950362.872482
Epoch 48: training loss 32372171294.118
Test Loss of 33056395967.200371, Test MSE of 33056396256.431065
Epoch 49: training loss 30879124954.353
Test Loss of 28580653344.103657, Test MSE of 28580652552.587070
Epoch 50: training loss 30234084141.176
Test Loss of 26427135209.610367, Test MSE of 26427135342.461266
Epoch 51: training loss 28547691768.471
Test Loss of 25676894177.673298, Test MSE of 25676893778.192547
Epoch 52: training loss 27797452656.941
Test Loss of 24386494408.085144, Test MSE of 24386493795.675297
Epoch 53: training loss 26184757473.882
Test Loss of 23266423765.826931, Test MSE of 23266423829.254742
Epoch 54: training loss 25053981703.529
Test Loss of 25053148897.791763, Test MSE of 25053149111.728981
Epoch 55: training loss 24200832037.647
Test Loss of 24733001041.858398, Test MSE of 24733000949.059853
Epoch 56: training loss 23590211041.882
Test Loss of 27923757335.574272, Test MSE of 27923756998.060577
Epoch 57: training loss 22527122258.824
Test Loss of 23236517798.915318, Test MSE of 23236517648.525036
Epoch 58: training loss 21942379719.529
Test Loss of 23604181372.979176, Test MSE of 23604181099.474968
Epoch 59: training loss 21188427753.412
Test Loss of 23927579368.899582, Test MSE of 23927578976.367901
Epoch 60: training loss 19985855856.941
Test Loss of 20984551028.805183, Test MSE of 20984550797.902592
Epoch 61: training loss 19753696952.471
Test Loss of 21808136614.678391, Test MSE of 21808137025.281734
Epoch 62: training loss 19176239465.412
Test Loss of 21385475290.447014, Test MSE of 21385475221.755077
Epoch 63: training loss 18460937773.176
Test Loss of 19424411589.242016, Test MSE of 19424411617.250767
Epoch 64: training loss 18337496018.824
Test Loss of 21632509505.628876, Test MSE of 21632509079.958557
Epoch 65: training loss 17572775815.529
Test Loss of 19813138272.784821, Test MSE of 19813137819.127556
Epoch 66: training loss 17028005605.647
Test Loss of 18722554737.369736, Test MSE of 18722554767.996002
Epoch 67: training loss 16487558008.471
Test Loss of 20536496429.371586, Test MSE of 20536495784.864799
Epoch 68: training loss 16014679514.353
Test Loss of 19017108906.943081, Test MSE of 19017108716.553661
Epoch 69: training loss 15626391171.765
Test Loss of 20056812764.342434, Test MSE of 20056812763.053013
Epoch 70: training loss 15082945965.176
Test Loss of 18713796820.286903, Test MSE of 18713796867.246227
Epoch 71: training loss 14902230411.294
Test Loss of 19196574578.317444, Test MSE of 19196574896.436604
Epoch 72: training loss 14551432293.647
Test Loss of 20098415522.650623, Test MSE of 20098415821.926056
Epoch 73: training loss 14179830351.059
Test Loss of 18985308684.083294, Test MSE of 18985309159.014214
Epoch 74: training loss 14068808222.118
Test Loss of 19599753151.555759, Test MSE of 19599753093.271713
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23250959753.015366, 'MSE - std': 4296108517.481817, 'R2 - mean': 0.8279488552095131, 'R2 - std': 0.025758929034922152} 
 

Saving model.....
Results After CV: {'MSE - mean': 23250959753.015366, 'MSE - std': 4296108517.481817, 'R2 - mean': 0.8279488552095131, 'R2 - std': 0.025758929034922152}
Train time: 101.43052513820001
Inference time: 0.07564306040001441
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 0 finished with value: 23250959753.015366 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.7}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524291041.882
Test Loss of 418110859063.605835, Test MSE of 418110855979.226074
Epoch 2: training loss 427502393584.941
Test Loss of 418093255689.238037, Test MSE of 418093257633.218811
Epoch 3: training loss 427475017607.529
Test Loss of 418069795772.017578, Test MSE of 418069798009.671265
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491501357.176
Test Loss of 418074573745.121460, Test MSE of 418074576042.257141
Epoch 2: training loss 427481854674.824
Test Loss of 418076379752.342346, Test MSE of 418076380108.859558
Epoch 3: training loss 427481451821.176
Test Loss of 418076287565.338867, Test MSE of 418076298325.152283
Epoch 4: training loss 427481151488.000
Test Loss of 418075862058.873901, Test MSE of 418075867523.589783
Epoch 5: training loss 427480929822.118
Test Loss of 418075595230.482544, Test MSE of 418075593413.492493
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418075593413.4925, 'MSE - std': 0.0, 'R2 - mean': -2.255600006767708, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005587 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916378112.000
Test Loss of 424555337522.868408, Test MSE of 424555332755.993896
Epoch 2: training loss 427893788190.118
Test Loss of 424537978479.685425, Test MSE of 424537972563.101990
Epoch 3: training loss 427865743600.941
Test Loss of 424515317751.472595, Test MSE of 424515322398.325745
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427882979087.059
Test Loss of 424517826826.718506, Test MSE of 424517825575.464050
Epoch 2: training loss 427874378450.824
Test Loss of 424519253026.820251, Test MSE of 424519251084.436646
Epoch 3: training loss 427873865728.000
Test Loss of 424519977409.584106, Test MSE of 424519977669.640625
Epoch 4: training loss 427873436611.765
Test Loss of 424519911441.528564, Test MSE of 424519909767.475220
Epoch 5: training loss 427873122665.412
Test Loss of 424519085033.023376, Test MSE of 424519080641.149536
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 421297337027.32104, 'MSE - std': 3221743613.8285217, 'R2 - mean': -2.14319219614158, 'R2 - std': 0.11240781062612815} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004092 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926352896.000
Test Loss of 447258129758.808228, Test MSE of 447258130111.644958
Epoch 2: training loss 421905234281.412
Test Loss of 447238945841.980103, Test MSE of 447238947603.703796
Epoch 3: training loss 421878123459.765
Test Loss of 447213965407.696533, Test MSE of 447213963410.498047
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895291482.353
Test Loss of 447222038084.574585, Test MSE of 447222036253.226196
Epoch 2: training loss 421886773127.529
Test Loss of 447223654032.847534, Test MSE of 447223650692.455566
Epoch 3: training loss 421886354733.176
Test Loss of 447224390710.243835, Test MSE of 447224388993.143311
Epoch 4: training loss 421886010669.176
Test Loss of 447224726667.517944, Test MSE of 447224718998.400085
Epoch 5: training loss 421885774064.941
Test Loss of 447224739007.629883, Test MSE of 447224736771.092407
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 429939803608.5781, 'MSE - std': 12502168284.946518, 'R2 - mean': -2.087843316914182, 'R2 - std': 0.12062617511675397} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005505 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430105113057.882
Test Loss of 410760805914.772766, Test MSE of 410760809993.593445
Epoch 2: training loss 430082473984.000
Test Loss of 410741910136.122192, Test MSE of 410741916005.537842
Epoch 3: training loss 430055922266.353
Test Loss of 410719218227.887085, Test MSE of 410719222612.674011
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075426213.647
Test Loss of 410720775670.759827, Test MSE of 410720777148.011353
Epoch 2: training loss 430061632090.353
Test Loss of 410721906977.999084, Test MSE of 410721909978.076843
Epoch 3: training loss 430061257908.706
Test Loss of 410721855832.018494, Test MSE of 410721852601.744995
Epoch 4: training loss 430061040579.765
Test Loss of 410721407800.507202, Test MSE of 410721414252.674500
Epoch 5: training loss 430060858910.118
Test Loss of 410721436898.976379, Test MSE of 410721443878.020447
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 425135213675.9387, 'MSE - std': 13655783102.089102, 'R2 - mean': -2.1633570197524223, 'R2 - std': 0.16739164716765148} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005649 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042650443.294
Test Loss of 431611781783.396545, Test MSE of 431611782861.878174
Epoch 2: training loss 424022140687.059
Test Loss of 431591593071.355835, Test MSE of 431591593807.163757
Epoch 3: training loss 423994469797.647
Test Loss of 431563995243.091187, Test MSE of 431563999582.306152
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010324811.294
Test Loss of 431567564392.484985, Test MSE of 431567564428.000244
Epoch 2: training loss 423998199687.529
Test Loss of 431569793981.186462, Test MSE of 431569788147.513855
Epoch 3: training loss 423997552037.647
Test Loss of 431569051826.169373, Test MSE of 431569047603.922607
Epoch 4: training loss 423997096176.941
Test Loss of 431569381447.078186, Test MSE of 431569384405.832275
Epoch 5: training loss 423996825359.059
Test Loss of 431568864922.713562, Test MSE of 431568871735.955139
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 426421945287.942, 'MSE - std': 12482269131.466284, 'R2 - mean': -2.175280087218789, 'R2 - std': 0.15160675771359844} 
 

Saving model.....
Results After CV: {'MSE - mean': 426421945287.942, 'MSE - std': 12482269131.466284, 'R2 - mean': -2.175280087218789, 'R2 - std': 0.15160675771359844}
Train time: 13.487675994599977
Inference time: 0.0730669328000431
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 1 finished with value: 426421945287.942 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 9, 'loss_dr': 0.7}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004131 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524943631.059
Test Loss of 418110535764.800354, Test MSE of 418110526395.812744
Epoch 2: training loss 427503481193.412
Test Loss of 418091945355.814026, Test MSE of 418091939271.460083
Epoch 3: training loss 427475632368.941
Test Loss of 418067884851.105225, Test MSE of 418067887756.621399
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494028227.765
Test Loss of 418074765297.313904, Test MSE of 418074772059.336182
Epoch 2: training loss 427480275184.941
Test Loss of 418075404875.917664, Test MSE of 418075412901.653503
Epoch 3: training loss 427479771858.824
Test Loss of 418074262286.153137, Test MSE of 418074265969.056885
Epoch 4: training loss 427479414422.588
Test Loss of 418074025004.295166, Test MSE of 418074027807.727539
Epoch 5: training loss 427479227452.235
Test Loss of 418072866299.617859, Test MSE of 418072872646.596375
Epoch 6: training loss 427479056745.412
Test Loss of 418073173513.830200, Test MSE of 418073177718.494080
Epoch 7: training loss 417561003791.059
Test Loss of 386877608861.697876, Test MSE of 386877604312.621094
Epoch 8: training loss 355713914036.706
Test Loss of 300909607159.768677, Test MSE of 300909605884.895508
Epoch 9: training loss 262330584726.588
Test Loss of 207890627076.855896, Test MSE of 207890631510.395630
Epoch 10: training loss 186547556382.118
Test Loss of 148800756954.633362, Test MSE of 148800755800.417603
Epoch 11: training loss 150564456839.529
Test Loss of 125936318238.023590, Test MSE of 125936317718.230469
Epoch 12: training loss 138857304184.471
Test Loss of 118968974992.373810, Test MSE of 118968976045.214554
Epoch 13: training loss 135244383397.647
Test Loss of 114263421569.450851, Test MSE of 114263422953.714615
Epoch 14: training loss 131365900860.235
Test Loss of 111153976394.141113, Test MSE of 111153976236.113525
Epoch 15: training loss 127952624007.529
Test Loss of 108254111761.528564, Test MSE of 108254113361.410126
Epoch 16: training loss 124044266797.176
Test Loss of 105363169425.913483, Test MSE of 105363170662.064133
Epoch 17: training loss 120867144011.294
Test Loss of 102083204272.470047, Test MSE of 102083205885.072174
Epoch 18: training loss 118212045869.176
Test Loss of 98952326814.586166, Test MSE of 98952323937.790665
Epoch 19: training loss 114339413142.588
Test Loss of 94976620880.832748, Test MSE of 94976621567.677567
Epoch 20: training loss 109472726979.765
Test Loss of 91337148986.862823, Test MSE of 91337150995.892471
Epoch 21: training loss 105826975503.059
Test Loss of 88450839050.067078, Test MSE of 88450840060.177216
Epoch 22: training loss 102011021643.294
Test Loss of 85715166789.048340, Test MSE of 85715166788.113800
Epoch 23: training loss 98271253699.765
Test Loss of 82191232556.887344, Test MSE of 82191233106.394608
Epoch 24: training loss 94403023077.647
Test Loss of 80073798913.954193, Test MSE of 80073797630.310608
Epoch 25: training loss 90609519329.882
Test Loss of 75838075574.510300, Test MSE of 75838075206.228943
Epoch 26: training loss 87212949669.647
Test Loss of 72980413554.409439, Test MSE of 72980413359.088043
Epoch 27: training loss 84235361295.059
Test Loss of 68852801471.570663, Test MSE of 68852800668.813934
Epoch 28: training loss 81242153185.882
Test Loss of 67786937706.178116, Test MSE of 67786937593.950958
Epoch 29: training loss 77745012630.588
Test Loss of 66917207240.631042, Test MSE of 66917206867.018074
Epoch 30: training loss 74506871040.000
Test Loss of 62443194927.492943, Test MSE of 62443195336.758224
Epoch 31: training loss 70955776058.353
Test Loss of 57230328639.896370, Test MSE of 57230329973.350456
Epoch 32: training loss 68380110418.824
Test Loss of 57363889023.851952, Test MSE of 57363888917.970886
Epoch 33: training loss 65659644069.647
Test Loss of 56985009747.734444, Test MSE of 56985008541.388000
Epoch 34: training loss 62363833590.588
Test Loss of 52921458196.252602, Test MSE of 52921458252.586243
Epoch 35: training loss 59455132344.471
Test Loss of 50174277212.735603, Test MSE of 50174278123.642136
Epoch 36: training loss 57640913438.118
Test Loss of 50246151030.377052, Test MSE of 50246150204.784348
Epoch 37: training loss 55028326588.235
Test Loss of 48380653333.022438, Test MSE of 48380653231.820915
Epoch 38: training loss 52302378176.000
Test Loss of 43194774797.324081, Test MSE of 43194775088.830475
Epoch 39: training loss 50105498925.176
Test Loss of 40762439495.713165, Test MSE of 40762438967.801270
Epoch 40: training loss 47385135736.471
Test Loss of 41397317592.205414, Test MSE of 41397317469.216942
Epoch 41: training loss 46269038396.235
Test Loss of 38101668219.943558, Test MSE of 38101668107.586800
Epoch 42: training loss 43178843064.471
Test Loss of 38437374503.913025, Test MSE of 38437374501.457802
Epoch 43: training loss 41597928481.882
Test Loss of 33460258658.242886, Test MSE of 33460258624.108387
Epoch 44: training loss 40022166640.941
Test Loss of 32865715716.619015, Test MSE of 32865715689.190304
Epoch 45: training loss 37607338676.706
Test Loss of 34261697774.056904, Test MSE of 34261697916.136269
Epoch 46: training loss 36295009814.588
Test Loss of 31560611758.989590, Test MSE of 31560611760.049110
Epoch 47: training loss 34928859580.235
Test Loss of 29836633031.387463, Test MSE of 29836633230.208710
Epoch 48: training loss 33101495649.882
Test Loss of 29613052073.600739, Test MSE of 29613052477.327770
Epoch 49: training loss 31264436547.765
Test Loss of 28303175144.668056, Test MSE of 28303174687.800400
Epoch 50: training loss 30082857035.294
Test Loss of 26850108255.163544, Test MSE of 26850109099.551548
Epoch 51: training loss 28477866533.647
Test Loss of 25030367663.581772, Test MSE of 25030367718.494770
Epoch 52: training loss 28012608271.059
Test Loss of 26266641848.819801, Test MSE of 26266641781.381641
Epoch 53: training loss 26489025460.706
Test Loss of 24388651463.269028, Test MSE of 24388651642.338573
Epoch 54: training loss 25420574189.176
Test Loss of 21433396946.224380, Test MSE of 21433397297.200050
Epoch 55: training loss 23951610243.765
Test Loss of 21249118140.254452, Test MSE of 21249117716.414490
Epoch 56: training loss 23581652649.412
Test Loss of 22586538448.270184, Test MSE of 22586539079.695713
Epoch 57: training loss 22410671510.588
Test Loss of 22320366270.090214, Test MSE of 22320366729.408157
Epoch 58: training loss 21418791326.118
Test Loss of 22320479385.730278, Test MSE of 22320479550.523811
Epoch 59: training loss 20866605556.706
Test Loss of 19750806112.288689, Test MSE of 19750806601.718258
Epoch 60: training loss 20288692653.176
Test Loss of 20539720943.951885, Test MSE of 20539721199.218983
Epoch 61: training loss 19562486960.941
Test Loss of 20609292033.125145, Test MSE of 20609292066.543842
Epoch 62: training loss 18599252758.588
Test Loss of 21051747667.438354, Test MSE of 21051747804.078876
Epoch 63: training loss 17995522578.824
Test Loss of 18699696978.372425, Test MSE of 18699697505.270832
Epoch 64: training loss 17454248519.529
Test Loss of 20327606430.230858, Test MSE of 20327606426.662334
Epoch 65: training loss 17084421816.471
Test Loss of 19331777269.044643, Test MSE of 19331777262.804432
Epoch 66: training loss 16641615943.529
Test Loss of 17590317609.334259, Test MSE of 17590317789.296234
Epoch 67: training loss 15979126814.118
Test Loss of 18464753066.607449, Test MSE of 18464753382.348671
Epoch 68: training loss 15869917387.294
Test Loss of 18771012087.591026, Test MSE of 18771011828.501251
Epoch 69: training loss 15310775717.647
Test Loss of 17730373990.625031, Test MSE of 17730373962.517544
Epoch 70: training loss 14557498432.000
Test Loss of 17405288917.007633, Test MSE of 17405289034.195755
Epoch 71: training loss 14298409743.059
Test Loss of 18108766195.682629, Test MSE of 18108766405.569019
Epoch 72: training loss 13858625029.647
Test Loss of 18005480860.868839, Test MSE of 18005480912.677856
Epoch 73: training loss 13671423088.941
Test Loss of 16982828967.883415, Test MSE of 16982829109.677315
Epoch 74: training loss 13167600086.588
Test Loss of 17402821776.729122, Test MSE of 17402821772.116795
Epoch 75: training loss 13063414840.471
Test Loss of 18949092804.663429, Test MSE of 18949092844.948994
Epoch 76: training loss 12405322800.941
Test Loss of 16645336312.479298, Test MSE of 16645336246.660154
Epoch 77: training loss 12115843621.647
Test Loss of 18348152627.815868, Test MSE of 18348152700.902172
Epoch 78: training loss 11862542979.765
Test Loss of 19192813265.750637, Test MSE of 19192813808.407364
Epoch 79: training loss 11744909545.412
Test Loss of 19365722268.335876, Test MSE of 19365722294.841286
Epoch 80: training loss 11386633406.118
Test Loss of 18074269584.196159, Test MSE of 18074269784.556530
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18074269784.55653, 'MSE - std': 0.0, 'R2 - mean': 0.8592537001443037, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002862 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918161317.647
Test Loss of 424556705645.612793, Test MSE of 424556702880.696289
Epoch 2: training loss 427897705893.647
Test Loss of 424540642496.103638, Test MSE of 424540652208.921448
Epoch 3: training loss 427869516016.941
Test Loss of 424518319885.442505, Test MSE of 424518320575.782410
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884935047.529
Test Loss of 424523689448.668030, Test MSE of 424523692039.071350
Epoch 2: training loss 427877317210.353
Test Loss of 424524863616.384888, Test MSE of 424524865596.564758
Epoch 3: training loss 427876821353.412
Test Loss of 424524413513.075195, Test MSE of 424524415480.807678
Epoch 4: training loss 427876505961.412
Test Loss of 424524009647.285706, Test MSE of 424524003880.507507
Epoch 5: training loss 427876290319.059
Test Loss of 424524042271.267151, Test MSE of 424524043892.747070
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221299156838.6518, 'MSE - std': 203224887054.09528, 'R2 - mean': -0.5857830598461553, 'R2 - std': 1.445036759990459} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005519 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927583623.529
Test Loss of 447259259758.323364, Test MSE of 447259260776.539124
Epoch 2: training loss 421907046400.000
Test Loss of 447241361411.316223, Test MSE of 447241355739.139832
Epoch 3: training loss 421879541278.118
Test Loss of 447217152218.870239, Test MSE of 447217156242.728516
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902370454.588
Test Loss of 447226333656.323853, Test MSE of 447226333819.786438
Epoch 2: training loss 421889584549.647
Test Loss of 447226055215.492920, Test MSE of 447226050488.782593
Epoch 3: training loss 421889033035.294
Test Loss of 447225427461.566528, Test MSE of 447225428610.181152
Epoch 4: training loss 421888726678.588
Test Loss of 447225634908.854065, Test MSE of 447225633019.900330
Epoch 5: training loss 421888511156.706
Test Loss of 447225061927.439270, Test MSE of 447225068149.035034
Epoch 6: training loss 421888376591.059
Test Loss of 447224619836.106384, Test MSE of 447224618974.896606
Epoch 7: training loss 412716043685.647
Test Loss of 417502489649.032593, Test MSE of 417502489072.619690
Epoch 8: training loss 352760357827.765
Test Loss of 330914686707.386536, Test MSE of 330914687739.477844
Epoch 9: training loss 259478097739.294
Test Loss of 235038349451.754791, Test MSE of 235038349987.787720
Epoch 10: training loss 183732405790.118
Test Loss of 174358420763.062683, Test MSE of 174358420310.871399
Epoch 11: training loss 146507494189.176
Test Loss of 148789163013.921814, Test MSE of 148789169521.694824
Epoch 12: training loss 136136115019.294
Test Loss of 141792706036.985413, Test MSE of 141792707822.550995
Epoch 13: training loss 132239521551.059
Test Loss of 136083666092.680084, Test MSE of 136083666915.709930
Epoch 14: training loss 126725652269.176
Test Loss of 133381544250.092987, Test MSE of 133381545737.861069
Epoch 15: training loss 123712643102.118
Test Loss of 129291745728.636597, Test MSE of 129291740446.681854
Epoch 16: training loss 120200705264.941
Test Loss of 125697878376.520004, Test MSE of 125697878481.008179
Epoch 17: training loss 117495574678.588
Test Loss of 123126206995.541992, Test MSE of 123126207421.762207
Epoch 18: training loss 113778832384.000
Test Loss of 119621404128.614395, Test MSE of 119621402649.955750
Epoch 19: training loss 110710851373.176
Test Loss of 117356591624.645844, Test MSE of 117356590970.272720
Epoch 20: training loss 107253927966.118
Test Loss of 110630104840.705063, Test MSE of 110630104215.232864
Epoch 21: training loss 102761230275.765
Test Loss of 110235101896.986359, Test MSE of 110235100056.330521
Epoch 22: training loss 99083502351.059
Test Loss of 106747305053.564651, Test MSE of 106747308280.013275
Epoch 23: training loss 96126047894.588
Test Loss of 101363556717.731201, Test MSE of 101363553577.006256
Epoch 24: training loss 91803699169.882
Test Loss of 98088040705.243576, Test MSE of 98088040532.978088
Epoch 25: training loss 88259671341.176
Test Loss of 94339635224.160995, Test MSE of 94339637386.354675
Epoch 26: training loss 84466623457.882
Test Loss of 92325526593.139954, Test MSE of 92325527428.471619
Epoch 27: training loss 81931272779.294
Test Loss of 85948515511.102478, Test MSE of 85948517304.581497
Epoch 28: training loss 77780824560.941
Test Loss of 82554240474.455704, Test MSE of 82554241455.577255
Epoch 29: training loss 75285483504.941
Test Loss of 80837008070.143875, Test MSE of 80837009853.908630
Epoch 30: training loss 71968501880.471
Test Loss of 78409816173.435120, Test MSE of 78409816931.907089
Epoch 31: training loss 69076934520.471
Test Loss of 72404289303.391159, Test MSE of 72404288050.931870
Epoch 32: training loss 66198896067.765
Test Loss of 72163611972.278503, Test MSE of 72163611047.583557
Epoch 33: training loss 64023533552.941
Test Loss of 68265578622.016190, Test MSE of 68265577673.924706
Epoch 34: training loss 61244113814.588
Test Loss of 62672504893.113113, Test MSE of 62672503796.544945
Epoch 35: training loss 58268737189.647
Test Loss of 61663271246.937775, Test MSE of 61663270542.013794
Epoch 36: training loss 56170660517.647
Test Loss of 59498280332.287766, Test MSE of 59498280941.432129
Epoch 37: training loss 52873594759.529
Test Loss of 56429571896.079575, Test MSE of 56429571823.038803
Epoch 38: training loss 51025805402.353
Test Loss of 54357856205.783020, Test MSE of 54357855896.664879
Epoch 39: training loss 48739612679.529
Test Loss of 52861828018.068932, Test MSE of 52861828703.904823
Epoch 40: training loss 46665438561.882
Test Loss of 50942462069.962524, Test MSE of 50942463002.709747
Epoch 41: training loss 44697768289.882
Test Loss of 48490624401.025215, Test MSE of 48490624746.578964
Epoch 42: training loss 42497465893.647
Test Loss of 47697943304.468193, Test MSE of 47697943724.851265
Epoch 43: training loss 40465002601.412
Test Loss of 44883910151.935226, Test MSE of 44883910597.307838
Epoch 44: training loss 38594422738.824
Test Loss of 42244802988.739304, Test MSE of 42244802781.857826
Epoch 45: training loss 37118591284.706
Test Loss of 43731305870.182747, Test MSE of 43731305859.077293
Epoch 46: training loss 34887948498.824
Test Loss of 38875795653.077957, Test MSE of 38875794817.849709
Epoch 47: training loss 33422379753.412
Test Loss of 39142009537.406433, Test MSE of 39142009751.994003
Epoch 48: training loss 32483480064.000
Test Loss of 33742660979.653019, Test MSE of 33742660303.211094
Epoch 49: training loss 30608945302.588
Test Loss of 33283450707.083046, Test MSE of 33283450974.279182
Epoch 50: training loss 29486858646.588
Test Loss of 33204218878.815636, Test MSE of 33204219683.910759
Epoch 51: training loss 28035126874.353
Test Loss of 32884520757.710850, Test MSE of 32884521104.041637
Epoch 52: training loss 26661845763.765
Test Loss of 30421951764.904003, Test MSE of 30421951903.468071
Epoch 53: training loss 25633278102.588
Test Loss of 30674520181.962524, Test MSE of 30674520533.040092
Epoch 54: training loss 24779479887.059
Test Loss of 27687736401.247280, Test MSE of 27687736587.638660
Epoch 55: training loss 23660801950.118
Test Loss of 28425872296.120285, Test MSE of 28425871824.891632
Epoch 56: training loss 22330053918.118
Test Loss of 25954410774.562111, Test MSE of 25954411222.528946
Epoch 57: training loss 21672388781.176
Test Loss of 26619314467.826973, Test MSE of 26619314498.453499
Epoch 58: training loss 20952310132.706
Test Loss of 28278491926.206802, Test MSE of 28278492024.762211
Epoch 59: training loss 20362150599.529
Test Loss of 27269921595.158916, Test MSE of 27269921974.626461
Epoch 60: training loss 19263184564.706
Test Loss of 25892693640.793892, Test MSE of 25892693256.324665
Epoch 61: training loss 18974911619.765
Test Loss of 28068351756.968773, Test MSE of 28068352151.403011
Epoch 62: training loss 18159888432.941
Test Loss of 26561960050.646309, Test MSE of 26561959803.359119
Epoch 63: training loss 17515596193.882
Test Loss of 26289725158.121674, Test MSE of 26289725077.113052
Epoch 64: training loss 17126986081.882
Test Loss of 24919754100.600510, Test MSE of 24919753829.280121
Epoch 65: training loss 16633020464.941
Test Loss of 23706172911.774231, Test MSE of 23706172925.227863
Epoch 66: training loss 16143873404.235
Test Loss of 23643407976.105484, Test MSE of 23643407970.946751
Epoch 67: training loss 15509712207.059
Test Loss of 23375282345.600739, Test MSE of 23375282456.686958
Epoch 68: training loss 15363719713.882
Test Loss of 22896634296.109184, Test MSE of 22896634279.552319
Epoch 69: training loss 14686570032.941
Test Loss of 22515700453.411057, Test MSE of 22515700948.464413
Epoch 70: training loss 14562116069.647
Test Loss of 21895857646.352997, Test MSE of 21895857905.408325
Epoch 71: training loss 13941400248.471
Test Loss of 22560657651.978718, Test MSE of 22560657544.404633
Epoch 72: training loss 13561002906.353
Test Loss of 25333557276.424706, Test MSE of 25333557136.465000
Epoch 73: training loss 13349248892.235
Test Loss of 23630303744.118435, Test MSE of 23630303712.358994
Epoch 74: training loss 12889284491.294
Test Loss of 23398694246.861900, Test MSE of 23398693912.397003
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 155332335863.23352, 'MSE - std': 190359693084.71164, 'R2 - mean': -0.10910989362990968, 'R2 - std': 1.3588679503899768} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003901 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109857912.471
Test Loss of 410764722426.195251, Test MSE of 410764723025.589294
Epoch 2: training loss 430088405232.941
Test Loss of 410745974399.229980, Test MSE of 410745975790.377686
Epoch 3: training loss 430060276314.353
Test Loss of 410722034213.671448, Test MSE of 410722026914.324158
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074566053.647
Test Loss of 410726128560.866272, Test MSE of 410726130583.815735
Epoch 2: training loss 430065531241.412
Test Loss of 410727217607.848206, Test MSE of 410727217090.695251
Epoch 3: training loss 430065004303.059
Test Loss of 410726661749.752869, Test MSE of 410726656211.710754
Epoch 4: training loss 430064597353.412
Test Loss of 410726231610.994934, Test MSE of 410726230345.151550
Epoch 5: training loss 430064336655.059
Test Loss of 410726007572.494202, Test MSE of 410726006209.888733
Epoch 6: training loss 430064174260.706
Test Loss of 410725245887.081909, Test MSE of 410725248765.319214
Epoch 7: training loss 420897619727.059
Test Loss of 381525434531.006042, Test MSE of 381525430823.793274
Epoch 8: training loss 360563264692.706
Test Loss of 296473684501.560364, Test MSE of 296473688564.206482
Epoch 9: training loss 267161789018.353
Test Loss of 203548545484.586761, Test MSE of 203548544475.322357
Epoch 10: training loss 190556305227.294
Test Loss of 142326897463.559448, Test MSE of 142326896167.117096
Epoch 11: training loss 153509067233.882
Test Loss of 120159411916.942154, Test MSE of 120159409887.082657
Epoch 12: training loss 140505946925.176
Test Loss of 113045223644.816284, Test MSE of 113045223240.397583
Epoch 13: training loss 137490197353.412
Test Loss of 109592424916.168442, Test MSE of 109592423294.783768
Epoch 14: training loss 132080399902.118
Test Loss of 106574237898.809814, Test MSE of 106574236704.309326
Epoch 15: training loss 130444235203.765
Test Loss of 103163392396.142532, Test MSE of 103163392423.399033
Epoch 16: training loss 125584395565.176
Test Loss of 100417016439.174454, Test MSE of 100417016403.084808
Epoch 17: training loss 123204510388.706
Test Loss of 97655123378.524750, Test MSE of 97655121610.495407
Epoch 18: training loss 119096762608.941
Test Loss of 94091989850.624710, Test MSE of 94091989910.544220
Epoch 19: training loss 116310092980.706
Test Loss of 91750424792.077744, Test MSE of 91750425292.469086
Epoch 20: training loss 111430298624.000
Test Loss of 87965497655.796387, Test MSE of 87965496309.178223
Epoch 21: training loss 107998843843.765
Test Loss of 84082081568.340576, Test MSE of 84082082019.780594
Epoch 22: training loss 104983029127.529
Test Loss of 82489899353.913925, Test MSE of 82489900148.662720
Epoch 23: training loss 99232526366.118
Test Loss of 80444564562.924576, Test MSE of 80444564580.492249
Epoch 24: training loss 95638962898.824
Test Loss of 76223947316.834793, Test MSE of 76223946333.951782
Epoch 25: training loss 92463972472.471
Test Loss of 74181750380.275803, Test MSE of 74181750046.288223
Epoch 26: training loss 89792135996.235
Test Loss of 70809679713.732529, Test MSE of 70809679352.891983
Epoch 27: training loss 85325552248.471
Test Loss of 68186093686.463676, Test MSE of 68186092174.183281
Epoch 28: training loss 81997596672.000
Test Loss of 66870908496.792229, Test MSE of 66870907462.594963
Epoch 29: training loss 78893410695.529
Test Loss of 62135944255.970383, Test MSE of 62135944327.180901
Epoch 30: training loss 76203915821.176
Test Loss of 60593176079.400276, Test MSE of 60593176299.645309
Epoch 31: training loss 72771662411.294
Test Loss of 58098188735.318832, Test MSE of 58098189621.034378
Epoch 32: training loss 70257960342.588
Test Loss of 57095217546.247108, Test MSE of 57095217148.125069
Epoch 33: training loss 67249020611.765
Test Loss of 54420252214.256363, Test MSE of 54420253381.398300
Epoch 34: training loss 64201752357.647
Test Loss of 50826485206.537712, Test MSE of 50826486788.624733
Epoch 35: training loss 60783443328.000
Test Loss of 47498174786.221191, Test MSE of 47498175127.387146
Epoch 36: training loss 58206982558.118
Test Loss of 46624968630.078667, Test MSE of 46624968736.822762
Epoch 37: training loss 56161248609.882
Test Loss of 45639953571.479874, Test MSE of 45639953840.786713
Epoch 38: training loss 53453377475.765
Test Loss of 42312386502.189728, Test MSE of 42312386169.610092
Epoch 39: training loss 51290697283.765
Test Loss of 41266866863.089310, Test MSE of 41266866848.288666
Epoch 40: training loss 49243207612.235
Test Loss of 39428746284.068489, Test MSE of 39428747080.078766
Epoch 41: training loss 45574927201.882
Test Loss of 37385779642.580284, Test MSE of 37385779892.824333
Epoch 42: training loss 44459637451.294
Test Loss of 36628103797.279037, Test MSE of 36628104329.629364
Epoch 43: training loss 42729012924.235
Test Loss of 33396314703.370663, Test MSE of 33396314574.773975
Epoch 44: training loss 40441891365.647
Test Loss of 32710564108.201759, Test MSE of 32710564455.699646
Epoch 45: training loss 38637288847.059
Test Loss of 31937771649.836185, Test MSE of 31937771412.254036
Epoch 46: training loss 37387018744.471
Test Loss of 29932270347.017120, Test MSE of 29932270138.833317
Epoch 47: training loss 34731691994.353
Test Loss of 28143883589.538177, Test MSE of 28143883536.021130
Epoch 48: training loss 33247602307.765
Test Loss of 28648649446.056454, Test MSE of 28648649065.080418
Epoch 49: training loss 32295882563.765
Test Loss of 26396750516.775566, Test MSE of 26396750812.913074
Epoch 50: training loss 30802810066.824
Test Loss of 23571959209.995373, Test MSE of 23571958834.674362
Epoch 51: training loss 30062924559.059
Test Loss of 26797362669.282738, Test MSE of 26797362907.856670
Epoch 52: training loss 28517592131.765
Test Loss of 25634712186.017586, Test MSE of 25634712125.425835
Epoch 53: training loss 26803602070.588
Test Loss of 24490649050.328552, Test MSE of 24490649871.631672
Epoch 54: training loss 25796904941.176
Test Loss of 23088667194.521053, Test MSE of 23088667155.112549
Epoch 55: training loss 24652872097.882
Test Loss of 22408348933.093937, Test MSE of 22408348945.853790
Epoch 56: training loss 23680740698.353
Test Loss of 21577436753.739937, Test MSE of 21577436744.781776
Epoch 57: training loss 23020355230.118
Test Loss of 20033468609.332718, Test MSE of 20033468598.215492
Epoch 58: training loss 22246537709.176
Test Loss of 22480314503.522442, Test MSE of 22480314461.291458
Epoch 59: training loss 21624951593.412
Test Loss of 19727575808.592319, Test MSE of 19727575771.866169
Epoch 60: training loss 20856746593.882
Test Loss of 19742022900.982880, Test MSE of 19742023079.835392
Epoch 61: training loss 20008523203.765
Test Loss of 19089922475.416935, Test MSE of 19089922605.256573
Epoch 62: training loss 19315956073.412
Test Loss of 19984791870.430355, Test MSE of 19984792073.842110
Epoch 63: training loss 18505744097.882
Test Loss of 19313712617.491901, Test MSE of 19313712588.484150
Epoch 64: training loss 18129491019.294
Test Loss of 18451730226.347061, Test MSE of 18451730120.576321
Epoch 65: training loss 17797828094.118
Test Loss of 19134999290.906063, Test MSE of 19134998968.669308
Epoch 66: training loss 17017775073.882
Test Loss of 18418823308.260990, Test MSE of 18418823255.749668
Epoch 67: training loss 16680995226.353
Test Loss of 20174915703.411385, Test MSE of 20174915635.347473
Epoch 68: training loss 16170775149.176
Test Loss of 17769528282.091625, Test MSE of 17769528252.523376
Epoch 69: training loss 15667821308.235
Test Loss of 18696931663.489124, Test MSE of 18696931829.342083
Epoch 70: training loss 15207746612.706
Test Loss of 16895330023.478020, Test MSE of 16895329799.538292
Epoch 71: training loss 14936055642.353
Test Loss of 18814177821.142063, Test MSE of 18814177939.633179
Epoch 72: training loss 14628121438.118
Test Loss of 19150575949.593708, Test MSE of 19150576233.515228
Epoch 73: training loss 14112050646.588
Test Loss of 19127787585.391949, Test MSE of 19127787224.643394
Epoch 74: training loss 13768924579.765
Test Loss of 18009257618.184174, Test MSE of 18009257383.140884
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 121001566243.21036, 'MSE - std': 175252431628.1761, 'R2 - mean': 0.1310076330517757, 'R2 - std': 1.2481429644904394} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005504 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043618183.529
Test Loss of 431613409220.294312, Test MSE of 431613402280.364746
Epoch 2: training loss 424024306627.765
Test Loss of 431593699472.999512, Test MSE of 431593701672.117859
Epoch 3: training loss 423996923181.176
Test Loss of 431566092410.254517, Test MSE of 431566090221.231262
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011645530.353
Test Loss of 431569150842.846802, Test MSE of 431569144731.100098
Epoch 2: training loss 424000414780.235
Test Loss of 431572236857.573364, Test MSE of 431572233756.895996
Epoch 3: training loss 423999828690.824
Test Loss of 431572221785.203125, Test MSE of 431572216808.316467
Epoch 4: training loss 423999388973.176
Test Loss of 431571674206.297058, Test MSE of 431571671168.758667
Epoch 5: training loss 423999110204.235
Test Loss of 431570539426.650635, Test MSE of 431570541584.871338
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 183115361311.54254, 'MSE - std': 200008064798.4823, 'R2 - mean': -0.3397908590741712, 'R2 - std': 1.4604429357553985} 
 

Saving model.....
Results After CV: {'MSE - mean': 183115361311.54254, 'MSE - std': 200008064798.4823, 'R2 - mean': -0.3397908590741712, 'R2 - std': 1.4604429357553985}
Train time: 74.40737904580001
Inference time: 0.07193332340000325
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 2 finished with value: 183115361311.54254 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 6, 'loss_dr': 0.7}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005450 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524358746.353
Test Loss of 418111302913.480469, Test MSE of 418111313377.270386
Epoch 2: training loss 427502866913.882
Test Loss of 418092467271.298645, Test MSE of 418092464857.881348
Epoch 3: training loss 427475288786.824
Test Loss of 418068474788.804077, Test MSE of 418068469670.080139
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491142354.824
Test Loss of 418074511203.190369, Test MSE of 418074506186.064148
Epoch 2: training loss 427480639247.059
Test Loss of 418076262589.498047, Test MSE of 418076264575.403564
Epoch 3: training loss 427480251331.765
Test Loss of 418076648415.074707, Test MSE of 418076648559.119690
Epoch 4: training loss 427479903292.235
Test Loss of 418075324665.189941, Test MSE of 418075328800.039734
Epoch 5: training loss 427479683915.294
Test Loss of 418075393900.191528, Test MSE of 418075397221.625671
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418075397221.6257, 'MSE - std': 0.0, 'R2 - mean': -2.255598479000354, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005435 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427915679623.529
Test Loss of 424555206124.931763, Test MSE of 424555208690.218689
Epoch 2: training loss 427892464218.353
Test Loss of 424537209511.824219, Test MSE of 424537213204.789734
Epoch 3: training loss 427864386861.176
Test Loss of 424514724410.389099, Test MSE of 424514731131.473022
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885922665.412
Test Loss of 424518384390.573242, Test MSE of 424518388146.517029
Epoch 2: training loss 427873941142.588
Test Loss of 424519980585.571106, Test MSE of 424519981926.132690
Epoch 3: training loss 427873424564.706
Test Loss of 424520651075.094177, Test MSE of 424520651817.904907
Epoch 4: training loss 427872990388.706
Test Loss of 424521135022.752747, Test MSE of 424521135319.854858
Epoch 5: training loss 427872674514.824
Test Loss of 424521379452.476501, Test MSE of 424521389428.656372
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 421298393325.141, 'MSE - std': 3222996103.5153503, 'R2 - mean': -2.143199673862868, 'R2 - std': 0.11239880513748646} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005204 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926954646.588
Test Loss of 447258849775.537354, Test MSE of 447258841688.455139
Epoch 2: training loss 421905966019.765
Test Loss of 447240475030.473267, Test MSE of 447240480621.682129
Epoch 3: training loss 421878864353.882
Test Loss of 447215850761.534119, Test MSE of 447215850101.164673
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899799853.176
Test Loss of 447223518973.808899, Test MSE of 447223520469.622070
Epoch 2: training loss 421888355870.118
Test Loss of 447225121778.498291, Test MSE of 447225124199.834534
Epoch 3: training loss 421887880372.706
Test Loss of 447224504788.060120, Test MSE of 447224496714.712952
Epoch 4: training loss 421887520165.647
Test Loss of 447223866528.362732, Test MSE of 447223862802.408325
Epoch 5: training loss 421887321389.176
Test Loss of 447223846320.292419, Test MSE of 447223842776.720398
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 429940209809.0008, 'MSE - std': 12501484719.968468, 'R2 - mean': -2.087846318307862, 'R2 - std': 0.12062468876624816} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005636 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430105517839.059
Test Loss of 410761458515.043030, Test MSE of 410761457364.624878
Epoch 2: training loss 430082741187.765
Test Loss of 410742312025.558533, Test MSE of 410742313297.019409
Epoch 3: training loss 430056042977.882
Test Loss of 410718808832.592346, Test MSE of 410718810855.552551
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074512323.765
Test Loss of 410720680429.756592, Test MSE of 410720676436.655334
Epoch 2: training loss 430061042507.294
Test Loss of 410722127635.072632, Test MSE of 410722130943.445618
Epoch 3: training loss 430060661097.412
Test Loss of 410721991528.840332, Test MSE of 410721990520.442505
Epoch 4: training loss 430060412446.118
Test Loss of 410721769097.654785, Test MSE of 410721768424.363708
Epoch 5: training loss 430060221379.765
Test Loss of 410721725578.839417, Test MSE of 410721723683.160156
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 425135588277.54065, 'MSE - std': 13655347096.812136, 'R2 - mean': -2.163359848142093, 'R2 - std': 0.1673906097098683} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005652 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043810093.176
Test Loss of 431613039962.387756, Test MSE of 431613039451.297791
Epoch 2: training loss 424024625633.882
Test Loss of 431593170044.623779, Test MSE of 431593164645.483826
Epoch 3: training loss 423997908148.706
Test Loss of 431565293210.713562, Test MSE of 431565295400.240601
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008842661.647
Test Loss of 431569327025.813965, Test MSE of 431569324170.063049
Epoch 2: training loss 423999343796.706
Test Loss of 431571485725.852844, Test MSE of 431571487787.937622
Epoch 3: training loss 423998707832.471
Test Loss of 431570793716.035156, Test MSE of 431570793943.450500
Epoch 4: training loss 423998280041.412
Test Loss of 431571159167.940796, Test MSE of 431571160261.719360
Epoch 5: training loss 423998036088.471
Test Loss of 431571139986.302612, Test MSE of 431571145984.696533
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 426422699818.9718, 'MSE - std': 12482044221.515715, 'R2 - mean': -2.175285746765333, 'R2 - std': 0.1516067320639831} 
 

Saving model.....
Results After CV: {'MSE - mean': 426422699818.9718, 'MSE - std': 12482044221.515715, 'R2 - mean': -2.175285746765333, 'R2 - std': 0.1516067320639831}
Train time: 13.264591224000014
Inference time: 0.07237321119998796
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 3 finished with value: 426422699818.9718 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 6, 'loss_dr': 0.7}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005457 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525188909.176
Test Loss of 418110681890.524170, Test MSE of 418110683150.662292
Epoch 2: training loss 427504995990.588
Test Loss of 418091986840.723572, Test MSE of 418091983333.213989
Epoch 3: training loss 427477920346.353
Test Loss of 418067744152.605164, Test MSE of 418067746405.932922
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492810511.059
Test Loss of 418070384299.851013, Test MSE of 418070385194.630676
Epoch 2: training loss 427479597537.882
Test Loss of 418072000581.640503, Test MSE of 418072002527.033813
Epoch 3: training loss 427479352500.706
Test Loss of 418072797815.028442, Test MSE of 418072790642.833313
Epoch 4: training loss 427479141195.294
Test Loss of 418072637711.929688, Test MSE of 418072639351.308350
Epoch 5: training loss 420419822049.882
Test Loss of 395801256169.082581, Test MSE of 395801261020.563965
Epoch 6: training loss 374470192188.235
Test Loss of 328878976966.203125, Test MSE of 328878978593.772278
Epoch 7: training loss 295622512880.941
Test Loss of 242459182973.720093, Test MSE of 242459183828.816040
Epoch 8: training loss 217826351616.000
Test Loss of 174050763288.279449, Test MSE of 174050765021.731720
Epoch 9: training loss 159768312289.882
Test Loss of 125750973796.493179, Test MSE of 125750974444.735092
Epoch 10: training loss 138884377630.118
Test Loss of 117877371218.253998, Test MSE of 117877373044.799881
Epoch 11: training loss 134732625453.176
Test Loss of 114855252179.290314, Test MSE of 114855249778.350571
Epoch 12: training loss 131514200545.882
Test Loss of 112342869633.687714, Test MSE of 112342869610.327911
Epoch 13: training loss 129841750106.353
Test Loss of 108593853169.254684, Test MSE of 108593855658.514618
Epoch 14: training loss 124176561242.353
Test Loss of 105044977179.358780, Test MSE of 105044976727.163864
Epoch 15: training loss 120865781180.235
Test Loss of 101576334710.021744, Test MSE of 101576333409.283386
Epoch 16: training loss 118463872361.412
Test Loss of 98406280906.170715, Test MSE of 98406282437.794876
Epoch 17: training loss 113987670859.294
Test Loss of 95819577834.326157, Test MSE of 95819578699.583527
Epoch 18: training loss 109519175785.412
Test Loss of 90911306536.209106, Test MSE of 90911306425.943298
Epoch 19: training loss 105871430806.588
Test Loss of 87614593211.129303, Test MSE of 87614593970.663055
Epoch 20: training loss 102053774486.588
Test Loss of 85012797765.462875, Test MSE of 85012797263.536148
Epoch 21: training loss 97507305200.941
Test Loss of 81684258766.730515, Test MSE of 81684256846.841064
Epoch 22: training loss 93519583006.118
Test Loss of 79662421585.365723, Test MSE of 79662422210.719482
Epoch 23: training loss 89875695691.294
Test Loss of 73926271000.634750, Test MSE of 73926271350.579544
Epoch 24: training loss 86024944760.471
Test Loss of 73228239173.936615, Test MSE of 73228240465.394135
Epoch 25: training loss 83151358509.176
Test Loss of 68667565701.477676, Test MSE of 68667565919.620110
Epoch 26: training loss 79515479100.235
Test Loss of 67018020116.903999, Test MSE of 67018020206.943787
Epoch 27: training loss 74969943973.647
Test Loss of 63700668524.961372, Test MSE of 63700669485.839409
Epoch 28: training loss 71531764148.706
Test Loss of 60663636657.772842, Test MSE of 60663634623.169090
Epoch 29: training loss 67801811949.176
Test Loss of 58237885577.149200, Test MSE of 58237886350.270721
Epoch 30: training loss 64994968440.471
Test Loss of 55534156306.120750, Test MSE of 55534156462.402527
Epoch 31: training loss 61203304289.882
Test Loss of 50716095343.270874, Test MSE of 50716095700.417412
Epoch 32: training loss 59869240914.824
Test Loss of 49892042092.073097, Test MSE of 49892042616.491791
Epoch 33: training loss 56805442394.353
Test Loss of 49345503031.132088, Test MSE of 49345503325.847794
Epoch 34: training loss 53629237443.765
Test Loss of 49690482545.402733, Test MSE of 49690481665.540123
Epoch 35: training loss 51015773221.647
Test Loss of 46081423870.223457, Test MSE of 46081424152.375526
Epoch 36: training loss 48294824295.529
Test Loss of 41592821970.816559, Test MSE of 41592822959.141586
Epoch 37: training loss 46004746872.471
Test Loss of 41260081242.959053, Test MSE of 41260081159.807350
Epoch 38: training loss 42877416628.706
Test Loss of 36233339801.907936, Test MSE of 36233339830.372040
Epoch 39: training loss 41363063382.588
Test Loss of 37912014769.358315, Test MSE of 37912014545.246674
Epoch 40: training loss 39267429707.294
Test Loss of 36097961407.925980, Test MSE of 36097962663.899132
Epoch 41: training loss 37288823710.118
Test Loss of 35525208955.114502, Test MSE of 35525208852.200645
Epoch 42: training loss 35821934957.176
Test Loss of 31774596062.600971, Test MSE of 31774596018.471237
Epoch 43: training loss 33519552745.412
Test Loss of 32142269914.455704, Test MSE of 32142269517.106560
Epoch 44: training loss 32204368673.882
Test Loss of 29480968996.419155, Test MSE of 29480969168.462627
Epoch 45: training loss 30379402970.353
Test Loss of 27970830657.199165, Test MSE of 27970830894.921520
Epoch 46: training loss 29015159006.118
Test Loss of 23069679195.314365, Test MSE of 23069679275.024780
Epoch 47: training loss 27413924709.647
Test Loss of 24408185700.848484, Test MSE of 24408185894.596607
Epoch 48: training loss 26507661658.353
Test Loss of 23912847646.615776, Test MSE of 23912848091.999485
Epoch 49: training loss 24948047582.118
Test Loss of 23536267041.813557, Test MSE of 23536267088.996334
Epoch 50: training loss 24151871819.294
Test Loss of 23643006771.815868, Test MSE of 23643006766.532433
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23643006766.532433, 'MSE - std': 0.0, 'R2 - mean': 0.8158893410622907, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003964 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917600406.588
Test Loss of 424555931227.788086, Test MSE of 424555929045.373596
Epoch 2: training loss 427895901726.118
Test Loss of 424539143596.976196, Test MSE of 424539143312.747864
Epoch 3: training loss 427867762447.059
Test Loss of 424516833803.962036, Test MSE of 424516833995.017700
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887921272.471
Test Loss of 424524532927.156128, Test MSE of 424524533642.133240
Epoch 2: training loss 427875330650.353
Test Loss of 424523915648.681030, Test MSE of 424523917683.880188
Epoch 3: training loss 427874958396.235
Test Loss of 424524047789.449890, Test MSE of 424524050140.064392
Epoch 4: training loss 427874680350.118
Test Loss of 424524323084.613464, Test MSE of 424524315461.244019
Epoch 5: training loss 421005939169.882
Test Loss of 403103459358.793457, Test MSE of 403103459182.830933
Epoch 6: training loss 375390993950.118
Test Loss of 336864011814.728638, Test MSE of 336864014727.531433
Epoch 7: training loss 296029128463.059
Test Loss of 253239910398.105011, Test MSE of 253239911230.492249
Epoch 8: training loss 217198800835.765
Test Loss of 184394753877.925507, Test MSE of 184394752362.669647
Epoch 9: training loss 156721136278.588
Test Loss of 136402172085.444366, Test MSE of 136402175328.043777
Epoch 10: training loss 136999908562.824
Test Loss of 129024099278.019897, Test MSE of 129024100927.323853
Epoch 11: training loss 133588844634.353
Test Loss of 125839632092.646774, Test MSE of 125839631361.165665
Epoch 12: training loss 128745081825.882
Test Loss of 122566314810.685165, Test MSE of 122566311295.008331
Epoch 13: training loss 126006212999.529
Test Loss of 119557716170.289154, Test MSE of 119557717777.568069
Epoch 14: training loss 122364640045.176
Test Loss of 114681568324.929916, Test MSE of 114681567205.258881
Epoch 15: training loss 116452159608.471
Test Loss of 112527092025.145508, Test MSE of 112527095048.178268
Epoch 16: training loss 115375693673.412
Test Loss of 108483850882.398331, Test MSE of 108483853702.939255
Epoch 17: training loss 109443978601.412
Test Loss of 105247320743.824203, Test MSE of 105247320546.049011
Epoch 18: training loss 104679259617.882
Test Loss of 99876047903.977798, Test MSE of 99876048837.539474
Epoch 19: training loss 100996755034.353
Test Loss of 95789419218.461258, Test MSE of 95789419261.182220
Epoch 20: training loss 97493773312.000
Test Loss of 91520370833.439743, Test MSE of 91520369781.161835
Epoch 21: training loss 93102161920.000
Test Loss of 87988606688.436737, Test MSE of 87988607889.621826
Epoch 22: training loss 89271832124.235
Test Loss of 85197678210.635208, Test MSE of 85197678438.561676
Epoch 23: training loss 84872171580.235
Test Loss of 78637194289.980103, Test MSE of 78637193296.870895
Epoch 24: training loss 80721843425.882
Test Loss of 79269376308.881790, Test MSE of 79269379144.691238
Epoch 25: training loss 77410467102.118
Test Loss of 72978961400.656952, Test MSE of 72978962538.215988
Epoch 26: training loss 73677944274.824
Test Loss of 70315707705.382370, Test MSE of 70315708404.901627
Epoch 27: training loss 70881258360.471
Test Loss of 69171432396.598663, Test MSE of 69171432684.882599
Epoch 28: training loss 67009994571.294
Test Loss of 65041171283.793663, Test MSE of 65041171339.630249
Epoch 29: training loss 64091884815.059
Test Loss of 61872183023.596573, Test MSE of 61872183561.872009
Epoch 30: training loss 59829799860.706
Test Loss of 59656179591.668747, Test MSE of 59656180257.626167
Epoch 31: training loss 57000957048.471
Test Loss of 54336513948.513535, Test MSE of 54336513708.183945
Epoch 32: training loss 53837761942.588
Test Loss of 52661093034.666664, Test MSE of 52661093801.153130
Epoch 33: training loss 50741981665.882
Test Loss of 49561231002.796204, Test MSE of 49561230134.388557
Epoch 34: training loss 48410572453.647
Test Loss of 47744197199.233864, Test MSE of 47744197929.036507
Epoch 35: training loss 44777901583.059
Test Loss of 41596211388.076797, Test MSE of 41596211922.496735
Epoch 36: training loss 43136577792.000
Test Loss of 39556229566.741615, Test MSE of 39556229476.202881
Epoch 37: training loss 40439788438.588
Test Loss of 42098738437.270416, Test MSE of 42098739895.851372
Epoch 38: training loss 37376162861.176
Test Loss of 40441441742.138329, Test MSE of 40441441209.652206
Epoch 39: training loss 36133491742.118
Test Loss of 34979104782.686096, Test MSE of 34979104324.969536
Epoch 40: training loss 33970315376.941
Test Loss of 32631180094.948879, Test MSE of 32631180172.882221
Epoch 41: training loss 32141757123.765
Test Loss of 33536486494.985889, Test MSE of 33536486706.203838
Epoch 42: training loss 30191784463.059
Test Loss of 32374743723.614159, Test MSE of 32374742940.351738
Epoch 43: training loss 28489634748.235
Test Loss of 30841223962.944252, Test MSE of 30841225046.912170
Epoch 44: training loss 27061739542.588
Test Loss of 30645916690.712933, Test MSE of 30645916571.162468
Epoch 45: training loss 25882735706.353
Test Loss of 29727898207.104324, Test MSE of 29727898652.663006
Epoch 46: training loss 24573937423.059
Test Loss of 29693432990.704605, Test MSE of 29693433284.704254
Epoch 47: training loss 23158259685.647
Test Loss of 27684461213.875549, Test MSE of 27684461753.680046
Epoch 48: training loss 22497490296.471
Test Loss of 29314089249.458248, Test MSE of 29314089103.975918
Epoch 49: training loss 21353770089.412
Test Loss of 26854452408.286839, Test MSE of 26854452339.276031
Epoch 50: training loss 20072120606.118
Test Loss of 27073459674.218830, Test MSE of 27073459874.289860
Epoch 51: training loss 19412028645.647
Test Loss of 26466457563.047882, Test MSE of 26466457980.415127
Epoch 52: training loss 18116818680.471
Test Loss of 28896502280.645847, Test MSE of 28896501702.557590
Epoch 53: training loss 17560670942.118
Test Loss of 28110118394.907242, Test MSE of 28110118018.098457
Epoch 54: training loss 16903094618.353
Test Loss of 27913807492.767059, Test MSE of 27913807808.495899
Epoch 55: training loss 16168673355.294
Test Loss of 28517358155.680779, Test MSE of 28517358776.160652
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26080182771.346542, 'MSE - std': 2437176004.81411, 'R2 - mean': 0.8061471664420536, 'R2 - std': 0.009742174620237076} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005606 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927504956.235
Test Loss of 447258721811.305115, Test MSE of 447258728851.597107
Epoch 2: training loss 421907567314.824
Test Loss of 447240045370.211426, Test MSE of 447240042265.959839
Epoch 3: training loss 421880671292.235
Test Loss of 447215718825.423096, Test MSE of 447215714807.529724
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897854976.000
Test Loss of 447223206665.889404, Test MSE of 447223207745.052063
Epoch 2: training loss 421887401622.588
Test Loss of 447224506834.638916, Test MSE of 447224513995.204590
Epoch 3: training loss 421887096470.588
Test Loss of 447224196179.142273, Test MSE of 447224195418.535828
Epoch 4: training loss 421886905645.176
Test Loss of 447224153746.742554, Test MSE of 447224149363.578125
Epoch 5: training loss 415346165880.471
Test Loss of 425743051878.092041, Test MSE of 425743055133.001709
Epoch 6: training loss 370536378127.059
Test Loss of 358723274655.829773, Test MSE of 358723281737.647278
Epoch 7: training loss 291723163527.529
Test Loss of 271441391148.887360, Test MSE of 271441391332.954590
Epoch 8: training loss 213438589108.706
Test Loss of 199257267759.729828, Test MSE of 199257266348.682556
Epoch 9: training loss 154566468367.059
Test Loss of 148717390762.489014, Test MSE of 148717391383.267151
Epoch 10: training loss 135381398949.647
Test Loss of 140081855208.253540, Test MSE of 140081852537.347015
Epoch 11: training loss 130114517805.176
Test Loss of 136264850934.880402, Test MSE of 136264850035.436127
Epoch 12: training loss 127866011196.235
Test Loss of 134185667418.899841, Test MSE of 134185667799.620895
Epoch 13: training loss 124562219911.529
Test Loss of 129000657745.898682, Test MSE of 129000660167.416641
Epoch 14: training loss 120937614516.706
Test Loss of 126131275338.733292, Test MSE of 126131278188.039307
Epoch 15: training loss 117855530224.941
Test Loss of 122048578002.165161, Test MSE of 122048579767.891495
Epoch 16: training loss 113353126610.824
Test Loss of 118941236834.894287, Test MSE of 118941239917.704437
Epoch 17: training loss 109322403478.588
Test Loss of 114847896803.634506, Test MSE of 114847898954.859070
Epoch 18: training loss 104962708178.824
Test Loss of 111407072350.038406, Test MSE of 111407072904.693497
Epoch 19: training loss 101580694949.647
Test Loss of 105033454182.921112, Test MSE of 105033455105.382599
Epoch 20: training loss 97242978334.118
Test Loss of 101433158792.201706, Test MSE of 101433159592.856613
Epoch 21: training loss 93340188566.588
Test Loss of 97675205302.984039, Test MSE of 97675204630.396912
Epoch 22: training loss 90092069767.529
Test Loss of 96075611131.025681, Test MSE of 96075613338.417648
Epoch 23: training loss 87732489200.941
Test Loss of 91553693459.127457, Test MSE of 91553695929.617935
Epoch 24: training loss 82769664256.000
Test Loss of 86553612885.866302, Test MSE of 86553613025.380295
Epoch 25: training loss 79543770352.941
Test Loss of 84619970715.862137, Test MSE of 84619972016.609589
Epoch 26: training loss 76553615405.176
Test Loss of 77527544477.875549, Test MSE of 77527546025.748489
Epoch 27: training loss 71880730819.765
Test Loss of 76588111718.032852, Test MSE of 76588111363.007843
Epoch 28: training loss 69101951488.000
Test Loss of 72975131120.484848, Test MSE of 72975132257.240280
Epoch 29: training loss 65495988510.118
Test Loss of 72565007810.294708, Test MSE of 72565008167.260010
Epoch 30: training loss 62240249871.059
Test Loss of 66643173154.287300, Test MSE of 66643173394.632431
Epoch 31: training loss 60272746736.941
Test Loss of 63531983748.115662, Test MSE of 63531984766.049736
Epoch 32: training loss 56292080519.529
Test Loss of 60040473931.384689, Test MSE of 60040473032.052979
Epoch 33: training loss 53290227817.412
Test Loss of 52987092946.520470, Test MSE of 52987092677.521935
Epoch 34: training loss 51002103213.176
Test Loss of 54090853116.150818, Test MSE of 54090853936.082527
Epoch 35: training loss 48154156852.706
Test Loss of 51360100289.465652, Test MSE of 51360100470.102112
Epoch 36: training loss 45785653451.294
Test Loss of 50432639032.849411, Test MSE of 50432638802.362839
Epoch 37: training loss 43025772408.471
Test Loss of 46263057692.010178, Test MSE of 46263057712.028427
Epoch 38: training loss 41591359841.882
Test Loss of 47232091314.365021, Test MSE of 47232092202.087753
Epoch 39: training loss 39079478452.706
Test Loss of 44036334945.650703, Test MSE of 44036334328.625549
Epoch 40: training loss 36921070433.882
Test Loss of 42446998977.820961, Test MSE of 42446999395.049461
Epoch 41: training loss 35719423201.882
Test Loss of 36444938221.997688, Test MSE of 36444937996.476997
Epoch 42: training loss 32894876928.000
Test Loss of 39188508952.457092, Test MSE of 39188508769.966606
Epoch 43: training loss 31398937682.824
Test Loss of 37943848664.383064, Test MSE of 37943848912.646675
Epoch 44: training loss 30357213869.176
Test Loss of 33207202694.958130, Test MSE of 33207201915.246689
Epoch 45: training loss 28741140762.353
Test Loss of 33900309653.466576, Test MSE of 33900309894.418858
Epoch 46: training loss 27592561581.176
Test Loss of 33240774069.977329, Test MSE of 33240774263.100033
Epoch 47: training loss 26093999420.235
Test Loss of 32025059871.859356, Test MSE of 32025059659.308601
Epoch 48: training loss 24862197202.824
Test Loss of 28587827225.582233, Test MSE of 28587827505.361210
Epoch 49: training loss 23515931666.824
Test Loss of 29393147295.237568, Test MSE of 29393147669.964123
Epoch 50: training loss 22452990802.824
Test Loss of 27792028995.331020, Test MSE of 27792028884.111683
Epoch 51: training loss 21580166870.588
Test Loss of 27522303527.676151, Test MSE of 27522303684.471607
Epoch 52: training loss 21074362424.471
Test Loss of 28057307298.494564, Test MSE of 28057307302.337772
Epoch 53: training loss 20032448026.353
Test Loss of 25443082907.033077, Test MSE of 25443082859.843540
Epoch 54: training loss 19358508077.176
Test Loss of 23293456984.708767, Test MSE of 23293456897.669044
Epoch 55: training loss 18508819584.000
Test Loss of 30061071809.820957, Test MSE of 30061071849.233852
Epoch 56: training loss 17999484408.471
Test Loss of 24323965828.589405, Test MSE of 24323965898.807549
Epoch 57: training loss 17386566426.353
Test Loss of 25252050747.395790, Test MSE of 25252050611.005333
Epoch 58: training loss 16754026413.176
Test Loss of 25501659249.698822, Test MSE of 25501659088.937523
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25887341543.876873, 'MSE - std': 2008546754.161276, 'R2 - mean': 0.814177157096634, 'R2 - std': 0.013864876873335699} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005523 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110881912.471
Test Loss of 410763597457.236450, Test MSE of 410763599770.139038
Epoch 2: training loss 430090812958.118
Test Loss of 410745549597.971313, Test MSE of 410745545192.270264
Epoch 3: training loss 430063915489.882
Test Loss of 410721851844.057373, Test MSE of 410721858249.168884
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076689950.118
Test Loss of 410730261013.560364, Test MSE of 410730253809.737366
Epoch 2: training loss 430064970812.235
Test Loss of 410729740319.274414, Test MSE of 410729738918.843933
Epoch 3: training loss 430064724088.471
Test Loss of 410729672842.365601, Test MSE of 410729679614.358887
Epoch 4: training loss 430064541214.118
Test Loss of 410729403374.467346, Test MSE of 410729404679.541077
Epoch 5: training loss 423274008576.000
Test Loss of 389156334207.703857, Test MSE of 389156331982.284241
Epoch 6: training loss 377955412570.353
Test Loss of 322536385765.819519, Test MSE of 322536393542.229004
Epoch 7: training loss 298679540916.706
Test Loss of 237916127292.653412, Test MSE of 237916132200.847626
Epoch 8: training loss 220554707184.941
Test Loss of 167874939106.502533, Test MSE of 167874938985.872162
Epoch 9: training loss 161570041253.647
Test Loss of 120382293635.968536, Test MSE of 120382293804.368057
Epoch 10: training loss 141671931060.706
Test Loss of 112258218553.573349, Test MSE of 112258219205.084747
Epoch 11: training loss 137062530439.529
Test Loss of 109333521085.304947, Test MSE of 109333519420.374695
Epoch 12: training loss 134825435346.824
Test Loss of 106611587330.724670, Test MSE of 106611589380.278320
Epoch 13: training loss 131661131474.824
Test Loss of 103866781603.124481, Test MSE of 103866784398.968079
Epoch 14: training loss 127224273920.000
Test Loss of 100649244676.738541, Test MSE of 100649243982.101349
Epoch 15: training loss 122603365315.765
Test Loss of 97512760747.890793, Test MSE of 97512760351.806641
Epoch 16: training loss 119155808828.235
Test Loss of 94826282807.085602, Test MSE of 94826283698.519577
Epoch 17: training loss 115612132080.941
Test Loss of 90548379049.047668, Test MSE of 90548380541.160980
Epoch 18: training loss 111505605963.294
Test Loss of 86937222133.101349, Test MSE of 86937222092.663422
Epoch 19: training loss 107851329325.176
Test Loss of 84192653708.142532, Test MSE of 84192653389.070450
Epoch 20: training loss 103782903506.824
Test Loss of 81672547485.793610, Test MSE of 81672544893.414703
Epoch 21: training loss 100462283625.412
Test Loss of 78735175465.817673, Test MSE of 78735173194.502975
Epoch 22: training loss 95918850590.118
Test Loss of 74005867241.847290, Test MSE of 74005867914.724289
Epoch 23: training loss 92465991408.941
Test Loss of 72850528780.083298, Test MSE of 72850528655.774506
Epoch 24: training loss 88603399047.529
Test Loss of 68944512374.819061, Test MSE of 68944512051.362106
Epoch 25: training loss 84267203087.059
Test Loss of 67729369727.703842, Test MSE of 67729370402.987068
Epoch 26: training loss 80826765869.176
Test Loss of 64332232450.961594, Test MSE of 64332232232.048355
Epoch 27: training loss 76545001893.647
Test Loss of 60455383409.606667, Test MSE of 60455383017.365395
Epoch 28: training loss 74869387941.647
Test Loss of 55590122038.730217, Test MSE of 55590122706.100288
Epoch 29: training loss 70617443584.000
Test Loss of 55190290136.314667, Test MSE of 55190290293.897789
Epoch 30: training loss 67557695759.059
Test Loss of 53036099914.750580, Test MSE of 53036100994.259560
Epoch 31: training loss 64255218281.412
Test Loss of 49466953836.038872, Test MSE of 49466953466.337906
Epoch 32: training loss 61532529325.176
Test Loss of 48412064158.149002, Test MSE of 48412064267.282364
Epoch 33: training loss 58160414102.588
Test Loss of 45373793493.708466, Test MSE of 45373792340.017166
Epoch 34: training loss 55556851666.824
Test Loss of 43749788319.925957, Test MSE of 43749788372.101883
Epoch 35: training loss 53019425769.412
Test Loss of 40381516144.658951, Test MSE of 40381516605.174858
Epoch 36: training loss 50566639405.176
Test Loss of 39975324084.894028, Test MSE of 39975324182.264687
Epoch 37: training loss 47847012472.471
Test Loss of 36056794886.752426, Test MSE of 36056794835.687614
Epoch 38: training loss 44121746597.647
Test Loss of 36161527381.530769, Test MSE of 36161527414.830559
Epoch 39: training loss 42818868886.588
Test Loss of 33281040939.357704, Test MSE of 33281040770.835197
Epoch 40: training loss 40778975887.059
Test Loss of 30589430227.220730, Test MSE of 30589431298.183582
Epoch 41: training loss 38722205500.235
Test Loss of 28942082258.391487, Test MSE of 28942082247.476517
Epoch 42: training loss 36210080790.588
Test Loss of 28024886089.565941, Test MSE of 28024886066.187698
Epoch 43: training loss 34507147651.765
Test Loss of 29269968098.976398, Test MSE of 29269968381.661182
Epoch 44: training loss 32822237793.882
Test Loss of 26070095371.609440, Test MSE of 26070095380.105221
Epoch 45: training loss 31259946541.176
Test Loss of 26143411929.262379, Test MSE of 26143411796.798656
Epoch 46: training loss 29944015932.235
Test Loss of 24131030992.614529, Test MSE of 24131031151.751781
Epoch 47: training loss 28203146537.412
Test Loss of 22647115047.211475, Test MSE of 22647115275.103672
Epoch 48: training loss 27379375909.647
Test Loss of 23648279240.677464, Test MSE of 23648279158.504860
Epoch 49: training loss 25906175962.353
Test Loss of 21169879808.592319, Test MSE of 21169879719.380947
Epoch 50: training loss 24872816150.588
Test Loss of 21860019637.367886, Test MSE of 21860019652.106918
Epoch 51: training loss 23864744839.529
Test Loss of 20738933934.852383, Test MSE of 20738933940.268581
Epoch 52: training loss 22659294042.353
Test Loss of 20737302169.765850, Test MSE of 20737302359.656910
Epoch 53: training loss 21880314620.235
Test Loss of 21163174630.056454, Test MSE of 21163174545.692352
Epoch 54: training loss 20723746183.529
Test Loss of 18499939164.520130, Test MSE of 18499938995.863159
Epoch 55: training loss 20150132623.059
Test Loss of 19041700977.251274, Test MSE of 19041700908.218678
Epoch 56: training loss 19721359363.765
Test Loss of 19313472643.257751, Test MSE of 19313472342.744049
Epoch 57: training loss 18887535802.353
Test Loss of 19673444435.398426, Test MSE of 19673444279.930241
Epoch 58: training loss 18209865475.765
Test Loss of 18047063038.578438, Test MSE of 18047063119.968426
Epoch 59: training loss 17968165342.118
Test Loss of 20460764229.656639, Test MSE of 20460764456.879772
Epoch 60: training loss 17418551563.294
Test Loss of 18414409943.130032, Test MSE of 18414409953.138012
Epoch 61: training loss 16623105219.765
Test Loss of 17419908725.752892, Test MSE of 17419908786.750946
Epoch 62: training loss 16307301210.353
Test Loss of 17573340327.270706, Test MSE of 17573340532.350555
Epoch 63: training loss 15737577027.765
Test Loss of 17581706735.178158, Test MSE of 17581706501.780048
Epoch 64: training loss 15383940502.588
Test Loss of 16956009045.056917, Test MSE of 16956008666.348543
Epoch 65: training loss 14993672720.941
Test Loss of 18100198177.288292, Test MSE of 18100198023.403656
Epoch 66: training loss 14446360207.059
Test Loss of 18375608385.391949, Test MSE of 18375608003.302620
Epoch 67: training loss 14283315265.882
Test Loss of 17179603119.326239, Test MSE of 17179603168.692951
Epoch 68: training loss 13777625302.588
Test Loss of 16793212531.857473, Test MSE of 16793212397.210876
Epoch 69: training loss 13595159454.118
Test Loss of 17791104530.243404, Test MSE of 17791104453.133099
Epoch 70: training loss 13031869389.176
Test Loss of 18277078748.105507, Test MSE of 18277078692.457245
Epoch 71: training loss 12827361758.118
Test Loss of 18011815306.720963, Test MSE of 18011815303.783409
Epoch 72: training loss 12634381093.647
Test Loss of 18295167765.441925, Test MSE of 18295168065.587509
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23989298174.30453, 'MSE - std': 3719328023.456847, 'R2 - mean': 0.822882978629761, 'R2 - std': 0.01927563471677501} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424044459309.176
Test Loss of 431612794892.320251, Test MSE of 431612794979.009766
Epoch 2: training loss 424026976015.059
Test Loss of 431594275786.454407, Test MSE of 431594284007.893799
Epoch 3: training loss 424001802962.824
Test Loss of 431568435344.999512, Test MSE of 431568436552.796631
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010339990.588
Test Loss of 431569700302.008301, Test MSE of 431569694653.547302
Epoch 2: training loss 424001732608.000
Test Loss of 431570192640.829224, Test MSE of 431570195542.697998
Epoch 3: training loss 424001336018.824
Test Loss of 431570413889.273499, Test MSE of 431570406887.054138
Epoch 4: training loss 424001000628.706
Test Loss of 431570635926.211914, Test MSE of 431570637906.629456
Epoch 5: training loss 417389903390.118
Test Loss of 409986395039.807495, Test MSE of 409986387761.231018
Epoch 6: training loss 372504857057.882
Test Loss of 342380366634.291504, Test MSE of 342380365630.653992
Epoch 7: training loss 294237260137.412
Test Loss of 256245729066.765381, Test MSE of 256245726879.315155
Epoch 8: training loss 216573478008.471
Test Loss of 184142246276.560852, Test MSE of 184142245727.321808
Epoch 9: training loss 156198785069.176
Test Loss of 133176455116.823700, Test MSE of 133176454358.456818
Epoch 10: training loss 138484514213.647
Test Loss of 123905306334.000931, Test MSE of 123905305915.524155
Epoch 11: training loss 134159576425.412
Test Loss of 120949072986.506241, Test MSE of 120949072339.578140
Epoch 12: training loss 131929289306.353
Test Loss of 117235709990.856079, Test MSE of 117235709099.591980
Epoch 13: training loss 128267509669.647
Test Loss of 114039024558.970840, Test MSE of 114039023307.626556
Epoch 14: training loss 124051279510.588
Test Loss of 110881885959.700134, Test MSE of 110881883912.169388
Epoch 15: training loss 121535186371.765
Test Loss of 107116180576.192505, Test MSE of 107116182702.674118
Epoch 16: training loss 117720013477.647
Test Loss of 105198515933.053223, Test MSE of 105198516131.912750
Epoch 17: training loss 113896712975.059
Test Loss of 101250026513.532623, Test MSE of 101250025601.768219
Epoch 18: training loss 109334319826.824
Test Loss of 95563291985.858398, Test MSE of 95563292855.044128
Epoch 19: training loss 105244031713.882
Test Loss of 92236730540.483109, Test MSE of 92236729836.374680
Epoch 20: training loss 101555153076.706
Test Loss of 89959854938.624710, Test MSE of 89959855258.321411
Epoch 21: training loss 97893990851.765
Test Loss of 86853037031.833405, Test MSE of 86853036879.321060
Epoch 22: training loss 95105390305.882
Test Loss of 82357217009.902817, Test MSE of 82357215496.448380
Epoch 23: training loss 89833396419.765
Test Loss of 77116522848.547897, Test MSE of 77116524592.322983
Epoch 24: training loss 87024742881.882
Test Loss of 73415384320.355392, Test MSE of 73415385191.132919
Epoch 25: training loss 83281285978.353
Test Loss of 73690287737.543732, Test MSE of 73690287023.839432
Epoch 26: training loss 79251598351.059
Test Loss of 69319226458.506241, Test MSE of 69319226549.397263
Epoch 27: training loss 75791088233.412
Test Loss of 65746213852.460899, Test MSE of 65746214809.893959
Epoch 28: training loss 73032673626.353
Test Loss of 64069560404.346138, Test MSE of 64069560307.001884
Epoch 29: training loss 69779116250.353
Test Loss of 60191243622.708008, Test MSE of 60191243696.238487
Epoch 30: training loss 67079257464.471
Test Loss of 55290442988.453491, Test MSE of 55290443222.470772
Epoch 31: training loss 64484221748.706
Test Loss of 53084923542.922722, Test MSE of 53084922709.264854
Epoch 32: training loss 60487481630.118
Test Loss of 51064973415.300323, Test MSE of 51064973531.635727
Epoch 33: training loss 58197669225.412
Test Loss of 49987637719.959282, Test MSE of 49987636981.510643
Epoch 34: training loss 54773014219.294
Test Loss of 47326581171.946320, Test MSE of 47326581616.540504
Epoch 35: training loss 52429118373.647
Test Loss of 45366936660.819992, Test MSE of 45366937077.567375
Epoch 36: training loss 49437721479.529
Test Loss of 43836461679.592781, Test MSE of 43836461420.969414
Epoch 37: training loss 47072474081.882
Test Loss of 41442003871.807495, Test MSE of 41442004184.660400
Epoch 38: training loss 45156098597.647
Test Loss of 40974124816.229523, Test MSE of 40974125385.139099
Epoch 39: training loss 43229450127.059
Test Loss of 37247814283.076355, Test MSE of 37247814621.725761
Epoch 40: training loss 40820359107.765
Test Loss of 35254361227.787132, Test MSE of 35254361225.930138
Epoch 41: training loss 38808309865.412
Test Loss of 32632409687.426193, Test MSE of 32632409893.200962
Epoch 42: training loss 36662077010.824
Test Loss of 29311241775.622398, Test MSE of 29311241947.782143
Epoch 43: training loss 34928828664.471
Test Loss of 27676293008.170292, Test MSE of 27676293636.097847
Epoch 44: training loss 33491798806.588
Test Loss of 25962129192.869968, Test MSE of 25962128417.273941
Epoch 45: training loss 31697339806.118
Test Loss of 28838774378.380379, Test MSE of 28838774868.878624
Epoch 46: training loss 30038539648.000
Test Loss of 24596765305.069874, Test MSE of 24596765760.444073
Epoch 47: training loss 28808174049.882
Test Loss of 26730319922.228600, Test MSE of 26730320072.098236
Epoch 48: training loss 27553516815.059
Test Loss of 24521699100.549744, Test MSE of 24521699368.157951
Epoch 49: training loss 26102092470.588
Test Loss of 21643671307.490978, Test MSE of 21643671558.366043
Epoch 50: training loss 25056107663.059
Test Loss of 21703142567.270706, Test MSE of 21703142927.601284
Epoch 51: training loss 24441540525.176
Test Loss of 22347998279.552059, Test MSE of 22347998691.282173
Epoch 52: training loss 23420012193.882
Test Loss of 20695469422.289680, Test MSE of 20695469061.040085
Epoch 53: training loss 22191427783.529
Test Loss of 22271982201.069874, Test MSE of 22271982182.658707
Epoch 54: training loss 21584605466.353
Test Loss of 21265497096.055531, Test MSE of 21265497124.776169
Epoch 55: training loss 20716000244.706
Test Loss of 21094489516.838501, Test MSE of 21094489688.503216
Epoch 56: training loss 19764778763.294
Test Loss of 21481117302.700600, Test MSE of 21481117123.063087
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23487661964.056244, 'MSE - std': 3474662042.0677166, 'R2 - mean': 0.8262220268618929, 'R2 - std': 0.018488835749544796} 
 

Saving model.....
Results After CV: {'MSE - mean': 23487661964.056244, 'MSE - std': 3474662042.0677166, 'R2 - mean': 0.8262220268618929, 'R2 - std': 0.018488835749544796}
Train time: 91.74809247880005
Inference time: 0.0730078763999245
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 4 finished with value: 23487661964.056244 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005619 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524045763.765
Test Loss of 418111699684.463562, Test MSE of 418111700790.654419
Epoch 2: training loss 427502637056.000
Test Loss of 418093592116.467285, Test MSE of 418093587523.294495
Epoch 3: training loss 427475320832.000
Test Loss of 418069720526.138306, Test MSE of 418069725871.721375
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492286584.471
Test Loss of 418073817731.345825, Test MSE of 418073819364.454285
Epoch 2: training loss 427481702159.059
Test Loss of 418075414866.253967, Test MSE of 418075417209.164917
Epoch 3: training loss 427481316894.118
Test Loss of 418074843938.997925, Test MSE of 418074853581.852417
Epoch 4: training loss 427480963915.294
Test Loss of 418074363114.266968, Test MSE of 418074369399.515320
Epoch 5: training loss 427480744056.471
Test Loss of 418073446527.437439, Test MSE of 418073446262.793945
Epoch 6: training loss 427480567567.059
Test Loss of 418072946919.661316, Test MSE of 418072946041.487305
Epoch 7: training loss 427480457697.882
Test Loss of 418071395400.719849, Test MSE of 418071391378.065308
Epoch 8: training loss 417161370202.353
Test Loss of 384405411214.182739, Test MSE of 384405411560.403198
Epoch 9: training loss 350443005831.529
Test Loss of 291718398702.412231, Test MSE of 291718400412.791687
Epoch 10: training loss 252185513622.588
Test Loss of 195961395366.284515, Test MSE of 195961397185.426117
Epoch 11: training loss 178052825057.882
Test Loss of 142540141903.648407, Test MSE of 142540142099.412048
Epoch 12: training loss 147208728244.706
Test Loss of 123660336030.171646, Test MSE of 123660336158.349258
Epoch 13: training loss 138661528229.647
Test Loss of 118359103985.906082, Test MSE of 118359106305.913666
Epoch 14: training loss 134481779230.118
Test Loss of 115246072044.872543, Test MSE of 115246071146.668991
Epoch 15: training loss 133233361904.941
Test Loss of 110868437643.399490, Test MSE of 110868439182.211288
Epoch 16: training loss 127792805722.353
Test Loss of 108795851387.765900, Test MSE of 108795852503.886536
Epoch 17: training loss 123615931663.059
Test Loss of 105233584646.277115, Test MSE of 105233583917.886017
Epoch 18: training loss 120947474010.353
Test Loss of 101628840789.214890, Test MSE of 101628840223.916809
Epoch 19: training loss 116882866838.588
Test Loss of 98339160001.465652, Test MSE of 98339159268.642700
Epoch 20: training loss 112797263179.294
Test Loss of 95311761541.596115, Test MSE of 95311760748.599808
Epoch 21: training loss 109018300280.471
Test Loss of 93498041582.056900, Test MSE of 93498041562.873215
Epoch 22: training loss 105149808579.765
Test Loss of 87990235487.518860, Test MSE of 87990235799.023422
Epoch 23: training loss 101617714477.176
Test Loss of 85915368691.268097, Test MSE of 85915368299.668228
Epoch 24: training loss 96250674695.529
Test Loss of 82224870921.356461, Test MSE of 82224870932.369125
Epoch 25: training loss 93621661816.471
Test Loss of 77122126957.198242, Test MSE of 77122127181.247665
Epoch 26: training loss 88731214381.176
Test Loss of 75511486330.877625, Test MSE of 75511485428.716293
Epoch 27: training loss 85313893940.706
Test Loss of 72335797082.899841, Test MSE of 72335798521.262802
Epoch 28: training loss 81734024297.412
Test Loss of 67981103233.806152, Test MSE of 67981103406.296753
Epoch 29: training loss 79004824161.882
Test Loss of 66299191688.260933, Test MSE of 66299193302.611816
Epoch 30: training loss 74788425831.529
Test Loss of 62560337302.236412, Test MSE of 62560338032.620461
Epoch 31: training loss 71419128304.941
Test Loss of 61301897494.088364, Test MSE of 61301897812.599045
Epoch 32: training loss 67761071066.353
Test Loss of 58487532678.306732, Test MSE of 58487533266.815773
Epoch 33: training loss 64689411169.882
Test Loss of 56203210678.095764, Test MSE of 56203212636.287231
Epoch 34: training loss 62399220826.353
Test Loss of 52141564746.318764, Test MSE of 52141564986.268311
Epoch 35: training loss 59126848180.706
Test Loss of 48824782159.174644, Test MSE of 48824782450.634911
Epoch 36: training loss 56881316801.882
Test Loss of 49734283648.681007, Test MSE of 49734284407.035301
Epoch 37: training loss 53850970240.000
Test Loss of 48225455484.654175, Test MSE of 48225455038.087624
Epoch 38: training loss 50366381575.529
Test Loss of 43060824862.260468, Test MSE of 43060824823.195206
Epoch 39: training loss 48491566524.235
Test Loss of 38983946501.507286, Test MSE of 38983946045.965790
Epoch 40: training loss 45414794345.412
Test Loss of 37349466215.276428, Test MSE of 37349466588.030258
Epoch 41: training loss 43522474307.765
Test Loss of 35497197592.634743, Test MSE of 35497197568.427521
Epoch 42: training loss 41467944670.118
Test Loss of 33818974000.973396, Test MSE of 33818974724.868103
Epoch 43: training loss 39337499655.529
Test Loss of 37135695815.150589, Test MSE of 37135695565.082115
Epoch 44: training loss 37039394514.824
Test Loss of 33024669693.631275, Test MSE of 33024669753.144150
Epoch 45: training loss 35613873528.471
Test Loss of 29423763702.347443, Test MSE of 29423764021.471455
Epoch 46: training loss 33347210059.294
Test Loss of 31449373993.275040, Test MSE of 31449373985.815365
Epoch 47: training loss 31706928312.471
Test Loss of 27843066800.173954, Test MSE of 27843067055.592247
Epoch 48: training loss 30407246765.176
Test Loss of 26376691375.640991, Test MSE of 26376691101.848949
Epoch 49: training loss 28909319988.706
Test Loss of 26791157146.026371, Test MSE of 26791157386.285442
Epoch 50: training loss 27768023909.647
Test Loss of 23452504993.014111, Test MSE of 23452505395.166462
Epoch 51: training loss 26623449370.353
Test Loss of 25015536015.840851, Test MSE of 25015535932.163349
Epoch 52: training loss 25367689219.765
Test Loss of 24462501504.266483, Test MSE of 24462501839.226501
Epoch 53: training loss 23668850544.941
Test Loss of 24785028262.995144, Test MSE of 24785028445.597973
Epoch 54: training loss 22948334279.529
Test Loss of 23762250434.353920, Test MSE of 23762250307.888939
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23762250307.88894, 'MSE - std': 0.0, 'R2 - mean': 0.8149607786679217, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427917357296.941
Test Loss of 424556656846.079102, Test MSE of 424556653928.631287
Epoch 2: training loss 427896100743.529
Test Loss of 424539975015.809387, Test MSE of 424539973027.117981
Epoch 3: training loss 427869375307.294
Test Loss of 424518547131.484619, Test MSE of 424518550202.632446
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890067576.471
Test Loss of 424520564838.328918, Test MSE of 424520557008.118103
Epoch 2: training loss 427877770661.647
Test Loss of 424521151334.743469, Test MSE of 424521144515.512268
Epoch 3: training loss 427877182524.235
Test Loss of 424521778292.778137, Test MSE of 424521783385.900513
Epoch 4: training loss 427876817257.412
Test Loss of 424522163496.801270, Test MSE of 424522164796.063660
Epoch 5: training loss 427876591495.529
Test Loss of 424522267114.326172, Test MSE of 424522259923.781250
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 224142255115.83508, 'MSE - std': 200380004807.94614, 'R2 - mean': -0.6079231524073521, 'R2 - std': 1.4228839310752739} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005494 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926197970.824
Test Loss of 447259534469.833008, Test MSE of 447259535897.106995
Epoch 2: training loss 421905220306.824
Test Loss of 447241048603.121887, Test MSE of 447241051485.059937
Epoch 3: training loss 421878702682.353
Test Loss of 447217354724.049011, Test MSE of 447217355743.377197
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899753351.529
Test Loss of 447226060945.913513, Test MSE of 447226067036.996338
Epoch 2: training loss 421889559130.353
Test Loss of 447227123241.571106, Test MSE of 447227129481.408020
Epoch 3: training loss 421889197598.118
Test Loss of 447227727967.696533, Test MSE of 447227724601.822449
Epoch 4: training loss 421888898228.706
Test Loss of 447227506391.435547, Test MSE of 447227508256.049805
Epoch 5: training loss 421888688007.529
Test Loss of 447226933410.020813, Test MSE of 447226926139.594543
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 298503812123.7549, 'MSE - std': 194492621640.27173, 'R2 - mean': -1.064335479251908, 'R2 - std': 1.329043595372053} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005567 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106051041.882
Test Loss of 410759911322.595093, Test MSE of 410759908653.094177
Epoch 2: training loss 430083207649.882
Test Loss of 410740654322.613586, Test MSE of 410740650281.649231
Epoch 3: training loss 430056863141.647
Test Loss of 410717027245.075439, Test MSE of 410717027389.214050
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430071831853.176
Test Loss of 410720224240.362793, Test MSE of 410720217074.115662
Epoch 2: training loss 430059513253.647
Test Loss of 410721034706.273010, Test MSE of 410721036267.995544
Epoch 3: training loss 430059154492.235
Test Loss of 410721308844.483093, Test MSE of 410721313145.084351
Epoch 4: training loss 430058968967.529
Test Loss of 410721807324.460876, Test MSE of 410721806226.433960
Epoch 5: training loss 430058828016.941
Test Loss of 410721897849.662170, Test MSE of 410721898648.436462
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 326558333754.9253, 'MSE - std': 175304601925.45105, 'R2 - mean': -1.3957270798700296, 'R2 - std': 1.2861682773573322} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005488 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043506507.294
Test Loss of 431611521747.102295, Test MSE of 431611520284.755615
Epoch 2: training loss 424024045929.412
Test Loss of 431591228817.354919, Test MSE of 431591229060.280640
Epoch 3: training loss 423997065456.941
Test Loss of 431563539326.637695, Test MSE of 431563534395.233093
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010659719.529
Test Loss of 431566726162.480347, Test MSE of 431566729495.018066
Epoch 2: training loss 423997498066.824
Test Loss of 431569274545.458557, Test MSE of 431569278658.214111
Epoch 3: training loss 423996895713.882
Test Loss of 431568522807.204102, Test MSE of 431568529499.545227
Epoch 4: training loss 423996441298.824
Test Loss of 431568338299.557617, Test MSE of 431568341331.372253
Epoch 5: training loss 423996192286.118
Test Loss of 431567212648.248047, Test MSE of 431567216874.542053
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 347560110378.84863, 'MSE - std': 162325787385.85367, 'R2 - mean': -1.56117366359958, 'R2 - std': 1.1970268829125676} 
 

Saving model.....
Results After CV: {'MSE - mean': 347560110378.84863, 'MSE - std': 162325787385.85367, 'R2 - mean': -1.56117366359958, 'R2 - std': 1.1970268829125676}
Train time: 27.66327033579996
Inference time: 0.07237497319997602
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 5 finished with value: 347560110378.84863 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 7, 'loss_dr': 0.9}. Best is trial 0 with value: 23250959753.015366.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005645 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524979531.294
Test Loss of 418111649005.109436, Test MSE of 418111653716.875305
Epoch 2: training loss 427504214016.000
Test Loss of 418094210016.495972, Test MSE of 418094206152.082947
Epoch 3: training loss 427477416297.412
Test Loss of 418071183920.914185, Test MSE of 418071191085.080872
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494396506.353
Test Loss of 418075746798.589844, Test MSE of 418075741294.385864
Epoch 2: training loss 427483635230.118
Test Loss of 418077158384.603271, Test MSE of 418077163090.345886
Epoch 3: training loss 427483158407.529
Test Loss of 418077335118.049500, Test MSE of 418077339769.514343
Epoch 4: training loss 422104136041.412
Test Loss of 400942193041.262085, Test MSE of 400942193049.273438
Epoch 5: training loss 385627931346.824
Test Loss of 346441162336.999329, Test MSE of 346441159276.313782
Epoch 6: training loss 317632946657.882
Test Loss of 268204172725.029846, Test MSE of 268204173119.499023
Epoch 7: training loss 226624423273.412
Test Loss of 169938901715.408752, Test MSE of 169938901994.102448
Epoch 8: training loss 161386029688.471
Test Loss of 130586541261.368500, Test MSE of 130586542731.882660
Epoch 9: training loss 141744981865.412
Test Loss of 120256137529.145508, Test MSE of 120256136086.334167
Epoch 10: training loss 136716440907.294
Test Loss of 115575982955.954666, Test MSE of 115575984250.012466
Epoch 11: training loss 132827165952.000
Test Loss of 112355987500.532043, Test MSE of 112355985398.112122
Epoch 12: training loss 129495929163.294
Test Loss of 109260409558.251221, Test MSE of 109260410287.393112
Epoch 13: training loss 124596294520.471
Test Loss of 106401122647.465179, Test MSE of 106401124343.136826
Epoch 14: training loss 120576924656.941
Test Loss of 102866029713.676620, Test MSE of 102866031186.128998
Epoch 15: training loss 117815951781.647
Test Loss of 100176310748.587555, Test MSE of 100176309939.186966
Epoch 16: training loss 114357301940.706
Test Loss of 96506211348.607910, Test MSE of 96506212672.669785
Epoch 17: training loss 109758889216.000
Test Loss of 93284376013.190842, Test MSE of 93284377753.457718
Epoch 18: training loss 107184021549.176
Test Loss of 90620841285.699753, Test MSE of 90620844361.590683
Epoch 19: training loss 101415307392.000
Test Loss of 86566907590.380753, Test MSE of 86566908761.091171
Epoch 20: training loss 98355552813.176
Test Loss of 82730479249.321304, Test MSE of 82730480107.704697
Epoch 21: training loss 95091806689.882
Test Loss of 79910929502.038406, Test MSE of 79910928988.197662
Epoch 22: training loss 90536973537.882
Test Loss of 77160277506.724030, Test MSE of 77160278403.299179
Epoch 23: training loss 87128873185.882
Test Loss of 72456592551.468887, Test MSE of 72456592376.123642
Epoch 24: training loss 83528676148.706
Test Loss of 68947777215.274582, Test MSE of 68947777057.647644
Epoch 25: training loss 79648231996.235
Test Loss of 65577476755.453156, Test MSE of 65577478443.473511
Epoch 26: training loss 76097016380.235
Test Loss of 61709688638.712006, Test MSE of 61709688967.952499
Epoch 27: training loss 72589414264.471
Test Loss of 62447931175.024750, Test MSE of 62447932963.261223
Epoch 28: training loss 68461237360.941
Test Loss of 59679293032.579224, Test MSE of 59679292374.095276
Epoch 29: training loss 65733952052.706
Test Loss of 53863145050.366875, Test MSE of 53863146124.113937
Epoch 30: training loss 63326556544.000
Test Loss of 52622599005.268562, Test MSE of 52622599245.153648
Epoch 31: training loss 59899197379.765
Test Loss of 49508406600.068474, Test MSE of 49508406342.934525
Epoch 32: training loss 56173007917.176
Test Loss of 45922657152.325699, Test MSE of 45922656608.947342
Epoch 33: training loss 53354917477.647
Test Loss of 49091369423.796440, Test MSE of 49091369289.371529
Epoch 34: training loss 51181918930.824
Test Loss of 45600650311.298637, Test MSE of 45600650517.830734
Epoch 35: training loss 48261474635.294
Test Loss of 41500426503.402267, Test MSE of 41500426264.859940
Epoch 36: training loss 46479000835.765
Test Loss of 39942347101.150124, Test MSE of 39942347309.131203
Epoch 37: training loss 43732051215.059
Test Loss of 36877374310.743462, Test MSE of 36877375169.107620
Epoch 38: training loss 42204850394.353
Test Loss of 36346981593.685867, Test MSE of 36346981537.528938
Epoch 39: training loss 39969152060.235
Test Loss of 32576614183.972240, Test MSE of 32576614090.978371
Epoch 40: training loss 37594918151.529
Test Loss of 33747497506.701828, Test MSE of 33747497797.161663
Epoch 41: training loss 36086760700.235
Test Loss of 32139454332.772610, Test MSE of 32139454475.414906
Epoch 42: training loss 34217188811.294
Test Loss of 30247875137.969002, Test MSE of 30247875387.317783
Epoch 43: training loss 32719415544.471
Test Loss of 27934411997.475826, Test MSE of 27934412128.626694
Epoch 44: training loss 31382801904.941
Test Loss of 29411074196.755955, Test MSE of 29411074825.568470
Epoch 45: training loss 30031835316.706
Test Loss of 27881517139.379135, Test MSE of 27881517854.815624
Epoch 46: training loss 28409812630.588
Test Loss of 24042650270.349293, Test MSE of 24042650380.765842
Epoch 47: training loss 27615323414.588
Test Loss of 25496781468.928059, Test MSE of 25496781444.569466
Epoch 48: training loss 26052498898.824
Test Loss of 24762528056.671757, Test MSE of 24762528000.028770
Epoch 49: training loss 24675217204.706
Test Loss of 23230820131.234791, Test MSE of 23230819984.064720
Epoch 50: training loss 23981566516.706
Test Loss of 22694874408.801296, Test MSE of 22694874389.739838
Epoch 51: training loss 23002692009.412
Test Loss of 21216623929.856117, Test MSE of 21216624390.006737
Epoch 52: training loss 22122298895.059
Test Loss of 20960398015.511452, Test MSE of 20960397790.434032
Epoch 53: training loss 21467520809.412
Test Loss of 21355607775.015499, Test MSE of 21355607576.868973
Epoch 54: training loss 20435926019.765
Test Loss of 21647074143.163544, Test MSE of 21647074042.008724
Epoch 55: training loss 19784717248.000
Test Loss of 21159756992.103630, Test MSE of 21159757149.061169
Epoch 56: training loss 19261674868.706
Test Loss of 18682617903.848255, Test MSE of 18682618107.722992
Epoch 57: training loss 18241345029.647
Test Loss of 20353769490.476059, Test MSE of 20353769468.211319
Epoch 58: training loss 18070416368.941
Test Loss of 19511641265.891277, Test MSE of 19511640985.079559
Epoch 59: training loss 17411609159.529
Test Loss of 18742007263.193153, Test MSE of 18742007255.899998
Epoch 60: training loss 16975188107.294
Test Loss of 19574903525.411057, Test MSE of 19574903682.596004
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19574903682.596004, 'MSE - std': 0.0, 'R2 - mean': 0.8475681011627307, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005518 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917968564.706
Test Loss of 424555989289.985657, Test MSE of 424555997659.605774
Epoch 2: training loss 427898092604.235
Test Loss of 424540617126.106873, Test MSE of 424540618339.053711
Epoch 3: training loss 427870760478.118
Test Loss of 424519447254.724976, Test MSE of 424519445477.024231
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888074270.118
Test Loss of 424527236525.923645, Test MSE of 424527235719.721130
Epoch 2: training loss 427878692743.529
Test Loss of 424528063055.707581, Test MSE of 424528066360.331482
Epoch 3: training loss 427878136410.353
Test Loss of 424526638993.143677, Test MSE of 424526646913.328735
Epoch 4: training loss 422705644483.765
Test Loss of 408096935091.786255, Test MSE of 408096934179.723450
Epoch 5: training loss 386808339516.235
Test Loss of 354489153366.872986, Test MSE of 354489151896.362671
Epoch 6: training loss 318580052570.353
Test Loss of 278647289762.672241, Test MSE of 278647288900.470154
Epoch 7: training loss 225520328041.412
Test Loss of 181166127704.235016, Test MSE of 181166127404.032043
Epoch 8: training loss 159318899531.294
Test Loss of 141271797175.398560, Test MSE of 141271800113.762390
Epoch 9: training loss 139829259896.471
Test Loss of 131852954589.653488, Test MSE of 131852955206.103226
Epoch 10: training loss 133503032832.000
Test Loss of 127816113933.442520, Test MSE of 127816111570.833038
Epoch 11: training loss 130420819124.706
Test Loss of 124583390600.024063, Test MSE of 124583391646.492157
Epoch 12: training loss 128452487258.353
Test Loss of 121850813175.413376, Test MSE of 121850812264.461853
Epoch 13: training loss 124506067787.294
Test Loss of 119140772200.520004, Test MSE of 119140773881.995544
Epoch 14: training loss 120901864297.412
Test Loss of 114875525768.557022, Test MSE of 114875525237.173325
Epoch 15: training loss 117197583691.294
Test Loss of 111886738004.918808, Test MSE of 111886734785.546356
Epoch 16: training loss 112759585611.294
Test Loss of 108578719356.002777, Test MSE of 108578719374.457581
Epoch 17: training loss 108352644156.235
Test Loss of 104211821372.106415, Test MSE of 104211821521.835403
Epoch 18: training loss 105577375954.824
Test Loss of 100130773509.803375, Test MSE of 100130772610.844925
Epoch 19: training loss 102350072771.765
Test Loss of 97993890551.413376, Test MSE of 97993893533.561798
Epoch 20: training loss 96596978477.176
Test Loss of 94597508362.955353, Test MSE of 94597507656.440521
Epoch 21: training loss 93107230147.765
Test Loss of 90909806550.310440, Test MSE of 90909807183.687210
Epoch 22: training loss 88998355576.471
Test Loss of 86486334190.885956, Test MSE of 86486334133.956207
Epoch 23: training loss 85561388875.294
Test Loss of 82395773107.549393, Test MSE of 82395774223.806702
Epoch 24: training loss 82769982720.000
Test Loss of 81674944385.036316, Test MSE of 81674943282.943176
Epoch 25: training loss 78351824143.059
Test Loss of 76726705299.571594, Test MSE of 76726705879.883759
Epoch 26: training loss 75025795343.059
Test Loss of 73698615592.564423, Test MSE of 73698616664.194962
Epoch 27: training loss 71909346108.235
Test Loss of 68524757095.276428, Test MSE of 68524758012.865456
Epoch 28: training loss 68660582701.176
Test Loss of 64467175406.471436, Test MSE of 64467173938.639114
Epoch 29: training loss 65371568097.882
Test Loss of 60881975617.672913, Test MSE of 60881975861.545349
Epoch 30: training loss 61981734731.294
Test Loss of 57560333959.846405, Test MSE of 57560335940.071129
Epoch 31: training loss 59119994232.471
Test Loss of 53440775170.131851, Test MSE of 53440774493.696854
Epoch 32: training loss 55579727676.235
Test Loss of 53724710501.263008, Test MSE of 53724710181.147644
Epoch 33: training loss 52969128734.118
Test Loss of 50118927501.886650, Test MSE of 50118929100.064171
Epoch 34: training loss 50281254595.765
Test Loss of 47744294344.453392, Test MSE of 47744292942.851112
Epoch 35: training loss 47763219696.941
Test Loss of 42956460801.362015, Test MSE of 42956459941.188629
Epoch 36: training loss 45000982829.176
Test Loss of 43266946148.197083, Test MSE of 43266946725.576096
Epoch 37: training loss 43099861308.235
Test Loss of 42840754424.242424, Test MSE of 42840754807.473213
Epoch 38: training loss 41031905829.647
Test Loss of 42481805592.457092, Test MSE of 42481805446.411819
Epoch 39: training loss 38575774599.529
Test Loss of 37034747756.191536, Test MSE of 37034748095.805023
Epoch 40: training loss 36619459742.118
Test Loss of 35095745119.341202, Test MSE of 35095745514.929214
Epoch 41: training loss 34508197150.118
Test Loss of 36918474530.761047, Test MSE of 36918473960.595322
Epoch 42: training loss 32832395986.824
Test Loss of 32356461140.918808, Test MSE of 32356461034.370419
Epoch 43: training loss 31094904832.000
Test Loss of 31055744218.633358, Test MSE of 31055744485.263317
Epoch 44: training loss 29627468242.824
Test Loss of 33863730126.493637, Test MSE of 33863729273.507133
Epoch 45: training loss 28399732931.765
Test Loss of 32806948473.160305, Test MSE of 32806948610.204590
Epoch 46: training loss 26564187271.529
Test Loss of 31147218038.199398, Test MSE of 31147217969.209496
Epoch 47: training loss 25698633411.765
Test Loss of 30456557771.473515, Test MSE of 30456558471.247238
Epoch 48: training loss 24558844431.059
Test Loss of 31226633074.587093, Test MSE of 31226632537.199192
Epoch 49: training loss 23507359412.706
Test Loss of 29590211675.195930, Test MSE of 29590211392.132000
Epoch 50: training loss 22670168011.294
Test Loss of 30563998388.141567, Test MSE of 30563998189.551411
Epoch 51: training loss 21334128677.647
Test Loss of 28569141609.467499, Test MSE of 28569142146.115646
Epoch 52: training loss 20715328658.824
Test Loss of 27101444568.560722, Test MSE of 27101444612.008652
Epoch 53: training loss 19637561468.235
Test Loss of 25323821225.126995, Test MSE of 25323821647.925053
Epoch 54: training loss 19071500178.824
Test Loss of 27547681332.941013, Test MSE of 27547681932.236034
Epoch 55: training loss 18205707504.941
Test Loss of 25288676531.786259, Test MSE of 25288676737.586216
Epoch 56: training loss 17475256395.294
Test Loss of 26555662283.414295, Test MSE of 26555662071.346439
Epoch 57: training loss 17001779523.765
Test Loss of 26574146866.276196, Test MSE of 26574146956.358448
Epoch 58: training loss 16637609701.647
Test Loss of 23624301308.624565, Test MSE of 23624301579.161297
Epoch 59: training loss 15907362480.941
Test Loss of 25997736763.395790, Test MSE of 25997736700.287930
Epoch 60: training loss 15202789067.294
Test Loss of 23481486230.828590, Test MSE of 23481486286.853329
Epoch 61: training loss 14854778002.824
Test Loss of 23726216420.818874, Test MSE of 23726215894.516373
Epoch 62: training loss 14428419211.294
Test Loss of 22262675534.404812, Test MSE of 22262675764.759514
Epoch 63: training loss 13774622268.235
Test Loss of 23478199227.070091, Test MSE of 23478199561.092449
Epoch 64: training loss 13555366302.118
Test Loss of 23441013431.931530, Test MSE of 23441013024.740898
Epoch 65: training loss 13146978522.353
Test Loss of 23393973635.286606, Test MSE of 23393973974.433750
Epoch 66: training loss 12904860848.941
Test Loss of 23314426878.815636, Test MSE of 23314426664.454472
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21444665173.525238, 'MSE - std': 1869761490.9292336, 'R2 - mean': 0.8405592869439398, 'R2 - std': 0.007008814218790904} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005651 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927862994.824
Test Loss of 447258548958.778625, Test MSE of 447258549980.386902
Epoch 2: training loss 421907489370.353
Test Loss of 447240602464.111023, Test MSE of 447240602074.275574
Epoch 3: training loss 421880710204.235
Test Loss of 447216738779.640076, Test MSE of 447216732317.765991
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898448775.529
Test Loss of 447225870614.088379, Test MSE of 447225873141.817017
Epoch 2: training loss 421888658492.235
Test Loss of 447226493638.143860, Test MSE of 447226498324.957703
Epoch 3: training loss 421888219256.471
Test Loss of 447226049970.187378, Test MSE of 447226049210.511658
Epoch 4: training loss 416885644709.647
Test Loss of 430945987723.991699, Test MSE of 430945989383.761658
Epoch 5: training loss 381347277643.294
Test Loss of 376392004651.821411, Test MSE of 376392002811.116150
Epoch 6: training loss 313579375917.176
Test Loss of 297121286779.292175, Test MSE of 297121282911.303406
Epoch 7: training loss 221827827952.941
Test Loss of 196968878016.518158, Test MSE of 196968880222.927582
Epoch 8: training loss 157536733967.059
Test Loss of 154707384929.709930, Test MSE of 154707387408.210358
Epoch 9: training loss 138119980754.824
Test Loss of 142305423954.076324, Test MSE of 142305424660.711945
Epoch 10: training loss 131424073908.706
Test Loss of 138141915840.458954, Test MSE of 138141917544.522491
Epoch 11: training loss 128772322364.235
Test Loss of 134626655452.528336, Test MSE of 134626651743.644638
Epoch 12: training loss 126708209001.412
Test Loss of 131496843615.992599, Test MSE of 131496844556.406601
Epoch 13: training loss 123028170119.529
Test Loss of 127730856886.332642, Test MSE of 127730857043.275589
Epoch 14: training loss 118336138631.529
Test Loss of 124187654035.512375, Test MSE of 124187657729.619415
Epoch 15: training loss 114999146255.059
Test Loss of 121448718538.762894, Test MSE of 121448719279.452286
Epoch 16: training loss 111387083986.824
Test Loss of 117349162998.525101, Test MSE of 117349165764.460953
Epoch 17: training loss 106102090872.471
Test Loss of 113118333843.749252, Test MSE of 113118332298.153244
Epoch 18: training loss 103030290944.000
Test Loss of 109941822585.515610, Test MSE of 109941823431.548630
Epoch 19: training loss 100439265520.941
Test Loss of 105450505385.837616, Test MSE of 105450505411.822327
Epoch 20: training loss 96677520353.882
Test Loss of 101762917401.819107, Test MSE of 101762918193.792465
Epoch 21: training loss 93222264079.059
Test Loss of 97827169836.413605, Test MSE of 97827171833.329742
Epoch 22: training loss 88440451493.647
Test Loss of 94472095121.025208, Test MSE of 94472096219.666290
Epoch 23: training loss 84788322243.765
Test Loss of 91394596106.007858, Test MSE of 91394595316.758362
Epoch 24: training loss 81229406554.353
Test Loss of 87600982115.012726, Test MSE of 87600981920.160446
Epoch 25: training loss 76802793999.059
Test Loss of 84148167785.408279, Test MSE of 84148168389.660492
Epoch 26: training loss 74524693805.176
Test Loss of 80072980852.837387, Test MSE of 80072980340.611923
Epoch 27: training loss 71764021007.059
Test Loss of 77361988017.950500, Test MSE of 77361988235.428574
Epoch 28: training loss 67984872734.118
Test Loss of 71023124830.571365, Test MSE of 71023125637.417511
Epoch 29: training loss 64551657110.588
Test Loss of 70668266167.931534, Test MSE of 70668266893.050171
Epoch 30: training loss 61961565952.000
Test Loss of 65744591243.577148, Test MSE of 65744590510.406532
Epoch 31: training loss 58808107745.882
Test Loss of 63147282008.471893, Test MSE of 63147282189.500542
Epoch 32: training loss 56558725496.471
Test Loss of 58836715677.757111, Test MSE of 58836715924.998436
Epoch 33: training loss 53653484310.588
Test Loss of 55103754299.691879, Test MSE of 55103753996.097702
Epoch 34: training loss 50743572788.706
Test Loss of 52568447911.646545, Test MSE of 52568448334.438606
Epoch 35: training loss 48110261225.412
Test Loss of 52048018692.322922, Test MSE of 52048019492.961006
Epoch 36: training loss 45843090048.000
Test Loss of 48911103746.546379, Test MSE of 48911103372.226006
Epoch 37: training loss 43070751450.353
Test Loss of 46851314745.086281, Test MSE of 46851315264.933182
Epoch 38: training loss 41297407405.176
Test Loss of 44944818166.525101, Test MSE of 44944819218.774033
Epoch 39: training loss 39271684254.118
Test Loss of 40069043796.208191, Test MSE of 40069044722.656403
Epoch 40: training loss 37288710384.941
Test Loss of 40194075570.068932, Test MSE of 40194074846.670906
Epoch 41: training loss 35269445541.647
Test Loss of 37524773196.095306, Test MSE of 37524773325.026688
Epoch 42: training loss 34052720240.941
Test Loss of 35467406259.016426, Test MSE of 35467406010.165131
Epoch 43: training loss 32181758885.647
Test Loss of 34097075691.984272, Test MSE of 34097075456.073387
Epoch 44: training loss 30912113807.059
Test Loss of 30823893325.990284, Test MSE of 30823893296.021984
Epoch 45: training loss 29230945091.765
Test Loss of 32539528659.823273, Test MSE of 32539529029.098701
Epoch 46: training loss 27927891576.471
Test Loss of 29659030504.549618, Test MSE of 29659030972.800362
Epoch 47: training loss 26674998972.235
Test Loss of 30579347697.609993, Test MSE of 30579347918.424427
Epoch 48: training loss 25892210138.353
Test Loss of 27114786669.139023, Test MSE of 27114786913.296734
Epoch 49: training loss 24658369942.588
Test Loss of 27572757219.042332, Test MSE of 27572756792.352268
Epoch 50: training loss 23600722447.059
Test Loss of 27083580364.124912, Test MSE of 27083581224.827187
Epoch 51: training loss 22548125432.471
Test Loss of 26564752601.212120, Test MSE of 26564752410.381229
Epoch 52: training loss 22055601652.706
Test Loss of 25772739999.237568, Test MSE of 25772740071.463711
Epoch 53: training loss 20738122311.529
Test Loss of 23274388436.652325, Test MSE of 23274388497.236713
Epoch 54: training loss 20310901368.471
Test Loss of 23067022308.759659, Test MSE of 23067022391.719822
Epoch 55: training loss 19406525699.765
Test Loss of 23771075199.082119, Test MSE of 23771075202.138763
Epoch 56: training loss 18677773703.529
Test Loss of 22166348966.521397, Test MSE of 22166349990.893402
Epoch 57: training loss 18061987568.941
Test Loss of 22402733827.493870, Test MSE of 22402734316.496681
Epoch 58: training loss 17535399363.765
Test Loss of 22548218382.330788, Test MSE of 22548218698.462524
Epoch 59: training loss 16846205857.882
Test Loss of 20674142838.554707, Test MSE of 20674142395.342812
Epoch 60: training loss 16496857776.941
Test Loss of 23408317045.607216, Test MSE of 23408317018.834141
Epoch 61: training loss 16241427504.941
Test Loss of 22781450244.737450, Test MSE of 22781450006.413147
Epoch 62: training loss 15508880685.176
Test Loss of 23313237622.317833, Test MSE of 23313237890.682613
Epoch 63: training loss 15055546563.765
Test Loss of 20541289739.902843, Test MSE of 20541289348.424973
Epoch 64: training loss 14469994970.353
Test Loss of 23070370343.913025, Test MSE of 23070370638.726044
Epoch 65: training loss 14545453624.471
Test Loss of 20899685912.516308, Test MSE of 20899685651.015919
Epoch 66: training loss 13846122582.588
Test Loss of 22119971038.186443, Test MSE of 22119971157.485447
Epoch 67: training loss 13658013526.588
Test Loss of 20187626322.372425, Test MSE of 20187626337.463219
Epoch 68: training loss 13174111137.882
Test Loss of 20575725575.579922, Test MSE of 20575725690.634792
Epoch 69: training loss 13025746593.882
Test Loss of 20661044182.073559, Test MSE of 20661043962.517948
Epoch 70: training loss 12528886277.647
Test Loss of 20086911832.057369, Test MSE of 20086911877.120930
Epoch 71: training loss 12296132600.471
Test Loss of 22214123000.301643, Test MSE of 22214123460.543896
Epoch 72: training loss 12069382529.882
Test Loss of 22762935708.395096, Test MSE of 22762935508.821983
Epoch 73: training loss 11742222369.882
Test Loss of 20137744504.331253, Test MSE of 20137744656.777283
Epoch 74: training loss 11430910501.647
Test Loss of 18230316990.149433, Test MSE of 18230317213.871033
Epoch 75: training loss 11298442503.529
Test Loss of 22516585176.146194, Test MSE of 22516584949.526123
Epoch 76: training loss 10888383058.824
Test Loss of 20184413078.117973, Test MSE of 20184412918.412678
Epoch 77: training loss 10597791019.294
Test Loss of 20043478877.979179, Test MSE of 20043478762.482689
Epoch 78: training loss 10212843115.294
Test Loss of 19177938158.530651, Test MSE of 19177938323.622883
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20689089556.891117, 'MSE - std': 1863454063.5279055, 'R2 - mean': 0.8511507680332214, 'R2 - std': 0.01603458536512777} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004391 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110789632.000
Test Loss of 410763907305.136536, Test MSE of 410763910898.993225
Epoch 2: training loss 430089460796.235
Test Loss of 410746046516.124023, Test MSE of 410746039835.278015
Epoch 3: training loss 430061571132.235
Test Loss of 410721808234.261902, Test MSE of 410721812815.080078
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076720067.765
Test Loss of 410727063821.149475, Test MSE of 410727071313.790527
Epoch 2: training loss 430064907444.706
Test Loss of 410727556175.133728, Test MSE of 410727559645.855408
Epoch 3: training loss 430064487484.235
Test Loss of 410727728779.550232, Test MSE of 410727731780.616699
Epoch 4: training loss 424898892257.882
Test Loss of 394485643575.796387, Test MSE of 394485643694.434082
Epoch 5: training loss 389009004544.000
Test Loss of 340347781741.697388, Test MSE of 340347782232.103516
Epoch 6: training loss 320501513155.765
Test Loss of 262939786316.290619, Test MSE of 262939778728.854919
Epoch 7: training loss 228907706247.529
Test Loss of 164915000465.473389, Test MSE of 164915001099.428040
Epoch 8: training loss 163994071100.235
Test Loss of 125038674526.534012, Test MSE of 125038674248.369995
Epoch 9: training loss 143361915934.118
Test Loss of 114651146629.508560, Test MSE of 114651146012.016022
Epoch 10: training loss 138830488786.824
Test Loss of 110754599653.108749, Test MSE of 110754600342.782227
Epoch 11: training loss 135184436796.235
Test Loss of 107509579832.862564, Test MSE of 107509582555.895447
Epoch 12: training loss 132217101101.176
Test Loss of 104821962108.505325, Test MSE of 104821962400.178772
Epoch 13: training loss 129861641125.647
Test Loss of 101831256859.128174, Test MSE of 101831255844.160263
Epoch 14: training loss 125575176613.647
Test Loss of 99057747182.348907, Test MSE of 99057747318.509018
Epoch 15: training loss 120901793129.412
Test Loss of 96038234740.331329, Test MSE of 96038233157.030548
Epoch 16: training loss 116169818142.118
Test Loss of 91812155382.522903, Test MSE of 91812157411.307892
Epoch 17: training loss 113392329517.176
Test Loss of 88668625704.396118, Test MSE of 88668626827.158813
Epoch 18: training loss 108566395723.294
Test Loss of 86419453388.112915, Test MSE of 86419453313.564240
Epoch 19: training loss 105223114360.471
Test Loss of 83662173158.885696, Test MSE of 83662173802.280807
Epoch 20: training loss 101724599597.176
Test Loss of 80364587560.514572, Test MSE of 80364587068.764572
Epoch 21: training loss 98412180811.294
Test Loss of 77008226541.875061, Test MSE of 77008227118.027054
Epoch 22: training loss 94169725214.118
Test Loss of 72665072283.187408, Test MSE of 72665071703.001923
Epoch 23: training loss 90138735073.882
Test Loss of 70681779348.316513, Test MSE of 70681778654.890671
Epoch 24: training loss 86552007183.059
Test Loss of 66403808886.700600, Test MSE of 66403808477.296997
Epoch 25: training loss 83205862098.824
Test Loss of 64142674264.018509, Test MSE of 64142675439.068947
Epoch 26: training loss 79600621221.647
Test Loss of 62719403333.538177, Test MSE of 62719404525.734390
Epoch 27: training loss 75648228065.882
Test Loss of 59464988024.714485, Test MSE of 59464988672.419350
Epoch 28: training loss 72079562691.765
Test Loss of 57505038938.743172, Test MSE of 57505039995.665169
Epoch 29: training loss 69022151002.353
Test Loss of 54728999056.051826, Test MSE of 54729000159.605995
Epoch 30: training loss 66520276931.765
Test Loss of 49225548856.388710, Test MSE of 49225547948.145836
Epoch 31: training loss 62795319552.000
Test Loss of 48807386633.714020, Test MSE of 48807387239.742218
Epoch 32: training loss 59663524833.882
Test Loss of 48223770551.026375, Test MSE of 48223770894.076859
Epoch 33: training loss 57076294490.353
Test Loss of 45250745347.316986, Test MSE of 45250744722.770172
Epoch 34: training loss 55050940950.588
Test Loss of 43764950811.602036, Test MSE of 43764950037.995598
Epoch 35: training loss 51550072169.412
Test Loss of 41435584526.215637, Test MSE of 41435584121.758972
Epoch 36: training loss 48965305246.118
Test Loss of 39565201624.077744, Test MSE of 39565201236.233810
Epoch 37: training loss 46491290300.235
Test Loss of 38108241747.516891, Test MSE of 38108240847.587395
Epoch 38: training loss 44747551525.647
Test Loss of 35282249054.178619, Test MSE of 35282249114.927086
Epoch 39: training loss 42510602157.176
Test Loss of 33630238095.459511, Test MSE of 33630237996.217117
Epoch 40: training loss 39820927186.824
Test Loss of 31420664438.226746, Test MSE of 31420663870.879311
Epoch 41: training loss 38148602518.588
Test Loss of 32429019623.122627, Test MSE of 32429020285.828854
Epoch 42: training loss 36746117609.412
Test Loss of 29964614053.730679, Test MSE of 29964614448.371315
Epoch 43: training loss 34961047612.235
Test Loss of 29292996866.724663, Test MSE of 29292997245.304867
Epoch 44: training loss 33287098955.294
Test Loss of 27572912660.612679, Test MSE of 27572913368.412853
Epoch 45: training loss 31758020728.471
Test Loss of 26181342053.997223, Test MSE of 26181341955.175323
Epoch 46: training loss 29866680726.588
Test Loss of 25613175768.670059, Test MSE of 25613175973.254524
Epoch 47: training loss 29026534196.706
Test Loss of 24160518709.782509, Test MSE of 24160518751.654167
Epoch 48: training loss 27543728203.294
Test Loss of 24126317413.997223, Test MSE of 24126317944.785168
Epoch 49: training loss 26513173835.294
Test Loss of 23724331486.593243, Test MSE of 23724331030.301800
Epoch 50: training loss 25401065106.824
Test Loss of 23657762927.829708, Test MSE of 23657762574.352406
Epoch 51: training loss 24351732950.588
Test Loss of 22163746560.118465, Test MSE of 22163746538.833519
Epoch 52: training loss 23846494863.059
Test Loss of 21276295174.633965, Test MSE of 21276295739.638645
Epoch 53: training loss 22815783736.471
Test Loss of 21492310084.235077, Test MSE of 21492310732.335838
Epoch 54: training loss 21891201799.529
Test Loss of 20351777477.834335, Test MSE of 20351777508.271164
Epoch 55: training loss 20724721912.471
Test Loss of 19526109237.071728, Test MSE of 19526109556.155743
Epoch 56: training loss 20329525289.412
Test Loss of 20495552972.586765, Test MSE of 20495552816.691822
Epoch 57: training loss 19625142622.118
Test Loss of 19055201555.309578, Test MSE of 19055201725.590343
Epoch 58: training loss 19182532412.235
Test Loss of 17458008687.592781, Test MSE of 17458008525.739315
Epoch 59: training loss 18417378910.118
Test Loss of 20012641325.963905, Test MSE of 20012641574.055431
Epoch 60: training loss 17991273784.471
Test Loss of 17352396784.836651, Test MSE of 17352397057.243687
Epoch 61: training loss 17491531279.059
Test Loss of 18844488266.632114, Test MSE of 18844487991.226173
Epoch 62: training loss 16777252088.471
Test Loss of 18206472014.778343, Test MSE of 18206472355.228786
Epoch 63: training loss 16593260122.353
Test Loss of 18536436142.260063, Test MSE of 18536436295.784771
Epoch 64: training loss 15935283745.882
Test Loss of 18813897693.408607, Test MSE of 18813897676.531738
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20220291586.801273, 'MSE - std': 1806560376.9152238, 'R2 - mean': 0.849542850377028, 'R2 - std': 0.014162879066099955} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005463 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043707693.176
Test Loss of 431612903689.358643, Test MSE of 431612909314.224670
Epoch 2: training loss 424024664545.882
Test Loss of 431593790619.898193, Test MSE of 431593788493.618713
Epoch 3: training loss 423997922484.706
Test Loss of 431567276303.992615, Test MSE of 431567280261.797180
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424014098432.000
Test Loss of 431568501851.453979, Test MSE of 431568501438.894226
Epoch 2: training loss 424001228800.000
Test Loss of 431571518205.749207, Test MSE of 431571518977.727234
Epoch 3: training loss 424000595847.529
Test Loss of 431571057603.820435, Test MSE of 431571057010.826721
Epoch 4: training loss 418612096301.176
Test Loss of 414108097980.001831, Test MSE of 414108098613.350830
Epoch 5: training loss 382167904496.941
Test Loss of 358562460028.979187, Test MSE of 358562454968.049377
Epoch 6: training loss 314563218070.588
Test Loss of 280076458784.814453, Test MSE of 280076461661.622009
Epoch 7: training loss 224220647363.765
Test Loss of 179878242716.253601, Test MSE of 179878242937.783752
Epoch 8: training loss 160154419350.588
Test Loss of 138553872916.612671, Test MSE of 138553872797.812378
Epoch 9: training loss 139879777942.588
Test Loss of 127020242201.943542, Test MSE of 127020241105.662476
Epoch 10: training loss 134342140461.176
Test Loss of 121807729074.050903, Test MSE of 121807730297.525482
Epoch 11: training loss 131949606475.294
Test Loss of 119013433423.607590, Test MSE of 119013433949.615250
Epoch 12: training loss 130063920278.588
Test Loss of 114734030495.452103, Test MSE of 114734028503.291153
Epoch 13: training loss 125792538172.235
Test Loss of 111745938490.284134, Test MSE of 111745938122.025879
Epoch 14: training loss 121914528647.529
Test Loss of 108000118244.753357, Test MSE of 108000120622.434921
Epoch 15: training loss 118927009822.118
Test Loss of 104567385939.516891, Test MSE of 104567386251.932648
Epoch 16: training loss 116218148171.294
Test Loss of 101034140148.390564, Test MSE of 101034140394.642944
Epoch 17: training loss 111012391875.765
Test Loss of 95825614811.039337, Test MSE of 95825615133.841339
Epoch 18: training loss 105766329404.235
Test Loss of 93387612743.788986, Test MSE of 93387611827.715057
Epoch 19: training loss 103032812107.294
Test Loss of 90218696675.568726, Test MSE of 90218699156.566330
Epoch 20: training loss 100032831457.882
Test Loss of 86302188149.279037, Test MSE of 86302187386.906479
Epoch 21: training loss 95841463130.353
Test Loss of 85390101919.570572, Test MSE of 85390102202.070831
Epoch 22: training loss 91934451019.294
Test Loss of 80385422807.485428, Test MSE of 80385423343.668579
Epoch 23: training loss 89042306228.706
Test Loss of 77627640868.486816, Test MSE of 77627638328.817551
Epoch 24: training loss 85155290910.118
Test Loss of 74732192325.893570, Test MSE of 74732191939.354645
Epoch 25: training loss 81604838234.353
Test Loss of 70498062531.228134, Test MSE of 70498064192.891891
Epoch 26: training loss 77169352613.647
Test Loss of 66054144680.455345, Test MSE of 66054145572.529198
Epoch 27: training loss 74833721615.059
Test Loss of 64187894790.160110, Test MSE of 64187894945.351547
Epoch 28: training loss 71151586349.176
Test Loss of 60655720215.811195, Test MSE of 60655719532.329620
Epoch 29: training loss 68741700005.647
Test Loss of 59701594160.333176, Test MSE of 59701593798.256348
Epoch 30: training loss 64873638957.176
Test Loss of 54080910149.775101, Test MSE of 54080910365.616783
Epoch 31: training loss 62052289863.529
Test Loss of 54286558914.517357, Test MSE of 54286558727.164101
Epoch 32: training loss 58746773436.235
Test Loss of 52517972040.973625, Test MSE of 52517972955.775093
Epoch 33: training loss 56645389899.294
Test Loss of 49598759253.175385, Test MSE of 49598759448.334877
Epoch 34: training loss 53991864982.588
Test Loss of 50048500165.005089, Test MSE of 50048501081.118706
Epoch 35: training loss 51464415066.353
Test Loss of 46462538499.435448, Test MSE of 46462538186.228241
Epoch 36: training loss 48308012318.118
Test Loss of 44220394073.795464, Test MSE of 44220393922.989090
Epoch 37: training loss 46247936297.412
Test Loss of 44153476116.375755, Test MSE of 44153476109.596771
Epoch 38: training loss 44299435188.706
Test Loss of 40123372490.454422, Test MSE of 40123373309.204437
Epoch 39: training loss 42014930251.294
Test Loss of 36920560787.368813, Test MSE of 36920560679.339783
Epoch 40: training loss 40239502351.059
Test Loss of 37524092661.693657, Test MSE of 37524093072.888611
Epoch 41: training loss 37838966912.000
Test Loss of 35080767305.092087, Test MSE of 35080766620.686836
Epoch 42: training loss 36800615213.176
Test Loss of 33903256797.290142, Test MSE of 33903256783.666805
Epoch 43: training loss 34649653255.529
Test Loss of 32480016584.914391, Test MSE of 32480016426.168221
Epoch 44: training loss 33282561155.765
Test Loss of 31434389694.489590, Test MSE of 31434390127.266369
Epoch 45: training loss 31722767962.353
Test Loss of 30042770763.224434, Test MSE of 30042771282.792637
Epoch 46: training loss 30555810944.000
Test Loss of 27172411337.980564, Test MSE of 27172411433.131889
Epoch 47: training loss 29289983179.294
Test Loss of 29215743641.765850, Test MSE of 29215743625.768978
Epoch 48: training loss 27835244152.471
Test Loss of 24693393099.994446, Test MSE of 24693393228.163677
Epoch 49: training loss 27080105968.941
Test Loss of 28186533456.792225, Test MSE of 28186532671.363670
Epoch 50: training loss 25779645997.176
Test Loss of 24613428567.544655, Test MSE of 24613428622.599171
Epoch 51: training loss 24729947392.000
Test Loss of 24683431837.438225, Test MSE of 24683431795.322842
Epoch 52: training loss 24017419738.353
Test Loss of 22927835061.130959, Test MSE of 22927835296.782806
Epoch 53: training loss 22549397974.588
Test Loss of 23326295474.524757, Test MSE of 23326295456.702942
Epoch 54: training loss 22174557692.235
Test Loss of 23305103475.620544, Test MSE of 23305103310.164371
Epoch 55: training loss 20866923892.706
Test Loss of 20406407204.012959, Test MSE of 20406407248.270741
Epoch 56: training loss 20523417182.118
Test Loss of 21661118424.196205, Test MSE of 21661118484.280174
Epoch 57: training loss 19823743702.588
Test Loss of 20702617969.606663, Test MSE of 20702617968.634575
Epoch 58: training loss 19243513690.353
Test Loss of 19364943341.282738, Test MSE of 19364943316.216690
Epoch 59: training loss 18804363038.118
Test Loss of 24602638819.331791, Test MSE of 24602639008.874622
Epoch 60: training loss 17756609953.882
Test Loss of 22449440846.659882, Test MSE of 22449440762.251347
Epoch 61: training loss 17607021219.765
Test Loss of 20762505236.849606, Test MSE of 20762505177.980659
Epoch 62: training loss 17211753069.176
Test Loss of 22320401042.658028, Test MSE of 22320400784.857403
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20640313426.4125, 'MSE - std': 1821153947.1866877, 'R2 - mean': 0.8462963639503936, 'R2 - std': 0.0142347606662726} 
 

Saving model.....
Results After CV: {'MSE - mean': 20640313426.4125, 'MSE - std': 1821153947.1866877, 'R2 - mean': 0.8462963639503936, 'R2 - std': 0.0142347606662726}
Train time: 101.68176441520004
Inference time: 0.07315782359992226
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 6 finished with value: 20640313426.4125 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005626 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427524441389.176
Test Loss of 418108461056.947510, Test MSE of 418108463261.588135
Epoch 2: training loss 427502992805.647
Test Loss of 418088271527.113586, Test MSE of 418088270761.366394
Epoch 3: training loss 427474736911.059
Test Loss of 418062682895.100647, Test MSE of 418062681197.938354
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427486025607.529
Test Loss of 418065186262.665771, Test MSE of 418065184198.063354
Epoch 2: training loss 427474389955.765
Test Loss of 418066901583.707581, Test MSE of 418066899417.697693
Epoch 3: training loss 427474106608.941
Test Loss of 418066453011.541992, Test MSE of 418066454542.958252
Epoch 4: training loss 427473868197.647
Test Loss of 418066714352.070312, Test MSE of 418066712737.076782
Epoch 5: training loss 427473705923.765
Test Loss of 418066241948.631958, Test MSE of 418066244845.393127
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418066244845.3931, 'MSE - std': 0.0, 'R2 - mean': -2.255527208453606, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005539 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918700062.118
Test Loss of 424556841076.067566, Test MSE of 424556841362.241150
Epoch 2: training loss 427899335619.765
Test Loss of 424540808238.663879, Test MSE of 424540813707.489502
Epoch 3: training loss 427872936417.882
Test Loss of 424518875364.818848, Test MSE of 424518873092.003784
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886737528.471
Test Loss of 424527245750.687927, Test MSE of 424527248659.648376
Epoch 2: training loss 427877522974.118
Test Loss of 424527733761.184387, Test MSE of 424527729135.907715
Epoch 3: training loss 427877221556.706
Test Loss of 424526961283.819580, Test MSE of 424526966446.722717
Epoch 4: training loss 427876996517.647
Test Loss of 424526691309.760803, Test MSE of 424526691807.153503
Epoch 5: training loss 427876803041.882
Test Loss of 424526303369.386047, Test MSE of 424526303972.474792
Epoch 6: training loss 427876629082.353
Test Loss of 424525496107.762207, Test MSE of 424525491325.268005
Epoch 7: training loss 427876486204.235
Test Loss of 424524956553.800598, Test MSE of 424524954264.438171
Epoch 8: training loss 427876377057.882
Test Loss of 424524775287.561401, Test MSE of 424524780829.271362
Epoch 9: training loss 427876289355.294
Test Loss of 424524240016.255371, Test MSE of 424524238852.532776
Epoch 10: training loss 415268537524.706
Test Loss of 384828265472.236877, Test MSE of 384828262122.284790
Epoch 11: training loss 337144286750.118
Test Loss of 281268219035.151489, Test MSE of 281268215308.717896
Epoch 12: training loss 231772264990.118
Test Loss of 187625397485.583160, Test MSE of 187625395053.302612
Epoch 13: training loss 164087150953.412
Test Loss of 144449710351.455933, Test MSE of 144449715229.124969
Epoch 14: training loss 142159699124.706
Test Loss of 132553484011.806610, Test MSE of 132553485726.109711
Epoch 15: training loss 135641846994.824
Test Loss of 128655180507.462418, Test MSE of 128655180108.815857
Epoch 16: training loss 133153495792.941
Test Loss of 126125019995.373581, Test MSE of 126125024007.912445
Epoch 17: training loss 129777346409.412
Test Loss of 123414207998.460327, Test MSE of 123414209474.102615
Epoch 18: training loss 127449302136.471
Test Loss of 119825461400.545914, Test MSE of 119825462240.268341
Epoch 19: training loss 122587085251.765
Test Loss of 116696255923.608612, Test MSE of 116696258027.654739
Epoch 20: training loss 118412085579.294
Test Loss of 111169058402.420547, Test MSE of 111169059583.302765
Epoch 21: training loss 114075389319.529
Test Loss of 107463667205.092758, Test MSE of 107463667309.931931
Epoch 22: training loss 109784983190.588
Test Loss of 103704080458.851730, Test MSE of 103704080198.653107
Epoch 23: training loss 105693329317.647
Test Loss of 99870403392.370117, Test MSE of 99870401583.984482
Epoch 24: training loss 101501323776.000
Test Loss of 97795325224.327545, Test MSE of 97795324617.667816
Epoch 25: training loss 99270247634.824
Test Loss of 93481377910.436264, Test MSE of 93481379941.422729
Epoch 26: training loss 95596007484.235
Test Loss of 89496827241.230621, Test MSE of 89496828444.740921
Epoch 27: training loss 90537473716.706
Test Loss of 88610958436.197083, Test MSE of 88610959567.909592
Epoch 28: training loss 88421821364.706
Test Loss of 84727532853.592407, Test MSE of 84727533874.142212
Epoch 29: training loss 83344504244.706
Test Loss of 80557901543.069168, Test MSE of 80557902173.675842
Epoch 30: training loss 79690208903.529
Test Loss of 73613991484.047195, Test MSE of 73613991900.233368
Epoch 31: training loss 76105911070.118
Test Loss of 73044762860.872543, Test MSE of 73044760850.896255
Epoch 32: training loss 72055480651.294
Test Loss of 71083582864.077728, Test MSE of 71083582455.094070
Epoch 33: training loss 68804304941.176
Test Loss of 66236223566.641685, Test MSE of 66236223253.323242
Epoch 34: training loss 65533065366.588
Test Loss of 64055722663.350449, Test MSE of 64055723145.458015
Epoch 35: training loss 63132681472.000
Test Loss of 60815511839.089523, Test MSE of 60815512948.851723
Epoch 36: training loss 60251755535.059
Test Loss of 56892695964.395096, Test MSE of 56892696307.040405
Epoch 37: training loss 57060400730.353
Test Loss of 53428525472.421928, Test MSE of 53428526658.020256
Epoch 38: training loss 53343770352.941
Test Loss of 50595313535.615082, Test MSE of 50595313595.145943
Epoch 39: training loss 51056394511.059
Test Loss of 48017005621.770065, Test MSE of 48017004871.759041
Epoch 40: training loss 48198932781.176
Test Loss of 47226938261.644226, Test MSE of 47226936857.164574
Epoch 41: training loss 46027421967.059
Test Loss of 40479950632.445984, Test MSE of 40479950933.074486
Epoch 42: training loss 43197345603.765
Test Loss of 40447910582.984039, Test MSE of 40447910938.595505
Epoch 43: training loss 40833484521.412
Test Loss of 36994861851.891739, Test MSE of 36994861832.869682
Epoch 44: training loss 38744758226.824
Test Loss of 37644376064.947487, Test MSE of 37644375946.745850
Epoch 45: training loss 36892033724.235
Test Loss of 35418923490.272499, Test MSE of 35418923432.015747
Epoch 46: training loss 35214332521.412
Test Loss of 37036214464.814247, Test MSE of 37036214406.601524
Epoch 47: training loss 32720078787.765
Test Loss of 33122438502.861900, Test MSE of 33122437952.054123
Epoch 48: training loss 31250786371.765
Test Loss of 31008417293.620171, Test MSE of 31008417947.680611
Epoch 49: training loss 28686632990.118
Test Loss of 28537148252.321072, Test MSE of 28537148194.375309
Epoch 50: training loss 27853688252.235
Test Loss of 30576432063.333797, Test MSE of 30576432515.819942
Epoch 51: training loss 26268562989.176
Test Loss of 28656088095.977795, Test MSE of 28656088106.492390
Epoch 52: training loss 24806929453.176
Test Loss of 28196257888.643997, Test MSE of 28196258072.670876
Epoch 53: training loss 24241306209.882
Test Loss of 28043663493.596115, Test MSE of 28043663996.701206
Epoch 54: training loss 22629089114.353
Test Loss of 29615168073.548923, Test MSE of 29615168308.192982
Epoch 55: training loss 21883115098.353
Test Loss of 26552729925.462872, Test MSE of 26552730491.059277
Epoch 56: training loss 20701487073.882
Test Loss of 26051411613.638676, Test MSE of 26051412152.259651
Epoch 57: training loss 19932722428.235
Test Loss of 26114389063.298634, Test MSE of 26114389042.789196
Epoch 58: training loss 19114741854.118
Test Loss of 28731234951.609531, Test MSE of 28731235617.228127
Epoch 59: training loss 18251888335.059
Test Loss of 24622771167.785336, Test MSE of 24622770847.948490
Epoch 60: training loss 17452139572.706
Test Loss of 23255233264.780941, Test MSE of 23255234040.339886
Epoch 61: training loss 16891697389.176
Test Loss of 26326764225.406429, Test MSE of 26326764793.014679
Epoch 62: training loss 16137041144.471
Test Loss of 24551920349.357391, Test MSE of 24551919930.692524
Epoch 63: training loss 15734056252.235
Test Loss of 24783906892.272957, Test MSE of 24783906350.151184
Epoch 64: training loss 15319165466.353
Test Loss of 25681864124.609760, Test MSE of 25681864435.445255
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221874054640.4192, 'MSE - std': 196192190204.97394, 'R2 - mean': -0.7194393348021124, 'R2 - std': 1.5360878736514938} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005691 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927412675.765
Test Loss of 447257758357.821899, Test MSE of 447257764545.463135
Epoch 2: training loss 421906271412.706
Test Loss of 447238461303.087646, Test MSE of 447238460544.318298
Epoch 3: training loss 421878434996.706
Test Loss of 447213325298.498291, Test MSE of 447213330419.735535
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897778356.706
Test Loss of 447222640203.207031, Test MSE of 447222648671.382263
Epoch 2: training loss 421885446505.412
Test Loss of 447222152223.030273, Test MSE of 447222146847.240417
Epoch 3: training loss 421885110392.471
Test Loss of 447221533655.968567, Test MSE of 447221541350.524048
Epoch 4: training loss 421884876559.059
Test Loss of 447221345067.525330, Test MSE of 447221346167.267151
Epoch 5: training loss 421884656700.235
Test Loss of 447221292750.908142, Test MSE of 447221295842.427124
Epoch 6: training loss 421884478162.824
Test Loss of 447221779988.252625, Test MSE of 447221781602.288391
Epoch 7: training loss 421884367329.882
Test Loss of 447221681282.516785, Test MSE of 447221691163.646301
Epoch 8: training loss 421884261315.765
Test Loss of 447221816053.518372, Test MSE of 447221816969.328918
Epoch 9: training loss 421884162288.941
Test Loss of 447221281259.747375, Test MSE of 447221283891.678406
Epoch 10: training loss 409133790268.235
Test Loss of 406567216777.267639, Test MSE of 406567222192.589661
Epoch 11: training loss 331478467764.706
Test Loss of 300696651641.930115, Test MSE of 300696654334.486389
Epoch 12: training loss 227502789933.176
Test Loss of 203939738330.514923, Test MSE of 203939735378.512512
Epoch 13: training loss 160370657761.882
Test Loss of 157035533198.538055, Test MSE of 157035536151.009827
Epoch 14: training loss 137509986846.118
Test Loss of 143163157060.337738, Test MSE of 143163157544.932465
Epoch 15: training loss 133632948555.294
Test Loss of 138491883723.710388, Test MSE of 138491885019.413452
Epoch 16: training loss 128591811945.412
Test Loss of 135281208520.631042, Test MSE of 135281208083.410477
Epoch 17: training loss 126368660931.765
Test Loss of 131879032100.063843, Test MSE of 131879032513.227509
Epoch 18: training loss 124040052675.765
Test Loss of 130141890026.089294, Test MSE of 130141891302.656113
Epoch 19: training loss 119744901029.647
Test Loss of 124232186209.887573, Test MSE of 124232187116.114578
Epoch 20: training loss 115613287363.765
Test Loss of 120036268008.786484, Test MSE of 120036266905.319122
Epoch 21: training loss 110627565266.824
Test Loss of 117077331752.919739, Test MSE of 117077333370.905136
Epoch 22: training loss 106468941914.353
Test Loss of 113422006474.289154, Test MSE of 113422005109.652588
Epoch 23: training loss 103147780638.118
Test Loss of 107842158895.196854, Test MSE of 107842160673.219543
Epoch 24: training loss 99105680926.118
Test Loss of 105332581790.290070, Test MSE of 105332581256.404816
Epoch 25: training loss 95685791382.588
Test Loss of 101691330534.417770, Test MSE of 101691330077.060196
Epoch 26: training loss 92188476973.176
Test Loss of 96749295615.526260, Test MSE of 96749296336.342957
Epoch 27: training loss 88419846415.059
Test Loss of 95287657063.868607, Test MSE of 95287658219.912231
Epoch 28: training loss 84507645334.588
Test Loss of 89147604063.459641, Test MSE of 89147605516.091309
Epoch 29: training loss 80661225441.882
Test Loss of 87131359581.150131, Test MSE of 87131360544.398254
Epoch 30: training loss 76484246784.000
Test Loss of 79269262031.145035, Test MSE of 79269261007.508591
Epoch 31: training loss 73016709135.059
Test Loss of 77034291925.777466, Test MSE of 77034290825.663635
Epoch 32: training loss 69299346552.471
Test Loss of 71642329077.577606, Test MSE of 71642330001.083450
Epoch 33: training loss 65611346989.176
Test Loss of 70619945768.682861, Test MSE of 70619946575.575348
Epoch 34: training loss 62905892427.294
Test Loss of 64424061794.479759, Test MSE of 64424060854.146385
Epoch 35: training loss 59694906322.824
Test Loss of 63810644012.058296, Test MSE of 63810644188.989342
Epoch 36: training loss 56681608365.176
Test Loss of 60319280322.946106, Test MSE of 60319280321.499893
Epoch 37: training loss 54262464828.235
Test Loss of 59958928572.787415, Test MSE of 59958928347.707550
Epoch 38: training loss 50966159804.235
Test Loss of 52236703970.923897, Test MSE of 52236704910.864143
Epoch 39: training loss 48369109564.235
Test Loss of 52407886776.227623, Test MSE of 52407887198.248032
Epoch 40: training loss 45818192225.882
Test Loss of 50545749990.417763, Test MSE of 50545749698.642685
Epoch 41: training loss 43585700668.235
Test Loss of 47763995130.433495, Test MSE of 47763995560.626343
Epoch 42: training loss 40990769167.059
Test Loss of 48248960506.196625, Test MSE of 48248960539.517555
Epoch 43: training loss 38965233927.529
Test Loss of 40057561969.165855, Test MSE of 40057562059.713043
Epoch 44: training loss 36883518893.176
Test Loss of 42102573046.998840, Test MSE of 42102572387.545753
Epoch 45: training loss 35399231299.765
Test Loss of 38797990547.926903, Test MSE of 38797990684.410828
Epoch 46: training loss 33238699994.353
Test Loss of 36613532807.254219, Test MSE of 36613531956.305779
Epoch 47: training loss 31644989033.412
Test Loss of 34239060968.075874, Test MSE of 34239061013.593716
Epoch 48: training loss 29611161569.882
Test Loss of 32450160798.704605, Test MSE of 32450160287.155888
Epoch 49: training loss 28574314081.882
Test Loss of 29342922874.699978, Test MSE of 29342922932.415096
Epoch 50: training loss 26545925688.471
Test Loss of 32635135163.366180, Test MSE of 32635135518.086281
Epoch 51: training loss 25372011730.824
Test Loss of 26668948676.841084, Test MSE of 26668949250.839771
Epoch 52: training loss 24686799680.000
Test Loss of 26430065403.203331, Test MSE of 26430065630.098289
Epoch 53: training loss 23454340867.765
Test Loss of 24438179028.000927, Test MSE of 24438178916.999115
Epoch 54: training loss 22429929916.235
Test Loss of 27847551061.747860, Test MSE of 27847550999.504581
Epoch 55: training loss 22036825532.235
Test Loss of 29258909794.065231, Test MSE of 29258909987.596668
Epoch 56: training loss 20735167589.647
Test Loss of 25281595192.079575, Test MSE of 25281595331.546669
Epoch 57: training loss 20103347456.000
Test Loss of 27085367301.684940, Test MSE of 27085368089.704666
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 156944492456.8477, 'MSE - std': 184641850859.93823, 'R2 - mean': -0.20639472583365803, 'R2 - std': 1.448956007603164} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005562 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110856854.588
Test Loss of 410764616009.802856, Test MSE of 410764616140.819580
Epoch 2: training loss 430090762360.471
Test Loss of 410746431938.161987, Test MSE of 410746429702.656433
Epoch 3: training loss 430063711653.647
Test Loss of 410722744692.449768, Test MSE of 410722741340.867126
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078928293.647
Test Loss of 410728507554.058289, Test MSE of 410728509666.924011
Epoch 2: training loss 430065860848.941
Test Loss of 410728115081.062500, Test MSE of 410728120050.731628
Epoch 3: training loss 430065531000.471
Test Loss of 410728378013.556702, Test MSE of 410728377768.930908
Epoch 4: training loss 430065283794.824
Test Loss of 410728041751.100403, Test MSE of 410728032905.441589
Epoch 5: training loss 430065085500.235
Test Loss of 410727546334.119385, Test MSE of 410727550604.429626
Epoch 6: training loss 430064928406.588
Test Loss of 410727477022.445190, Test MSE of 410727485207.593445
Epoch 7: training loss 430064812272.941
Test Loss of 410726921331.146667, Test MSE of 410726918636.013428
Epoch 8: training loss 430064719269.647
Test Loss of 410727092586.024963, Test MSE of 410727091187.281616
Epoch 9: training loss 430064647951.059
Test Loss of 410726959679.259583, Test MSE of 410726961733.608948
Epoch 10: training loss 417552294851.765
Test Loss of 370799720592.999512, Test MSE of 370799715631.377991
Epoch 11: training loss 339736681411.765
Test Loss of 266177162136.225830, Test MSE of 266177161894.792908
Epoch 12: training loss 235827211926.588
Test Loss of 172259454128.747803, Test MSE of 172259453511.373962
Epoch 13: training loss 168463317775.059
Test Loss of 127065377456.984726, Test MSE of 127065376345.297455
Epoch 14: training loss 144463898051.765
Test Loss of 115046278352.022217, Test MSE of 115046278152.470444
Epoch 15: training loss 139547044291.765
Test Loss of 111537649718.019440, Test MSE of 111537650541.662827
Epoch 16: training loss 135893886915.765
Test Loss of 108951270283.905594, Test MSE of 108951271595.127457
Epoch 17: training loss 133292630407.529
Test Loss of 106340742243.983337, Test MSE of 106340740499.074585
Epoch 18: training loss 130917607273.412
Test Loss of 103889277809.369736, Test MSE of 103889277086.250809
Epoch 19: training loss 127468782802.824
Test Loss of 100250420898.295227, Test MSE of 100250419899.739288
Epoch 20: training loss 121626320715.294
Test Loss of 95947458445.327164, Test MSE of 95947458177.733139
Epoch 21: training loss 118267441242.353
Test Loss of 92871007437.179077, Test MSE of 92871005241.596786
Epoch 22: training loss 114309835143.529
Test Loss of 89885053536.429428, Test MSE of 89885056677.046127
Epoch 23: training loss 109738967823.059
Test Loss of 86105604619.135590, Test MSE of 86105604425.060699
Epoch 24: training loss 106396711393.882
Test Loss of 83436938800.570099, Test MSE of 83436937924.679306
Epoch 25: training loss 103460474639.059
Test Loss of 80960008811.801941, Test MSE of 80960009106.252258
Epoch 26: training loss 99710896037.647
Test Loss of 77871104894.637665, Test MSE of 77871106588.257660
Epoch 27: training loss 95440226499.765
Test Loss of 75229078375.892639, Test MSE of 75229078208.482361
Epoch 28: training loss 91086188664.471
Test Loss of 72051199713.791763, Test MSE of 72051199684.418350
Epoch 29: training loss 87006233584.941
Test Loss of 69139504662.981949, Test MSE of 69139505034.886688
Epoch 30: training loss 83400384421.647
Test Loss of 66244837751.292923, Test MSE of 66244837998.452682
Epoch 31: training loss 78891111318.588
Test Loss of 62721006838.878296, Test MSE of 62721006947.799728
Epoch 32: training loss 75577610661.647
Test Loss of 59744690217.699211, Test MSE of 59744690490.754768
Epoch 33: training loss 72166350998.588
Test Loss of 56286728572.979179, Test MSE of 56286728556.572609
Epoch 34: training loss 68840907098.353
Test Loss of 55280732742.841278, Test MSE of 55280732354.877968
Epoch 35: training loss 66172672090.353
Test Loss of 52860652953.884315, Test MSE of 52860652747.186211
Epoch 36: training loss 64023438426.353
Test Loss of 50820507787.313278, Test MSE of 50820507489.991676
Epoch 37: training loss 59583593419.294
Test Loss of 47847234469.493752, Test MSE of 47847234497.788147
Epoch 38: training loss 57243108901.647
Test Loss of 46059183445.649239, Test MSE of 46059184134.697495
Epoch 39: training loss 54001813443.765
Test Loss of 42948810237.393799, Test MSE of 42948810486.990463
Epoch 40: training loss 50929712549.647
Test Loss of 39600835284.997688, Test MSE of 39600835447.370247
Epoch 41: training loss 48506355591.529
Test Loss of 37986003049.195740, Test MSE of 37986003043.386826
Epoch 42: training loss 46104322733.176
Test Loss of 39730355128.447937, Test MSE of 39730355722.646896
Epoch 43: training loss 43376012912.941
Test Loss of 33742765608.040722, Test MSE of 33742765542.664448
Epoch 44: training loss 41862290529.882
Test Loss of 32257486074.669136, Test MSE of 32257486272.890133
Epoch 45: training loss 39373431785.412
Test Loss of 31777478474.987507, Test MSE of 31777478429.820778
Epoch 46: training loss 37925534464.000
Test Loss of 30960620050.717262, Test MSE of 30960620102.028877
Epoch 47: training loss 35815926053.647
Test Loss of 28966105923.879684, Test MSE of 28966105938.180870
Epoch 48: training loss 33857551405.176
Test Loss of 26804332292.857010, Test MSE of 26804331891.369617
Epoch 49: training loss 31428550415.059
Test Loss of 26924606780.061081, Test MSE of 26924606907.190266
Epoch 50: training loss 30429437451.294
Test Loss of 23050593712.629337, Test MSE of 23050594168.160847
Epoch 51: training loss 29116924679.529
Test Loss of 25013881981.571495, Test MSE of 25013882086.871456
Epoch 52: training loss 27408602601.412
Test Loss of 22173731704.003700, Test MSE of 22173731667.253460
Epoch 53: training loss 26960670851.765
Test Loss of 24503034391.455807, Test MSE of 24503034567.331486
Epoch 54: training loss 25284257317.647
Test Loss of 22310097835.180008, Test MSE of 22310097625.447617
Epoch 55: training loss 24206478614.588
Test Loss of 22568770461.912079, Test MSE of 22568770430.084270
Epoch 56: training loss 23199399574.588
Test Loss of 21516142937.913929, Test MSE of 21516142794.169579
Epoch 57: training loss 22538078768.941
Test Loss of 20633637562.461823, Test MSE of 20633637390.985584
Epoch 58: training loss 21473901575.529
Test Loss of 19010000722.569180, Test MSE of 19010000360.992245
Epoch 59: training loss 20612685029.647
Test Loss of 21597769441.791763, Test MSE of 21597769241.132778
Epoch 60: training loss 19931984764.235
Test Loss of 18605799167.170753, Test MSE of 18605799440.124809
Epoch 61: training loss 19256306488.471
Test Loss of 19233765087.896343, Test MSE of 19233764910.258053
Epoch 62: training loss 18835102603.294
Test Loss of 21371621865.018047, Test MSE of 21371622241.891499
Epoch 63: training loss 18181180777.412
Test Loss of 19466057715.679779, Test MSE of 19466057517.963608
Epoch 64: training loss 17419307584.000
Test Loss of 19844367062.893105, Test MSE of 19844367399.763573
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 122669461192.57664, 'MSE - std': 170569027438.2721, 'R2 - mean': 0.05425747906028802, 'R2 - std': 1.3335755883058156} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005482 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043637097.412
Test Loss of 431612355321.958374, Test MSE of 431612356350.315918
Epoch 2: training loss 424024393968.941
Test Loss of 431591915004.446106, Test MSE of 431591916649.772766
Epoch 3: training loss 423997679856.941
Test Loss of 431564701491.294800, Test MSE of 431564703630.486084
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009318038.588
Test Loss of 431565275226.980103, Test MSE of 431565274828.284363
Epoch 2: training loss 423998596397.176
Test Loss of 431566733365.071716, Test MSE of 431566733097.882568
Epoch 3: training loss 423998144391.529
Test Loss of 431566597016.225830, Test MSE of 431566603553.835266
Epoch 4: training loss 423997841528.471
Test Loss of 431566536362.824646, Test MSE of 431566532513.094910
Epoch 5: training loss 423997598057.412
Test Loss of 431566661475.627930, Test MSE of 431566666865.951355
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 184448902327.2516, 'MSE - std': 196320737276.1255, 'R2 - mean': -0.40118519495841093, 'R2 - std': 1.5008168432721471} 
 

Saving model.....
Results After CV: {'MSE - mean': 184448902327.2516, 'MSE - std': 196320737276.1255, 'R2 - mean': -0.40118519495841093, 'R2 - std': 1.5008168432721471}
Train time: 63.644458961000055
Inference time: 0.0790057433999209
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 7 finished with value: 184448902327.2516 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 9, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005409 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524723049.412
Test Loss of 418112930261.955139, Test MSE of 418112935874.800537
Epoch 2: training loss 427503261093.647
Test Loss of 418094390888.579224, Test MSE of 418094393361.928772
Epoch 3: training loss 427475951736.471
Test Loss of 418070536838.188293, Test MSE of 418070532221.730042
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496276690.824
Test Loss of 418072980665.471191, Test MSE of 418072982603.351746
Epoch 2: training loss 427482901082.353
Test Loss of 418075143717.307434, Test MSE of 418075141742.817688
Epoch 3: training loss 427482504613.647
Test Loss of 418075160855.509583, Test MSE of 418075156739.969116
Epoch 4: training loss 427482120192.000
Test Loss of 418074855877.374023, Test MSE of 418074859187.005981
Epoch 5: training loss 427481899730.824
Test Loss of 418074417409.243591, Test MSE of 418074416236.851929
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418074416236.8519, 'MSE - std': 0.0, 'R2 - mean': -2.2555908399654854, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003617 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916286554.353
Test Loss of 424555591465.393494, Test MSE of 424555596176.206482
Epoch 2: training loss 427894510049.882
Test Loss of 424537916051.453186, Test MSE of 424537918488.815857
Epoch 3: training loss 427867397180.235
Test Loss of 424515071305.489685, Test MSE of 424515075060.103821
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886572242.824
Test Loss of 424517611208.275757, Test MSE of 424517615353.014587
Epoch 2: training loss 427874498680.471
Test Loss of 424517867318.658325, Test MSE of 424517863572.087585
Epoch 3: training loss 427874025953.882
Test Loss of 424517859200.562561, Test MSE of 424517860782.835388
Epoch 4: training loss 427873593223.529
Test Loss of 424517913904.854980, Test MSE of 424517913623.769226
Epoch 5: training loss 427873287710.118
Test Loss of 424517450945.998596, Test MSE of 424517452172.995483
Epoch 6: training loss 427873060261.647
Test Loss of 424517798326.214233, Test MSE of 424517796362.368835
Epoch 7: training loss 427872923888.941
Test Loss of 424517596594.187378, Test MSE of 424517604786.167358
Epoch 8: training loss 427872772577.882
Test Loss of 424517743887.219055, Test MSE of 424517745125.643494
Epoch 9: training loss 427872700536.471
Test Loss of 424517989704.068481, Test MSE of 424517996870.655334
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 421296206553.75366, 'MSE - std': 3221790316.901703, 'R2 - mean': -2.143183744039577, 'R2 - std': 0.11240709592590803} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005526 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926756954.353
Test Loss of 447259152305.358337, Test MSE of 447259153941.132812
Epoch 2: training loss 421905765315.765
Test Loss of 447240336818.187378, Test MSE of 447240331956.753296
Epoch 3: training loss 421878931456.000
Test Loss of 447215124332.191528, Test MSE of 447215131753.624451
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898583702.588
Test Loss of 447223534831.004395, Test MSE of 447223534785.827698
Epoch 2: training loss 421887701955.765
Test Loss of 447224275268.041626, Test MSE of 447224281321.667664
Epoch 3: training loss 421887234048.000
Test Loss of 447224137495.391174, Test MSE of 447224135564.466553
Epoch 4: training loss 421886914439.529
Test Loss of 447223855143.320862, Test MSE of 447223848251.665039
Epoch 5: training loss 421886709037.176
Test Loss of 447223796747.606750, Test MSE of 447223793917.977173
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 429938735675.16144, 'MSE - std': 12502262773.573689, 'R2 - mean': -2.087835590009192, 'R2 - std': 0.1206250656047079} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005411 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430105727216.941
Test Loss of 410761871958.952332, Test MSE of 410761875089.088684
Epoch 2: training loss 430082751307.294
Test Loss of 410741916095.318848, Test MSE of 410741915209.073730
Epoch 3: training loss 430056492574.118
Test Loss of 410718286728.114746, Test MSE of 410718281304.515015
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074895179.294
Test Loss of 410721346222.141602, Test MSE of 410721338204.277283
Epoch 2: training loss 430060835297.882
Test Loss of 410721911906.087952, Test MSE of 410721910753.372498
Epoch 3: training loss 430060468344.471
Test Loss of 410722274173.689941, Test MSE of 410722278473.778503
Epoch 4: training loss 430060244269.176
Test Loss of 410722293795.065247, Test MSE of 410722295232.520020
Epoch 5: training loss 430060076333.176
Test Loss of 410722236917.338257, Test MSE of 410722237781.345703
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 425134611201.7075, 'MSE - std': 13655356704.016642, 'R2 - mean': -2.1633528626981535, 'R2 - std': 0.1673958788687522} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005417 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042962462.118
Test Loss of 431612372411.054138, Test MSE of 431612369261.697449
Epoch 2: training loss 424023410085.647
Test Loss of 431591558529.243896, Test MSE of 431591555133.648926
Epoch 3: training loss 423995877978.353
Test Loss of 431563163578.817200, Test MSE of 431563162297.183044
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010384444.235
Test Loss of 431569868706.176758, Test MSE of 431569872162.521973
Epoch 2: training loss 423997712384.000
Test Loss of 431572275569.606689, Test MSE of 431572268516.031067
Epoch 3: training loss 423996976188.235
Test Loss of 431571411243.476196, Test MSE of 431571404612.906128
Epoch 4: training loss 423996534663.529
Test Loss of 431570148591.296631, Test MSE of 431570144869.614502
Epoch 5: training loss 423996307576.471
Test Loss of 431570212292.531250, Test MSE of 431570205844.150391
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 426421730130.1961, 'MSE - std': 12482055676.668007, 'R2 - mean': -2.175278754209281, 'R2 - std': 0.15161138401562818} 
 

Saving model.....
Results After CV: {'MSE - mean': 426421730130.1961, 'MSE - std': 12482055676.668007, 'R2 - mean': -2.175278754209281, 'R2 - std': 0.15161138401562818}
Train time: 13.909431179799867
Inference time: 0.07068641139994725
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 8 finished with value: 426421730130.1961 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 10, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427525157827.765
Test Loss of 418112080886.288208, Test MSE of 418112082404.921265
Epoch 2: training loss 427504390384.941
Test Loss of 418094446972.417297, Test MSE of 418094449775.052063
Epoch 3: training loss 427477907094.588
Test Loss of 418071587460.767090, Test MSE of 418071584949.469482
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495430505.412
Test Loss of 418076061115.188538, Test MSE of 418076060880.134277
Epoch 2: training loss 427483952429.176
Test Loss of 418077486686.156860, Test MSE of 418077486909.198914
Epoch 3: training loss 427483564995.765
Test Loss of 418077129793.139954, Test MSE of 418077133096.601257
Epoch 4: training loss 427483215028.706
Test Loss of 418076320628.955811, Test MSE of 418076321139.196594
Epoch 5: training loss 427482997338.353
Test Loss of 418075661683.653015, Test MSE of 418075664341.263123
Epoch 6: training loss 427482825306.353
Test Loss of 418073884305.795044, Test MSE of 418073887130.697510
Epoch 7: training loss 427482739049.412
Test Loss of 418074754435.286621, Test MSE of 418074762651.254639
Epoch 8: training loss 427482662791.529
Test Loss of 418073432330.244751, Test MSE of 418073432607.458008
Epoch 9: training loss 416021154273.882
Test Loss of 381443875545.567444, Test MSE of 381443874668.067261
Epoch 10: training loss 343582565436.235
Test Loss of 282394937998.241943, Test MSE of 282394935525.869568
Epoch 11: training loss 242575997048.471
Test Loss of 186240120765.201935, Test MSE of 186240123154.272736
Epoch 12: training loss 172510980939.294
Test Loss of 137474174190.530640, Test MSE of 137474176712.568512
Epoch 13: training loss 145169897923.765
Test Loss of 122400359324.750412, Test MSE of 122400356590.720032
Epoch 14: training loss 138736773059.765
Test Loss of 118074940668.742996, Test MSE of 118074943043.524384
Epoch 15: training loss 136066080978.824
Test Loss of 114980386406.210495, Test MSE of 114980387986.987823
Epoch 16: training loss 132132827075.765
Test Loss of 113083940238.656494, Test MSE of 113083940393.980194
Epoch 17: training loss 129181414400.000
Test Loss of 108857235031.050659, Test MSE of 108857237143.977829
Epoch 18: training loss 125595375766.588
Test Loss of 104959148513.325012, Test MSE of 104959147977.503723
Epoch 19: training loss 120476106450.824
Test Loss of 102032027019.814011, Test MSE of 102032028829.013733
Epoch 20: training loss 117718343604.706
Test Loss of 98627264464.625488, Test MSE of 98627264926.661148
Epoch 21: training loss 114493696692.706
Test Loss of 95791428329.674759, Test MSE of 95791427780.178467
Epoch 22: training loss 111337002345.412
Test Loss of 92564209552.196167, Test MSE of 92564207662.326935
Epoch 23: training loss 106629946563.765
Test Loss of 89558723784.631042, Test MSE of 89558725385.781784
Epoch 24: training loss 103024568470.588
Test Loss of 87093761751.909317, Test MSE of 87093762330.995865
Epoch 25: training loss 98582752451.765
Test Loss of 83577791121.558182, Test MSE of 83577790060.698425
Epoch 26: training loss 94919325108.706
Test Loss of 80618944142.005096, Test MSE of 80618943681.815308
Epoch 27: training loss 91709017374.118
Test Loss of 76387711282.513077, Test MSE of 76387710153.413681
Epoch 28: training loss 88214490081.882
Test Loss of 75194906041.767288, Test MSE of 75194906127.109070
Epoch 29: training loss 85112345088.000
Test Loss of 69907107336.882721, Test MSE of 69907107463.459656
Epoch 30: training loss 80610931245.176
Test Loss of 68457396778.755493, Test MSE of 68457396508.213104
Epoch 31: training loss 78128575352.471
Test Loss of 64495836492.332176, Test MSE of 64495837452.255287
Epoch 32: training loss 74365873152.000
Test Loss of 63070814185.260239, Test MSE of 63070813998.705917
Epoch 33: training loss 71151858891.294
Test Loss of 58676099719.609528, Test MSE of 58676099871.831161
Epoch 34: training loss 68315806855.529
Test Loss of 54552355856.344208, Test MSE of 54552355659.515411
Epoch 35: training loss 65681688131.765
Test Loss of 50666552698.759193, Test MSE of 50666553834.631348
Epoch 36: training loss 62541787324.235
Test Loss of 53471838511.196854, Test MSE of 53471838844.656494
Epoch 37: training loss 60861839337.412
Test Loss of 49459139099.121902, Test MSE of 49459139673.077667
Epoch 38: training loss 57112606840.471
Test Loss of 48704775299.938004, Test MSE of 48704775532.683197
Epoch 39: training loss 54923106063.059
Test Loss of 48085373816.745781, Test MSE of 48085373419.290894
Epoch 40: training loss 52684055273.412
Test Loss of 43516010788.774460, Test MSE of 43516010656.492409
Epoch 41: training loss 50236603911.529
Test Loss of 44025129490.120750, Test MSE of 44025129969.553604
Epoch 42: training loss 47984141180.235
Test Loss of 39030906075.580849, Test MSE of 39030905721.362900
Epoch 43: training loss 45709926341.647
Test Loss of 38633994602.414993, Test MSE of 38633994963.244476
Epoch 44: training loss 43966289054.118
Test Loss of 34573986117.462875, Test MSE of 34573985920.302078
Epoch 45: training loss 41789890368.000
Test Loss of 33496283658.540829, Test MSE of 33496283730.347588
Epoch 46: training loss 39531213880.471
Test Loss of 33327939511.990746, Test MSE of 33327939776.686054
Epoch 47: training loss 38559040922.353
Test Loss of 32449810925.642380, Test MSE of 32449811587.923267
Epoch 48: training loss 36641525662.118
Test Loss of 30977734035.867683, Test MSE of 30977733917.349178
Epoch 49: training loss 34687946691.765
Test Loss of 26244119576.397873, Test MSE of 26244119716.111771
Epoch 50: training loss 33533647164.235
Test Loss of 29037281910.080963, Test MSE of 29037282384.714611
Epoch 51: training loss 31844329298.824
Test Loss of 27499708814.893360, Test MSE of 27499709373.031258
Epoch 52: training loss 30158013707.294
Test Loss of 27324519692.376591, Test MSE of 27324520126.501396
Epoch 53: training loss 29086736444.235
Test Loss of 25400723524.219292, Test MSE of 25400723537.560196
Epoch 54: training loss 27881111521.882
Test Loss of 24574402641.484154, Test MSE of 24574402726.119144
Epoch 55: training loss 26953050838.588
Test Loss of 23837774715.114502, Test MSE of 23837774466.803646
Epoch 56: training loss 25632671570.824
Test Loss of 23469082484.245201, Test MSE of 23469082421.293461
Epoch 57: training loss 24819226435.765
Test Loss of 22283975255.761276, Test MSE of 22283975261.515038
Epoch 58: training loss 23938583627.294
Test Loss of 20763497287.476288, Test MSE of 20763496763.584179
Epoch 59: training loss 22841836122.353
Test Loss of 22379246623.977795, Test MSE of 22379246345.021454
Epoch 60: training loss 22172933372.235
Test Loss of 21559153211.810318, Test MSE of 21559153185.702915
Epoch 61: training loss 21115060948.706
Test Loss of 21750430945.739532, Test MSE of 21750431144.251114
Epoch 62: training loss 20509360440.471
Test Loss of 20176448218.988667, Test MSE of 20176448436.053005
Epoch 63: training loss 19587265001.412
Test Loss of 19989578039.250519, Test MSE of 19989577823.850349
Epoch 64: training loss 19204977479.529
Test Loss of 18853404175.041405, Test MSE of 18853403979.466099
Epoch 65: training loss 18354957737.412
Test Loss of 20356407196.513531, Test MSE of 20356407463.469254
Epoch 66: training loss 17869225946.353
Test Loss of 18661015532.102707, Test MSE of 18661015313.620903
Epoch 67: training loss 17563288903.529
Test Loss of 20184045993.423084, Test MSE of 20184046261.657043
Epoch 68: training loss 16928703149.176
Test Loss of 18554444844.768909, Test MSE of 18554445045.648483
Epoch 69: training loss 16222955049.412
Test Loss of 20504455470.249363, Test MSE of 20504455808.645138
Epoch 70: training loss 15894442428.235
Test Loss of 18598888912.033310, Test MSE of 18598888639.849144
Epoch 71: training loss 15389384022.588
Test Loss of 19153703946.659264, Test MSE of 19153703726.645405
Epoch 72: training loss 15077908694.588
Test Loss of 21170013837.057598, Test MSE of 21170014249.995430
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21170014249.99543, 'MSE - std': 0.0, 'R2 - mean': 0.8351468021062114, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005409 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427915375073.882
Test Loss of 424556497955.767761, Test MSE of 424556499605.376038
Epoch 2: training loss 427892218217.412
Test Loss of 424538931503.196838, Test MSE of 424538925448.109192
Epoch 3: training loss 427864161340.235
Test Loss of 424516596074.888733, Test MSE of 424516597737.758240
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889051045.647
Test Loss of 424521093272.545898, Test MSE of 424521088233.873596
Epoch 2: training loss 427875253549.176
Test Loss of 424521132195.442078, Test MSE of 424521129276.855164
Epoch 3: training loss 427874780581.647
Test Loss of 424521011750.491760, Test MSE of 424521016842.363892
Epoch 4: training loss 427874390016.000
Test Loss of 424520285844.163757, Test MSE of 424520287468.050598
Epoch 5: training loss 427874074383.059
Test Loss of 424520562200.516296, Test MSE of 424520561458.450806
Epoch 6: training loss 427873878497.882
Test Loss of 424520233792.843872, Test MSE of 424520236550.958923
Epoch 7: training loss 427873710320.941
Test Loss of 424520312586.126282, Test MSE of 424520305675.747070
Epoch 8: training loss 427873609848.471
Test Loss of 424518683342.671265, Test MSE of 424518686602.166321
Epoch 9: training loss 416433231751.529
Test Loss of 388810404502.295654, Test MSE of 388810406919.719849
Epoch 10: training loss 343631922838.588
Test Loss of 288984233242.588928, Test MSE of 288984241849.530762
Epoch 11: training loss 241707042936.471
Test Loss of 196958140531.120056, Test MSE of 196958143919.498444
Epoch 12: training loss 169290207051.294
Test Loss of 147590561124.966919, Test MSE of 147590563945.044495
Epoch 13: training loss 143139351220.706
Test Loss of 133310057606.306732, Test MSE of 133310059362.536255
Epoch 14: training loss 134904496790.588
Test Loss of 128464282676.348831, Test MSE of 128464280056.078186
Epoch 15: training loss 133660496865.882
Test Loss of 125763742642.068939, Test MSE of 125763744158.865234
Epoch 16: training loss 130272539196.235
Test Loss of 124342370593.931992, Test MSE of 124342370682.142929
Epoch 17: training loss 127464555791.059
Test Loss of 120000611547.107101, Test MSE of 120000610504.896286
Epoch 18: training loss 122633936263.529
Test Loss of 116811057326.811935, Test MSE of 116811059630.822678
Epoch 19: training loss 117399600640.000
Test Loss of 112886730707.467957, Test MSE of 112886731416.541794
Epoch 20: training loss 113415430384.941
Test Loss of 108950213330.934998, Test MSE of 108950215085.742249
Epoch 21: training loss 109422301334.588
Test Loss of 104949903747.523483, Test MSE of 104949902023.109177
Epoch 22: training loss 105642005805.176
Test Loss of 101750781292.073105, Test MSE of 101750779995.196167
Epoch 23: training loss 101173246765.176
Test Loss of 98656496440.790192, Test MSE of 98656497099.764954
Epoch 24: training loss 99067463439.059
Test Loss of 94749636337.491562, Test MSE of 94749635444.845001
Epoch 25: training loss 95103537784.471
Test Loss of 92851286319.196854, Test MSE of 92851286578.847702
Epoch 26: training loss 91572206080.000
Test Loss of 85816284406.584320, Test MSE of 85816286599.315613
Epoch 27: training loss 87438632282.353
Test Loss of 81429220583.898224, Test MSE of 81429219613.190460
Epoch 28: training loss 83201652103.529
Test Loss of 79835106466.731430, Test MSE of 79835107345.883591
Epoch 29: training loss 78204743936.000
Test Loss of 77905071411.697433, Test MSE of 77905071974.876541
Epoch 30: training loss 75959736515.765
Test Loss of 71899153615.737213, Test MSE of 71899152755.736176
Epoch 31: training loss 73095017667.765
Test Loss of 70242523288.072174, Test MSE of 70242521880.764282
Epoch 32: training loss 69228060928.000
Test Loss of 68181262865.647003, Test MSE of 68181264174.963448
Epoch 33: training loss 66254817355.294
Test Loss of 66502440576.740227, Test MSE of 66502439977.109718
Epoch 34: training loss 63578020758.588
Test Loss of 61854548984.656952, Test MSE of 61854548989.303001
Epoch 35: training loss 60049755934.118
Test Loss of 59088866353.032616, Test MSE of 59088867982.693382
Epoch 36: training loss 57221952015.059
Test Loss of 57331522944.444138, Test MSE of 57331523012.852341
Epoch 37: training loss 54762970985.412
Test Loss of 51767049735.935226, Test MSE of 51767050305.790771
Epoch 38: training loss 51627658051.765
Test Loss of 52379048856.960442, Test MSE of 52379049511.844444
Epoch 39: training loss 50252750652.235
Test Loss of 49246445055.881561, Test MSE of 49246443492.385910
Epoch 40: training loss 47118241679.059
Test Loss of 48185240429.375893, Test MSE of 48185241361.407761
Epoch 41: training loss 45061816839.529
Test Loss of 44830748217.204720, Test MSE of 44830749700.393768
Epoch 42: training loss 42347323489.882
Test Loss of 42705788709.603516, Test MSE of 42705789332.441200
Epoch 43: training loss 40386151439.059
Test Loss of 40638836289.732132, Test MSE of 40638837059.577606
Epoch 44: training loss 38431271107.765
Test Loss of 38977289386.785103, Test MSE of 38977288867.878807
Epoch 45: training loss 36554916352.000
Test Loss of 40052125997.301872, Test MSE of 40052126001.125862
Epoch 46: training loss 34786316950.588
Test Loss of 34676280861.253761, Test MSE of 34676280674.245026
Epoch 47: training loss 32523968301.176
Test Loss of 35978432511.052513, Test MSE of 35978432151.981834
Epoch 48: training loss 31246975751.529
Test Loss of 33794416703.008095, Test MSE of 33794416473.266106
Epoch 49: training loss 30027597989.647
Test Loss of 34120454495.992599, Test MSE of 34120454793.314301
Epoch 50: training loss 28156382215.529
Test Loss of 30402753134.501041, Test MSE of 30402753382.200989
Epoch 51: training loss 26719805537.882
Test Loss of 29891373818.255840, Test MSE of 29891374293.207680
Epoch 52: training loss 25816857411.765
Test Loss of 29139222045.490631, Test MSE of 29139222132.703777
Epoch 53: training loss 24619957680.941
Test Loss of 28065799114.703678, Test MSE of 28065798572.782890
Epoch 54: training loss 23188850718.118
Test Loss of 27488150063.256073, Test MSE of 27488150353.948467
Epoch 55: training loss 22051629357.176
Test Loss of 28521794597.188988, Test MSE of 28521794563.973484
Epoch 56: training loss 21513555843.765
Test Loss of 30357211012.352531, Test MSE of 30357210718.619251
Epoch 57: training loss 20556076611.765
Test Loss of 27256515953.994911, Test MSE of 27256515285.965782
Epoch 58: training loss 19423283934.118
Test Loss of 25024680303.152439, Test MSE of 25024680421.302929
Epoch 59: training loss 18737213854.118
Test Loss of 25564991970.509369, Test MSE of 25564991503.432785
Epoch 60: training loss 18400541451.294
Test Loss of 28657344574.060608, Test MSE of 28657344671.216419
Epoch 61: training loss 17362999472.941
Test Loss of 24349342734.449226, Test MSE of 24349342841.738335
Epoch 62: training loss 16852248214.588
Test Loss of 26096173354.696278, Test MSE of 26096173156.300842
Epoch 63: training loss 16232192248.471
Test Loss of 25314251155.157066, Test MSE of 25314251170.521751
Epoch 64: training loss 15821158614.588
Test Loss of 25121405123.419846, Test MSE of 25121404694.563026
Epoch 65: training loss 15301680990.118
Test Loss of 24711648915.453159, Test MSE of 24711648587.764271
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22940831418.879852, 'MSE - std': 1770817168.8844204, 'R2 - mean': 0.829361018977103, 'R2 - std': 0.005785783129108324} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003590 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926415902.118
Test Loss of 447257176735.296814, Test MSE of 447257170928.315979
Epoch 2: training loss 421904774083.765
Test Loss of 447237746304.266479, Test MSE of 447237744190.871277
Epoch 3: training loss 421876900803.765
Test Loss of 447212561424.344177, Test MSE of 447212561336.032471
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896776041.412
Test Loss of 447219158260.215576, Test MSE of 447219156525.298889
Epoch 2: training loss 421885478189.176
Test Loss of 447220862514.572266, Test MSE of 447220859251.773132
Epoch 3: training loss 421885017027.765
Test Loss of 447221474926.737915, Test MSE of 447221477184.967834
Epoch 4: training loss 421884650315.294
Test Loss of 447221662711.709473, Test MSE of 447221670247.912781
Epoch 5: training loss 421884412988.235
Test Loss of 447221387242.207703, Test MSE of 447221391912.471802
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 164367684916.74384, 'MSE - std': 200013000351.41794, 'R2 - mean': -0.10613375133402798, 'R2 - std': 1.3229978259303992} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005458 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106331256.471
Test Loss of 410763253218.857910, Test MSE of 410763254549.184143
Epoch 2: training loss 430083601106.824
Test Loss of 410743902933.945374, Test MSE of 410743907756.712585
Epoch 3: training loss 430056934460.235
Test Loss of 410719613339.305847, Test MSE of 410719611479.718628
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074323184.941
Test Loss of 410722513815.278137, Test MSE of 410722518515.554810
Epoch 2: training loss 430061929170.824
Test Loss of 410723445542.500671, Test MSE of 410723447721.991577
Epoch 3: training loss 430061542219.294
Test Loss of 410723413001.950928, Test MSE of 410723417318.582703
Epoch 4: training loss 430061321999.059
Test Loss of 410723302931.191101, Test MSE of 410723302581.367188
Epoch 5: training loss 430061155267.765
Test Loss of 410723343902.563599, Test MSE of 410723344003.209045
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 225956599688.36014, 'MSE - std': 203429308361.45142, 'R2 - mean': -0.6770787662481699, 'R2 - std': 1.5134982873225695} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005615 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043193404.235
Test Loss of 431612256411.424316, Test MSE of 431612253626.452271
Epoch 2: training loss 424023431288.471
Test Loss of 431591488762.669128, Test MSE of 431591487332.590027
Epoch 3: training loss 423996101210.353
Test Loss of 431563591700.849609, Test MSE of 431563593714.712036
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009088060.235
Test Loss of 431565069854.563599, Test MSE of 431565073227.556396
Epoch 2: training loss 423997876224.000
Test Loss of 431566681081.839905, Test MSE of 431566688117.769531
Epoch 3: training loss 423997302302.118
Test Loss of 431566035396.057373, Test MSE of 431566030620.267761
Epoch 4: training loss 423996907038.118
Test Loss of 431565433683.990723, Test MSE of 431565431958.852722
Epoch 5: training loss 423996636099.765
Test Loss of 431565578448.496094, Test MSE of 431565574316.500244
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 267078394613.9882, 'MSE - std': 199676725914.55524, 'R2 - mean': -0.9862525593652045, 'R2 - std': 1.4882524617766235} 
 

Saving model.....
Results After CV: {'MSE - mean': 267078394613.9882, 'MSE - std': 199676725914.55524, 'R2 - mean': -0.9862525593652045, 'R2 - std': 1.4882524617766235}
Train time: 50.11693578219983
Inference time: 0.09993308079992858
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 9 finished with value: 267078394613.9882 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 8, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525838848.000
Test Loss of 418110177734.795288, Test MSE of 418110176008.353088
Epoch 2: training loss 427506311288.471
Test Loss of 418091693346.168884, Test MSE of 418091694279.051758
Epoch 3: training loss 427480765861.647
Test Loss of 418067693170.054138, Test MSE of 418067693398.852722
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491138138.353
Test Loss of 418069470335.674316, Test MSE of 418069471084.652527
Epoch 2: training loss 427479357560.471
Test Loss of 418069901807.537354, Test MSE of 418069897516.905212
Epoch 3: training loss 423698616560.941
Test Loss of 406232122734.915588, Test MSE of 406232116314.973633
Epoch 4: training loss 397204032813.176
Test Loss of 365008933744.218384, Test MSE of 365008938475.152893
Epoch 5: training loss 331399101138.824
Test Loss of 278545657434.366882, Test MSE of 278545657731.276001
Epoch 6: training loss 248663474296.471
Test Loss of 201325411738.026367, Test MSE of 201325416424.685089
Epoch 7: training loss 180126026330.353
Test Loss of 141636955207.772369, Test MSE of 141636956887.474182
Epoch 8: training loss 146382168109.176
Test Loss of 122326887419.973160, Test MSE of 122326885237.799561
Epoch 9: training loss 137692001430.588
Test Loss of 116905456990.808228, Test MSE of 116905457585.229218
Epoch 10: training loss 133459779674.353
Test Loss of 113570499689.882019, Test MSE of 113570497109.877731
Epoch 11: training loss 131012984711.529
Test Loss of 110299377297.321304, Test MSE of 110299375787.928604
Epoch 12: training loss 127562535476.706
Test Loss of 107679137185.606293, Test MSE of 107679137514.602005
Epoch 13: training loss 124167161223.529
Test Loss of 104397516706.198471, Test MSE of 104397518412.404694
Epoch 14: training loss 119588393652.706
Test Loss of 101668730991.330093, Test MSE of 101668730636.928406
Epoch 15: training loss 117563251922.824
Test Loss of 98979358392.642151, Test MSE of 98979358372.467987
Epoch 16: training loss 113137827448.471
Test Loss of 94992242492.343277, Test MSE of 94992242739.443176
Epoch 17: training loss 109659296015.059
Test Loss of 91399925254.987747, Test MSE of 91399923947.265533
Epoch 18: training loss 104312285018.353
Test Loss of 89439429576.334946, Test MSE of 89439430621.938950
Epoch 19: training loss 102013476336.941
Test Loss of 84529417511.380066, Test MSE of 84529418431.610458
Epoch 20: training loss 97134052653.176
Test Loss of 82369506008.146194, Test MSE of 82369505573.043198
Epoch 21: training loss 93609132423.529
Test Loss of 78977924320.318298, Test MSE of 78977926012.001984
Epoch 22: training loss 90297646305.882
Test Loss of 75026913633.887573, Test MSE of 75026913554.903336
Epoch 23: training loss 86185822192.941
Test Loss of 71377293466.440903, Test MSE of 71377293798.107040
Epoch 24: training loss 83666835922.824
Test Loss of 71565878225.809860, Test MSE of 71565879319.948883
Epoch 25: training loss 79913608741.647
Test Loss of 67348553076.126762, Test MSE of 67348553384.303490
Epoch 26: training loss 74717145984.000
Test Loss of 64243211213.546150, Test MSE of 64243210453.295319
Epoch 27: training loss 73232159382.588
Test Loss of 61083887665.743233, Test MSE of 61083887986.749817
Epoch 28: training loss 69708026021.647
Test Loss of 57115869157.470276, Test MSE of 57115869630.183334
Epoch 29: training loss 66918689554.824
Test Loss of 57333144301.938469, Test MSE of 57333144823.220520
Epoch 30: training loss 62882048079.059
Test Loss of 54052394046.771225, Test MSE of 54052394079.841240
Epoch 31: training loss 61407226360.471
Test Loss of 49452869280.007401, Test MSE of 49452869669.252373
Epoch 32: training loss 58637457679.059
Test Loss of 50446670980.411751, Test MSE of 50446670160.527786
Epoch 33: training loss 55745753675.294
Test Loss of 50519961935.411522, Test MSE of 50519961618.740082
Epoch 34: training loss 53518255879.529
Test Loss of 45631099386.907242, Test MSE of 45631099667.748222
Epoch 35: training loss 51086255932.235
Test Loss of 44047982915.094147, Test MSE of 44047983131.774223
Epoch 36: training loss 48794092235.294
Test Loss of 44396975433.963448, Test MSE of 44396976133.048027
Epoch 37: training loss 46438591879.529
Test Loss of 41526900422.854500, Test MSE of 41526901213.932220
Epoch 38: training loss 44213641306.353
Test Loss of 41755008998.417763, Test MSE of 41755009825.932472
Epoch 39: training loss 42115722006.588
Test Loss of 35917244448.688408, Test MSE of 35917244405.123352
Epoch 40: training loss 40135069455.059
Test Loss of 34195150782.149433, Test MSE of 34195151061.147778
Epoch 41: training loss 38303883695.059
Test Loss of 38242845462.206802, Test MSE of 38242845873.179939
Epoch 42: training loss 36920272737.882
Test Loss of 35580875129.101089, Test MSE of 35580875137.694595
Epoch 43: training loss 35129712444.235
Test Loss of 31886919387.699284, Test MSE of 31886919152.047459
Epoch 44: training loss 33868285963.294
Test Loss of 31502556104.808697, Test MSE of 31502555947.267414
Epoch 45: training loss 32153909677.176
Test Loss of 28590945789.039093, Test MSE of 28590946121.820114
Epoch 46: training loss 30829103951.059
Test Loss of 29291560048.514458, Test MSE of 29291559044.530083
Epoch 47: training loss 29181759958.588
Test Loss of 27167038107.506824, Test MSE of 27167038652.539707
Epoch 48: training loss 27830822475.294
Test Loss of 27539223987.134861, Test MSE of 27539224009.657364
Epoch 49: training loss 26693846761.412
Test Loss of 24974606907.336571, Test MSE of 24974606583.079357
Epoch 50: training loss 25912069266.824
Test Loss of 27593112019.112652, Test MSE of 27593111922.269764
Epoch 51: training loss 24912385739.294
Test Loss of 26860742435.708534, Test MSE of 26860742611.993565
Epoch 52: training loss 23641136000.000
Test Loss of 23809818463.400417, Test MSE of 23809819001.851467
Epoch 53: training loss 22545263484.235
Test Loss of 22089938926.471432, Test MSE of 22089939103.250813
Epoch 54: training loss 22034117688.471
Test Loss of 22736188374.547306, Test MSE of 22736188737.245823
Epoch 55: training loss 21088988242.824
Test Loss of 22635746615.487392, Test MSE of 22635746919.612942
Epoch 56: training loss 20438634829.176
Test Loss of 22365348465.343510, Test MSE of 22365348389.777946
Epoch 57: training loss 19580829180.235
Test Loss of 20327426458.263245, Test MSE of 20327426442.578857
Epoch 58: training loss 19219125116.235
Test Loss of 23881591854.663891, Test MSE of 23881592086.913700
Epoch 59: training loss 18499361438.118
Test Loss of 21172188763.788109, Test MSE of 21172188630.555073
Epoch 60: training loss 18080682672.941
Test Loss of 22827745969.299099, Test MSE of 22827746051.024071
Epoch 61: training loss 17553152233.412
Test Loss of 20150176005.033543, Test MSE of 20150175992.883411
Epoch 62: training loss 16775433703.529
Test Loss of 22000697623.509602, Test MSE of 22000697775.118515
Epoch 63: training loss 15950894844.235
Test Loss of 21126213352.964146, Test MSE of 21126213770.044754
Epoch 64: training loss 15851280705.882
Test Loss of 19466732711.942631, Test MSE of 19466732794.254627
Epoch 65: training loss 15365365884.235
Test Loss of 21927693267.704834, Test MSE of 21927693385.632236
Epoch 66: training loss 15039732193.882
Test Loss of 22025632068.515385, Test MSE of 22025632171.089962
Epoch 67: training loss 14477841340.235
Test Loss of 17621967488.503353, Test MSE of 17621967390.863564
Epoch 68: training loss 14084330548.706
Test Loss of 19645291954.187370, Test MSE of 19645292079.995651
Epoch 69: training loss 13738925982.118
Test Loss of 20343983103.289383, Test MSE of 20343983348.615646
Epoch 70: training loss 13430801167.059
Test Loss of 19079141439.008095, Test MSE of 19079141032.295082
Epoch 71: training loss 13445774987.294
Test Loss of 19319364983.442978, Test MSE of 19319365195.281025
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19319365195.281025, 'MSE - std': 0.0, 'R2 - mean': 0.8495580070891675, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005285 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918597180.235
Test Loss of 424555332717.198242, Test MSE of 424555334638.361816
Epoch 2: training loss 427898304150.588
Test Loss of 424539189826.916504, Test MSE of 424539195877.124207
Epoch 3: training loss 427871415356.235
Test Loss of 424517200315.899170, Test MSE of 424517199924.862915
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887944884.706
Test Loss of 424524720719.707581, Test MSE of 424524726993.668518
Epoch 2: training loss 427876156837.647
Test Loss of 424525644818.239197, Test MSE of 424525641694.098633
Epoch 3: training loss 424270264440.471
Test Loss of 413045618408.964172, Test MSE of 413045615990.981506
Epoch 4: training loss 398076469488.941
Test Loss of 372558656392.853088, Test MSE of 372558648514.306274
Epoch 5: training loss 331886532246.588
Test Loss of 287975300557.190857, Test MSE of 287975301411.178772
Epoch 6: training loss 248743458153.412
Test Loss of 211171988533.296326, Test MSE of 211171989786.902283
Epoch 7: training loss 178848411587.765
Test Loss of 150996080237.790436, Test MSE of 150996077019.135956
Epoch 8: training loss 143729709086.118
Test Loss of 133453487199.459641, Test MSE of 133453488334.617798
Epoch 9: training loss 135179391939.765
Test Loss of 128310443535.515152, Test MSE of 128310444601.714127
Epoch 10: training loss 132714884397.176
Test Loss of 125069551358.756424, Test MSE of 125069557121.815491
Epoch 11: training loss 127437893662.118
Test Loss of 121364827977.134399, Test MSE of 121364825937.147675
Epoch 12: training loss 125166131772.235
Test Loss of 118106703294.504745, Test MSE of 118106705542.500519
Epoch 13: training loss 121441586868.706
Test Loss of 116072439266.035629, Test MSE of 116072440996.527176
Epoch 14: training loss 117409610752.000
Test Loss of 112498789425.032623, Test MSE of 112498788297.059235
Epoch 15: training loss 114339009686.588
Test Loss of 108476267332.160080, Test MSE of 108476268957.717331
Epoch 16: training loss 109074455913.412
Test Loss of 104090220518.891510, Test MSE of 104090219800.340408
Epoch 17: training loss 106498487838.118
Test Loss of 100403817815.702057, Test MSE of 100403817743.462128
Epoch 18: training loss 102576172784.941
Test Loss of 96702087339.495728, Test MSE of 96702089656.457916
Epoch 19: training loss 97999236818.824
Test Loss of 93081806813.416611, Test MSE of 93081805912.136658
Epoch 20: training loss 94727725537.882
Test Loss of 89421597515.739990, Test MSE of 89421600276.828476
Epoch 21: training loss 90791672410.353
Test Loss of 88181354183.328247, Test MSE of 88181353947.640350
Epoch 22: training loss 87403698040.471
Test Loss of 83348188434.535278, Test MSE of 83348188611.748932
Epoch 23: training loss 83177258721.882
Test Loss of 77150224936.386765, Test MSE of 77150223127.283524
Epoch 24: training loss 79063117101.176
Test Loss of 76279152458.081894, Test MSE of 76279153584.711868
Epoch 25: training loss 76422380032.000
Test Loss of 73498303483.025681, Test MSE of 73498303336.256226
Epoch 26: training loss 72881018819.765
Test Loss of 70763148645.440674, Test MSE of 70763148971.871826
Epoch 27: training loss 68979098277.647
Test Loss of 63923289372.010178, Test MSE of 63923289470.496414
Epoch 28: training loss 65741825927.529
Test Loss of 63393971478.325233, Test MSE of 63393971731.977760
Epoch 29: training loss 63380194906.353
Test Loss of 63624846660.752258, Test MSE of 63624847445.515411
Epoch 30: training loss 60107983299.765
Test Loss of 56254959877.507286, Test MSE of 56254959075.952751
Epoch 31: training loss 57116165737.412
Test Loss of 54599341706.215126, Test MSE of 54599341199.961250
Epoch 32: training loss 54241044434.824
Test Loss of 55275106997.562805, Test MSE of 55275107857.781601
Epoch 33: training loss 51192156310.588
Test Loss of 52361208700.061996, Test MSE of 52361208427.785370
Epoch 34: training loss 49547281520.941
Test Loss of 47688909125.936615, Test MSE of 47688909813.471039
Epoch 35: training loss 47049204412.235
Test Loss of 45619153458.335419, Test MSE of 45619153342.781624
Epoch 36: training loss 45210693278.118
Test Loss of 44079085074.357620, Test MSE of 44079083712.471809
Epoch 37: training loss 42637894927.059
Test Loss of 42659226710.932220, Test MSE of 42659227117.161690
Epoch 38: training loss 40275389861.647
Test Loss of 42032184821.932915, Test MSE of 42032184106.389702
Epoch 39: training loss 38355115535.059
Test Loss of 39551439873.184364, Test MSE of 39551439017.745766
Epoch 40: training loss 37173342795.294
Test Loss of 35995730317.472122, Test MSE of 35995729885.036118
Epoch 41: training loss 35542514597.647
Test Loss of 39109256639.452232, Test MSE of 39109257228.971741
Epoch 42: training loss 32802938684.235
Test Loss of 38352903085.805229, Test MSE of 38352903407.646301
Epoch 43: training loss 31321182614.588
Test Loss of 36121003373.494331, Test MSE of 36121003830.066322
Epoch 44: training loss 30045154055.529
Test Loss of 32716375999.570667, Test MSE of 32716376382.815495
Epoch 45: training loss 28628718848.000
Test Loss of 36782135516.291466, Test MSE of 36782135750.447830
Epoch 46: training loss 27356409517.176
Test Loss of 32043008954.122601, Test MSE of 32043008324.103119
Epoch 47: training loss 25836576824.471
Test Loss of 31763053023.903770, Test MSE of 31763052303.642681
Epoch 48: training loss 24671702038.588
Test Loss of 29146448361.852417, Test MSE of 29146447696.094646
Epoch 49: training loss 23635955147.294
Test Loss of 30208217442.361324, Test MSE of 30208218109.119129
Epoch 50: training loss 22442604928.000
Test Loss of 29662786591.030304, Test MSE of 29662787643.726101
Epoch 51: training loss 21388287126.588
Test Loss of 29690427136.177654, Test MSE of 29690427551.921272
Epoch 52: training loss 20656020860.235
Test Loss of 28529363578.344669, Test MSE of 28529362985.645672
Epoch 53: training loss 19416808786.824
Test Loss of 27389463823.219059, Test MSE of 27389463303.569706
Epoch 54: training loss 18980763087.059
Test Loss of 27561927545.693268, Test MSE of 27561927517.808613
Epoch 55: training loss 18328474842.353
Test Loss of 27399555904.370113, Test MSE of 27399555710.327522
Epoch 56: training loss 17707132032.000
Test Loss of 28601831081.482304, Test MSE of 28601829997.442070
Epoch 57: training loss 17057983442.824
Test Loss of 26378112477.298172, Test MSE of 26378112236.627060
Epoch 58: training loss 16409416289.882
Test Loss of 24923112878.397408, Test MSE of 24923112573.539890
Epoch 59: training loss 15915782949.647
Test Loss of 28923456326.055054, Test MSE of 28923456149.018314
Epoch 60: training loss 15247096624.941
Test Loss of 27254492921.071480, Test MSE of 27254493296.593651
Epoch 61: training loss 14659923821.176
Test Loss of 26717874595.264400, Test MSE of 26717875423.774101
Epoch 62: training loss 14452476231.529
Test Loss of 26364120171.540134, Test MSE of 26364119890.048500
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22841742542.664764, 'MSE - std': 3522377347.3837376, 'R2 - mean': 0.8306678474807176, 'R2 - std': 0.018890159608449864} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005374 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927693492.706
Test Loss of 447258684427.132996, Test MSE of 447258684550.391052
Epoch 2: training loss 421907995949.176
Test Loss of 447240535093.770081, Test MSE of 447240539080.383789
Epoch 3: training loss 421881341952.000
Test Loss of 447216319337.112183, Test MSE of 447216320083.314148
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897255514.353
Test Loss of 447223837815.620605, Test MSE of 447223825703.861389
Epoch 2: training loss 421887823751.529
Test Loss of 447224596414.149414, Test MSE of 447224598487.567444
Epoch 3: training loss 418013646968.471
Test Loss of 434810306745.708069, Test MSE of 434810310166.755005
Epoch 4: training loss 391302265675.294
Test Loss of 393095144498.453857, Test MSE of 393095147066.074463
Epoch 5: training loss 325505561057.882
Test Loss of 305035655135.311584, Test MSE of 305035655945.791016
Epoch 6: training loss 241952003855.059
Test Loss of 226474235581.379608, Test MSE of 226474240655.643066
Epoch 7: training loss 173392852329.412
Test Loss of 165192326169.345367, Test MSE of 165192324975.347382
Epoch 8: training loss 141960552929.882
Test Loss of 145115264506.196625, Test MSE of 145115268031.121277
Epoch 9: training loss 133682670592.000
Test Loss of 139046302240.806854, Test MSE of 139046301359.562897
Epoch 10: training loss 129595229786.353
Test Loss of 135444799084.369186, Test MSE of 135444801666.886078
Epoch 11: training loss 126938226115.765
Test Loss of 132180258636.924362, Test MSE of 132180261320.726181
Epoch 12: training loss 122929288161.882
Test Loss of 128684631336.801300, Test MSE of 128684634870.585770
Epoch 13: training loss 119310946544.941
Test Loss of 125092658267.669678, Test MSE of 125092658167.905151
Epoch 14: training loss 116369308521.412
Test Loss of 122595964648.964142, Test MSE of 122595966308.628616
Epoch 15: training loss 111771357967.059
Test Loss of 117122456210.979416, Test MSE of 117122454794.435883
Epoch 16: training loss 108993421432.471
Test Loss of 115990858636.169327, Test MSE of 115990858869.340729
Epoch 17: training loss 105592737912.471
Test Loss of 110980189348.626419, Test MSE of 110980194749.816544
Epoch 18: training loss 101009659030.588
Test Loss of 107318387739.714081, Test MSE of 107318388461.839661
Epoch 19: training loss 97820282925.176
Test Loss of 102216750834.439041, Test MSE of 102216751028.060699
Epoch 20: training loss 94732563395.765
Test Loss of 97413768521.015961, Test MSE of 97413768951.916367
Epoch 21: training loss 89272313298.824
Test Loss of 94469693665.502655, Test MSE of 94469693350.286484
Epoch 22: training loss 85812869285.647
Test Loss of 91857239523.930603, Test MSE of 91857240997.177078
Epoch 23: training loss 83090596788.706
Test Loss of 86609330030.086517, Test MSE of 86609330581.707214
Epoch 24: training loss 79506959796.706
Test Loss of 85013863528.934540, Test MSE of 85013863767.842789
Epoch 25: training loss 75659699727.059
Test Loss of 81436899031.909317, Test MSE of 81436899083.940765
Epoch 26: training loss 72485184828.235
Test Loss of 74964091943.083969, Test MSE of 74964091311.716431
Epoch 27: training loss 69369211256.471
Test Loss of 74740919695.603973, Test MSE of 74740919094.050430
Epoch 28: training loss 66935212348.235
Test Loss of 70191953117.949570, Test MSE of 70191954626.400635
Epoch 29: training loss 63860076393.412
Test Loss of 64598271179.947258, Test MSE of 64598270330.808029
Epoch 30: training loss 61115736824.471
Test Loss of 65339440111.655792, Test MSE of 65339441392.804970
Epoch 31: training loss 57691173428.706
Test Loss of 61473268134.580612, Test MSE of 61473267798.422981
Epoch 32: training loss 55305770706.824
Test Loss of 58128897350.173492, Test MSE of 58128897557.230972
Epoch 33: training loss 53528392041.412
Test Loss of 57619578656.392319, Test MSE of 57619578498.345146
Epoch 34: training loss 50767607823.059
Test Loss of 53872202858.355774, Test MSE of 53872203336.256432
Epoch 35: training loss 48571856768.000
Test Loss of 49180009631.415222, Test MSE of 49180009244.167870
Epoch 36: training loss 46130785031.529
Test Loss of 51342038551.568817, Test MSE of 51342038557.685356
Epoch 37: training loss 44324699256.471
Test Loss of 44773901314.131851, Test MSE of 44773901085.747971
Epoch 38: training loss 41651235064.471
Test Loss of 46304474981.085358, Test MSE of 46304474949.929085
Epoch 39: training loss 40194102189.176
Test Loss of 44420824464.551468, Test MSE of 44420825169.512634
Epoch 40: training loss 38198450620.235
Test Loss of 45622961060.330330, Test MSE of 45622961624.947891
Epoch 41: training loss 36408282721.882
Test Loss of 43054352635.558640, Test MSE of 43054352159.202438
Epoch 42: training loss 34671575130.353
Test Loss of 41745748534.599121, Test MSE of 41745748787.719795
Epoch 43: training loss 33028388871.529
Test Loss of 35050849035.784409, Test MSE of 35050848492.961746
Epoch 44: training loss 31555483745.882
Test Loss of 35590485048.138794, Test MSE of 35590485564.081535
Epoch 45: training loss 29974674484.706
Test Loss of 34974518123.007172, Test MSE of 34974518242.152435
Epoch 46: training loss 28611084288.000
Test Loss of 35906838922.155907, Test MSE of 35906838580.028564
Epoch 47: training loss 27509794477.176
Test Loss of 31906166066.276196, Test MSE of 31906165513.826973
Epoch 48: training loss 26240049618.824
Test Loss of 26683617003.806614, Test MSE of 26683616814.562840
Epoch 49: training loss 25062185091.765
Test Loss of 31969657932.036087, Test MSE of 31969657946.957348
Epoch 50: training loss 24045803975.529
Test Loss of 28944584848.965996, Test MSE of 28944585749.568718
Epoch 51: training loss 23184330582.588
Test Loss of 32598063919.315289, Test MSE of 32598063934.994297
Epoch 52: training loss 22202572566.588
Test Loss of 28184833900.191532, Test MSE of 28184834128.539715
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24622773071.28975, 'MSE - std': 3823031206.5205245, 'R2 - mean': 0.8245703714008369, 'R2 - std': 0.017670611641307385} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005490 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110617118.118
Test Loss of 410765033259.713074, Test MSE of 410765025558.352051
Epoch 2: training loss 430091075102.118
Test Loss of 410746871114.276733, Test MSE of 410746877449.617188
Epoch 3: training loss 430064557598.118
Test Loss of 410723174512.777405, Test MSE of 410723172966.237488
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080232749.176
Test Loss of 410730341546.113831, Test MSE of 410730343984.811768
Epoch 2: training loss 430066658846.118
Test Loss of 410730177084.416504, Test MSE of 410730175407.128296
Epoch 3: training loss 426345262622.118
Test Loss of 398878965107.975952, Test MSE of 398878956393.242249
Epoch 4: training loss 399868971008.000
Test Loss of 357391899379.324402, Test MSE of 357391906286.576355
Epoch 5: training loss 334235467776.000
Test Loss of 272135492491.905609, Test MSE of 272135494586.396820
Epoch 6: training loss 251664778601.412
Test Loss of 195117019554.887543, Test MSE of 195117025123.167328
Epoch 7: training loss 182815268502.588
Test Loss of 134874500299.283661, Test MSE of 134874501561.634811
Epoch 8: training loss 149824093455.059
Test Loss of 116590767106.369278, Test MSE of 116590767641.568161
Epoch 9: training loss 140020851651.765
Test Loss of 111066782087.403976, Test MSE of 111066780834.688507
Epoch 10: training loss 135928086558.118
Test Loss of 108413325169.843597, Test MSE of 108413325412.747101
Epoch 11: training loss 133698826330.353
Test Loss of 105432889261.549286, Test MSE of 105432890313.339951
Epoch 12: training loss 129721300781.176
Test Loss of 102388066488.803329, Test MSE of 102388064167.753342
Epoch 13: training loss 126594728688.941
Test Loss of 99955886317.875061, Test MSE of 99955885339.127197
Epoch 14: training loss 121766476649.412
Test Loss of 97514753238.656174, Test MSE of 97514753377.273407
Epoch 15: training loss 118426238012.235
Test Loss of 93718080821.427124, Test MSE of 93718082713.632599
Epoch 16: training loss 114403742358.588
Test Loss of 90924527291.883392, Test MSE of 90924527136.454666
Epoch 17: training loss 110486756291.765
Test Loss of 86707843161.084686, Test MSE of 86707843265.503174
Epoch 18: training loss 106595559574.588
Test Loss of 84391527753.329010, Test MSE of 84391527797.415787
Epoch 19: training loss 102671235041.882
Test Loss of 81237926331.528000, Test MSE of 81237925160.221298
Epoch 20: training loss 99650217351.529
Test Loss of 78785874650.683945, Test MSE of 78785873750.635040
Epoch 21: training loss 95021188848.941
Test Loss of 75432964295.018967, Test MSE of 75432962212.428772
Epoch 22: training loss 92308935951.059
Test Loss of 72468131167.126328, Test MSE of 72468130585.745697
Epoch 23: training loss 88037285722.353
Test Loss of 69763461273.528915, Test MSE of 69763460495.588882
Epoch 24: training loss 84621379448.471
Test Loss of 68032765394.746880, Test MSE of 68032766518.924049
Epoch 25: training loss 80985860879.059
Test Loss of 64043581599.215179, Test MSE of 64043581664.145500
Epoch 26: training loss 77788859105.882
Test Loss of 61835867217.029152, Test MSE of 61835867566.474709
Epoch 27: training loss 73563697784.471
Test Loss of 58011913324.512726, Test MSE of 58011914231.474602
Epoch 28: training loss 71871047634.824
Test Loss of 56777871589.345673, Test MSE of 56777872590.002106
Epoch 29: training loss 67832980073.412
Test Loss of 54551940964.575661, Test MSE of 54551941734.772850
Epoch 30: training loss 64888980449.882
Test Loss of 53561208957.097641, Test MSE of 53561209209.376381
Epoch 31: training loss 62587247977.412
Test Loss of 50327063508.879219, Test MSE of 50327062955.731041
Epoch 32: training loss 59509758674.824
Test Loss of 48310119642.447014, Test MSE of 48310119729.506508
Epoch 33: training loss 56827997989.647
Test Loss of 44908261376.000000, Test MSE of 44908260919.449982
Epoch 34: training loss 54118314112.000
Test Loss of 45079037130.335953, Test MSE of 45079037432.524422
Epoch 35: training loss 52155408903.529
Test Loss of 41454980884.968071, Test MSE of 41454981376.624878
Epoch 36: training loss 49611885447.529
Test Loss of 39250467055.770477, Test MSE of 39250466270.429359
Epoch 37: training loss 46666818883.765
Test Loss of 39635803750.589539, Test MSE of 39635804444.455963
Epoch 38: training loss 45004543442.824
Test Loss of 37340251643.498383, Test MSE of 37340251604.664055
Epoch 39: training loss 43243939840.000
Test Loss of 35145560937.788063, Test MSE of 35145560912.771561
Epoch 40: training loss 41531833434.353
Test Loss of 32839556056.670059, Test MSE of 32839556318.433640
Epoch 41: training loss 39082083508.706
Test Loss of 33341371291.068951, Test MSE of 33341371355.256458
Epoch 42: training loss 37409936210.824
Test Loss of 31579939269.005089, Test MSE of 31579938593.172634
Epoch 43: training loss 35830964728.471
Test Loss of 31570314891.550209, Test MSE of 31570315079.622677
Epoch 44: training loss 34033452491.294
Test Loss of 28635701934.141602, Test MSE of 28635701991.124077
Epoch 45: training loss 32545331885.176
Test Loss of 30115239269.760296, Test MSE of 30115239598.789299
Epoch 46: training loss 31017266740.706
Test Loss of 25112308997.093937, Test MSE of 25112309217.616764
Epoch 47: training loss 30173324867.765
Test Loss of 27197522513.266079, Test MSE of 27197522988.529747
Epoch 48: training loss 28713331056.941
Test Loss of 25160554843.809349, Test MSE of 25160555104.302143
Epoch 49: training loss 27381183574.588
Test Loss of 26191130079.067097, Test MSE of 26191129790.796593
Epoch 50: training loss 26168930552.471
Test Loss of 23341014843.824154, Test MSE of 23341015101.495895
Epoch 51: training loss 25208314959.059
Test Loss of 24427363410.924572, Test MSE of 24427363351.944435
Epoch 52: training loss 24181596265.412
Test Loss of 23847598198.463673, Test MSE of 23847598167.175346
Epoch 53: training loss 23048764269.176
Test Loss of 22315984098.028690, Test MSE of 22315984235.404266
Epoch 54: training loss 22191768195.765
Test Loss of 23064976713.329014, Test MSE of 23064976655.759892
Epoch 55: training loss 21626167104.000
Test Loss of 19219374187.091160, Test MSE of 19219374342.822163
Epoch 56: training loss 20675453537.882
Test Loss of 21210085872.599724, Test MSE of 21210085915.609463
Epoch 57: training loss 20316046458.353
Test Loss of 19071084814.097176, Test MSE of 19071085192.710869
Epoch 58: training loss 19349134885.647
Test Loss of 19814841374.800556, Test MSE of 19814841118.401005
Epoch 59: training loss 18851085991.529
Test Loss of 21554471184.940304, Test MSE of 21554471500.420624
Epoch 60: training loss 18124384120.471
Test Loss of 19641480164.042572, Test MSE of 19641480411.732647
Epoch 61: training loss 17332182125.176
Test Loss of 20721286619.276260, Test MSE of 20721286820.427963
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23647401508.574303, 'MSE - std': 3716951003.2531023, 'R2 - mean': 0.825671883737453, 'R2 - std': 0.015421669012029269} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005509 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424044180660.706
Test Loss of 431612804263.270691, Test MSE of 431612806474.653564
Epoch 2: training loss 424026067666.824
Test Loss of 431593851985.976868, Test MSE of 431593849085.934204
Epoch 3: training loss 423999919405.176
Test Loss of 431568046238.267456, Test MSE of 431568041748.010437
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011079800.471
Test Loss of 431569579541.086548, Test MSE of 431569579852.877747
Epoch 2: training loss 424001301443.765
Test Loss of 431571585030.633972, Test MSE of 431571579991.339600
Epoch 3: training loss 420370425374.118
Test Loss of 419847997348.072205, Test MSE of 419847997629.450256
Epoch 4: training loss 394347930443.294
Test Loss of 378208426496.710754, Test MSE of 378208423337.041199
Epoch 5: training loss 329407355482.353
Test Loss of 290762381301.575195, Test MSE of 290762387732.249756
Epoch 6: training loss 246902538661.647
Test Loss of 212364839836.016663, Test MSE of 212364839203.127075
Epoch 7: training loss 179504773059.765
Test Loss of 150467242782.919006, Test MSE of 150467245369.202820
Epoch 8: training loss 146037365157.647
Test Loss of 129721475629.253128, Test MSE of 129721476447.848328
Epoch 9: training loss 136479316841.412
Test Loss of 122121344194.754288, Test MSE of 122121342712.903900
Epoch 10: training loss 132582435659.294
Test Loss of 119483145929.151321, Test MSE of 119483145817.328232
Epoch 11: training loss 129499315606.588
Test Loss of 116261260802.132339, Test MSE of 116261259370.753586
Epoch 12: training loss 126395101545.412
Test Loss of 113154878951.596481, Test MSE of 113154879073.258972
Epoch 13: training loss 123642444709.647
Test Loss of 109870848254.933823, Test MSE of 109870848444.175568
Epoch 14: training loss 119409957677.176
Test Loss of 106296875456.266541, Test MSE of 106296876124.777832
Epoch 15: training loss 115689617829.647
Test Loss of 101724357885.512268, Test MSE of 101724358676.224396
Epoch 16: training loss 112494042624.000
Test Loss of 98959471295.200363, Test MSE of 98959470734.944000
Epoch 17: training loss 108676336489.412
Test Loss of 97035121820.845901, Test MSE of 97035121626.401810
Epoch 18: training loss 105321932694.588
Test Loss of 93582695485.127258, Test MSE of 93582696124.337723
Epoch 19: training loss 100399257750.588
Test Loss of 90094950218.987503, Test MSE of 90094952078.717545
Epoch 20: training loss 96641808745.412
Test Loss of 86250776412.046280, Test MSE of 86250775694.450714
Epoch 21: training loss 93586205123.765
Test Loss of 82053734961.043961, Test MSE of 82053736096.976349
Epoch 22: training loss 90668772939.294
Test Loss of 78955406288.140671, Test MSE of 78955406503.338074
Epoch 23: training loss 86553539072.000
Test Loss of 74101903710.652481, Test MSE of 74101905043.912430
Epoch 24: training loss 83960412672.000
Test Loss of 73799718856.085144, Test MSE of 73799719388.788742
Epoch 25: training loss 79749085522.824
Test Loss of 69962245587.694580, Test MSE of 69962245336.183121
Epoch 26: training loss 76981022720.000
Test Loss of 65257895231.851921, Test MSE of 65257894449.774338
Epoch 27: training loss 72753372559.059
Test Loss of 63093988195.154099, Test MSE of 63093988499.243813
Epoch 28: training loss 70258935107.765
Test Loss of 62154951341.193893, Test MSE of 62154951357.512367
Epoch 29: training loss 67451749082.353
Test Loss of 59003761729.865807, Test MSE of 59003760378.175949
Epoch 30: training loss 64765549470.118
Test Loss of 53593196184.818138, Test MSE of 53593196140.914337
Epoch 31: training loss 61942735013.647
Test Loss of 50754669975.988892, Test MSE of 50754670101.955269
Epoch 32: training loss 59500876924.235
Test Loss of 49417647276.009254, Test MSE of 49417647094.409157
Epoch 33: training loss 55543414528.000
Test Loss of 45587357595.542801, Test MSE of 45587357182.253113
Epoch 34: training loss 53817792353.882
Test Loss of 46638632235.950020, Test MSE of 46638632148.619949
Epoch 35: training loss 51564444141.176
Test Loss of 45020842447.429893, Test MSE of 45020842666.446686
Epoch 36: training loss 49000944203.294
Test Loss of 44648428285.749191, Test MSE of 44648428116.848701
Epoch 37: training loss 46642541048.471
Test Loss of 40127168096.903282, Test MSE of 40127167692.459641
Epoch 38: training loss 45084397470.118
Test Loss of 39330270223.637207, Test MSE of 39330270635.295799
Epoch 39: training loss 42992202880.000
Test Loss of 35769242350.585838, Test MSE of 35769242255.525650
Epoch 40: training loss 41288641415.529
Test Loss of 36705871894.745026, Test MSE of 36705871508.698494
Epoch 41: training loss 39383999322.353
Test Loss of 33041026510.008331, Test MSE of 33041025809.296463
Epoch 42: training loss 37607546586.353
Test Loss of 34453204997.686256, Test MSE of 34453205143.189095
Epoch 43: training loss 35902303360.000
Test Loss of 33711074211.598335, Test MSE of 33711074062.297546
Epoch 44: training loss 33902853044.706
Test Loss of 32269083995.335491, Test MSE of 32269083367.211475
Epoch 45: training loss 33057583344.941
Test Loss of 29879637702.308189, Test MSE of 29879637699.965908
Epoch 46: training loss 31183957504.000
Test Loss of 26105715131.527996, Test MSE of 26105714426.695259
Epoch 47: training loss 29567085816.471
Test Loss of 27019210782.326702, Test MSE of 27019211377.771793
Epoch 48: training loss 28734238046.118
Test Loss of 27383613592.581211, Test MSE of 27383613749.568020
Epoch 49: training loss 27783380517.647
Test Loss of 24675477456.614529, Test MSE of 24675477745.626663
Epoch 50: training loss 26468338462.118
Test Loss of 25209982042.506248, Test MSE of 25209981542.907238
Epoch 51: training loss 25410225061.647
Test Loss of 25824429769.151318, Test MSE of 25824429988.037128
Epoch 52: training loss 24400525594.353
Test Loss of 22550405448.855160, Test MSE of 22550405157.448208
Epoch 53: training loss 23437695284.706
Test Loss of 21833696929.347523, Test MSE of 21833697059.737816
Epoch 54: training loss 22545062848.000
Test Loss of 23092916242.954189, Test MSE of 23092916246.533012
Epoch 55: training loss 21732717692.235
Test Loss of 20374942639.918556, Test MSE of 20374942364.614529
Epoch 56: training loss 21175044291.765
Test Loss of 21481866032.925499, Test MSE of 21481865424.126503
Epoch 57: training loss 20220259474.824
Test Loss of 22046613689.751041, Test MSE of 22046614083.508984
Epoch 58: training loss 19728293918.118
Test Loss of 22110969949.823231, Test MSE of 22110969385.292206
Epoch 59: training loss 19194088000.000
Test Loss of 22382189930.498844, Test MSE of 22382189459.895126
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23394359098.838467, 'MSE - std': 3362841306.5393186, 'R2 - mean': 0.8271073026148889, 'R2 - std': 0.014089145095003554} 
 

Saving model.....
Results After CV: {'MSE - mean': 23394359098.838467, 'MSE - std': 3362841306.5393186, 'R2 - mean': 0.8271073026148889, 'R2 - std': 0.014089145095003554}
Train time: 95.8430445682
Inference time: 0.07404429279995384
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 10 finished with value: 23394359098.838467 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005545 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525213123.765
Test Loss of 418111688807.276428, Test MSE of 418111687147.368408
Epoch 2: training loss 427503886095.059
Test Loss of 418093285084.173035, Test MSE of 418093282726.576294
Epoch 3: training loss 427476021488.941
Test Loss of 418068770511.855652, Test MSE of 418068773962.127502
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490413989.647
Test Loss of 418074276301.427734, Test MSE of 418074278296.369202
Epoch 2: training loss 427481326772.706
Test Loss of 418076577088.014832, Test MSE of 418076577198.911072
Epoch 3: training loss 423723580958.118
Test Loss of 406275344454.588013, Test MSE of 406275348248.801758
Epoch 4: training loss 397192989997.176
Test Loss of 364984067857.706238, Test MSE of 364984074018.872864
Epoch 5: training loss 332964170691.765
Test Loss of 281193129014.243835, Test MSE of 281193136518.816345
Epoch 6: training loss 251672343190.588
Test Loss of 203759580795.055298, Test MSE of 203759586617.290955
Epoch 7: training loss 183719934433.882
Test Loss of 145497509858.627808, Test MSE of 145497512822.202362
Epoch 8: training loss 149503551909.647
Test Loss of 124642124278.169785, Test MSE of 124642125816.879898
Epoch 9: training loss 138352007243.294
Test Loss of 118094747249.580383, Test MSE of 118094745876.968201
Epoch 10: training loss 135531556577.882
Test Loss of 115134546511.233871, Test MSE of 115134545678.523956
Epoch 11: training loss 132061232896.000
Test Loss of 112765207090.809158, Test MSE of 112765206326.575638
Epoch 12: training loss 130876754853.647
Test Loss of 110279883202.531570, Test MSE of 110279883643.389709
Epoch 13: training loss 127668991307.294
Test Loss of 107754197894.958130, Test MSE of 107754197198.778198
Epoch 14: training loss 123822756954.353
Test Loss of 105110774599.476288, Test MSE of 105110775014.127487
Epoch 15: training loss 120709119548.235
Test Loss of 102173559618.265091, Test MSE of 102173559821.955368
Epoch 16: training loss 118116088365.176
Test Loss of 100504687239.135788, Test MSE of 100504689800.067596
Epoch 17: training loss 115598773052.235
Test Loss of 96683105807.515152, Test MSE of 96683106359.638794
Epoch 18: training loss 112615214140.235
Test Loss of 95353635155.201477, Test MSE of 95353634993.507294
Epoch 19: training loss 108023403279.059
Test Loss of 91814652395.273651, Test MSE of 91814653025.511307
Epoch 20: training loss 105544325692.235
Test Loss of 89961519969.532272, Test MSE of 89961521404.435165
Epoch 21: training loss 102632210597.647
Test Loss of 86032375525.647934, Test MSE of 86032376379.858170
Epoch 22: training loss 99265387399.529
Test Loss of 83691219547.077499, Test MSE of 83691219271.654694
Epoch 23: training loss 95479809204.706
Test Loss of 82070730325.155685, Test MSE of 82070730044.852402
Epoch 24: training loss 92775414904.471
Test Loss of 78049030000.928986, Test MSE of 78049031038.520340
Epoch 25: training loss 88341038592.000
Test Loss of 77154704951.309738, Test MSE of 77154703936.360077
Epoch 26: training loss 86579941541.647
Test Loss of 73652599842.820267, Test MSE of 73652599499.502457
Epoch 27: training loss 82531235885.176
Test Loss of 70468727594.104095, Test MSE of 70468726584.956131
Epoch 28: training loss 79615464975.059
Test Loss of 69143404790.347443, Test MSE of 69143405672.404602
Epoch 29: training loss 76268752828.235
Test Loss of 67741934373.366646, Test MSE of 67741932804.350685
Epoch 30: training loss 73899644867.765
Test Loss of 61908890262.295631, Test MSE of 61908891479.204445
Epoch 31: training loss 71500344207.059
Test Loss of 63282648879.078415, Test MSE of 63282649910.183838
Epoch 32: training loss 68749499693.176
Test Loss of 61140172347.099701, Test MSE of 61140172029.203857
Epoch 33: training loss 65375147312.941
Test Loss of 54804564065.117744, Test MSE of 54804564573.203529
Epoch 34: training loss 62946322183.529
Test Loss of 54856275091.097847, Test MSE of 54856275589.852760
Epoch 35: training loss 60235589037.176
Test Loss of 50640728390.884109, Test MSE of 50640729275.542564
Epoch 36: training loss 58530412408.471
Test Loss of 47580103244.391396, Test MSE of 47580104263.550385
Epoch 37: training loss 55948373827.765
Test Loss of 47796453477.381447, Test MSE of 47796453372.903412
Epoch 38: training loss 52902972363.294
Test Loss of 43116964446.867455, Test MSE of 43116964224.791191
Epoch 39: training loss 50538937084.235
Test Loss of 45258748813.590561, Test MSE of 45258748567.594154
Epoch 40: training loss 48086907986.824
Test Loss of 40086962322.860977, Test MSE of 40086962671.631172
Epoch 41: training loss 46405408598.588
Test Loss of 41291038782.534348, Test MSE of 41291038658.721138
Epoch 42: training loss 44029078494.118
Test Loss of 39168761696.821655, Test MSE of 39168762605.016060
Epoch 43: training loss 41727259478.588
Test Loss of 36424787141.314827, Test MSE of 36424787873.036934
Epoch 44: training loss 39830447939.765
Test Loss of 34924061738.873932, Test MSE of 34924061085.966286
Epoch 45: training loss 38684062832.941
Test Loss of 36016083741.786720, Test MSE of 36016083556.878601
Epoch 46: training loss 36500696500.706
Test Loss of 32652192586.792503, Test MSE of 32652192456.476768
Epoch 47: training loss 35037470298.353
Test Loss of 30977162415.996300, Test MSE of 30977162384.348221
Epoch 48: training loss 33224523459.765
Test Loss of 31459061824.429333, Test MSE of 31459061754.024246
Epoch 49: training loss 31525497381.647
Test Loss of 31032817970.513069, Test MSE of 31032817839.527893
Epoch 50: training loss 30146637891.765
Test Loss of 26903712066.146656, Test MSE of 26903711965.798645
Epoch 51: training loss 29337087401.412
Test Loss of 27268296251.099701, Test MSE of 27268296368.716091
Epoch 52: training loss 27948792568.471
Test Loss of 25864835042.627804, Test MSE of 25864835127.609329
Epoch 53: training loss 26740112041.412
Test Loss of 23605537187.264400, Test MSE of 23605537514.043564
Epoch 54: training loss 25646704101.647
Test Loss of 23270572971.910248, Test MSE of 23270572989.951279
Epoch 55: training loss 24570655887.059
Test Loss of 24196645606.832291, Test MSE of 24196645521.248314
Epoch 56: training loss 23758126716.235
Test Loss of 23335677449.830212, Test MSE of 23335677637.621708
Epoch 57: training loss 22710002447.059
Test Loss of 21781998643.875088, Test MSE of 21781998547.051258
Epoch 58: training loss 21972861071.059
Test Loss of 22544260261.573906, Test MSE of 22544260451.999763
Epoch 59: training loss 21064974964.706
Test Loss of 20925017507.501274, Test MSE of 20925017447.674324
Epoch 60: training loss 20663273837.176
Test Loss of 23220571783.372658, Test MSE of 23220571871.734913
Epoch 61: training loss 19608037914.353
Test Loss of 21406167910.269722, Test MSE of 21406168099.499702
Epoch 62: training loss 19030585532.235
Test Loss of 19895954009.182510, Test MSE of 19895954119.059437
Epoch 63: training loss 18103231346.824
Test Loss of 21559739879.246819, Test MSE of 21559740327.070843
Epoch 64: training loss 17722753347.765
Test Loss of 19226471996.520935, Test MSE of 19226472168.512135
Epoch 65: training loss 17330419994.353
Test Loss of 19523662989.886654, Test MSE of 19523662999.448032
Epoch 66: training loss 16639676184.471
Test Loss of 20449085831.787186, Test MSE of 20449086012.814125
Epoch 67: training loss 15721272963.765
Test Loss of 18612113904.484848, Test MSE of 18612114032.512810
Epoch 68: training loss 15622713942.588
Test Loss of 19365945839.537357, Test MSE of 19365945782.369305
Epoch 69: training loss 15135221206.588
Test Loss of 20263716854.998844, Test MSE of 20263716832.505005
Epoch 70: training loss 15136109010.824
Test Loss of 19098540068.478371, Test MSE of 19098540204.766968
Epoch 71: training loss 14457272572.235
Test Loss of 18458937714.468655, Test MSE of 18458937815.054607
Epoch 72: training loss 14033001822.118
Test Loss of 18704709502.667591, Test MSE of 18704709523.931622
Epoch 73: training loss 13541419120.941
Test Loss of 19362081814.029148, Test MSE of 19362081635.843548
Epoch 74: training loss 13338842898.824
Test Loss of 19837571762.720333, Test MSE of 19837571749.582603
Epoch 75: training loss 13106477490.824
Test Loss of 17072410467.190376, Test MSE of 17072410705.774502
Epoch 76: training loss 12573516777.412
Test Loss of 19961557673.719177, Test MSE of 19961557530.219212
Epoch 77: training loss 12399860107.294
Test Loss of 19963078087.032154, Test MSE of 19963078267.837379
Epoch 78: training loss 12208958678.588
Test Loss of 19212479111.135784, Test MSE of 19212478953.033474
Epoch 79: training loss 11842237379.765
Test Loss of 19298911677.557251, Test MSE of 19298911794.914303
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19298911794.914303, 'MSE - std': 0.0, 'R2 - mean': 0.8497172799370002, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003628 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918213601.882
Test Loss of 424556940554.955383, Test MSE of 424556947394.588684
Epoch 2: training loss 427897221842.824
Test Loss of 424540280221.342590, Test MSE of 424540283077.340149
Epoch 3: training loss 427869589022.118
Test Loss of 424517704653.546143, Test MSE of 424517702542.942993
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889198742.588
Test Loss of 424523394680.686584, Test MSE of 424523394114.404602
Epoch 2: training loss 427877238181.647
Test Loss of 424524152634.922058, Test MSE of 424524159117.061279
Epoch 3: training loss 424042603459.765
Test Loss of 412568534035.660400, Test MSE of 412568535850.151062
Epoch 4: training loss 397172849362.824
Test Loss of 371730740855.265320, Test MSE of 371730743888.680359
Epoch 5: training loss 332761107275.294
Test Loss of 289238726790.069885, Test MSE of 289238728267.703796
Epoch 6: training loss 249946511661.176
Test Loss of 213483419524.352539, Test MSE of 213483417691.230011
Epoch 7: training loss 181846707019.294
Test Loss of 156120876469.029846, Test MSE of 156120877450.930267
Epoch 8: training loss 146864089479.529
Test Loss of 135382116955.077499, Test MSE of 135382117028.271942
Epoch 9: training loss 138079961569.882
Test Loss of 130079867759.270874, Test MSE of 130079866204.766708
Epoch 10: training loss 132458726490.353
Test Loss of 126754308173.694199, Test MSE of 126754307499.419174
Epoch 11: training loss 129942711085.176
Test Loss of 124402388356.234100, Test MSE of 124402387362.664932
Epoch 12: training loss 127071609976.471
Test Loss of 121614499741.461029, Test MSE of 121614500847.277588
Epoch 13: training loss 125412471296.000
Test Loss of 119982127529.896835, Test MSE of 119982126639.400131
Epoch 14: training loss 121378027309.176
Test Loss of 116755615731.682632, Test MSE of 116755614935.932190
Epoch 15: training loss 118103022019.765
Test Loss of 113690495381.999542, Test MSE of 113690494765.569412
Epoch 16: training loss 116769568527.059
Test Loss of 111195898348.694885, Test MSE of 111195894484.348282
Epoch 17: training loss 113005127890.824
Test Loss of 108132537802.822113, Test MSE of 108132536276.006271
Epoch 18: training loss 109536361712.941
Test Loss of 104716904194.546387, Test MSE of 104716905131.240005
Epoch 19: training loss 105886256880.941
Test Loss of 102240592385.539673, Test MSE of 102240591523.122864
Epoch 20: training loss 102638814147.765
Test Loss of 98102290945.065933, Test MSE of 98102291685.011185
Epoch 21: training loss 99789304048.941
Test Loss of 97965646701.375900, Test MSE of 97965646431.698410
Epoch 22: training loss 96789378981.647
Test Loss of 93143254482.402039, Test MSE of 93143255245.550171
Epoch 23: training loss 93998157251.765
Test Loss of 88196326682.588943, Test MSE of 88196325299.535080
Epoch 24: training loss 90271160139.294
Test Loss of 88250711707.743698, Test MSE of 88250711544.173233
Epoch 25: training loss 87799397616.941
Test Loss of 84073379607.391159, Test MSE of 84073381011.709198
Epoch 26: training loss 84543531414.588
Test Loss of 81581419564.295166, Test MSE of 81581417854.612045
Epoch 27: training loss 81258773940.706
Test Loss of 78870965867.421692, Test MSE of 78870967906.339630
Epoch 28: training loss 78061878829.176
Test Loss of 74883266995.371735, Test MSE of 74883266389.693695
Epoch 29: training loss 74354579124.706
Test Loss of 73090484774.965530, Test MSE of 73090484545.267807
Epoch 30: training loss 72205465012.706
Test Loss of 71044807151.063614, Test MSE of 71044807225.178223
Epoch 31: training loss 69655086622.118
Test Loss of 69366644324.789261, Test MSE of 69366643354.880646
Epoch 32: training loss 65816372148.706
Test Loss of 64168070873.330559, Test MSE of 64168070487.370354
Epoch 33: training loss 63246353935.059
Test Loss of 61227365767.550316, Test MSE of 61227365744.774300
Epoch 34: training loss 61172201953.882
Test Loss of 62469685188.308121, Test MSE of 62469683531.854820
Epoch 35: training loss 58048639864.471
Test Loss of 60582692027.839928, Test MSE of 60582692044.562202
Epoch 36: training loss 55831937566.118
Test Loss of 56827729743.293083, Test MSE of 56827730455.167290
Epoch 37: training loss 53141737908.706
Test Loss of 52227232611.900993, Test MSE of 52227234336.327652
Epoch 38: training loss 50838756668.235
Test Loss of 51865428632.190605, Test MSE of 51865429143.729713
Epoch 39: training loss 48689124841.412
Test Loss of 50013040148.726349, Test MSE of 50013038550.540848
Epoch 40: training loss 45597365737.412
Test Loss of 47118298452.859589, Test MSE of 47118297935.019661
Epoch 41: training loss 43556266187.294
Test Loss of 45378293651.512375, Test MSE of 45378293219.896271
Epoch 42: training loss 41782779151.059
Test Loss of 42581551311.026604, Test MSE of 42581551913.830490
Epoch 43: training loss 39414270155.294
Test Loss of 40137403320.701363, Test MSE of 40137403312.740250
Epoch 44: training loss 38279849532.235
Test Loss of 40625961833.112190, Test MSE of 40625963158.256401
Epoch 45: training loss 35956621123.765
Test Loss of 40978876040.793892, Test MSE of 40978875606.954338
Epoch 46: training loss 33642351006.118
Test Loss of 35450277089.739532, Test MSE of 35450276491.910233
Epoch 47: training loss 32780918761.412
Test Loss of 33917419118.737915, Test MSE of 33917419255.990826
Epoch 48: training loss 31135689035.294
Test Loss of 32396963282.875782, Test MSE of 32396963243.013760
Epoch 49: training loss 29796120304.941
Test Loss of 32188740999.550312, Test MSE of 32188741775.167530
Epoch 50: training loss 28143976304.941
Test Loss of 32188408882.453850, Test MSE of 32188408713.539364
Epoch 51: training loss 26482535356.235
Test Loss of 33539281629.831135, Test MSE of 33539281203.340260
Epoch 52: training loss 25866872225.882
Test Loss of 28415377702.669441, Test MSE of 28415377618.024765
Epoch 53: training loss 24076675655.529
Test Loss of 28722398073.219524, Test MSE of 28722397979.326950
Epoch 54: training loss 23141227892.706
Test Loss of 29722848319.244968, Test MSE of 29722848209.715542
Epoch 55: training loss 21908901673.412
Test Loss of 27251174237.268562, Test MSE of 27251174473.869202
Epoch 56: training loss 21415966110.118
Test Loss of 28308183848.445988, Test MSE of 28308183827.654594
Epoch 57: training loss 20650772525.176
Test Loss of 27606869664.481148, Test MSE of 27606869312.632645
Epoch 58: training loss 19585382674.824
Test Loss of 27056928806.847095, Test MSE of 27056928671.528667
Epoch 59: training loss 19113222437.647
Test Loss of 23587437581.501736, Test MSE of 23587437435.558552
Epoch 60: training loss 18181289136.941
Test Loss of 24616051584.562572, Test MSE of 24616051426.428753
Epoch 61: training loss 17402607122.824
Test Loss of 27244955745.354614, Test MSE of 27244955666.733204
Epoch 62: training loss 17014848734.118
Test Loss of 25251464198.632431, Test MSE of 25251464670.312382
Epoch 63: training loss 16068371252.706
Test Loss of 25631467314.157761, Test MSE of 25631467416.346172
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22465189605.630238, 'MSE - std': 3166277810.7159348, 'R2 - mean': 0.833362810021369, 'R2 - std': 0.01635446991563111} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005562 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927113426.824
Test Loss of 447256363242.977539, Test MSE of 447256365538.062195
Epoch 2: training loss 421905845187.765
Test Loss of 447237391200.111023, Test MSE of 447237390441.263794
Epoch 3: training loss 421877587124.706
Test Loss of 447211895406.974792, Test MSE of 447211895044.320618
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896871213.176
Test Loss of 447220660282.270630, Test MSE of 447220655094.417358
Epoch 2: training loss 421885535894.588
Test Loss of 447220581739.125610, Test MSE of 447220582798.716003
Epoch 3: training loss 417965270799.059
Test Loss of 434968464556.206360, Test MSE of 434968467356.362366
Epoch 4: training loss 391191221308.235
Test Loss of 392543519512.101807, Test MSE of 392543518921.628601
Epoch 5: training loss 326920222840.471
Test Loss of 308130167600.262756, Test MSE of 308130173798.670837
Epoch 6: training loss 246129508894.118
Test Loss of 229155705443.131165, Test MSE of 229155705618.819427
Epoch 7: training loss 178569253466.353
Test Loss of 169819354656.096222, Test MSE of 169819352061.675049
Epoch 8: training loss 144041132754.824
Test Loss of 146984875643.055298, Test MSE of 146984876733.045868
Epoch 9: training loss 134554296410.353
Test Loss of 140904442008.072174, Test MSE of 140904445886.783234
Epoch 10: training loss 130305002255.059
Test Loss of 136614405469.860748, Test MSE of 136614402741.318466
Epoch 11: training loss 128526224624.941
Test Loss of 134308205860.063843, Test MSE of 134308205457.052856
Epoch 12: training loss 125209310388.706
Test Loss of 131505369386.459396, Test MSE of 131505372891.318466
Epoch 13: training loss 122345670746.353
Test Loss of 128690944304.618088, Test MSE of 128690946012.201828
Epoch 14: training loss 120010708992.000
Test Loss of 126034150683.536438, Test MSE of 126034154079.251892
Epoch 15: training loss 117433076585.412
Test Loss of 123457619659.591949, Test MSE of 123457619282.186218
Epoch 16: training loss 114605110723.765
Test Loss of 120465250042.018967, Test MSE of 120465250487.059250
Epoch 17: training loss 111047990061.176
Test Loss of 115879077623.176498, Test MSE of 115879076811.844284
Epoch 18: training loss 107790199386.353
Test Loss of 114239126816.747635, Test MSE of 114239127294.667343
Epoch 19: training loss 103978291922.824
Test Loss of 110889445395.423553, Test MSE of 110889447496.343948
Epoch 20: training loss 101316432052.706
Test Loss of 107074609494.991440, Test MSE of 107074609739.564392
Epoch 21: training loss 98403592493.176
Test Loss of 104755682643.912094, Test MSE of 104755681923.497391
Epoch 22: training loss 96375557692.235
Test Loss of 101720185024.814255, Test MSE of 101720184796.068649
Epoch 23: training loss 92297891177.412
Test Loss of 99073525538.761047, Test MSE of 99073526789.323700
Epoch 24: training loss 89612016850.824
Test Loss of 94244267399.550308, Test MSE of 94244269263.266861
Epoch 25: training loss 85930451004.235
Test Loss of 92157008885.340729, Test MSE of 92157008266.283768
Epoch 26: training loss 83421118373.647
Test Loss of 89119849952.140640, Test MSE of 89119850168.197861
Epoch 27: training loss 80188204016.941
Test Loss of 86044679672.775391, Test MSE of 86044680335.827469
Epoch 28: training loss 76148045808.941
Test Loss of 85064931143.713165, Test MSE of 85064931521.902115
Epoch 29: training loss 74006671751.529
Test Loss of 79050125949.660889, Test MSE of 79050126506.593765
Epoch 30: training loss 72156715218.824
Test Loss of 75715596559.929672, Test MSE of 75715598259.296204
Epoch 31: training loss 68502771621.647
Test Loss of 73562734040.560715, Test MSE of 73562734283.274948
Epoch 32: training loss 65957936218.353
Test Loss of 71083382758.891510, Test MSE of 71083383405.258240
Epoch 33: training loss 63219549952.000
Test Loss of 66070254611.186676, Test MSE of 66070254867.310661
Epoch 34: training loss 60148600048.941
Test Loss of 64665646370.168861, Test MSE of 64665646586.828201
Epoch 35: training loss 57400098665.412
Test Loss of 63151644116.770760, Test MSE of 63151643013.667435
Epoch 36: training loss 55092034138.353
Test Loss of 62506906273.665512, Test MSE of 62506905794.577660
Epoch 37: training loss 52785676754.824
Test Loss of 57471996153.189911, Test MSE of 57471996637.560371
Epoch 38: training loss 50908457750.588
Test Loss of 55015997584.018509, Test MSE of 55015998351.634781
Epoch 39: training loss 47703078671.059
Test Loss of 50460833163.340271, Test MSE of 50460834216.992889
Epoch 40: training loss 46436795542.588
Test Loss of 50003849918.090218, Test MSE of 50003850042.698013
Epoch 41: training loss 43627969408.000
Test Loss of 49565452315.003471, Test MSE of 49565452230.713776
Epoch 42: training loss 42246308133.647
Test Loss of 46989582252.620865, Test MSE of 46989581980.863045
Epoch 43: training loss 40221998177.882
Test Loss of 48980637270.103172, Test MSE of 48980637735.900703
Epoch 44: training loss 38371708905.412
Test Loss of 42128025262.219757, Test MSE of 42128024842.903366
Epoch 45: training loss 36678109876.706
Test Loss of 38599233872.359009, Test MSE of 38599234998.964729
Epoch 46: training loss 34571894362.353
Test Loss of 42033903691.325470, Test MSE of 42033903509.487083
Epoch 47: training loss 33561983134.118
Test Loss of 39229537390.619476, Test MSE of 39229538057.234489
Epoch 48: training loss 31602836773.647
Test Loss of 38945796311.080269, Test MSE of 38945795906.983841
Epoch 49: training loss 30345483738.353
Test Loss of 37333268742.928520, Test MSE of 37333268809.746643
Epoch 50: training loss 28903844615.529
Test Loss of 34187409525.015038, Test MSE of 34187409880.288803
Epoch 51: training loss 27651616512.000
Test Loss of 32724983252.770760, Test MSE of 32724983741.402084
Epoch 52: training loss 26660566008.471
Test Loss of 32495994751.615082, Test MSE of 32495994448.737713
Epoch 53: training loss 25353288361.412
Test Loss of 30043353241.493408, Test MSE of 30043353061.882755
Epoch 54: training loss 23943920271.059
Test Loss of 29831404959.711311, Test MSE of 29831404925.567192
Epoch 55: training loss 23096611166.118
Test Loss of 28348892611.242191, Test MSE of 28348892153.264458
Epoch 56: training loss 22321836562.824
Test Loss of 29815496225.517464, Test MSE of 29815496478.388100
Epoch 57: training loss 20989604709.647
Test Loss of 28394783696.388618, Test MSE of 28394783753.888042
Epoch 58: training loss 20415856519.529
Test Loss of 28143084314.470505, Test MSE of 28143084887.827888
Epoch 59: training loss 20057196901.647
Test Loss of 27619500285.453621, Test MSE of 27619500392.938606
Epoch 60: training loss 19229156811.294
Test Loss of 25055085765.314827, Test MSE of 25055086241.332085
Epoch 61: training loss 18672444897.882
Test Loss of 25524069994.947952, Test MSE of 25524069876.320377
Epoch 62: training loss 18224268679.529
Test Loss of 26588384995.752949, Test MSE of 26588384835.317707
Epoch 63: training loss 17510268841.412
Test Loss of 24231776716.480221, Test MSE of 24231776512.298679
Epoch 64: training loss 16945594548.706
Test Loss of 23081427072.384918, Test MSE of 23081427362.286453
Epoch 65: training loss 16489483271.529
Test Loss of 23662833798.543606, Test MSE of 23662833775.715645
Epoch 66: training loss 15750576941.176
Test Loss of 25078931266.028221, Test MSE of 25078931233.682613
Epoch 67: training loss 15399113724.235
Test Loss of 22404864841.371269, Test MSE of 22404864835.968208
Epoch 68: training loss 15017842070.588
Test Loss of 22312907633.639603, Test MSE of 22312907461.950821
Epoch 69: training loss 14597430678.588
Test Loss of 25977644333.538746, Test MSE of 25977644483.247974
Epoch 70: training loss 14050659237.647
Test Loss of 22095323411.245895, Test MSE of 22095323235.904835
Epoch 71: training loss 13847437831.529
Test Loss of 22915879155.504974, Test MSE of 22915879110.225639
Epoch 72: training loss 13364808576.000
Test Loss of 22209323068.165627, Test MSE of 22209323202.902065
Epoch 73: training loss 12926949916.235
Test Loss of 22743845416.386768, Test MSE of 22743845280.989491
Epoch 74: training loss 12513517835.294
Test Loss of 23097254050.968307, Test MSE of 23097254420.722755
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22675877877.327744, 'MSE - std': 2602368641.3374434, 'R2 - mean': 0.8376562405493818, 'R2 - std': 0.014668999586836201} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005800 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109935736.471
Test Loss of 410764039377.917603, Test MSE of 410764044940.026611
Epoch 2: training loss 430089171666.824
Test Loss of 410746241638.115662, Test MSE of 410746240057.577515
Epoch 3: training loss 430061944591.059
Test Loss of 410722669921.969482, Test MSE of 410722669009.022461
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076828973.176
Test Loss of 410726631498.869019, Test MSE of 410726631864.100281
Epoch 2: training loss 430065730740.706
Test Loss of 410727112920.077759, Test MSE of 410727116126.023499
Epoch 3: training loss 426554697607.529
Test Loss of 399277691560.455322, Test MSE of 399277682290.309021
Epoch 4: training loss 400683515663.059
Test Loss of 358896517991.892639, Test MSE of 358896514145.641602
Epoch 5: training loss 336900835568.941
Test Loss of 276312906945.806580, Test MSE of 276312903672.084412
Epoch 6: training loss 255206561551.059
Test Loss of 199493419303.685333, Test MSE of 199493422078.201050
Epoch 7: training loss 186783497517.176
Test Loss of 140194082317.504852, Test MSE of 140194080479.049805
Epoch 8: training loss 151496780709.647
Test Loss of 118632441919.496536, Test MSE of 118632443646.341797
Epoch 9: training loss 141101492073.412
Test Loss of 112337817400.981033, Test MSE of 112337816092.788208
Epoch 10: training loss 137260792079.059
Test Loss of 109787048037.404907, Test MSE of 109787047786.067154
Epoch 11: training loss 135154092032.000
Test Loss of 106774367886.393341, Test MSE of 106774367935.309647
Epoch 12: training loss 132599878686.118
Test Loss of 104441141715.220734, Test MSE of 104441139978.340576
Epoch 13: training loss 129857772001.882
Test Loss of 102800453793.110596, Test MSE of 102800451753.818436
Epoch 14: training loss 125979815544.471
Test Loss of 100734926563.687180, Test MSE of 100734926324.815994
Epoch 15: training loss 123284853308.235
Test Loss of 98698585815.366959, Test MSE of 98698588034.473282
Epoch 16: training loss 119492545927.529
Test Loss of 96117082747.913010, Test MSE of 96117081864.643906
Epoch 17: training loss 117969798384.941
Test Loss of 93060402451.309586, Test MSE of 93060401685.390076
Epoch 18: training loss 113842357579.294
Test Loss of 90092031835.098572, Test MSE of 90092031071.313568
Epoch 19: training loss 110627564303.059
Test Loss of 87569134350.807953, Test MSE of 87569133856.746506
Epoch 20: training loss 107213123824.941
Test Loss of 85434480659.901901, Test MSE of 85434480740.862900
Epoch 21: training loss 104082652491.294
Test Loss of 81951505688.995834, Test MSE of 81951505585.676880
Epoch 22: training loss 101209367973.647
Test Loss of 80181843688.425735, Test MSE of 80181842978.224030
Epoch 23: training loss 97725605707.294
Test Loss of 78056912294.204529, Test MSE of 78056911031.217026
Epoch 24: training loss 94857574746.353
Test Loss of 75857671783.537247, Test MSE of 75857672321.353409
Epoch 25: training loss 92354434981.647
Test Loss of 73630510503.152237, Test MSE of 73630511194.042175
Epoch 26: training loss 88416314774.588
Test Loss of 69700625498.506241, Test MSE of 69700625220.051880
Epoch 27: training loss 84700687465.412
Test Loss of 67339605925.967606, Test MSE of 67339605246.318863
Epoch 28: training loss 81716651580.235
Test Loss of 65959235699.620544, Test MSE of 65959235478.179054
Epoch 29: training loss 78637232474.353
Test Loss of 64901300587.920410, Test MSE of 64901300415.224998
Epoch 30: training loss 76611834337.882
Test Loss of 62557082022.678391, Test MSE of 62557081787.425346
Epoch 31: training loss 73119258488.471
Test Loss of 60149771057.873207, Test MSE of 60149772531.445084
Epoch 32: training loss 70273988412.235
Test Loss of 55316701191.581673, Test MSE of 55316700751.574333
Epoch 33: training loss 67476432323.765
Test Loss of 54178325596.401665, Test MSE of 54178324251.070915
Epoch 34: training loss 64638563990.588
Test Loss of 53304180341.752892, Test MSE of 53304179238.505592
Epoch 35: training loss 61938613872.941
Test Loss of 51404063008.103653, Test MSE of 51404064005.348740
Epoch 36: training loss 59408651723.294
Test Loss of 48808846684.283203, Test MSE of 48808845116.302406
Epoch 37: training loss 57077631518.118
Test Loss of 45004325538.769089, Test MSE of 45004325450.224709
Epoch 38: training loss 53862730119.529
Test Loss of 45483966172.105507, Test MSE of 45483966265.326698
Epoch 39: training loss 52673721554.824
Test Loss of 42706667642.254509, Test MSE of 42706667790.855400
Epoch 40: training loss 50102371922.824
Test Loss of 41203083060.242477, Test MSE of 41203082934.455688
Epoch 41: training loss 47803423277.176
Test Loss of 39778189545.136513, Test MSE of 39778188925.691566
Epoch 42: training loss 45952862795.294
Test Loss of 37599122865.577049, Test MSE of 37599122557.326614
Epoch 43: training loss 42725160387.765
Test Loss of 35602455503.192963, Test MSE of 35602455923.359505
Epoch 44: training loss 41233545517.176
Test Loss of 35050946448.170288, Test MSE of 35050946368.264877
Epoch 45: training loss 39891730183.529
Test Loss of 32742841751.988895, Test MSE of 32742842200.363705
Epoch 46: training loss 37584234405.647
Test Loss of 34152985336.062935, Test MSE of 34152984849.766514
Epoch 47: training loss 35565834789.647
Test Loss of 31180644193.732533, Test MSE of 31180644249.491894
Epoch 48: training loss 34667364931.765
Test Loss of 30381747171.094864, Test MSE of 30381747752.339787
Epoch 49: training loss 32434443723.294
Test Loss of 29241711387.128181, Test MSE of 29241711623.782436
Epoch 50: training loss 31329911902.118
Test Loss of 27440483268.768162, Test MSE of 27440483484.205753
Epoch 51: training loss 29900100382.118
Test Loss of 24952544324.708931, Test MSE of 24952544624.573456
Epoch 52: training loss 28468288602.353
Test Loss of 24862448324.886627, Test MSE of 24862448440.387444
Epoch 53: training loss 27479169524.706
Test Loss of 24832776176.836651, Test MSE of 24832775734.016773
Epoch 54: training loss 26728896647.529
Test Loss of 24318467433.551132, Test MSE of 24318467638.864803
Epoch 55: training loss 25912492668.235
Test Loss of 24665223535.711246, Test MSE of 24665224095.253868
Epoch 56: training loss 24256438166.588
Test Loss of 22381202031.592781, Test MSE of 22381202088.503487
Epoch 57: training loss 23452583954.824
Test Loss of 21565651640.566406, Test MSE of 21565651547.014927
Epoch 58: training loss 22758616542.118
Test Loss of 22132269720.344284, Test MSE of 22132269559.022354
Epoch 59: training loss 21684808090.353
Test Loss of 20435577088.829247, Test MSE of 20435576790.080143
Epoch 60: training loss 20957805963.294
Test Loss of 20405117029.404903, Test MSE of 20405117284.851768
Epoch 61: training loss 20381172796.235
Test Loss of 20785802822.367424, Test MSE of 20785802984.971790
Epoch 62: training loss 19630452954.353
Test Loss of 23047927572.020359, Test MSE of 23047927439.920631
Epoch 63: training loss 19029442733.176
Test Loss of 20973055410.524757, Test MSE of 20973055735.660793
Epoch 64: training loss 18389401743.059
Test Loss of 21002151211.950024, Test MSE of 21002150852.893078
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22257446121.219078, 'MSE - std': 2367381953.260377, 'R2 - mean': 0.8349067562900427, 'R2 - std': 0.013567006034347094} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005523 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042870784.000
Test Loss of 431611677535.363281, Test MSE of 431611676020.835815
Epoch 2: training loss 424022372833.882
Test Loss of 431591715333.449341, Test MSE of 431591717515.095276
Epoch 3: training loss 423994738688.000
Test Loss of 431564276083.502075, Test MSE of 431564273780.214355
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010530213.647
Test Loss of 431569090583.692749, Test MSE of 431569090721.457336
Epoch 2: training loss 423998849264.941
Test Loss of 431570716125.171692, Test MSE of 431570715267.079407
Epoch 3: training loss 420086492340.706
Test Loss of 418867156685.889893, Test MSE of 418867159502.404846
Epoch 4: training loss 393172505539.765
Test Loss of 376374470044.727417, Test MSE of 376374468764.135376
Epoch 5: training loss 328872424086.588
Test Loss of 291581502861.564087, Test MSE of 291581499174.629089
Epoch 6: training loss 247921594789.647
Test Loss of 213182834425.484497, Test MSE of 213182834424.739227
Epoch 7: training loss 181042805097.412
Test Loss of 153740705784.418335, Test MSE of 153740706215.992462
Epoch 8: training loss 146929220577.882
Test Loss of 130935777699.835266, Test MSE of 130935776506.073929
Epoch 9: training loss 137268661910.588
Test Loss of 123938788800.740402, Test MSE of 123938787680.656937
Epoch 10: training loss 135424930785.882
Test Loss of 120987309220.427582, Test MSE of 120987306993.944870
Epoch 11: training loss 132014999612.235
Test Loss of 118571232273.532623, Test MSE of 118571231922.706558
Epoch 12: training loss 128325832553.412
Test Loss of 115768598262.641373, Test MSE of 115768600072.933792
Epoch 13: training loss 126124498401.882
Test Loss of 112793001699.687180, Test MSE of 112793002430.787659
Epoch 14: training loss 122863077120.000
Test Loss of 110350480965.419708, Test MSE of 110350482114.293640
Epoch 15: training loss 119545327600.941
Test Loss of 107742531748.901428, Test MSE of 107742529189.187622
Epoch 16: training loss 117309465449.412
Test Loss of 103779903043.524292, Test MSE of 103779901900.493774
Epoch 17: training loss 113590324766.118
Test Loss of 100132644142.793152, Test MSE of 100132644004.105896
Epoch 18: training loss 111597176500.706
Test Loss of 98679589499.913010, Test MSE of 98679589954.076996
Epoch 19: training loss 108506452660.706
Test Loss of 94332214594.695053, Test MSE of 94332215409.118164
Epoch 20: training loss 106322473562.353
Test Loss of 93642111848.840347, Test MSE of 93642110522.998367
Epoch 21: training loss 100968529739.294
Test Loss of 89988379016.351685, Test MSE of 89988378475.128784
Epoch 22: training loss 99404455815.529
Test Loss of 86972713383.626099, Test MSE of 86972713251.169449
Epoch 23: training loss 95242531056.941
Test Loss of 82416611022.837570, Test MSE of 82416612160.015503
Epoch 24: training loss 91956530537.412
Test Loss of 80862835584.533081, Test MSE of 80862835123.700729
Epoch 25: training loss 89946355245.176
Test Loss of 80442466087.922256, Test MSE of 80442465480.453171
Epoch 26: training loss 87058632161.882
Test Loss of 77406173186.843124, Test MSE of 77406172357.357819
Epoch 27: training loss 83357104640.000
Test Loss of 73627297876.346130, Test MSE of 73627297363.751221
Epoch 28: training loss 81261335657.412
Test Loss of 69798240301.490051, Test MSE of 69798242278.573578
Epoch 29: training loss 77185684239.059
Test Loss of 68926552804.161041, Test MSE of 68926552217.212570
Epoch 30: training loss 74587838441.412
Test Loss of 68712648904.914391, Test MSE of 68712648798.346542
Epoch 31: training loss 71793875501.176
Test Loss of 62934277698.576584, Test MSE of 62934278513.585564
Epoch 32: training loss 69748287616.000
Test Loss of 59480243988.968071, Test MSE of 59480243855.117638
Epoch 33: training loss 66401717782.588
Test Loss of 58307666628.886627, Test MSE of 58307666867.461288
Epoch 34: training loss 64010544399.059
Test Loss of 54530590167.485420, Test MSE of 54530589036.877258
Epoch 35: training loss 61352778556.235
Test Loss of 53782140185.943542, Test MSE of 53782140594.800438
Epoch 36: training loss 58958699956.706
Test Loss of 51997359563.165199, Test MSE of 51997359092.214951
Epoch 37: training loss 56010557266.824
Test Loss of 49640583368.914391, Test MSE of 49640581883.805458
Epoch 38: training loss 53443076314.353
Test Loss of 49043242329.440071, Test MSE of 49043243424.388062
Epoch 39: training loss 51472635301.647
Test Loss of 46126363025.828781, Test MSE of 46126363141.452766
Epoch 40: training loss 49416613330.824
Test Loss of 45672182722.872742, Test MSE of 45672182324.977211
Epoch 41: training loss 47037453244.235
Test Loss of 41282855338.469231, Test MSE of 41282855223.630249
Epoch 42: training loss 45043189692.235
Test Loss of 42678284369.503006, Test MSE of 42678285400.303764
Epoch 43: training loss 42683791751.529
Test Loss of 35526656470.063858, Test MSE of 35526656522.511429
Epoch 44: training loss 41158155090.824
Test Loss of 36139212333.726974, Test MSE of 36139211365.930008
Epoch 45: training loss 39440869948.235
Test Loss of 33143477981.053215, Test MSE of 33143478151.181934
Epoch 46: training loss 37426307802.353
Test Loss of 33716757616.303562, Test MSE of 33716757446.972141
Epoch 47: training loss 35649886885.647
Test Loss of 32382431224.418324, Test MSE of 32382431162.870209
Epoch 48: training loss 34472024478.118
Test Loss of 32071623652.042572, Test MSE of 32071623288.022434
Epoch 49: training loss 32434918934.588
Test Loss of 28744899084.083294, Test MSE of 28744899499.073795
Epoch 50: training loss 31261177705.412
Test Loss of 28571532609.747337, Test MSE of 28571532251.357182
Epoch 51: training loss 29835391653.647
Test Loss of 27226678414.630264, Test MSE of 27226678272.473343
Epoch 52: training loss 28631119661.176
Test Loss of 26263461507.968533, Test MSE of 26263461383.271816
Epoch 53: training loss 28021999721.412
Test Loss of 27733803268.620083, Test MSE of 27733803433.284897
Epoch 54: training loss 26900054727.529
Test Loss of 24708512735.304028, Test MSE of 24708512834.156330
Epoch 55: training loss 25760124160.000
Test Loss of 25676984486.796852, Test MSE of 25676984086.217487
Epoch 56: training loss 24226529592.471
Test Loss of 24265578923.416935, Test MSE of 24265579399.699203
Epoch 57: training loss 23643802974.118
Test Loss of 24952626786.798706, Test MSE of 24952626330.609909
Epoch 58: training loss 22637504161.882
Test Loss of 23987131140.383156, Test MSE of 23987130904.385635
Epoch 59: training loss 21931664033.882
Test Loss of 22550704322.280426, Test MSE of 22550704435.982307
Epoch 60: training loss 21265369020.235
Test Loss of 23750663706.772789, Test MSE of 23750663277.839119
Epoch 61: training loss 20553696041.412
Test Loss of 23204376511.555759, Test MSE of 23204376174.334591
Epoch 62: training loss 19874230332.235
Test Loss of 21653148527.474316, Test MSE of 21653148636.243717
Epoch 63: training loss 19091897807.059
Test Loss of 21611735645.586304, Test MSE of 21611735594.724178
Epoch 64: training loss 18462269888.000
Test Loss of 22343693639.433594, Test MSE of 22343693710.175518
Epoch 65: training loss 18002047356.235
Test Loss of 21952363434.232300, Test MSE of 21952363786.668430
Epoch 66: training loss 17519294072.471
Test Loss of 22307130540.009254, Test MSE of 22307130398.431366
Epoch 67: training loss 16805285936.941
Test Loss of 20321254495.244793, Test MSE of 20321254316.841148
Epoch 68: training loss 16822976628.706
Test Loss of 19846517106.554375, Test MSE of 19846516776.349670
Epoch 69: training loss 16046733549.176
Test Loss of 20797565464.403519, Test MSE of 20797565465.068710
Epoch 70: training loss 15776729886.118
Test Loss of 20444548730.491440, Test MSE of 20444549121.647018
Epoch 71: training loss 15208411354.353
Test Loss of 20486777680.199909, Test MSE of 20486777922.559715
Epoch 72: training loss 14665637648.941
Test Loss of 18227683193.425266, Test MSE of 18227682844.429916
Epoch 73: training loss 14667889908.706
Test Loss of 23327324030.163815, Test MSE of 23327324227.318218
Epoch 74: training loss 14233120594.824
Test Loss of 18245171404.705231, Test MSE of 18245171882.460621
Epoch 75: training loss 14068584929.882
Test Loss of 20926662477.830635, Test MSE of 20926662367.222103
Epoch 76: training loss 13516994921.412
Test Loss of 19291658415.326237, Test MSE of 19291658872.528393
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21664288671.48094, 'MSE - std': 2427126055.815341, 'R2 - mean': 0.8391112400908243, 'R2 - std': 0.014763524589776925} 
 

Saving model.....
Results After CV: {'MSE - mean': 21664288671.48094, 'MSE - std': 2427126055.815341, 'R2 - mean': 0.8391112400908243, 'R2 - std': 0.014763524589776925}
Train time: 109.39998745879993
Inference time: 0.07391698719993656
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 11 finished with value: 21664288671.48094 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005616 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525492615.529
Test Loss of 418111874386.490845, Test MSE of 418111875002.527649
Epoch 2: training loss 427505509556.706
Test Loss of 418094231793.609985, Test MSE of 418094231978.882935
Epoch 3: training loss 427479179143.529
Test Loss of 418071075171.782532, Test MSE of 418071079210.792419
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495559408.941
Test Loss of 418076866064.936401, Test MSE of 418076870135.307678
Epoch 2: training loss 427483246351.059
Test Loss of 418077402283.732605, Test MSE of 418077403323.063721
Epoch 3: training loss 427482766516.706
Test Loss of 418076301504.814270, Test MSE of 418076305943.069275
Epoch 4: training loss 427482408598.588
Test Loss of 418075260182.799011, Test MSE of 418075267163.829834
Epoch 5: training loss 420742978740.706
Test Loss of 396623206156.968750, Test MSE of 396623207505.667236
Epoch 6: training loss 375677079552.000
Test Loss of 330555912498.039307, Test MSE of 330555921751.427002
Epoch 7: training loss 296713633792.000
Test Loss of 244446239807.244965, Test MSE of 244446237363.251160
Epoch 8: training loss 218444990464.000
Test Loss of 175065139679.903778, Test MSE of 175065140702.547272
Epoch 9: training loss 158425252743.529
Test Loss of 126041409155.582703, Test MSE of 126041408954.974380
Epoch 10: training loss 138477849027.765
Test Loss of 118497254675.482773, Test MSE of 118497254232.412354
Epoch 11: training loss 134914091911.529
Test Loss of 115385700356.737457, Test MSE of 115385699577.020508
Epoch 12: training loss 132778242017.882
Test Loss of 112428935971.945404, Test MSE of 112428935553.103302
Epoch 13: training loss 129128353084.235
Test Loss of 108540960678.935928, Test MSE of 108540963183.143112
Epoch 14: training loss 124354223224.471
Test Loss of 105544483556.937317, Test MSE of 105544483094.030792
Epoch 15: training loss 122494921712.941
Test Loss of 103113389753.589630, Test MSE of 103113391060.793106
Epoch 16: training loss 118168398667.294
Test Loss of 99439748533.029846, Test MSE of 99439749185.416336
Epoch 17: training loss 114280405007.059
Test Loss of 95286567171.612305, Test MSE of 95286567990.582382
Epoch 18: training loss 110906937419.294
Test Loss of 92103967622.010635, Test MSE of 92103967496.406082
Epoch 19: training loss 105347187584.000
Test Loss of 89521371772.239655, Test MSE of 89521371969.872314
Epoch 20: training loss 101740594010.353
Test Loss of 85829651379.963913, Test MSE of 85829651982.362289
Epoch 21: training loss 98560783367.529
Test Loss of 82065978252.643066, Test MSE of 82065979381.210159
Epoch 22: training loss 93844248839.529
Test Loss of 79435062862.049500, Test MSE of 79435064729.444229
Epoch 23: training loss 90939254302.118
Test Loss of 75316248289.857971, Test MSE of 75316249293.844757
Epoch 24: training loss 87726337686.588
Test Loss of 73864461280.495956, Test MSE of 73864459952.217239
Epoch 25: training loss 84097387474.824
Test Loss of 68850380154.759201, Test MSE of 68850379372.546265
Epoch 26: training loss 80025133869.176
Test Loss of 66736821930.429794, Test MSE of 66736822277.571884
Epoch 27: training loss 76756109447.529
Test Loss of 64167086459.469810, Test MSE of 64167087291.719612
Epoch 28: training loss 72862216583.529
Test Loss of 59284448787.778854, Test MSE of 59284448476.087280
Epoch 29: training loss 70364674966.588
Test Loss of 55540859815.172798, Test MSE of 55540859962.955315
Epoch 30: training loss 66729935495.529
Test Loss of 55621749433.826508, Test MSE of 55621748038.456276
Epoch 31: training loss 63201732065.882
Test Loss of 52556359589.514687, Test MSE of 52556359971.890541
Epoch 32: training loss 60701125104.941
Test Loss of 50206639664.203560, Test MSE of 50206640209.917404
Epoch 33: training loss 57272205447.529
Test Loss of 44548948816.003700, Test MSE of 44548948866.549217
Epoch 34: training loss 54698207533.176
Test Loss of 43227682242.294701, Test MSE of 43227681847.104919
Epoch 35: training loss 52416025645.176
Test Loss of 41262324699.758499, Test MSE of 41262324940.456558
Epoch 36: training loss 49122277594.353
Test Loss of 38889673646.752716, Test MSE of 38889674208.655678
Epoch 37: training loss 47615414000.941
Test Loss of 37458356788.230392, Test MSE of 37458357591.128380
Epoch 38: training loss 44520284581.647
Test Loss of 34600058847.785332, Test MSE of 34600058483.624435
Epoch 39: training loss 42755633152.000
Test Loss of 33383959456.066620, Test MSE of 33383959148.235168
Epoch 40: training loss 40630734851.765
Test Loss of 29824125705.415684, Test MSE of 29824125875.591339
Epoch 41: training loss 38148621153.882
Test Loss of 31476429988.152672, Test MSE of 31476429914.620216
Epoch 42: training loss 35921108141.176
Test Loss of 31240671608.153599, Test MSE of 31240671591.235943
Epoch 43: training loss 34615642021.647
Test Loss of 30181756398.352997, Test MSE of 30181756712.971466
Epoch 44: training loss 33031546368.000
Test Loss of 26953467269.655331, Test MSE of 26953466849.495251
Epoch 45: training loss 31559592824.471
Test Loss of 26361918969.249134, Test MSE of 26361919430.315048
Epoch 46: training loss 30546926260.706
Test Loss of 22871674321.454544, Test MSE of 22871674076.735291
Epoch 47: training loss 28415018736.941
Test Loss of 25311134183.009945, Test MSE of 25311134110.348232
Epoch 48: training loss 27038289743.059
Test Loss of 24269511501.634975, Test MSE of 24269511737.599735
Epoch 49: training loss 25761000907.294
Test Loss of 22144950425.019661, Test MSE of 22144950443.118305
Epoch 50: training loss 24868223288.471
Test Loss of 20275445493.281517, Test MSE of 20275445606.088516
Epoch 51: training loss 23818066405.647
Test Loss of 21925610119.609531, Test MSE of 21925610354.231102
Epoch 52: training loss 22857243207.529
Test Loss of 21816599259.936157, Test MSE of 21816599778.963516
Epoch 53: training loss 21822814479.059
Test Loss of 20362425809.454544, Test MSE of 20362425712.500141
Epoch 54: training loss 20950664256.000
Test Loss of 19937777465.500809, Test MSE of 19937777601.446060
Epoch 55: training loss 20315261086.118
Test Loss of 18566776036.582005, Test MSE of 18566775989.733776
Epoch 56: training loss 19704350234.353
Test Loss of 19956154026.429794, Test MSE of 19956153935.296963
Epoch 57: training loss 19101783725.176
Test Loss of 20040102813.224152, Test MSE of 20040102705.510872
Epoch 58: training loss 18417885022.118
Test Loss of 18979268046.612076, Test MSE of 18979267798.508411
Epoch 59: training loss 18085719484.235
Test Loss of 17716827653.092758, Test MSE of 17716827729.872074
Epoch 60: training loss 17307143213.176
Test Loss of 19146801096.571827, Test MSE of 19146800758.228153
Epoch 61: training loss 16700359024.941
Test Loss of 18287558600.571827, Test MSE of 18287559044.433041
Epoch 62: training loss 16026397906.824
Test Loss of 17561943953.143650, Test MSE of 17561943895.868515
Epoch 63: training loss 15563479535.059
Test Loss of 16483294903.220911, Test MSE of 16483295006.519604
Epoch 64: training loss 15205139019.294
Test Loss of 16406174666.703678, Test MSE of 16406174653.146767
Epoch 65: training loss 14617700532.706
Test Loss of 18829300980.689335, Test MSE of 18829301218.796860
Epoch 66: training loss 14359536225.882
Test Loss of 18396288066.561184, Test MSE of 18396288276.173836
Epoch 67: training loss 13780245872.941
Test Loss of 17810092427.577145, Test MSE of 17810092571.225285
Epoch 68: training loss 13617149191.529
Test Loss of 17818163479.509602, Test MSE of 17818163792.399815
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17818163792.399815, 'MSE - std': 0.0, 'R2 - mean': 0.8612480252925168, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005535 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917950012.235
Test Loss of 424557170590.408508, Test MSE of 424557165516.190613
Epoch 2: training loss 427897600843.294
Test Loss of 424542000454.173462, Test MSE of 424542000598.364990
Epoch 3: training loss 427870176195.765
Test Loss of 424520499385.708069, Test MSE of 424520496907.030884
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427891173737.412
Test Loss of 424530420517.366638, Test MSE of 424530417615.091492
Epoch 2: training loss 427880054543.059
Test Loss of 424529709149.327759, Test MSE of 424529711801.296936
Epoch 3: training loss 427879448576.000
Test Loss of 424528481361.247253, Test MSE of 424528475543.348145
Epoch 4: training loss 427879096560.941
Test Loss of 424527594980.404358, Test MSE of 424527600317.844543
Epoch 5: training loss 421240311808.000
Test Loss of 403463538678.998840, Test MSE of 403463539996.394165
Epoch 6: training loss 375879680240.941
Test Loss of 338215662397.053894, Test MSE of 338215662245.774231
Epoch 7: training loss 296497812058.353
Test Loss of 253734354163.268097, Test MSE of 253734357647.458405
Epoch 8: training loss 215945165402.353
Test Loss of 184438403650.916504, Test MSE of 184438404116.992310
Epoch 9: training loss 156980325978.353
Test Loss of 137135361203.786255, Test MSE of 137135360955.348450
Epoch 10: training loss 137332565744.941
Test Loss of 129154152443.736298, Test MSE of 129154150581.753586
Epoch 11: training loss 134077527371.294
Test Loss of 127159616865.650711, Test MSE of 127159616936.193207
Epoch 12: training loss 130572585652.706
Test Loss of 123791558178.938705, Test MSE of 123791556572.057678
Epoch 13: training loss 126818478772.706
Test Loss of 120346219054.071716, Test MSE of 120346218983.561172
Epoch 14: training loss 122936045417.412
Test Loss of 116345869971.690033, Test MSE of 116345868543.439743
Epoch 15: training loss 117958677323.294
Test Loss of 113195036185.937546, Test MSE of 113195035416.650970
Epoch 16: training loss 115860531380.706
Test Loss of 110507649596.757812, Test MSE of 110507649534.823868
Epoch 17: training loss 110314163742.118
Test Loss of 105297061176.908630, Test MSE of 105297060534.006714
Epoch 18: training loss 107267420672.000
Test Loss of 103297586861.272263, Test MSE of 103297586291.576462
Epoch 19: training loss 101698932013.176
Test Loss of 98799102205.453613, Test MSE of 98799102481.480255
Epoch 20: training loss 99303659700.706
Test Loss of 95681954040.242432, Test MSE of 95681955583.012833
Epoch 21: training loss 94775152399.059
Test Loss of 91950415839.785339, Test MSE of 91950416067.250687
Epoch 22: training loss 90826176225.882
Test Loss of 88480496244.659729, Test MSE of 88480497523.127289
Epoch 23: training loss 87716670795.294
Test Loss of 82369622330.566742, Test MSE of 82369621782.940659
Epoch 24: training loss 83403228355.765
Test Loss of 80388158021.758957, Test MSE of 80388155174.699799
Epoch 25: training loss 79592264101.647
Test Loss of 76759256642.679626, Test MSE of 76759254581.884598
Epoch 26: training loss 76359987968.000
Test Loss of 74933256446.874863, Test MSE of 74933256862.281311
Epoch 27: training loss 72313149590.588
Test Loss of 70407214819.279205, Test MSE of 70407214982.229477
Epoch 28: training loss 69192230023.529
Test Loss of 67454238956.872543, Test MSE of 67454240799.372009
Epoch 29: training loss 65227384274.824
Test Loss of 62979586198.177193, Test MSE of 62979586166.612854
Epoch 30: training loss 62215270354.824
Test Loss of 60305312141.472122, Test MSE of 60305313303.554306
Epoch 31: training loss 59156929957.647
Test Loss of 56671135991.768677, Test MSE of 56671135618.133156
Epoch 32: training loss 55982105840.941
Test Loss of 54669902067.031227, Test MSE of 54669903062.048576
Epoch 33: training loss 53686648937.412
Test Loss of 54901895215.848251, Test MSE of 54901893449.634415
Epoch 34: training loss 50416253138.824
Test Loss of 48183015099.958359, Test MSE of 48183014892.539322
Epoch 35: training loss 46903508382.118
Test Loss of 47840958543.589172, Test MSE of 47840959564.563705
Epoch 36: training loss 44906491971.765
Test Loss of 48722358726.795280, Test MSE of 48722359170.660011
Epoch 37: training loss 42408009389.176
Test Loss of 44645486608.107330, Test MSE of 44645486064.540276
Epoch 38: training loss 40408389541.647
Test Loss of 41041509160.682861, Test MSE of 41041507938.044487
Epoch 39: training loss 38393074996.706
Test Loss of 40714791644.883644, Test MSE of 40714791152.890701
Epoch 40: training loss 36423195075.765
Test Loss of 40175975419.499420, Test MSE of 40175975984.652168
Epoch 41: training loss 33942462396.235
Test Loss of 34177009798.069859, Test MSE of 34177010086.394634
Epoch 42: training loss 32346906721.882
Test Loss of 35775332211.771454, Test MSE of 35775331822.092094
Epoch 43: training loss 30442203587.765
Test Loss of 32858779088.033310, Test MSE of 32858779058.902470
Epoch 44: training loss 28598567928.471
Test Loss of 32515888247.146889, Test MSE of 32515887944.964947
Epoch 45: training loss 27554883335.529
Test Loss of 32809777193.689568, Test MSE of 32809777305.348904
Epoch 46: training loss 25800133025.882
Test Loss of 29832478722.368725, Test MSE of 29832478579.923916
Epoch 47: training loss 24842754981.647
Test Loss of 35443559815.076569, Test MSE of 35443560844.002602
Epoch 48: training loss 23412467542.588
Test Loss of 31185084129.384224, Test MSE of 31185083826.022945
Epoch 49: training loss 22291450706.824
Test Loss of 33226067523.153366, Test MSE of 33226068140.165352
Epoch 50: training loss 21222769336.471
Test Loss of 30450151959.095074, Test MSE of 30450151353.717892
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24134157573.058853, 'MSE - std': 6315993780.659039, 'R2 - mean': 0.8219270806732026, 'R2 - std': 0.039320944619314124} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005545 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926981029.647
Test Loss of 447259659538.535278, Test MSE of 447259656377.368164
Epoch 2: training loss 421906244306.824
Test Loss of 447241723261.601685, Test MSE of 447241722386.975220
Epoch 3: training loss 421879614524.235
Test Loss of 447218180632.279419, Test MSE of 447218187611.962158
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901747862.588
Test Loss of 447226588735.837158, Test MSE of 447226588868.654175
Epoch 2: training loss 421890637101.176
Test Loss of 447227653927.024780, Test MSE of 447227657907.057617
Epoch 3: training loss 421890218947.765
Test Loss of 447227765776.344177, Test MSE of 447227769102.457703
Epoch 4: training loss 421889888617.412
Test Loss of 447227848852.282227, Test MSE of 447227851634.793518
Epoch 5: training loss 415002661948.235
Test Loss of 425177439511.272705, Test MSE of 425177443702.998474
Epoch 6: training loss 369265196815.059
Test Loss of 357436970787.708557, Test MSE of 357436984845.451599
Epoch 7: training loss 290468066123.294
Test Loss of 270929832035.012726, Test MSE of 270929829611.443451
Epoch 8: training loss 212066820698.353
Test Loss of 198288240093.298187, Test MSE of 198288245848.979706
Epoch 9: training loss 153913476818.824
Test Loss of 148201495531.865845, Test MSE of 148201496509.100006
Epoch 10: training loss 135166676841.412
Test Loss of 140399023536.292389, Test MSE of 140399026713.435577
Epoch 11: training loss 131018177024.000
Test Loss of 136547616774.158691, Test MSE of 136547617658.144547
Epoch 12: training loss 129017322285.176
Test Loss of 134199659377.639603, Test MSE of 134199659797.113403
Epoch 13: training loss 125614052653.176
Test Loss of 130163284949.126068, Test MSE of 130163286162.815659
Epoch 14: training loss 121691630863.059
Test Loss of 126493649616.566269, Test MSE of 126493647405.578247
Epoch 15: training loss 117448008523.294
Test Loss of 123793180784.277588, Test MSE of 123793181210.986099
Epoch 16: training loss 113995022817.882
Test Loss of 121479568432.085129, Test MSE of 121479569287.959686
Epoch 17: training loss 109378177626.353
Test Loss of 117246295177.149200, Test MSE of 117246295510.928406
Epoch 18: training loss 107007770081.882
Test Loss of 111715768808.431183, Test MSE of 111715769672.855545
Epoch 19: training loss 101595566622.118
Test Loss of 107014715772.417297, Test MSE of 107014716178.162979
Epoch 20: training loss 97892040854.588
Test Loss of 104407690239.763123, Test MSE of 104407688381.561676
Epoch 21: training loss 94118079518.118
Test Loss of 99860013800.727280, Test MSE of 99860014563.786530
Epoch 22: training loss 90071777912.471
Test Loss of 96880662032.936386, Test MSE of 96880659190.144516
Epoch 23: training loss 87478101654.588
Test Loss of 93237639254.695343, Test MSE of 93237640628.023285
Epoch 24: training loss 83686779904.000
Test Loss of 87788429147.136703, Test MSE of 87788428717.150269
Epoch 25: training loss 80035338571.294
Test Loss of 85273188021.799683, Test MSE of 85273187823.913376
Epoch 26: training loss 76506667986.824
Test Loss of 81116573719.687256, Test MSE of 81116572817.997543
Epoch 27: training loss 72109132860.235
Test Loss of 75779515336.334946, Test MSE of 75779514975.094162
Epoch 28: training loss 68625414716.235
Test Loss of 73750622898.720337, Test MSE of 73750622005.711105
Epoch 29: training loss 66101397985.882
Test Loss of 69641823489.006714, Test MSE of 69641823857.766769
Epoch 30: training loss 63200315376.941
Test Loss of 65685627367.009949, Test MSE of 65685627312.806702
Epoch 31: training loss 60279688402.824
Test Loss of 63358589160.608833, Test MSE of 63358589436.325478
Epoch 32: training loss 56617692717.176
Test Loss of 59334187147.754799, Test MSE of 59334187334.920380
Epoch 33: training loss 54112750930.824
Test Loss of 57834585952.821655, Test MSE of 57834587849.644684
Epoch 34: training loss 50879100649.412
Test Loss of 54837609111.243118, Test MSE of 54837610468.555740
Epoch 35: training loss 48818351984.941
Test Loss of 51119161451.066391, Test MSE of 51119161687.939857
Epoch 36: training loss 46660995975.529
Test Loss of 46784629953.051125, Test MSE of 46784630802.814857
Epoch 37: training loss 44385498744.471
Test Loss of 47596351065.419388, Test MSE of 47596351346.504120
Epoch 38: training loss 41220167040.000
Test Loss of 45268178932.156372, Test MSE of 45268178206.794807
Epoch 39: training loss 39252707215.059
Test Loss of 43279870129.654404, Test MSE of 43279870240.655769
Epoch 40: training loss 37076525997.176
Test Loss of 38764319926.154984, Test MSE of 38764319413.853271
Epoch 41: training loss 35361386428.235
Test Loss of 40877694171.580849, Test MSE of 40877694367.757996
Epoch 42: training loss 33595592997.647
Test Loss of 33444328658.579689, Test MSE of 33444329218.125065
Epoch 43: training loss 32193052965.647
Test Loss of 35108087717.514687, Test MSE of 35108088164.397682
Epoch 44: training loss 30413444871.529
Test Loss of 32614708041.371269, Test MSE of 32614708385.544666
Epoch 45: training loss 29028879623.529
Test Loss of 34463819276.198936, Test MSE of 34463820058.430199
Epoch 46: training loss 27459525330.824
Test Loss of 31266871182.774925, Test MSE of 31266870373.254581
Epoch 47: training loss 26370719491.765
Test Loss of 33120646539.340275, Test MSE of 33120646906.196247
Epoch 48: training loss 25053298706.824
Test Loss of 27876709806.871155, Test MSE of 27876709749.831184
Epoch 49: training loss 24079893982.118
Test Loss of 27745334103.109879, Test MSE of 27745334329.645855
Epoch 50: training loss 22896021338.353
Test Loss of 28052706263.021049, Test MSE of 28052706431.706699
Epoch 51: training loss 21907594932.706
Test Loss of 24950666215.128384, Test MSE of 24950666086.602951
Epoch 52: training loss 21441610100.706
Test Loss of 27370139639.946335, Test MSE of 27370139978.052605
Epoch 53: training loss 20302446080.000
Test Loss of 24680878096.581078, Test MSE of 24680878023.614849
Epoch 54: training loss 19878776229.647
Test Loss of 24529486259.845478, Test MSE of 24529486619.249451
Epoch 55: training loss 18476577848.471
Test Loss of 25683745876.563499, Test MSE of 25683745968.615162
Epoch 56: training loss 17981238866.824
Test Loss of 25805103248.018505, Test MSE of 25805103339.897243
Epoch 57: training loss 17502862633.412
Test Loss of 25101007386.648163, Test MSE of 25101007437.786160
Epoch 58: training loss 17262321758.118
Test Loss of 24124818454.976635, Test MSE of 24124818432.114071
Epoch 59: training loss 16927548860.235
Test Loss of 21988930532.759659, Test MSE of 21988930343.141232
Epoch 60: training loss 15760177152.000
Test Loss of 22226231119.766830, Test MSE of 22226231068.435390
Epoch 61: training loss 15642419245.176
Test Loss of 22287331424.170254, Test MSE of 22287331290.934044
Epoch 62: training loss 15008405496.471
Test Loss of 23031819132.535740, Test MSE of 23031819063.364090
Epoch 63: training loss 14658321897.412
Test Loss of 21674424662.991440, Test MSE of 21674424561.409954
Epoch 64: training loss 14309663329.882
Test Loss of 23053551182.523247, Test MSE of 23053551241.378838
Epoch 65: training loss 14076801637.647
Test Loss of 22360927780.833679, Test MSE of 22360927606.610115
Epoch 66: training loss 13459994198.588
Test Loss of 22176525446.780476, Test MSE of 22176525390.469906
Epoch 67: training loss 13244890782.118
Test Loss of 21763871786.873932, Test MSE of 21763871907.829712
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23344062351.315807, 'MSE - std': 5276648482.738972, 'R2 - mean': 0.8329911674482228, 'R2 - std': 0.03571534436112572} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003723 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110530861.176
Test Loss of 410767403215.548340, Test MSE of 410767402573.910950
Epoch 2: training loss 430090738748.235
Test Loss of 410749570327.100403, Test MSE of 410749568599.632080
Epoch 3: training loss 430064607713.882
Test Loss of 410726561277.393799, Test MSE of 410726559302.974060
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080163599.059
Test Loss of 410731734891.209595, Test MSE of 410731739488.314148
Epoch 2: training loss 430069912033.882
Test Loss of 410732876448.873657, Test MSE of 410732870228.254150
Epoch 3: training loss 430069327269.647
Test Loss of 410732795112.662659, Test MSE of 410732796360.521545
Epoch 4: training loss 430068896466.824
Test Loss of 410732651667.368835, Test MSE of 410732645611.920044
Epoch 5: training loss 423616801370.353
Test Loss of 390071112064.770020, Test MSE of 390071115349.130188
Epoch 6: training loss 379072223593.412
Test Loss of 323915334337.095764, Test MSE of 323915333274.585571
Epoch 7: training loss 301034293850.353
Test Loss of 239765998711.885223, Test MSE of 239765999808.571625
Epoch 8: training loss 220500040523.294
Test Loss of 168088132741.153168, Test MSE of 168088133544.132324
Epoch 9: training loss 161160638343.529
Test Loss of 120575823362.132339, Test MSE of 120575821897.823868
Epoch 10: training loss 142816138511.059
Test Loss of 112831645187.080063, Test MSE of 112831641899.865662
Epoch 11: training loss 139971278486.588
Test Loss of 110184028949.441925, Test MSE of 110184028324.376877
Epoch 12: training loss 134009729957.647
Test Loss of 107198272267.017120, Test MSE of 107198271770.323166
Epoch 13: training loss 131650758565.647
Test Loss of 103627731999.748260, Test MSE of 103627731610.884537
Epoch 14: training loss 127639651328.000
Test Loss of 100855385840.955109, Test MSE of 100855386315.061020
Epoch 15: training loss 124031856790.588
Test Loss of 97782251599.607590, Test MSE of 97782250716.412506
Epoch 16: training loss 120569371888.941
Test Loss of 95771497139.827850, Test MSE of 95771494737.340118
Epoch 17: training loss 116563044864.000
Test Loss of 90809827676.757050, Test MSE of 90809828039.570099
Epoch 18: training loss 112389571192.471
Test Loss of 88318725315.701996, Test MSE of 88318725395.540085
Epoch 19: training loss 108991692016.941
Test Loss of 85458682515.605743, Test MSE of 85458683815.819794
Epoch 20: training loss 104761463235.765
Test Loss of 82228587806.682098, Test MSE of 82228587517.318405
Epoch 21: training loss 100502667053.176
Test Loss of 78557576173.519669, Test MSE of 78557575856.024780
Epoch 22: training loss 96795145155.765
Test Loss of 76210156678.100876, Test MSE of 76210157364.581909
Epoch 23: training loss 92435530827.294
Test Loss of 72993941010.243408, Test MSE of 72993940366.210846
Epoch 24: training loss 89683020709.647
Test Loss of 69756820950.537720, Test MSE of 69756821602.881317
Epoch 25: training loss 85637949680.941
Test Loss of 66751061743.059692, Test MSE of 66751062200.878555
Epoch 26: training loss 80692854753.882
Test Loss of 63809477079.011566, Test MSE of 63809476132.503105
Epoch 27: training loss 78167986688.000
Test Loss of 61596410132.731140, Test MSE of 61596410898.828583
Epoch 28: training loss 75102744726.588
Test Loss of 57398583151.948174, Test MSE of 57398582209.403069
Epoch 29: training loss 71133863092.706
Test Loss of 55895984566.789452, Test MSE of 55895984216.012794
Epoch 30: training loss 67945865351.529
Test Loss of 52309848926.415550, Test MSE of 52309849025.779427
Epoch 31: training loss 65057321155.765
Test Loss of 52615314578.421104, Test MSE of 52615315099.489731
Epoch 32: training loss 61654348551.529
Test Loss of 49773611126.463676, Test MSE of 49773610972.276512
Epoch 33: training loss 59695159190.588
Test Loss of 47895568054.197128, Test MSE of 47895568297.015900
Epoch 34: training loss 56360672873.412
Test Loss of 42318949837.060623, Test MSE of 42318950389.393471
Epoch 35: training loss 53164489637.647
Test Loss of 41079088248.832947, Test MSE of 41079088088.332497
Epoch 36: training loss 51304399412.706
Test Loss of 41075685028.190651, Test MSE of 41075685151.057350
Epoch 37: training loss 48597161178.353
Test Loss of 37248741895.344749, Test MSE of 37248740781.129189
Epoch 38: training loss 45991733007.059
Test Loss of 37505340227.879684, Test MSE of 37505339917.957497
Epoch 39: training loss 43729381210.353
Test Loss of 33876823315.309578, Test MSE of 33876822904.003788
Epoch 40: training loss 41603190836.706
Test Loss of 33059218029.697361, Test MSE of 33059218011.327827
Epoch 41: training loss 38967621692.235
Test Loss of 30541550024.795929, Test MSE of 30541549345.739117
Epoch 42: training loss 37326226236.235
Test Loss of 29999907606.389633, Test MSE of 29999907815.970257
Epoch 43: training loss 35412457667.765
Test Loss of 31135850124.497917, Test MSE of 31135850060.973030
Epoch 44: training loss 33824872824.471
Test Loss of 26024689252.694122, Test MSE of 26024689676.891682
Epoch 45: training loss 32427439427.765
Test Loss of 25609399744.740398, Test MSE of 25609399715.974113
Epoch 46: training loss 30694332175.059
Test Loss of 26761923256.566406, Test MSE of 26761923173.155853
Epoch 47: training loss 29231679954.824
Test Loss of 24477014572.305412, Test MSE of 24477014803.480137
Epoch 48: training loss 27675789835.294
Test Loss of 21811770809.158722, Test MSE of 21811770819.676414
Epoch 49: training loss 26471390083.765
Test Loss of 21221329994.395187, Test MSE of 21221330080.688267
Epoch 50: training loss 25818356943.059
Test Loss of 22307038276.708931, Test MSE of 22307038249.052879
Epoch 51: training loss 24450024463.059
Test Loss of 20610546931.087460, Test MSE of 20610547206.180820
Epoch 52: training loss 23737674872.471
Test Loss of 21185731757.904675, Test MSE of 21185731973.570797
Epoch 53: training loss 22680634812.235
Test Loss of 19306255354.787598, Test MSE of 19306255227.127331
Epoch 54: training loss 22030958528.000
Test Loss of 20273544592.407219, Test MSE of 20273544624.573772
Epoch 55: training loss 21082457630.118
Test Loss of 19712984798.948635, Test MSE of 19712984946.068630
Epoch 56: training loss 20014055277.176
Test Loss of 18654634169.751041, Test MSE of 18654634039.220478
Epoch 57: training loss 19278307384.471
Test Loss of 19793322887.167053, Test MSE of 19793322886.914417
Epoch 58: training loss 18829028495.059
Test Loss of 20034971948.423878, Test MSE of 20034971916.160133
Epoch 59: training loss 18307948133.647
Test Loss of 18573306470.115688, Test MSE of 18573306338.679356
Epoch 60: training loss 17862759277.176
Test Loss of 20226605494.789448, Test MSE of 20226605544.291946
Epoch 61: training loss 17400439555.765
Test Loss of 18047713360.555298, Test MSE of 18047713277.226421
Epoch 62: training loss 17022932062.118
Test Loss of 19370016061.008793, Test MSE of 19370015793.845959
Epoch 63: training loss 16413230087.529
Test Loss of 20042293066.039795, Test MSE of 20042292672.089348
Epoch 64: training loss 15825048402.824
Test Loss of 19196456869.019897, Test MSE of 19196456884.000866
Epoch 65: training loss 15273967329.882
Test Loss of 17703609946.269321, Test MSE of 17703609827.809101
Epoch 66: training loss 15099443407.059
Test Loss of 18310916464.185101, Test MSE of 18310916432.563763
Epoch 67: training loss 14609524096.000
Test Loss of 18731420372.997684, Test MSE of 18731420297.655098
Epoch 68: training loss 14066023405.176
Test Loss of 19261357955.850071, Test MSE of 19261357945.936401
Epoch 69: training loss 13970235422.118
Test Loss of 18745356966.559925, Test MSE of 18745356926.890629
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22194385995.20951, 'MSE - std': 4984729840.138095, 'R2 - mean': 0.8360645755701652, 'R2 - std': 0.031385137877304445} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005515 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042610567.529
Test Loss of 431611423276.305420, Test MSE of 431611423427.245178
Epoch 2: training loss 424022130085.647
Test Loss of 431591177140.657104, Test MSE of 431591171202.232849
Epoch 3: training loss 423994673031.529
Test Loss of 431563721923.701965, Test MSE of 431563731771.040283
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424006777675.294
Test Loss of 431565336380.771851, Test MSE of 431565336310.899902
Epoch 2: training loss 423997929472.000
Test Loss of 431568345805.416016, Test MSE of 431568343900.108337
Epoch 3: training loss 423997353622.588
Test Loss of 431567796831.481750, Test MSE of 431567800461.042603
Epoch 4: training loss 423996976429.176
Test Loss of 431567688474.180481, Test MSE of 431567691590.916565
Epoch 5: training loss 417365684946.824
Test Loss of 409247230023.078186, Test MSE of 409247229981.661560
Epoch 6: training loss 372675874093.176
Test Loss of 342690063264.755188, Test MSE of 342690064389.806458
Epoch 7: training loss 294188803433.412
Test Loss of 255914755806.948639, Test MSE of 255914756443.000427
Epoch 8: training loss 215535520466.824
Test Loss of 183272161242.091614, Test MSE of 183272159957.625519
Epoch 9: training loss 157221906883.765
Test Loss of 132556534084.116608, Test MSE of 132556535155.310684
Epoch 10: training loss 136983159597.176
Test Loss of 123982214873.262375, Test MSE of 123982214037.741638
Epoch 11: training loss 134089981921.882
Test Loss of 120346900109.445633, Test MSE of 120346900091.622345
Epoch 12: training loss 131042098085.647
Test Loss of 117254658435.613144, Test MSE of 117254658847.568832
Epoch 13: training loss 127809534042.353
Test Loss of 113600612009.876907, Test MSE of 113600612848.783737
Epoch 14: training loss 122658606923.294
Test Loss of 110480722010.032394, Test MSE of 110480720657.787430
Epoch 15: training loss 119576863894.588
Test Loss of 107341975235.465057, Test MSE of 107341975911.410233
Epoch 16: training loss 115927928079.059
Test Loss of 102443354505.299393, Test MSE of 102443354312.112305
Epoch 17: training loss 112297748992.000
Test Loss of 98490478707.620544, Test MSE of 98490478912.591751
Epoch 18: training loss 107183091335.529
Test Loss of 94825715872.162888, Test MSE of 94825716079.984039
Epoch 19: training loss 104272835056.941
Test Loss of 91804749727.333649, Test MSE of 91804750540.926025
Epoch 20: training loss 98247929908.706
Test Loss of 87201790449.547424, Test MSE of 87201787944.485764
Epoch 21: training loss 95790109997.176
Test Loss of 84663811512.211014, Test MSE of 84663808979.580078
Epoch 22: training loss 93151489024.000
Test Loss of 82102237359.326233, Test MSE of 82102238910.461716
Epoch 23: training loss 88429329889.882
Test Loss of 76705223273.906525, Test MSE of 76705223415.650742
Epoch 24: training loss 84801041739.294
Test Loss of 73047462071.855621, Test MSE of 73047462296.966064
Epoch 25: training loss 81078853692.235
Test Loss of 70709580261.227203, Test MSE of 70709580029.455582
Epoch 26: training loss 77874027730.824
Test Loss of 66740252846.378525, Test MSE of 66740252302.582733
Epoch 27: training loss 74275713445.647
Test Loss of 64401177568.251732, Test MSE of 64401177288.225243
Epoch 28: training loss 70435463303.529
Test Loss of 58201856004.738548, Test MSE of 58201855787.693848
Epoch 29: training loss 67766632229.647
Test Loss of 56620564279.085609, Test MSE of 56620565026.809227
Epoch 30: training loss 64295739098.353
Test Loss of 50966030869.560387, Test MSE of 50966030897.453087
Epoch 31: training loss 60941283456.000
Test Loss of 51748713634.532158, Test MSE of 51748714164.309105
Epoch 32: training loss 58681271401.412
Test Loss of 47565988435.161499, Test MSE of 47565988088.629829
Epoch 33: training loss 55335337381.647
Test Loss of 43201629980.549744, Test MSE of 43201630022.339455
Epoch 34: training loss 52231730183.529
Test Loss of 42412136049.014343, Test MSE of 42412135298.147949
Epoch 35: training loss 49955771136.000
Test Loss of 41296094912.621933, Test MSE of 41296093221.647713
Epoch 36: training loss 47018745434.353
Test Loss of 36371882450.273026, Test MSE of 36371882796.767212
Epoch 37: training loss 45821799198.118
Test Loss of 37684366222.748726, Test MSE of 37684366273.766327
Epoch 38: training loss 43133905438.118
Test Loss of 37194540747.994446, Test MSE of 37194541128.504395
Epoch 39: training loss 40346530868.706
Test Loss of 33244792305.073578, Test MSE of 33244792531.077938
Epoch 40: training loss 38861222256.941
Test Loss of 32510122038.019436, Test MSE of 32510122546.682991
Epoch 41: training loss 36936669955.765
Test Loss of 30987396302.126793, Test MSE of 30987396670.309650
Epoch 42: training loss 35193062603.294
Test Loss of 29362430448.599724, Test MSE of 29362429996.848930
Epoch 43: training loss 33177659926.588
Test Loss of 26017834942.608051, Test MSE of 26017835739.816586
Epoch 44: training loss 31752284333.176
Test Loss of 27098860716.009254, Test MSE of 27098859935.893116
Epoch 45: training loss 29996252182.588
Test Loss of 25007286097.621471, Test MSE of 25007286331.050873
Epoch 46: training loss 28649131038.118
Test Loss of 23618151087.089310, Test MSE of 23618150936.162823
Epoch 47: training loss 27233901485.176
Test Loss of 22167021508.294308, Test MSE of 22167021902.734974
Epoch 48: training loss 25943903390.118
Test Loss of 22604840504.151783, Test MSE of 22604840353.464130
Epoch 49: training loss 25226523768.471
Test Loss of 22584327185.532623, Test MSE of 22584327573.769249
Epoch 50: training loss 23779720037.647
Test Loss of 21415421228.897732, Test MSE of 21415421153.401421
Epoch 51: training loss 23259700525.176
Test Loss of 22345021285.523369, Test MSE of 22345021333.908745
Epoch 52: training loss 22076859245.176
Test Loss of 20654828365.356781, Test MSE of 20654828257.322098
Epoch 53: training loss 21072850240.000
Test Loss of 20936280281.973160, Test MSE of 20936280433.008289
Epoch 54: training loss 20513524050.824
Test Loss of 20019591248.555298, Test MSE of 20019591451.475079
Epoch 55: training loss 19993620167.529
Test Loss of 19195938217.995373, Test MSE of 19195938197.039135
Epoch 56: training loss 19189985453.176
Test Loss of 16712443169.525219, Test MSE of 16712443155.296463
Epoch 57: training loss 18452681502.118
Test Loss of 18632982164.079594, Test MSE of 18632981961.877571
Epoch 58: training loss 18046307478.588
Test Loss of 18634480869.819527, Test MSE of 18634481033.214863
Epoch 59: training loss 17690474198.588
Test Loss of 18393158908.564552, Test MSE of 18393158909.055344
Epoch 60: training loss 16801245952.000
Test Loss of 17798274010.565479, Test MSE of 17798274069.868240
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21315163610.14126, 'MSE - std': 4792718776.74363, 'R2 - mean': 0.842268026145226, 'R2 - std': 0.030691247935051798} 
 

Saving model.....
Results After CV: {'MSE - mean': 21315163610.14126, 'MSE - std': 4792718776.74363, 'R2 - mean': 0.842268026145226, 'R2 - std': 0.030691247935051798}
Train time: 97.11873657880005
Inference time: 0.09915460860020175
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 12 finished with value: 21315163610.14126 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005463 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525976304.941
Test Loss of 418112795081.637756, Test MSE of 418112795351.933472
Epoch 2: training loss 427506082635.294
Test Loss of 418094938234.699951, Test MSE of 418094942682.297058
Epoch 3: training loss 427478329946.353
Test Loss of 418071007740.802246, Test MSE of 418071015876.605286
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496963132.235
Test Loss of 418078299337.104797, Test MSE of 418078301123.049561
Epoch 2: training loss 427483446452.706
Test Loss of 418079683279.145020, Test MSE of 418079686042.898010
Epoch 3: training loss 427482984086.588
Test Loss of 418078947776.636597, Test MSE of 418078944289.094482
Epoch 4: training loss 427482671585.882
Test Loss of 418078279348.852173, Test MSE of 418078278074.843079
Epoch 5: training loss 420551360752.941
Test Loss of 396307932979.815857, Test MSE of 396307934898.572937
Epoch 6: training loss 375035503194.353
Test Loss of 329836451286.192017, Test MSE of 329836452920.032410
Epoch 7: training loss 296417795433.412
Test Loss of 243279347063.916718, Test MSE of 243279350026.375183
Epoch 8: training loss 217791772762.353
Test Loss of 174090494666.407593, Test MSE of 174090493617.239838
Epoch 9: training loss 158648897731.765
Test Loss of 125910985030.410370, Test MSE of 125910985563.141998
Epoch 10: training loss 139631671898.353
Test Loss of 118785372526.678696, Test MSE of 118785373643.145218
Epoch 11: training loss 134633025701.647
Test Loss of 115829436236.924362, Test MSE of 115829435833.874680
Epoch 12: training loss 132890352519.529
Test Loss of 113154817233.158447, Test MSE of 113154816756.648849
Epoch 13: training loss 130147515723.294
Test Loss of 109345275972.219299, Test MSE of 109345276832.526688
Epoch 14: training loss 126189775420.235
Test Loss of 106235226923.525330, Test MSE of 106235227096.966003
Epoch 15: training loss 120579631600.941
Test Loss of 103306148404.230392, Test MSE of 103306149619.270386
Epoch 16: training loss 118789801351.529
Test Loss of 100112642371.094147, Test MSE of 100112644689.446335
Epoch 17: training loss 114437939983.059
Test Loss of 96534572672.266479, Test MSE of 96534571859.652039
Epoch 18: training loss 111661848636.235
Test Loss of 92717488894.993286, Test MSE of 92717490944.993118
Epoch 19: training loss 107657947256.471
Test Loss of 89836533003.429108, Test MSE of 89836532806.674393
Epoch 20: training loss 102767574904.471
Test Loss of 87272710435.826965, Test MSE of 87272708390.493988
Epoch 21: training loss 99531246396.235
Test Loss of 83884283141.744156, Test MSE of 83884283567.677948
Epoch 22: training loss 95813065246.118
Test Loss of 79170152056.212814, Test MSE of 79170153318.701340
Epoch 23: training loss 92038652144.941
Test Loss of 76760472597.555405, Test MSE of 76760473011.896606
Epoch 24: training loss 88008270712.471
Test Loss of 72801839502.419617, Test MSE of 72801839331.225082
Epoch 25: training loss 85153168805.647
Test Loss of 71866042423.428177, Test MSE of 71866042097.437576
Epoch 26: training loss 81039113118.118
Test Loss of 68543796048.003700, Test MSE of 68543796732.319717
Epoch 27: training loss 78040888124.235
Test Loss of 65069271353.145500, Test MSE of 65069272258.278702
Epoch 28: training loss 74451456097.882
Test Loss of 60318405330.934998, Test MSE of 60318406590.068626
Epoch 29: training loss 70715106469.647
Test Loss of 56874291676.587555, Test MSE of 56874291443.996559
Epoch 30: training loss 67831626108.235
Test Loss of 55605949683.741844, Test MSE of 55605949935.171333
Epoch 31: training loss 64286468005.647
Test Loss of 53532563818.414993, Test MSE of 53532564794.170723
Epoch 32: training loss 62318316792.471
Test Loss of 50234471107.775154, Test MSE of 50234471553.956078
Epoch 33: training loss 59133440557.176
Test Loss of 47922446571.925049, Test MSE of 47922446042.328171
Epoch 34: training loss 56085840956.235
Test Loss of 46806874142.793434, Test MSE of 46806873634.046776
Epoch 35: training loss 53269577848.471
Test Loss of 43568978153.556328, Test MSE of 43568977964.092323
Epoch 36: training loss 50846462230.588
Test Loss of 42797593265.772842, Test MSE of 42797592637.899712
Epoch 37: training loss 48412612039.529
Test Loss of 41257563467.858429, Test MSE of 41257562742.403656
Epoch 38: training loss 45964111623.529
Test Loss of 36509841170.653709, Test MSE of 36509841348.937584
Epoch 39: training loss 43941715742.118
Test Loss of 34739035617.088135, Test MSE of 34739035296.409454
Epoch 40: training loss 42106078462.118
Test Loss of 35523254956.561646, Test MSE of 35523255994.036812
Epoch 41: training loss 39537715591.529
Test Loss of 34644954731.421700, Test MSE of 34644954999.095703
Epoch 42: training loss 37963743036.235
Test Loss of 28599308831.859356, Test MSE of 28599309248.989952
Epoch 43: training loss 35875018804.706
Test Loss of 33011528502.421467, Test MSE of 33011528279.030846
Epoch 44: training loss 34052167363.765
Test Loss of 31667921239.938931, Test MSE of 31667921122.532238
Epoch 45: training loss 32635571508.706
Test Loss of 28962617480.912331, Test MSE of 28962618061.406937
Epoch 46: training loss 30999723196.235
Test Loss of 26704377795.360630, Test MSE of 26704377591.272026
Epoch 47: training loss 29690128414.118
Test Loss of 26667822752.954891, Test MSE of 26667822998.306805
Epoch 48: training loss 28375391299.765
Test Loss of 24521723481.893131, Test MSE of 24521723284.165131
Epoch 49: training loss 26760236924.235
Test Loss of 22993613231.108028, Test MSE of 22993613600.911366
Epoch 50: training loss 25931459019.294
Test Loss of 20179583707.936157, Test MSE of 20179583407.228031
Epoch 51: training loss 24805011380.706
Test Loss of 20351532110.404812, Test MSE of 20351532191.658817
Epoch 52: training loss 23727198467.765
Test Loss of 22772098813.335182, Test MSE of 22772098897.098091
Epoch 53: training loss 23248279412.706
Test Loss of 20002401920.740227, Test MSE of 20002402158.128868
Epoch 54: training loss 21791470742.588
Test Loss of 18914816762.255840, Test MSE of 18914816659.036346
Epoch 55: training loss 21226712993.882
Test Loss of 22008595139.301411, Test MSE of 22008594599.473667
Epoch 56: training loss 20589787388.235
Test Loss of 20984529825.961601, Test MSE of 20984529734.700584
Epoch 57: training loss 19654848003.765
Test Loss of 20609606317.035393, Test MSE of 20609606297.107765
Epoch 58: training loss 18830350753.882
Test Loss of 18857936202.437195, Test MSE of 18857935954.971603
Epoch 59: training loss 18401746748.235
Test Loss of 16952772215.739071, Test MSE of 16952772105.728682
Epoch 60: training loss 17759904045.176
Test Loss of 18320825557.895905, Test MSE of 18320825252.541828
Epoch 61: training loss 17091529528.471
Test Loss of 19208151921.165855, Test MSE of 19208152377.932747
Epoch 62: training loss 16811282567.529
Test Loss of 19021030841.293545, Test MSE of 19021031256.018063
Epoch 63: training loss 16397328489.412
Test Loss of 19567700770.997917, Test MSE of 19567700837.439930
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19567700837.43993, 'MSE - std': 0.0, 'R2 - mean': 0.8476241904994639, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005456 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917766896.941
Test Loss of 424556294859.355103, Test MSE of 424556288818.202271
Epoch 2: training loss 427896883320.471
Test Loss of 424539950381.065002, Test MSE of 424539949173.353027
Epoch 3: training loss 427868664771.765
Test Loss of 424517518665.015991, Test MSE of 424517519844.969299
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890642221.176
Test Loss of 424522778281.482300, Test MSE of 424522769207.305969
Epoch 2: training loss 427876903755.294
Test Loss of 424523265746.224365, Test MSE of 424523265429.053223
Epoch 3: training loss 427876328387.765
Test Loss of 424522697972.215576, Test MSE of 424522696572.710022
Epoch 4: training loss 427875945291.294
Test Loss of 424522210045.098328, Test MSE of 424522212890.036255
Epoch 5: training loss 421180501534.118
Test Loss of 403470439401.497131, Test MSE of 403470435059.545410
Epoch 6: training loss 375798300069.647
Test Loss of 337716669860.448792, Test MSE of 337716672506.109741
Epoch 7: training loss 296590199265.882
Test Loss of 254193881983.378204, Test MSE of 254193887015.841888
Epoch 8: training loss 217361565696.000
Test Loss of 184972055571.186676, Test MSE of 184972054776.251221
Epoch 9: training loss 157840405925.647
Test Loss of 137932722396.765198, Test MSE of 137932718120.380066
Epoch 10: training loss 137432170044.235
Test Loss of 130411491666.253998, Test MSE of 130411487768.576447
Epoch 11: training loss 132880052314.353
Test Loss of 126939511221.266708, Test MSE of 126939513394.686356
Epoch 12: training loss 130315727329.882
Test Loss of 124613056409.197311, Test MSE of 124613055563.284073
Epoch 13: training loss 126506577347.765
Test Loss of 121490360625.565582, Test MSE of 121490359761.714111
Epoch 14: training loss 122042467930.353
Test Loss of 117060227414.043945, Test MSE of 117060225432.788376
Epoch 15: training loss 119630418040.471
Test Loss of 113712886090.674072, Test MSE of 113712884937.401367
Epoch 16: training loss 116367270821.647
Test Loss of 111054654321.639603, Test MSE of 111054657426.827682
Epoch 17: training loss 110962797778.824
Test Loss of 107350270061.908859, Test MSE of 107350271047.788498
Epoch 18: training loss 107856081347.765
Test Loss of 103357223389.535049, Test MSE of 103357223062.289413
Epoch 19: training loss 103458576685.176
Test Loss of 98312237128.009247, Test MSE of 98312238916.987488
Epoch 20: training loss 99328932352.000
Test Loss of 94614487354.566742, Test MSE of 94614489627.851868
Epoch 21: training loss 95642230482.824
Test Loss of 90916583160.123993, Test MSE of 90916584691.545197
Epoch 22: training loss 91264960481.882
Test Loss of 87425493631.082123, Test MSE of 87425495011.464752
Epoch 23: training loss 88169174558.118
Test Loss of 86079981703.964844, Test MSE of 86079981179.252060
Epoch 24: training loss 84215556562.824
Test Loss of 81685600206.256760, Test MSE of 81685600300.374252
Epoch 25: training loss 79781772122.353
Test Loss of 76223997179.321762, Test MSE of 76223997992.917877
Epoch 26: training loss 76098593551.059
Test Loss of 73264838839.339340, Test MSE of 73264840309.603134
Epoch 27: training loss 73081349827.765
Test Loss of 73057512432.603287, Test MSE of 73057511223.794418
Epoch 28: training loss 69127836084.706
Test Loss of 67279654316.976173, Test MSE of 67279656043.433769
Epoch 29: training loss 66469790433.882
Test Loss of 65230802925.050194, Test MSE of 65230804646.013664
Epoch 30: training loss 62882415088.941
Test Loss of 60098853633.835762, Test MSE of 60098853192.680367
Epoch 31: training loss 59936942787.765
Test Loss of 61023837060.589409, Test MSE of 61023837959.958099
Epoch 32: training loss 57112752534.588
Test Loss of 56195538995.638214, Test MSE of 56195539189.045250
Epoch 33: training loss 53878576790.588
Test Loss of 53924647055.544762, Test MSE of 53924647647.265564
Epoch 34: training loss 50751201536.000
Test Loss of 52571005092.389542, Test MSE of 52571005178.823753
Epoch 35: training loss 48558907580.235
Test Loss of 49563015556.470970, Test MSE of 49563015693.115303
Epoch 36: training loss 45620959390.118
Test Loss of 48299529107.275505, Test MSE of 48299528674.783264
Epoch 37: training loss 43597553852.235
Test Loss of 43684028525.435112, Test MSE of 43684029409.215332
Epoch 38: training loss 41037860186.353
Test Loss of 42953625883.773308, Test MSE of 42953625706.830719
Epoch 39: training loss 39031947873.882
Test Loss of 40474443118.915565, Test MSE of 40474444458.047928
Epoch 40: training loss 36861110369.882
Test Loss of 38078033019.410591, Test MSE of 38078032905.971962
Epoch 41: training loss 34854023220.706
Test Loss of 36813192876.324776, Test MSE of 36813192508.997749
Epoch 42: training loss 32743855247.059
Test Loss of 35892119648.643997, Test MSE of 35892120447.220032
Epoch 43: training loss 31115010560.000
Test Loss of 35504550727.002548, Test MSE of 35504550922.955132
Epoch 44: training loss 29807915745.882
Test Loss of 34979924201.082581, Test MSE of 34979924660.782043
Epoch 45: training loss 27787952256.000
Test Loss of 32664917389.235252, Test MSE of 32664916597.294846
Epoch 46: training loss 26588676058.353
Test Loss of 31251604825.123295, Test MSE of 31251605778.834984
Epoch 47: training loss 24761402228.706
Test Loss of 28211043814.773075, Test MSE of 28211043897.289074
Epoch 48: training loss 23991709440.000
Test Loss of 31520973622.895210, Test MSE of 31520973546.628017
Epoch 49: training loss 22724653643.294
Test Loss of 31636865662.134628, Test MSE of 31636867195.199444
Epoch 50: training loss 21680141353.412
Test Loss of 26491712006.750866, Test MSE of 26491712223.402996
Epoch 51: training loss 20502053052.235
Test Loss of 27073633612.805923, Test MSE of 27073633197.918850
Epoch 52: training loss 19746290458.353
Test Loss of 26485385833.763592, Test MSE of 26485385701.871689
Epoch 53: training loss 18940154036.706
Test Loss of 25940462401.080730, Test MSE of 25940462219.188023
Epoch 54: training loss 18428185219.765
Test Loss of 26482101640.024055, Test MSE of 26482101950.794769
Epoch 55: training loss 17416058917.647
Test Loss of 27360156965.958824, Test MSE of 27360156638.621994
Epoch 56: training loss 16647722962.824
Test Loss of 27834496759.176498, Test MSE of 27834496012.363243
Epoch 57: training loss 16535165263.059
Test Loss of 25312096451.419846, Test MSE of 25312096404.639904
Epoch 58: training loss 15590969103.059
Test Loss of 25927294004.111958, Test MSE of 25927294773.241753
Epoch 59: training loss 15553076551.529
Test Loss of 25374310957.597965, Test MSE of 25374310611.881962
Epoch 60: training loss 14764043885.176
Test Loss of 25776294195.934303, Test MSE of 25776293544.231659
Epoch 61: training loss 14501247943.529
Test Loss of 23099566066.735138, Test MSE of 23099566237.728790
Epoch 62: training loss 14030113671.529
Test Loss of 25756646710.776775, Test MSE of 25756646106.938751
Epoch 63: training loss 13562295017.412
Test Loss of 25060683183.818645, Test MSE of 25060684079.975903
Epoch 64: training loss 13116631066.353
Test Loss of 25892549986.124451, Test MSE of 25892550131.801006
Epoch 65: training loss 12486490499.765
Test Loss of 23878175941.788574, Test MSE of 23878175806.479858
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21722938321.959892, 'MSE - std': 2155237484.519964, 'R2 - mean': 0.838574934319801, 'R2 - std': 0.009049256179662901} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005433 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927887691.294
Test Loss of 447258553173.214905, Test MSE of 447258558324.938843
Epoch 2: training loss 421906625234.824
Test Loss of 447239564363.562317, Test MSE of 447239561572.238831
Epoch 3: training loss 421878762917.647
Test Loss of 447214168231.231995, Test MSE of 447214177633.237061
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899406878.118
Test Loss of 447223431683.434631, Test MSE of 447223433539.151428
Epoch 2: training loss 421887197304.471
Test Loss of 447224196906.814697, Test MSE of 447224198656.057373
Epoch 3: training loss 421886635429.647
Test Loss of 447224058254.893372, Test MSE of 447224062986.340881
Epoch 4: training loss 421886244984.471
Test Loss of 447223800249.530396, Test MSE of 447223805496.070801
Epoch 5: training loss 414688609701.647
Test Loss of 424433143877.166809, Test MSE of 424433137416.382629
Epoch 6: training loss 368253036182.588
Test Loss of 356332569008.055542, Test MSE of 356332567273.651489
Epoch 7: training loss 290084414042.353
Test Loss of 270203353028.781860, Test MSE of 270203355647.017242
Epoch 8: training loss 211842487958.588
Test Loss of 199140425345.213959, Test MSE of 199140424971.442444
Epoch 9: training loss 153218608640.000
Test Loss of 148866170986.118896, Test MSE of 148866171886.572174
Epoch 10: training loss 133965460028.235
Test Loss of 140591562721.680328, Test MSE of 140591564045.395142
Epoch 11: training loss 130892199604.706
Test Loss of 136845886148.722641, Test MSE of 136845883760.158051
Epoch 12: training loss 128586441185.882
Test Loss of 134391164229.699753, Test MSE of 134391165979.961655
Epoch 13: training loss 123603837259.294
Test Loss of 129666683328.162857, Test MSE of 129666683313.859131
Epoch 14: training loss 121497783265.882
Test Loss of 126939991848.919739, Test MSE of 126939991809.890106
Epoch 15: training loss 118393735559.529
Test Loss of 124076038738.550079, Test MSE of 124076037940.171356
Epoch 16: training loss 113826738356.706
Test Loss of 119204678149.803375, Test MSE of 119204675345.502106
Epoch 17: training loss 110403772476.235
Test Loss of 115258682837.718246, Test MSE of 115258681942.745239
Epoch 18: training loss 105975509594.353
Test Loss of 112995971829.044647, Test MSE of 112995973046.486404
Epoch 19: training loss 102210920809.412
Test Loss of 108408807861.740463, Test MSE of 108408806414.230453
Epoch 20: training loss 98394523346.824
Test Loss of 104695344969.134399, Test MSE of 104695344677.609360
Epoch 21: training loss 95000645601.882
Test Loss of 101171270154.067078, Test MSE of 101171272027.947083
Epoch 22: training loss 90796645225.412
Test Loss of 94609220650.873932, Test MSE of 94609221004.727188
Epoch 23: training loss 87222457328.941
Test Loss of 94118656450.531570, Test MSE of 94118655199.422546
Epoch 24: training loss 84275469025.882
Test Loss of 89490144539.062683, Test MSE of 89490144696.729492
Epoch 25: training loss 79452325767.529
Test Loss of 86157188134.610229, Test MSE of 86157188881.514130
Epoch 26: training loss 76200420306.824
Test Loss of 81880358150.217911, Test MSE of 81880358594.283783
Epoch 27: training loss 72874297268.706
Test Loss of 76858448875.865829, Test MSE of 76858449638.543549
Epoch 28: training loss 69960928150.588
Test Loss of 77059166694.062454, Test MSE of 77059166966.498322
Epoch 29: training loss 66873498021.647
Test Loss of 72838470237.919968, Test MSE of 72838471090.440521
Epoch 30: training loss 63477172946.824
Test Loss of 68741292990.860046, Test MSE of 68741293191.159470
Epoch 31: training loss 60580671563.294
Test Loss of 65254198969.115891, Test MSE of 65254199705.775040
Epoch 32: training loss 57958146017.882
Test Loss of 61329640811.599350, Test MSE of 61329641468.777496
Epoch 33: training loss 53927763011.765
Test Loss of 55713145997.412910, Test MSE of 55713145451.194374
Epoch 34: training loss 51965583209.412
Test Loss of 57380931010.294701, Test MSE of 57380930880.823921
Epoch 35: training loss 49388794571.294
Test Loss of 54744632315.973167, Test MSE of 54744631666.526764
Epoch 36: training loss 46633348321.882
Test Loss of 49392135842.376129, Test MSE of 49392134982.561188
Epoch 37: training loss 44793881750.588
Test Loss of 50997610114.635208, Test MSE of 50997610442.217094
Epoch 38: training loss 42328552884.706
Test Loss of 47985212086.036552, Test MSE of 47985211728.851250
Epoch 39: training loss 39902211779.765
Test Loss of 44176264510.119827, Test MSE of 44176265042.913033
Epoch 40: training loss 37512120568.471
Test Loss of 40791987675.876938, Test MSE of 40791987655.040627
Epoch 41: training loss 36548826051.765
Test Loss of 40669483835.158920, Test MSE of 40669484458.479645
Epoch 42: training loss 34120549007.059
Test Loss of 41885315680.288689, Test MSE of 41885316240.656487
Epoch 43: training loss 32156428694.588
Test Loss of 35253037123.745544, Test MSE of 35253037772.098473
Epoch 44: training loss 31234428175.059
Test Loss of 33979167604.008327, Test MSE of 33979167586.967175
Epoch 45: training loss 29341550810.353
Test Loss of 34290207791.137634, Test MSE of 34290207946.493443
Epoch 46: training loss 28010832670.118
Test Loss of 32881035588.515385, Test MSE of 32881036039.687340
Epoch 47: training loss 26652341007.059
Test Loss of 32596456435.919498, Test MSE of 32596456056.422985
Epoch 48: training loss 25477113479.529
Test Loss of 34544239962.070786, Test MSE of 34544240080.548431
Epoch 49: training loss 24177646501.647
Test Loss of 32636251717.522091, Test MSE of 32636251397.503998
Epoch 50: training loss 23236309360.941
Test Loss of 28530731211.236641, Test MSE of 28530730991.113636
Epoch 51: training loss 22227230046.118
Test Loss of 29218029747.075642, Test MSE of 29218029593.766857
Epoch 52: training loss 21372899090.824
Test Loss of 27861308509.090908, Test MSE of 27861308298.349842
Epoch 53: training loss 20485140717.176
Test Loss of 28072711988.763359, Test MSE of 28072711713.686386
Epoch 54: training loss 19683921445.647
Test Loss of 24756468636.276661, Test MSE of 24756468945.211838
Epoch 55: training loss 18871128335.059
Test Loss of 27386447595.332870, Test MSE of 27386447566.636738
Epoch 56: training loss 18277572653.176
Test Loss of 23726265431.879715, Test MSE of 23726265257.058636
Epoch 57: training loss 17951816112.941
Test Loss of 24708207764.992828, Test MSE of 24708207475.626736
Epoch 58: training loss 17337582366.118
Test Loss of 23863624013.516541, Test MSE of 23863623849.403576
Epoch 59: training loss 16882166189.176
Test Loss of 23264962566.158688, Test MSE of 23264963112.255974
Epoch 60: training loss 16337109688.471
Test Loss of 25128523283.778858, Test MSE of 25128523117.895470
Epoch 61: training loss 15826123305.412
Test Loss of 23790514062.301178, Test MSE of 23790514630.292267
Epoch 62: training loss 15148560101.647
Test Loss of 23767383576.042564, Test MSE of 23767384107.428459
Epoch 63: training loss 14884565488.941
Test Loss of 23336097166.656490, Test MSE of 23336096818.931881
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22260657820.950558, 'MSE - std': 1917024672.6745155, 'R2 - mean': 0.8406010040541125, 'R2 - std': 0.007924809697401643} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005548 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111367168.000
Test Loss of 410765055094.937500, Test MSE of 410765057720.980652
Epoch 2: training loss 430090212050.824
Test Loss of 410747423227.024536, Test MSE of 410747421069.010803
Epoch 3: training loss 430062456109.176
Test Loss of 410724336252.860718, Test MSE of 410724333847.616516
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077825987.765
Test Loss of 410729274243.376221, Test MSE of 410729268059.639221
Epoch 2: training loss 430067133741.176
Test Loss of 410729676269.282715, Test MSE of 410729676116.603210
Epoch 3: training loss 430066646076.235
Test Loss of 410729271013.582581, Test MSE of 410729271456.933228
Epoch 4: training loss 430066287314.824
Test Loss of 410728525431.648315, Test MSE of 410728525897.354980
Epoch 5: training loss 423094077078.588
Test Loss of 388913611745.199463, Test MSE of 388913607595.273926
Epoch 6: training loss 377182867335.529
Test Loss of 321853619511.322510, Test MSE of 321853618891.658325
Epoch 7: training loss 298072845492.706
Test Loss of 236638557120.977325, Test MSE of 236638553147.440430
Epoch 8: training loss 219818868013.176
Test Loss of 167468669546.854248, Test MSE of 167468674693.939941
Epoch 9: training loss 161219599691.294
Test Loss of 120393841672.055527, Test MSE of 120393839535.505157
Epoch 10: training loss 141709946819.765
Test Loss of 112557232429.371582, Test MSE of 112557231460.574585
Epoch 11: training loss 136647345513.412
Test Loss of 109637988228.797775, Test MSE of 109637988370.054871
Epoch 12: training loss 134800305904.941
Test Loss of 107166980373.678848, Test MSE of 107166983137.108643
Epoch 13: training loss 131397946548.706
Test Loss of 103990890903.515045, Test MSE of 103990889415.754105
Epoch 14: training loss 128862904862.118
Test Loss of 101002909094.204529, Test MSE of 101002908034.530853
Epoch 15: training loss 123626456937.412
Test Loss of 97642789949.127258, Test MSE of 97642788024.158936
Epoch 16: training loss 120518576188.235
Test Loss of 95356338270.770935, Test MSE of 95356337577.727493
Epoch 17: training loss 116222222637.176
Test Loss of 92137219162.506241, Test MSE of 92137217496.181396
Epoch 18: training loss 112112268980.706
Test Loss of 87938624829.008789, Test MSE of 87938624555.320740
Epoch 19: training loss 106727438004.706
Test Loss of 85630949107.798248, Test MSE of 85630949976.311691
Epoch 20: training loss 104133600888.471
Test Loss of 83185183780.486816, Test MSE of 83185184471.772552
Epoch 21: training loss 99949605376.000
Test Loss of 80380535083.476166, Test MSE of 80380534953.598846
Epoch 22: training loss 95148948495.059
Test Loss of 76403282827.905594, Test MSE of 76403283868.827682
Epoch 23: training loss 92199586514.824
Test Loss of 72459925485.993515, Test MSE of 72459925819.581131
Epoch 24: training loss 89618000911.059
Test Loss of 69262467815.004166, Test MSE of 69262467926.399826
Epoch 25: training loss 85056267309.176
Test Loss of 68600156317.319763, Test MSE of 68600155432.787483
Epoch 26: training loss 81459124028.235
Test Loss of 63602274358.967148, Test MSE of 63602274741.720345
Epoch 27: training loss 76842685394.824
Test Loss of 62225147042.532158, Test MSE of 62225146110.274223
Epoch 28: training loss 73604883049.412
Test Loss of 60704731395.672371, Test MSE of 60704730923.781082
Epoch 29: training loss 71081666695.529
Test Loss of 55739208754.702454, Test MSE of 55739208622.375603
Epoch 30: training loss 67952480316.235
Test Loss of 53134870375.418785, Test MSE of 53134870844.808838
Epoch 31: training loss 64294068148.706
Test Loss of 49944669679.178162, Test MSE of 49944669114.842720
Epoch 32: training loss 61835743006.118
Test Loss of 50461326632.159187, Test MSE of 50461327796.507301
Epoch 33: training loss 58203986831.059
Test Loss of 45503647355.439148, Test MSE of 45503647071.501389
Epoch 34: training loss 55881722834.824
Test Loss of 45272180316.164742, Test MSE of 45272180333.319313
Epoch 35: training loss 52687544455.529
Test Loss of 43439898900.731140, Test MSE of 43439898917.625465
Epoch 36: training loss 50158497867.294
Test Loss of 38669856479.896347, Test MSE of 38669856397.904083
Epoch 37: training loss 48025345362.824
Test Loss of 41016474332.579361, Test MSE of 41016474822.168747
Epoch 38: training loss 45090591262.118
Test Loss of 35265447167.407684, Test MSE of 35265447042.772041
Epoch 39: training loss 42987536037.647
Test Loss of 35184410138.772789, Test MSE of 35184410204.209694
Epoch 40: training loss 41132749741.176
Test Loss of 33411735876.116611, Test MSE of 33411735560.076202
Epoch 41: training loss 38021737298.824
Test Loss of 33018083639.796391, Test MSE of 33018083374.982258
Epoch 42: training loss 36661900160.000
Test Loss of 32953311112.588615, Test MSE of 32953311082.795582
Epoch 43: training loss 34674987331.765
Test Loss of 30137378458.713558, Test MSE of 30137378881.822395
Epoch 44: training loss 33114068724.706
Test Loss of 26231866716.283203, Test MSE of 26231866367.363895
Epoch 45: training loss 31411462113.882
Test Loss of 27765386520.048126, Test MSE of 27765386534.936317
Epoch 46: training loss 30406727115.294
Test Loss of 26290522541.312355, Test MSE of 26290522811.057503
Epoch 47: training loss 28610496896.000
Test Loss of 27662719312.436836, Test MSE of 27662719289.309059
Epoch 48: training loss 27139234642.824
Test Loss of 24916995812.634892, Test MSE of 24916995777.879864
Epoch 49: training loss 26351120466.824
Test Loss of 24373283432.958817, Test MSE of 24373283114.964344
Epoch 50: training loss 25053834368.000
Test Loss of 23486386118.189728, Test MSE of 23486386371.624897
Epoch 51: training loss 23801220378.353
Test Loss of 21895459591.226284, Test MSE of 21895459535.869728
Epoch 52: training loss 22930641125.647
Test Loss of 22893985396.805183, Test MSE of 22893985498.937927
Epoch 53: training loss 22049090315.294
Test Loss of 25410613170.287830, Test MSE of 25410613494.318291
Epoch 54: training loss 21411430027.294
Test Loss of 22961734191.148540, Test MSE of 22961734071.699238
Epoch 55: training loss 20533523331.765
Test Loss of 21863535282.880150, Test MSE of 21863534854.182549
Epoch 56: training loss 19859868306.824
Test Loss of 20877090786.621010, Test MSE of 20877091004.835278
Epoch 57: training loss 19197134588.235
Test Loss of 20368380233.802868, Test MSE of 20368380442.003998
Epoch 58: training loss 18683716024.471
Test Loss of 20915863574.745026, Test MSE of 20915863629.219967
Epoch 59: training loss 17857162501.647
Test Loss of 21461191204.723740, Test MSE of 21461191265.081684
Epoch 60: training loss 17434323651.765
Test Loss of 21601463698.302639, Test MSE of 21601463716.425335
Epoch 61: training loss 16695370691.765
Test Loss of 19762093522.746876, Test MSE of 19762093700.708237
Epoch 62: training loss 16117372423.529
Test Loss of 21333031632.259140, Test MSE of 21333031726.637383
Epoch 63: training loss 15963224039.529
Test Loss of 19878235425.525219, Test MSE of 19878235112.887291
Epoch 64: training loss 15782507636.706
Test Loss of 19218989421.815826, Test MSE of 19218989183.661110
Epoch 65: training loss 15082123392.000
Test Loss of 19902674391.485424, Test MSE of 19902674407.306381
Epoch 66: training loss 14716861033.412
Test Loss of 19531624394.928272, Test MSE of 19531624370.489334
Epoch 67: training loss 14349932508.235
Test Loss of 20389917183.763073, Test MSE of 20389917048.554924
Epoch 68: training loss 13957248952.471
Test Loss of 18055299569.547432, Test MSE of 18055299638.561115
Epoch 69: training loss 13721547115.294
Test Loss of 20345979626.795002, Test MSE of 20345979803.528675
Epoch 70: training loss 13368583744.000
Test Loss of 18860030381.312355, Test MSE of 18860030508.345428
Epoch 71: training loss 13054131124.706
Test Loss of 19691250122.691345, Test MSE of 19691250051.373432
Epoch 72: training loss 12745197647.059
Test Loss of 18288443741.704765, Test MSE of 18288443952.831654
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21267604353.92083, 'MSE - std': 2390544553.4169407, 'R2 - mean': 0.8427147382495995, 'R2 - std': 0.00777853285025759} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005895 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043699862.588
Test Loss of 431613676140.749634, Test MSE of 431613677012.921265
Epoch 2: training loss 424024422279.529
Test Loss of 431593545913.751038, Test MSE of 431593544494.105408
Epoch 3: training loss 423997577336.471
Test Loss of 431566061370.876465, Test MSE of 431566067643.889954
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424013305133.176
Test Loss of 431567270344.795898, Test MSE of 431567271725.966248
Epoch 2: training loss 423999942294.588
Test Loss of 431568479591.655701, Test MSE of 431568480719.321899
Epoch 3: training loss 423999320786.824
Test Loss of 431567530896.644165, Test MSE of 431567537912.593445
Epoch 4: training loss 423998860950.588
Test Loss of 431567075086.334106, Test MSE of 431567070024.807373
Epoch 5: training loss 417150640609.882
Test Loss of 409415055406.437744, Test MSE of 409415056045.289368
Epoch 6: training loss 372059838464.000
Test Loss of 342297561989.271606, Test MSE of 342297560062.941895
Epoch 7: training loss 293706044175.059
Test Loss of 255402398490.180481, Test MSE of 255402394457.287720
Epoch 8: training loss 216298592436.706
Test Loss of 184282612538.402588, Test MSE of 184282616284.688171
Epoch 9: training loss 158244458255.059
Test Loss of 133128645149.142059, Test MSE of 133128645720.046860
Epoch 10: training loss 137429541647.059
Test Loss of 124319498171.764923, Test MSE of 124319496349.314041
Epoch 11: training loss 134244734192.941
Test Loss of 120724075775.407684, Test MSE of 120724077119.291153
Epoch 12: training loss 131465350144.000
Test Loss of 118149259218.509949, Test MSE of 118149258264.707275
Epoch 13: training loss 128638612299.294
Test Loss of 114886589288.840347, Test MSE of 114886589610.597946
Epoch 14: training loss 124030662053.647
Test Loss of 110819197981.852844, Test MSE of 110819198792.121552
Epoch 15: training loss 121283708205.176
Test Loss of 106147021853.852844, Test MSE of 106147020717.528854
Epoch 16: training loss 116450343484.235
Test Loss of 103887164705.525223, Test MSE of 103887164663.382004
Epoch 17: training loss 113348720158.118
Test Loss of 99830509801.136505, Test MSE of 99830508909.442795
Epoch 18: training loss 108639133485.176
Test Loss of 96916677267.605743, Test MSE of 96916676484.077164
Epoch 19: training loss 105583453334.588
Test Loss of 92069638938.180466, Test MSE of 92069640067.882721
Epoch 20: training loss 101025830580.706
Test Loss of 89553272631.085602, Test MSE of 89553269391.047562
Epoch 21: training loss 97846786996.706
Test Loss of 82551462931.428040, Test MSE of 82551465360.247421
Epoch 22: training loss 93735841159.529
Test Loss of 80967059216.229523, Test MSE of 80967059584.458786
Epoch 23: training loss 90485799740.235
Test Loss of 78970433570.117538, Test MSE of 78970433561.131943
Epoch 24: training loss 86598451952.941
Test Loss of 74504848772.086990, Test MSE of 74504848021.004745
Epoch 25: training loss 83319140698.353
Test Loss of 70632929512.188797, Test MSE of 70632930605.459595
Epoch 26: training loss 79515614117.647
Test Loss of 66803480527.666824, Test MSE of 66803480728.769684
Epoch 27: training loss 76596876272.941
Test Loss of 67217414937.232765, Test MSE of 67217416262.533432
Epoch 28: training loss 73086431728.941
Test Loss of 61697539754.824615, Test MSE of 61697540800.054810
Epoch 29: training loss 69196778744.471
Test Loss of 59253150981.093941, Test MSE of 59253149957.058006
Epoch 30: training loss 66245556043.294
Test Loss of 52877167193.795464, Test MSE of 52877167109.207420
Epoch 31: training loss 63415015115.294
Test Loss of 53390162722.709854, Test MSE of 53390161935.699020
Epoch 32: training loss 59847115264.000
Test Loss of 50533498347.387321, Test MSE of 50533499229.651825
Epoch 33: training loss 57665506017.882
Test Loss of 48665790262.137901, Test MSE of 48665789924.818893
Epoch 34: training loss 54175137731.765
Test Loss of 47087769284.412773, Test MSE of 47087768996.429550
Epoch 35: training loss 52361872926.118
Test Loss of 43784956488.736694, Test MSE of 43784956497.051758
Epoch 36: training loss 48758940551.529
Test Loss of 40318889422.956039, Test MSE of 40318889752.427055
Epoch 37: training loss 47342978439.529
Test Loss of 40283893039.740860, Test MSE of 40283892947.534706
Epoch 38: training loss 44544912542.118
Test Loss of 33298907459.168903, Test MSE of 33298907214.779217
Epoch 39: training loss 42235356039.529
Test Loss of 33406736878.704304, Test MSE of 33406737197.758381
Epoch 40: training loss 39795385788.235
Test Loss of 31858450305.954651, Test MSE of 31858449783.719925
Epoch 41: training loss 38443598004.706
Test Loss of 28199664840.440536, Test MSE of 28199665014.684734
Epoch 42: training loss 36593799002.353
Test Loss of 29613049018.224895, Test MSE of 29613049312.279236
Epoch 43: training loss 34799926851.765
Test Loss of 30166975722.558075, Test MSE of 30166975920.840801
Epoch 44: training loss 33037071066.353
Test Loss of 28402662792.825542, Test MSE of 28402662532.057316
Epoch 45: training loss 31889279826.824
Test Loss of 28303451138.369274, Test MSE of 28303451507.986340
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22674773784.733932, 'MSE - std': 3534439960.949485, 'R2 - mean': 0.8318975452615561, 'R2 - std': 0.02272556083365892} 
 

Saving model.....
Results After CV: {'MSE - mean': 22674773784.733932, 'MSE - std': 3534439960.949485, 'R2 - mean': 0.8318975452615561, 'R2 - std': 0.02272556083365892}
Train time: 95.21841617739992
Inference time: 0.07403019340017636
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 13 finished with value: 22674773784.733932 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005561 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525489242.353
Test Loss of 418111890061.768188, Test MSE of 418111887787.928040
Epoch 2: training loss 427506001197.176
Test Loss of 418094352799.474426, Test MSE of 418094350327.334167
Epoch 3: training loss 427480211335.529
Test Loss of 418071433679.322693, Test MSE of 418071435215.563049
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495285338.353
Test Loss of 418077435241.230652, Test MSE of 418077431684.110168
Epoch 2: training loss 427483798949.647
Test Loss of 418077637193.075195, Test MSE of 418077632198.416626
Epoch 3: training loss 427483271047.529
Test Loss of 418075552896.621765, Test MSE of 418075560318.222168
Epoch 4: training loss 427482939873.882
Test Loss of 418075048308.837402, Test MSE of 418075048698.173279
Epoch 5: training loss 420687411200.000
Test Loss of 396638140042.215149, Test MSE of 396638140478.654480
Epoch 6: training loss 375290738928.941
Test Loss of 330143616213.895935, Test MSE of 330143614433.618530
Epoch 7: training loss 296639789839.059
Test Loss of 244763694849.598877, Test MSE of 244763696498.322754
Epoch 8: training loss 217773921008.941
Test Loss of 174849393432.101776, Test MSE of 174849393794.888275
Epoch 9: training loss 159348053714.824
Test Loss of 125217025793.125137, Test MSE of 125217027054.911484
Epoch 10: training loss 138519737449.412
Test Loss of 117900229524.696732, Test MSE of 117900229142.814285
Epoch 11: training loss 134667197816.471
Test Loss of 115454970436.337723, Test MSE of 115454971455.580063
Epoch 12: training loss 132272277202.824
Test Loss of 112554877955.079346, Test MSE of 112554879568.375961
Epoch 13: training loss 128515867888.941
Test Loss of 109572470241.561874, Test MSE of 109572470986.814117
Epoch 14: training loss 124495717436.235
Test Loss of 105284156647.898224, Test MSE of 105284155478.082062
Epoch 15: training loss 121335922928.941
Test Loss of 102373660914.083740, Test MSE of 102373661203.557968
Epoch 16: training loss 117138977844.706
Test Loss of 98488236636.024979, Test MSE of 98488236562.312149
Epoch 17: training loss 113459499384.471
Test Loss of 96236828684.791122, Test MSE of 96236828847.737167
Epoch 18: training loss 111225410168.471
Test Loss of 93129793227.118210, Test MSE of 93129792711.446609
Epoch 19: training loss 106653821078.588
Test Loss of 89707154863.818649, Test MSE of 89707154678.559647
Epoch 20: training loss 101841313099.294
Test Loss of 85198820164.633820, Test MSE of 85198821609.357483
Epoch 21: training loss 98799271680.000
Test Loss of 83561919524.241501, Test MSE of 83561919541.767578
Epoch 22: training loss 94257916476.235
Test Loss of 79864527999.674301, Test MSE of 79864527730.317551
Epoch 23: training loss 89756343145.412
Test Loss of 77957452055.509598, Test MSE of 77957451446.534973
Epoch 24: training loss 86934094742.588
Test Loss of 73892187816.061066, Test MSE of 73892188951.734741
Epoch 25: training loss 83109214863.059
Test Loss of 68005627599.381912, Test MSE of 68005628511.141785
Epoch 26: training loss 79451352673.882
Test Loss of 65312624199.417068, Test MSE of 65312625064.787849
Epoch 27: training loss 76399983013.647
Test Loss of 63363773965.146423, Test MSE of 63363773616.984169
Epoch 28: training loss 73215775698.824
Test Loss of 61172703518.142029, Test MSE of 61172702873.969742
Epoch 29: training loss 69171612638.118
Test Loss of 55786215793.047424, Test MSE of 55786216074.267853
Epoch 30: training loss 66114321106.824
Test Loss of 54112214300.010178, Test MSE of 54112215179.386887
Epoch 31: training loss 63509867324.235
Test Loss of 53140430222.656487, Test MSE of 53140429490.131950
Epoch 32: training loss 60888066081.882
Test Loss of 52061976372.763359, Test MSE of 52061976828.533485
Epoch 33: training loss 57562219105.882
Test Loss of 47027701864.697662, Test MSE of 47027702625.903625
Epoch 34: training loss 54027626413.176
Test Loss of 45132695309.916260, Test MSE of 45132695692.554588
Epoch 35: training loss 52573544704.000
Test Loss of 43593332553.608139, Test MSE of 43593333176.291626
Epoch 36: training loss 49287938371.765
Test Loss of 41369052986.448303, Test MSE of 41369052634.389771
Epoch 37: training loss 46931702400.000
Test Loss of 39977123185.284294, Test MSE of 39977122813.004677
Epoch 38: training loss 44570946861.176
Test Loss of 35920086489.034470, Test MSE of 35920087288.627609
Epoch 39: training loss 42149910098.824
Test Loss of 36470726818.020821, Test MSE of 36470728228.344612
Epoch 40: training loss 39742009626.353
Test Loss of 33167849228.021282, Test MSE of 33167849844.077862
Epoch 41: training loss 38406793667.765
Test Loss of 31327554540.813324, Test MSE of 31327554587.330467
Epoch 42: training loss 36344789338.353
Test Loss of 32558367548.106407, Test MSE of 32558367804.248936
Epoch 43: training loss 34183951947.294
Test Loss of 31013487189.866295, Test MSE of 31013487270.985050
Epoch 44: training loss 32884937336.471
Test Loss of 28443872186.596344, Test MSE of 28443872585.038517
Epoch 45: training loss 31068146492.235
Test Loss of 27683391414.569511, Test MSE of 27683391703.975807
Epoch 46: training loss 29683987169.882
Test Loss of 27724236107.858433, Test MSE of 27724236105.195496
Epoch 47: training loss 28062932613.647
Test Loss of 25230370492.432106, Test MSE of 25230370388.317032
Epoch 48: training loss 27354645274.353
Test Loss of 23490342066.838768, Test MSE of 23490342295.527832
Epoch 49: training loss 25692083392.000
Test Loss of 25377581202.387230, Test MSE of 25377581429.678074
Epoch 50: training loss 24731041136.941
Test Loss of 23263695529.719177, Test MSE of 23263695298.920902
Epoch 51: training loss 23746264688.941
Test Loss of 23052122697.075180, Test MSE of 23052122516.095905
Epoch 52: training loss 22960198078.118
Test Loss of 21877659652.737450, Test MSE of 21877659480.255814
Epoch 53: training loss 22152700024.471
Test Loss of 21650590096.314598, Test MSE of 21650589798.946869
Epoch 54: training loss 21112455747.765
Test Loss of 22006048909.649780, Test MSE of 22006048608.914822
Epoch 55: training loss 20454748423.529
Test Loss of 23137376122.403885, Test MSE of 23137376094.168171
Epoch 56: training loss 19637844163.765
Test Loss of 20905330785.591488, Test MSE of 20905330711.496960
Epoch 57: training loss 18944533432.471
Test Loss of 19396455507.142262, Test MSE of 19396455243.202602
Epoch 58: training loss 18331856914.824
Test Loss of 20524262663.165394, Test MSE of 20524262526.471848
Epoch 59: training loss 17832060698.353
Test Loss of 19347142170.411289, Test MSE of 19347142504.506702
Epoch 60: training loss 17069334128.941
Test Loss of 18447030933.821884, Test MSE of 18447031269.611069
Epoch 61: training loss 16406859696.941
Test Loss of 20538982402.368725, Test MSE of 20538982347.829121
Epoch 62: training loss 16182707433.412
Test Loss of 20598902320.203564, Test MSE of 20598902325.709156
Epoch 63: training loss 15786197703.529
Test Loss of 19395893356.961369, Test MSE of 19395893496.333557
Epoch 64: training loss 15450025212.235
Test Loss of 18277095158.229008, Test MSE of 18277095231.857578
Epoch 65: training loss 14723237541.647
Test Loss of 21322243377.802452, Test MSE of 21322243661.502136
Epoch 66: training loss 14392961472.000
Test Loss of 18332567806.401112, Test MSE of 18332567892.556580
Epoch 67: training loss 14051463766.588
Test Loss of 19186811103.607681, Test MSE of 19186811027.052158
Epoch 68: training loss 13919900675.765
Test Loss of 22679516204.768909, Test MSE of 22679515801.928219
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22679515801.92822, 'MSE - std': 0.0, 'R2 - mean': 0.8233921497416381, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005395 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918171919.059
Test Loss of 424556742082.294678, Test MSE of 424556749035.831604
Epoch 2: training loss 427896965722.353
Test Loss of 424540618710.310425, Test MSE of 424540625201.412476
Epoch 3: training loss 427868729825.882
Test Loss of 424517901344.925293, Test MSE of 424517906451.150635
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887363975.529
Test Loss of 424526955636.778137, Test MSE of 424526955028.930359
Epoch 2: training loss 427877193366.588
Test Loss of 424527135008.036987, Test MSE of 424527134049.879456
Epoch 3: training loss 427876684980.706
Test Loss of 424526089221.448059, Test MSE of 424526102488.586731
Epoch 4: training loss 427876265502.118
Test Loss of 424525900087.250549, Test MSE of 424525901381.812500
Epoch 5: training loss 420752138962.824
Test Loss of 402547376719.233887, Test MSE of 402547386003.204163
Epoch 6: training loss 374853355399.529
Test Loss of 336480890246.129089, Test MSE of 336480896270.731873
Epoch 7: training loss 295331430038.588
Test Loss of 251835099587.479065, Test MSE of 251835096203.672974
Epoch 8: training loss 216145959875.765
Test Loss of 183879902554.070770, Test MSE of 183879905985.564880
Epoch 9: training loss 156992691440.941
Test Loss of 137015452860.787415, Test MSE of 137015450408.728775
Epoch 10: training loss 137460428709.647
Test Loss of 129799375659.525330, Test MSE of 129799375320.603210
Epoch 11: training loss 133410800700.235
Test Loss of 127162723538.105942, Test MSE of 127162721929.213531
Epoch 12: training loss 131300546409.412
Test Loss of 124242962881.584091, Test MSE of 124242962015.525452
Epoch 13: training loss 126507469763.765
Test Loss of 119614011505.225082, Test MSE of 119614013390.099136
Epoch 14: training loss 123406871913.412
Test Loss of 117103709185.658112, Test MSE of 117103707352.053619
Epoch 15: training loss 119209809588.706
Test Loss of 113917480134.736069, Test MSE of 113917480059.505554
Epoch 16: training loss 115994834763.294
Test Loss of 111214819412.800369, Test MSE of 111214819515.582581
Epoch 17: training loss 110270338831.059
Test Loss of 106745782691.738144, Test MSE of 106745782063.232361
Epoch 18: training loss 107057540547.765
Test Loss of 104018293070.464035, Test MSE of 104018291423.818497
Epoch 19: training loss 102917259113.412
Test Loss of 99841872321.110336, Test MSE of 99841874086.860977
Epoch 20: training loss 98706456274.824
Test Loss of 95983715285.599808, Test MSE of 95983717157.571732
Epoch 21: training loss 95950400090.353
Test Loss of 92293010156.517227, Test MSE of 92293011216.190689
Epoch 22: training loss 91042340080.941
Test Loss of 89039558006.495483, Test MSE of 89039559668.968399
Epoch 23: training loss 88160810209.882
Test Loss of 86725471719.720566, Test MSE of 86725469864.401138
Epoch 24: training loss 83914715376.941
Test Loss of 80650185597.009491, Test MSE of 80650184346.471527
Epoch 25: training loss 80700607472.941
Test Loss of 78380050380.361786, Test MSE of 78380051057.105743
Epoch 26: training loss 77148669711.059
Test Loss of 74184670226.712936, Test MSE of 74184670676.384140
Epoch 27: training loss 73311512184.471
Test Loss of 70338291642.359467, Test MSE of 70338290308.688950
Epoch 28: training loss 69301201317.647
Test Loss of 69292487570.328018, Test MSE of 69292487107.252884
Epoch 29: training loss 67189254083.765
Test Loss of 62918366602.392784, Test MSE of 62918367421.100639
Epoch 30: training loss 63339166870.588
Test Loss of 62992046304.081429, Test MSE of 62992045497.626656
Epoch 31: training loss 59818914650.353
Test Loss of 58131251904.932686, Test MSE of 58131252174.240547
Epoch 32: training loss 56898995501.176
Test Loss of 55566331335.032150, Test MSE of 55566330554.615623
Epoch 33: training loss 53781219915.294
Test Loss of 53654220836.478371, Test MSE of 53654221386.433495
Epoch 34: training loss 51160801280.000
Test Loss of 49741601225.637749, Test MSE of 49741602421.459404
Epoch 35: training loss 48283491456.000
Test Loss of 47081193955.693733, Test MSE of 47081192878.638145
Epoch 36: training loss 45869372175.059
Test Loss of 45748599713.487854, Test MSE of 45748598905.240746
Epoch 37: training loss 43141809039.059
Test Loss of 43258345942.428871, Test MSE of 43258345385.397614
Epoch 38: training loss 40830304722.824
Test Loss of 41800581349.292618, Test MSE of 41800580592.088699
Epoch 39: training loss 38677098834.824
Test Loss of 41543777481.578537, Test MSE of 41543777927.171600
Epoch 40: training loss 36410261925.647
Test Loss of 37757448868.981728, Test MSE of 37757448948.920273
Epoch 41: training loss 34570904169.412
Test Loss of 37056152672.880867, Test MSE of 37056154154.357475
Epoch 42: training loss 33093692980.706
Test Loss of 36330322867.490166, Test MSE of 36330322329.765213
Epoch 43: training loss 31170124310.588
Test Loss of 37134927302.084663, Test MSE of 37134926894.456596
Epoch 44: training loss 29470329750.588
Test Loss of 37013352449.421234, Test MSE of 37013353194.249985
Epoch 45: training loss 27633868837.647
Test Loss of 31508125504.133240, Test MSE of 31508126502.056889
Epoch 46: training loss 26143257991.529
Test Loss of 34072664521.637753, Test MSE of 34072665349.829407
Epoch 47: training loss 25216456192.000
Test Loss of 29178273272.301643, Test MSE of 29178273855.203888
Epoch 48: training loss 23515835949.176
Test Loss of 32370834264.531113, Test MSE of 32370833888.347649
Epoch 49: training loss 22879203712.000
Test Loss of 29027936557.538746, Test MSE of 29027936851.962811
Epoch 50: training loss 21907171489.882
Test Loss of 29812406544.877167, Test MSE of 29812407018.612301
Epoch 51: training loss 20664610066.824
Test Loss of 30311887109.744160, Test MSE of 30311886834.367783
Epoch 52: training loss 20045176282.353
Test Loss of 30321613239.635437, Test MSE of 30321612893.340916
Epoch 53: training loss 18981859640.471
Test Loss of 26567692768.614388, Test MSE of 26567693577.845737
Epoch 54: training loss 18282777298.824
Test Loss of 27483395527.505898, Test MSE of 27483395552.812416
Epoch 55: training loss 17792370341.647
Test Loss of 26817367183.307888, Test MSE of 26817366586.192947
Epoch 56: training loss 16757396547.765
Test Loss of 26204842054.351147, Test MSE of 26204842991.069180
Epoch 57: training loss 16359529950.118
Test Loss of 26602503665.432339, Test MSE of 26602503970.827942
Epoch 58: training loss 16084646140.235
Test Loss of 26865222408.705067, Test MSE of 26865221213.379211
Epoch 59: training loss 15317750196.706
Test Loss of 26359707805.993984, Test MSE of 26359707835.329071
Epoch 60: training loss 14708390309.647
Test Loss of 25377841339.603054, Test MSE of 25377840614.458458
Epoch 61: training loss 14468427666.824
Test Loss of 24807923449.071480, Test MSE of 24807923030.191387
Epoch 62: training loss 13731933108.706
Test Loss of 24897341355.199631, Test MSE of 24897342086.005405
Epoch 63: training loss 13420479439.059
Test Loss of 26934297188.789268, Test MSE of 26934296944.497349
Epoch 64: training loss 13250711232.000
Test Loss of 25963574502.713856, Test MSE of 25963574172.710060
Epoch 65: training loss 12776677097.412
Test Loss of 24935739230.216053, Test MSE of 24935739206.474537
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23807627504.201378, 'MSE - std': 1128111702.273159, 'R2 - mean': 0.8226837636760806, 'R2 - std': 0.0007083860655574536} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005666 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927837816.471
Test Loss of 447259818171.129333, Test MSE of 447259814480.402893
Epoch 2: training loss 421908344229.647
Test Loss of 447241632999.898193, Test MSE of 447241641758.467346
Epoch 3: training loss 421881760105.412
Test Loss of 447217242207.696533, Test MSE of 447217246355.268799
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899651674.353
Test Loss of 447224878372.063843, Test MSE of 447224888426.755249
Epoch 2: training loss 421890099200.000
Test Loss of 447225174959.226440, Test MSE of 447225174013.624146
Epoch 3: training loss 421889661409.882
Test Loss of 447225236174.671265, Test MSE of 447225231605.571838
Epoch 4: training loss 421889321682.824
Test Loss of 447225209265.950500, Test MSE of 447225206964.664673
Epoch 5: training loss 415257588916.706
Test Loss of 425796543163.247742, Test MSE of 425796540220.763245
Epoch 6: training loss 370251653842.824
Test Loss of 358562365242.922058, Test MSE of 358562364375.212341
Epoch 7: training loss 291792133300.706
Test Loss of 271428509633.465637, Test MSE of 271428508832.186554
Epoch 8: training loss 213187784463.059
Test Loss of 200123317358.619476, Test MSE of 200123318690.140137
Epoch 9: training loss 154096061078.588
Test Loss of 149212194649.715485, Test MSE of 149212195490.126831
Epoch 10: training loss 134630882364.235
Test Loss of 140065858034.379822, Test MSE of 140065858502.801636
Epoch 11: training loss 131835263126.588
Test Loss of 137511409212.520935, Test MSE of 137511411428.765717
Epoch 12: training loss 128404700762.353
Test Loss of 133883361929.504517, Test MSE of 133883362197.587814
Epoch 13: training loss 125117314740.706
Test Loss of 128896111809.524872, Test MSE of 128896113503.858749
Epoch 14: training loss 121821056060.235
Test Loss of 126814516582.388153, Test MSE of 126814512585.404831
Epoch 15: training loss 117780858428.235
Test Loss of 123451259415.805695, Test MSE of 123451261075.424072
Epoch 16: training loss 114633085500.235
Test Loss of 119910996941.546143, Test MSE of 119910998432.951904
Epoch 17: training loss 110255713430.588
Test Loss of 117028678223.707611, Test MSE of 117028678449.502518
Epoch 18: training loss 107969340536.471
Test Loss of 111259127736.938232, Test MSE of 111259127712.114792
Epoch 19: training loss 102295574016.000
Test Loss of 109405438418.165161, Test MSE of 109405437884.361511
Epoch 20: training loss 99475062603.294
Test Loss of 105727877917.786728, Test MSE of 105727880658.048157
Epoch 21: training loss 96766363467.294
Test Loss of 100530689844.763351, Test MSE of 100530690592.792648
Epoch 22: training loss 91237480869.647
Test Loss of 95396652969.304657, Test MSE of 95396652978.003159
Epoch 23: training loss 88113975130.353
Test Loss of 94483588216.568130, Test MSE of 94483587059.996185
Epoch 24: training loss 85133232188.235
Test Loss of 91108237503.393021, Test MSE of 91108236906.532669
Epoch 25: training loss 81541554386.824
Test Loss of 87139131035.743698, Test MSE of 87139129316.065781
Epoch 26: training loss 78185501244.235
Test Loss of 83291598043.580841, Test MSE of 83291599074.466339
Epoch 27: training loss 73660256752.941
Test Loss of 76736838588.491333, Test MSE of 76736840462.881836
Epoch 28: training loss 70829780208.941
Test Loss of 75928569229.235260, Test MSE of 75928569932.894943
Epoch 29: training loss 68962700227.765
Test Loss of 71556357264.729126, Test MSE of 71556357529.994080
Epoch 30: training loss 64673008564.706
Test Loss of 70916406331.455002, Test MSE of 70916406801.674362
Epoch 31: training loss 62740422746.353
Test Loss of 66475847434.600044, Test MSE of 66475847494.600769
Epoch 32: training loss 57840660894.118
Test Loss of 63086716991.008095, Test MSE of 63086718381.900429
Epoch 33: training loss 55931501312.000
Test Loss of 63681012045.516541, Test MSE of 63681011648.562569
Epoch 34: training loss 53221884393.412
Test Loss of 55485697599.837151, Test MSE of 55485697914.957420
Epoch 35: training loss 50849509601.882
Test Loss of 55066560665.967155, Test MSE of 55066561096.180664
Epoch 36: training loss 47635871984.941
Test Loss of 54793744098.805458, Test MSE of 54793743868.525536
Epoch 37: training loss 45738972581.647
Test Loss of 49960999842.909088, Test MSE of 49961000697.468712
Epoch 38: training loss 43342952124.235
Test Loss of 48886623205.233406, Test MSE of 48886622879.077171
Epoch 39: training loss 41482103627.294
Test Loss of 42783447515.166321, Test MSE of 42783448035.224609
Epoch 40: training loss 39366165699.765
Test Loss of 42901951073.236176, Test MSE of 42901950960.645714
Epoch 41: training loss 37106100374.588
Test Loss of 41019846150.040253, Test MSE of 41019845538.127922
Epoch 42: training loss 35139467218.824
Test Loss of 36729252196.730049, Test MSE of 36729252380.894600
Epoch 43: training loss 33857631706.353
Test Loss of 38243054653.823730, Test MSE of 38243054441.558044
Epoch 44: training loss 31789781760.000
Test Loss of 35526117202.135551, Test MSE of 35526117434.159409
Epoch 45: training loss 30628709504.000
Test Loss of 37025058178.339119, Test MSE of 37025057957.094177
Epoch 46: training loss 29209144884.706
Test Loss of 32314453830.291927, Test MSE of 32314454009.789677
Epoch 47: training loss 27821263789.176
Test Loss of 28999306727.720566, Test MSE of 28999306537.257347
Epoch 48: training loss 26162804600.471
Test Loss of 32054088700.446911, Test MSE of 32054089036.517719
Epoch 49: training loss 25013510490.353
Test Loss of 28545635800.560722, Test MSE of 28545636759.544876
Epoch 50: training loss 24100016022.588
Test Loss of 31651918813.179737, Test MSE of 31651919330.019615
Epoch 51: training loss 23376150151.529
Test Loss of 26291941809.003006, Test MSE of 26291941592.376915
Epoch 52: training loss 22083602375.529
Test Loss of 27182134463.629887, Test MSE of 27182134114.086021
Epoch 53: training loss 20953432131.765
Test Loss of 26591584189.201942, Test MSE of 26591584535.507530
Epoch 54: training loss 20131064222.118
Test Loss of 27526005491.149666, Test MSE of 27526005315.441528
Epoch 55: training loss 19754348608.000
Test Loss of 28173501829.655331, Test MSE of 28173501575.245056
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25262918861.21594, 'MSE - std': 2254810385.850582, 'R2 - mean': 0.8192727955508791, 'R2 - std': 0.0048583894087993905} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005485 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110770115.765
Test Loss of 410765302030.097168, Test MSE of 410765298616.186035
Epoch 2: training loss 430090083629.176
Test Loss of 410748463675.468750, Test MSE of 410748472160.552551
Epoch 3: training loss 430062874142.118
Test Loss of 410725664562.347046, Test MSE of 410725663295.336243
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078679883.294
Test Loss of 410729649051.068970, Test MSE of 410729648598.865601
Epoch 2: training loss 430068590471.529
Test Loss of 410730560762.669128, Test MSE of 410730555154.089966
Epoch 3: training loss 430068140152.471
Test Loss of 410730470874.328552, Test MSE of 410730472725.024353
Epoch 4: training loss 430067803557.647
Test Loss of 410730190200.714478, Test MSE of 410730195355.475708
Epoch 5: training loss 423438778608.941
Test Loss of 389389834214.411865, Test MSE of 389389835911.776672
Epoch 6: training loss 378383675994.353
Test Loss of 323687584712.559021, Test MSE of 323687584318.993652
Epoch 7: training loss 300134114605.176
Test Loss of 238511553771.505798, Test MSE of 238511550720.628052
Epoch 8: training loss 220986208135.529
Test Loss of 168604340434.391479, Test MSE of 168604340694.142853
Epoch 9: training loss 161883127808.000
Test Loss of 120443863896.729294, Test MSE of 120443862732.128006
Epoch 10: training loss 141283709349.647
Test Loss of 112016524909.223511, Test MSE of 112016525752.964325
Epoch 11: training loss 137450033121.882
Test Loss of 109474901418.943085, Test MSE of 109474903706.198166
Epoch 12: training loss 133968673701.647
Test Loss of 107193715721.950943, Test MSE of 107193715992.377029
Epoch 13: training loss 131301739550.118
Test Loss of 103974507513.839890, Test MSE of 103974507317.492218
Epoch 14: training loss 126851983480.471
Test Loss of 100731174453.308655, Test MSE of 100731174334.718307
Epoch 15: training loss 123615298740.706
Test Loss of 97296997550.852386, Test MSE of 97296996005.335846
Epoch 16: training loss 118976757729.882
Test Loss of 94969055007.392868, Test MSE of 94969054612.788040
Epoch 17: training loss 116428807951.059
Test Loss of 91707876271.918564, Test MSE of 91707875827.108200
Epoch 18: training loss 112572067900.235
Test Loss of 88229265715.057846, Test MSE of 88229265427.932266
Epoch 19: training loss 108674734110.118
Test Loss of 85264357285.967606, Test MSE of 85264355962.925522
Epoch 20: training loss 104122527382.588
Test Loss of 83045801328.185104, Test MSE of 83045802657.111099
Epoch 21: training loss 100185421281.882
Test Loss of 78504865213.897263, Test MSE of 78504865443.903976
Epoch 22: training loss 96693009573.647
Test Loss of 75720456147.457657, Test MSE of 75720456035.508026
Epoch 23: training loss 92530404336.941
Test Loss of 72954998437.138367, Test MSE of 72954999326.942474
Epoch 24: training loss 88461909127.529
Test Loss of 69706192847.666824, Test MSE of 69706193419.977127
Epoch 25: training loss 84951794748.235
Test Loss of 67713401537.095787, Test MSE of 67713400901.651855
Epoch 26: training loss 80197241178.353
Test Loss of 63786914961.473389, Test MSE of 63786914687.829964
Epoch 27: training loss 78278287450.353
Test Loss of 61682535672.773712, Test MSE of 61682536909.708229
Epoch 28: training loss 73912657573.647
Test Loss of 60009505177.410461, Test MSE of 60009506495.086029
Epoch 29: training loss 71220612111.059
Test Loss of 53639007067.572418, Test MSE of 53639007447.543503
Epoch 30: training loss 67160868126.118
Test Loss of 51439117571.198517, Test MSE of 51439117434.350822
Epoch 31: training loss 64719259708.235
Test Loss of 51535376624.718185, Test MSE of 51535377246.638977
Epoch 32: training loss 61248810300.235
Test Loss of 47511254822.974548, Test MSE of 47511256422.234512
Epoch 33: training loss 58248771689.412
Test Loss of 46357583486.282280, Test MSE of 46357583612.953896
Epoch 34: training loss 55646858368.000
Test Loss of 42997063105.688110, Test MSE of 42997063075.319672
Epoch 35: training loss 52756367427.765
Test Loss of 41222862024.440536, Test MSE of 41222862530.144394
Epoch 36: training loss 49557274774.588
Test Loss of 39032295429.686256, Test MSE of 39032295231.997856
Epoch 37: training loss 47995722337.882
Test Loss of 37937688284.105507, Test MSE of 37937687879.826210
Epoch 38: training loss 44967940939.294
Test Loss of 35325075253.190193, Test MSE of 35325075090.171944
Epoch 39: training loss 43072314051.765
Test Loss of 33078301049.188339, Test MSE of 33078300879.248829
Epoch 40: training loss 40811443802.353
Test Loss of 31492868449.495605, Test MSE of 31492868224.728382
Epoch 41: training loss 39477577321.412
Test Loss of 30372468596.686718, Test MSE of 30372468333.416214
Epoch 42: training loss 36933657208.471
Test Loss of 28478368961.806572, Test MSE of 28478368314.412296
Epoch 43: training loss 35002255728.941
Test Loss of 26619489093.775105, Test MSE of 26619489074.958275
Epoch 44: training loss 33091101921.882
Test Loss of 26456774837.012493, Test MSE of 26456774972.339333
Epoch 45: training loss 31532951055.059
Test Loss of 24253545330.317444, Test MSE of 24253545154.107777
Epoch 46: training loss 30442529558.588
Test Loss of 23918160451.050438, Test MSE of 23918160672.758793
Epoch 47: training loss 28429228788.706
Test Loss of 21997826217.639980, Test MSE of 21997826149.591133
Epoch 48: training loss 27460333116.235
Test Loss of 21508939049.106895, Test MSE of 21508939436.175064
Epoch 49: training loss 26039601163.294
Test Loss of 22735605960.914391, Test MSE of 22735605452.215584
Epoch 50: training loss 24757239488.000
Test Loss of 21940726117.286442, Test MSE of 21940726201.448902
Epoch 51: training loss 24098411599.059
Test Loss of 19963642078.711708, Test MSE of 19963642385.607105
Epoch 52: training loss 23188826849.882
Test Loss of 19879630684.993984, Test MSE of 19879630659.178989
Epoch 53: training loss 22143055634.824
Test Loss of 19678651909.923183, Test MSE of 19678651748.251614
Epoch 54: training loss 21462299198.118
Test Loss of 20104317328.881073, Test MSE of 20104317254.129871
Epoch 55: training loss 20614346518.588
Test Loss of 17390674825.062469, Test MSE of 17390674969.129536
Epoch 56: training loss 19934519789.176
Test Loss of 19300667002.491440, Test MSE of 19300666581.559914
Epoch 57: training loss 19173607361.882
Test Loss of 18658429660.579361, Test MSE of 18658429370.725700
Epoch 58: training loss 18651506627.765
Test Loss of 18816063306.987507, Test MSE of 18816063159.505444
Epoch 59: training loss 18094903134.118
Test Loss of 17981280889.069874, Test MSE of 17981281038.970268
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23442509405.65452, 'MSE - std': 3708746306.340862, 'R2 - mean': 0.8273523757745211, 'R2 - std': 0.014613069852240406} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043590234.353
Test Loss of 431612662379.801941, Test MSE of 431612660690.951477
Epoch 2: training loss 424023391412.706
Test Loss of 431592983338.291504, Test MSE of 431592984539.154114
Epoch 3: training loss 423996092416.000
Test Loss of 431566068027.587219, Test MSE of 431566068378.342712
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009733180.235
Test Loss of 431568371507.294800, Test MSE of 431568378430.419434
Epoch 2: training loss 424000407311.059
Test Loss of 431571479660.512695, Test MSE of 431571484724.491211
Epoch 3: training loss 423999841219.765
Test Loss of 431569793981.186462, Test MSE of 431569787702.527100
Epoch 4: training loss 423999389696.000
Test Loss of 431569942506.202698, Test MSE of 431569946357.185364
Epoch 5: training loss 417732905562.353
Test Loss of 410559969129.314209, Test MSE of 410559963245.155029
Epoch 6: training loss 373586722454.588
Test Loss of 344058669703.285522, Test MSE of 344058667882.158142
Epoch 7: training loss 295717453342.118
Test Loss of 257003282329.647400, Test MSE of 257003286886.409302
Epoch 8: training loss 216186028935.529
Test Loss of 184160169568.903290, Test MSE of 184160170249.889984
Epoch 9: training loss 157239296542.118
Test Loss of 132825459062.345215, Test MSE of 132825455742.911560
Epoch 10: training loss 139030606275.765
Test Loss of 123683617113.913925, Test MSE of 123683615263.967926
Epoch 11: training loss 133815927040.000
Test Loss of 120848661163.298477, Test MSE of 120848662289.616013
Epoch 12: training loss 131475028570.353
Test Loss of 117548157412.279495, Test MSE of 117548156621.850571
Epoch 13: training loss 128864756464.941
Test Loss of 113077890546.968994, Test MSE of 113077892035.539841
Epoch 14: training loss 123970086490.353
Test Loss of 110136862537.092087, Test MSE of 110136863256.286713
Epoch 15: training loss 120684696892.235
Test Loss of 107879474876.831100, Test MSE of 107879471981.573792
Epoch 16: training loss 116601731915.294
Test Loss of 104064350197.575195, Test MSE of 104064350711.336945
Epoch 17: training loss 113050395738.353
Test Loss of 99094762232.062927, Test MSE of 99094764621.769867
Epoch 18: training loss 108925908675.765
Test Loss of 96097911037.986115, Test MSE of 96097910229.086853
Epoch 19: training loss 104656736000.000
Test Loss of 92933126114.621002, Test MSE of 92933126401.461121
Epoch 20: training loss 101647588984.471
Test Loss of 87989293690.965286, Test MSE of 87989291493.104095
Epoch 21: training loss 96567653225.412
Test Loss of 86800183203.124481, Test MSE of 86800182781.884201
Epoch 22: training loss 93544273212.235
Test Loss of 83305264644.975479, Test MSE of 83305264399.033722
Epoch 23: training loss 89455264933.647
Test Loss of 78129791747.435440, Test MSE of 78129790095.673538
Epoch 24: training loss 86366912391.529
Test Loss of 74685086083.613144, Test MSE of 74685087261.819061
Epoch 25: training loss 82086628939.294
Test Loss of 71439476048.910690, Test MSE of 71439476264.289474
Epoch 26: training loss 79704237146.353
Test Loss of 67275075674.980103, Test MSE of 67275076120.068542
Epoch 27: training loss 75785027361.882
Test Loss of 62704633129.106895, Test MSE of 62704632412.477180
Epoch 28: training loss 71839691158.588
Test Loss of 62738619976.262840, Test MSE of 62738618982.576752
Epoch 29: training loss 68891350302.118
Test Loss of 58791378151.714951, Test MSE of 58791375682.806419
Epoch 30: training loss 65482470219.294
Test Loss of 57906216452.027763, Test MSE of 57906217970.488243
Epoch 31: training loss 62548941417.412
Test Loss of 52381729926.100876, Test MSE of 52381729524.507088
Epoch 32: training loss 59873786699.294
Test Loss of 49584053354.617310, Test MSE of 49584052734.026711
Epoch 33: training loss 56565956096.000
Test Loss of 47573613026.384079, Test MSE of 47573612957.125015
Epoch 34: training loss 53768608504.471
Test Loss of 46986649330.850533, Test MSE of 46986649858.051231
Epoch 35: training loss 51362026684.235
Test Loss of 44970804363.787132, Test MSE of 44970803348.065498
Epoch 36: training loss 48931018774.588
Test Loss of 42855154979.420639, Test MSE of 42855155171.163223
Epoch 37: training loss 46219004912.941
Test Loss of 39357376744.188805, Test MSE of 39357376950.067421
Epoch 38: training loss 43589785178.353
Test Loss of 38033440104.603424, Test MSE of 38033440483.596138
Epoch 39: training loss 41770024274.824
Test Loss of 34574171231.244797, Test MSE of 34574171259.194710
Epoch 40: training loss 40002526388.706
Test Loss of 36994032413.971306, Test MSE of 36994032998.269478
Epoch 41: training loss 38648845379.765
Test Loss of 34929216260.383156, Test MSE of 34929216563.879524
Epoch 42: training loss 35840423213.176
Test Loss of 31135764824.966221, Test MSE of 31135764314.208050
Epoch 43: training loss 33857943973.647
Test Loss of 30841206726.189728, Test MSE of 30841206898.302612
Epoch 44: training loss 32851403286.588
Test Loss of 28505888368.066635, Test MSE of 28505888397.230026
Epoch 45: training loss 30538134550.588
Test Loss of 28444792401.739937, Test MSE of 28444791985.017532
Epoch 46: training loss 29824039544.471
Test Loss of 29581408934.559925, Test MSE of 29581408164.179829
Epoch 47: training loss 28208781835.294
Test Loss of 24868414584.832947, Test MSE of 24868414692.844532
Epoch 48: training loss 26994142336.000
Test Loss of 24661438235.602036, Test MSE of 24661437919.392593
Epoch 49: training loss 25682810401.882
Test Loss of 26484224750.585838, Test MSE of 26484224521.954952
Epoch 50: training loss 24728181549.176
Test Loss of 24372407057.651089, Test MSE of 24372406840.815598
Epoch 51: training loss 23896271691.294
Test Loss of 23555698148.753353, Test MSE of 23555698023.257393
Epoch 52: training loss 22909381496.471
Test Loss of 24392665302.182323, Test MSE of 24392665201.843952
Epoch 53: training loss 21798885127.529
Test Loss of 22083420228.235077, Test MSE of 22083420218.447178
Epoch 54: training loss 20984472071.529
Test Loss of 25407070212.738548, Test MSE of 25407070573.304592
Epoch 55: training loss 20156984986.353
Test Loss of 20753858501.242016, Test MSE of 20753858722.707382
Epoch 56: training loss 19679460404.706
Test Loss of 20663960540.460899, Test MSE of 20663960261.530014
Epoch 57: training loss 18788752591.059
Test Loss of 21446586112.118465, Test MSE of 21446586190.171844
Epoch 58: training loss 18694259983.059
Test Loss of 20296951412.805183, Test MSE of 20296951176.049095
Epoch 59: training loss 17904093492.706
Test Loss of 22108873475.435448, Test MSE of 22108873619.849926
Epoch 60: training loss 17427758686.118
Test Loss of 21784136149.116150, Test MSE of 21784135759.861595
Epoch 61: training loss 16838859919.059
Test Loss of 22561145195.920406, Test MSE of 22561145167.731007
Epoch 62: training loss 16573507553.882
Test Loss of 19499368396.823692, Test MSE of 19499368289.119667
Epoch 63: training loss 16150139625.412
Test Loss of 19458753580.068485, Test MSE of 19458753456.543652
Epoch 64: training loss 15576428833.882
Test Loss of 21039341647.607590, Test MSE of 21039341822.739582
Epoch 65: training loss 15148140197.647
Test Loss of 21088268954.239704, Test MSE of 21088269228.633858
Epoch 66: training loss 15017168668.235
Test Loss of 20166940001.021748, Test MSE of 20166940040.244442
Epoch 67: training loss 14624557878.588
Test Loss of 20524868077.756596, Test MSE of 20524867514.918442
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22858981027.5073, 'MSE - std': 3516512591.225868, 'R2 - mean': 0.8312258060930071, 'R2 - std': 0.015193659800516578} 
 

Saving model.....
Results After CV: {'MSE - mean': 22858981027.5073, 'MSE - std': 3516512591.225868, 'R2 - mean': 0.8312258060930071, 'R2 - std': 0.015193659800516578}
Train time: 96.96426389919998
Inference time: 0.07389181520011334
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 14 finished with value: 22858981027.5073 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005570 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525461534.118
Test Loss of 418109737727.940796, Test MSE of 418109729974.543396
Epoch 2: training loss 427504310874.353
Test Loss of 418089868411.884338, Test MSE of 418089878026.525818
Epoch 3: training loss 427476547945.412
Test Loss of 418064652386.065247, Test MSE of 418064654372.705872
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427487888564.706
Test Loss of 418066351160.138794, Test MSE of 418066356527.444519
Epoch 2: training loss 427476807920.941
Test Loss of 418066736462.700928, Test MSE of 418066742522.858215
Epoch 3: training loss 427476482048.000
Test Loss of 418066311357.971802, Test MSE of 418066320567.330994
Epoch 4: training loss 427476221470.118
Test Loss of 418066591337.526733, Test MSE of 418066590960.936523
Epoch 5: training loss 427476048715.294
Test Loss of 418066388104.675476, Test MSE of 418066392246.399963
Epoch 6: training loss 419919049547.294
Test Loss of 393330710595.745544, Test MSE of 393330712539.778259
Epoch 7: training loss 367917158881.882
Test Loss of 317518664105.423096, Test MSE of 317518669523.169067
Epoch 8: training loss 281028992000.000
Test Loss of 226348823588.241486, Test MSE of 226348820731.376587
Epoch 9: training loss 202179379320.471
Test Loss of 160277798208.962280, Test MSE of 160277797614.329346
Epoch 10: training loss 157220488914.824
Test Loss of 129938938045.024292, Test MSE of 129938938142.275742
Epoch 11: training loss 140276561709.176
Test Loss of 117712449296.284988, Test MSE of 117712449032.144867
Epoch 12: training loss 135588327755.294
Test Loss of 113883727216.810547, Test MSE of 113883727967.970627
Epoch 13: training loss 131996283602.824
Test Loss of 111515707172.656021, Test MSE of 111515708839.846222
Epoch 14: training loss 127675323030.588
Test Loss of 107917796205.375900, Test MSE of 107917796870.022736
Epoch 15: training loss 125029186138.353
Test Loss of 104777777055.356003, Test MSE of 104777777914.975388
Epoch 16: training loss 119857772890.353
Test Loss of 101984698784.895676, Test MSE of 101984700090.380371
Epoch 17: training loss 115046185923.765
Test Loss of 98810969113.345367, Test MSE of 98810969723.581070
Epoch 18: training loss 111960675267.765
Test Loss of 94172818590.941483, Test MSE of 94172820137.087067
Epoch 19: training loss 108467735943.529
Test Loss of 90237836133.322235, Test MSE of 90237835154.363693
Epoch 20: training loss 105264296056.471
Test Loss of 89140692791.605835, Test MSE of 89140692929.956604
Epoch 21: training loss 100121237669.647
Test Loss of 85984016023.479996, Test MSE of 85984018045.672653
Epoch 22: training loss 96528833942.588
Test Loss of 80151190390.377060, Test MSE of 80151190083.887283
Epoch 23: training loss 92337299486.118
Test Loss of 78651310765.982880, Test MSE of 78651309836.053314
Epoch 24: training loss 88756411791.059
Test Loss of 75579884075.229233, Test MSE of 75579884277.603653
Epoch 25: training loss 85220021586.824
Test Loss of 72155653142.029144, Test MSE of 72155654781.550018
Epoch 26: training loss 82216003237.647
Test Loss of 68715674730.829514, Test MSE of 68715674232.940842
Epoch 27: training loss 77832724736.000
Test Loss of 66000040461.857040, Test MSE of 66000041646.252083
Epoch 28: training loss 74422131056.941
Test Loss of 63351207003.195930, Test MSE of 63351206853.804405
Epoch 29: training loss 71055383235.765
Test Loss of 62505555664.329399, Test MSE of 62505554688.864372
Epoch 30: training loss 67736548623.059
Test Loss of 57303478860.391396, Test MSE of 57303479442.103745
Epoch 31: training loss 64368539000.471
Test Loss of 54894371208.260933, Test MSE of 54894371065.836716
Epoch 32: training loss 60628038753.882
Test Loss of 51879240557.375893, Test MSE of 51879240204.601425
Epoch 33: training loss 57776680914.824
Test Loss of 46119096443.173721, Test MSE of 46119096286.812180
Epoch 34: training loss 55246400263.529
Test Loss of 47429700351.940781, Test MSE of 47429700421.706306
Epoch 35: training loss 52616709556.706
Test Loss of 43435318005.518387, Test MSE of 43435318420.923523
Epoch 36: training loss 49514810349.176
Test Loss of 41372835963.173721, Test MSE of 41372836574.098648
Epoch 37: training loss 46995187230.118
Test Loss of 38001887977.201019, Test MSE of 38001888851.461441
Epoch 38: training loss 44754326445.176
Test Loss of 36188028921.367569, Test MSE of 36188028818.793945
Epoch 39: training loss 42344436372.706
Test Loss of 36861187606.621330, Test MSE of 36861188105.213707
Epoch 40: training loss 40703329076.706
Test Loss of 33012869108.156372, Test MSE of 33012869254.878498
Epoch 41: training loss 38566046682.353
Test Loss of 29634093532.587555, Test MSE of 29634094228.996986
Epoch 42: training loss 36250311815.529
Test Loss of 30848071138.509369, Test MSE of 30848071046.868130
Epoch 43: training loss 34169270283.294
Test Loss of 30049731631.611382, Test MSE of 30049732189.272854
Epoch 44: training loss 33083144346.353
Test Loss of 26830013460.134167, Test MSE of 26830013526.122089
Epoch 45: training loss 31569806840.471
Test Loss of 28743110477.871849, Test MSE of 28743110750.809093
Epoch 46: training loss 29463362281.412
Test Loss of 24459807824.299793, Test MSE of 24459807367.348244
Epoch 47: training loss 28414530804.706
Test Loss of 23258751152.706917, Test MSE of 23258750848.027725
Epoch 48: training loss 27204921088.000
Test Loss of 24004605352.712467, Test MSE of 24004605522.745121
Epoch 49: training loss 25873904677.647
Test Loss of 20209174733.605366, Test MSE of 20209174541.948135
Epoch 50: training loss 24459707218.824
Test Loss of 24474851382.480686, Test MSE of 24474851107.389118
Epoch 51: training loss 23691578522.353
Test Loss of 21659450451.142262, Test MSE of 21659450432.241364
Epoch 52: training loss 22581809268.706
Test Loss of 21602948057.389774, Test MSE of 21602948033.446957
Epoch 53: training loss 21465171888.941
Test Loss of 19614036924.491325, Test MSE of 19614036785.707134
Epoch 54: training loss 20976621413.647
Test Loss of 21253192559.744621, Test MSE of 21253192198.620350
Epoch 55: training loss 20186290394.353
Test Loss of 19566171637.459171, Test MSE of 19566171795.462627
Epoch 56: training loss 19489844476.235
Test Loss of 18662241960.061069, Test MSE of 18662242099.209866
Epoch 57: training loss 18602026955.294
Test Loss of 17119002172.047190, Test MSE of 17119002000.803308
Epoch 58: training loss 18277636370.824
Test Loss of 19299419533.235252, Test MSE of 19299419297.736374
Epoch 59: training loss 17668438046.118
Test Loss of 17376923729.957901, Test MSE of 17376923682.721958
Epoch 60: training loss 16620487017.412
Test Loss of 18382333523.260700, Test MSE of 18382333071.629200
Epoch 61: training loss 16067409242.353
Test Loss of 17380733854.171638, Test MSE of 17380734037.705181
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17380734037.70518, 'MSE - std': 0.0, 'R2 - mean': 0.8646543382530912, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917560169.412
Test Loss of 424556049595.839905, Test MSE of 424556052151.152222
Epoch 2: training loss 427896364574.118
Test Loss of 424539223997.201965, Test MSE of 424539218780.954834
Epoch 3: training loss 427868160963.765
Test Loss of 424516717376.370117, Test MSE of 424516715300.172668
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887777189.647
Test Loss of 424524170728.194336, Test MSE of 424524172768.225586
Epoch 2: training loss 427875220540.235
Test Loss of 424524965626.966431, Test MSE of 424524967902.172424
Epoch 3: training loss 427874904907.294
Test Loss of 424525073458.927612, Test MSE of 424525076887.904907
Epoch 4: training loss 427874629511.529
Test Loss of 424524756110.360413, Test MSE of 424524760538.541016
Epoch 5: training loss 427874409050.353
Test Loss of 424523760480.111023, Test MSE of 424523760207.452087
Epoch 6: training loss 419869430482.824
Test Loss of 399250177574.254944, Test MSE of 399250180975.109314
Epoch 7: training loss 366599269918.118
Test Loss of 323606196602.048584, Test MSE of 323606196852.691528
Epoch 8: training loss 279652753889.882
Test Loss of 234744547459.227386, Test MSE of 234744548528.237213
Epoch 9: training loss 199778116367.059
Test Loss of 169938422044.720795, Test MSE of 169938420292.978607
Epoch 10: training loss 155462677564.235
Test Loss of 140290017187.619720, Test MSE of 140290017546.255219
Epoch 11: training loss 138442794014.118
Test Loss of 129339533710.419617, Test MSE of 129339536454.224609
Epoch 12: training loss 133268604265.412
Test Loss of 126142968307.327316, Test MSE of 126142967331.187363
Epoch 13: training loss 128601685323.294
Test Loss of 121660902450.927597, Test MSE of 121660902720.291977
Epoch 14: training loss 125997755843.765
Test Loss of 117967596030.223450, Test MSE of 117967596016.491928
Epoch 15: training loss 120693167224.471
Test Loss of 115905850761.919037, Test MSE of 115905851081.106308
Epoch 16: training loss 117136845101.176
Test Loss of 110785224588.406204, Test MSE of 110785223752.888702
Epoch 17: training loss 112798952899.765
Test Loss of 108123714779.107101, Test MSE of 108123714291.325623
Epoch 18: training loss 109173837944.471
Test Loss of 105045622608.714325, Test MSE of 105045619959.045349
Epoch 19: training loss 104042347279.059
Test Loss of 100487125567.600281, Test MSE of 100487123266.274750
Epoch 20: training loss 100766922390.588
Test Loss of 96240944339.527176, Test MSE of 96240944536.359726
Epoch 21: training loss 97732134972.235
Test Loss of 93075510724.189682, Test MSE of 93075510019.043365
Epoch 22: training loss 92107197500.235
Test Loss of 88988857318.654633, Test MSE of 88988859552.009079
Epoch 23: training loss 88467600655.059
Test Loss of 82779146942.563965, Test MSE of 82779147495.172241
Epoch 24: training loss 84699488993.882
Test Loss of 80016480606.097610, Test MSE of 80016483048.677277
Epoch 25: training loss 81329412216.471
Test Loss of 76931772000.051819, Test MSE of 76931771815.357712
Epoch 26: training loss 76972342377.412
Test Loss of 74137609238.029144, Test MSE of 74137607705.830399
Epoch 27: training loss 72904555188.706
Test Loss of 69724687839.193146, Test MSE of 69724687303.026077
Epoch 28: training loss 69632001310.118
Test Loss of 67005675188.852188, Test MSE of 67005673981.253036
Epoch 29: training loss 66255459855.059
Test Loss of 62358768797.993988, Test MSE of 62358768847.680679
Epoch 30: training loss 62547387843.765
Test Loss of 59792301436.654175, Test MSE of 59792301286.609924
Epoch 31: training loss 60045095047.529
Test Loss of 61152273958.728661, Test MSE of 61152273685.240067
Epoch 32: training loss 56582804781.176
Test Loss of 54851475230.023598, Test MSE of 54851475626.545647
Epoch 33: training loss 53198488124.235
Test Loss of 50933418530.701828, Test MSE of 50933416817.703819
Epoch 34: training loss 51042351495.529
Test Loss of 48634114710.769371, Test MSE of 48634115102.417618
Epoch 35: training loss 47832953705.412
Test Loss of 45649655390.156837, Test MSE of 45649655905.948906
Epoch 36: training loss 44981803241.412
Test Loss of 47679897157.995834, Test MSE of 47679897768.248352
Epoch 37: training loss 42585715546.353
Test Loss of 41122643063.146889, Test MSE of 41122642789.889259
Epoch 38: training loss 39031387015.529
Test Loss of 38285168685.479530, Test MSE of 38285167120.605804
Epoch 39: training loss 38248780122.353
Test Loss of 41083934062.915565, Test MSE of 41083934680.830879
Epoch 40: training loss 35753904873.412
Test Loss of 35861554143.548462, Test MSE of 35861554793.570503
Epoch 41: training loss 33549404280.471
Test Loss of 31502989996.561646, Test MSE of 31502990154.486320
Epoch 42: training loss 32274875136.000
Test Loss of 33809631742.697201, Test MSE of 33809631178.804001
Epoch 43: training loss 30195491584.000
Test Loss of 28965324396.842934, Test MSE of 28965324594.287075
Epoch 44: training loss 28212475783.529
Test Loss of 31830894183.157993, Test MSE of 31830893703.929142
Epoch 45: training loss 26750066221.176
Test Loss of 29405875726.567661, Test MSE of 29405875572.856678
Epoch 46: training loss 25487765104.941
Test Loss of 27486222665.252834, Test MSE of 27486222602.162148
Epoch 47: training loss 24241714560.000
Test Loss of 26151571814.151283, Test MSE of 26151571778.715714
Epoch 48: training loss 22823680463.059
Test Loss of 28357506636.628269, Test MSE of 28357506926.148418
Epoch 49: training loss 21676175834.353
Test Loss of 30116132233.445293, Test MSE of 30116132600.290478
Epoch 50: training loss 20719318396.235
Test Loss of 27480935016.342354, Test MSE of 27480935231.406559
Epoch 51: training loss 19950171704.471
Test Loss of 27333981133.072403, Test MSE of 27333980612.913155
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22357357325.309166, 'MSE - std': 4976623287.603987, 'R2 - mean': 0.8347539322410784, 'R2 - std': 0.029900406012012826} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003591 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927210526.118
Test Loss of 447257884229.995850, Test MSE of 447257887609.032043
Epoch 2: training loss 421907215058.824
Test Loss of 447239306570.437195, Test MSE of 447239298117.501648
Epoch 3: training loss 421880479864.471
Test Loss of 447214760571.765930, Test MSE of 447214760855.287354
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421903337953.882
Test Loss of 447224472573.394409, Test MSE of 447224474908.831116
Epoch 2: training loss 421886339915.294
Test Loss of 447224415375.307861, Test MSE of 447224411820.432129
Epoch 3: training loss 421885999585.882
Test Loss of 447224625126.891541, Test MSE of 447224626338.917969
Epoch 4: training loss 421885822976.000
Test Loss of 447224863181.901428, Test MSE of 447224859088.233032
Epoch 5: training loss 421885696481.882
Test Loss of 447225125143.983337, Test MSE of 447225135081.273560
Epoch 6: training loss 413760778842.353
Test Loss of 421348271207.513306, Test MSE of 421348276636.294128
Epoch 7: training loss 360594770160.941
Test Loss of 343639235298.568604, Test MSE of 343639231268.428040
Epoch 8: training loss 273600548020.706
Test Loss of 249591551794.868378, Test MSE of 249591557989.032837
Epoch 9: training loss 195026758174.118
Test Loss of 183917117000.364563, Test MSE of 183917114758.724396
Epoch 10: training loss 151319814896.941
Test Loss of 153040473919.659485, Test MSE of 153040474873.237854
Epoch 11: training loss 135516680161.882
Test Loss of 139386551164.061981, Test MSE of 139386555249.582916
Epoch 12: training loss 130882578010.353
Test Loss of 135715473736.305344, Test MSE of 135715474192.959000
Epoch 13: training loss 126439767943.529
Test Loss of 132969194072.708771, Test MSE of 132969194382.117950
Epoch 14: training loss 124709515474.824
Test Loss of 130270271199.489243, Test MSE of 130270273138.686386
Epoch 15: training loss 119813112711.529
Test Loss of 126892225424.433029, Test MSE of 126892228282.885468
Epoch 16: training loss 117637465178.353
Test Loss of 122275678562.835068, Test MSE of 122275676079.553650
Epoch 17: training loss 112999537332.706
Test Loss of 117523592944.544067, Test MSE of 117523592074.509506
Epoch 18: training loss 108271335845.647
Test Loss of 116054967048.941940, Test MSE of 116054970235.082642
Epoch 19: training loss 104082730134.588
Test Loss of 110478861275.284760, Test MSE of 110478862253.588333
Epoch 20: training loss 100362434288.941
Test Loss of 106734745593.841309, Test MSE of 106734745388.548920
Epoch 21: training loss 96611142023.529
Test Loss of 102285855802.507523, Test MSE of 102285858953.172775
Epoch 22: training loss 92765035821.176
Test Loss of 99077930166.628723, Test MSE of 99077932436.450256
Epoch 23: training loss 89864906872.471
Test Loss of 95920845437.660889, Test MSE of 95920846363.341553
Epoch 24: training loss 86412548216.471
Test Loss of 91123910847.866760, Test MSE of 91123911839.797394
Epoch 25: training loss 82582359476.706
Test Loss of 88323313221.522095, Test MSE of 88323313244.910019
Epoch 26: training loss 79536314262.588
Test Loss of 86935015000.471893, Test MSE of 86935016882.712891
Epoch 27: training loss 74629314590.118
Test Loss of 79155572990.874863, Test MSE of 79155573955.524277
Epoch 28: training loss 70643224380.235
Test Loss of 77343967886.241959, Test MSE of 77343967189.045074
Epoch 29: training loss 68297339060.706
Test Loss of 72225845152.303497, Test MSE of 72225844316.254654
Epoch 30: training loss 65101214539.294
Test Loss of 69482515988.252609, Test MSE of 69482516367.897949
Epoch 31: training loss 61940321716.706
Test Loss of 67949341599.119133, Test MSE of 67949343038.595047
Epoch 32: training loss 58728102957.176
Test Loss of 61170053312.340508, Test MSE of 61170053839.963860
Epoch 33: training loss 56177337532.235
Test Loss of 60987567291.366180, Test MSE of 60987567160.071053
Epoch 34: training loss 53040637869.176
Test Loss of 58384633642.814713, Test MSE of 58384634522.220505
Epoch 35: training loss 50407280097.882
Test Loss of 52381777056.836456, Test MSE of 52381776329.439751
Epoch 36: training loss 47443680150.588
Test Loss of 52045327848.668053, Test MSE of 52045326875.414764
Epoch 37: training loss 45256903868.235
Test Loss of 48959972352.236870, Test MSE of 48959973026.535782
Epoch 38: training loss 42509561976.471
Test Loss of 42540075062.480682, Test MSE of 42540074096.867844
Epoch 39: training loss 40653075328.000
Test Loss of 45255018575.115433, Test MSE of 45255018838.334129
Epoch 40: training loss 37907753095.529
Test Loss of 45130221685.962524, Test MSE of 45130221681.891953
Epoch 41: training loss 36332938857.412
Test Loss of 39088902954.104095, Test MSE of 39088904281.824165
Epoch 42: training loss 34381995565.176
Test Loss of 39111087456.703217, Test MSE of 39111087764.783844
Epoch 43: training loss 32657888963.765
Test Loss of 35771596274.142960, Test MSE of 35771596203.753151
Epoch 44: training loss 31076401483.294
Test Loss of 35094756932.574600, Test MSE of 35094757318.866920
Epoch 45: training loss 29837663570.824
Test Loss of 38394482604.620865, Test MSE of 38394482461.578468
Epoch 46: training loss 28282511141.647
Test Loss of 32881730322.416840, Test MSE of 32881730691.867180
Epoch 47: training loss 26829853688.471
Test Loss of 30478756646.787880, Test MSE of 30478756666.124603
Epoch 48: training loss 25498632756.706
Test Loss of 30807204325.114967, Test MSE of 30807204440.563431
Epoch 49: training loss 24202332065.882
Test Loss of 28376591494.306732, Test MSE of 28376591416.816113
Epoch 50: training loss 22971667934.118
Test Loss of 30967893615.211658, Test MSE of 30967893630.128063
Epoch 51: training loss 22581760515.765
Test Loss of 27275016933.411057, Test MSE of 27275016729.889576
Epoch 52: training loss 21150164931.765
Test Loss of 26595060969.319454, Test MSE of 26595060553.173695
Epoch 53: training loss 20270854264.471
Test Loss of 27188072577.332409, Test MSE of 27188072822.865517
Epoch 54: training loss 19739035945.412
Test Loss of 24706140087.043259, Test MSE of 24706140134.942806
Epoch 55: training loss 18610340709.647
Test Loss of 25946660227.286606, Test MSE of 25946661327.287624
Epoch 56: training loss 18381255962.353
Test Loss of 25860452386.109646, Test MSE of 25860452511.160568
Epoch 57: training loss 17592746763.294
Test Loss of 23766582614.280823, Test MSE of 23766582947.939114
Epoch 58: training loss 17044583766.588
Test Loss of 23280909410.302105, Test MSE of 23280909622.811512
Epoch 59: training loss 16357041456.941
Test Loss of 23334883391.481842, Test MSE of 23334883712.859505
Epoch 60: training loss 15958506115.765
Test Loss of 23663605129.919037, Test MSE of 23663604970.421455
Epoch 61: training loss 15402432225.882
Test Loss of 21630192117.222298, Test MSE of 21630192185.638931
Epoch 62: training loss 15006056779.294
Test Loss of 23999975253.451771, Test MSE of 23999975060.619530
Epoch 63: training loss 14645566057.412
Test Loss of 24311555662.049503, Test MSE of 24311555796.462337
Epoch 64: training loss 14003538300.235
Test Loss of 23361343171.064537, Test MSE of 23361343549.288532
Epoch 65: training loss 13641460773.647
Test Loss of 22922940794.996067, Test MSE of 22922940767.431274
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22545885139.34987, 'MSE - std': 4072133556.808045, 'R2 - mean': 0.8389704535246136, 'R2 - std': 0.025131274472082635} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005451 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110852035.765
Test Loss of 410764345162.039795, Test MSE of 410764343366.052979
Epoch 2: training loss 430090212291.765
Test Loss of 410745246664.559021, Test MSE of 410745242104.539917
Epoch 3: training loss 430063001840.941
Test Loss of 410720651270.633972, Test MSE of 410720650128.322815
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076565383.529
Test Loss of 410727297033.477112, Test MSE of 410727297666.794800
Epoch 2: training loss 430063915730.824
Test Loss of 410727238973.008789, Test MSE of 410727236920.819824
Epoch 3: training loss 430063683222.588
Test Loss of 410727061304.033325, Test MSE of 410727057269.426331
Epoch 4: training loss 430063519141.647
Test Loss of 410726710894.171204, Test MSE of 410726712931.045471
Epoch 5: training loss 430063362048.000
Test Loss of 410726826772.494202, Test MSE of 410726833070.377197
Epoch 6: training loss 421994334569.412
Test Loss of 384819123182.467346, Test MSE of 384819119117.208435
Epoch 7: training loss 368520076348.235
Test Loss of 307982780271.000488, Test MSE of 307982787337.637573
Epoch 8: training loss 281224379452.235
Test Loss of 218462234482.791290, Test MSE of 218462232518.490173
Epoch 9: training loss 203497633792.000
Test Loss of 153383850709.945404, Test MSE of 153383850027.771301
Epoch 10: training loss 160212054016.000
Test Loss of 122915716195.035629, Test MSE of 122915719221.957855
Epoch 11: training loss 141533614531.765
Test Loss of 111908995458.665436, Test MSE of 111908997319.560471
Epoch 12: training loss 137412619926.588
Test Loss of 108522179806.237854, Test MSE of 108522178792.990311
Epoch 13: training loss 132712064060.235
Test Loss of 105462656760.536789, Test MSE of 105462657364.663712
Epoch 14: training loss 131164817197.176
Test Loss of 103143103120.762604, Test MSE of 103143104260.288116
Epoch 15: training loss 127215468001.882
Test Loss of 101060895891.368805, Test MSE of 101060895143.914185
Epoch 16: training loss 122913707459.765
Test Loss of 96827844042.217499, Test MSE of 96827845244.726105
Epoch 17: training loss 119214006633.412
Test Loss of 93285048877.253128, Test MSE of 93285048509.370087
Epoch 18: training loss 114250984357.647
Test Loss of 91596801470.844986, Test MSE of 91596802985.932312
Epoch 19: training loss 111222858902.588
Test Loss of 86975932919.233688, Test MSE of 86975932410.528168
Epoch 20: training loss 106440506729.412
Test Loss of 84688055264.251740, Test MSE of 84688053249.042953
Epoch 21: training loss 102087166614.588
Test Loss of 81803376409.706619, Test MSE of 81803375597.368439
Epoch 22: training loss 99210219595.294
Test Loss of 77767044416.799637, Test MSE of 77767045515.831619
Epoch 23: training loss 94539904557.176
Test Loss of 74526506875.320679, Test MSE of 74526507665.121185
Epoch 24: training loss 91435691776.000
Test Loss of 72302512631.233688, Test MSE of 72302511650.510956
Epoch 25: training loss 86981760587.294
Test Loss of 69380408952.596024, Test MSE of 69380409437.928787
Epoch 26: training loss 83217078272.000
Test Loss of 66807594591.007866, Test MSE of 66807594454.730995
Epoch 27: training loss 80404973643.294
Test Loss of 63394554553.514114, Test MSE of 63394554393.014015
Epoch 28: training loss 75017858981.647
Test Loss of 60129194050.339661, Test MSE of 60129194050.018219
Epoch 29: training loss 72404058639.059
Test Loss of 58707598450.198982, Test MSE of 58707597314.620979
Epoch 30: training loss 69983880613.647
Test Loss of 54648863771.009720, Test MSE of 54648864216.964592
Epoch 31: training loss 65933463100.235
Test Loss of 52335899685.434525, Test MSE of 52335899291.363747
Epoch 32: training loss 63769999947.294
Test Loss of 51571476238.334106, Test MSE of 51571476940.763863
Epoch 33: training loss 59973650627.765
Test Loss of 49051230770.939377, Test MSE of 49051230921.035789
Epoch 34: training loss 56572992165.647
Test Loss of 44417710219.313278, Test MSE of 44417710256.944092
Epoch 35: training loss 54039921814.588
Test Loss of 43011790739.013420, Test MSE of 43011790979.985580
Epoch 36: training loss 51063021952.000
Test Loss of 41472568521.388245, Test MSE of 41472568659.378426
Epoch 37: training loss 48105616308.706
Test Loss of 40485900314.535866, Test MSE of 40485900021.683807
Epoch 38: training loss 45761404054.588
Test Loss of 36746224501.160576, Test MSE of 36746224202.267540
Epoch 39: training loss 44120258266.353
Test Loss of 35867451062.197128, Test MSE of 35867450758.633034
Epoch 40: training loss 41610780995.765
Test Loss of 34072995351.929661, Test MSE of 34072995899.987579
Epoch 41: training loss 39764871559.529
Test Loss of 33994854741.175381, Test MSE of 33994854505.205288
Epoch 42: training loss 37350512218.353
Test Loss of 29903267192.714485, Test MSE of 29903266866.855824
Epoch 43: training loss 35749424489.412
Test Loss of 29009823553.510410, Test MSE of 29009823492.966816
Epoch 44: training loss 33735388528.941
Test Loss of 30340788019.294769, Test MSE of 30340788954.261230
Epoch 45: training loss 32399371399.529
Test Loss of 29392126533.893566, Test MSE of 29392126319.838955
Epoch 46: training loss 30570446757.647
Test Loss of 27532185534.608051, Test MSE of 27532185713.710976
Epoch 47: training loss 28875536888.471
Test Loss of 26406946857.699215, Test MSE of 26406947112.825939
Epoch 48: training loss 27667686576.941
Test Loss of 24167504290.887550, Test MSE of 24167504189.058445
Epoch 49: training loss 26391957345.882
Test Loss of 24382570572.764462, Test MSE of 24382571301.768539
Epoch 50: training loss 25555252664.471
Test Loss of 22484787547.809349, Test MSE of 22484787582.510265
Epoch 51: training loss 24031766610.824
Test Loss of 22229204943.666821, Test MSE of 22229204813.404366
Epoch 52: training loss 23313858597.647
Test Loss of 23113356131.154095, Test MSE of 23113356240.239578
Epoch 53: training loss 22066830347.294
Test Loss of 20722922266.180473, Test MSE of 20722922215.590454
Epoch 54: training loss 21611376316.235
Test Loss of 21434433686.685795, Test MSE of 21434433358.249039
Epoch 55: training loss 20501717089.882
Test Loss of 22646332155.853771, Test MSE of 22646332274.986290
Epoch 56: training loss 19850129575.529
Test Loss of 22313669958.485886, Test MSE of 22313669979.748882
Epoch 57: training loss 18894587873.882
Test Loss of 19573484565.323460, Test MSE of 19573484765.156578
Epoch 58: training loss 18316661306.353
Test Loss of 20959479986.169365, Test MSE of 20959480420.296520
Epoch 59: training loss 17852331151.059
Test Loss of 18802698140.490513, Test MSE of 18802698185.397537
Epoch 60: training loss 17011152771.765
Test Loss of 18469341231.385471, Test MSE of 18469341279.759724
Epoch 61: training loss 16890636623.059
Test Loss of 19241099083.935215, Test MSE of 19241099217.158653
Epoch 62: training loss 16372780284.235
Test Loss of 19370024812.157333, Test MSE of 19370024590.312534
Epoch 63: training loss 15793770085.647
Test Loss of 17954677605.049515, Test MSE of 17954677593.811096
Epoch 64: training loss 15571979531.294
Test Loss of 18792022319.267006, Test MSE of 18792022249.721939
Epoch 65: training loss 14911058620.235
Test Loss of 18715852419.968533, Test MSE of 18715851958.330330
Epoch 66: training loss 14441765372.235
Test Loss of 17646118592.621933, Test MSE of 17646118460.288834
Epoch 67: training loss 14080640986.353
Test Loss of 18535133981.971310, Test MSE of 18535134074.829266
Epoch 68: training loss 13707439501.176
Test Loss of 18649431216.747803, Test MSE of 18649431355.142403
Epoch 69: training loss 13218094953.412
Test Loss of 18792463534.852383, Test MSE of 18792463761.984001
Epoch 70: training loss 12969779802.353
Test Loss of 20385553520.303562, Test MSE of 20385553489.809971
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22005802226.964893, 'MSE - std': 3648530174.619954, 'R2 - mean': 0.8371646908674513, 'R2 - std': 0.02198790698919229} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004012 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043706006.588
Test Loss of 431612607291.350281, Test MSE of 431612607154.314575
Epoch 2: training loss 424024413846.588
Test Loss of 431592543980.216553, Test MSE of 431592547410.768372
Epoch 3: training loss 423996914627.765
Test Loss of 431565163200.148071, Test MSE of 431565157438.431885
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010880180.706
Test Loss of 431567706685.364197, Test MSE of 431567701141.877625
Epoch 2: training loss 423998991661.176
Test Loss of 431569220776.218445, Test MSE of 431569215519.837463
Epoch 3: training loss 423998622780.235
Test Loss of 431568618594.087952, Test MSE of 431568618088.119019
Epoch 4: training loss 423998287149.176
Test Loss of 431569422236.490540, Test MSE of 431569430303.810059
Epoch 5: training loss 423998007296.000
Test Loss of 431569070446.763550, Test MSE of 431569069195.071350
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 103918455620.58618, 'MSE - std': 163857805922.94724, 'R2 - mean': 0.22513698635070778, 'R2 - std': 1.224213387783008} 
 

Saving model.....
Results After CV: {'MSE - mean': 103918455620.58618, 'MSE - std': 163857805922.94724, 'R2 - mean': 0.22513698635070778, 'R2 - std': 1.224213387783008}
Train time: 80.3062025520001
Inference time: 0.07420006739994278
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 15 finished with value: 103918455620.58618 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005757 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526260615.529
Test Loss of 418111839094.377075, Test MSE of 418111836824.490479
Epoch 2: training loss 427505681829.647
Test Loss of 418093848946.942383, Test MSE of 418093846384.919617
Epoch 3: training loss 427477947813.647
Test Loss of 418069849991.195007, Test MSE of 418069847657.455383
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492434160.941
Test Loss of 418074818637.220459, Test MSE of 418074821382.067993
Epoch 2: training loss 427482561475.765
Test Loss of 418076137930.111511, Test MSE of 418076149120.593567
Epoch 3: training loss 427481976470.588
Test Loss of 418075417147.810303, Test MSE of 418075421883.341980
Epoch 4: training loss 422338080888.471
Test Loss of 401597943831.687256, Test MSE of 401597944163.887146
Epoch 5: training loss 386420797319.529
Test Loss of 346657539120.558899, Test MSE of 346657544213.569336
Epoch 6: training loss 318731016673.882
Test Loss of 270001537372.676392, Test MSE of 270001534660.665985
Epoch 7: training loss 227359693824.000
Test Loss of 170759952883.801056, Test MSE of 170759955381.439911
Epoch 8: training loss 161851856655.059
Test Loss of 130840216692.778168, Test MSE of 130840217054.969299
Epoch 9: training loss 141556087777.882
Test Loss of 120395732343.442978, Test MSE of 120395735323.513824
Epoch 10: training loss 137658780672.000
Test Loss of 115843638834.098541, Test MSE of 115843640136.285568
Epoch 11: training loss 132027656734.118
Test Loss of 113010532929.969009, Test MSE of 113010533556.065933
Epoch 12: training loss 129782855800.471
Test Loss of 110221886929.454544, Test MSE of 110221886409.423111
Epoch 13: training loss 127268936071.529
Test Loss of 106376089293.960678, Test MSE of 106376087617.073074
Epoch 14: training loss 121651833253.647
Test Loss of 102805623544.834610, Test MSE of 102805620913.459000
Epoch 15: training loss 119564214452.706
Test Loss of 99668558839.709457, Test MSE of 99668556351.199570
Epoch 16: training loss 113573911265.882
Test Loss of 96962123432.534821, Test MSE of 96962124064.362473
Epoch 17: training loss 110334071446.588
Test Loss of 93048041277.764511, Test MSE of 93048041080.419510
Epoch 18: training loss 106826233780.706
Test Loss of 90462720465.217667, Test MSE of 90462720828.787125
Epoch 19: training loss 103715144161.882
Test Loss of 88482822545.262085, Test MSE of 88482822336.046844
Epoch 20: training loss 98935377076.706
Test Loss of 84112687539.845474, Test MSE of 84112686731.251450
Epoch 21: training loss 95875367424.000
Test Loss of 79199792620.458008, Test MSE of 79199792673.458359
Epoch 22: training loss 92278904877.176
Test Loss of 78218963255.250519, Test MSE of 78218963245.928238
Epoch 23: training loss 88188562319.059
Test Loss of 73335970390.340042, Test MSE of 73335969820.801041
Epoch 24: training loss 83895674925.176
Test Loss of 72109166958.678696, Test MSE of 72109166089.283478
Epoch 25: training loss 81508788766.118
Test Loss of 66650402898.431648, Test MSE of 66650403199.710556
Epoch 26: training loss 76639182908.235
Test Loss of 65071618612.230392, Test MSE of 65071617566.792091
Epoch 27: training loss 74364187407.059
Test Loss of 62709235768.849411, Test MSE of 62709235767.558792
Epoch 28: training loss 70743881110.588
Test Loss of 61163351845.129768, Test MSE of 61163352963.178001
Epoch 29: training loss 67008116261.647
Test Loss of 59864405282.168861, Test MSE of 59864405457.468147
Epoch 30: training loss 64392361468.235
Test Loss of 53080596293.344437, Test MSE of 53080596226.881729
Epoch 31: training loss 61103239823.059
Test Loss of 52222947136.606987, Test MSE of 52222948901.420891
Epoch 32: training loss 58227093451.294
Test Loss of 52181845409.132545, Test MSE of 52181845564.424202
Epoch 33: training loss 55256392673.882
Test Loss of 45956653559.117279, Test MSE of 45956653394.013672
Epoch 34: training loss 52501110241.882
Test Loss of 45109495350.835991, Test MSE of 45109495241.396873
Epoch 35: training loss 50377994985.412
Test Loss of 41764191287.665047, Test MSE of 41764191609.879730
Epoch 36: training loss 47619081532.235
Test Loss of 39290399041.672913, Test MSE of 39290399348.488632
Epoch 37: training loss 45218835410.824
Test Loss of 36339930723.368034, Test MSE of 36339930845.429749
Epoch 38: training loss 43048129931.294
Test Loss of 37693489013.666435, Test MSE of 37693489366.045219
Epoch 39: training loss 40679925820.235
Test Loss of 34008182350.049503, Test MSE of 34008183064.050003
Epoch 40: training loss 38877479740.235
Test Loss of 35440481513.319450, Test MSE of 35440482617.793503
Epoch 41: training loss 37262565278.118
Test Loss of 31461034427.899143, Test MSE of 31461034268.522278
Epoch 42: training loss 35371996427.294
Test Loss of 28333499753.230625, Test MSE of 28333499631.355396
Epoch 43: training loss 34116168681.412
Test Loss of 28709173319.535507, Test MSE of 28709173440.807564
Epoch 44: training loss 32464227380.706
Test Loss of 29924026674.513069, Test MSE of 29924026323.081425
Epoch 45: training loss 30607271040.000
Test Loss of 26741050743.916725, Test MSE of 26741050747.159145
Epoch 46: training loss 29343259414.588
Test Loss of 24001386869.784870, Test MSE of 24001387102.446838
Epoch 47: training loss 28171297121.882
Test Loss of 24565028103.639141, Test MSE of 24565028335.828518
Epoch 48: training loss 26689664048.941
Test Loss of 24803411868.039787, Test MSE of 24803412333.473629
Epoch 49: training loss 25828416903.529
Test Loss of 24955109478.328938, Test MSE of 24955109359.417847
Epoch 50: training loss 24944829876.706
Test Loss of 23145752354.761047, Test MSE of 23145752258.082249
Epoch 51: training loss 23678771772.235
Test Loss of 21385019419.477215, Test MSE of 21385019143.874481
Epoch 52: training loss 23178967213.176
Test Loss of 20934864361.141800, Test MSE of 20934864472.462410
Epoch 53: training loss 22303235075.765
Test Loss of 20117132622.227158, Test MSE of 20117132847.135292
Epoch 54: training loss 21234367751.529
Test Loss of 19825360890.788803, Test MSE of 19825360987.783752
Epoch 55: training loss 20753943563.294
Test Loss of 20870510478.301178, Test MSE of 20870510324.986488
Epoch 56: training loss 20051186368.000
Test Loss of 20325949025.946796, Test MSE of 20325949100.054058
Epoch 57: training loss 19208630802.824
Test Loss of 21501199876.145271, Test MSE of 21501199418.564575
Epoch 58: training loss 18382606674.824
Test Loss of 21731973350.713856, Test MSE of 21731973377.447197
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21731973377.447197, 'MSE - std': 0.0, 'R2 - mean': 0.8307707654086429, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003558 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918723674.353
Test Loss of 424558010733.731201, Test MSE of 424558007532.423218
Epoch 2: training loss 427898524129.882
Test Loss of 424541974455.043274, Test MSE of 424541970881.958679
Epoch 3: training loss 427871192726.588
Test Loss of 424519975567.663208, Test MSE of 424519972026.726135
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890868946.824
Test Loss of 424525962083.427246, Test MSE of 424525959654.369934
Epoch 2: training loss 427879633859.765
Test Loss of 424527309459.926880, Test MSE of 424527311728.230164
Epoch 3: training loss 427879168843.294
Test Loss of 424526949716.859558, Test MSE of 424526950280.500427
Epoch 4: training loss 422523502832.941
Test Loss of 407922259237.958801, Test MSE of 407922266934.364014
Epoch 5: training loss 385891078746.353
Test Loss of 353803784283.906555, Test MSE of 353803781223.920166
Epoch 6: training loss 316985916837.647
Test Loss of 276719674623.111755, Test MSE of 276719671419.528992
Epoch 7: training loss 224725723919.059
Test Loss of 180552436976.425629, Test MSE of 180552440691.177490
Epoch 8: training loss 160015896515.765
Test Loss of 141870579035.492004, Test MSE of 141870580971.246246
Epoch 9: training loss 139931830633.412
Test Loss of 132123344619.332870, Test MSE of 132123343404.640350
Epoch 10: training loss 134186074804.706
Test Loss of 127520215389.623871, Test MSE of 127520214875.681900
Epoch 11: training loss 131150889291.294
Test Loss of 124773147956.171173, Test MSE of 124773147384.489761
Epoch 12: training loss 128032809743.059
Test Loss of 122114507184.055511, Test MSE of 122114509643.083191
Epoch 13: training loss 124310403252.706
Test Loss of 118182823644.646774, Test MSE of 118182820603.103134
Epoch 14: training loss 120431449630.118
Test Loss of 115348375344.025909, Test MSE of 115348375140.811279
Epoch 15: training loss 115930095104.000
Test Loss of 112758813609.067780, Test MSE of 112758813247.986694
Epoch 16: training loss 113491693025.882
Test Loss of 107078601035.147812, Test MSE of 107078601535.411606
Epoch 17: training loss 108049378093.176
Test Loss of 105256009159.505905, Test MSE of 105256006533.274231
Epoch 18: training loss 104428562612.706
Test Loss of 101229197557.873703, Test MSE of 101229198006.414780
Epoch 19: training loss 101616418063.059
Test Loss of 98465030235.906540, Test MSE of 98465030480.158447
Epoch 20: training loss 96477252547.765
Test Loss of 92883606441.541519, Test MSE of 92883606180.898926
Epoch 21: training loss 93540136688.941
Test Loss of 90400585448.016663, Test MSE of 90400585172.765900
Epoch 22: training loss 89163882435.765
Test Loss of 87275889320.297943, Test MSE of 87275888472.870468
Epoch 23: training loss 85718890496.000
Test Loss of 81363362691.168167, Test MSE of 81363364471.772385
Epoch 24: training loss 82992068442.353
Test Loss of 78371684579.397644, Test MSE of 78371683235.728348
Epoch 25: training loss 77644206034.824
Test Loss of 74449944541.890350, Test MSE of 74449944046.603119
Epoch 26: training loss 75606631544.471
Test Loss of 75284244070.210495, Test MSE of 75284245853.346146
Epoch 27: training loss 71559977366.588
Test Loss of 69244123989.214890, Test MSE of 69244124491.889435
Epoch 28: training loss 68619808768.000
Test Loss of 66903507792.003700, Test MSE of 66903507572.141060
Epoch 29: training loss 64978580871.529
Test Loss of 61670728820.778160, Test MSE of 61670730701.959747
Epoch 30: training loss 61510291169.882
Test Loss of 58294615699.216286, Test MSE of 58294614439.234184
Epoch 31: training loss 58599288320.000
Test Loss of 56276395210.526024, Test MSE of 56276394250.771584
Epoch 32: training loss 55642670682.353
Test Loss of 56511917887.659492, Test MSE of 56511916715.565926
Epoch 33: training loss 53054484946.824
Test Loss of 51891146514.653709, Test MSE of 51891144687.727028
Epoch 34: training loss 49465811395.765
Test Loss of 50139098513.972702, Test MSE of 50139098446.017563
Epoch 35: training loss 46916035132.235
Test Loss of 47592790221.131622, Test MSE of 47592788385.933304
Epoch 36: training loss 45165997891.765
Test Loss of 45442930213.781174, Test MSE of 45442929276.944923
Epoch 37: training loss 42843034089.412
Test Loss of 42882776329.534119, Test MSE of 42882775886.069206
Epoch 38: training loss 40560115147.294
Test Loss of 38764448852.089752, Test MSE of 38764449997.946693
Epoch 39: training loss 38471107508.706
Test Loss of 41975414129.758041, Test MSE of 41975413237.867821
Epoch 40: training loss 35826440470.588
Test Loss of 38529486083.849182, Test MSE of 38529487488.874519
Epoch 41: training loss 34144036043.294
Test Loss of 39113363544.116585, Test MSE of 39113362368.114792
Epoch 42: training loss 32481665249.882
Test Loss of 34578336997.529495, Test MSE of 34578337653.985626
Epoch 43: training loss 31078257852.235
Test Loss of 35217841300.992828, Test MSE of 35217841230.322952
Epoch 44: training loss 29769086968.471
Test Loss of 33621821593.967152, Test MSE of 33621822222.757851
Epoch 45: training loss 27716884224.000
Test Loss of 33841266164.985428, Test MSE of 33841266707.332436
Epoch 46: training loss 26312111841.882
Test Loss of 30674313391.048809, Test MSE of 30674313429.215637
Epoch 47: training loss 25321734038.588
Test Loss of 31133885715.956512, Test MSE of 31133887141.311569
Epoch 48: training loss 24102772336.941
Test Loss of 31361163566.486237, Test MSE of 31361163246.290405
Epoch 49: training loss 22761612577.882
Test Loss of 30276524278.821190, Test MSE of 30276524858.216484
Epoch 50: training loss 22186252009.412
Test Loss of 26943816589.116817, Test MSE of 26943816489.506680
Epoch 51: training loss 20686142313.412
Test Loss of 30125399140.197086, Test MSE of 30125398918.479160
Epoch 52: training loss 20118183085.176
Test Loss of 27866535010.538979, Test MSE of 27866535599.758430
Epoch 53: training loss 19848252216.471
Test Loss of 27002371705.160305, Test MSE of 27002371913.166195
Epoch 54: training loss 18652247446.588
Test Loss of 27733711946.851723, Test MSE of 27733711161.836441
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24732842269.64182, 'MSE - std': 3000868892.194622, 'R2 - mean': 0.8163852404554246, 'R2 - std': 0.014385524953218376} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005436 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927887329.882
Test Loss of 447260311851.406921, Test MSE of 447260317717.955688
Epoch 2: training loss 421909081509.647
Test Loss of 447243376988.202637, Test MSE of 447243381463.942871
Epoch 3: training loss 421883110460.235
Test Loss of 447219603110.639832, Test MSE of 447219610677.746643
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902042533.647
Test Loss of 447227706895.515137, Test MSE of 447227714835.745605
Epoch 2: training loss 421891600384.000
Test Loss of 447228639513.878296, Test MSE of 447228647466.820007
Epoch 3: training loss 421891172472.471
Test Loss of 447228636769.946777, Test MSE of 447228632710.162659
Epoch 4: training loss 416455902750.118
Test Loss of 429829113941.747864, Test MSE of 429829122727.422363
Epoch 5: training loss 379888963824.941
Test Loss of 374164699929.523010, Test MSE of 374164698237.649902
Epoch 6: training loss 312187298153.412
Test Loss of 296324418948.234070, Test MSE of 296324414864.819458
Epoch 7: training loss 220357869086.118
Test Loss of 196379829923.797363, Test MSE of 196379832875.556030
Epoch 8: training loss 156790239201.882
Test Loss of 154879801633.458252, Test MSE of 154879801940.210266
Epoch 9: training loss 137520657709.176
Test Loss of 143449310067.060852, Test MSE of 143449310069.353546
Epoch 10: training loss 132814673498.353
Test Loss of 138300000834.442749, Test MSE of 138300004246.044952
Epoch 11: training loss 128798742558.118
Test Loss of 135888265767.439285, Test MSE of 135888265639.660446
Epoch 12: training loss 126315479762.824
Test Loss of 132400703484.683777, Test MSE of 132400706108.102615
Epoch 13: training loss 123735814896.941
Test Loss of 127551022759.587326, Test MSE of 127551023553.983871
Epoch 14: training loss 119087898202.353
Test Loss of 125545786985.526718, Test MSE of 125545788003.823944
Epoch 15: training loss 116798551853.176
Test Loss of 121532335361.006714, Test MSE of 121532334201.436478
Epoch 16: training loss 112510609106.824
Test Loss of 119285686876.498734, Test MSE of 119285687339.511688
Epoch 17: training loss 108522882258.824
Test Loss of 114004096419.027527, Test MSE of 114004096492.767227
Epoch 18: training loss 104053640161.882
Test Loss of 110465273110.325241, Test MSE of 110465272188.872787
Epoch 19: training loss 101440945995.294
Test Loss of 107850941689.426788, Test MSE of 107850943587.903488
Epoch 20: training loss 98011301616.941
Test Loss of 103201508683.384689, Test MSE of 103201510461.309677
Epoch 21: training loss 93491406396.235
Test Loss of 98375435412.992828, Test MSE of 98375433561.210297
Epoch 22: training loss 90060698112.000
Test Loss of 95483906198.177185, Test MSE of 95483908289.652405
Epoch 23: training loss 85675540690.824
Test Loss of 91710024065.154755, Test MSE of 91710024388.997375
Epoch 24: training loss 81975220178.824
Test Loss of 89215190096.773544, Test MSE of 89215188211.472122
Epoch 25: training loss 80227134238.118
Test Loss of 84756138628.767059, Test MSE of 84756138594.977554
Epoch 26: training loss 75911844382.118
Test Loss of 81738459452.935455, Test MSE of 81738458406.856781
Epoch 27: training loss 72811981583.059
Test Loss of 76762194086.758270, Test MSE of 76762193755.040955
Epoch 28: training loss 69580762413.176
Test Loss of 74373849878.206802, Test MSE of 74373849546.715652
Epoch 29: training loss 66901454110.118
Test Loss of 70077229814.465881, Test MSE of 70077229390.170471
Epoch 30: training loss 63083959808.000
Test Loss of 66510643093.881104, Test MSE of 66510642500.371513
Epoch 31: training loss 60258651813.647
Test Loss of 66531501426.705528, Test MSE of 66531500939.987267
Epoch 32: training loss 57785870637.176
Test Loss of 63797294142.771225, Test MSE of 63797295079.956009
Epoch 33: training loss 54478203437.176
Test Loss of 57884580771.856583, Test MSE of 57884581880.886490
Epoch 34: training loss 52162177212.235
Test Loss of 53599406121.215820, Test MSE of 53599406349.926506
Epoch 35: training loss 49851051685.647
Test Loss of 51812188046.301178, Test MSE of 51812188266.325943
Epoch 36: training loss 47096786266.353
Test Loss of 53914267690.637054, Test MSE of 53914267557.041260
Epoch 37: training loss 44992799939.765
Test Loss of 50618333463.035858, Test MSE of 50618333577.418427
Epoch 38: training loss 42265769795.765
Test Loss of 44667611265.332405, Test MSE of 44667610877.851189
Epoch 39: training loss 40377377076.706
Test Loss of 44495510702.101318, Test MSE of 44495511315.924080
Epoch 40: training loss 39144758324.706
Test Loss of 41645272228.626419, Test MSE of 41645272005.006683
Epoch 41: training loss 37096565820.235
Test Loss of 42454975969.561882, Test MSE of 42454975579.713387
Epoch 42: training loss 34945885221.647
Test Loss of 38901231205.736755, Test MSE of 38901232831.121239
Epoch 43: training loss 33322914499.765
Test Loss of 39785923621.188988, Test MSE of 39785923942.593567
Epoch 44: training loss 31641579632.941
Test Loss of 36714758449.091835, Test MSE of 36714758736.099007
Epoch 45: training loss 30419061481.412
Test Loss of 34642059496.135094, Test MSE of 34642059307.042610
Epoch 46: training loss 28900905743.059
Test Loss of 33735461762.694424, Test MSE of 33735461425.768471
Epoch 47: training loss 27939075975.529
Test Loss of 38141801530.981262, Test MSE of 38141801491.448647
Epoch 48: training loss 26978252867.765
Test Loss of 32596332720.233170, Test MSE of 32596332159.787548
Epoch 49: training loss 25340658394.353
Test Loss of 27629000762.270645, Test MSE of 27629000364.540150
Epoch 50: training loss 24361127649.882
Test Loss of 32384759678.667591, Test MSE of 32384759727.853851
Epoch 51: training loss 23531641091.765
Test Loss of 26132686247.291233, Test MSE of 26132686033.615665
Epoch 52: training loss 22560235911.529
Test Loss of 31303036383.193153, Test MSE of 31303036181.316010
Epoch 53: training loss 21616079416.471
Test Loss of 28980755545.774693, Test MSE of 28980756549.519733
Epoch 54: training loss 21089513189.647
Test Loss of 25760241066.607449, Test MSE of 25760241321.220783
Epoch 55: training loss 20169355493.647
Test Loss of 27876606143.393013, Test MSE of 27876605752.257084
Epoch 56: training loss 19478917135.059
Test Loss of 24968972618.674068, Test MSE of 24968972628.814938
Epoch 57: training loss 18807194270.118
Test Loss of 23618568435.031227, Test MSE of 23618568038.007645
Epoch 58: training loss 18482286595.765
Test Loss of 25877319814.069859, Test MSE of 25877319670.573109
Epoch 59: training loss 17566919179.294
Test Loss of 27148129046.206802, Test MSE of 27148129421.538925
Epoch 60: training loss 17058928335.059
Test Loss of 22345247364.767059, Test MSE of 22345247317.030704
Epoch 61: training loss 16876023051.294
Test Loss of 23247463994.152210, Test MSE of 23247464017.078930
Epoch 62: training loss 16061157628.235
Test Loss of 23684799196.883644, Test MSE of 23684799519.204384
Epoch 63: training loss 15710032617.412
Test Loss of 22210805328.891972, Test MSE of 22210806101.049774
Epoch 64: training loss 15073710520.471
Test Loss of 23810970646.029148, Test MSE of 23810970615.493153
Epoch 65: training loss 14689722522.353
Test Loss of 23878794997.281517, Test MSE of 23878794477.896034
Epoch 66: training loss 14445348818.824
Test Loss of 23396674444.169327, Test MSE of 23396674390.683056
Epoch 67: training loss 13960156852.706
Test Loss of 20915710933.599815, Test MSE of 20915710822.966934
Epoch 68: training loss 13685902177.882
Test Loss of 20789799844.804070, Test MSE of 20789800040.948009
Epoch 69: training loss 13152313581.176
Test Loss of 23003403985.276890, Test MSE of 23003404460.178612
Epoch 70: training loss 13071260141.176
Test Loss of 22540657907.268101, Test MSE of 22540657950.440277
Epoch 71: training loss 12680637729.882
Test Loss of 21966021872.899376, Test MSE of 21966021699.823002
Epoch 72: training loss 12428011149.176
Test Loss of 22206766965.192692, Test MSE of 22206767027.175854
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23890817188.819828, 'MSE - std': 2724240911.806591, 'R2 - mean': 0.8283138327760403, 'R2 - std': 0.02055589570632607} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005378 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110908175.059
Test Loss of 410764657087.318848, Test MSE of 410764655744.368225
Epoch 2: training loss 430090907407.059
Test Loss of 410747025583.326233, Test MSE of 410747026108.115601
Epoch 3: training loss 430063571907.765
Test Loss of 410723785186.384094, Test MSE of 410723784519.625977
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075059019.294
Test Loss of 410727861913.765869, Test MSE of 410727855506.748047
Epoch 2: training loss 430066640293.647
Test Loss of 410729567396.427551, Test MSE of 410729566352.092102
Epoch 3: training loss 430066273822.118
Test Loss of 410729844703.777893, Test MSE of 410729839136.715881
Epoch 4: training loss 425050876506.353
Test Loss of 394647216670.089783, Test MSE of 394647212634.090576
Epoch 5: training loss 389303163361.882
Test Loss of 340728823754.928284, Test MSE of 340728824788.294434
Epoch 6: training loss 321546775371.294
Test Loss of 263205434612.035156, Test MSE of 263205434395.486359
Epoch 7: training loss 229865236058.353
Test Loss of 165958359438.985657, Test MSE of 165958360373.859161
Epoch 8: training loss 164126719608.471
Test Loss of 125389623112.144379, Test MSE of 125389624355.470642
Epoch 9: training loss 144689525609.412
Test Loss of 114694488776.677460, Test MSE of 114694490442.875885
Epoch 10: training loss 139710135838.118
Test Loss of 110418840953.662201, Test MSE of 110418841955.688034
Epoch 11: training loss 136357938236.235
Test Loss of 107840207030.907913, Test MSE of 107840205439.389511
Epoch 12: training loss 131441740800.000
Test Loss of 105335988690.273026, Test MSE of 105335989872.724625
Epoch 13: training loss 129691615623.529
Test Loss of 101319428530.998611, Test MSE of 101319429591.487473
Epoch 14: training loss 125331296256.000
Test Loss of 99297146604.216568, Test MSE of 99297149268.236160
Epoch 15: training loss 121487361626.353
Test Loss of 96059137600.681168, Test MSE of 96059138683.790604
Epoch 16: training loss 117739423774.118
Test Loss of 92951466596.220276, Test MSE of 92951464256.769318
Epoch 17: training loss 113272059602.824
Test Loss of 89522166234.802399, Test MSE of 89522166541.784668
Epoch 18: training loss 109529146880.000
Test Loss of 87034616407.900040, Test MSE of 87034616025.022537
Epoch 19: training loss 105852983341.176
Test Loss of 83519902133.841736, Test MSE of 83519902743.673096
Epoch 20: training loss 102336538608.941
Test Loss of 80455040648.707077, Test MSE of 80455041167.891235
Epoch 21: training loss 97095738157.176
Test Loss of 76295728134.160110, Test MSE of 76295727723.809357
Epoch 22: training loss 94784779504.941
Test Loss of 74852775269.760300, Test MSE of 74852775407.853012
Epoch 23: training loss 90684803704.471
Test Loss of 70069861914.772797, Test MSE of 70069861646.431274
Epoch 24: training loss 87764997948.235
Test Loss of 69811375155.650162, Test MSE of 69811376087.621414
Epoch 25: training loss 83346664056.471
Test Loss of 64143899515.794540, Test MSE of 64143900186.265831
Epoch 26: training loss 79368202420.706
Test Loss of 62003369666.517357, Test MSE of 62003369706.701958
Epoch 27: training loss 76245047280.941
Test Loss of 60034068455.359558, Test MSE of 60034068947.924553
Epoch 28: training loss 73451571184.941
Test Loss of 57169916008.248032, Test MSE of 57169916610.816689
Epoch 29: training loss 69516733696.000
Test Loss of 53694183913.965759, Test MSE of 53694184339.658997
Epoch 30: training loss 66581808474.353
Test Loss of 51112100186.861641, Test MSE of 51112100132.879539
Epoch 31: training loss 63442134889.412
Test Loss of 48226564134.856087, Test MSE of 48226564177.580750
Epoch 32: training loss 60295812924.235
Test Loss of 48711164849.813972, Test MSE of 48711165125.370178
Epoch 33: training loss 58164805165.176
Test Loss of 45948759944.588615, Test MSE of 45948759570.619644
Epoch 34: training loss 54751574309.647
Test Loss of 45610746362.076813, Test MSE of 45610747455.938400
Epoch 35: training loss 52468063013.647
Test Loss of 41164384751.652016, Test MSE of 41164384061.781265
Epoch 36: training loss 49415784583.529
Test Loss of 37178192317.423416, Test MSE of 37178192190.670425
Epoch 37: training loss 47009076600.471
Test Loss of 37552293532.608978, Test MSE of 37552293767.293404
Epoch 38: training loss 44954276879.059
Test Loss of 36570779964.534935, Test MSE of 36570779958.254860
Epoch 39: training loss 42803088143.059
Test Loss of 35362056862.030540, Test MSE of 35362056892.551086
Epoch 40: training loss 40527792775.529
Test Loss of 32997673648.984730, Test MSE of 32997673307.699806
Epoch 41: training loss 38510690108.235
Test Loss of 32602453059.761223, Test MSE of 32602452596.984787
Epoch 42: training loss 36904565632.000
Test Loss of 29391819009.776955, Test MSE of 29391819070.425190
Epoch 43: training loss 34753138868.706
Test Loss of 27897966851.672375, Test MSE of 27897966548.932419
Epoch 44: training loss 33784015465.412
Test Loss of 27152705480.085144, Test MSE of 27152705226.157398
Epoch 45: training loss 32303401118.118
Test Loss of 27484116054.715408, Test MSE of 27484116461.056862
Epoch 46: training loss 30589634243.765
Test Loss of 26055922370.991207, Test MSE of 26055922196.985428
Epoch 47: training loss 29118603651.765
Test Loss of 24462054016.651550, Test MSE of 24462053321.376057
Epoch 48: training loss 27807349485.176
Test Loss of 23404806864.259140, Test MSE of 23404806342.119728
Epoch 49: training loss 26581771862.588
Test Loss of 23110138509.919483, Test MSE of 23110138565.113937
Epoch 50: training loss 25735066164.706
Test Loss of 23424208822.078667, Test MSE of 23424209397.515285
Epoch 51: training loss 24647194108.235
Test Loss of 21097988202.143452, Test MSE of 21097987859.809868
Epoch 52: training loss 23506952647.529
Test Loss of 21574379159.870430, Test MSE of 21574379303.730503
Epoch 53: training loss 23107862629.647
Test Loss of 20148227427.864876, Test MSE of 20148227766.870193
Epoch 54: training loss 22410264700.235
Test Loss of 20126204927.526146, Test MSE of 20126204812.567104
Epoch 55: training loss 21514614283.294
Test Loss of 21829603545.973160, Test MSE of 21829603333.588768
Epoch 56: training loss 20651857389.176
Test Loss of 21166711350.730217, Test MSE of 21166711598.542103
Epoch 57: training loss 19832930283.294
Test Loss of 19276203503.178158, Test MSE of 19276203505.585831
Epoch 58: training loss 19177778119.529
Test Loss of 18986101133.090237, Test MSE of 18986100833.070042
Epoch 59: training loss 18497118208.000
Test Loss of 18913953568.340584, Test MSE of 18913953590.664471
Epoch 60: training loss 17746868946.824
Test Loss of 19385902360.048126, Test MSE of 19385902581.161350
Epoch 61: training loss 17429345116.235
Test Loss of 19776606989.860249, Test MSE of 19776606827.489170
Epoch 62: training loss 17073393874.824
Test Loss of 18498287319.366959, Test MSE of 18498287162.164730
Epoch 63: training loss 16268296591.059
Test Loss of 18840534896.895882, Test MSE of 18840534848.735725
Epoch 64: training loss 16052370266.353
Test Loss of 17396196632.048126, Test MSE of 17396196423.546284
Epoch 65: training loss 15650651610.353
Test Loss of 18570385536.888477, Test MSE of 18570385794.258423
Epoch 66: training loss 15037034012.235
Test Loss of 18519986686.815365, Test MSE of 18519986854.212574
Epoch 67: training loss 14932072088.471
Test Loss of 17952399250.539566, Test MSE of 17952399323.431335
Epoch 68: training loss 14401583580.235
Test Loss of 16443441139.205923, Test MSE of 16443441173.313801
Epoch 69: training loss 13899429536.000
Test Loss of 17751297852.771866, Test MSE of 17751298018.845757
Epoch 70: training loss 13584982927.059
Test Loss of 17516325041.221657, Test MSE of 17516324669.833302
Epoch 71: training loss 13476205624.471
Test Loss of 17309438055.300323, Test MSE of 17309438049.568825
Epoch 72: training loss 12908375845.647
Test Loss of 18926420516.723740, Test MSE of 18926420920.804996
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22649718121.81612, 'MSE - std': 3191723216.621866, 'R2 - mean': 0.8321829706897177, 'R2 - std': 0.019021548853923077} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005368 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043026913.882
Test Loss of 431612488622.970825, Test MSE of 431612488238.243042
Epoch 2: training loss 424022625701.647
Test Loss of 431592322155.565002, Test MSE of 431592325967.775940
Epoch 3: training loss 423995306104.471
Test Loss of 431565204292.827393, Test MSE of 431565201479.665405
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012275230.118
Test Loss of 431568247835.009705, Test MSE of 431568252198.237976
Epoch 2: training loss 423999643768.471
Test Loss of 431571050249.595581, Test MSE of 431571056310.357666
Epoch 3: training loss 423999094784.000
Test Loss of 431570614318.437744, Test MSE of 431570613083.625732
Epoch 4: training loss 418826385769.412
Test Loss of 414793967823.548340, Test MSE of 414793977235.713379
Epoch 5: training loss 382987098834.824
Test Loss of 359767300434.332275, Test MSE of 359767299876.670105
Epoch 6: training loss 315558838512.941
Test Loss of 281136756782.437744, Test MSE of 281136760444.239563
Epoch 7: training loss 224011898729.412
Test Loss of 180826828817.058777, Test MSE of 180826826740.722443
Epoch 8: training loss 160735407540.706
Test Loss of 138382538514.124939, Test MSE of 138382539787.226288
Epoch 9: training loss 141268784399.059
Test Loss of 126463280798.504395, Test MSE of 126463280310.819473
Epoch 10: training loss 135469288990.118
Test Loss of 121130957430.226746, Test MSE of 121130959802.776657
Epoch 11: training loss 132834877500.235
Test Loss of 117835361206.552521, Test MSE of 117835359463.906693
Epoch 12: training loss 128026147900.235
Test Loss of 115130408732.549744, Test MSE of 115130405899.575043
Epoch 13: training loss 125325761445.647
Test Loss of 111194536880.866272, Test MSE of 111194534462.731033
Epoch 14: training loss 121425714040.471
Test Loss of 108440144140.201752, Test MSE of 108440144836.275421
Epoch 15: training loss 116926128112.941
Test Loss of 104168101161.106888, Test MSE of 104168101867.304138
Epoch 16: training loss 113415670422.588
Test Loss of 102098250132.671906, Test MSE of 102098249940.177277
Epoch 17: training loss 109220933481.412
Test Loss of 99307134505.936142, Test MSE of 99307133771.421753
Epoch 18: training loss 106521310810.353
Test Loss of 94046221302.049057, Test MSE of 94046221252.586838
Epoch 19: training loss 102206506315.294
Test Loss of 88972115497.936142, Test MSE of 88972114276.047333
Epoch 20: training loss 97978766064.941
Test Loss of 84456233025.391953, Test MSE of 84456231408.417191
Epoch 21: training loss 94721521573.647
Test Loss of 83356524900.338730, Test MSE of 83356524647.887177
Epoch 22: training loss 90802935386.353
Test Loss of 78864881937.888016, Test MSE of 78864882793.996323
Epoch 23: training loss 86971835286.588
Test Loss of 75970641582.615463, Test MSE of 75970640237.195953
Epoch 24: training loss 82865958354.824
Test Loss of 73934441354.957886, Test MSE of 73934440612.395416
Epoch 25: training loss 80540282970.353
Test Loss of 71621968425.462280, Test MSE of 71621968084.252838
Epoch 26: training loss 74991386413.176
Test Loss of 67555177653.486351, Test MSE of 67555178298.231544
Epoch 27: training loss 73632444702.118
Test Loss of 65903077994.854233, Test MSE of 65903075934.314064
Epoch 28: training loss 70799166832.941
Test Loss of 62367448164.931053, Test MSE of 62367447539.691902
Epoch 29: training loss 66441471939.765
Test Loss of 55163416009.269783, Test MSE of 55163416657.909981
Epoch 30: training loss 63712516848.941
Test Loss of 53263817241.351227, Test MSE of 53263816543.975044
Epoch 31: training loss 60578632146.824
Test Loss of 48932886261.693657, Test MSE of 48932885639.535172
Epoch 32: training loss 58075487755.294
Test Loss of 48993958688.814438, Test MSE of 48993958519.544937
Epoch 33: training loss 54454820826.353
Test Loss of 42550789718.478485, Test MSE of 42550789987.486092
Epoch 34: training loss 52196552274.824
Test Loss of 44136067911.670525, Test MSE of 44136067682.526611
Epoch 35: training loss 49455072218.353
Test Loss of 42632615015.774178, Test MSE of 42632614875.340660
Epoch 36: training loss 46903763380.706
Test Loss of 41277939327.229988, Test MSE of 41277938690.034920
Epoch 37: training loss 44636083343.059
Test Loss of 38903479532.453491, Test MSE of 38903479745.948997
Epoch 38: training loss 42928469609.412
Test Loss of 38077880197.745491, Test MSE of 38077880539.422127
Epoch 39: training loss 40444858360.471
Test Loss of 35886596781.193893, Test MSE of 35886597104.686195
Epoch 40: training loss 38460654464.000
Test Loss of 34210138678.730217, Test MSE of 34210137958.213913
Epoch 41: training loss 37085674390.588
Test Loss of 28295682629.893566, Test MSE of 28295682208.281853
Epoch 42: training loss 35189023706.353
Test Loss of 31475080713.714020, Test MSE of 31475081241.339630
Epoch 43: training loss 33527948754.824
Test Loss of 27246254488.462749, Test MSE of 27246254131.878838
Epoch 44: training loss 32357375774.118
Test Loss of 30468855619.405830, Test MSE of 30468855694.402020
Epoch 45: training loss 31015257065.412
Test Loss of 27283206906.906063, Test MSE of 27283207386.401375
Epoch 46: training loss 29249903977.412
Test Loss of 27879524532.064785, Test MSE of 27879525053.113468
Epoch 47: training loss 27933242322.824
Test Loss of 25537490255.015270, Test MSE of 25537490682.768890
Epoch 48: training loss 27207735506.824
Test Loss of 25567911828.908840, Test MSE of 25567911924.491772
Epoch 49: training loss 25311036555.294
Test Loss of 24143673936.318371, Test MSE of 24143673212.397617
Epoch 50: training loss 24752769920.000
Test Loss of 27210013830.100880, Test MSE of 27210013857.614277
Epoch 51: training loss 23820197831.529
Test Loss of 22931824950.848682, Test MSE of 22931825011.526100
Epoch 52: training loss 22960355365.647
Test Loss of 22830859112.840351, Test MSE of 22830859758.749104
Epoch 53: training loss 21972831747.765
Test Loss of 24328702254.319298, Test MSE of 24328701897.926208
Epoch 54: training loss 21180790445.176
Test Loss of 22490868076.868118, Test MSE of 22490868081.937405
Epoch 55: training loss 20670308340.706
Test Loss of 23121134966.819065, Test MSE of 23121135082.338650
Epoch 56: training loss 19813754714.353
Test Loss of 21940515382.730217, Test MSE of 21940514771.117386
Epoch 57: training loss 19367142102.588
Test Loss of 20469497620.020359, Test MSE of 20469497570.105778
Epoch 58: training loss 18778253556.706
Test Loss of 19339736816.955112, Test MSE of 19339737133.494896
Epoch 59: training loss 18403015879.529
Test Loss of 19248369506.680241, Test MSE of 19248369745.384697
Epoch 60: training loss 17723575920.941
Test Loss of 20944722264.966221, Test MSE of 20944722587.934883
Epoch 61: training loss 17296981172.706
Test Loss of 19713637784.936604, Test MSE of 19713637952.487850
Epoch 62: training loss 16892782226.824
Test Loss of 20269509756.623787, Test MSE of 20269509325.750492
Epoch 63: training loss 16602935860.706
Test Loss of 19275269288.218418, Test MSE of 19275269098.214584
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21974828317.095814, 'MSE - std': 3157781288.7025733, 'R2 - mean': 0.8369566914991807, 'R2 - std': 0.019509205470970663} 
 

Saving model.....
Results After CV: {'MSE - mean': 21974828317.095814, 'MSE - std': 3157781288.7025733, 'R2 - mean': 0.8369566914991807, 'R2 - std': 0.019509205470970663}
Train time: 98.41171060760007
Inference time: 0.07519660379966808
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 16 finished with value: 21974828317.095814 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005594 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526472523.294
Test Loss of 418113154650.366882, Test MSE of 418113156204.462708
Epoch 2: training loss 427506964118.588
Test Loss of 418096337192.564453, Test MSE of 418096337103.351135
Epoch 3: training loss 427480448783.059
Test Loss of 418073217992.808716, Test MSE of 418073214226.498047
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427498633216.000
Test Loss of 418079400737.576660, Test MSE of 418079402962.521606
Epoch 2: training loss 427485746959.059
Test Loss of 418080182189.568359, Test MSE of 418080181474.054749
Epoch 3: training loss 427485347478.588
Test Loss of 418079143907.101562, Test MSE of 418079151334.671326
Epoch 4: training loss 427485076178.824
Test Loss of 418078152301.790405, Test MSE of 418078149495.019287
Epoch 5: training loss 427484893906.824
Test Loss of 418076807312.966003, Test MSE of 418076807772.626160
Epoch 6: training loss 419246771019.294
Test Loss of 391458341512.083252, Test MSE of 391458341730.612122
Epoch 7: training loss 365267308784.941
Test Loss of 313941013972.296997, Test MSE of 313941017562.479858
Epoch 8: training loss 278211653330.824
Test Loss of 224186137354.363159, Test MSE of 224186132776.167755
Epoch 9: training loss 201566739877.647
Test Loss of 159339449141.000244, Test MSE of 159339447301.824432
Epoch 10: training loss 156346204400.941
Test Loss of 129955089110.251221, Test MSE of 129955090798.589172
Epoch 11: training loss 141595070132.706
Test Loss of 118335815983.196854, Test MSE of 118335817841.781265
Epoch 12: training loss 135482858631.529
Test Loss of 114767623465.748779, Test MSE of 114767623766.637527
Epoch 13: training loss 131808782848.000
Test Loss of 111931611451.987976, Test MSE of 111931612842.704849
Epoch 14: training loss 129409723632.941
Test Loss of 108924894485.377747, Test MSE of 108924893415.913132
Epoch 15: training loss 125949710863.059
Test Loss of 105706702467.582703, Test MSE of 105706703394.294205
Epoch 16: training loss 122848230339.765
Test Loss of 102509754088.964142, Test MSE of 102509753153.450439
Epoch 17: training loss 117880634864.941
Test Loss of 99295811593.001160, Test MSE of 99295812569.999390
Epoch 18: training loss 113858938849.882
Test Loss of 95247872775.994446, Test MSE of 95247872743.934250
Epoch 19: training loss 110068723380.706
Test Loss of 92602936094.260468, Test MSE of 92602935148.170593
Epoch 20: training loss 107194065167.059
Test Loss of 89736721055.770523, Test MSE of 89736720343.144730
Epoch 21: training loss 102270596397.176
Test Loss of 85226570012.247055, Test MSE of 85226568661.285568
Epoch 22: training loss 98660503446.588
Test Loss of 82711139659.858429, Test MSE of 82711139072.394928
Epoch 23: training loss 94881857456.941
Test Loss of 79189237411.323624, Test MSE of 79189235393.539688
Epoch 24: training loss 91172571738.353
Test Loss of 76392546783.430023, Test MSE of 76392547307.005295
Epoch 25: training loss 87758643320.471
Test Loss of 74418194853.159378, Test MSE of 74418196431.390289
Epoch 26: training loss 83979528523.294
Test Loss of 70100545934.893356, Test MSE of 70100546341.723602
Epoch 27: training loss 80514168410.353
Test Loss of 67281856463.204254, Test MSE of 67281855914.752907
Epoch 28: training loss 76820590260.706
Test Loss of 64040298007.568817, Test MSE of 64040298084.101303
Epoch 29: training loss 73247293477.647
Test Loss of 61932006178.050430, Test MSE of 61932005608.895508
Epoch 30: training loss 69856976896.000
Test Loss of 57799023887.692802, Test MSE of 57799024731.135284
Epoch 31: training loss 66971971749.647
Test Loss of 55357964816.699516, Test MSE of 55357965179.661110
Epoch 32: training loss 63320440922.353
Test Loss of 53072456804.197083, Test MSE of 53072457156.114738
Epoch 33: training loss 61586258560.000
Test Loss of 50084444506.307655, Test MSE of 50084444361.314941
Epoch 34: training loss 58363259975.529
Test Loss of 49947789409.117744, Test MSE of 49947789959.945686
Epoch 35: training loss 55243053696.000
Test Loss of 45273886584.508904, Test MSE of 45273886964.524315
Epoch 36: training loss 52550525417.412
Test Loss of 43356500028.639374, Test MSE of 43356500764.465652
Epoch 37: training loss 50280480474.353
Test Loss of 38384835479.539207, Test MSE of 38384836211.313339
Epoch 38: training loss 46940162277.647
Test Loss of 38979981628.461716, Test MSE of 38979980994.294716
Epoch 39: training loss 45219770601.412
Test Loss of 37714452348.298866, Test MSE of 37714452571.776588
Epoch 40: training loss 42632493101.176
Test Loss of 34081601755.343975, Test MSE of 34081602631.823891
Epoch 41: training loss 40759418518.588
Test Loss of 35559686389.399956, Test MSE of 35559686500.701698
Epoch 42: training loss 38771781662.118
Test Loss of 32601527517.949574, Test MSE of 32601527552.947548
Epoch 43: training loss 37123015732.706
Test Loss of 28552426600.460793, Test MSE of 28552426598.230305
Epoch 44: training loss 35519423284.706
Test Loss of 29548638081.983807, Test MSE of 29548637672.173477
Epoch 45: training loss 33297361486.118
Test Loss of 30024013156.493176, Test MSE of 30024013705.771168
Epoch 46: training loss 32436450620.235
Test Loss of 28629313457.832062, Test MSE of 28629313617.868591
Epoch 47: training loss 30226204822.588
Test Loss of 25963224844.495026, Test MSE of 25963225169.388767
Epoch 48: training loss 29106583830.588
Test Loss of 26375716677.344437, Test MSE of 26375717089.819901
Epoch 49: training loss 27860839032.471
Test Loss of 25300697174.221607, Test MSE of 25300697400.668533
Epoch 50: training loss 26540693150.118
Test Loss of 24181582214.365948, Test MSE of 24181582133.015247
Epoch 51: training loss 25452848225.882
Test Loss of 21974234596.878094, Test MSE of 21974234714.796185
Epoch 52: training loss 23751245383.529
Test Loss of 21549190306.257690, Test MSE of 21549190366.110313
Epoch 53: training loss 23211276239.059
Test Loss of 23254319584.377514, Test MSE of 23254319647.916500
Epoch 54: training loss 22378444645.647
Test Loss of 20748453626.492714, Test MSE of 20748453891.219566
Epoch 55: training loss 21117759939.765
Test Loss of 22084474732.665279, Test MSE of 22084475341.720642
Epoch 56: training loss 20789856700.235
Test Loss of 24366712773.729355, Test MSE of 24366712638.037689
Epoch 57: training loss 19874681155.765
Test Loss of 23533733232.810547, Test MSE of 23533733089.411556
Epoch 58: training loss 19114614400.000
Test Loss of 19965203526.351147, Test MSE of 19965203674.195560
Epoch 59: training loss 18485184286.118
Test Loss of 20201322123.162617, Test MSE of 20201321967.479855
Epoch 60: training loss 17899915930.353
Test Loss of 18453677734.639832, Test MSE of 18453677581.964195
Epoch 61: training loss 17552015119.059
Test Loss of 19411428312.916031, Test MSE of 19411428265.289845
Epoch 62: training loss 16750252073.412
Test Loss of 19624287950.908165, Test MSE of 19624287895.519081
Epoch 63: training loss 16260765289.412
Test Loss of 20061556440.856812, Test MSE of 20061556549.249397
Epoch 64: training loss 15806545648.941
Test Loss of 19297589335.169094, Test MSE of 19297589308.364933
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19297589308.364933, 'MSE - std': 0.0, 'R2 - mean': 0.8497275782832489, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005441 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917647390.118
Test Loss of 424555546243.582703, Test MSE of 424555543804.126282
Epoch 2: training loss 427896454686.118
Test Loss of 424539214105.404602, Test MSE of 424539213358.915527
Epoch 3: training loss 427868652724.706
Test Loss of 424517124971.480896, Test MSE of 424517126535.950989
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888157153.882
Test Loss of 424523099730.786926, Test MSE of 424523099928.048767
Epoch 2: training loss 427876738951.529
Test Loss of 424525548553.238037, Test MSE of 424525549261.890503
Epoch 3: training loss 427876063111.529
Test Loss of 424524868672.192444, Test MSE of 424524869936.286499
Epoch 4: training loss 427875624598.588
Test Loss of 424524344755.608582, Test MSE of 424524352126.615234
Epoch 5: training loss 427875321976.471
Test Loss of 424524378296.760559, Test MSE of 424524380801.329651
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221910985054.8473, 'MSE - std': 202613395746.48236, 'R2 - mean': -0.590547323428481, 'R2 - std': 1.4402749017117298} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005512 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926371568.941
Test Loss of 447257680269.472107, Test MSE of 447257678080.941711
Epoch 2: training loss 421904142697.412
Test Loss of 447237557685.503601, Test MSE of 447237567610.869324
Epoch 3: training loss 421875534305.882
Test Loss of 447211487925.562805, Test MSE of 447211490087.494690
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896698096.941
Test Loss of 447217478958.959961, Test MSE of 447217478970.179077
Epoch 2: training loss 421884741150.118
Test Loss of 447218720034.642639, Test MSE of 447218719747.822815
Epoch 3: training loss 421884110004.706
Test Loss of 447219232937.600769, Test MSE of 447219238075.139954
Epoch 4: training loss 421883667034.353
Test Loss of 447219152393.356445, Test MSE of 447219144520.888611
Epoch 5: training loss 421883398746.353
Test Loss of 447218724324.878113, Test MSE of 447218721173.995972
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 297013563761.23016, 'MSE - std': 196593285950.0649, 'R2 - mean': -1.052733386627824, 'R2 - std': 1.3454217835677293} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109762499.765
Test Loss of 410764371925.353088, Test MSE of 410764374691.998596
Epoch 2: training loss 430087695661.176
Test Loss of 410745463348.834778, Test MSE of 410745467481.263184
Epoch 3: training loss 430059821417.412
Test Loss of 410721389695.466919, Test MSE of 410721393817.062195
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075809792.000
Test Loss of 410727534294.419250, Test MSE of 410727525195.331665
Epoch 2: training loss 430064780950.588
Test Loss of 410728496363.505798, Test MSE of 410728496143.689941
Epoch 3: training loss 430064319789.176
Test Loss of 410727951362.369263, Test MSE of 410727947313.775940
Epoch 4: training loss 430063933078.588
Test Loss of 410726732638.415527, Test MSE of 410726738705.934082
Epoch 5: training loss 430063661296.941
Test Loss of 410725810085.019897, Test MSE of 410725812341.492737
Epoch 6: training loss 421950487612.235
Test Loss of 385263028063.837097, Test MSE of 385263026008.116089
Epoch 7: training loss 368493671725.176
Test Loss of 308741038690.324829, Test MSE of 308741039284.644897
Epoch 8: training loss 281146431849.412
Test Loss of 218580550996.701538, Test MSE of 218580549677.494202
Epoch 9: training loss 202824774053.647
Test Loss of 153001244741.182800, Test MSE of 153001246130.550568
Epoch 10: training loss 159132735397.647
Test Loss of 123930639110.278580, Test MSE of 123930639450.242020
Epoch 11: training loss 142185603192.471
Test Loss of 112018481307.424347, Test MSE of 112018482629.059479
Epoch 12: training loss 137131934870.588
Test Loss of 108622372689.621475, Test MSE of 108622371280.955795
Epoch 13: training loss 133010297976.471
Test Loss of 105674779290.239700, Test MSE of 105674779556.307755
Epoch 14: training loss 130923178797.176
Test Loss of 102885832598.804260, Test MSE of 102885833361.539764
Epoch 15: training loss 126120068698.353
Test Loss of 100672651544.521988, Test MSE of 100672650369.969955
Epoch 16: training loss 123531615141.647
Test Loss of 96872615710.445160, Test MSE of 96872617493.986923
Epoch 17: training loss 119084728320.000
Test Loss of 93190363720.736694, Test MSE of 93190363870.191177
Epoch 18: training loss 114287983676.235
Test Loss of 89312262179.065247, Test MSE of 89312263475.448639
Epoch 19: training loss 110999441257.412
Test Loss of 86477691013.627029, Test MSE of 86477691969.902664
Epoch 20: training loss 106853635975.529
Test Loss of 83754687302.722809, Test MSE of 83754687897.549744
Epoch 21: training loss 103028715836.235
Test Loss of 80011299414.004623, Test MSE of 80011299576.586090
Epoch 22: training loss 99293253903.059
Test Loss of 77082403615.866730, Test MSE of 77082404369.516663
Epoch 23: training loss 94637217144.471
Test Loss of 74286983576.462753, Test MSE of 74286984847.239792
Epoch 24: training loss 91425987011.765
Test Loss of 70882840195.020828, Test MSE of 70882840441.538040
Epoch 25: training loss 86754094652.235
Test Loss of 67638513468.771866, Test MSE of 67638513095.572128
Epoch 26: training loss 83448394300.235
Test Loss of 64018338984.218414, Test MSE of 64018338959.955620
Epoch 27: training loss 79755171644.235
Test Loss of 62104735757.267929, Test MSE of 62104734967.131020
Epoch 28: training loss 75746861040.941
Test Loss of 59594937080.062935, Test MSE of 59594937891.893097
Epoch 29: training loss 72571075584.000
Test Loss of 57895232208.259140, Test MSE of 57895232339.518974
Epoch 30: training loss 69314788306.824
Test Loss of 52944507913.003242, Test MSE of 52944507965.285866
Epoch 31: training loss 65625142603.294
Test Loss of 52273504547.894493, Test MSE of 52273504713.716034
Epoch 32: training loss 62025077684.706
Test Loss of 50545836062.326698, Test MSE of 50545836576.934029
Epoch 33: training loss 59243853839.059
Test Loss of 46301841014.700600, Test MSE of 46301841106.634949
Epoch 34: training loss 56526722002.824
Test Loss of 42810265744.051826, Test MSE of 42810265248.758774
Epoch 35: training loss 54436848730.353
Test Loss of 42575810080.459045, Test MSE of 42575809875.114067
Epoch 36: training loss 50358075120.941
Test Loss of 39465393465.217957, Test MSE of 39465393703.433006
Epoch 37: training loss 48249943341.176
Test Loss of 36098098990.556221, Test MSE of 36098099825.746918
Epoch 38: training loss 45808236574.118
Test Loss of 34904887305.477097, Test MSE of 34904887581.510109
Epoch 39: training loss 43447456880.941
Test Loss of 32590728171.624249, Test MSE of 32590728080.846375
Epoch 40: training loss 41386385505.882
Test Loss of 35376688947.294769, Test MSE of 35376689385.662468
Epoch 41: training loss 39210256007.529
Test Loss of 31522766957.934288, Test MSE of 31522766295.341022
Epoch 42: training loss 37176417212.235
Test Loss of 30955555224.462749, Test MSE of 30955554768.715908
Epoch 43: training loss 35207119480.471
Test Loss of 29822965199.903748, Test MSE of 29822964860.895439
Epoch 44: training loss 33471353697.882
Test Loss of 27094532453.286442, Test MSE of 27094532705.377312
Epoch 45: training loss 31751871216.941
Test Loss of 26631089962.765385, Test MSE of 26631090114.910011
Epoch 46: training loss 30716929972.706
Test Loss of 24480257308.786674, Test MSE of 24480256899.417629
Epoch 47: training loss 29082224414.118
Test Loss of 23352199693.031006, Test MSE of 23352199292.660957
Epoch 48: training loss 27696992259.765
Test Loss of 25549426322.184174, Test MSE of 25549426650.346989
Epoch 49: training loss 26351301123.765
Test Loss of 22752909706.247108, Test MSE of 22752909670.062065
Epoch 50: training loss 25130755467.294
Test Loss of 22661501063.522442, Test MSE of 22661501081.229504
Epoch 51: training loss 24088952154.353
Test Loss of 22188652268.216568, Test MSE of 22188652429.531090
Epoch 52: training loss 22747815698.824
Test Loss of 22142381940.686718, Test MSE of 22142381995.830257
Epoch 53: training loss 22119562104.471
Test Loss of 23145721170.806107, Test MSE of 23145721226.973682
Epoch 54: training loss 21359230979.765
Test Loss of 21648870058.350765, Test MSE of 21648869919.820259
Epoch 55: training loss 20630686768.941
Test Loss of 18976353106.095325, Test MSE of 18976352962.073090
Epoch 56: training loss 19623473298.824
Test Loss of 20028212547.642757, Test MSE of 20028212274.382008
Epoch 57: training loss 19113215540.706
Test Loss of 18633315879.093014, Test MSE of 18633316171.563339
Epoch 58: training loss 18283614836.706
Test Loss of 20100194717.201294, Test MSE of 20100194881.070202
Epoch 59: training loss 17851921626.353
Test Loss of 20194616915.635353, Test MSE of 20194616846.045891
Epoch 60: training loss 17245028826.353
Test Loss of 18022996078.882000, Test MSE of 18022995939.091187
Epoch 61: training loss 16959994718.118
Test Loss of 18453593321.136513, Test MSE of 18453593183.397572
Epoch 62: training loss 16502106940.235
Test Loss of 17866751111.996300, Test MSE of 17866751187.379650
Epoch 63: training loss 16122478095.059
Test Loss of 20619060125.912079, Test MSE of 20619060076.315365
Epoch 64: training loss 15118302593.882
Test Loss of 19236605676.216568, Test MSE of 19236605643.932945
Epoch 65: training loss 14815930881.882
Test Loss of 18819532819.901897, Test MSE of 18819533174.364037
Epoch 66: training loss 14511962657.882
Test Loss of 19453725510.722813, Test MSE of 19453725350.503857
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 227623604158.5486, 'MSE - std': 208402470094.4505, 'R2 - mean': -0.5796904734303645, 'R2 - std': 1.4244046563224038} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005316 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043049803.294
Test Loss of 431612805885.749207, Test MSE of 431612801807.577942
Epoch 2: training loss 424022310671.059
Test Loss of 431591849468.446106, Test MSE of 431591844919.554077
Epoch 3: training loss 423993985505.882
Test Loss of 431563564664.596008, Test MSE of 431563571952.082153
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009285872.941
Test Loss of 431565270101.767700, Test MSE of 431565275746.850586
Epoch 2: training loss 423998155715.765
Test Loss of 431567463268.101807, Test MSE of 431567457362.822754
Epoch 3: training loss 423997414460.235
Test Loss of 431567219259.468750, Test MSE of 431567208797.018066
Epoch 4: training loss 423996954624.000
Test Loss of 431567706655.037476, Test MSE of 431567711731.084229
Epoch 5: training loss 423996683926.588
Test Loss of 431567030960.984741, Test MSE of 431567026949.413696
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 268412288716.72162, 'MSE - std': 203470240539.29666, 'R2 - mean': -0.908344094774266, 'R2 - std': 1.433595379196026} 
 

Saving model.....
Results After CV: {'MSE - mean': 268412288716.72162, 'MSE - std': 203470240539.29666, 'R2 - mean': -0.908344094774266, 'R2 - std': 1.433595379196026}
Train time: 47.20793489239986
Inference time: 0.07401420000023791
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 17 finished with value: 268412288716.72162 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005550 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525547911.529
Test Loss of 418109955529.400879, Test MSE of 418109957981.499084
Epoch 2: training loss 427505159228.235
Test Loss of 418090746841.389771, Test MSE of 418090749664.386047
Epoch 3: training loss 427478238870.588
Test Loss of 418066064214.636108, Test MSE of 418066063383.652710
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427489146518.588
Test Loss of 418067495250.727722, Test MSE of 418067492379.268311
Epoch 2: training loss 427478007085.176
Test Loss of 418069611989.244507, Test MSE of 418069620694.661743
Epoch 3: training loss 427477723979.294
Test Loss of 418070699048.505188, Test MSE of 418070702743.064087
Epoch 4: training loss 421843898608.941
Test Loss of 400632350589.720093, Test MSE of 400632357331.682617
Epoch 5: training loss 384768068306.824
Test Loss of 345171083525.507263, Test MSE of 345171091180.008423
Epoch 6: training loss 316591146526.118
Test Loss of 268007546631.757568, Test MSE of 268007545730.035736
Epoch 7: training loss 225140409825.882
Test Loss of 168552190237.194550, Test MSE of 168552193706.543610
Epoch 8: training loss 162047166373.647
Test Loss of 130649585555.512375, Test MSE of 130649583891.804657
Epoch 9: training loss 141349612303.059
Test Loss of 119824221099.910248, Test MSE of 119824220618.172775
Epoch 10: training loss 137394160218.353
Test Loss of 115786630251.303268, Test MSE of 115786631343.203690
Epoch 11: training loss 132818128293.647
Test Loss of 113346049909.192688, Test MSE of 113346047797.044144
Epoch 12: training loss 130383896048.941
Test Loss of 110403555421.801529, Test MSE of 110403554043.920151
Epoch 13: training loss 126345140163.765
Test Loss of 106943897768.653244, Test MSE of 106943896427.277939
Epoch 14: training loss 121674489434.353
Test Loss of 104422807765.659027, Test MSE of 104422808464.236557
Epoch 15: training loss 118681515640.471
Test Loss of 100027264257.480453, Test MSE of 100027265215.687057
Epoch 16: training loss 115274749500.235
Test Loss of 97043124757.436966, Test MSE of 97043126880.502258
Epoch 17: training loss 112315002849.882
Test Loss of 92964945679.337494, Test MSE of 92964946416.758301
Epoch 18: training loss 106500455484.235
Test Loss of 90276720891.084900, Test MSE of 90276720392.810638
Epoch 19: training loss 103842715527.529
Test Loss of 85579599039.156143, Test MSE of 85579597968.189041
Epoch 20: training loss 99483307595.294
Test Loss of 83050192681.867218, Test MSE of 83050193876.534042
Epoch 21: training loss 95619774840.471
Test Loss of 81429504289.931992, Test MSE of 81429503592.658401
Epoch 22: training loss 91142560512.000
Test Loss of 76514428066.257690, Test MSE of 76514430009.642975
Epoch 23: training loss 87185500619.294
Test Loss of 75600418482.009720, Test MSE of 75600418529.520172
Epoch 24: training loss 84244708886.588
Test Loss of 72022513472.133240, Test MSE of 72022513665.449280
Epoch 25: training loss 81306329291.294
Test Loss of 68838619135.526260, Test MSE of 68838619261.766373
Epoch 26: training loss 76193358072.471
Test Loss of 66834475978.703674, Test MSE of 66834475995.774147
Epoch 27: training loss 73415504534.588
Test Loss of 61967282929.254684, Test MSE of 61967283001.406120
Epoch 28: training loss 70433620013.176
Test Loss of 59298034849.310204, Test MSE of 59298035203.130440
Epoch 29: training loss 66953015175.529
Test Loss of 57278746486.613922, Test MSE of 57278746827.326332
Epoch 30: training loss 63739495740.235
Test Loss of 51575725060.500580, Test MSE of 51575724476.853775
Epoch 31: training loss 60803373176.471
Test Loss of 52669721241.611847, Test MSE of 52669721723.045212
Epoch 32: training loss 58744708600.471
Test Loss of 49524831806.415916, Test MSE of 49524831057.924286
Epoch 33: training loss 55180234307.765
Test Loss of 47740502994.757347, Test MSE of 47740502750.148277
Epoch 34: training loss 52324148009.412
Test Loss of 41838416994.775848, Test MSE of 41838417210.014168
Epoch 35: training loss 50173033633.882
Test Loss of 41205156276.319221, Test MSE of 41205156275.684624
Epoch 36: training loss 47819736192.000
Test Loss of 36945076718.589867, Test MSE of 36945076839.999939
Epoch 37: training loss 44874822512.941
Test Loss of 39882381267.941704, Test MSE of 39882381752.475105
Epoch 38: training loss 42245095597.176
Test Loss of 34735959272.845711, Test MSE of 34735959643.133194
Epoch 39: training loss 40921400071.529
Test Loss of 34117268766.378902, Test MSE of 34117269473.975624
Epoch 40: training loss 38376047296.000
Test Loss of 33820714703.381912, Test MSE of 33820714542.083260
Epoch 41: training loss 37143453613.176
Test Loss of 30840510492.898449, Test MSE of 30840510910.679672
Epoch 42: training loss 34995416101.647
Test Loss of 33719373944.331253, Test MSE of 33719374178.014790
Epoch 43: training loss 33243980468.706
Test Loss of 31313354154.133705, Test MSE of 31313354144.846798
Epoch 44: training loss 31598120308.706
Test Loss of 27865232532.992828, Test MSE of 27865232888.824421
Epoch 45: training loss 30485388980.706
Test Loss of 23480507558.521397, Test MSE of 23480507448.802345
Epoch 46: training loss 28741906496.000
Test Loss of 26483344027.980568, Test MSE of 26483344436.656242
Epoch 47: training loss 27822012412.235
Test Loss of 25582650407.083969, Test MSE of 25582650352.881603
Epoch 48: training loss 26874147403.294
Test Loss of 26498397297.935692, Test MSE of 26498397075.388283
Epoch 49: training loss 25489286592.000
Test Loss of 23096969598.312283, Test MSE of 23096969622.801277
Epoch 50: training loss 24496040071.529
Test Loss of 21594395443.578995, Test MSE of 21594395029.835243
Epoch 51: training loss 23373392816.941
Test Loss of 21760356492.228546, Test MSE of 21760356496.679893
Epoch 52: training loss 22703612065.882
Test Loss of 22991990134.021744, Test MSE of 22991990778.041386
Epoch 53: training loss 21548736207.059
Test Loss of 19894408466.061531, Test MSE of 19894408477.243248
Epoch 54: training loss 21020275843.765
Test Loss of 20657373953.835762, Test MSE of 20657374117.551758
Epoch 55: training loss 20417039928.471
Test Loss of 19863028869.832985, Test MSE of 19863028928.465599
Epoch 56: training loss 19277747632.941
Test Loss of 19826563115.584549, Test MSE of 19826563663.466187
Epoch 57: training loss 18965979437.176
Test Loss of 19948987722.910942, Test MSE of 19948987801.436111
Epoch 58: training loss 18312328711.529
Test Loss of 21322164950.251213, Test MSE of 21322165135.789795
Epoch 59: training loss 17722447232.000
Test Loss of 18281845231.537357, Test MSE of 18281845621.106674
Epoch 60: training loss 16840286117.647
Test Loss of 19892230358.606522, Test MSE of 19892230209.867229
Epoch 61: training loss 16442765741.176
Test Loss of 18660051095.124683, Test MSE of 18660051097.941162
Epoch 62: training loss 16202619911.529
Test Loss of 17909464987.566044, Test MSE of 17909465057.980568
Epoch 63: training loss 15818138051.765
Test Loss of 18557948447.385612, Test MSE of 18557948644.333324
Epoch 64: training loss 15125521705.412
Test Loss of 17989687644.202637, Test MSE of 17989687637.387318
Epoch 65: training loss 14469844163.765
Test Loss of 17521956636.128613, Test MSE of 17521956784.469074
Epoch 66: training loss 14631700592.941
Test Loss of 19651893279.740921, Test MSE of 19651893718.726494
Epoch 67: training loss 14135462320.941
Test Loss of 18707253951.037704, Test MSE of 18707254196.589973
Epoch 68: training loss 13673078761.412
Test Loss of 19940268124.854038, Test MSE of 19940267999.582912
Epoch 69: training loss 13351548899.765
Test Loss of 19302675705.426788, Test MSE of 19302675699.406555
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19302675699.406555, 'MSE - std': 0.0, 'R2 - mean': 0.8496879700043385, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005383 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918350938.353
Test Loss of 424556208675.649292, Test MSE of 424556211860.697693
Epoch 2: training loss 427898392094.118
Test Loss of 424539648624.395996, Test MSE of 424539651004.670166
Epoch 3: training loss 427871297776.941
Test Loss of 424517499374.116150, Test MSE of 424517498868.676758
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888952500.706
Test Loss of 424525300212.274780, Test MSE of 424525301992.908447
Epoch 2: training loss 427876581135.059
Test Loss of 424526619361.147339, Test MSE of 424526621056.989746
Epoch 3: training loss 427876204303.059
Test Loss of 424526487197.638672, Test MSE of 424526489579.469055
Epoch 4: training loss 422643386970.353
Test Loss of 407715902519.191284, Test MSE of 407715909955.696777
Epoch 5: training loss 386241913434.353
Test Loss of 354272080450.916504, Test MSE of 354272080970.521423
Epoch 6: training loss 318319509985.882
Test Loss of 277090134171.862122, Test MSE of 277090132881.211426
Epoch 7: training loss 225761819346.824
Test Loss of 180383678001.624786, Test MSE of 180383677829.723297
Epoch 8: training loss 160607460562.824
Test Loss of 141716591629.501740, Test MSE of 141716591654.730896
Epoch 9: training loss 139707716186.353
Test Loss of 131789811635.253296, Test MSE of 131789812590.188461
Epoch 10: training loss 134338777961.412
Test Loss of 126781117919.666901, Test MSE of 126781118986.897156
Epoch 11: training loss 130443807984.941
Test Loss of 124663747358.734207, Test MSE of 124663751057.300827
Epoch 12: training loss 128101656365.176
Test Loss of 121215617985.228775, Test MSE of 121215618160.225525
Epoch 13: training loss 124265929125.647
Test Loss of 118109597242.152206, Test MSE of 118109595607.959610
Epoch 14: training loss 119944115350.588
Test Loss of 112383041666.753647, Test MSE of 112383041317.359268
Epoch 15: training loss 116335943619.765
Test Loss of 108933697082.152206, Test MSE of 108933695618.428833
Epoch 16: training loss 112348664018.824
Test Loss of 106289736286.867447, Test MSE of 106289736279.391220
Epoch 17: training loss 108555983992.471
Test Loss of 100922281534.889664, Test MSE of 100922281919.218216
Epoch 18: training loss 104536500705.882
Test Loss of 99747907911.357849, Test MSE of 99747908027.140686
Epoch 19: training loss 100639718128.941
Test Loss of 94607275779.493866, Test MSE of 94607273759.381592
Epoch 20: training loss 96470015789.176
Test Loss of 90827056408.693970, Test MSE of 90827059379.163116
Epoch 21: training loss 91916866258.824
Test Loss of 86922388711.424469, Test MSE of 86922389653.807373
Epoch 22: training loss 88375985664.000
Test Loss of 82978224617.378677, Test MSE of 82978223665.094986
Epoch 23: training loss 85287530721.882
Test Loss of 80331615171.123764, Test MSE of 80331615748.800507
Epoch 24: training loss 81082290522.353
Test Loss of 75734681386.340973, Test MSE of 75734681701.605637
Epoch 25: training loss 77371543988.706
Test Loss of 74997624982.650940, Test MSE of 74997625507.494278
Epoch 26: training loss 73503334219.294
Test Loss of 68465016941.908859, Test MSE of 68465016059.321739
Epoch 27: training loss 70176660193.882
Test Loss of 64711279385.049271, Test MSE of 64711279605.844841
Epoch 28: training loss 66934001106.824
Test Loss of 62678692087.294937, Test MSE of 62678692352.167580
Epoch 29: training loss 63031234936.471
Test Loss of 61174779848.098083, Test MSE of 61174780731.465408
Epoch 30: training loss 60475508013.176
Test Loss of 57394084368.225769, Test MSE of 57394083696.306763
Epoch 31: training loss 57195076321.882
Test Loss of 53300559142.432571, Test MSE of 53300559889.812294
Epoch 32: training loss 54306383028.706
Test Loss of 50436150983.328247, Test MSE of 50436150838.311226
Epoch 33: training loss 51583892668.235
Test Loss of 47253597699.197777, Test MSE of 47253599257.733643
Epoch 34: training loss 48584056289.882
Test Loss of 43140622813.061302, Test MSE of 43140621563.718124
Epoch 35: training loss 45784574667.294
Test Loss of 44690846961.846863, Test MSE of 44690846499.187401
Epoch 36: training loss 43610038204.235
Test Loss of 41683873323.466110, Test MSE of 41683872976.443535
Epoch 37: training loss 41304710844.235
Test Loss of 40193718417.913483, Test MSE of 40193718538.686615
Epoch 38: training loss 39111218996.706
Test Loss of 38466780869.907005, Test MSE of 38466781174.009865
Epoch 39: training loss 36826046057.412
Test Loss of 36244066159.981491, Test MSE of 36244065790.732880
Epoch 40: training loss 35096832903.529
Test Loss of 36679690249.948647, Test MSE of 36679689871.444481
Epoch 41: training loss 33749199661.176
Test Loss of 31969658524.691185, Test MSE of 31969658751.307648
Epoch 42: training loss 31950442488.471
Test Loss of 34499222693.100159, Test MSE of 34499223367.234512
Epoch 43: training loss 29776145551.059
Test Loss of 32248236406.258617, Test MSE of 32248236914.851742
Epoch 44: training loss 28264882913.882
Test Loss of 33299021858.583389, Test MSE of 33299021944.486237
Epoch 45: training loss 27213058989.176
Test Loss of 30132955647.170944, Test MSE of 30132954493.899998
Epoch 46: training loss 25728898778.353
Test Loss of 29002777550.493637, Test MSE of 29002778165.427967
Epoch 47: training loss 24554030479.059
Test Loss of 27746053710.049503, Test MSE of 27746053342.021446
Epoch 48: training loss 23406078275.765
Test Loss of 30369000611.205181, Test MSE of 30368999629.180317
Epoch 49: training loss 22039624195.765
Test Loss of 28711204971.540134, Test MSE of 28711204999.916901
Epoch 50: training loss 21647679936.000
Test Loss of 28991967083.244045, Test MSE of 28991967064.431156
Epoch 51: training loss 20444784922.353
Test Loss of 25637234979.590099, Test MSE of 25637234857.871552
Epoch 52: training loss 19477249648.941
Test Loss of 25487157669.633125, Test MSE of 25487158100.116020
Epoch 53: training loss 18675087604.706
Test Loss of 27372040074.511219, Test MSE of 27372039693.669197
Epoch 54: training loss 18169419132.235
Test Loss of 26217159811.938007, Test MSE of 26217160074.995861
Epoch 55: training loss 17397194394.353
Test Loss of 26305769120.007401, Test MSE of 26305769471.370010
Epoch 56: training loss 16757046949.647
Test Loss of 25886404322.568588, Test MSE of 25886404545.373135
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22594540122.389847, 'MSE - std': 3291864422.9832897, 'R2 - mean': 0.8324381141344086, 'R2 - std': 0.017249855869929942} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005531 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927533266.824
Test Loss of 447257720094.378906, Test MSE of 447257726610.572876
Epoch 2: training loss 421907671160.471
Test Loss of 447239310299.758484, Test MSE of 447239311930.162476
Epoch 3: training loss 421881156668.235
Test Loss of 447215189443.715942, Test MSE of 447215190991.555908
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895496764.235
Test Loss of 447226682453.984741, Test MSE of 447226680639.141235
Epoch 2: training loss 421886984432.941
Test Loss of 447226065054.230835, Test MSE of 447226071850.827637
Epoch 3: training loss 421886691809.882
Test Loss of 447225543585.961609, Test MSE of 447225549728.840149
Epoch 4: training loss 416484096722.824
Test Loss of 430120913989.877380, Test MSE of 430120918885.685181
Epoch 5: training loss 380123084920.471
Test Loss of 374729760508.387695, Test MSE of 374729763021.755676
Epoch 6: training loss 312368769385.412
Test Loss of 296470838151.431885, Test MSE of 296470837914.935364
Epoch 7: training loss 220733939350.588
Test Loss of 196350534558.645386, Test MSE of 196350538053.804230
Epoch 8: training loss 156598943141.647
Test Loss of 154167636004.952118, Test MSE of 154167636390.078033
Epoch 9: training loss 137403456180.706
Test Loss of 142155414058.755493, Test MSE of 142155411765.915405
Epoch 10: training loss 133225797451.294
Test Loss of 137504320966.558411, Test MSE of 137504321816.012085
Epoch 11: training loss 128345285722.353
Test Loss of 134849728372.718948, Test MSE of 134849729331.237946
Epoch 12: training loss 126788661760.000
Test Loss of 131166032585.460098, Test MSE of 131166033908.872162
Epoch 13: training loss 122759787068.235
Test Loss of 126749830052.804077, Test MSE of 126749830558.488861
Epoch 14: training loss 118909109850.353
Test Loss of 124317260105.252838, Test MSE of 124317259515.777802
Epoch 15: training loss 114870712410.353
Test Loss of 120624370510.108719, Test MSE of 120624369273.297791
Epoch 16: training loss 111307511416.471
Test Loss of 116736464016.255371, Test MSE of 116736466341.275909
Epoch 17: training loss 107171697543.529
Test Loss of 113422184882.897995, Test MSE of 113422183506.406036
Epoch 18: training loss 103776977859.765
Test Loss of 107100052971.510529, Test MSE of 107100054084.942490
Epoch 19: training loss 99794640384.000
Test Loss of 105368447594.237335, Test MSE of 105368449807.275879
Epoch 20: training loss 96161647555.765
Test Loss of 103290858184.986359, Test MSE of 103290859774.775208
Epoch 21: training loss 92066906970.353
Test Loss of 98699595415.006241, Test MSE of 98699598226.899338
Epoch 22: training loss 88276035538.824
Test Loss of 94829552845.842239, Test MSE of 94829552433.192520
Epoch 23: training loss 85245615661.176
Test Loss of 87950831302.380753, Test MSE of 87950830583.685501
Epoch 24: training loss 81570002868.706
Test Loss of 86644919155.534576, Test MSE of 86644917795.385895
Epoch 25: training loss 78022095450.353
Test Loss of 80860837698.028229, Test MSE of 80860837877.604172
Epoch 26: training loss 74633353291.294
Test Loss of 77923555643.987976, Test MSE of 77923554184.640244
Epoch 27: training loss 70755887013.647
Test Loss of 76410138048.873474, Test MSE of 76410140242.255783
Epoch 28: training loss 68326638652.235
Test Loss of 70728393043.201477, Test MSE of 70728395042.033997
Epoch 29: training loss 65602353829.647
Test Loss of 65944171563.821419, Test MSE of 65944171521.972229
Epoch 30: training loss 61704736828.235
Test Loss of 67129771318.066154, Test MSE of 67129772439.870911
Epoch 31: training loss 58640038023.529
Test Loss of 61259818462.719406, Test MSE of 61259818249.071152
Epoch 32: training loss 55871083158.588
Test Loss of 57295980713.126999, Test MSE of 57295979450.039825
Epoch 33: training loss 53118547719.529
Test Loss of 59015523322.078186, Test MSE of 59015523014.314148
Epoch 34: training loss 50622799811.765
Test Loss of 54611407519.770531, Test MSE of 54611408255.871384
Epoch 35: training loss 47509951179.294
Test Loss of 50223909950.297478, Test MSE of 50223909297.912277
Epoch 36: training loss 45902855898.353
Test Loss of 48584857748.282211, Test MSE of 48584858923.688011
Epoch 37: training loss 43118645285.647
Test Loss of 49442381222.343742, Test MSE of 49442381966.835182
Epoch 38: training loss 40682750418.824
Test Loss of 43300690375.269028, Test MSE of 43300691244.547302
Epoch 39: training loss 38879598004.706
Test Loss of 44655913278.830444, Test MSE of 44655913807.159576
Epoch 40: training loss 37182645157.647
Test Loss of 42607242773.200096, Test MSE of 42607243258.012634
Epoch 41: training loss 35413526076.235
Test Loss of 36466074699.088593, Test MSE of 36466074890.427322
Epoch 42: training loss 33915683614.118
Test Loss of 41554822695.676147, Test MSE of 41554823599.363029
Epoch 43: training loss 32288037270.588
Test Loss of 34314950465.080730, Test MSE of 34314950231.410767
Epoch 44: training loss 30211202868.706
Test Loss of 37790114744.227623, Test MSE of 37790114846.705482
Epoch 45: training loss 29535577539.765
Test Loss of 34398897020.535736, Test MSE of 34398896661.800545
Epoch 46: training loss 27383693432.471
Test Loss of 30355610025.896832, Test MSE of 30355610046.914383
Epoch 47: training loss 26709781601.882
Test Loss of 29246410266.411289, Test MSE of 29246411083.836395
Epoch 48: training loss 25512318004.706
Test Loss of 32738882319.574371, Test MSE of 32738882222.495083
Epoch 49: training loss 24231505750.588
Test Loss of 31459871354.344669, Test MSE of 31459871055.209045
Epoch 50: training loss 23044155181.176
Test Loss of 29235492060.528336, Test MSE of 29235491674.259911
Epoch 51: training loss 22351326629.647
Test Loss of 29157703598.042099, Test MSE of 29157703606.506550
Epoch 52: training loss 21398816030.118
Test Loss of 25348138922.252140, Test MSE of 25348139213.281082
Epoch 53: training loss 20910732333.176
Test Loss of 26923229046.140179, Test MSE of 26923228966.247173
Epoch 54: training loss 20127095085.176
Test Loss of 25582479789.213047, Test MSE of 25582480119.612076
Epoch 55: training loss 19141898191.059
Test Loss of 28459942374.536201, Test MSE of 28459942859.058662
Epoch 56: training loss 18687663774.118
Test Loss of 25896807943.935230, Test MSE of 25896807930.410591
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23695296058.39676, 'MSE - std': 3106054546.770065, 'R2 - mean': 0.8308276289714821, 'R2 - std': 0.014267410765175677} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005466 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110642176.000
Test Loss of 410763573574.959717, Test MSE of 410763569327.457642
Epoch 2: training loss 430090237108.706
Test Loss of 410745121870.186035, Test MSE of 410745119019.105164
Epoch 3: training loss 430063134358.588
Test Loss of 410721210707.279968, Test MSE of 410721211781.016357
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074847232.000
Test Loss of 410728403882.232300, Test MSE of 410728411319.110962
Epoch 2: training loss 430064606750.118
Test Loss of 410728214628.457214, Test MSE of 410728209083.915222
Epoch 3: training loss 430064372796.235
Test Loss of 410727876773.849121, Test MSE of 410727882854.215759
Epoch 4: training loss 424751407104.000
Test Loss of 394064979394.635803, Test MSE of 394064976986.542358
Epoch 5: training loss 388488074300.235
Test Loss of 339596675662.422974, Test MSE of 339596676500.487183
Epoch 6: training loss 320210636800.000
Test Loss of 260136693322.158264, Test MSE of 260136691246.830048
Epoch 7: training loss 228273877413.647
Test Loss of 164511963322.698761, Test MSE of 164511962491.862213
Epoch 8: training loss 163037442258.824
Test Loss of 125211016131.820450, Test MSE of 125211015168.763474
Epoch 9: training loss 145041186364.235
Test Loss of 114028990704.718185, Test MSE of 114028990366.744659
Epoch 10: training loss 138906636438.588
Test Loss of 110519374675.043030, Test MSE of 110519373592.869736
Epoch 11: training loss 134327019550.118
Test Loss of 107180777551.607590, Test MSE of 107180777674.190247
Epoch 12: training loss 131660275712.000
Test Loss of 104984740480.651550, Test MSE of 104984740063.912277
Epoch 13: training loss 128113183322.353
Test Loss of 100450029789.763992, Test MSE of 100450026734.841187
Epoch 14: training loss 124102835561.412
Test Loss of 98226905539.109665, Test MSE of 98226904383.668777
Epoch 15: training loss 119855397225.412
Test Loss of 95328366821.345673, Test MSE of 95328365891.983826
Epoch 16: training loss 116327936451.765
Test Loss of 91777137342.252655, Test MSE of 91777137009.513351
Epoch 17: training loss 112396573515.294
Test Loss of 89063427510.315598, Test MSE of 89063427780.533661
Epoch 18: training loss 107850439981.176
Test Loss of 84908424670.119385, Test MSE of 84908424873.316620
Epoch 19: training loss 104427575567.059
Test Loss of 81152209144.299866, Test MSE of 81152209860.564240
Epoch 20: training loss 100479337502.118
Test Loss of 79454436629.205002, Test MSE of 79454437243.422699
Epoch 21: training loss 96522605899.294
Test Loss of 74932453368.418320, Test MSE of 74932455029.911987
Epoch 22: training loss 92948716092.235
Test Loss of 73890824464.940308, Test MSE of 73890824483.339401
Epoch 23: training loss 89624600801.882
Test Loss of 68774556950.152710, Test MSE of 68774555736.380020
Epoch 24: training loss 85663327262.118
Test Loss of 67761339661.623322, Test MSE of 67761340156.638321
Epoch 25: training loss 81893261808.941
Test Loss of 63325055983.415085, Test MSE of 63325056415.747437
Epoch 26: training loss 77555191567.059
Test Loss of 62012073588.805183, Test MSE of 62012073230.376617
Epoch 27: training loss 74060744041.412
Test Loss of 59339893830.604347, Test MSE of 59339893027.328056
Epoch 28: training loss 71405263314.824
Test Loss of 55278994492.179550, Test MSE of 55278993758.486580
Epoch 29: training loss 67313885214.118
Test Loss of 54197533448.173996, Test MSE of 54197534348.936707
Epoch 30: training loss 64938407454.118
Test Loss of 51068453180.534935, Test MSE of 51068453765.187210
Epoch 31: training loss 61190219294.118
Test Loss of 47573043576.240631, Test MSE of 47573043843.459602
Epoch 32: training loss 58228008869.647
Test Loss of 43882882978.176773, Test MSE of 43882883122.534966
Epoch 33: training loss 55304281479.529
Test Loss of 44494827298.236000, Test MSE of 44494827502.359047
Epoch 34: training loss 52812268589.176
Test Loss of 39833171347.250343, Test MSE of 39833171283.070335
Epoch 35: training loss 50659437620.706
Test Loss of 38967241082.136047, Test MSE of 38967240718.142235
Epoch 36: training loss 47582990960.941
Test Loss of 34809912225.702919, Test MSE of 34809911993.603394
Epoch 37: training loss 45459369509.647
Test Loss of 34600125340.964371, Test MSE of 34600125188.828888
Epoch 38: training loss 42073455344.941
Test Loss of 34951736684.394264, Test MSE of 34951736875.709122
Epoch 39: training loss 40807965974.588
Test Loss of 31883940402.939381, Test MSE of 31883939966.962994
Epoch 40: training loss 38778196231.529
Test Loss of 30861491988.968071, Test MSE of 30861491329.901840
Epoch 41: training loss 36684309029.647
Test Loss of 29830653945.366035, Test MSE of 29830653802.651043
Epoch 42: training loss 34708249562.353
Test Loss of 26049034110.163815, Test MSE of 26049033811.699566
Epoch 43: training loss 33381879412.706
Test Loss of 26963818042.047199, Test MSE of 26963818181.426964
Epoch 44: training loss 32175906243.765
Test Loss of 26055972013.904675, Test MSE of 26055972164.296680
Epoch 45: training loss 30300923979.294
Test Loss of 23577711431.670525, Test MSE of 23577711606.902996
Epoch 46: training loss 28907296907.294
Test Loss of 24562374381.164276, Test MSE of 24562374576.333809
Epoch 47: training loss 27841878810.353
Test Loss of 25338424437.042110, Test MSE of 25338424097.573219
Epoch 48: training loss 26490423582.118
Test Loss of 22132354976.281353, Test MSE of 22132355104.671558
Epoch 49: training loss 25589995685.647
Test Loss of 22058185320.958817, Test MSE of 22058185571.492310
Epoch 50: training loss 24334275516.235
Test Loss of 22234615778.621010, Test MSE of 22234616004.004021
Epoch 51: training loss 23214783905.882
Test Loss of 23367365910.152706, Test MSE of 23367365578.927124
Epoch 52: training loss 22548873340.235
Test Loss of 18546780821.501156, Test MSE of 18546780383.230621
Epoch 53: training loss 21581783085.176
Test Loss of 18944052978.850533, Test MSE of 18944052904.668800
Epoch 54: training loss 21164731787.294
Test Loss of 19893444319.896343, Test MSE of 19893444226.594589
Epoch 55: training loss 20346700781.176
Test Loss of 18802371759.800091, Test MSE of 18802371833.339008
Epoch 56: training loss 19393991631.059
Test Loss of 21677710979.020824, Test MSE of 21677710682.779419
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23190899714.492424, 'MSE - std': 2828237640.45128, 'R2 - mean': 0.8283913607687691, 'R2 - std': 0.013056625357393796} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005377 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043083896.471
Test Loss of 431611145544.381287, Test MSE of 431611141418.914978
Epoch 2: training loss 424024001234.824
Test Loss of 431591342557.645508, Test MSE of 431591339835.415039
Epoch 3: training loss 423997885500.235
Test Loss of 431565004985.751038, Test MSE of 431565011396.789673
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012802288.941
Test Loss of 431567207432.055542, Test MSE of 431567199620.057983
Epoch 2: training loss 423998888056.471
Test Loss of 431568795944.633057, Test MSE of 431568793112.768616
Epoch 3: training loss 423998465325.176
Test Loss of 431568133366.878296, Test MSE of 431568134499.897583
Epoch 4: training loss 418879553415.529
Test Loss of 414618883623.093018, Test MSE of 414618889216.087708
Epoch 5: training loss 383098157658.353
Test Loss of 359666033515.683472, Test MSE of 359666035937.317200
Epoch 6: training loss 315400046712.471
Test Loss of 280916830749.142090, Test MSE of 280916828662.425171
Epoch 7: training loss 224588024741.647
Test Loss of 180673742566.530304, Test MSE of 180673744478.325836
Epoch 8: training loss 160195287416.471
Test Loss of 137926468842.558075, Test MSE of 137926468397.568207
Epoch 9: training loss 139801643339.294
Test Loss of 125495583152.155487, Test MSE of 125495583961.977493
Epoch 10: training loss 135174950881.882
Test Loss of 120946365376.977325, Test MSE of 120946364633.797775
Epoch 11: training loss 131560421135.059
Test Loss of 117622314061.238312, Test MSE of 117622312559.825745
Epoch 12: training loss 127468377133.176
Test Loss of 113910501946.521057, Test MSE of 113910504503.703735
Epoch 13: training loss 124422471800.471
Test Loss of 109012335589.937988, Test MSE of 109012335335.497711
Epoch 14: training loss 121351207966.118
Test Loss of 107380693292.423874, Test MSE of 107380694447.695938
Epoch 15: training loss 116359282025.412
Test Loss of 103055030966.670990, Test MSE of 103055031027.662521
Epoch 16: training loss 113157904850.824
Test Loss of 99053042851.479874, Test MSE of 99053045810.607819
Epoch 17: training loss 108623085025.882
Test Loss of 94867225259.298477, Test MSE of 94867224890.428192
Epoch 18: training loss 105925077443.765
Test Loss of 94833005220.664505, Test MSE of 94833004968.151566
Epoch 19: training loss 100603704854.588
Test Loss of 87145333164.364639, Test MSE of 87145333313.125549
Epoch 20: training loss 97317933718.588
Test Loss of 85501777648.007401, Test MSE of 85501779450.170593
Epoch 21: training loss 93956611734.588
Test Loss of 83606174940.342438, Test MSE of 83606175044.260162
Epoch 22: training loss 89281518787.765
Test Loss of 79070582580.242477, Test MSE of 79070582435.563828
Epoch 23: training loss 84926509568.000
Test Loss of 73742650937.099487, Test MSE of 73742651967.305359
Epoch 24: training loss 82110614317.176
Test Loss of 70355632713.684402, Test MSE of 70355631871.183167
Epoch 25: training loss 78676498876.235
Test Loss of 70088504727.041183, Test MSE of 70088505275.305527
Epoch 26: training loss 75867056632.471
Test Loss of 64229385655.263306, Test MSE of 64229386729.817650
Epoch 27: training loss 71473066601.412
Test Loss of 60412032204.231377, Test MSE of 60412032218.866844
Epoch 28: training loss 68395694727.529
Test Loss of 56854471912.662659, Test MSE of 56854472164.678619
Epoch 29: training loss 65253617152.000
Test Loss of 58012895867.913002, Test MSE of 58012895478.129257
Epoch 30: training loss 62761811516.235
Test Loss of 53062644020.953262, Test MSE of 53062644555.080383
Epoch 31: training loss 58608327137.882
Test Loss of 47978934624.547897, Test MSE of 47978934532.110771
Epoch 32: training loss 56058726912.000
Test Loss of 46607109539.361404, Test MSE of 46607110226.613777
Epoch 33: training loss 53150361773.176
Test Loss of 46744669495.322533, Test MSE of 46744668898.823647
Epoch 34: training loss 50780830659.765
Test Loss of 41790759636.049980, Test MSE of 41790759121.535210
Epoch 35: training loss 48912844739.765
Test Loss of 42795100348.594170, Test MSE of 42795099981.270187
Epoch 36: training loss 46342941101.176
Test Loss of 35502961498.624710, Test MSE of 35502961138.307770
Epoch 37: training loss 43598761118.118
Test Loss of 41499843180.749657, Test MSE of 41499842644.435600
Epoch 38: training loss 41002573590.588
Test Loss of 37288103865.869507, Test MSE of 37288103653.536560
Epoch 39: training loss 39970323440.941
Test Loss of 32361656881.043961, Test MSE of 32361656963.447178
Epoch 40: training loss 37581655495.529
Test Loss of 31910475168.518280, Test MSE of 31910475378.786510
Epoch 41: training loss 35946771297.882
Test Loss of 29382449987.879684, Test MSE of 29382450744.268005
Epoch 42: training loss 34078509786.353
Test Loss of 28096415887.104118, Test MSE of 28096415409.425541
Epoch 43: training loss 32996813974.588
Test Loss of 28665352771.050438, Test MSE of 28665352840.313881
Epoch 44: training loss 30921637744.941
Test Loss of 24829464086.034245, Test MSE of 24829463974.469097
Epoch 45: training loss 29536963395.765
Test Loss of 25690457448.603424, Test MSE of 25690457883.988834
Epoch 46: training loss 28488749202.824
Test Loss of 25086367886.156410, Test MSE of 25086367731.728729
Epoch 47: training loss 26623055137.882
Test Loss of 21982404444.993984, Test MSE of 21982404233.101704
Epoch 48: training loss 25934213278.118
Test Loss of 23650600753.399353, Test MSE of 23650600736.628761
Epoch 49: training loss 24721993008.941
Test Loss of 23178225161.714020, Test MSE of 23178225290.824886
Epoch 50: training loss 23624968545.882
Test Loss of 21809931527.937065, Test MSE of 21809931867.804119
Epoch 51: training loss 22797200380.235
Test Loss of 22971382572.186951, Test MSE of 22971382894.739883
Epoch 52: training loss 21972875166.118
Test Loss of 20711635068.149929, Test MSE of 20711635257.982475
Epoch 53: training loss 21520387527.529
Test Loss of 20398867498.173069, Test MSE of 20398867128.154587
Epoch 54: training loss 20182708818.824
Test Loss of 20209083401.003239, Test MSE of 20209083593.379448
Epoch 55: training loss 20223051346.824
Test Loss of 22160687748.442387, Test MSE of 22160687380.480045
Epoch 56: training loss 18989769253.647
Test Loss of 19862580620.616383, Test MSE of 19862580510.938446
Epoch 57: training loss 18558887348.706
Test Loss of 22084509426.850533, Test MSE of 22084509873.959709
Epoch 58: training loss 18082758671.059
Test Loss of 18384867621.789913, Test MSE of 18384867367.186417
Epoch 59: training loss 17768289163.294
Test Loss of 20633050455.544655, Test MSE of 20633050574.434891
Epoch 60: training loss 16743518102.588
Test Loss of 20509329168.229523, Test MSE of 20509328951.345036
Epoch 61: training loss 16560613552.941
Test Loss of 21042188023.589081, Test MSE of 21042188148.851044
Epoch 62: training loss 16138115395.765
Test Loss of 18248785506.798706, Test MSE of 18248785776.100647
Epoch 63: training loss 15854078144.000
Test Loss of 19116482806.404442, Test MSE of 19116482761.264526
Epoch 64: training loss 15201989191.529
Test Loss of 21026739197.630726, Test MSE of 21026739584.562813
Epoch 65: training loss 14909498266.353
Test Loss of 18627966257.636280, Test MSE of 18627966503.704556
Epoch 66: training loss 14599077831.529
Test Loss of 18251466736.836651, Test MSE of 18251466724.929756
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22203013116.579887, 'MSE - std': 3209800966.1088243, 'R2 - mean': 0.835452562389351, 'R2 - std': 0.0183254644111604} 
 

Saving model.....
Results After CV: {'MSE - mean': 22203013116.579887, 'MSE - std': 3209800966.1088243, 'R2 - mean': 0.835452562389351, 'R2 - std': 0.0183254644111604}
Train time: 94.88385139579987
Inference time: 0.0738487153997994
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 18 finished with value: 22203013116.579887 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005453 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525191318.588
Test Loss of 418111359474.853577, Test MSE of 418111364841.579468
Epoch 2: training loss 427503873807.059
Test Loss of 418092027150.745300, Test MSE of 418092031463.089966
Epoch 3: training loss 427475929931.294
Test Loss of 418066847698.046753, Test MSE of 418066847123.440552
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490337852.235
Test Loss of 418072764402.735107, Test MSE of 418072762808.386353
Epoch 2: training loss 427479116739.765
Test Loss of 418072972979.430969, Test MSE of 418072972926.603882
Epoch 3: training loss 427478703525.647
Test Loss of 418072121784.582947, Test MSE of 418072120079.223938
Epoch 4: training loss 427478378014.118
Test Loss of 418071311263.592896, Test MSE of 418071312126.214478
Epoch 5: training loss 427478142494.118
Test Loss of 418071012326.654663, Test MSE of 418071008724.771362
Epoch 6: training loss 418801903134.118
Test Loss of 391062689317.781189, Test MSE of 391062696966.558289
Epoch 7: training loss 364033154831.059
Test Loss of 312958879416.642151, Test MSE of 312958877797.274048
Epoch 8: training loss 276781190987.294
Test Loss of 223236566879.874176, Test MSE of 223236561066.638275
Epoch 9: training loss 199394807296.000
Test Loss of 158778599219.815857, Test MSE of 158778601636.228485
Epoch 10: training loss 156288434868.706
Test Loss of 129151598709.488785, Test MSE of 129151601816.941956
Epoch 11: training loss 140782455898.353
Test Loss of 117965335187.926895, Test MSE of 117965339573.565109
Epoch 12: training loss 134526104154.353
Test Loss of 114325244296.734680, Test MSE of 114325241852.429794
Epoch 13: training loss 129981807254.588
Test Loss of 111627761906.083740, Test MSE of 111627761652.735046
Epoch 14: training loss 128028749025.882
Test Loss of 109305442670.678696, Test MSE of 109305444187.011047
Epoch 15: training loss 125340960225.882
Test Loss of 105201368079.870453, Test MSE of 105201369473.778168
Epoch 16: training loss 122010615386.353
Test Loss of 102772931232.244278, Test MSE of 102772931168.354202
Epoch 17: training loss 116657941022.118
Test Loss of 96762847999.467041, Test MSE of 96762849627.276047
Epoch 18: training loss 112137774682.353
Test Loss of 94470625641.467499, Test MSE of 94470625886.853256
Epoch 19: training loss 108843387376.941
Test Loss of 91153550036.356232, Test MSE of 91153550174.878708
Epoch 20: training loss 105592765485.176
Test Loss of 89312443208.897522, Test MSE of 89312440820.298340
Epoch 21: training loss 101640830569.412
Test Loss of 84687656252.698593, Test MSE of 84687656393.773773
Epoch 22: training loss 96732376741.647
Test Loss of 80467057827.678925, Test MSE of 80467058577.322952
Epoch 23: training loss 93511623936.000
Test Loss of 77078859746.627808, Test MSE of 77078860840.783890
Epoch 24: training loss 89658290176.000
Test Loss of 75167716788.082352, Test MSE of 75167717724.395187
Epoch 25: training loss 85913972690.824
Test Loss of 72978997340.854034, Test MSE of 72978996520.314102
Epoch 26: training loss 82556769716.706
Test Loss of 66495926017.125145, Test MSE of 66495925771.361351
Epoch 27: training loss 78175991988.706
Test Loss of 65306347078.943329, Test MSE of 65306347808.974670
Epoch 28: training loss 74468488086.588
Test Loss of 60870732833.872772, Test MSE of 60870731984.520195
Epoch 29: training loss 72031573790.118
Test Loss of 58462672772.352531, Test MSE of 58462672838.122215
Epoch 30: training loss 69155853583.059
Test Loss of 58135507788.213737, Test MSE of 58135506558.956703
Epoch 31: training loss 65042588973.176
Test Loss of 52815513603.316216, Test MSE of 52815513480.277397
Epoch 32: training loss 61513586936.471
Test Loss of 48378106589.357391, Test MSE of 48378105845.057587
Epoch 33: training loss 58845730906.353
Test Loss of 49316932742.306732, Test MSE of 49316932650.539307
Epoch 34: training loss 56483326991.059
Test Loss of 44061214981.981033, Test MSE of 44061214795.742867
Epoch 35: training loss 53028850288.941
Test Loss of 42522461184.236870, Test MSE of 42522460590.992409
Epoch 36: training loss 50211912015.059
Test Loss of 39666414340.204491, Test MSE of 39666415261.447533
Epoch 37: training loss 48047803572.706
Test Loss of 37213453397.984734, Test MSE of 37213453214.673088
Epoch 38: training loss 45233073347.765
Test Loss of 38223022933.214897, Test MSE of 38223023266.122963
Epoch 39: training loss 43117204638.118
Test Loss of 35929575584.125839, Test MSE of 35929576022.616165
Epoch 40: training loss 40857893202.824
Test Loss of 36328875505.669212, Test MSE of 36328875522.621170
Epoch 41: training loss 38779808986.353
Test Loss of 30505017789.320381, Test MSE of 30505018071.451286
Epoch 42: training loss 37258356943.059
Test Loss of 32038572183.598427, Test MSE of 32038571528.354572
Epoch 43: training loss 35382168101.647
Test Loss of 30465691668.844784, Test MSE of 30465691984.660900
Epoch 44: training loss 33726384259.765
Test Loss of 28194477544.194309, Test MSE of 28194477799.247166
Epoch 45: training loss 31902722288.941
Test Loss of 24609595468.746704, Test MSE of 24609595376.406914
Epoch 46: training loss 30406734336.000
Test Loss of 26436283244.902153, Test MSE of 26436283622.229565
Epoch 47: training loss 28911775134.118
Test Loss of 25165444912.973396, Test MSE of 25165445100.062641
Epoch 48: training loss 27546706480.941
Test Loss of 23687725553.432339, Test MSE of 23687725375.003796
Epoch 49: training loss 26485437477.647
Test Loss of 23622586821.374046, Test MSE of 23622587468.301838
Epoch 50: training loss 25087474989.176
Test Loss of 23258819006.741615, Test MSE of 23258818876.911835
Epoch 51: training loss 24443921434.353
Test Loss of 20857402795.081192, Test MSE of 20857402747.824650
Epoch 52: training loss 23190329054.118
Test Loss of 20142953457.550774, Test MSE of 20142953539.719715
Epoch 53: training loss 22268720564.706
Test Loss of 19603515270.721260, Test MSE of 19603514894.870125
Epoch 54: training loss 21330257264.941
Test Loss of 20521304587.014572, Test MSE of 20521304784.742020
Epoch 55: training loss 20374954127.059
Test Loss of 19602084693.214897, Test MSE of 19602084647.813282
Epoch 56: training loss 19566695755.294
Test Loss of 19197327560.394169, Test MSE of 19197328034.772980
Epoch 57: training loss 19041896030.118
Test Loss of 17965118801.069626, Test MSE of 17965119191.732754
Epoch 58: training loss 18189750317.176
Test Loss of 18298692974.915569, Test MSE of 18298693534.636532
Epoch 59: training loss 17842872481.882
Test Loss of 18284549940.289612, Test MSE of 18284549829.164555
Epoch 60: training loss 17124634191.059
Test Loss of 19299292048.906776, Test MSE of 19299291789.906994
Epoch 61: training loss 16542489080.471
Test Loss of 18187656103.646542, Test MSE of 18187656115.672264
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18187656115.672264, 'MSE - std': 0.0, 'R2 - mean': 0.858370748481582, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003568 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918146620.235
Test Loss of 424556222069.370361, Test MSE of 424556222941.274902
Epoch 2: training loss 427898039356.235
Test Loss of 424540683647.496643, Test MSE of 424540684193.363464
Epoch 3: training loss 427870859745.882
Test Loss of 424519161271.872314, Test MSE of 424519156314.735107
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889202838.588
Test Loss of 424527543368.719849, Test MSE of 424527542278.695801
Epoch 2: training loss 427878235437.176
Test Loss of 424527538070.354858, Test MSE of 424527545253.148926
Epoch 3: training loss 427877754759.529
Test Loss of 424526596007.409668, Test MSE of 424526595638.245789
Epoch 4: training loss 427877369735.529
Test Loss of 424525651048.934509, Test MSE of 424525645209.803162
Epoch 5: training loss 427877123975.529
Test Loss of 424525626376.290527, Test MSE of 424525626720.450806
Epoch 6: training loss 419519453545.412
Test Loss of 398435038517.118652, Test MSE of 398435041562.882629
Epoch 7: training loss 365459857046.588
Test Loss of 322408190199.294922, Test MSE of 322408189973.430298
Epoch 8: training loss 276922750494.118
Test Loss of 232209552937.571136, Test MSE of 232209551099.309601
Epoch 9: training loss 199257608432.941
Test Loss of 169101905320.238708, Test MSE of 169101907749.148499
Epoch 10: training loss 155163088534.588
Test Loss of 140544641043.423553, Test MSE of 140544639779.158325
Epoch 11: training loss 138355566170.353
Test Loss of 129669532561.854263, Test MSE of 129669534292.553284
Epoch 12: training loss 133314666706.824
Test Loss of 126128731352.264633, Test MSE of 126128731982.497604
Epoch 13: training loss 131056754928.941
Test Loss of 123716770566.573212, Test MSE of 123716771976.431015
Epoch 14: training loss 125851794793.412
Test Loss of 120131634880.222061, Test MSE of 120131635909.925247
Epoch 15: training loss 123044225295.059
Test Loss of 117509634409.467499, Test MSE of 117509631427.558731
Epoch 16: training loss 119120351864.471
Test Loss of 113384942932.859589, Test MSE of 113384945095.362595
Epoch 17: training loss 115067046400.000
Test Loss of 110377957230.560257, Test MSE of 110377955647.343506
Epoch 18: training loss 110548094915.765
Test Loss of 105308340485.507294, Test MSE of 105308339880.388092
Epoch 19: training loss 107002895299.765
Test Loss of 102671095012.582001, Test MSE of 102671095824.775467
Epoch 20: training loss 102813970974.118
Test Loss of 99051985395.801071, Test MSE of 99051984066.772995
Epoch 21: training loss 100289417336.471
Test Loss of 96378719155.016418, Test MSE of 96378716537.964386
Epoch 22: training loss 94916404495.059
Test Loss of 93027683049.201019, Test MSE of 93027683304.064743
Epoch 23: training loss 91285980732.235
Test Loss of 86226367614.253067, Test MSE of 86226368738.678024
Epoch 24: training loss 87182443610.353
Test Loss of 85726839183.840851, Test MSE of 85726838138.734741
Epoch 25: training loss 82997075546.353
Test Loss of 84822339168.051819, Test MSE of 84822340953.456467
Epoch 26: training loss 79532256286.118
Test Loss of 79614385352.631042, Test MSE of 79614384035.064728
Epoch 27: training loss 76038454287.059
Test Loss of 72573643605.214890, Test MSE of 72573644228.657150
Epoch 28: training loss 73162778036.706
Test Loss of 70430697829.677536, Test MSE of 70430697480.835190
Epoch 29: training loss 68957315824.941
Test Loss of 66972362436.959518, Test MSE of 66972359724.962730
Epoch 30: training loss 66085040655.059
Test Loss of 64548624470.695351, Test MSE of 64548624969.676003
Epoch 31: training loss 62707231247.059
Test Loss of 63488919770.396484, Test MSE of 63488919193.595406
Epoch 32: training loss 59731766558.118
Test Loss of 60494207616.977097, Test MSE of 60494209280.195137
Epoch 33: training loss 56243973888.000
Test Loss of 57431434181.492485, Test MSE of 57431433894.802010
Epoch 34: training loss 53439585438.118
Test Loss of 51878240542.615776, Test MSE of 51878238549.033241
Epoch 35: training loss 51041789605.647
Test Loss of 50914295465.482307, Test MSE of 50914295366.184265
Epoch 36: training loss 48083879589.647
Test Loss of 49165597705.711777, Test MSE of 49165596904.093147
Epoch 37: training loss 45616725172.706
Test Loss of 46569067834.329865, Test MSE of 46569068378.447083
Epoch 38: training loss 43586428965.647
Test Loss of 44322957611.406891, Test MSE of 44322957113.543411
Epoch 39: training loss 40393904670.118
Test Loss of 41295874165.488777, Test MSE of 41295874291.785217
Epoch 40: training loss 38692193415.529
Test Loss of 39392228033.406433, Test MSE of 39392228099.029320
Epoch 41: training loss 36536195267.765
Test Loss of 35747246754.612999, Test MSE of 35747246371.163719
Epoch 42: training loss 34865618612.706
Test Loss of 34274748378.100391, Test MSE of 34274748682.752384
Epoch 43: training loss 32449916032.000
Test Loss of 35078468715.777008, Test MSE of 35078467590.855721
Epoch 44: training loss 30735948920.471
Test Loss of 32834700432.729122, Test MSE of 32834699960.117939
Epoch 45: training loss 29165549168.941
Test Loss of 34348081522.942402, Test MSE of 34348081048.834476
Epoch 46: training loss 27987671273.412
Test Loss of 29662633000.978951, Test MSE of 29662632919.172405
Epoch 47: training loss 26303033464.471
Test Loss of 32027072755.031227, Test MSE of 32027072156.917908
Epoch 48: training loss 25078386258.824
Test Loss of 29176273205.355541, Test MSE of 29176273167.396961
Epoch 49: training loss 24018715655.529
Test Loss of 29695752604.631969, Test MSE of 29695751690.554462
Epoch 50: training loss 22330566704.941
Test Loss of 28598202829.427711, Test MSE of 28598203032.212902
Epoch 51: training loss 21376259471.059
Test Loss of 28220842444.953968, Test MSE of 28220843560.496819
Epoch 52: training loss 20681690823.529
Test Loss of 28256395810.228081, Test MSE of 28256395535.940960
Epoch 53: training loss 19838347241.412
Test Loss of 26164373842.727734, Test MSE of 26164373132.272511
Epoch 54: training loss 18873129852.235
Test Loss of 26981680655.752026, Test MSE of 26981681318.468483
Epoch 55: training loss 18175975868.235
Test Loss of 28084227723.873238, Test MSE of 28084228293.060654
Epoch 56: training loss 17570071488.000
Test Loss of 26839207995.218136, Test MSE of 26839207017.627251
Epoch 57: training loss 16572981432.471
Test Loss of 23980156559.900070, Test MSE of 23980156682.571831
Epoch 58: training loss 16277601716.706
Test Loss of 25905540554.822113, Test MSE of 25905540632.116982
Epoch 59: training loss 15590098386.824
Test Loss of 24465840622.116123, Test MSE of 24465840906.230358
Epoch 60: training loss 14840576158.118
Test Loss of 25573260136.638447, Test MSE of 25573260665.428101
Epoch 61: training loss 14219943894.588
Test Loss of 22082680031.844551, Test MSE of 22082679533.486549
Epoch 62: training loss 14146219399.529
Test Loss of 25882545969.210270, Test MSE of 25882545872.217976
Epoch 63: training loss 13582825965.176
Test Loss of 25080227744.540367, Test MSE of 25080226694.920513
Epoch 64: training loss 13278313276.235
Test Loss of 25097326395.869534, Test MSE of 25097326461.729652
Epoch 65: training loss 12875922729.412
Test Loss of 22755682417.935692, Test MSE of 22755682369.687996
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20471669242.68013, 'MSE - std': 2284013127.007866, 'R2 - mean': 0.8479551422525866, 'R2 - std': 0.010415606228995344} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005428 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927365210.353
Test Loss of 447259295383.953735, Test MSE of 447259306333.109192
Epoch 2: training loss 421907317338.353
Test Loss of 447241677448.557007, Test MSE of 447241681605.020508
Epoch 3: training loss 421880792967.529
Test Loss of 447217742588.624573, Test MSE of 447217744994.587708
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900674349.176
Test Loss of 447224960356.493164, Test MSE of 447224960923.324402
Epoch 2: training loss 421890189914.353
Test Loss of 447226844891.699280, Test MSE of 447226852363.437317
Epoch 3: training loss 421889650808.471
Test Loss of 447226660987.647461, Test MSE of 447226662485.739441
Epoch 4: training loss 421889268675.765
Test Loss of 447225854711.413391, Test MSE of 447225859035.625916
Epoch 5: training loss 421889024843.294
Test Loss of 447224845384.246155, Test MSE of 447224842729.982971
Epoch 6: training loss 413459910053.647
Test Loss of 420148711997.231567, Test MSE of 420148711120.858459
Epoch 7: training loss 359558157492.706
Test Loss of 342195623185.114014, Test MSE of 342195622534.246033
Epoch 8: training loss 272208072583.529
Test Loss of 250660577816.279449, Test MSE of 250660581210.333771
Epoch 9: training loss 195123490454.588
Test Loss of 184336026762.096680, Test MSE of 184336023308.741974
Epoch 10: training loss 151604562974.118
Test Loss of 153469780828.321075, Test MSE of 153469778807.135437
Epoch 11: training loss 136233597349.647
Test Loss of 140642947110.847107, Test MSE of 140642946823.893738
Epoch 12: training loss 131165209539.765
Test Loss of 137383006633.423080, Test MSE of 137383009495.206909
Epoch 13: training loss 128829326064.941
Test Loss of 133758688482.213272, Test MSE of 133758687956.121445
Epoch 14: training loss 125698644540.235
Test Loss of 130029240748.265549, Test MSE of 130029239695.080688
Epoch 15: training loss 120216718064.941
Test Loss of 126676172762.100388, Test MSE of 126676173310.155243
Epoch 16: training loss 116144160828.235
Test Loss of 124168773046.451080, Test MSE of 124168773638.410553
Epoch 17: training loss 113744308736.000
Test Loss of 118425835784.823502, Test MSE of 118425838109.347656
Epoch 18: training loss 109568815224.471
Test Loss of 115298007355.987976, Test MSE of 115298006852.278030
Epoch 19: training loss 105623183510.588
Test Loss of 112557083937.695114, Test MSE of 112557083046.666168
Epoch 20: training loss 101973254686.118
Test Loss of 108472669900.539444, Test MSE of 108472670837.424850
Epoch 21: training loss 98841926083.765
Test Loss of 106199505577.719177, Test MSE of 106199503796.247665
Epoch 22: training loss 94047725266.824
Test Loss of 100479574834.631500, Test MSE of 100479575097.794739
Epoch 23: training loss 90187754827.294
Test Loss of 95481032502.184601, Test MSE of 95481032466.666168
Epoch 24: training loss 87327667998.118
Test Loss of 90494433307.950958, Test MSE of 90494433357.135635
Epoch 25: training loss 83361241464.471
Test Loss of 90592449782.584320, Test MSE of 90592452285.154602
Epoch 26: training loss 78651310320.941
Test Loss of 84326603536.048111, Test MSE of 84326602133.031113
Epoch 27: training loss 76038578070.588
Test Loss of 84049895236.396942, Test MSE of 84049896135.578094
Epoch 28: training loss 72988519378.824
Test Loss of 78030451458.309509, Test MSE of 78030452144.851257
Epoch 29: training loss 69436401392.941
Test Loss of 73191475905.406433, Test MSE of 73191474838.009628
Epoch 30: training loss 66225844239.059
Test Loss of 71998930676.807770, Test MSE of 71998931807.324432
Epoch 31: training loss 62594820879.059
Test Loss of 69373257662.149429, Test MSE of 69373256309.167618
Epoch 32: training loss 59927163030.588
Test Loss of 64977494899.060837, Test MSE of 64977494588.958786
Epoch 33: training loss 57158222576.941
Test Loss of 63870002827.162621, Test MSE of 63870002580.106911
Epoch 34: training loss 54508404720.941
Test Loss of 57635167755.725189, Test MSE of 57635167745.464211
Epoch 35: training loss 51225278388.706
Test Loss of 56489944726.532501, Test MSE of 56489945600.261887
Epoch 36: training loss 48994427158.588
Test Loss of 52156173552.425629, Test MSE of 52156173746.942795
Epoch 37: training loss 45790985118.118
Test Loss of 51142292244.311821, Test MSE of 51142292116.336899
Epoch 38: training loss 43471152632.471
Test Loss of 46107812534.510292, Test MSE of 46107813007.859955
Epoch 39: training loss 41726106872.471
Test Loss of 47050973240.849411, Test MSE of 47050972918.563225
Epoch 40: training loss 39490524815.059
Test Loss of 43838148958.571365, Test MSE of 43838149342.493698
Epoch 41: training loss 38087911228.235
Test Loss of 44551783674.374275, Test MSE of 44551783430.379730
Epoch 42: training loss 35506174366.118
Test Loss of 41852188607.096924, Test MSE of 41852189253.358719
Epoch 43: training loss 33913223823.059
Test Loss of 40114176069.166779, Test MSE of 40114177447.394272
Epoch 44: training loss 31989157421.176
Test Loss of 33452043871.104324, Test MSE of 33452044062.485107
Epoch 45: training loss 31087311216.941
Test Loss of 36968296748.354385, Test MSE of 36968296875.504662
Epoch 46: training loss 28770240496.941
Test Loss of 36580082217.334259, Test MSE of 36580082106.419266
Epoch 47: training loss 27282208670.118
Test Loss of 33649768980.015728, Test MSE of 33649768602.366528
Epoch 48: training loss 26192085466.353
Test Loss of 31297959547.055286, Test MSE of 31297959708.820312
Epoch 49: training loss 24773439484.235
Test Loss of 31770875881.733982, Test MSE of 31770875933.046413
Epoch 50: training loss 23928221120.000
Test Loss of 29466043108.937313, Test MSE of 29466043195.770184
Epoch 51: training loss 22765283120.941
Test Loss of 31356050235.632664, Test MSE of 31356050871.950726
Epoch 52: training loss 21587349010.824
Test Loss of 30409623659.540134, Test MSE of 30409624289.760731
Epoch 53: training loss 20888554789.647
Test Loss of 26565925255.313438, Test MSE of 26565925528.838570
Epoch 54: training loss 20102081554.824
Test Loss of 28486977802.718483, Test MSE of 28486977568.881023
Epoch 55: training loss 19344134851.765
Test Loss of 26657272456.557022, Test MSE of 26657272709.061310
Epoch 56: training loss 18854667952.941
Test Loss of 27025267016.779087, Test MSE of 27025267371.556210
Epoch 57: training loss 18429405609.412
Test Loss of 29417262986.037476, Test MSE of 29417263009.650093
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23453533831.670116, 'MSE - std': 4610948221.03231, 'R2 - mean': 0.8333605020059398, 'R2 - std': 0.02232331255145043} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005591 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110892754.824
Test Loss of 410765580702.148987, Test MSE of 410765586963.475037
Epoch 2: training loss 430090034718.118
Test Loss of 410747743901.556702, Test MSE of 410747737278.067871
Epoch 3: training loss 430062887634.824
Test Loss of 410724303803.291077, Test MSE of 410724295701.703979
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076650676.706
Test Loss of 410728160722.746887, Test MSE of 410728163972.053894
Epoch 2: training loss 430067429135.059
Test Loss of 410729290847.244812, Test MSE of 410729288367.664185
Epoch 3: training loss 430067064832.000
Test Loss of 410728925926.056458, Test MSE of 410728918795.482300
Epoch 4: training loss 430066776907.294
Test Loss of 410728911869.630737, Test MSE of 410728915244.989746
Epoch 5: training loss 430066600056.471
Test Loss of 410728610209.939819, Test MSE of 410728613790.747681
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 120272303821.43951, 'MSE - std': 167742565556.92236, 'R2 - mean': 0.027531050181065164, 'R2 - std': 1.3958714353397974} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005578 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042959932.235
Test Loss of 431611372721.695496, Test MSE of 431611375228.016357
Epoch 2: training loss 424023010243.765
Test Loss of 431591331063.825989, Test MSE of 431591336241.711487
Epoch 3: training loss 423995402601.412
Test Loss of 431564027571.354004, Test MSE of 431564023721.964478
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011588065.882
Test Loss of 431567191434.720947, Test MSE of 431567195536.353210
Epoch 2: training loss 423998333289.412
Test Loss of 431568227228.016663, Test MSE of 431568228022.365295
Epoch 3: training loss 423997731297.882
Test Loss of 431568363773.986145, Test MSE of 431568360782.692017
Epoch 4: training loss 423997279292.235
Test Loss of 431568708694.715393, Test MSE of 431568704538.390198
Epoch 5: training loss 423997020521.412
Test Loss of 431568226303.052307, Test MSE of 431568229667.605713
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 182531488990.67276, 'MSE - std': 194974047481.81796, 'R2 - mean': -0.4225686722739447, 'R2 - std': 1.5391961186978016} 
 

Saving model.....
Results After CV: {'MSE - mean': 182531488990.67276, 'MSE - std': 194974047481.81796, 'R2 - mean': -0.4225686722739447, 'R2 - std': 1.5391961186978016}
Train time: 61.63167176920033
Inference time: 0.09941158720012026
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 19 finished with value: 182531488990.67276 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004060 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526557455.059
Test Loss of 418113199606.880432, Test MSE of 418113199695.032654
Epoch 2: training loss 427507613455.059
Test Loss of 418096427325.409180, Test MSE of 418096432184.026489
Epoch 3: training loss 427481531331.765
Test Loss of 418073617970.098572, Test MSE of 418073619592.400574
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427497133718.588
Test Loss of 418079598042.929443, Test MSE of 418079602628.437683
Epoch 2: training loss 427486312809.412
Test Loss of 418078553863.283813, Test MSE of 418078559105.897949
Epoch 3: training loss 423637631337.412
Test Loss of 405690335357.068726, Test MSE of 405690337244.805725
Epoch 4: training loss 396915905837.176
Test Loss of 364758623252.134155, Test MSE of 364758627583.383728
Epoch 5: training loss 330958482492.235
Test Loss of 278428944852.533875, Test MSE of 278428947314.458679
Epoch 6: training loss 247577939727.059
Test Loss of 201033931138.102234, Test MSE of 201033931774.295197
Epoch 7: training loss 179430106744.471
Test Loss of 142153917986.938690, Test MSE of 142153915706.532745
Epoch 8: training loss 146554433776.941
Test Loss of 122997289224.349762, Test MSE of 122997289500.568069
Epoch 9: training loss 138135700555.294
Test Loss of 117453517488.825348, Test MSE of 117453520220.203934
Epoch 10: training loss 135081515610.353
Test Loss of 114597317114.433502, Test MSE of 114597319568.415680
Epoch 11: training loss 132214808515.765
Test Loss of 111455142092.657883, Test MSE of 111455140351.579407
Epoch 12: training loss 128731108592.941
Test Loss of 109459939544.027756, Test MSE of 109459938503.803314
Epoch 13: training loss 124476327152.941
Test Loss of 105425580921.693268, Test MSE of 105425582396.523300
Epoch 14: training loss 120678030471.529
Test Loss of 102892866641.247284, Test MSE of 102892866176.417007
Epoch 15: training loss 118164741586.824
Test Loss of 99967062401.628494, Test MSE of 99967063943.665771
Epoch 16: training loss 114147295277.176
Test Loss of 96883713377.650711, Test MSE of 96883714926.321716
Epoch 17: training loss 110276470392.471
Test Loss of 92210722264.086975, Test MSE of 92210722494.349533
Epoch 18: training loss 106688911932.235
Test Loss of 90594915862.621323, Test MSE of 90594916288.129700
Epoch 19: training loss 102385618160.941
Test Loss of 86003986050.161469, Test MSE of 86003985669.612610
Epoch 20: training loss 98545107516.235
Test Loss of 85739275064.316452, Test MSE of 85739274432.964462
Epoch 21: training loss 95368782080.000
Test Loss of 82101370919.794586, Test MSE of 82101373017.080780
Epoch 22: training loss 92291312790.588
Test Loss of 78574097908.511688, Test MSE of 78574098753.540558
Epoch 23: training loss 87809196679.529
Test Loss of 74638116737.510056, Test MSE of 74638116220.621323
Epoch 24: training loss 85063085010.824
Test Loss of 73601009537.510056, Test MSE of 73601008918.793808
Epoch 25: training loss 81197139049.412
Test Loss of 69458753563.950958, Test MSE of 69458754669.737137
Epoch 26: training loss 77882005737.412
Test Loss of 65080766392.938240, Test MSE of 65080764511.739250
Epoch 27: training loss 75423742930.824
Test Loss of 66596304594.224380, Test MSE of 66596304677.405907
Epoch 28: training loss 71935572683.294
Test Loss of 63945573055.037704, Test MSE of 63945573194.240227
Epoch 29: training loss 69358138699.294
Test Loss of 58125668462.856346, Test MSE of 58125668579.501984
Epoch 30: training loss 66508992933.647
Test Loss of 56528666763.281052, Test MSE of 56528667022.963318
Epoch 31: training loss 64103103781.647
Test Loss of 52913826937.041870, Test MSE of 52913827203.940048
Epoch 32: training loss 61347085598.118
Test Loss of 52414527694.079109, Test MSE of 52414527129.254799
Epoch 33: training loss 58535098654.118
Test Loss of 51407779712.562569, Test MSE of 51407779899.811111
Epoch 34: training loss 55771525037.176
Test Loss of 48323659395.582695, Test MSE of 48323660266.885864
Epoch 35: training loss 53483467783.529
Test Loss of 47694395811.264397, Test MSE of 47694396134.082985
Epoch 36: training loss 51034997195.294
Test Loss of 39680923163.358780, Test MSE of 39680923173.699409
Epoch 37: training loss 48758824609.882
Test Loss of 43184045042.498268, Test MSE of 43184045382.260254
Epoch 38: training loss 47149033622.588
Test Loss of 40906475114.947952, Test MSE of 40906475579.781792
Epoch 39: training loss 44260774031.059
Test Loss of 39038845634.827667, Test MSE of 39038845472.431511
Epoch 40: training loss 43045152760.471
Test Loss of 37766413692.654175, Test MSE of 37766413291.008514
Epoch 41: training loss 41234504869.647
Test Loss of 35005375679.629890, Test MSE of 35005375062.072777
Epoch 42: training loss 39030728071.529
Test Loss of 35375625912.642143, Test MSE of 35375625720.652626
Epoch 43: training loss 37259668321.882
Test Loss of 32443745013.044643, Test MSE of 32443744744.459522
Epoch 44: training loss 35369190264.471
Test Loss of 32208014487.361553, Test MSE of 32208014741.223129
Epoch 45: training loss 34538955858.824
Test Loss of 30591975693.087208, Test MSE of 30591975512.970963
Epoch 46: training loss 32570797530.353
Test Loss of 28480563503.670601, Test MSE of 28480563004.085983
Epoch 47: training loss 31391005628.235
Test Loss of 26181384724.963219, Test MSE of 26181384817.467400
Epoch 48: training loss 29950980419.765
Test Loss of 26645292106.141106, Test MSE of 26645292163.382366
Epoch 49: training loss 29045701511.529
Test Loss of 28080054380.013878, Test MSE of 28080055066.956520
Epoch 50: training loss 27370703408.941
Test Loss of 24932125216.333103, Test MSE of 24932125399.979298
Epoch 51: training loss 26318390332.235
Test Loss of 25338197421.449921, Test MSE of 25338196989.732998
Epoch 52: training loss 25132657219.765
Test Loss of 24766452339.712238, Test MSE of 24766452451.663246
Epoch 53: training loss 24378807341.176
Test Loss of 25543921116.824429, Test MSE of 25543921715.909580
Epoch 54: training loss 23446839130.353
Test Loss of 24160791323.417995, Test MSE of 24160791694.963173
Epoch 55: training loss 22371415439.059
Test Loss of 23196409158.173492, Test MSE of 23196409162.713409
Epoch 56: training loss 21758136286.118
Test Loss of 22308649862.010639, Test MSE of 22308650377.123806
Epoch 57: training loss 21357603403.294
Test Loss of 20310655523.175571, Test MSE of 20310655806.992275
Epoch 58: training loss 20095544026.353
Test Loss of 21588831552.014805, Test MSE of 21588831547.410042
Epoch 59: training loss 19422555911.529
Test Loss of 19873438060.309971, Test MSE of 19873438323.613140
Epoch 60: training loss 18957991608.471
Test Loss of 20780956562.801758, Test MSE of 20780956432.664394
Epoch 61: training loss 18714853368.471
Test Loss of 19038469891.020126, Test MSE of 19038470143.461884
Epoch 62: training loss 17972487416.471
Test Loss of 19309005318.277122, Test MSE of 19309005541.344887
Epoch 63: training loss 16990833268.706
Test Loss of 19431600344.027760, Test MSE of 19431600826.324337
Epoch 64: training loss 16689063578.353
Test Loss of 19215787042.820263, Test MSE of 19215787447.713589
Epoch 65: training loss 16259052308.706
Test Loss of 18681059757.213047, Test MSE of 18681059835.222042
Epoch 66: training loss 15829727036.235
Test Loss of 19603135729.846867, Test MSE of 19603135762.057625
Epoch 67: training loss 15307117997.176
Test Loss of 20333784780.065697, Test MSE of 20333784523.413742
Epoch 68: training loss 14797793833.412
Test Loss of 18870417878.902615, Test MSE of 18870417815.823219
Epoch 69: training loss 14668371870.118
Test Loss of 19382328256.755032, Test MSE of 19382328107.278675
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19382328107.278675, 'MSE - std': 0.0, 'R2 - mean': 0.8490677080620178, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005569 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918307328.000
Test Loss of 424557556560.003723, Test MSE of 424557553552.168640
Epoch 2: training loss 427897726373.647
Test Loss of 424541426851.205200, Test MSE of 424541426495.835999
Epoch 3: training loss 427869472165.647
Test Loss of 424518946661.559082, Test MSE of 424518941156.959167
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888637349.647
Test Loss of 424527057200.144348, Test MSE of 424527053138.084656
Epoch 2: training loss 427878060513.882
Test Loss of 424527876263.705750, Test MSE of 424527873326.847961
Epoch 3: training loss 424051455397.647
Test Loss of 412560115752.268311, Test MSE of 412560111289.812012
Epoch 4: training loss 397324483644.235
Test Loss of 371270271904.540344, Test MSE of 371270266540.418213
Epoch 5: training loss 330860630618.353
Test Loss of 286589480720.284973, Test MSE of 286589478452.959656
Epoch 6: training loss 247611669744.941
Test Loss of 209988754082.139252, Test MSE of 209988749583.175812
Epoch 7: training loss 178753402639.059
Test Loss of 152081633930.925751, Test MSE of 152081636765.253571
Epoch 8: training loss 144684825178.353
Test Loss of 134051778168.686554, Test MSE of 134051777809.951370
Epoch 9: training loss 136072690748.235
Test Loss of 128764571343.855652, Test MSE of 128764572866.192001
Epoch 10: training loss 132498945144.471
Test Loss of 125653858165.192688, Test MSE of 125653856405.778900
Epoch 11: training loss 128445417321.412
Test Loss of 122347417250.376129, Test MSE of 122347418694.390167
Epoch 12: training loss 126163246802.824
Test Loss of 119867546545.121445, Test MSE of 119867548702.122375
Epoch 13: training loss 121856457637.647
Test Loss of 117745601145.397171, Test MSE of 117745600101.934326
Epoch 14: training loss 118663171102.118
Test Loss of 113944344376.553314, Test MSE of 113944343109.379272
Epoch 15: training loss 114501486471.529
Test Loss of 110125179029.703445, Test MSE of 110125178311.481659
Epoch 16: training loss 110107743141.647
Test Loss of 105996085518.508438, Test MSE of 105996082111.245895
Epoch 17: training loss 106479168481.882
Test Loss of 102896983611.099701, Test MSE of 102896981526.229385
Epoch 18: training loss 102696698096.941
Test Loss of 98651020294.395554, Test MSE of 98651021131.708542
Epoch 19: training loss 98466154315.294
Test Loss of 95493862946.938705, Test MSE of 95493863929.726532
Epoch 20: training loss 94800027316.706
Test Loss of 93503061238.110565, Test MSE of 93503061213.059616
Epoch 21: training loss 91225877624.471
Test Loss of 87231505716.171173, Test MSE of 87231503332.393127
Epoch 22: training loss 87700067102.118
Test Loss of 83799002330.633362, Test MSE of 83799003790.564560
Epoch 23: training loss 84092832752.941
Test Loss of 81257262774.273422, Test MSE of 81257262542.394348
Epoch 24: training loss 80396505660.235
Test Loss of 79749793756.469116, Test MSE of 79749794445.810715
Epoch 25: training loss 76997587230.118
Test Loss of 77684443115.628967, Test MSE of 77684441765.815796
Epoch 26: training loss 73486424139.294
Test Loss of 73029179996.024979, Test MSE of 73029177232.920715
Epoch 27: training loss 70934589138.824
Test Loss of 66983892163.419846, Test MSE of 66983893941.702438
Epoch 28: training loss 67182169690.353
Test Loss of 66904737739.414299, Test MSE of 66904736668.011360
Epoch 29: training loss 63678411806.118
Test Loss of 62184484293.610916, Test MSE of 62184483926.867584
Epoch 30: training loss 61806813244.235
Test Loss of 58530696671.193153, Test MSE of 58530694930.581482
Epoch 31: training loss 58875427207.529
Test Loss of 58753569204.319221, Test MSE of 58753568488.813515
Epoch 32: training loss 56614921366.588
Test Loss of 53903646588.772614, Test MSE of 53903645514.631180
Epoch 33: training loss 53144139023.059
Test Loss of 51545528433.225075, Test MSE of 51545526656.995689
Epoch 34: training loss 51115626149.647
Test Loss of 50550614060.532036, Test MSE of 50550614023.907600
Epoch 35: training loss 48234358829.176
Test Loss of 47650432192.103630, Test MSE of 47650432090.347809
Epoch 36: training loss 46102031021.176
Test Loss of 46315470567.069168, Test MSE of 46315470760.582855
Epoch 37: training loss 43983230772.706
Test Loss of 46539146682.241035, Test MSE of 46539145559.444473
Epoch 38: training loss 42342628668.235
Test Loss of 41483424840.246124, Test MSE of 41483424546.491142
Epoch 39: training loss 40158215725.176
Test Loss of 42102065646.589867, Test MSE of 42102065994.560043
Epoch 40: training loss 37692448737.882
Test Loss of 36194075831.576218, Test MSE of 36194075332.020935
Epoch 41: training loss 35888973251.765
Test Loss of 38547268579.812164, Test MSE of 38547268302.545258
Epoch 42: training loss 34458028393.412
Test Loss of 35546779156.726349, Test MSE of 35546779343.718170
Epoch 43: training loss 32090030147.765
Test Loss of 36326561513.911636, Test MSE of 36326561223.708519
Epoch 44: training loss 30783887322.353
Test Loss of 34134649781.858894, Test MSE of 34134649292.297009
Epoch 45: training loss 29591605760.000
Test Loss of 35572011287.035858, Test MSE of 35572011008.235474
Epoch 46: training loss 28336747151.059
Test Loss of 33353731975.668747, Test MSE of 33353732370.983837
Epoch 47: training loss 26499175604.706
Test Loss of 31506827149.827435, Test MSE of 31506826848.598682
Epoch 48: training loss 25719893504.000
Test Loss of 31100427348.089752, Test MSE of 31100427149.306160
Epoch 49: training loss 24559469869.176
Test Loss of 30389549707.873238, Test MSE of 30389549827.939598
Epoch 50: training loss 23354446144.000
Test Loss of 31294301357.627575, Test MSE of 31294301523.516529
Epoch 51: training loss 22523722398.118
Test Loss of 31066222106.411289, Test MSE of 31066222186.896420
Epoch 52: training loss 21646324909.176
Test Loss of 32078871887.648392, Test MSE of 32078871111.551559
Epoch 53: training loss 20308662313.412
Test Loss of 26932553894.758270, Test MSE of 26932553109.158195
Epoch 54: training loss 19753112297.412
Test Loss of 26273846128.928986, Test MSE of 26273846366.411163
Epoch 55: training loss 18925025912.471
Test Loss of 28058060765.653481, Test MSE of 28058060634.788731
Epoch 56: training loss 17856745739.294
Test Loss of 28883730443.843628, Test MSE of 28883729899.338585
Epoch 57: training loss 17304975905.882
Test Loss of 25591242904.545918, Test MSE of 25591243064.235146
Epoch 58: training loss 16938638098.824
Test Loss of 25929706734.767521, Test MSE of 25929705748.566895
Epoch 59: training loss 16755900171.294
Test Loss of 26536563640.938236, Test MSE of 26536563053.315731
Epoch 60: training loss 15584656670.118
Test Loss of 26185500786.172565, Test MSE of 26185501475.066940
Epoch 61: training loss 15121952094.118
Test Loss of 23329852658.557484, Test MSE of 23329852611.805973
Epoch 62: training loss 14640743826.824
Test Loss of 25825072731.314365, Test MSE of 25825071186.813057
Epoch 63: training loss 14093095412.706
Test Loss of 24663391403.258846, Test MSE of 24663390920.537914
Epoch 64: training loss 13768196171.294
Test Loss of 24899133506.798058, Test MSE of 24899132894.697613
Epoch 65: training loss 13737209483.294
Test Loss of 23050141474.287300, Test MSE of 23050141378.646999
Epoch 66: training loss 13151823856.941
Test Loss of 24176744664.738377, Test MSE of 24176745351.244347
Epoch 67: training loss 12888966810.353
Test Loss of 26858162800.869766, Test MSE of 26858163395.824192
Epoch 68: training loss 12504391439.059
Test Loss of 27053884990.652786, Test MSE of 27053884464.691933
Epoch 69: training loss 12094086113.882
Test Loss of 24470443277.324081, Test MSE of 24470443710.792614
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21926385909.035645, 'MSE - std': 2544057801.7569695, 'R2 - mean': 0.8371824932944143, 'R2 - std': 0.011885214767603458} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005469 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927833118.118
Test Loss of 447258864344.146179, Test MSE of 447258866505.383606
Epoch 2: training loss 421907547919.059
Test Loss of 447240595475.423523, Test MSE of 447240592646.208923
Epoch 3: training loss 421881418450.824
Test Loss of 447216635131.795532, Test MSE of 447216643774.959412
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900012483.765
Test Loss of 447223604566.280823, Test MSE of 447223612127.717651
Epoch 2: training loss 421888782215.529
Test Loss of 447224987841.288025, Test MSE of 447224983837.314026
Epoch 3: training loss 418399396562.824
Test Loss of 435925402945.436035, Test MSE of 435925409865.586426
Epoch 4: training loss 392736512000.000
Test Loss of 394820846568.549622, Test MSE of 394820858224.130066
Epoch 5: training loss 327404494366.118
Test Loss of 307547545715.593811, Test MSE of 307547546117.481445
Epoch 6: training loss 244341468943.059
Test Loss of 228814622952.135101, Test MSE of 228814626001.545776
Epoch 7: training loss 175468240474.353
Test Loss of 165965446485.807068, Test MSE of 165965444900.191711
Epoch 8: training loss 142152670147.765
Test Loss of 145212206712.686554, Test MSE of 145212208379.574371
Epoch 9: training loss 132465513441.882
Test Loss of 139217686468.308105, Test MSE of 139217685574.341492
Epoch 10: training loss 129539071939.765
Test Loss of 135512180124.395096, Test MSE of 135512178002.271515
Epoch 11: training loss 125769910272.000
Test Loss of 132452580837.825577, Test MSE of 132452579633.278015
Epoch 12: training loss 124412790000.941
Test Loss of 128683706241.510056, Test MSE of 128683705419.419769
Epoch 13: training loss 120134883840.000
Test Loss of 124467250698.303955, Test MSE of 124467251240.065308
Epoch 14: training loss 116558614226.824
Test Loss of 122469881677.398102, Test MSE of 122469883651.634506
Epoch 15: training loss 113255093458.824
Test Loss of 118701053352.712463, Test MSE of 118701052660.775894
Epoch 16: training loss 109060253696.000
Test Loss of 115703570062.478836, Test MSE of 115703569398.524719
Epoch 17: training loss 104946056312.471
Test Loss of 112308982405.240799, Test MSE of 112308982277.808075
Epoch 18: training loss 101871558595.765
Test Loss of 107864767901.816330, Test MSE of 107864768201.046432
Epoch 19: training loss 97030590042.353
Test Loss of 103351091203.789963, Test MSE of 103351090120.378098
Epoch 20: training loss 94362250149.647
Test Loss of 99481227300.952118, Test MSE of 99481228352.740021
Epoch 21: training loss 90648847796.706
Test Loss of 97881358011.958359, Test MSE of 97881361513.210037
Epoch 22: training loss 86742574396.235
Test Loss of 92353029005.590561, Test MSE of 92353028139.660828
Epoch 23: training loss 83088160873.412
Test Loss of 88937826836.963226, Test MSE of 88937826448.124741
Epoch 24: training loss 80140563516.235
Test Loss of 86714712994.435349, Test MSE of 86714712136.034882
Epoch 25: training loss 76754775009.882
Test Loss of 85463098098.202179, Test MSE of 85463099571.873657
Epoch 26: training loss 73548484276.706
Test Loss of 79420505916.817032, Test MSE of 79420508197.212753
Epoch 27: training loss 69917620118.588
Test Loss of 74942464902.010635, Test MSE of 74942464331.050537
Epoch 28: training loss 68185041212.235
Test Loss of 73531114120.083282, Test MSE of 73531115241.308075
Epoch 29: training loss 63359561366.588
Test Loss of 69990952242.749939, Test MSE of 69990952857.173004
Epoch 30: training loss 62628217901.176
Test Loss of 68515531588.160072, Test MSE of 68515531435.276497
Epoch 31: training loss 58892445613.176
Test Loss of 65103798486.132774, Test MSE of 65103798299.724701
Epoch 32: training loss 55528436495.059
Test Loss of 59882222732.465416, Test MSE of 59882222340.895660
Epoch 33: training loss 53792122639.059
Test Loss of 62843460737.569283, Test MSE of 62843461584.505013
Epoch 34: training loss 51891778537.412
Test Loss of 57085992196.322922, Test MSE of 57085993425.752121
Epoch 35: training loss 49178436495.059
Test Loss of 53405221661.786720, Test MSE of 53405221258.827568
Epoch 36: training loss 46768600726.588
Test Loss of 52760122133.259308, Test MSE of 52760123507.430336
Epoch 37: training loss 44926872508.235
Test Loss of 52359062906.285446, Test MSE of 52359062746.627312
Epoch 38: training loss 42733243354.353
Test Loss of 48736657227.976868, Test MSE of 48736657203.549881
Epoch 39: training loss 40347058778.353
Test Loss of 47037249864.305344, Test MSE of 47037249815.436859
Epoch 40: training loss 39346264553.412
Test Loss of 43749728310.006943, Test MSE of 43749729285.903587
Epoch 41: training loss 37009689728.000
Test Loss of 42176254887.172798, Test MSE of 42176255192.307205
Epoch 42: training loss 35352415111.529
Test Loss of 43896947408.092529, Test MSE of 43896947667.849640
Epoch 43: training loss 33767811162.353
Test Loss of 41302267328.399719, Test MSE of 41302266098.548172
Epoch 44: training loss 32041923410.824
Test Loss of 38321664775.046959, Test MSE of 38321664943.406586
Epoch 45: training loss 30690302215.529
Test Loss of 33275990497.325005, Test MSE of 33275990691.469421
Epoch 46: training loss 29324549669.647
Test Loss of 37861651914.822113, Test MSE of 37861652508.924583
Epoch 47: training loss 27950548276.706
Test Loss of 36456485079.080269, Test MSE of 36456484567.706131
Epoch 48: training loss 26619511201.882
Test Loss of 35630826288.736526, Test MSE of 35630825852.459579
Epoch 49: training loss 25792685925.647
Test Loss of 34997208600.516312, Test MSE of 34997209116.860626
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26283326978.31064, 'MSE - std': 6502360419.72892, 'R2 - mean': 0.8137969583698973, 'R2 - std': 0.03446648672928013} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005506 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110390392.471
Test Loss of 410764929026.843140, Test MSE of 410764936827.668884
Epoch 2: training loss 430090271563.294
Test Loss of 410747259296.044434, Test MSE of 410747260105.493164
Epoch 3: training loss 430063756709.647
Test Loss of 410724074973.171692, Test MSE of 410724075481.176941
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078315339.294
Test Loss of 410728044601.810303, Test MSE of 410728037781.491760
Epoch 2: training loss 430067208673.882
Test Loss of 410729116028.979187, Test MSE of 410729116235.200500
Epoch 3: training loss 426275617129.412
Test Loss of 398865033234.006470, Test MSE of 398865028264.583313
Epoch 4: training loss 399766546191.059
Test Loss of 356742095579.631653, Test MSE of 356742093837.646667
Epoch 5: training loss 333941536406.588
Test Loss of 271809211614.237854, Test MSE of 271809211409.419250
Epoch 6: training loss 250341914864.941
Test Loss of 194906537051.453949, Test MSE of 194906538866.118011
Epoch 7: training loss 182125787075.765
Test Loss of 135622167542.996765, Test MSE of 135622167100.759201
Epoch 8: training loss 149771919781.647
Test Loss of 116976696070.278580, Test MSE of 116976695927.845810
Epoch 9: training loss 139981574776.471
Test Loss of 111385996714.943085, Test MSE of 111385996577.011673
Epoch 10: training loss 135654279800.471
Test Loss of 108457517382.012024, Test MSE of 108457515404.163437
Epoch 11: training loss 133158768278.588
Test Loss of 105475095889.858398, Test MSE of 105475098913.666794
Epoch 12: training loss 130525454878.118
Test Loss of 102942787787.757523, Test MSE of 102942788837.073517
Epoch 13: training loss 125552177633.882
Test Loss of 100062099600.999542, Test MSE of 100062100603.854889
Epoch 14: training loss 122935071834.353
Test Loss of 96696646891.505783, Test MSE of 96696646950.748367
Epoch 15: training loss 118875070735.059
Test Loss of 94068637070.511795, Test MSE of 94068634974.549835
Epoch 16: training loss 115161291173.647
Test Loss of 90065747069.571487, Test MSE of 90065747312.222946
Epoch 17: training loss 110073349993.412
Test Loss of 87813646930.213791, Test MSE of 87813647404.331802
Epoch 18: training loss 107815009852.235
Test Loss of 84814599472.688568, Test MSE of 84814598656.431381
Epoch 19: training loss 103564839122.824
Test Loss of 82474287888.703384, Test MSE of 82474287925.628616
Epoch 20: training loss 100488324999.529
Test Loss of 78044440116.360947, Test MSE of 78044440057.292435
Epoch 21: training loss 97428728741.647
Test Loss of 73517497063.951874, Test MSE of 73517495857.079941
Epoch 22: training loss 92117234853.647
Test Loss of 72846340662.730225, Test MSE of 72846340780.121826
Epoch 23: training loss 89431987998.118
Test Loss of 69645951006.326706, Test MSE of 69645951446.535629
Epoch 24: training loss 85349287032.471
Test Loss of 65043235051.031929, Test MSE of 65043234564.939102
Epoch 25: training loss 81486395798.588
Test Loss of 63812152936.484962, Test MSE of 63812152076.453438
Epoch 26: training loss 78875955877.647
Test Loss of 63144200843.076355, Test MSE of 63144202005.357666
Epoch 27: training loss 74599536835.765
Test Loss of 59472998686.208237, Test MSE of 59472998786.120384
Epoch 28: training loss 71805551179.294
Test Loss of 56106757907.072655, Test MSE of 56106759604.396996
Epoch 29: training loss 69084328026.353
Test Loss of 52720952351.748268, Test MSE of 52720952826.587639
Epoch 30: training loss 66093476803.765
Test Loss of 53300692860.268394, Test MSE of 53300693295.402298
Epoch 31: training loss 63610535514.353
Test Loss of 49472285884.120316, Test MSE of 49472285660.824150
Epoch 32: training loss 60281362469.647
Test Loss of 49648462583.115227, Test MSE of 49648462371.022469
Epoch 33: training loss 58275386059.294
Test Loss of 46285712750.289680, Test MSE of 46285712163.946434
Epoch 34: training loss 55437552790.588
Test Loss of 44627977757.615921, Test MSE of 44627976815.643524
Epoch 35: training loss 53665370910.118
Test Loss of 43115898283.890793, Test MSE of 43115899043.258224
Epoch 36: training loss 50843360188.235
Test Loss of 38944785212.298012, Test MSE of 38944785367.019997
Epoch 37: training loss 48263515896.471
Test Loss of 38369440360.011108, Test MSE of 38369439732.641006
Epoch 38: training loss 46142009517.176
Test Loss of 35362680612.131424, Test MSE of 35362680474.112335
Epoch 39: training loss 43806085609.412
Test Loss of 34685148812.497917, Test MSE of 34685148189.908714
Epoch 40: training loss 41855329641.412
Test Loss of 34912092400.718185, Test MSE of 34912092204.318199
Epoch 41: training loss 39644415555.765
Test Loss of 34686235808.636742, Test MSE of 34686235395.330185
Epoch 42: training loss 38427786405.647
Test Loss of 31428879177.565941, Test MSE of 31428879394.071537
Epoch 43: training loss 36253289803.294
Test Loss of 31338389581.712170, Test MSE of 31338389750.763054
Epoch 44: training loss 34953687115.294
Test Loss of 28826373842.628414, Test MSE of 28826374184.253902
Epoch 45: training loss 33385073912.471
Test Loss of 28906889561.913929, Test MSE of 28906889448.574162
Epoch 46: training loss 31976369935.059
Test Loss of 28292821367.292919, Test MSE of 28292821890.850491
Epoch 47: training loss 30695207280.941
Test Loss of 26966335243.490978, Test MSE of 26966335273.500477
Epoch 48: training loss 29355902275.765
Test Loss of 25811962344.070339, Test MSE of 25811961820.290611
Epoch 49: training loss 28353140291.765
Test Loss of 26687274302.904209, Test MSE of 26687273885.075233
Epoch 50: training loss 27046379591.529
Test Loss of 24122119026.317444, Test MSE of 24122118966.466068
Epoch 51: training loss 25671166384.941
Test Loss of 22763956167.137436, Test MSE of 22763956509.766857
Epoch 52: training loss 24797700928.000
Test Loss of 21931164808.944008, Test MSE of 21931164875.698410
Epoch 53: training loss 23664641227.294
Test Loss of 22168119198.385933, Test MSE of 22168118799.282837
Epoch 54: training loss 22936794469.647
Test Loss of 23607918067.916706, Test MSE of 23607917863.750282
Epoch 55: training loss 21740338492.235
Test Loss of 20926264546.028690, Test MSE of 20926264126.705421
Epoch 56: training loss 21217694369.882
Test Loss of 22547834092.453495, Test MSE of 22547833937.428329
Epoch 57: training loss 20371378635.294
Test Loss of 20363314223.859325, Test MSE of 20363314613.448627
Epoch 58: training loss 20012332939.294
Test Loss of 21767054231.278111, Test MSE of 21767054341.803795
Epoch 59: training loss 19165581891.765
Test Loss of 18940577383.537251, Test MSE of 18940577229.656128
Epoch 60: training loss 18362460163.765
Test Loss of 18319748889.706615, Test MSE of 18319748872.231216
Epoch 61: training loss 18212587143.529
Test Loss of 19741097933.771400, Test MSE of 19741097644.935463
Epoch 62: training loss 17532447943.529
Test Loss of 19905106412.808884, Test MSE of 19905106425.485016
Epoch 63: training loss 17170745987.765
Test Loss of 19770485036.423878, Test MSE of 19770485331.444378
Epoch 64: training loss 16359786496.000
Test Loss of 19593291532.438686, Test MSE of 19593291669.797951
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24610818151.182465, 'MSE - std': 6332643650.286024, 'R2 - mean': 0.8199193069255171, 'R2 - std': 0.031676544731145} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005336 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043131241.412
Test Loss of 431612135968.932922, Test MSE of 431612131809.209412
Epoch 2: training loss 424022951333.647
Test Loss of 431591416024.077759, Test MSE of 431591416122.079956
Epoch 3: training loss 423995264662.588
Test Loss of 431563255681.006958, Test MSE of 431563257140.527710
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424007201731.765
Test Loss of 431564908319.392883, Test MSE of 431564918661.831238
Epoch 2: training loss 423997661545.412
Test Loss of 431567158226.983826, Test MSE of 431567156848.219910
Epoch 3: training loss 420199618680.471
Test Loss of 419417986326.152710, Test MSE of 419417992257.909912
Epoch 4: training loss 393781064402.824
Test Loss of 377250106275.598328, Test MSE of 377250106366.566956
Epoch 5: training loss 328513654663.529
Test Loss of 289894394131.783447, Test MSE of 289894396649.200928
Epoch 6: training loss 245829879024.941
Test Loss of 211198073481.180939, Test MSE of 211198072386.229004
Epoch 7: training loss 178105910031.059
Test Loss of 148330607291.409546, Test MSE of 148330606057.650970
Epoch 8: training loss 145015669323.294
Test Loss of 129447975997.127258, Test MSE of 129447976354.614868
Epoch 9: training loss 135936541108.706
Test Loss of 122993968282.950485, Test MSE of 122993968598.597290
Epoch 10: training loss 133593822870.588
Test Loss of 118929965965.801025, Test MSE of 118929965113.205704
Epoch 11: training loss 130850222351.059
Test Loss of 115941330782.889404, Test MSE of 115941328949.700165
Epoch 12: training loss 126883150516.706
Test Loss of 112514558565.167984, Test MSE of 112514559595.638367
Epoch 13: training loss 123304186428.235
Test Loss of 109164484245.027298, Test MSE of 109164484735.441376
Epoch 14: training loss 119934235979.294
Test Loss of 106722745480.944000, Test MSE of 106722748882.903030
Epoch 15: training loss 115656096496.941
Test Loss of 102913340167.226288, Test MSE of 102913341429.657684
Epoch 16: training loss 111643214396.235
Test Loss of 100402030917.064316, Test MSE of 100402029807.035751
Epoch 17: training loss 107666039747.765
Test Loss of 94281712275.605743, Test MSE of 94281710535.037048
Epoch 18: training loss 104662079804.235
Test Loss of 91435235764.894028, Test MSE of 91435236186.962860
Epoch 19: training loss 100073123343.059
Test Loss of 88783223994.224899, Test MSE of 88783222257.024078
Epoch 20: training loss 97795532257.882
Test Loss of 85642122654.622864, Test MSE of 85642123240.629318
Epoch 21: training loss 93904428513.882
Test Loss of 82587490085.079132, Test MSE of 82587490910.182541
Epoch 22: training loss 89228916841.412
Test Loss of 79206124002.384079, Test MSE of 79206125724.516357
Epoch 23: training loss 86072806083.765
Test Loss of 76597385977.010651, Test MSE of 76597386001.318207
Epoch 24: training loss 82477351777.882
Test Loss of 73028730116.146225, Test MSE of 73028730226.637451
Epoch 25: training loss 79009359796.706
Test Loss of 67876127768.166588, Test MSE of 67876128415.073624
Epoch 26: training loss 75725588630.588
Test Loss of 65602309590.537712, Test MSE of 65602311688.448402
Epoch 27: training loss 73043146194.824
Test Loss of 67008279682.310043, Test MSE of 67008277880.244942
Epoch 28: training loss 69828570774.588
Test Loss of 60715110430.326698, Test MSE of 60715111286.751427
Epoch 29: training loss 66546737046.588
Test Loss of 60007544188.505325, Test MSE of 60007544971.576500
Epoch 30: training loss 64369723888.941
Test Loss of 56695291052.956963, Test MSE of 56695291362.996933
Epoch 31: training loss 60968820510.118
Test Loss of 56445312123.676079, Test MSE of 56445312609.384689
Epoch 32: training loss 59010370078.118
Test Loss of 52326373291.653862, Test MSE of 52326373881.439507
Epoch 33: training loss 56023226488.471
Test Loss of 50146273316.960663, Test MSE of 50146272379.642784
Epoch 34: training loss 53053434639.059
Test Loss of 45177640108.009254, Test MSE of 45177638851.370476
Epoch 35: training loss 50992134279.529
Test Loss of 47869411587.672371, Test MSE of 47869411774.664772
Epoch 36: training loss 48878070144.000
Test Loss of 43561221838.837578, Test MSE of 43561221261.315544
Epoch 37: training loss 46359559770.353
Test Loss of 40762112527.874130, Test MSE of 40762113589.096527
Epoch 38: training loss 44090750848.000
Test Loss of 38288877675.091164, Test MSE of 38288877576.689438
Epoch 39: training loss 42624567348.706
Test Loss of 37208629116.268394, Test MSE of 37208629550.101143
Epoch 40: training loss 40941895333.647
Test Loss of 35633040137.121704, Test MSE of 35633039629.805756
Epoch 41: training loss 38293059358.118
Test Loss of 33349273110.034245, Test MSE of 33349272833.280869
Epoch 42: training loss 37393387904.000
Test Loss of 33028866073.588154, Test MSE of 33028865794.302235
Epoch 43: training loss 35751230283.294
Test Loss of 31269321302.952335, Test MSE of 31269321271.319511
Epoch 44: training loss 33627184752.941
Test Loss of 30510195491.657566, Test MSE of 30510195465.175125
Epoch 45: training loss 32680731783.529
Test Loss of 31083624436.627487, Test MSE of 31083624297.969032
Epoch 46: training loss 31300031469.176
Test Loss of 28591299201.599258, Test MSE of 28591299062.254612
Epoch 47: training loss 29567438844.235
Test Loss of 25721321484.320221, Test MSE of 25721321734.027641
Epoch 48: training loss 28526908016.941
Test Loss of 24865054248.040722, Test MSE of 24865054282.671867
Epoch 49: training loss 27472572848.941
Test Loss of 23475644679.463211, Test MSE of 23475644620.474110
Epoch 50: training loss 26102277131.294
Test Loss of 23392636568.818138, Test MSE of 23392636872.101227
Epoch 51: training loss 25100731132.235
Test Loss of 23356557084.549744, Test MSE of 23356556803.088432
Epoch 52: training loss 24285748483.765
Test Loss of 27642791564.497917, Test MSE of 27642791830.064388
Epoch 53: training loss 23672850744.471
Test Loss of 22841791214.585838, Test MSE of 22841791297.246994
Epoch 54: training loss 22522553596.235
Test Loss of 22118043406.334106, Test MSE of 22118043274.910995
Epoch 55: training loss 22046906232.471
Test Loss of 21751859493.789913, Test MSE of 21751860327.646648
Epoch 56: training loss 20910242797.176
Test Loss of 22416005029.967609, Test MSE of 22416004849.754295
Epoch 57: training loss 19893475384.471
Test Loss of 20630464367.000462, Test MSE of 20630464474.702084
Epoch 58: training loss 19509983834.353
Test Loss of 21913618590.267467, Test MSE of 21913618820.436214
Epoch 59: training loss 18898453857.882
Test Loss of 21176263009.495605, Test MSE of 21176262966.901638
Epoch 60: training loss 18303126192.941
Test Loss of 22344972071.922256, Test MSE of 22344971403.739319
Epoch 61: training loss 17812387166.118
Test Loss of 22547517669.345673, Test MSE of 22547518052.510746
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24198158131.44812, 'MSE - std': 5723901960.092863, 'R2 - mean': 0.8222583051517706, 'R2 - std': 0.028715961409868405} 
 

Saving model.....
Results After CV: {'MSE - mean': 24198158131.44812, 'MSE - std': 5723901960.092863, 'R2 - mean': 0.8222583051517706, 'R2 - std': 0.028715961409868405}
Train time: 95.29573704680024
Inference time: 0.07214135120011633
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 20 finished with value: 24198158131.44812 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525383228.235
Test Loss of 418112502663.431885, Test MSE of 418112503502.169312
Epoch 2: training loss 427503406019.765
Test Loss of 418093091993.256531, Test MSE of 418093096539.092590
Epoch 3: training loss 427474601020.235
Test Loss of 418068337357.250061, Test MSE of 418068338750.068176
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492278994.824
Test Loss of 418074834850.672241, Test MSE of 418074836844.872620
Epoch 2: training loss 427481110648.471
Test Loss of 418075569034.274353, Test MSE of 418075571480.414185
Epoch 3: training loss 423887393249.882
Test Loss of 406528283936.273865, Test MSE of 406528289210.502380
Epoch 4: training loss 397992922413.176
Test Loss of 365975765128.438599, Test MSE of 365975758644.483887
Epoch 5: training loss 333722671465.412
Test Loss of 282536840652.243347, Test MSE of 282536842797.642639
Epoch 6: training loss 252695329069.176
Test Loss of 205855254240.436737, Test MSE of 205855252231.878113
Epoch 7: training loss 184991990362.353
Test Loss of 145979566383.670593, Test MSE of 145979567759.239655
Epoch 8: training loss 148383548702.118
Test Loss of 124419960785.572983, Test MSE of 124419960551.952118
Epoch 9: training loss 138487249076.706
Test Loss of 118215715328.118439, Test MSE of 118215714311.174423
Epoch 10: training loss 135749178940.235
Test Loss of 115391124461.287064, Test MSE of 115391125310.318497
Epoch 11: training loss 132189870501.647
Test Loss of 112652004573.475830, Test MSE of 112652005079.893280
Epoch 12: training loss 129675352274.824
Test Loss of 110657386482.261398, Test MSE of 110657387840.295670
Epoch 13: training loss 127549139847.529
Test Loss of 108372797946.433502, Test MSE of 108372797543.026276
Epoch 14: training loss 122973029737.412
Test Loss of 105503431256.235016, Test MSE of 105503431311.794128
Epoch 15: training loss 121038828077.176
Test Loss of 102659638015.467041, Test MSE of 102659637999.452087
Epoch 16: training loss 117357307392.000
Test Loss of 100681130694.617630, Test MSE of 100681133644.729004
Epoch 17: training loss 115128681441.882
Test Loss of 97889135185.602585, Test MSE of 97889135947.101929
Epoch 18: training loss 111793378123.294
Test Loss of 94740494017.169556, Test MSE of 94740495243.805710
Epoch 19: training loss 108567600368.941
Test Loss of 91028287348.718948, Test MSE of 91028288111.149490
Epoch 20: training loss 105894752421.647
Test Loss of 91519312372.274811, Test MSE of 91519314544.728226
Epoch 21: training loss 103090743416.471
Test Loss of 86294581608.993759, Test MSE of 86294581661.463425
Epoch 22: training loss 99657831168.000
Test Loss of 86261872089.981964, Test MSE of 86261871874.778839
Epoch 23: training loss 96181585995.294
Test Loss of 82065254184.445984, Test MSE of 82065256628.524811
Epoch 24: training loss 93355960304.941
Test Loss of 79269244129.265793, Test MSE of 79269244912.659897
Epoch 25: training loss 89887725763.765
Test Loss of 77258036608.917877, Test MSE of 77258036634.985275
Epoch 26: training loss 86628679499.294
Test Loss of 74971935691.177429, Test MSE of 74971934575.979919
Epoch 27: training loss 84008844047.059
Test Loss of 71525489420.258148, Test MSE of 71525489205.798294
Epoch 28: training loss 80084113874.824
Test Loss of 68417779967.348602, Test MSE of 68417779481.267738
Epoch 29: training loss 78152088402.824
Test Loss of 67433464924.143417, Test MSE of 67433465979.404984
Epoch 30: training loss 73842486264.471
Test Loss of 63705999822.848946, Test MSE of 63705999531.598930
Epoch 31: training loss 72250432752.941
Test Loss of 61941323457.169556, Test MSE of 61941323714.748787
Epoch 32: training loss 68514461884.235
Test Loss of 62137196735.393013, Test MSE of 62137196648.994881
Epoch 33: training loss 65910269101.176
Test Loss of 56343042526.008789, Test MSE of 56343044048.752136
Epoch 34: training loss 63503892668.235
Test Loss of 53941269323.266251, Test MSE of 53941270208.298111
Epoch 35: training loss 61125060096.000
Test Loss of 53870471226.270645, Test MSE of 53870470728.221184
Epoch 36: training loss 57536150625.882
Test Loss of 48189959253.984734, Test MSE of 48189958328.416245
Epoch 37: training loss 55580558817.882
Test Loss of 48679146141.875549, Test MSE of 48679146238.841293
Epoch 38: training loss 53506282413.176
Test Loss of 47593127876.308121, Test MSE of 47593127003.883774
Epoch 39: training loss 50556585140.706
Test Loss of 42108013006.848946, Test MSE of 42108013635.051018
Epoch 40: training loss 48889280978.824
Test Loss of 41117174371.604904, Test MSE of 41117173747.331566
Epoch 41: training loss 46605962849.882
Test Loss of 41510301014.754570, Test MSE of 41510300678.471634
Epoch 42: training loss 44494255984.941
Test Loss of 43136516786.720337, Test MSE of 43136517079.497452
Epoch 43: training loss 42342919616.000
Test Loss of 37855790453.547997, Test MSE of 37855790231.676552
Epoch 44: training loss 40553431928.471
Test Loss of 36340815310.612076, Test MSE of 36340814843.820404
Epoch 45: training loss 38450899681.882
Test Loss of 33017626575.678001, Test MSE of 33017625986.836891
Epoch 46: training loss 36473322469.647
Test Loss of 33277661358.338192, Test MSE of 33277661006.140079
Epoch 47: training loss 35198065859.765
Test Loss of 32086411085.398102, Test MSE of 32086411012.976753
Epoch 48: training loss 33758830426.353
Test Loss of 29940296790.932224, Test MSE of 29940296777.290524
Epoch 49: training loss 32657920308.706
Test Loss of 28269118344.853111, Test MSE of 28269117960.540924
Epoch 50: training loss 30560726136.471
Test Loss of 28300673565.964378, Test MSE of 28300674141.608612
Epoch 51: training loss 29544331862.588
Test Loss of 26949259940.507980, Test MSE of 26949260369.830982
Epoch 52: training loss 28531938778.353
Test Loss of 24605855115.340275, Test MSE of 24605855190.073833
Epoch 53: training loss 27000542584.471
Test Loss of 23937452266.503819, Test MSE of 23937452568.501823
Epoch 54: training loss 26128262746.353
Test Loss of 25196083939.042332, Test MSE of 25196084020.112522
Epoch 55: training loss 24967830185.412
Test Loss of 22744295734.303032, Test MSE of 22744295500.498943
Epoch 56: training loss 24073232960.000
Test Loss of 26185873807.367107, Test MSE of 26185873838.121880
Epoch 57: training loss 22787691004.235
Test Loss of 23559931340.717094, Test MSE of 23559931374.331512
Epoch 58: training loss 22058353374.118
Test Loss of 23133344592.951191, Test MSE of 23133344289.422791
Epoch 59: training loss 21167494369.882
Test Loss of 22407104010.777702, Test MSE of 22407104308.749992
Epoch 60: training loss 20531705076.706
Test Loss of 20451933807.685402, Test MSE of 20451933458.993385
Epoch 61: training loss 19862450736.941
Test Loss of 22000852052.089752, Test MSE of 22000851824.361431
Epoch 62: training loss 19065751977.412
Test Loss of 24573097713.965302, Test MSE of 24573097516.976849
Epoch 63: training loss 18535156562.824
Test Loss of 20599342123.347675, Test MSE of 20599341752.632362
Epoch 64: training loss 18201069541.647
Test Loss of 21555026401.325005, Test MSE of 21555026596.681068
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21555026596.68107, 'MSE - std': 0.0, 'R2 - mean': 0.8321486691890484, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005452 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917630524.235
Test Loss of 424556780967.291260, Test MSE of 424556779276.168762
Epoch 2: training loss 427896543352.471
Test Loss of 424540197228.783691, Test MSE of 424540201769.484741
Epoch 3: training loss 427867842921.412
Test Loss of 424517078703.640991, Test MSE of 424517082912.998840
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887078942.118
Test Loss of 424525874474.696289, Test MSE of 424525866636.669006
Epoch 2: training loss 427876999890.824
Test Loss of 424527457821.727478, Test MSE of 424527454934.386658
Epoch 3: training loss 424043824549.647
Test Loss of 412643027873.014099, Test MSE of 412643028463.691223
Epoch 4: training loss 397253528274.824
Test Loss of 371835995544.605164, Test MSE of 371835999893.489258
Epoch 5: training loss 331992471552.000
Test Loss of 288617055151.700195, Test MSE of 288617060261.897522
Epoch 6: training loss 250454098281.412
Test Loss of 213706945128.816101, Test MSE of 213706944394.085236
Epoch 7: training loss 183056719088.941
Test Loss of 156634868368.136932, Test MSE of 156634867860.642883
Epoch 8: training loss 146583868144.941
Test Loss of 135869548675.227386, Test MSE of 135869547973.095947
Epoch 9: training loss 137127243474.824
Test Loss of 129867469583.574371, Test MSE of 129867468906.384384
Epoch 10: training loss 132283535661.176
Test Loss of 127123929115.950958, Test MSE of 127123924855.025650
Epoch 11: training loss 131034491000.471
Test Loss of 124659831139.308807, Test MSE of 124659831980.673950
Epoch 12: training loss 128303673856.000
Test Loss of 121751150766.338196, Test MSE of 121751154245.181168
Epoch 13: training loss 124983553415.529
Test Loss of 119322116171.799210, Test MSE of 119322114098.686234
Epoch 14: training loss 121838184839.529
Test Loss of 115983226318.612076, Test MSE of 115983227165.394989
Epoch 15: training loss 118686825863.529
Test Loss of 112367008433.299103, Test MSE of 112367007072.557373
Epoch 16: training loss 116041941504.000
Test Loss of 110929987569.313904, Test MSE of 110929987726.816269
Epoch 17: training loss 112743133003.294
Test Loss of 107257800204.198929, Test MSE of 107257799025.222122
Epoch 18: training loss 109427399408.941
Test Loss of 103734255940.041641, Test MSE of 103734255181.373993
Epoch 19: training loss 106193835429.647
Test Loss of 101089411550.482529, Test MSE of 101089410558.770462
Epoch 20: training loss 104836682962.824
Test Loss of 98411325726.615784, Test MSE of 98411323489.177048
Epoch 21: training loss 100728682044.235
Test Loss of 96053613833.534119, Test MSE of 96053616801.179657
Epoch 22: training loss 97185739384.471
Test Loss of 93209812750.389999, Test MSE of 93209812256.035370
Epoch 23: training loss 93523563369.412
Test Loss of 89036156569.848709, Test MSE of 89036155820.897629
Epoch 24: training loss 92580014923.294
Test Loss of 86484429694.904465, Test MSE of 86484428835.593796
Epoch 25: training loss 87482938985.412
Test Loss of 81285891126.243805, Test MSE of 81285890246.154480
Epoch 26: training loss 84644916781.176
Test Loss of 78956535727.463333, Test MSE of 78956534985.488678
Epoch 27: training loss 81849093225.412
Test Loss of 77884430919.180206, Test MSE of 77884433626.174042
Epoch 28: training loss 78160282262.588
Test Loss of 72354755091.541992, Test MSE of 72354754164.755219
Epoch 29: training loss 74613552353.882
Test Loss of 71678100939.769608, Test MSE of 71678099637.582809
Epoch 30: training loss 71410954240.000
Test Loss of 66902511215.211662, Test MSE of 66902511089.777412
Epoch 31: training loss 69028063578.353
Test Loss of 67528753993.371269, Test MSE of 67528754392.713882
Epoch 32: training loss 66177357598.118
Test Loss of 64842270300.735603, Test MSE of 64842271083.208694
Epoch 33: training loss 62943632474.353
Test Loss of 62916988657.965302, Test MSE of 62916989388.759155
Epoch 34: training loss 61212094358.588
Test Loss of 61984148011.702988, Test MSE of 61984147687.862076
Epoch 35: training loss 58096324954.353
Test Loss of 57175918057.141800, Test MSE of 57175918169.351997
Epoch 36: training loss 56200150467.765
Test Loss of 54768483567.004395, Test MSE of 54768484363.169136
Epoch 37: training loss 53409828984.471
Test Loss of 52616733086.053207, Test MSE of 52616733262.923080
Epoch 38: training loss 50430153818.353
Test Loss of 51208092115.586395, Test MSE of 51208093181.362953
Epoch 39: training loss 47816922172.235
Test Loss of 49397501320.497803, Test MSE of 49397501474.212105
Epoch 40: training loss 46198643599.059
Test Loss of 43772571029.762665, Test MSE of 43772569744.705650
Epoch 41: training loss 43939956126.118
Test Loss of 42511279875.493874, Test MSE of 42511279637.208221
Epoch 42: training loss 42139333647.059
Test Loss of 43995413908.341431, Test MSE of 43995414667.393669
Epoch 43: training loss 39810988393.412
Test Loss of 41169069870.130928, Test MSE of 41169068948.356293
Epoch 44: training loss 38240795489.882
Test Loss of 39394426450.786957, Test MSE of 39394426214.602852
Epoch 45: training loss 35814196442.353
Test Loss of 41063833029.610916, Test MSE of 41063832349.786507
Epoch 46: training loss 34259743969.882
Test Loss of 37304901057.820961, Test MSE of 37304900655.225708
Epoch 47: training loss 32575423503.059
Test Loss of 35377045008.936386, Test MSE of 35377044609.196632
Epoch 48: training loss 30793595527.529
Test Loss of 37590562286.352997, Test MSE of 37590561343.954086
Epoch 49: training loss 29356627147.294
Test Loss of 32617443470.597271, Test MSE of 32617443979.771404
Epoch 50: training loss 28043178270.118
Test Loss of 32822471023.152439, Test MSE of 32822470946.011871
Epoch 51: training loss 26401309319.529
Test Loss of 33096643782.736061, Test MSE of 33096643245.320602
Epoch 52: training loss 25584882281.412
Test Loss of 32461947416.279434, Test MSE of 32461946650.788166
Epoch 53: training loss 24217945396.706
Test Loss of 31848223401.719177, Test MSE of 31848223205.788189
Epoch 54: training loss 23103514221.176
Test Loss of 32007269541.100163, Test MSE of 32007271015.237003
Epoch 55: training loss 22144794319.059
Test Loss of 27304329486.508442, Test MSE of 27304328709.870121
Epoch 56: training loss 21323295570.824
Test Loss of 30838557276.735600, Test MSE of 30838558016.095848
Epoch 57: training loss 20385283937.882
Test Loss of 30136493857.813557, Test MSE of 30136494013.584652
Epoch 58: training loss 19463292525.176
Test Loss of 28150986079.755726, Test MSE of 28150985437.598976
Epoch 59: training loss 18673309940.706
Test Loss of 26308896915.097847, Test MSE of 26308897481.228451
Epoch 60: training loss 17954748201.412
Test Loss of 27051208667.521629, Test MSE of 27051209043.295612
Epoch 61: training loss 17462968225.882
Test Loss of 29162149978.722183, Test MSE of 29162150495.166370
Epoch 62: training loss 16728586255.059
Test Loss of 26659207413.636826, Test MSE of 26659208457.293121
Epoch 63: training loss 16158393754.353
Test Loss of 29343166948.641220, Test MSE of 29343166745.079613
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25449096670.88034, 'MSE - std': 3894070074.199272, 'R2 - mean': 0.8113289701950623, 'R2 - std': 0.020819698993986113} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005467 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927435565.176
Test Loss of 447258902895.626160, Test MSE of 447258909399.876831
Epoch 2: training loss 421907325409.882
Test Loss of 447240641531.025696, Test MSE of 447240643199.737244
Epoch 3: training loss 421880618405.647
Test Loss of 447216400002.635193, Test MSE of 447216403389.101685
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897784621.176
Test Loss of 447225652069.795959, Test MSE of 447225656494.455566
Epoch 2: training loss 421888911360.000
Test Loss of 447225418289.861694, Test MSE of 447225418423.989685
Epoch 3: training loss 417990306755.765
Test Loss of 435023475547.373596, Test MSE of 435023476552.036438
Epoch 4: training loss 391200839077.647
Test Loss of 392483654574.278992, Test MSE of 392483652382.588562
Epoch 5: training loss 326958651632.941
Test Loss of 308123786072.057373, Test MSE of 308123788991.933716
Epoch 6: training loss 245340006339.765
Test Loss of 229988697670.943329, Test MSE of 229988700700.300018
Epoch 7: training loss 178262515049.412
Test Loss of 169551307163.210724, Test MSE of 169551306353.560028
Epoch 8: training loss 144088590878.118
Test Loss of 147695973791.237579, Test MSE of 147695974566.073944
Epoch 9: training loss 134379580325.647
Test Loss of 140768009536.962280, Test MSE of 140768010177.865906
Epoch 10: training loss 132511978134.588
Test Loss of 137214819173.322235, Test MSE of 137214818025.802658
Epoch 11: training loss 127965165718.588
Test Loss of 134696555091.971313, Test MSE of 134696557755.308380
Epoch 12: training loss 126107555237.647
Test Loss of 132627524482.694427, Test MSE of 132627523453.618790
Epoch 13: training loss 122315739136.000
Test Loss of 130154011938.642609, Test MSE of 130154013259.406281
Epoch 14: training loss 120475915354.353
Test Loss of 128219531228.469116, Test MSE of 128219531895.412994
Epoch 15: training loss 117528542418.824
Test Loss of 125679083703.813095, Test MSE of 125679085014.818863
Epoch 16: training loss 115236618330.353
Test Loss of 121339267498.844315, Test MSE of 121339268210.215363
Epoch 17: training loss 112171022637.176
Test Loss of 117799381420.739304, Test MSE of 117799381010.039764
Epoch 18: training loss 107726936003.765
Test Loss of 115065195335.950027, Test MSE of 115065194861.164398
Epoch 19: training loss 105713176997.647
Test Loss of 113136794431.422623, Test MSE of 113136797648.630554
Epoch 20: training loss 102714317763.765
Test Loss of 108398074488.449692, Test MSE of 108398076681.607834
Epoch 21: training loss 101278136139.294
Test Loss of 108486240707.479065, Test MSE of 108486241693.749664
Epoch 22: training loss 97643651734.588
Test Loss of 104573277350.995148, Test MSE of 104573278470.500092
Epoch 23: training loss 93826269665.882
Test Loss of 97280444524.487625, Test MSE of 97280443805.533051
Epoch 24: training loss 91019466857.412
Test Loss of 94819849725.986588, Test MSE of 94819850398.797318
Epoch 25: training loss 86630000188.235
Test Loss of 94699414633.408279, Test MSE of 94699415953.661255
Epoch 26: training loss 85044863352.471
Test Loss of 90487154089.659958, Test MSE of 90487153709.883392
Epoch 27: training loss 82301005552.941
Test Loss of 86109442075.477219, Test MSE of 86109440531.953903
Epoch 28: training loss 78611057603.765
Test Loss of 86681191464.978943, Test MSE of 86681192324.542709
Epoch 29: training loss 75925371361.882
Test Loss of 79325314020.285919, Test MSE of 79325315084.829666
Epoch 30: training loss 73188499666.824
Test Loss of 78155495440.581085, Test MSE of 78155494848.804276
Epoch 31: training loss 70426069157.647
Test Loss of 75616569062.121674, Test MSE of 75616569171.091629
Epoch 32: training loss 67751240990.118
Test Loss of 70542937440.229477, Test MSE of 70542937837.485367
Epoch 33: training loss 65042619497.412
Test Loss of 70402368422.225311, Test MSE of 70402369244.964340
Epoch 34: training loss 62734168869.647
Test Loss of 65965459098.322464, Test MSE of 65965459601.613029
Epoch 35: training loss 59974642582.588
Test Loss of 65866361258.844322, Test MSE of 65866361035.228271
Epoch 36: training loss 57555386880.000
Test Loss of 60873533228.235947, Test MSE of 60873532952.422150
Epoch 37: training loss 54855677259.294
Test Loss of 59326084746.215126, Test MSE of 59326085135.633240
Epoch 38: training loss 52846132359.529
Test Loss of 54832002609.151054, Test MSE of 54832004023.942413
Epoch 39: training loss 50074652032.000
Test Loss of 53323723364.078651, Test MSE of 53323723458.273033
Epoch 40: training loss 48134182422.588
Test Loss of 52911046836.023132, Test MSE of 52911046317.793747
Epoch 41: training loss 45899337110.588
Test Loss of 49683469192.853111, Test MSE of 49683468949.194397
Epoch 42: training loss 43658201825.882
Test Loss of 48450667843.331017, Test MSE of 48450668171.041374
Epoch 43: training loss 41700675794.824
Test Loss of 44457064599.124680, Test MSE of 44457065323.118393
Epoch 44: training loss 39896812950.588
Test Loss of 40103679918.989594, Test MSE of 40103680297.837982
Epoch 45: training loss 38142165692.235
Test Loss of 41080566204.372887, Test MSE of 41080566288.948181
Epoch 46: training loss 36341208071.529
Test Loss of 38471712641.983810, Test MSE of 38471712837.165909
Epoch 47: training loss 34769054328.471
Test Loss of 40578821213.564651, Test MSE of 40578821713.082748
Epoch 48: training loss 33228383533.176
Test Loss of 37617662185.556328, Test MSE of 37617662025.091415
Epoch 49: training loss 31384847661.176
Test Loss of 34961459544.412674, Test MSE of 34961459741.116119
Epoch 50: training loss 30684548984.471
Test Loss of 35584004270.338188, Test MSE of 35584005207.320740
Epoch 51: training loss 28820117187.765
Test Loss of 34400883878.047653, Test MSE of 34400883992.744164
Epoch 52: training loss 27643057099.294
Test Loss of 31798408162.864677, Test MSE of 31798408533.521057
Epoch 53: training loss 26405447017.412
Test Loss of 33887167277.894054, Test MSE of 33887168116.095261
Epoch 54: training loss 25282831969.882
Test Loss of 29750759986.809162, Test MSE of 29750759783.917446
Epoch 55: training loss 23852626443.294
Test Loss of 30990241378.894287, Test MSE of 30990241510.732433
Epoch 56: training loss 23222397519.059
Test Loss of 30376082726.432571, Test MSE of 30376082668.721645
Epoch 57: training loss 22147171004.235
Test Loss of 28099603710.637981, Test MSE of 28099603772.574799
Epoch 58: training loss 21973007273.412
Test Loss of 29358295064.634743, Test MSE of 29358295520.858498
Epoch 59: training loss 20784788336.941
Test Loss of 27181427447.413372, Test MSE of 27181428004.221886
Epoch 60: training loss 19993244690.824
Test Loss of 25216772409.145500, Test MSE of 25216772493.044380
Epoch 61: training loss 19491608914.824
Test Loss of 24934332618.762897, Test MSE of 24934332685.764923
Epoch 62: training loss 18829733816.471
Test Loss of 25728392218.766598, Test MSE of 25728391793.201389
Epoch 63: training loss 18027515102.118
Test Loss of 23475283154.105946, Test MSE of 23475283141.160690
Epoch 64: training loss 17387900291.765
Test Loss of 23458015159.043259, Test MSE of 23458015064.920696
Epoch 65: training loss 17066817588.706
Test Loss of 22010839485.320381, Test MSE of 22010839495.620136
Epoch 66: training loss 16610118878.118
Test Loss of 22131322477.316677, Test MSE of 22131322509.009510
Epoch 67: training loss 16146087043.765
Test Loss of 21499745898.474209, Test MSE of 21499745665.545101
Epoch 68: training loss 15390292544.000
Test Loss of 22221963603.201481, Test MSE of 22221963725.977123
Epoch 69: training loss 15166880752.941
Test Loss of 22562125257.400879, Test MSE of 22562125089.076832
Epoch 70: training loss 14732825758.118
Test Loss of 21939081111.776081, Test MSE of 21939080745.474628
Epoch 71: training loss 14565240602.353
Test Loss of 23385765934.900764, Test MSE of 23385765831.178352
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24761319724.313007, 'MSE - std': 3324945425.8641863, 'R2 - mean': 0.8223268135254003, 'R2 - std': 0.023040797728611208} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005479 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110917571.765
Test Loss of 410765839874.132324, Test MSE of 410765843578.757568
Epoch 2: training loss 430090831992.471
Test Loss of 410748243170.028687, Test MSE of 410748252319.964111
Epoch 3: training loss 430063827064.471
Test Loss of 410725621559.085632, Test MSE of 410725621070.969971
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080997255.529
Test Loss of 410728895462.885681, Test MSE of 410728902470.706543
Epoch 2: training loss 430069115964.235
Test Loss of 410729607700.612671, Test MSE of 410729603440.061279
Epoch 3: training loss 426160269191.529
Test Loss of 398625309217.880615, Test MSE of 398625311390.773804
Epoch 4: training loss 399213783642.353
Test Loss of 357165157814.315613, Test MSE of 357165163447.785950
Epoch 5: training loss 335125511830.588
Test Loss of 273952290945.836182, Test MSE of 273952292474.255432
Epoch 6: training loss 252969155523.765
Test Loss of 197104606614.093475, Test MSE of 197104603752.208099
Epoch 7: training loss 185210571053.176
Test Loss of 139265784506.461823, Test MSE of 139265785481.613708
Epoch 8: training loss 151454533872.941
Test Loss of 118652126222.689499, Test MSE of 118652127276.286957
Epoch 9: training loss 141355082390.588
Test Loss of 113151528905.032852, Test MSE of 113151529704.484726
Epoch 10: training loss 138774330880.000
Test Loss of 110422202956.527527, Test MSE of 110422203025.824097
Epoch 11: training loss 135791052167.529
Test Loss of 107731823117.978714, Test MSE of 107731820259.788055
Epoch 12: training loss 133308691486.118
Test Loss of 105621480345.173538, Test MSE of 105621483062.166336
Epoch 13: training loss 130732161355.294
Test Loss of 103592836793.987976, Test MSE of 103592836393.062012
Epoch 14: training loss 127347552587.294
Test Loss of 100756250991.711243, Test MSE of 100756250622.743408
Epoch 15: training loss 124802578462.118
Test Loss of 99089468892.223969, Test MSE of 99089468035.674164
Epoch 16: training loss 122635427508.706
Test Loss of 96002949616.599716, Test MSE of 96002949183.436829
Epoch 17: training loss 118605037869.176
Test Loss of 94267101129.032852, Test MSE of 94267099667.824005
Epoch 18: training loss 116198474691.765
Test Loss of 91472479121.591858, Test MSE of 91472478901.952789
Epoch 19: training loss 112596699888.941
Test Loss of 89029679760.762604, Test MSE of 89029680260.064697
Epoch 20: training loss 109197574354.824
Test Loss of 87768868499.131882, Test MSE of 87768869245.366226
Epoch 21: training loss 106518409968.941
Test Loss of 83566034356.420181, Test MSE of 83566033519.884918
Epoch 22: training loss 102402501632.000
Test Loss of 81747888085.353073, Test MSE of 81747889097.182144
Epoch 23: training loss 100579722270.118
Test Loss of 79746250358.700607, Test MSE of 79746250501.889832
Epoch 24: training loss 96333839299.765
Test Loss of 77691368710.989349, Test MSE of 77691368369.253967
Epoch 25: training loss 93305031047.529
Test Loss of 73651764000.814438, Test MSE of 73651764291.599319
Epoch 26: training loss 90265218921.412
Test Loss of 71302924298.424805, Test MSE of 71302923663.706299
Epoch 27: training loss 88017845729.882
Test Loss of 71260605154.265625, Test MSE of 71260604619.195099
Epoch 28: training loss 84469913856.000
Test Loss of 66971149402.506248, Test MSE of 66971149817.070656
Epoch 29: training loss 82085371557.647
Test Loss of 66232381259.935219, Test MSE of 66232380579.776375
Epoch 30: training loss 79138691809.882
Test Loss of 61918131940.634888, Test MSE of 61918131908.706665
Epoch 31: training loss 75782905735.529
Test Loss of 62208232674.028694, Test MSE of 62208232862.342163
Epoch 32: training loss 73094572227.765
Test Loss of 58686294769.428970, Test MSE of 58686294423.397636
Epoch 33: training loss 70466549473.882
Test Loss of 57647583719.122627, Test MSE of 57647584143.167587
Epoch 34: training loss 68003063883.294
Test Loss of 53586100032.562706, Test MSE of 53586100665.631264
Epoch 35: training loss 64587642864.941
Test Loss of 52545764500.316521, Test MSE of 52545765101.924492
Epoch 36: training loss 62112473404.235
Test Loss of 50045416094.978249, Test MSE of 50045416196.316162
Epoch 37: training loss 59161480794.353
Test Loss of 47387257102.097176, Test MSE of 47387257215.330902
Epoch 38: training loss 57497123320.471
Test Loss of 47562927026.761681, Test MSE of 47562927892.810059
Epoch 39: training loss 54605639747.765
Test Loss of 43412875196.712631, Test MSE of 43412875838.800476
Epoch 40: training loss 52604339230.118
Test Loss of 41745498862.585838, Test MSE of 41745498967.991714
Epoch 41: training loss 49914599002.353
Test Loss of 40857510316.364647, Test MSE of 40857510788.644020
Epoch 42: training loss 48523629025.882
Test Loss of 39220777938.036095, Test MSE of 39220777622.095314
Epoch 43: training loss 46065569272.471
Test Loss of 40261477479.774178, Test MSE of 40261477797.075577
Epoch 44: training loss 43584347429.647
Test Loss of 39141423163.705688, Test MSE of 39141423439.110710
Epoch 45: training loss 41732268762.353
Test Loss of 35649088277.441925, Test MSE of 35649088202.917763
Epoch 46: training loss 40501276905.412
Test Loss of 34347645215.155945, Test MSE of 34347645733.725838
Epoch 47: training loss 38724040779.294
Test Loss of 31641107373.075428, Test MSE of 31641107804.121941
Epoch 48: training loss 36944090383.059
Test Loss of 33646825428.405369, Test MSE of 33646826292.857285
Epoch 49: training loss 34858298646.588
Test Loss of 28923108397.016197, Test MSE of 28923108670.348030
Epoch 50: training loss 33273777498.353
Test Loss of 28278249545.921333, Test MSE of 28278249495.558472
Epoch 51: training loss 32442564976.941
Test Loss of 27700935343.089310, Test MSE of 27700934820.311489
Epoch 52: training loss 30832918761.412
Test Loss of 28441508564.997684, Test MSE of 28441508707.598022
Epoch 53: training loss 29409126731.294
Test Loss of 26815867735.781582, Test MSE of 26815868830.095097
Epoch 54: training loss 27878959559.529
Test Loss of 24957874876.357243, Test MSE of 24957874869.088333
Epoch 55: training loss 27183917952.000
Test Loss of 24906708990.104580, Test MSE of 24906708948.971321
Epoch 56: training loss 25913377795.765
Test Loss of 22484151332.486813, Test MSE of 22484151113.597404
Epoch 57: training loss 24666913675.294
Test Loss of 25052706049.303101, Test MSE of 25052705409.726963
Epoch 58: training loss 23993852122.353
Test Loss of 24427223005.882462, Test MSE of 24427223463.322628
Epoch 59: training loss 23174950927.059
Test Loss of 22408758788.975475, Test MSE of 22408758876.708260
Epoch 60: training loss 22019414400.000
Test Loss of 23694318053.227211, Test MSE of 23694317907.252144
Epoch 61: training loss 21579760879.059
Test Loss of 22660915972.383156, Test MSE of 22660916225.732002
Epoch 62: training loss 20507775265.882
Test Loss of 21000850513.976864, Test MSE of 21000850763.808773
Epoch 63: training loss 20244990957.176
Test Loss of 23434614718.608051, Test MSE of 23434615091.390465
Epoch 64: training loss 19859331026.824
Test Loss of 23233612401.488201, Test MSE of 23233612586.637154
Epoch 65: training loss 19064370793.412
Test Loss of 19585308769.614067, Test MSE of 19585308806.150043
Epoch 66: training loss 18654660641.882
Test Loss of 21885636232.707081, Test MSE of 21885636207.913330
Epoch 67: training loss 17891323026.824
Test Loss of 21546289170.006477, Test MSE of 21546289135.956882
Epoch 68: training loss 17153833667.765
Test Loss of 19116138169.987968, Test MSE of 19116137774.750904
Epoch 69: training loss 16687060822.588
Test Loss of 20510977020.683018, Test MSE of 20510977349.782585
Epoch 70: training loss 16274543348.706
Test Loss of 20680502092.409069, Test MSE of 20680502224.542271
Epoch 71: training loss 15790827653.647
Test Loss of 19690021370.076817, Test MSE of 19690021245.214478
Epoch 72: training loss 15423058108.235
Test Loss of 20845925445.656639, Test MSE of 20845925143.910519
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23782471079.212387, 'MSE - std': 3341538653.6410675, 'R2 - mean': 0.8237320390707962, 'R2 - std': 0.020101809532437155} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005477 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043498194.824
Test Loss of 431614605366.019409, Test MSE of 431614611207.536621
Epoch 2: training loss 424024148690.824
Test Loss of 431595000882.702454, Test MSE of 431595003329.073669
Epoch 3: training loss 423996456357.647
Test Loss of 431567508667.172607, Test MSE of 431567509742.527893
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010189884.235
Test Loss of 431568229002.128662, Test MSE of 431568232406.747131
Epoch 2: training loss 424001693816.471
Test Loss of 431572419090.717285, Test MSE of 431572418519.742432
Epoch 3: training loss 420304468811.294
Test Loss of 419165163904.770020, Test MSE of 419165162745.294800
Epoch 4: training loss 394044858849.882
Test Loss of 377749902598.989380, Test MSE of 377749907073.481750
Epoch 5: training loss 330142847819.294
Test Loss of 292865454483.724182, Test MSE of 292865449035.650513
Epoch 6: training loss 248742953984.000
Test Loss of 214870951988.124023, Test MSE of 214870950929.015839
Epoch 7: training loss 182725295887.059
Test Loss of 154439160897.865814, Test MSE of 154439159971.231232
Epoch 8: training loss 147234580931.765
Test Loss of 131637566286.304489, Test MSE of 131637569401.385696
Epoch 9: training loss 137310855258.353
Test Loss of 124365963969.095795, Test MSE of 124365964914.833206
Epoch 10: training loss 134385647826.824
Test Loss of 121755713380.575653, Test MSE of 121755714388.783890
Epoch 11: training loss 132248089600.000
Test Loss of 119427234577.651093, Test MSE of 119427233247.430969
Epoch 12: training loss 129641500581.647
Test Loss of 116538677715.220734, Test MSE of 116538679036.429581
Epoch 13: training loss 126400649020.235
Test Loss of 113385043047.774185, Test MSE of 113385043036.615463
Epoch 14: training loss 123953683034.353
Test Loss of 111156697312.607132, Test MSE of 111156696238.045761
Epoch 15: training loss 121442623247.059
Test Loss of 107613356752.259140, Test MSE of 107613357530.230255
Epoch 16: training loss 118652132472.471
Test Loss of 105466413791.422485, Test MSE of 105466413551.807404
Epoch 17: training loss 114356849302.588
Test Loss of 103206105620.612686, Test MSE of 103206102911.295593
Epoch 18: training loss 112086075407.059
Test Loss of 99451654384.718185, Test MSE of 99451653983.019806
Epoch 19: training loss 109349769351.529
Test Loss of 98046532608.473862, Test MSE of 98046532788.109756
Epoch 20: training loss 105049668683.294
Test Loss of 93566947587.672379, Test MSE of 93566947876.328003
Epoch 21: training loss 103073849705.412
Test Loss of 90551755927.159653, Test MSE of 90551754752.059555
Epoch 22: training loss 100189644890.353
Test Loss of 88800788570.980103, Test MSE of 88800787360.028549
Epoch 23: training loss 96974643154.824
Test Loss of 85436010486.522903, Test MSE of 85436008815.331284
Epoch 24: training loss 94434212728.471
Test Loss of 82065584979.043030, Test MSE of 82065586012.011307
Epoch 25: training loss 90285777377.882
Test Loss of 80407466744.062927, Test MSE of 80407467510.987701
Epoch 26: training loss 87630433957.647
Test Loss of 74412115623.981491, Test MSE of 74412114650.100006
Epoch 27: training loss 85424891015.529
Test Loss of 74912090751.703842, Test MSE of 74912089757.586472
Epoch 28: training loss 81833987448.471
Test Loss of 70655377214.667282, Test MSE of 70655377844.969025
Epoch 29: training loss 79381470117.647
Test Loss of 67997904397.504860, Test MSE of 67997903155.980392
Epoch 30: training loss 75992194078.118
Test Loss of 68771853127.670517, Test MSE of 68771853346.223740
Epoch 31: training loss 73817818774.588
Test Loss of 64272053687.263306, Test MSE of 64272053243.103081
Epoch 32: training loss 70620235429.647
Test Loss of 60703672368.333176, Test MSE of 60703673191.266899
Epoch 33: training loss 68426399555.765
Test Loss of 56565845282.472931, Test MSE of 56565845522.463226
Epoch 34: training loss 64754966339.765
Test Loss of 55986599478.256363, Test MSE of 55986599621.852020
Epoch 35: training loss 62934367307.294
Test Loss of 54693728991.422493, Test MSE of 54693728946.643509
Epoch 36: training loss 60250349244.235
Test Loss of 51293464836.620087, Test MSE of 51293464643.489571
Epoch 37: training loss 57383362605.176
Test Loss of 49885389747.709396, Test MSE of 49885390625.970917
Epoch 38: training loss 55279703559.529
Test Loss of 44612563451.972237, Test MSE of 44612564115.245110
Epoch 39: training loss 52789284916.706
Test Loss of 41550693064.677467, Test MSE of 41550692953.429359
Epoch 40: training loss 51356502648.471
Test Loss of 41988867987.013420, Test MSE of 41988867444.480003
Epoch 41: training loss 48190594454.588
Test Loss of 41171102402.043495, Test MSE of 41171102526.391663
Epoch 42: training loss 46543835136.000
Test Loss of 39180714696.677467, Test MSE of 39180714352.184074
Epoch 43: training loss 44454021955.765
Test Loss of 36822768451.879684, Test MSE of 36822768672.358643
Epoch 44: training loss 41695747843.765
Test Loss of 33300543541.071728, Test MSE of 33300543790.782528
Epoch 45: training loss 40724916141.176
Test Loss of 32893638521.425266, Test MSE of 32893638933.412598
Epoch 46: training loss 39005958290.824
Test Loss of 35299235078.041649, Test MSE of 35299235142.553696
Epoch 47: training loss 37550197285.647
Test Loss of 33767170166.463673, Test MSE of 33767169540.925758
Epoch 48: training loss 35598314669.176
Test Loss of 30979974976.562702, Test MSE of 30979975384.962517
Epoch 49: training loss 33983754096.941
Test Loss of 26348420777.876907, Test MSE of 26348420959.681728
Epoch 50: training loss 32688112045.176
Test Loss of 29121094405.330864, Test MSE of 29121093974.666523
Epoch 51: training loss 31065283049.412
Test Loss of 28063460889.825081, Test MSE of 28063460996.858162
Epoch 52: training loss 29786271789.176
Test Loss of 24987584477.882462, Test MSE of 24987584348.828213
Epoch 53: training loss 28851120421.647
Test Loss of 27280686806.419250, Test MSE of 27280686728.345486
Epoch 54: training loss 27216312944.941
Test Loss of 25808574672.496067, Test MSE of 25808574531.581791
Epoch 55: training loss 26532951352.471
Test Loss of 23040272897.658493, Test MSE of 23040272509.408062
Epoch 56: training loss 25082769133.176
Test Loss of 22787744204.586765, Test MSE of 22787744139.463406
Epoch 57: training loss 24299693620.706
Test Loss of 23160981887.822304, Test MSE of 23160981658.610050
Epoch 58: training loss 23429230682.353
Test Loss of 23449315499.535400, Test MSE of 23449316103.587376
Epoch 59: training loss 22509300416.000
Test Loss of 23201699631.503933, Test MSE of 23201699320.040375
Epoch 60: training loss 21661093033.412
Test Loss of 22070317193.417862, Test MSE of 22070317123.029949
Epoch 61: training loss 21338281999.059
Test Loss of 20286590065.251274, Test MSE of 20286589778.403957
Epoch 62: training loss 20420356190.118
Test Loss of 23149377623.189262, Test MSE of 23149377440.426689
Epoch 63: training loss 19658404788.706
Test Loss of 20085201345.214252, Test MSE of 20085201091.642960
Epoch 64: training loss 19292451821.176
Test Loss of 21160629802.883850, Test MSE of 21160629893.991272
Epoch 65: training loss 18566354394.353
Test Loss of 20608918126.645073, Test MSE of 20608918284.127453
Epoch 66: training loss 18200597620.706
Test Loss of 20340432170.054604, Test MSE of 20340431771.429237
Epoch 67: training loss 17583593219.765
Test Loss of 19471637149.082832, Test MSE of 19471636962.724533
Epoch 68: training loss 17036646027.294
Test Loss of 20546016959.674225, Test MSE of 20546016992.926800
Epoch 69: training loss 16560290556.235
Test Loss of 20192342480.377602, Test MSE of 20192342357.238197
Epoch 70: training loss 16290774913.882
Test Loss of 19375473727.970383, Test MSE of 19375473700.535870
Epoch 71: training loss 15905868208.941
Test Loss of 21056097143.055992, Test MSE of 21056097229.065044
Epoch 72: training loss 15626892220.235
Test Loss of 20517325575.226284, Test MSE of 20517325290.507050
Epoch 73: training loss 14766540374.588
Test Loss of 20039658432.977325, Test MSE of 20039658370.658539
Epoch 74: training loss 14650747674.353
Test Loss of 19595545716.094402, Test MSE of 19595545336.463646
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22945085930.66264, 'MSE - std': 3426012260.2040734, 'R2 - mean': 0.8297175792364004, 'R2 - std': 0.021600300032151194} 
 

Saving model.....
Results After CV: {'MSE - mean': 22945085930.66264, 'MSE - std': 3426012260.2040734, 'R2 - mean': 0.8297175792364004, 'R2 - std': 0.021600300032151194}
Train time: 104.15014729220002
Inference time: 0.0719296438001038
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 21 finished with value: 22945085930.66264 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005385 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526272060.235
Test Loss of 418112121522.246582, Test MSE of 418112117433.765503
Epoch 2: training loss 427506622464.000
Test Loss of 418094985427.290283, Test MSE of 418094987546.041321
Epoch 3: training loss 427479431288.471
Test Loss of 418071990197.148254, Test MSE of 418071995906.727417
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492939776.000
Test Loss of 418077097100.939148, Test MSE of 418077096346.587891
Epoch 2: training loss 427484280229.647
Test Loss of 418078604724.556091, Test MSE of 418078613323.759094
Epoch 3: training loss 427483882435.765
Test Loss of 418078562807.591003, Test MSE of 418078558396.626953
Epoch 4: training loss 422147830603.294
Test Loss of 400938718246.136475, Test MSE of 400938718196.833313
Epoch 5: training loss 385579139794.824
Test Loss of 346444011098.840637, Test MSE of 346444018561.110229
Epoch 6: training loss 317426683542.588
Test Loss of 269098089919.925964, Test MSE of 269098086745.915222
Epoch 7: training loss 228237102983.529
Test Loss of 174882406532.648621, Test MSE of 174882405430.984619
Epoch 8: training loss 163595303559.529
Test Loss of 133596324277.740463, Test MSE of 133596323037.615219
Epoch 9: training loss 144150807401.412
Test Loss of 121360819098.855423, Test MSE of 121360818974.023300
Epoch 10: training loss 138025069206.588
Test Loss of 117381387245.287064, Test MSE of 117381389917.510284
Epoch 11: training loss 135229928719.059
Test Loss of 114266225617.336105, Test MSE of 114266225185.584305
Epoch 12: training loss 131238598106.353
Test Loss of 111652005604.226700, Test MSE of 111652004052.225571
Epoch 13: training loss 127865738059.294
Test Loss of 108714597416.505203, Test MSE of 108714601281.695892
Epoch 14: training loss 125580189214.118
Test Loss of 106185220522.370575, Test MSE of 106185220470.688049
Epoch 15: training loss 122091016402.824
Test Loss of 103208055916.250748, Test MSE of 103208054908.299683
Epoch 16: training loss 118912051712.000
Test Loss of 100161451292.483917, Test MSE of 100161452545.229965
Epoch 17: training loss 114614477522.824
Test Loss of 97442549497.545227, Test MSE of 97442549273.152695
Epoch 18: training loss 112183820724.706
Test Loss of 96039208414.719406, Test MSE of 96039209579.029541
Epoch 19: training loss 108946489792.000
Test Loss of 91907447395.604904, Test MSE of 91907449348.367706
Epoch 20: training loss 106088519408.941
Test Loss of 89879933063.491089, Test MSE of 89879935465.530182
Epoch 21: training loss 102907509760.000
Test Loss of 87327009930.096695, Test MSE of 87327011678.558548
Epoch 22: training loss 99528937615.059
Test Loss of 83175153836.443207, Test MSE of 83175155266.601639
Epoch 23: training loss 96043208508.235
Test Loss of 82503509140.992828, Test MSE of 82503509816.732681
Epoch 24: training loss 92653128086.588
Test Loss of 77289755555.382843, Test MSE of 77289755559.861252
Epoch 25: training loss 89766900193.882
Test Loss of 78504892131.279205, Test MSE of 78504890971.396332
Epoch 26: training loss 87136132954.353
Test Loss of 73185787716.396942, Test MSE of 73185787936.832092
Epoch 27: training loss 84973139365.647
Test Loss of 69441487818.229935, Test MSE of 69441487263.015076
Epoch 28: training loss 81116687058.824
Test Loss of 66734700867.567894, Test MSE of 66734702148.247414
Epoch 29: training loss 78461015642.353
Test Loss of 64302078709.518387, Test MSE of 64302078810.585716
Epoch 30: training loss 75297077955.765
Test Loss of 63646899693.405502, Test MSE of 63646899585.612724
Epoch 31: training loss 72614696824.471
Test Loss of 61257673964.635666, Test MSE of 61257673747.567833
Epoch 32: training loss 69391832711.529
Test Loss of 59539524033.584084, Test MSE of 59539523314.863953
Epoch 33: training loss 66968096361.412
Test Loss of 56697271192.012955, Test MSE of 56697271995.814087
Epoch 34: training loss 63855383318.588
Test Loss of 53454504946.972008, Test MSE of 53454504770.416206
Epoch 35: training loss 61303325342.118
Test Loss of 54418167492.959518, Test MSE of 54418167501.466270
Epoch 36: training loss 58665816425.412
Test Loss of 50327727302.736061, Test MSE of 50327727009.897003
Epoch 37: training loss 56852284611.765
Test Loss of 49053525190.262321, Test MSE of 49053524694.040436
Epoch 38: training loss 54490611636.706
Test Loss of 48919450931.223686, Test MSE of 48919450687.363373
Epoch 39: training loss 52446160248.471
Test Loss of 44378046656.340508, Test MSE of 44378046583.705437
Epoch 40: training loss 49509078561.882
Test Loss of 44438835146.940552, Test MSE of 44438835447.919220
Epoch 41: training loss 48133668784.941
Test Loss of 39403875705.811707, Test MSE of 39403875394.818306
Epoch 42: training loss 45159653782.588
Test Loss of 38659825246.156837, Test MSE of 38659824976.116104
Epoch 43: training loss 43585142878.118
Test Loss of 36577245959.520706, Test MSE of 36577246004.790161
Epoch 44: training loss 41664989903.059
Test Loss of 35400824620.235947, Test MSE of 35400824651.533165
Epoch 45: training loss 39845223695.059
Test Loss of 35513500624.862366, Test MSE of 35513500464.668762
Epoch 46: training loss 37805364359.529
Test Loss of 32540952299.806614, Test MSE of 32540952183.908424
Epoch 47: training loss 36584856071.529
Test Loss of 31753824586.200325, Test MSE of 31753825189.029774
Epoch 48: training loss 34698530913.882
Test Loss of 31025539994.381680, Test MSE of 31025540456.676273
Epoch 49: training loss 33370825586.824
Test Loss of 29216369205.177887, Test MSE of 29216369867.540363
Epoch 50: training loss 31597260997.647
Test Loss of 29101505919.496647, Test MSE of 29101505570.465557
Epoch 51: training loss 30895014151.529
Test Loss of 26860457061.144577, Test MSE of 26860457220.848789
Epoch 52: training loss 29274890255.059
Test Loss of 27280013144.767986, Test MSE of 27280013325.318226
Epoch 53: training loss 27538724784.941
Test Loss of 27016905831.513302, Test MSE of 27016906243.863239
Epoch 54: training loss 26720598738.824
Test Loss of 24723509227.155216, Test MSE of 24723508995.557446
Epoch 55: training loss 25834491904.000
Test Loss of 24956195671.583622, Test MSE of 24956195493.618298
Epoch 56: training loss 24472760572.235
Test Loss of 26134653558.554707, Test MSE of 26134652985.904125
Epoch 57: training loss 23785361524.706
Test Loss of 23955079094.095768, Test MSE of 23955079002.658707
Epoch 58: training loss 22936171983.059
Test Loss of 22621413928.860512, Test MSE of 22621413819.587273
Epoch 59: training loss 22065228340.706
Test Loss of 21712125467.832523, Test MSE of 21712125525.577236
Epoch 60: training loss 21534778808.471
Test Loss of 22291475286.162388, Test MSE of 22291475230.922989
Epoch 61: training loss 20573248282.353
Test Loss of 23291775149.390701, Test MSE of 23291774792.733898
Epoch 62: training loss 20130834537.412
Test Loss of 21246440796.913254, Test MSE of 21246440478.145802
Epoch 63: training loss 19054127457.882
Test Loss of 20276288366.797131, Test MSE of 20276288412.576237
Epoch 64: training loss 18529589323.294
Test Loss of 21335348244.134167, Test MSE of 21335348311.272270
Epoch 65: training loss 18117036299.294
Test Loss of 20874050677.725655, Test MSE of 20874050311.299366
Epoch 66: training loss 17389793167.059
Test Loss of 20707077801.956051, Test MSE of 20707077772.252579
Epoch 67: training loss 16987833197.176
Test Loss of 21227962056.749481, Test MSE of 21227961897.307426
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21227961897.307426, 'MSE - std': 0.0, 'R2 - mean': 0.8346955574893209, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005426 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918003983.059
Test Loss of 424556265608.438599, Test MSE of 424556272452.390869
Epoch 2: training loss 427897024512.000
Test Loss of 424539775413.740479, Test MSE of 424539781066.395020
Epoch 3: training loss 427869242789.647
Test Loss of 424517050066.698120, Test MSE of 424517058619.093323
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888077884.235
Test Loss of 424523041054.615784, Test MSE of 424523045117.647461
Epoch 2: training loss 427876339471.059
Test Loss of 424523735125.274109, Test MSE of 424523742675.247253
Epoch 3: training loss 427875826266.353
Test Loss of 424523493090.805481, Test MSE of 424523495064.331360
Epoch 4: training loss 422473161547.294
Test Loss of 407904533743.478149, Test MSE of 407904536246.406006
Epoch 5: training loss 386050148954.353
Test Loss of 353471683309.938477, Test MSE of 353471679119.873291
Epoch 6: training loss 317382680094.118
Test Loss of 276433690603.155212, Test MSE of 276433694629.925415
Epoch 7: training loss 227362556205.176
Test Loss of 184663823724.073090, Test MSE of 184663829075.399780
Epoch 8: training loss 161995685948.235
Test Loss of 143419897702.032837, Test MSE of 143419893361.067566
Epoch 9: training loss 141280919672.471
Test Loss of 132123028646.521393, Test MSE of 132123027577.324951
Epoch 10: training loss 134001781790.118
Test Loss of 127754894933.629425, Test MSE of 127754892220.141479
Epoch 11: training loss 131857441520.941
Test Loss of 125418354884.604218, Test MSE of 125418356393.231384
Epoch 12: training loss 128142696146.824
Test Loss of 123048435207.935226, Test MSE of 123048435023.861176
Epoch 13: training loss 123791442010.353
Test Loss of 119834044967.913025, Test MSE of 119834045545.216751
Epoch 14: training loss 121912912655.059
Test Loss of 116827519676.668976, Test MSE of 116827522616.565643
Epoch 15: training loss 118762821632.000
Test Loss of 113816188207.433731, Test MSE of 113816187362.143066
Epoch 16: training loss 114803439013.647
Test Loss of 110672098529.739532, Test MSE of 110672096923.550659
Epoch 17: training loss 110981374915.765
Test Loss of 107318032050.009720, Test MSE of 107318031894.997452
Epoch 18: training loss 108706022219.294
Test Loss of 104816290622.475128, Test MSE of 104816288348.677689
Epoch 19: training loss 105333071902.118
Test Loss of 101768045439.378204, Test MSE of 101768043605.612915
Epoch 20: training loss 102604404404.706
Test Loss of 98321081878.621323, Test MSE of 98321084481.362610
Epoch 21: training loss 98597974467.765
Test Loss of 95882914188.524643, Test MSE of 95882909015.567612
Epoch 22: training loss 95398151258.353
Test Loss of 92360908407.739075, Test MSE of 92360905225.297058
Epoch 23: training loss 92002705076.706
Test Loss of 87804920385.969009, Test MSE of 87804922045.816772
Epoch 24: training loss 88548041336.471
Test Loss of 85966742194.720337, Test MSE of 85966741811.982483
Epoch 25: training loss 85927322804.706
Test Loss of 82224046332.979874, Test MSE of 82224046368.713531
Epoch 26: training loss 82263650424.471
Test Loss of 78705604019.134857, Test MSE of 78705606130.622055
Epoch 27: training loss 79748814351.059
Test Loss of 78054183820.643066, Test MSE of 78054183377.077911
Epoch 28: training loss 77140599461.647
Test Loss of 72338850184.260925, Test MSE of 72338850762.068878
Epoch 29: training loss 72894469044.706
Test Loss of 70231302600.690262, Test MSE of 70231301873.285965
Epoch 30: training loss 70013142768.941
Test Loss of 70278493345.310196, Test MSE of 70278491936.799698
Epoch 31: training loss 67414718825.412
Test Loss of 66567492221.660881, Test MSE of 66567492680.902786
Epoch 32: training loss 64573352960.000
Test Loss of 62374640323.301414, Test MSE of 62374638223.220703
Epoch 33: training loss 61667221308.235
Test Loss of 62481834399.000694, Test MSE of 62481836067.882866
Epoch 34: training loss 59452987888.941
Test Loss of 59675840012.672684, Test MSE of 59675838328.524635
Epoch 35: training loss 56870970729.412
Test Loss of 58442493507.627113, Test MSE of 58442490939.718773
Epoch 36: training loss 54286485609.412
Test Loss of 51885250739.549385, Test MSE of 51885251319.973442
Epoch 37: training loss 50987756378.353
Test Loss of 51667396235.162621, Test MSE of 51667396971.969345
Epoch 38: training loss 49241266507.294
Test Loss of 50255948205.213043, Test MSE of 50255948337.878967
Epoch 39: training loss 46449304176.941
Test Loss of 47056941075.897293, Test MSE of 47056942440.035255
Epoch 40: training loss 44331419015.529
Test Loss of 47765312534.502892, Test MSE of 47765311925.172638
Epoch 41: training loss 42735407081.412
Test Loss of 44951765935.226463, Test MSE of 44951765746.656555
Epoch 42: training loss 40557752372.706
Test Loss of 42186859344.477448, Test MSE of 42186858736.957214
Epoch 43: training loss 38327263683.765
Test Loss of 39200116826.959053, Test MSE of 39200116436.875275
Epoch 44: training loss 37419200308.706
Test Loss of 41251493373.039093, Test MSE of 41251494248.315086
Epoch 45: training loss 35008443565.176
Test Loss of 38539265930.984962, Test MSE of 38539265655.952171
Epoch 46: training loss 33124934113.882
Test Loss of 34626147479.835297, Test MSE of 34626147083.609161
Epoch 47: training loss 30979913660.235
Test Loss of 35132772388.004623, Test MSE of 35132772440.225815
Epoch 48: training loss 29652974305.882
Test Loss of 35267221656.072174, Test MSE of 35267220958.868088
Epoch 49: training loss 28447619840.000
Test Loss of 32023528292.848484, Test MSE of 32023528138.686970
Epoch 50: training loss 27235638008.471
Test Loss of 31470453508.678234, Test MSE of 31470452702.476761
Epoch 51: training loss 25516119296.000
Test Loss of 33808804341.222298, Test MSE of 33808803976.232689
Epoch 52: training loss 24799028325.647
Test Loss of 32515373929.585938, Test MSE of 32515373580.062088
Epoch 53: training loss 23337792922.353
Test Loss of 31985517248.458942, Test MSE of 31985517089.303867
Epoch 54: training loss 22696142885.647
Test Loss of 27864811947.318066, Test MSE of 27864811838.283615
Epoch 55: training loss 21311547120.941
Test Loss of 33971538061.176037, Test MSE of 33971537336.262356
Epoch 56: training loss 20777465656.471
Test Loss of 30863409180.661579, Test MSE of 30863409137.396320
Epoch 57: training loss 20045419885.176
Test Loss of 26917741030.062458, Test MSE of 26917740832.255775
Epoch 58: training loss 18899676412.235
Test Loss of 29988417974.214203, Test MSE of 29988417496.627289
Epoch 59: training loss 18271322910.118
Test Loss of 27431352142.108723, Test MSE of 27431352622.564091
Epoch 60: training loss 17656189737.412
Test Loss of 28299216730.189220, Test MSE of 28299217117.894039
Epoch 61: training loss 16964301929.412
Test Loss of 28969706846.097618, Test MSE of 28969706995.575771
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25098834446.441597, 'MSE - std': 3870872549.1341724, 'R2 - mean': 0.8139355416756653, 'R2 - std': 0.020760015813655552} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003726 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926921035.294
Test Loss of 447259325627.839905, Test MSE of 447259332044.610840
Epoch 2: training loss 421906034447.059
Test Loss of 447241058153.822815, Test MSE of 447241053584.330994
Epoch 3: training loss 421879001810.824
Test Loss of 447217101327.278259, Test MSE of 447217101828.410095
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902908114.824
Test Loss of 447226199067.240356, Test MSE of 447226194830.926086
Epoch 2: training loss 421889295058.824
Test Loss of 447226516817.543396, Test MSE of 447226514956.939392
Epoch 3: training loss 421888834258.824
Test Loss of 447225662651.366211, Test MSE of 447225662529.971436
Epoch 4: training loss 416653710275.765
Test Loss of 430432685235.786255, Test MSE of 430432691131.781494
Epoch 5: training loss 380567727646.118
Test Loss of 375365043904.222046, Test MSE of 375365042713.884888
Epoch 6: training loss 312755723324.235
Test Loss of 296691155387.425415, Test MSE of 296691156611.710510
Epoch 7: training loss 223464334275.765
Test Loss of 200186573390.286377, Test MSE of 200186573953.215057
Epoch 8: training loss 159178244909.176
Test Loss of 156328056801.206573, Test MSE of 156328059704.934479
Epoch 9: training loss 137977350264.471
Test Loss of 143097179435.170013, Test MSE of 143097181396.807770
Epoch 10: training loss 132858561415.529
Test Loss of 137667336244.585693, Test MSE of 137667336795.112091
Epoch 11: training loss 129836475060.706
Test Loss of 135189289573.499878, Test MSE of 135189286879.889923
Epoch 12: training loss 125668158102.588
Test Loss of 131928837584.033310, Test MSE of 131928837195.383621
Epoch 13: training loss 122407315486.118
Test Loss of 128673929488.403427, Test MSE of 128673931073.419769
Epoch 14: training loss 119187202048.000
Test Loss of 126057326528.044418, Test MSE of 126057325557.609818
Epoch 15: training loss 117393038998.588
Test Loss of 122125793933.294464, Test MSE of 122125793403.038864
Epoch 16: training loss 113607765714.824
Test Loss of 119504545674.984970, Test MSE of 119504545966.523407
Epoch 17: training loss 110551256214.588
Test Loss of 117666216028.380295, Test MSE of 117666219419.940002
Epoch 18: training loss 106657050955.294
Test Loss of 114584900820.474670, Test MSE of 114584901354.703064
Epoch 19: training loss 104741064734.118
Test Loss of 110940062397.142731, Test MSE of 110940063321.263214
Epoch 20: training loss 101055603983.059
Test Loss of 107141413464.235016, Test MSE of 107141413847.733780
Epoch 21: training loss 96662901067.294
Test Loss of 103691659643.469818, Test MSE of 103691660740.179871
Epoch 22: training loss 93799591875.765
Test Loss of 100488422886.299332, Test MSE of 100488424270.047974
Epoch 23: training loss 90264796521.412
Test Loss of 96751766103.287537, Test MSE of 96751768048.532455
Epoch 24: training loss 87073682266.353
Test Loss of 92061635252.615311, Test MSE of 92061634863.139511
Epoch 25: training loss 85185877308.235
Test Loss of 90852457279.422623, Test MSE of 90852458663.525238
Epoch 26: training loss 81771666718.118
Test Loss of 86856493297.136246, Test MSE of 86856493237.098633
Epoch 27: training loss 78574976120.471
Test Loss of 84196370644.711548, Test MSE of 84196369375.555084
Epoch 28: training loss 75529146217.412
Test Loss of 79481012779.229233, Test MSE of 79481013158.942047
Epoch 29: training loss 72451837801.412
Test Loss of 78549656833.006714, Test MSE of 78549658218.988754
Epoch 30: training loss 69455705103.059
Test Loss of 75578509858.228088, Test MSE of 75578508807.375641
Epoch 31: training loss 67281130134.588
Test Loss of 71020702725.448074, Test MSE of 71020703753.464798
Epoch 32: training loss 63822223856.941
Test Loss of 68446675493.544296, Test MSE of 68446675614.704124
Epoch 33: training loss 61714445568.000
Test Loss of 63979319569.824661, Test MSE of 63979318960.424850
Epoch 34: training loss 59192423973.647
Test Loss of 61079386918.787880, Test MSE of 61079386850.349548
Epoch 35: training loss 56670805669.647
Test Loss of 61511846011.647469, Test MSE of 61511846500.867493
Epoch 36: training loss 53297864365.176
Test Loss of 55710133939.430954, Test MSE of 55710134342.540054
Epoch 37: training loss 51772677097.412
Test Loss of 55711448773.433266, Test MSE of 55711448061.432434
Epoch 38: training loss 49104868329.412
Test Loss of 51941863911.957436, Test MSE of 51941864602.145920
Epoch 39: training loss 47737884717.176
Test Loss of 49472921216.266479, Test MSE of 49472920836.367943
Epoch 40: training loss 45106912067.765
Test Loss of 50063565080.457092, Test MSE of 50063564610.913239
Epoch 41: training loss 43224747625.412
Test Loss of 47058386510.996994, Test MSE of 47058387009.510956
Epoch 42: training loss 40611033720.471
Test Loss of 42289252158.238258, Test MSE of 42289252832.706650
Epoch 43: training loss 39465684615.529
Test Loss of 43015916396.902153, Test MSE of 43015917008.574631
Epoch 44: training loss 37355169129.412
Test Loss of 41980192104.283134, Test MSE of 41980191802.760536
Epoch 45: training loss 35658732099.765
Test Loss of 39854797613.894058, Test MSE of 39854797845.477097
Epoch 46: training loss 33682046960.941
Test Loss of 41008378985.408279, Test MSE of 41008379301.198021
Epoch 47: training loss 32758699361.882
Test Loss of 39389700709.026138, Test MSE of 39389701140.162560
Epoch 48: training loss 31239858831.059
Test Loss of 37072055150.797134, Test MSE of 37072055200.996399
Epoch 49: training loss 29582890661.647
Test Loss of 28545007457.769142, Test MSE of 28545007850.174625
Epoch 50: training loss 28517433389.176
Test Loss of 33749099554.346519, Test MSE of 33749100772.296692
Epoch 51: training loss 26706222309.647
Test Loss of 34959224845.264862, Test MSE of 34959224694.456573
Epoch 52: training loss 25853575228.235
Test Loss of 30836225781.281517, Test MSE of 30836225524.495945
Epoch 53: training loss 24689804856.471
Test Loss of 31282981762.694424, Test MSE of 31282981786.078320
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 27160216892.98717, 'MSE - std': 4299732322.24357, 'R2 - mean': 0.8065407793652861, 'R2 - std': 0.019916923883375254} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005355 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111160681.412
Test Loss of 410764862474.898682, Test MSE of 410764862841.493347
Epoch 2: training loss 430090825968.941
Test Loss of 410747111650.502563, Test MSE of 410747112069.645630
Epoch 3: training loss 430063489505.882
Test Loss of 410723543467.416931, Test MSE of 410723550547.171997
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077572276.706
Test Loss of 410729301037.016174, Test MSE of 410729303697.076721
Epoch 2: training loss 430066681494.588
Test Loss of 410730236524.749634, Test MSE of 410730243032.787720
Epoch 3: training loss 430066321287.529
Test Loss of 410729880868.368347, Test MSE of 410729877253.685669
Epoch 4: training loss 424643222347.294
Test Loss of 393855518832.777405, Test MSE of 393855520880.614990
Epoch 5: training loss 387942884050.824
Test Loss of 338538921695.896362, Test MSE of 338538920445.667419
Epoch 6: training loss 319767954612.706
Test Loss of 261709528966.693207, Test MSE of 261709521487.320618
Epoch 7: training loss 230579366008.471
Test Loss of 168218732774.293396, Test MSE of 168218730867.062164
Epoch 8: training loss 167297825249.882
Test Loss of 127295885469.319763, Test MSE of 127295884485.592468
Epoch 9: training loss 144721547023.059
Test Loss of 115093472778.661728, Test MSE of 115093473743.142868
Epoch 10: training loss 140019321584.941
Test Loss of 110870358473.743637, Test MSE of 110870357846.184875
Epoch 11: training loss 135885763734.588
Test Loss of 109043952286.030548, Test MSE of 109043954618.421158
Epoch 12: training loss 134554656256.000
Test Loss of 106328265130.469223, Test MSE of 106328265660.742004
Epoch 13: training loss 130417862806.588
Test Loss of 103883619085.860245, Test MSE of 103883620241.115982
Epoch 14: training loss 127158481889.882
Test Loss of 100809178863.059692, Test MSE of 100809179270.478485
Epoch 15: training loss 123761973338.353
Test Loss of 98236822574.911621, Test MSE of 98236822229.740189
Epoch 16: training loss 120852883154.824
Test Loss of 96087519453.763992, Test MSE of 96087519357.382523
Epoch 17: training loss 117191526430.118
Test Loss of 93746684533.279037, Test MSE of 93746683910.658249
Epoch 18: training loss 114090953065.412
Test Loss of 91683612864.385010, Test MSE of 91683612168.406357
Epoch 19: training loss 111579628333.176
Test Loss of 88698502637.282745, Test MSE of 88698503556.726685
Epoch 20: training loss 108244713321.412
Test Loss of 85808537941.175385, Test MSE of 85808537710.311310
Epoch 21: training loss 104783528598.588
Test Loss of 83270760846.985657, Test MSE of 83270760185.439468
Epoch 22: training loss 101936319216.941
Test Loss of 80482270454.878296, Test MSE of 80482272166.314606
Epoch 23: training loss 98129167269.647
Test Loss of 78318065752.136978, Test MSE of 78318065850.956680
Epoch 24: training loss 94362969569.882
Test Loss of 75014564240.881073, Test MSE of 75014565639.670776
Epoch 25: training loss 90422931983.059
Test Loss of 72709614181.167984, Test MSE of 72709613948.575012
Epoch 26: training loss 88612926464.000
Test Loss of 70458462721.658493, Test MSE of 70458461749.182114
Epoch 27: training loss 84685928613.647
Test Loss of 67235398924.675613, Test MSE of 67235398190.977097
Epoch 28: training loss 82421294366.118
Test Loss of 64613263442.450714, Test MSE of 64613262145.956833
Epoch 29: training loss 79296868246.588
Test Loss of 63311025941.915779, Test MSE of 63311025857.147125
Epoch 30: training loss 76641815070.118
Test Loss of 60872752230.826469, Test MSE of 60872751960.734642
Epoch 31: training loss 73535139267.765
Test Loss of 57956605061.153168, Test MSE of 57956605400.281181
Epoch 32: training loss 71241487525.647
Test Loss of 58475335795.620544, Test MSE of 58475335613.887093
Epoch 33: training loss 67940197195.294
Test Loss of 56130027554.117538, Test MSE of 56130028018.767090
Epoch 34: training loss 65332290439.529
Test Loss of 53187415825.177231, Test MSE of 53187415723.440239
Epoch 35: training loss 62906667550.118
Test Loss of 50910813161.728828, Test MSE of 50910813565.086143
Epoch 36: training loss 60270371026.824
Test Loss of 48394146508.468300, Test MSE of 48394146662.623909
Epoch 37: training loss 57785305825.882
Test Loss of 45888086611.161499, Test MSE of 45888087233.578659
Epoch 38: training loss 55139579467.294
Test Loss of 46227260654.348915, Test MSE of 46227260755.067406
Epoch 39: training loss 53071115354.353
Test Loss of 44424632211.013420, Test MSE of 44424632000.602135
Epoch 40: training loss 50611260491.294
Test Loss of 40159819634.791298, Test MSE of 40159818958.617325
Epoch 41: training loss 48272878080.000
Test Loss of 39888910716.031464, Test MSE of 39888910543.256378
Epoch 42: training loss 45995244634.353
Test Loss of 37524312283.868576, Test MSE of 37524313540.321144
Epoch 43: training loss 43619785396.706
Test Loss of 37798599494.722816, Test MSE of 37798599449.182602
Epoch 44: training loss 42085845428.706
Test Loss of 36511048002.221191, Test MSE of 36511047955.184082
Epoch 45: training loss 40706056606.118
Test Loss of 32822876382.711708, Test MSE of 32822877002.194962
Epoch 46: training loss 38402612434.824
Test Loss of 34000810080.666359, Test MSE of 34000809945.033447
Epoch 47: training loss 36389301451.294
Test Loss of 31025272000.858860, Test MSE of 31025271274.249420
Epoch 48: training loss 35206952207.059
Test Loss of 30974448315.409534, Test MSE of 30974448232.503109
Epoch 49: training loss 33502668491.294
Test Loss of 30576028462.082371, Test MSE of 30576028361.260227
Epoch 50: training loss 31914435945.412
Test Loss of 29068329263.267006, Test MSE of 29068329441.396664
Epoch 51: training loss 30575706593.882
Test Loss of 28154749519.370663, Test MSE of 28154749830.575420
Epoch 52: training loss 29261999051.294
Test Loss of 26013813100.868118, Test MSE of 26013813472.408730
Epoch 53: training loss 28008864399.059
Test Loss of 23680883539.043037, Test MSE of 23680883574.463360
Epoch 54: training loss 27028884438.588
Test Loss of 25416442248.351688, Test MSE of 25416442022.196171
Epoch 55: training loss 26326345822.118
Test Loss of 23587522935.292919, Test MSE of 23587522910.435417
Epoch 56: training loss 24906953592.471
Test Loss of 24870137443.746414, Test MSE of 24870137445.333271
Epoch 57: training loss 23872478561.882
Test Loss of 24129214870.567329, Test MSE of 24129214924.317726
Epoch 58: training loss 23118291862.588
Test Loss of 23164553697.436371, Test MSE of 23164553355.670856
Epoch 59: training loss 22414122569.412
Test Loss of 23320283657.240166, Test MSE of 23320283259.716991
Epoch 60: training loss 21156884329.412
Test Loss of 22716604579.006016, Test MSE of 22716605139.733761
Epoch 61: training loss 20671217027.765
Test Loss of 21186168665.203148, Test MSE of 21186168564.027988
Epoch 62: training loss 19742577671.529
Test Loss of 21907526000.658955, Test MSE of 21907526224.813984
Epoch 63: training loss 19672192986.353
Test Loss of 23741232565.367886, Test MSE of 23741232269.218300
Epoch 64: training loss 18667491429.647
Test Loss of 21292377153.865803, Test MSE of 21292376852.286572
Epoch 65: training loss 18354595459.765
Test Loss of 20876706379.579823, Test MSE of 20876706448.497978
Epoch 66: training loss 17604512668.235
Test Loss of 22082331568.866264, Test MSE of 22082331422.634911
Epoch 67: training loss 17314648990.118
Test Loss of 20326703387.365108, Test MSE of 20326703008.242531
Epoch 68: training loss 16929662870.588
Test Loss of 19930634918.559925, Test MSE of 19930634574.043846
Epoch 69: training loss 16268482955.294
Test Loss of 18601189066.099030, Test MSE of 18601189459.627838
Epoch 70: training loss 16038088391.529
Test Loss of 19933497150.667282, Test MSE of 19933497374.928638
Epoch 71: training loss 15211761603.765
Test Loss of 18906995575.055992, Test MSE of 18906995504.317928
Epoch 72: training loss 15097178447.059
Test Loss of 19289119828.819992, Test MSE of 19289120088.486938
Epoch 73: training loss 14578475881.412
Test Loss of 18369253001.654789, Test MSE of 18369253420.317692
Epoch 74: training loss 14246138352.941
Test Loss of 19303806377.995373, Test MSE of 19303806442.562920
Epoch 75: training loss 13876025904.941
Test Loss of 18740315439.740860, Test MSE of 18740315855.222130
Epoch 76: training loss 13543163290.353
Test Loss of 22085842902.300785, Test MSE of 22085843004.559658
Epoch 77: training loss 13076523847.529
Test Loss of 19457266938.669136, Test MSE of 19457266804.824905
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25234479370.946606, 'MSE - std': 4999116776.351038, 'R2 - mean': 0.8147578436974233, 'R2 - std': 0.022362319455896475} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005439 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424044192105.412
Test Loss of 431612624623.059692, Test MSE of 431612631328.260376
Epoch 2: training loss 424024837903.059
Test Loss of 431592493016.196228, Test MSE of 431592496200.371704
Epoch 3: training loss 423997543484.235
Test Loss of 431565006850.843140, Test MSE of 431565002729.506470
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012551469.176
Test Loss of 431567560450.013855, Test MSE of 431567566819.602356
Epoch 2: training loss 423999220193.882
Test Loss of 431570298344.544189, Test MSE of 431570299462.765869
Epoch 3: training loss 423998652054.588
Test Loss of 431570234734.289673, Test MSE of 431570241311.619324
Epoch 4: training loss 419032638403.765
Test Loss of 415173730793.018066, Test MSE of 415173732519.428162
Epoch 5: training loss 383719954552.471
Test Loss of 360606001776.540466, Test MSE of 360606002259.381897
Epoch 6: training loss 316630724969.412
Test Loss of 282223382419.961121, Test MSE of 282223382039.704651
Epoch 7: training loss 227105174648.471
Test Loss of 184460237570.487732, Test MSE of 184460234199.848694
Epoch 8: training loss 162497723512.471
Test Loss of 140786351657.936127, Test MSE of 140786351759.730042
Epoch 9: training loss 141992688865.882
Test Loss of 127672338461.378998, Test MSE of 127672337923.656021
Epoch 10: training loss 136548009758.118
Test Loss of 122188107349.530777, Test MSE of 122188107869.940186
Epoch 11: training loss 131878817219.765
Test Loss of 119300317093.019897, Test MSE of 119300318804.811752
Epoch 12: training loss 129728983792.941
Test Loss of 116537967403.239243, Test MSE of 116537967209.519470
Epoch 13: training loss 126706088960.000
Test Loss of 114286723246.378525, Test MSE of 114286721833.870392
Epoch 14: training loss 123561298447.059
Test Loss of 111180516089.010651, Test MSE of 111180516367.964203
Epoch 15: training loss 121648573394.824
Test Loss of 106616968219.957428, Test MSE of 106616965849.026062
Epoch 16: training loss 117953914277.647
Test Loss of 103629755040.873672, Test MSE of 103629754505.939209
Epoch 17: training loss 114476466522.353
Test Loss of 100270392766.844986, Test MSE of 100270393820.590729
Epoch 18: training loss 111835198915.765
Test Loss of 96972242879.555756, Test MSE of 96972244669.816116
Epoch 19: training loss 107441427900.235
Test Loss of 94062126083.316986, Test MSE of 94062126686.835876
Epoch 20: training loss 105728717462.588
Test Loss of 91232294086.545120, Test MSE of 91232294012.755722
Epoch 21: training loss 101698223525.647
Test Loss of 88910773058.931976, Test MSE of 88910771693.530792
Epoch 22: training loss 98668408771.765
Test Loss of 86184271762.065704, Test MSE of 86184271411.570923
Epoch 23: training loss 94386281321.412
Test Loss of 81768695204.309113, Test MSE of 81768695328.500275
Epoch 24: training loss 92173353682.824
Test Loss of 80510286651.350296, Test MSE of 80510286843.628586
Epoch 25: training loss 89595003376.941
Test Loss of 75655193436.520126, Test MSE of 75655193258.032654
Epoch 26: training loss 86360870746.353
Test Loss of 74521204968.188797, Test MSE of 74521204160.127289
Epoch 27: training loss 83211418654.118
Test Loss of 70530280033.377136, Test MSE of 70530278631.471573
Epoch 28: training loss 79262327695.059
Test Loss of 68852113057.347519, Test MSE of 68852113282.159027
Epoch 29: training loss 77456349138.824
Test Loss of 64741763084.320221, Test MSE of 64741762338.192833
Epoch 30: training loss 74662867591.529
Test Loss of 63061128373.486351, Test MSE of 63061127993.022682
Epoch 31: training loss 71650886392.471
Test Loss of 62293261492.538643, Test MSE of 62293261557.634056
Epoch 32: training loss 69173193682.824
Test Loss of 58996728727.278114, Test MSE of 58996730510.440536
Epoch 33: training loss 66957105844.706
Test Loss of 57937458782.060158, Test MSE of 57937457594.205292
Epoch 34: training loss 64192509334.588
Test Loss of 53576002346.291534, Test MSE of 53576002382.963356
Epoch 35: training loss 60764711137.882
Test Loss of 51457403222.123093, Test MSE of 51457403498.043915
Epoch 36: training loss 58578487446.588
Test Loss of 47812033993.269783, Test MSE of 47812034353.979927
Epoch 37: training loss 56216158223.059
Test Loss of 46090003394.872742, Test MSE of 46090003429.323059
Epoch 38: training loss 53930979591.529
Test Loss of 45079446637.460434, Test MSE of 45079447051.354477
Epoch 39: training loss 50836652958.118
Test Loss of 46573450567.433594, Test MSE of 46573449975.915245
Epoch 40: training loss 48817387271.529
Test Loss of 43440511210.084221, Test MSE of 43440511447.702713
Epoch 41: training loss 47401920240.941
Test Loss of 42496470025.950951, Test MSE of 42496471074.560776
Epoch 42: training loss 44930110351.059
Test Loss of 35529951675.528000, Test MSE of 35529951918.259407
Epoch 43: training loss 43786098213.647
Test Loss of 36267971608.166588, Test MSE of 36267971863.512787
Epoch 44: training loss 41687474409.412
Test Loss of 34144643543.011570, Test MSE of 34144643647.186337
Epoch 45: training loss 39392227937.882
Test Loss of 37305397473.554832, Test MSE of 37305397246.836517
Epoch 46: training loss 38268355011.765
Test Loss of 31237252364.675613, Test MSE of 31237252767.876301
Epoch 47: training loss 36473276468.706
Test Loss of 30152713635.361408, Test MSE of 30152713949.354916
Epoch 48: training loss 35109183232.000
Test Loss of 28997209989.745487, Test MSE of 28997210161.673336
Epoch 49: training loss 33519109940.706
Test Loss of 25334200011.046738, Test MSE of 25334200014.697014
Epoch 50: training loss 31698468818.824
Test Loss of 27394514200.995834, Test MSE of 27394514043.361485
Epoch 51: training loss 30635612001.882
Test Loss of 27730047527.566868, Test MSE of 27730047096.535706
Epoch 52: training loss 28818627147.294
Test Loss of 25813008583.018974, Test MSE of 25813007838.461033
Epoch 53: training loss 27860299504.941
Test Loss of 24388037876.035168, Test MSE of 24388038094.888672
Epoch 54: training loss 26782010443.294
Test Loss of 23399509255.937065, Test MSE of 23399509176.077545
Epoch 55: training loss 25872984956.235
Test Loss of 23712968915.339195, Test MSE of 23712968748.990238
Epoch 56: training loss 24719445138.824
Test Loss of 23278222548.760757, Test MSE of 23278222547.814899
Epoch 57: training loss 24149349010.824
Test Loss of 24005509644.083294, Test MSE of 24005510066.935352
Epoch 58: training loss 23305592545.882
Test Loss of 20729566466.250809, Test MSE of 20729566238.485580
Epoch 59: training loss 22132948318.118
Test Loss of 21708505860.383156, Test MSE of 21708506234.096439
Epoch 60: training loss 21437352722.824
Test Loss of 20882187457.806572, Test MSE of 20882187133.156677
Epoch 61: training loss 20640757360.941
Test Loss of 21224145822.859787, Test MSE of 21224145722.587299
Epoch 62: training loss 20331507008.000
Test Loss of 20349160169.847294, Test MSE of 20349160199.978386
Epoch 63: training loss 19254986503.529
Test Loss of 20363593033.329014, Test MSE of 20363593168.772045
Epoch 64: training loss 18588288316.235
Test Loss of 19990456117.190189, Test MSE of 19990455968.270126
Epoch 65: training loss 18437206663.529
Test Loss of 20000150347.461361, Test MSE of 20000150604.885456
Epoch 66: training loss 17818790181.647
Test Loss of 19067305363.250347, Test MSE of 19067305355.092411
Epoch 67: training loss 17111996299.294
Test Loss of 19354852583.714947, Test MSE of 19354852857.607319
Epoch 68: training loss 16621164555.294
Test Loss of 19575604792.151783, Test MSE of 19575605071.618011
Epoch 69: training loss 16685427369.412
Test Loss of 19342776566.404442, Test MSE of 19342776914.613674
Epoch 70: training loss 16182540355.765
Test Loss of 19155075390.430355, Test MSE of 19155075269.524235
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24018598550.662132, 'MSE - std': 5089832955.158558, 'R2 - mean': 0.8231961123003702, 'R2 - std': 0.02617013896065456} 
 

Saving model.....
Results After CV: {'MSE - mean': 24018598550.662132, 'MSE - std': 5089832955.158558, 'R2 - mean': 0.8231961123003702, 'R2 - std': 0.02617013896065456}
Train time: 98.90194101759953
Inference time: 0.07128888379957062
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 22 finished with value: 24018598550.662132 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005396 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525158189.176
Test Loss of 418111618147.249573, Test MSE of 418111612751.273682
Epoch 2: training loss 427503531188.706
Test Loss of 418092937491.719666, Test MSE of 418092928654.545410
Epoch 3: training loss 427475412630.588
Test Loss of 418068260656.025879, Test MSE of 418068270128.911316
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495748788.706
Test Loss of 418072239788.798523, Test MSE of 418072236362.825378
Epoch 2: training loss 427481128598.588
Test Loss of 418072980233.415710, Test MSE of 418072978865.425537
Epoch 3: training loss 423755266288.941
Test Loss of 406373166849.125122, Test MSE of 406373161422.779968
Epoch 4: training loss 397371438742.588
Test Loss of 365275061766.040222, Test MSE of 365275060898.868042
Epoch 5: training loss 332815336387.765
Test Loss of 281553040454.351135, Test MSE of 281553037948.871033
Epoch 6: training loss 251874902377.412
Test Loss of 204744669670.536194, Test MSE of 204744673759.156342
Epoch 7: training loss 183971370496.000
Test Loss of 145382996746.363159, Test MSE of 145383001276.609192
Epoch 8: training loss 149126086264.471
Test Loss of 124012048629.399948, Test MSE of 124012047968.243546
Epoch 9: training loss 137739282281.412
Test Loss of 117886340129.872772, Test MSE of 117886339062.877869
Epoch 10: training loss 135533504451.765
Test Loss of 115428980068.493179, Test MSE of 115428980147.589951
Epoch 11: training loss 133067668781.176
Test Loss of 112638749582.064301, Test MSE of 112638750548.432022
Epoch 12: training loss 130539901801.412
Test Loss of 110106167165.720108, Test MSE of 110106168469.614883
Epoch 13: training loss 127118006648.471
Test Loss of 107620937192.668060, Test MSE of 107620940174.100906
Epoch 14: training loss 123849120828.235
Test Loss of 105459539443.090439, Test MSE of 105459539881.354706
Epoch 15: training loss 121201445526.588
Test Loss of 102494205303.442978, Test MSE of 102494206224.909821
Epoch 16: training loss 117498325737.412
Test Loss of 100806084405.710846, Test MSE of 100806082107.999695
Epoch 17: training loss 114555794597.647
Test Loss of 96959892367.959290, Test MSE of 96959891652.815552
Epoch 18: training loss 111056812062.118
Test Loss of 92974853765.951416, Test MSE of 92974855749.193146
Epoch 19: training loss 108966856086.588
Test Loss of 92267519753.652557, Test MSE of 92267518766.614471
Epoch 20: training loss 105368874390.588
Test Loss of 89101167627.133011, Test MSE of 89101167002.156082
Epoch 21: training loss 102649157812.706
Test Loss of 86680182338.679626, Test MSE of 86680181936.369934
Epoch 22: training loss 99078196660.706
Test Loss of 84830374472.127686, Test MSE of 84830373967.458023
Epoch 23: training loss 95528255006.118
Test Loss of 81532881076.496872, Test MSE of 81532879935.042282
Epoch 24: training loss 92424811384.471
Test Loss of 78161514290.394638, Test MSE of 78161513485.724274
Epoch 25: training loss 89491997153.882
Test Loss of 75678444611.982422, Test MSE of 75678446330.170547
Epoch 26: training loss 86926645052.235
Test Loss of 74385099302.018036, Test MSE of 74385099317.611984
Epoch 27: training loss 84154051011.765
Test Loss of 71009179757.435120, Test MSE of 71009180925.525040
Epoch 28: training loss 80132736195.765
Test Loss of 68110789345.621094, Test MSE of 68110789361.589737
Epoch 29: training loss 77154524973.176
Test Loss of 62900708118.206802, Test MSE of 62900707410.382698
Epoch 30: training loss 75079668833.882
Test Loss of 62230727427.730743, Test MSE of 62230727517.596100
Epoch 31: training loss 72341875440.941
Test Loss of 58741145365.022438, Test MSE of 58741146311.309105
Epoch 32: training loss 68818257972.706
Test Loss of 59300316693.673836, Test MSE of 59300316604.518700
Epoch 33: training loss 65600090947.765
Test Loss of 56237798081.880173, Test MSE of 56237798514.819107
Epoch 34: training loss 63254034198.588
Test Loss of 53569985124.552391, Test MSE of 53569984543.175995
Epoch 35: training loss 61591296037.647
Test Loss of 53323890877.498032, Test MSE of 53323891107.887695
Epoch 36: training loss 57543203207.529
Test Loss of 47950727794.764748, Test MSE of 47950728350.836021
Epoch 37: training loss 55569967555.765
Test Loss of 48350736238.086517, Test MSE of 48350736790.012466
Epoch 38: training loss 53728950814.118
Test Loss of 43989573401.759888, Test MSE of 43989573716.906136
Epoch 39: training loss 51540775480.471
Test Loss of 44672833996.006477, Test MSE of 44672832837.289085
Epoch 40: training loss 48588016858.353
Test Loss of 41628252001.295395, Test MSE of 41628252332.422943
Epoch 41: training loss 46975716720.941
Test Loss of 41889046031.515152, Test MSE of 41889045971.589607
Epoch 42: training loss 44432693323.294
Test Loss of 40015506886.558411, Test MSE of 40015508239.392380
Epoch 43: training loss 42113364336.941
Test Loss of 37765396556.272957, Test MSE of 37765396191.694908
Epoch 44: training loss 40168837925.647
Test Loss of 37892064476.054588, Test MSE of 37892064393.325600
Epoch 45: training loss 38624914021.647
Test Loss of 31318946900.563499, Test MSE of 31318947141.764557
Epoch 46: training loss 36283710942.118
Test Loss of 33450145907.120056, Test MSE of 33450146161.821442
Epoch 47: training loss 35298460769.882
Test Loss of 32943379413.599815, Test MSE of 32943378970.435101
Epoch 48: training loss 33713447808.000
Test Loss of 26911719051.636364, Test MSE of 26911719256.163761
Epoch 49: training loss 32207169287.529
Test Loss of 27792127721.911636, Test MSE of 27792127804.466827
Epoch 50: training loss 30434093379.765
Test Loss of 28099958654.667591, Test MSE of 28099959256.657772
Epoch 51: training loss 29036760884.706
Test Loss of 25755439776.244274, Test MSE of 25755438952.115650
Epoch 52: training loss 28199799830.588
Test Loss of 25487666532.966923, Test MSE of 25487666465.247318
Epoch 53: training loss 27033275267.765
Test Loss of 26032459352.471893, Test MSE of 26032459264.274921
Epoch 54: training loss 26151300649.412
Test Loss of 26564325769.208420, Test MSE of 26564326047.761620
Epoch 55: training loss 25074031416.471
Test Loss of 24477034214.121674, Test MSE of 24477034172.901535
Epoch 56: training loss 23776668284.235
Test Loss of 22232207758.893360, Test MSE of 22232207719.762001
Epoch 57: training loss 23002083881.412
Test Loss of 25091934641.476753, Test MSE of 25091935345.068752
Epoch 58: training loss 22061664892.235
Test Loss of 22447346272.762432, Test MSE of 22447346174.944271
Epoch 59: training loss 21544951838.118
Test Loss of 23590094939.906548, Test MSE of 23590094931.664318
Epoch 60: training loss 20315915132.235
Test Loss of 21569021220.537590, Test MSE of 21569021429.882149
Epoch 61: training loss 19615944143.059
Test Loss of 21378126825.497108, Test MSE of 21378126517.581852
Epoch 62: training loss 19240493970.824
Test Loss of 23153710760.534813, Test MSE of 23153710776.422096
Epoch 63: training loss 18431113246.118
Test Loss of 20974712185.811707, Test MSE of 20974711833.453621
Epoch 64: training loss 18052708890.353
Test Loss of 21095318497.917187, Test MSE of 21095318828.338291
Epoch 65: training loss 17293075312.941
Test Loss of 20473117196.435810, Test MSE of 20473117123.830338
Epoch 66: training loss 16682771008.000
Test Loss of 20294281725.749710, Test MSE of 20294281704.392456
Epoch 67: training loss 16241502893.176
Test Loss of 19051480778.170715, Test MSE of 19051480901.618122
Epoch 68: training loss 15934053120.000
Test Loss of 18802269197.027988, Test MSE of 18802269244.759621
Epoch 69: training loss 15465029842.824
Test Loss of 19345441309.016888, Test MSE of 19345441290.119297
Epoch 70: training loss 14878015495.529
Test Loss of 19664291319.354153, Test MSE of 19664291711.558357
Epoch 71: training loss 14412580739.765
Test Loss of 19930891736.086975, Test MSE of 19930891661.728828
Epoch 72: training loss 14205938932.706
Test Loss of 20359649126.032848, Test MSE of 20359649024.929989
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20359649024.92999, 'MSE - std': 0.0, 'R2 - mean': 0.841457204037754, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003570 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917404039.529
Test Loss of 424558596358.454773, Test MSE of 424558595717.725891
Epoch 2: training loss 427896515162.353
Test Loss of 424542137794.768433, Test MSE of 424542140700.787659
Epoch 3: training loss 427869181590.588
Test Loss of 424519896607.622498, Test MSE of 424519898528.407288
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890959540.706
Test Loss of 424526011330.176270, Test MSE of 424526014398.550720
Epoch 2: training loss 427879367860.706
Test Loss of 424526409791.481812, Test MSE of 424526409720.707825
Epoch 3: training loss 423908798223.059
Test Loss of 412446705749.747864, Test MSE of 412446706833.998901
Epoch 4: training loss 396920965601.882
Test Loss of 371303259017.326843, Test MSE of 371303261126.062683
Epoch 5: training loss 332426157718.588
Test Loss of 288940155181.301880, Test MSE of 288940159456.540588
Epoch 6: training loss 249488123181.176
Test Loss of 213305402202.426086, Test MSE of 213305401211.904694
Epoch 7: training loss 182340288391.529
Test Loss of 154855101392.862366, Test MSE of 154855100511.173950
Epoch 8: training loss 146107667184.941
Test Loss of 135832038020.767059, Test MSE of 135832036365.443848
Epoch 9: training loss 135131481600.000
Test Loss of 129474023468.532043, Test MSE of 129474024938.843094
Epoch 10: training loss 132852546319.059
Test Loss of 127165993739.784409, Test MSE of 127165995623.170517
Epoch 11: training loss 128623283109.647
Test Loss of 124213953696.362717, Test MSE of 124213953218.862015
Epoch 12: training loss 128244374106.353
Test Loss of 121770936263.624329, Test MSE of 121770936686.087601
Epoch 13: training loss 124173389432.471
Test Loss of 118785850200.057373, Test MSE of 118785849690.203094
Epoch 14: training loss 123273087457.882
Test Loss of 115497598307.071945, Test MSE of 115497594937.690781
Epoch 15: training loss 118085612122.353
Test Loss of 113065800262.706451, Test MSE of 113065802883.462906
Epoch 16: training loss 115514319510.588
Test Loss of 110766075326.978485, Test MSE of 110766075969.410446
Epoch 17: training loss 112407230674.824
Test Loss of 106646500134.314133, Test MSE of 106646501063.450897
Epoch 18: training loss 109886854505.412
Test Loss of 103875523508.437653, Test MSE of 103875522696.791031
Epoch 19: training loss 105445277334.588
Test Loss of 101959411035.018280, Test MSE of 101959409005.728973
Epoch 20: training loss 103054140566.588
Test Loss of 98904036662.066162, Test MSE of 98904035087.352295
Epoch 21: training loss 99478696990.118
Test Loss of 95569073261.198242, Test MSE of 95569072422.045471
Epoch 22: training loss 96492075700.706
Test Loss of 91940136514.205872, Test MSE of 91940135246.890289
Epoch 23: training loss 92914525515.294
Test Loss of 88116729889.635895, Test MSE of 88116731779.154312
Epoch 24: training loss 89746176557.176
Test Loss of 85607131179.110809, Test MSE of 85607130582.783539
Epoch 25: training loss 86617165673.412
Test Loss of 82102820955.906540, Test MSE of 82102821863.810455
Epoch 26: training loss 83435980348.235
Test Loss of 81474332627.941711, Test MSE of 81474333782.343506
Epoch 27: training loss 79651777551.059
Test Loss of 77147354549.977325, Test MSE of 77147355399.417526
Epoch 28: training loss 77438476453.647
Test Loss of 73403223402.651855, Test MSE of 73403224445.170135
Epoch 29: training loss 74699003316.706
Test Loss of 74952391812.411758, Test MSE of 74952391283.575302
Epoch 30: training loss 72289331410.824
Test Loss of 67022468257.310204, Test MSE of 67022468286.337746
Epoch 31: training loss 68348731542.588
Test Loss of 65056173440.444138, Test MSE of 65056174329.841347
Epoch 32: training loss 65578981692.235
Test Loss of 63964509349.100159, Test MSE of 63964510321.663940
Epoch 33: training loss 63252995885.176
Test Loss of 60178904008.334953, Test MSE of 60178905247.734581
Epoch 34: training loss 60870992956.235
Test Loss of 60415990528.888275, Test MSE of 60415989249.280174
Epoch 35: training loss 57622997594.353
Test Loss of 56365120789.851494, Test MSE of 56365119748.994705
Epoch 36: training loss 54370859429.647
Test Loss of 56621804278.465881, Test MSE of 56621803830.994980
Epoch 37: training loss 52956978032.941
Test Loss of 52979672134.351143, Test MSE of 52979671159.803001
Epoch 38: training loss 50014071589.647
Test Loss of 51661123063.827896, Test MSE of 51661124040.663818
Epoch 39: training loss 47339123388.235
Test Loss of 46369279233.954201, Test MSE of 46369278203.743668
Epoch 40: training loss 45798131177.412
Test Loss of 45868303661.065002, Test MSE of 45868303083.941086
Epoch 41: training loss 43678478426.353
Test Loss of 46136913548.110107, Test MSE of 46136914185.420181
Epoch 42: training loss 40951103683.765
Test Loss of 43437166006.924820, Test MSE of 43437166064.458900
Epoch 43: training loss 39017017584.941
Test Loss of 36464902503.572517, Test MSE of 36464901929.972260
Epoch 44: training loss 37080943209.412
Test Loss of 36838504642.709229, Test MSE of 36838504243.014420
Epoch 45: training loss 35403621300.706
Test Loss of 38115353057.325005, Test MSE of 38115354037.016960
Epoch 46: training loss 33707947663.059
Test Loss of 37782774803.186676, Test MSE of 37782774198.639267
Epoch 47: training loss 32178417558.588
Test Loss of 33797196689.380524, Test MSE of 33797195093.896152
Epoch 48: training loss 30945058944.000
Test Loss of 32184248455.964840, Test MSE of 32184248261.520016
Epoch 49: training loss 29141604841.412
Test Loss of 33314265178.011566, Test MSE of 33314265382.261204
Epoch 50: training loss 27805991936.000
Test Loss of 32982590388.437660, Test MSE of 32982590758.285225
Epoch 51: training loss 26189860359.529
Test Loss of 30152218353.491558, Test MSE of 30152219044.410404
Epoch 52: training loss 24902033362.824
Test Loss of 29864271400.623642, Test MSE of 29864271606.419357
Epoch 53: training loss 24266613383.529
Test Loss of 24743014914.487164, Test MSE of 24743014757.978371
Epoch 54: training loss 22978255623.529
Test Loss of 28509889125.026138, Test MSE of 28509889263.185577
Epoch 55: training loss 22443834917.647
Test Loss of 28503890234.092991, Test MSE of 28503889824.713547
Epoch 56: training loss 21201141007.059
Test Loss of 30384117230.826740, Test MSE of 30384116939.392948
Epoch 57: training loss 20289440869.647
Test Loss of 28138043590.025444, Test MSE of 28138044185.399143
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24248846605.164566, 'MSE - std': 3889197580.234577, 'R2 - mean': 0.8202851250999444, 'R2 - std': 0.02117207893780959} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005563 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927453515.294
Test Loss of 447259294792.719849, Test MSE of 447259295870.627136
Epoch 2: training loss 421906472839.529
Test Loss of 447239851915.458679, Test MSE of 447239863033.113953
Epoch 3: training loss 421878921095.529
Test Loss of 447214623981.583130, Test MSE of 447214624758.300354
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898010021.647
Test Loss of 447223106307.730713, Test MSE of 447223105528.795105
Epoch 2: training loss 421886821556.706
Test Loss of 447224136206.804504, Test MSE of 447224136944.282654
Epoch 3: training loss 418147181266.824
Test Loss of 435251874950.306702, Test MSE of 435251875807.402954
Epoch 4: training loss 391742386537.412
Test Loss of 393620869484.783691, Test MSE of 393620871226.226379
Epoch 5: training loss 327790471649.882
Test Loss of 309108812178.209595, Test MSE of 309108819203.958618
Epoch 6: training loss 246596698473.412
Test Loss of 230768168043.777008, Test MSE of 230768168931.942932
Epoch 7: training loss 179093604231.529
Test Loss of 169679221189.847778, Test MSE of 169679222268.578003
Epoch 8: training loss 144907866533.647
Test Loss of 147321160002.146667, Test MSE of 147321165035.857086
Epoch 9: training loss 135521653609.412
Test Loss of 140780146150.773071, Test MSE of 140780147465.725800
Epoch 10: training loss 130743522093.176
Test Loss of 137652170375.846405, Test MSE of 137652175678.475006
Epoch 11: training loss 129800368007.529
Test Loss of 135271706312.275742, Test MSE of 135271705849.743347
Epoch 12: training loss 126137825280.000
Test Loss of 132413976726.650940, Test MSE of 132413978960.934158
Epoch 13: training loss 123361512327.529
Test Loss of 129055522823.579926, Test MSE of 129055521704.526688
Epoch 14: training loss 120193267200.000
Test Loss of 126912636166.691650, Test MSE of 126912634856.387573
Epoch 15: training loss 118152513867.294
Test Loss of 124184440126.119827, Test MSE of 124184441976.268509
Epoch 16: training loss 115341220532.706
Test Loss of 120663724082.216980, Test MSE of 120663725567.101349
Epoch 17: training loss 111459541383.529
Test Loss of 117586750755.590103, Test MSE of 117586751757.951813
Epoch 18: training loss 108419944658.824
Test Loss of 114874599759.885269, Test MSE of 114874600151.534393
Epoch 19: training loss 105799146902.588
Test Loss of 111881593925.403656, Test MSE of 111881595196.684464
Epoch 20: training loss 103743819083.294
Test Loss of 110743560622.160538, Test MSE of 110743562345.834091
Epoch 21: training loss 100482200877.176
Test Loss of 106707163855.618790, Test MSE of 106707164653.157639
Epoch 22: training loss 96509261342.118
Test Loss of 102260260450.894287, Test MSE of 102260263082.817871
Epoch 23: training loss 93971849788.235
Test Loss of 99516891118.945175, Test MSE of 99516891481.216782
Epoch 24: training loss 90553144591.059
Test Loss of 97539783911.898224, Test MSE of 97539783314.662720
Epoch 25: training loss 87709528907.294
Test Loss of 92266863002.736984, Test MSE of 92266863593.975281
Epoch 26: training loss 84826617027.765
Test Loss of 90469971575.265320, Test MSE of 90469971806.733841
Epoch 27: training loss 81998434966.588
Test Loss of 85684995522.294708, Test MSE of 85684997362.137375
Epoch 28: training loss 78812065340.235
Test Loss of 82173819305.896835, Test MSE of 82173819806.997971
Epoch 29: training loss 75076303676.235
Test Loss of 77811032062.105026, Test MSE of 77811033572.169464
Epoch 30: training loss 72577542490.353
Test Loss of 75936388189.090912, Test MSE of 75936387105.809708
Epoch 31: training loss 69551715840.000
Test Loss of 75006852642.464951, Test MSE of 75006853177.552307
Epoch 32: training loss 66939125323.294
Test Loss of 74406498140.084198, Test MSE of 74406499179.720184
Epoch 33: training loss 63999204314.353
Test Loss of 70259170767.085815, Test MSE of 70259171006.786423
Epoch 34: training loss 61850055936.000
Test Loss of 66616447986.972008, Test MSE of 66616449217.138618
Epoch 35: training loss 59199158031.059
Test Loss of 65027058814.726807, Test MSE of 65027058935.399956
Epoch 36: training loss 57174211448.471
Test Loss of 61732301497.826508, Test MSE of 61732301655.813553
Epoch 37: training loss 55042069910.588
Test Loss of 57769273897.808006, Test MSE of 57769274432.132736
Epoch 38: training loss 52314825110.588
Test Loss of 57001162831.115433, Test MSE of 57001163330.453545
Epoch 39: training loss 49565011719.529
Test Loss of 51416432211.734444, Test MSE of 51416432440.470512
Epoch 40: training loss 47261402917.647
Test Loss of 50746212279.990746, Test MSE of 50746212543.352989
Epoch 41: training loss 45412830614.588
Test Loss of 54024295992.730972, Test MSE of 54024295858.938324
Epoch 42: training loss 43453106846.118
Test Loss of 46831970434.279900, Test MSE of 46831970672.636078
Epoch 43: training loss 41908051553.882
Test Loss of 47177028701.090912, Test MSE of 47177029215.302856
Epoch 44: training loss 39495507072.000
Test Loss of 47617585879.672447, Test MSE of 47617586386.175858
Epoch 45: training loss 37906512542.118
Test Loss of 41170501231.211662, Test MSE of 41170501095.093513
Epoch 46: training loss 36384257167.059
Test Loss of 40814488427.007172, Test MSE of 40814489165.585274
Epoch 47: training loss 34739622640.941
Test Loss of 40156652490.229935, Test MSE of 40156651818.321999
Epoch 48: training loss 32530782373.647
Test Loss of 37504601832.490402, Test MSE of 37504601481.201668
Epoch 49: training loss 31525626127.059
Test Loss of 33795847336.890121, Test MSE of 33795847510.139805
Epoch 50: training loss 30628227312.941
Test Loss of 34681077765.684944, Test MSE of 34681077549.400436
Epoch 51: training loss 28960883681.882
Test Loss of 36951222225.099236, Test MSE of 36951222489.943184
Epoch 52: training loss 27702601027.765
Test Loss of 30494044425.297249, Test MSE of 30494044308.461548
Epoch 53: training loss 26180993054.118
Test Loss of 32596892942.745316, Test MSE of 32596892858.299553
Epoch 54: training loss 25239911631.059
Test Loss of 31158989363.756649, Test MSE of 31158989719.383949
Epoch 55: training loss 24206586680.471
Test Loss of 31763385690.544529, Test MSE of 31763385367.061752
Epoch 56: training loss 23553244886.588
Test Loss of 32285671297.036316, Test MSE of 32285671027.660931
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26927788079.33002, 'MSE - std': 4943415828.764144, 'R2 - mean': 0.8085488896116155, 'R2 - std': 0.02396489928849245} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005837 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109339888.941
Test Loss of 410764251240.248047, Test MSE of 410764251756.903748
Epoch 2: training loss 430087732525.176
Test Loss of 410745733711.370667, Test MSE of 410745737992.737244
Epoch 3: training loss 430059754917.647
Test Loss of 410722120735.748291, Test MSE of 410722121974.278931
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074849159.529
Test Loss of 410727029551.977783, Test MSE of 410727029073.341125
Epoch 2: training loss 430065424504.471
Test Loss of 410727520086.360046, Test MSE of 410727515012.619751
Epoch 3: training loss 426296486972.235
Test Loss of 399030937572.516418, Test MSE of 399030931426.780701
Epoch 4: training loss 399785920512.000
Test Loss of 357769014736.851440, Test MSE of 357769007287.777832
Epoch 5: training loss 335356014110.118
Test Loss of 274586688114.435913, Test MSE of 274586687627.833252
Epoch 6: training loss 253901258029.176
Test Loss of 198134557827.257751, Test MSE of 198134558521.266754
Epoch 7: training loss 185519230192.941
Test Loss of 138731074728.218414, Test MSE of 138731073498.976868
Epoch 8: training loss 149949099309.176
Test Loss of 118393903351.352158, Test MSE of 118393904167.466660
Epoch 9: training loss 140998643350.588
Test Loss of 112132104946.376678, Test MSE of 112132103452.574066
Epoch 10: training loss 136638352534.588
Test Loss of 109396304144.940308, Test MSE of 109396302749.582565
Epoch 11: training loss 135529801667.765
Test Loss of 106630270143.911148, Test MSE of 106630269943.413788
Epoch 12: training loss 131216378880.000
Test Loss of 104589040526.748734, Test MSE of 104589040582.873474
Epoch 13: training loss 128722284965.647
Test Loss of 102114955986.154556, Test MSE of 102114957684.212463
Epoch 14: training loss 125968336173.176
Test Loss of 100081814651.676071, Test MSE of 100081812701.126831
Epoch 15: training loss 122828188009.412
Test Loss of 98474594177.006943, Test MSE of 98474595510.886200
Epoch 16: training loss 119594857893.647
Test Loss of 94674247046.930130, Test MSE of 94674245236.525513
Epoch 17: training loss 116390547757.176
Test Loss of 92234882226.643219, Test MSE of 92234882907.629913
Epoch 18: training loss 113621836227.765
Test Loss of 90331275468.705231, Test MSE of 90331276861.049210
Epoch 19: training loss 111257543860.706
Test Loss of 86992499406.363724, Test MSE of 86992498562.918030
Epoch 20: training loss 106715543386.353
Test Loss of 84858943511.692734, Test MSE of 84858944194.037643
Epoch 21: training loss 103591387316.706
Test Loss of 81549302784.473862, Test MSE of 81549303448.619797
Epoch 22: training loss 100981838938.353
Test Loss of 79587446118.708008, Test MSE of 79587446077.774414
Epoch 23: training loss 97665049328.941
Test Loss of 77469295805.068024, Test MSE of 77469296510.215012
Epoch 24: training loss 93887787414.588
Test Loss of 74265106065.236465, Test MSE of 74265106486.085800
Epoch 25: training loss 91829425362.824
Test Loss of 72438000138.187881, Test MSE of 72438000299.114273
Epoch 26: training loss 86875604766.118
Test Loss of 67958058577.739937, Test MSE of 67958059083.936905
Epoch 27: training loss 84878106563.765
Test Loss of 67375950380.305412, Test MSE of 67375951179.466766
Epoch 28: training loss 81103669985.882
Test Loss of 65493118401.688110, Test MSE of 65493119543.785591
Epoch 29: training loss 78062556340.706
Test Loss of 61979458367.141136, Test MSE of 61979458528.663002
Epoch 30: training loss 75613697912.471
Test Loss of 60767526669.386398, Test MSE of 60767526710.600609
Epoch 31: training loss 72814720918.588
Test Loss of 57737748692.286903, Test MSE of 57737748794.627922
Epoch 32: training loss 70010572664.471
Test Loss of 56389103875.672371, Test MSE of 56389103588.710205
Epoch 33: training loss 66607492246.588
Test Loss of 54600262225.266083, Test MSE of 54600261470.038490
Epoch 34: training loss 64143659008.000
Test Loss of 49846120908.586769, Test MSE of 49846120381.425552
Epoch 35: training loss 61411470652.235
Test Loss of 49590043782.100876, Test MSE of 49590044430.600761
Epoch 36: training loss 58984492152.471
Test Loss of 46935294901.604813, Test MSE of 46935294516.170303
Epoch 37: training loss 56084692502.588
Test Loss of 43464704485.227211, Test MSE of 43464703878.116066
Epoch 38: training loss 54201331267.765
Test Loss of 42755221821.482643, Test MSE of 42755221586.361649
Epoch 39: training loss 51557139659.294
Test Loss of 43066498078.800552, Test MSE of 43066498171.809990
Epoch 40: training loss 49475611211.294
Test Loss of 42419371237.819527, Test MSE of 42419371201.618851
Epoch 41: training loss 47009257562.353
Test Loss of 41291462393.484497, Test MSE of 41291462823.033623
Epoch 42: training loss 45332417001.412
Test Loss of 35960987860.286903, Test MSE of 35960988476.221138
Epoch 43: training loss 43159942618.353
Test Loss of 37171114255.044884, Test MSE of 37171114154.807938
Epoch 44: training loss 41457993426.824
Test Loss of 33163278151.196667, Test MSE of 33163277997.624668
Epoch 45: training loss 39121363395.765
Test Loss of 32658060514.976398, Test MSE of 32658061097.968307
Epoch 46: training loss 37023952986.353
Test Loss of 32199693442.310043, Test MSE of 32199693232.409267
Epoch 47: training loss 35497365180.235
Test Loss of 28593118862.393337, Test MSE of 28593118575.436916
Epoch 48: training loss 33631151367.529
Test Loss of 29240179498.291531, Test MSE of 29240179475.558369
Epoch 49: training loss 31786433871.059
Test Loss of 26789740296.647850, Test MSE of 26789739934.052773
Epoch 50: training loss 31054655713.882
Test Loss of 29045596323.006016, Test MSE of 29045596198.529270
Epoch 51: training loss 29927956208.941
Test Loss of 27905464125.719574, Test MSE of 27905463545.529716
Epoch 52: training loss 28951713475.765
Test Loss of 26689886612.671909, Test MSE of 26689886876.345535
Epoch 53: training loss 26962290104.471
Test Loss of 23664034105.691811, Test MSE of 23664033698.894409
Epoch 54: training loss 26013799137.882
Test Loss of 25684330246.752430, Test MSE of 25684330032.095036
Epoch 55: training loss 24962424451.765
Test Loss of 22365625474.310043, Test MSE of 22365626072.914902
Epoch 56: training loss 23705439329.882
Test Loss of 25079941132.320221, Test MSE of 25079940869.075329
Epoch 57: training loss 23043911623.529
Test Loss of 23359080652.705231, Test MSE of 23359081173.626347
Epoch 58: training loss 22284986902.588
Test Loss of 22880734378.113834, Test MSE of 22880734380.857819
Epoch 59: training loss 21263457008.941
Test Loss of 21579086453.279037, Test MSE of 21579086566.792213
Epoch 60: training loss 20696491440.941
Test Loss of 21280163020.231373, Test MSE of 21280163277.454472
Epoch 61: training loss 19788666970.353
Test Loss of 21011423033.928738, Test MSE of 21011423140.105682
Epoch 62: training loss 19221382070.588
Test Loss of 20072138334.060158, Test MSE of 20072138260.031864
Epoch 63: training loss 18617071201.882
Test Loss of 19169275591.255901, Test MSE of 19169275705.583477
Epoch 64: training loss 18201230840.471
Test Loss of 21754434668.512726, Test MSE of 21754434585.210117
Epoch 65: training loss 17452981552.941
Test Loss of 18547807423.911152, Test MSE of 18547807297.291958
Epoch 66: training loss 17304132886.588
Test Loss of 21370276996.205460, Test MSE of 21370276902.794247
Epoch 67: training loss 16382015149.176
Test Loss of 18728143768.225822, Test MSE of 18728143488.926033
Epoch 68: training loss 16161978618.353
Test Loss of 20864702972.919945, Test MSE of 20864703101.833607
Epoch 69: training loss 15818304417.882
Test Loss of 20648197730.798706, Test MSE of 20648197843.433990
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25357890520.356014, 'MSE - std': 5071661964.19621, 'R2 - mean': 0.8138065827500984, 'R2 - std': 0.02266422974305636} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005889 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043858642.824
Test Loss of 431611860026.284119, Test MSE of 431611854443.159424
Epoch 2: training loss 424025078482.824
Test Loss of 431593048828.801453, Test MSE of 431593042179.987061
Epoch 3: training loss 423998699640.471
Test Loss of 431567223535.533569, Test MSE of 431567219975.999023
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010551296.000
Test Loss of 431570657033.595581, Test MSE of 431570664791.977417
Epoch 2: training loss 424001000749.176
Test Loss of 431572465793.836182, Test MSE of 431572464675.389648
Epoch 3: training loss 420186492687.059
Test Loss of 419353602634.632141, Test MSE of 419353598311.823792
Epoch 4: training loss 393674369024.000
Test Loss of 376932926183.004150, Test MSE of 376932937627.899841
Epoch 5: training loss 329570902136.471
Test Loss of 292112985097.950928, Test MSE of 292112983895.767212
Epoch 6: training loss 249051474221.176
Test Loss of 214703364237.208710, Test MSE of 214703365105.135468
Epoch 7: training loss 182534307328.000
Test Loss of 154093249084.416473, Test MSE of 154093248352.828552
Epoch 8: training loss 147445110603.294
Test Loss of 131764698207.244797, Test MSE of 131764694226.325912
Epoch 9: training loss 137955172141.176
Test Loss of 124844874562.931976, Test MSE of 124844874875.993652
Epoch 10: training loss 134638710377.412
Test Loss of 120893945443.746414, Test MSE of 120893946252.838516
Epoch 11: training loss 132399088820.706
Test Loss of 117370029351.211472, Test MSE of 117370029737.785980
Epoch 12: training loss 130329735559.529
Test Loss of 115366070456.803329, Test MSE of 115366073235.761566
Epoch 13: training loss 125763608786.824
Test Loss of 112365927678.459976, Test MSE of 112365927174.787888
Epoch 14: training loss 124293554552.471
Test Loss of 110798794655.333649, Test MSE of 110798792604.340866
Epoch 15: training loss 120786430253.176
Test Loss of 108166295534.941238, Test MSE of 108166296054.845581
Epoch 16: training loss 117432586059.294
Test Loss of 104389950506.173065, Test MSE of 104389950212.427612
Epoch 17: training loss 114423365451.294
Test Loss of 102300340404.064789, Test MSE of 102300340699.648163
Epoch 18: training loss 111555491840.000
Test Loss of 98387247263.215179, Test MSE of 98387247493.134369
Epoch 19: training loss 109052270592.000
Test Loss of 95252055596.779266, Test MSE of 95252053179.084335
Epoch 20: training loss 106362336978.824
Test Loss of 94367761259.683487, Test MSE of 94367760972.361710
Epoch 21: training loss 102493207220.706
Test Loss of 91018961657.958359, Test MSE of 91018961993.124817
Epoch 22: training loss 100536013161.412
Test Loss of 90058018018.028687, Test MSE of 90058019261.254135
Epoch 23: training loss 96559403745.882
Test Loss of 83877426294.937531, Test MSE of 83877427221.182816
Epoch 24: training loss 93462491151.059
Test Loss of 83247740458.883850, Test MSE of 83247741402.895477
Epoch 25: training loss 91228467877.647
Test Loss of 78501480128.621933, Test MSE of 78501480587.722092
Epoch 26: training loss 87855485259.294
Test Loss of 75529040326.900513, Test MSE of 75529040670.024445
Epoch 27: training loss 84248233291.294
Test Loss of 75022485292.660812, Test MSE of 75022484909.984528
Epoch 28: training loss 82284352180.706
Test Loss of 72249248575.141144, Test MSE of 72249247972.318848
Epoch 29: training loss 78945709417.412
Test Loss of 71458687174.071259, Test MSE of 71458686936.211578
Epoch 30: training loss 75963395523.765
Test Loss of 66171382168.462746, Test MSE of 66171383385.459999
Epoch 31: training loss 72664990938.353
Test Loss of 61446108557.564087, Test MSE of 61446108081.812134
Epoch 32: training loss 70038449061.647
Test Loss of 61517440763.379913, Test MSE of 61517440854.788162
Epoch 33: training loss 67738310181.647
Test Loss of 59166203183.740860, Test MSE of 59166203565.427490
Epoch 34: training loss 65191525888.000
Test Loss of 58513763919.844513, Test MSE of 58513764026.320686
Epoch 35: training loss 61888574629.647
Test Loss of 54659854500.901436, Test MSE of 54659854562.110252
Epoch 36: training loss 60245643926.588
Test Loss of 54915005605.375290, Test MSE of 54915005850.651451
Epoch 37: training loss 57187528568.471
Test Loss of 49640246101.886162, Test MSE of 49640245147.326324
Epoch 38: training loss 54178903393.882
Test Loss of 46861596991.378067, Test MSE of 46861597066.512627
Epoch 39: training loss 52570597451.294
Test Loss of 46957815896.136971, Test MSE of 46957815951.935623
Epoch 40: training loss 50010057524.706
Test Loss of 46028784829.068024, Test MSE of 46028785510.093201
Epoch 41: training loss 48237529682.824
Test Loss of 40622819548.816292, Test MSE of 40622820343.186996
Epoch 42: training loss 46197354157.176
Test Loss of 40287732980.035172, Test MSE of 40287733166.618904
Epoch 43: training loss 44074941108.706
Test Loss of 39381192341.975014, Test MSE of 39381193092.142509
Epoch 44: training loss 42161540216.471
Test Loss of 38313122890.395187, Test MSE of 38313122925.770752
Epoch 45: training loss 40351956916.706
Test Loss of 33936517597.645535, Test MSE of 33936517890.925526
Epoch 46: training loss 38225787339.294
Test Loss of 36992663334.974548, Test MSE of 36992663151.448013
Epoch 47: training loss 36567577065.412
Test Loss of 29824319095.648312, Test MSE of 29824319994.986557
Epoch 48: training loss 35129638814.118
Test Loss of 31815053945.069874, Test MSE of 31815054446.972958
Epoch 49: training loss 33573565168.941
Test Loss of 30995453610.824619, Test MSE of 30995453105.583286
Epoch 50: training loss 32861298763.294
Test Loss of 28907803317.723278, Test MSE of 28907802990.696148
Epoch 51: training loss 30761365519.059
Test Loss of 30133810461.734383, Test MSE of 30133809707.692104
Epoch 52: training loss 29171360376.471
Test Loss of 28298574336.710781, Test MSE of 28298574485.802891
Epoch 53: training loss 28664718125.176
Test Loss of 27228367054.600647, Test MSE of 27228367914.399441
Epoch 54: training loss 26939104112.941
Test Loss of 27173588357.034706, Test MSE of 27173587731.682293
Epoch 55: training loss 26392983951.059
Test Loss of 24153772592.096252, Test MSE of 24153773266.582317
Epoch 56: training loss 24938430166.588
Test Loss of 23772379823.089310, Test MSE of 23772379949.465141
Epoch 57: training loss 24100887341.176
Test Loss of 27041444465.488201, Test MSE of 27041443569.041813
Epoch 58: training loss 23257911055.059
Test Loss of 23397099164.608978, Test MSE of 23397099397.507294
Epoch 59: training loss 22703500272.941
Test Loss of 24175113213.630726, Test MSE of 24175112841.652302
Epoch 60: training loss 21824366618.353
Test Loss of 23813456636.801479, Test MSE of 23813456993.891277
Epoch 61: training loss 20710798080.000
Test Loss of 22338871748.057381, Test MSE of 22338871322.474991
Epoch 62: training loss 20195836306.824
Test Loss of 21911782769.132809, Test MSE of 21911782076.830742
Epoch 63: training loss 19669471627.294
Test Loss of 22622730378.365570, Test MSE of 22622730585.814865
Epoch 64: training loss 19026667817.412
Test Loss of 20630285616.688572, Test MSE of 20630285457.355743
Epoch 65: training loss 18363384828.235
Test Loss of 19725312844.409069, Test MSE of 19725313250.039646
Epoch 66: training loss 17912218977.882
Test Loss of 19880927755.135586, Test MSE of 19880927897.949913
Epoch 67: training loss 17206087239.529
Test Loss of 25012985563.157799, Test MSE of 25012985422.576889
Epoch 68: training loss 17023111868.235
Test Loss of 20820378438.722813, Test MSE of 20820378713.518265
Epoch 69: training loss 16565703653.647
Test Loss of 19853197616.214714, Test MSE of 19853197627.091862
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24256951941.703182, 'MSE - std': 5042387041.781313, 'R2 - mean': 0.8213923828053764, 'R2 - std': 0.025320175706172527} 
 

Saving model.....
Results After CV: {'MSE - mean': 24256951941.703182, 'MSE - std': 5042387041.781313, 'R2 - mean': 0.8213923828053764, 'R2 - std': 0.025320175706172527}
Train time: 98.53744300519938
Inference time: 0.07202314019959885
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 23 finished with value: 24256951941.703182 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525551525.647
Test Loss of 418111851912.024048, Test MSE of 418111853339.361572
Epoch 2: training loss 427504995267.765
Test Loss of 418094045373.024292, Test MSE of 418094043571.672119
Epoch 3: training loss 427477883723.294
Test Loss of 418070043961.382385, Test MSE of 418070046753.606812
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496502934.588
Test Loss of 418077069131.029358, Test MSE of 418077077357.985535
Epoch 2: training loss 427483056128.000
Test Loss of 418077980154.196594, Test MSE of 418077983502.278748
Epoch 3: training loss 427482566053.647
Test Loss of 418077753347.789978, Test MSE of 418077753211.751648
Epoch 4: training loss 427482229338.353
Test Loss of 418077642013.905151, Test MSE of 418077648769.124817
Epoch 5: training loss 420504528052.706
Test Loss of 396148043725.783020, Test MSE of 396148043156.924194
Epoch 6: training loss 374802229248.000
Test Loss of 328877565236.171204, Test MSE of 328877569704.203125
Epoch 7: training loss 295489957707.294
Test Loss of 243839599948.805908, Test MSE of 243839602636.622711
Epoch 8: training loss 216953166637.176
Test Loss of 174545286926.863739, Test MSE of 174545284296.082336
Epoch 9: training loss 160664028852.706
Test Loss of 128099850375.964844, Test MSE of 128099847720.108353
Epoch 10: training loss 140593203471.059
Test Loss of 118724470716.254456, Test MSE of 118724471639.305222
Epoch 11: training loss 136384124114.824
Test Loss of 115962913171.630814, Test MSE of 115962912975.057739
Epoch 12: training loss 133276301824.000
Test Loss of 113584410037.503586, Test MSE of 113584409600.841187
Epoch 13: training loss 130121743209.412
Test Loss of 110310556826.677765, Test MSE of 110310555552.477997
Epoch 14: training loss 126146588160.000
Test Loss of 105782298933.355545, Test MSE of 105782299442.656494
Epoch 15: training loss 123031975107.765
Test Loss of 103763247162.744385, Test MSE of 103763248646.408813
Epoch 16: training loss 119807806584.471
Test Loss of 100477886848.681015, Test MSE of 100477885413.042801
Epoch 17: training loss 116089265543.529
Test Loss of 97775915833.263931, Test MSE of 97775915208.088226
Epoch 18: training loss 112824752323.765
Test Loss of 95340269055.407822, Test MSE of 95340269040.655441
Epoch 19: training loss 109114151936.000
Test Loss of 91629664628.837387, Test MSE of 91629667140.891968
Epoch 20: training loss 105613934049.882
Test Loss of 88736966853.551697, Test MSE of 88736966672.114899
Epoch 21: training loss 102321983141.647
Test Loss of 86455383862.658340, Test MSE of 86455383246.844284
Epoch 22: training loss 98546591021.176
Test Loss of 84791065653.770065, Test MSE of 84791064215.258530
Epoch 23: training loss 95903931873.882
Test Loss of 80754248993.695114, Test MSE of 80754248031.624039
Epoch 24: training loss 92835789914.353
Test Loss of 78119365720.827209, Test MSE of 78119365320.517944
Epoch 25: training loss 88967127536.941
Test Loss of 75275365245.009491, Test MSE of 75275366514.544678
Epoch 26: training loss 86855372822.588
Test Loss of 74394013124.663422, Test MSE of 74394013547.812637
Epoch 27: training loss 82458668604.235
Test Loss of 71133520639.467041, Test MSE of 71133520558.214340
Epoch 28: training loss 80440575849.412
Test Loss of 68839284224.355316, Test MSE of 68839283412.403458
Epoch 29: training loss 77126922586.353
Test Loss of 66380794407.913025, Test MSE of 66380794242.155777
Epoch 30: training loss 74103267885.176
Test Loss of 63581622708.792969, Test MSE of 63581621555.292030
Epoch 31: training loss 71386486196.706
Test Loss of 61466215273.585938, Test MSE of 61466215083.901222
Epoch 32: training loss 68620073660.235
Test Loss of 58794374076.017578, Test MSE of 58794372795.071075
Epoch 33: training loss 66482670599.529
Test Loss of 58257707713.880173, Test MSE of 58257707738.242599
Epoch 34: training loss 63696679168.000
Test Loss of 51817710287.145035, Test MSE of 51817710316.629181
Epoch 35: training loss 60946553494.588
Test Loss of 52282710492.113808, Test MSE of 52282710613.074219
Epoch 36: training loss 58182468336.941
Test Loss of 48389653907.867683, Test MSE of 48389653532.642410
Epoch 37: training loss 55357147625.412
Test Loss of 48216069355.214432, Test MSE of 48216068913.100876
Epoch 38: training loss 53382058928.941
Test Loss of 46001630476.850334, Test MSE of 46001630139.782974
Epoch 39: training loss 50710944399.059
Test Loss of 43009153120.170250, Test MSE of 43009153014.760437
Epoch 40: training loss 48725430934.588
Test Loss of 39702343391.962990, Test MSE of 39702344224.450027
Epoch 41: training loss 46378717024.000
Test Loss of 39727824442.152206, Test MSE of 39727824657.869820
Epoch 42: training loss 44860604291.765
Test Loss of 36352142731.814018, Test MSE of 36352142014.008812
Epoch 43: training loss 42875698063.059
Test Loss of 36286247924.866989, Test MSE of 36286248829.418533
Epoch 44: training loss 40918198851.765
Test Loss of 36315133439.881561, Test MSE of 36315134329.969612
Epoch 45: training loss 38818140175.059
Test Loss of 32321254573.627575, Test MSE of 32321254873.884930
Epoch 46: training loss 37203938917.647
Test Loss of 35529821266.194771, Test MSE of 35529821261.943932
Epoch 47: training loss 35391244483.765
Test Loss of 32806854395.440205, Test MSE of 32806854023.400620
Epoch 48: training loss 33808404242.824
Test Loss of 28913378422.673145, Test MSE of 28913378116.242760
Epoch 49: training loss 32074756833.882
Test Loss of 27992152764.668980, Test MSE of 27992153346.943737
Epoch 50: training loss 31267095770.353
Test Loss of 28022968030.068008, Test MSE of 28022967697.879242
Epoch 51: training loss 29016194025.412
Test Loss of 27010376295.868610, Test MSE of 27010377099.872379
Epoch 52: training loss 28591814960.941
Test Loss of 26528983922.587093, Test MSE of 26528984059.742878
Epoch 53: training loss 27004193667.765
Test Loss of 27191420238.700901, Test MSE of 27191420506.211506
Epoch 54: training loss 26302824858.353
Test Loss of 23816467063.502197, Test MSE of 23816467284.807243
Epoch 55: training loss 25122085687.529
Test Loss of 21498627487.711311, Test MSE of 21498627082.989468
Epoch 56: training loss 24173515124.706
Test Loss of 21360794511.959286, Test MSE of 21360794357.826107
Epoch 57: training loss 23127427489.882
Test Loss of 23292718815.015499, Test MSE of 23292718992.375710
Epoch 58: training loss 22673748227.765
Test Loss of 19875599665.802452, Test MSE of 19875600014.409206
Epoch 59: training loss 21435737908.706
Test Loss of 23517626298.359474, Test MSE of 23517626395.792686
Epoch 60: training loss 20732771384.471
Test Loss of 21491384668.202637, Test MSE of 21491385090.803192
Epoch 61: training loss 20172968768.000
Test Loss of 22679265279.052509, Test MSE of 22679265665.049126
Epoch 62: training loss 19508598411.294
Test Loss of 21285929898.962757, Test MSE of 21285930487.431194
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21285930487.431194, 'MSE - std': 0.0, 'R2 - mean': 0.8342441497884829, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004031 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917799905.882
Test Loss of 424557583726.441833, Test MSE of 424557590204.868835
Epoch 2: training loss 427897962977.882
Test Loss of 424541567291.987976, Test MSE of 424541564116.607239
Epoch 3: training loss 427871114420.706
Test Loss of 424519984064.755005, Test MSE of 424519993222.756958
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890848466.824
Test Loss of 424527167146.903564, Test MSE of 424527161421.850098
Epoch 2: training loss 427879146676.706
Test Loss of 424527259159.568848, Test MSE of 424527259149.414368
Epoch 3: training loss 427878589138.824
Test Loss of 424526877010.253967, Test MSE of 424526876947.740662
Epoch 4: training loss 427878142433.882
Test Loss of 424526737304.723572, Test MSE of 424526737946.963196
Epoch 5: training loss 420902527939.765
Test Loss of 402942955638.436279, Test MSE of 402942958311.837036
Epoch 6: training loss 374627670618.353
Test Loss of 336518344032.229492, Test MSE of 336518348431.593933
Epoch 7: training loss 295793317406.118
Test Loss of 252597339790.952576, Test MSE of 252597342853.199463
Epoch 8: training loss 215603027486.118
Test Loss of 184088485541.218597, Test MSE of 184088487478.020813
Epoch 9: training loss 159297031348.706
Test Loss of 138593250709.288910, Test MSE of 138593246984.949066
Epoch 10: training loss 136739441724.235
Test Loss of 130927376140.968765, Test MSE of 130927376019.830292
Epoch 11: training loss 133274962251.294
Test Loss of 127066950849.051117, Test MSE of 127066950482.298828
Epoch 12: training loss 131584266059.294
Test Loss of 124884649944.679153, Test MSE of 124884648594.474518
Epoch 13: training loss 127967001901.176
Test Loss of 121674786894.167938, Test MSE of 121674783856.371887
Epoch 14: training loss 124599359156.706
Test Loss of 119875418397.668289, Test MSE of 119875419919.100647
Epoch 15: training loss 121583985242.353
Test Loss of 115933500910.826736, Test MSE of 115933501191.906433
Epoch 16: training loss 117287744752.941
Test Loss of 113025332061.268570, Test MSE of 113025329813.214813
Epoch 17: training loss 113258574787.765
Test Loss of 108196402564.707840, Test MSE of 108196402022.875687
Epoch 18: training loss 109574969584.941
Test Loss of 106460663623.239410, Test MSE of 106460662108.405258
Epoch 19: training loss 106325968896.000
Test Loss of 102873459701.340729, Test MSE of 102873458802.366104
Epoch 20: training loss 103711275941.647
Test Loss of 99130083150.345596, Test MSE of 99130083987.328094
Epoch 21: training loss 99198053827.765
Test Loss of 95687826293.429565, Test MSE of 95687826592.487900
Epoch 22: training loss 96423802277.647
Test Loss of 93481286402.783249, Test MSE of 93481285265.695129
Epoch 23: training loss 91562972220.235
Test Loss of 89938439370.526016, Test MSE of 89938440647.016998
Epoch 24: training loss 90060946100.706
Test Loss of 84341301537.931992, Test MSE of 84341303438.603485
Epoch 25: training loss 86821261372.235
Test Loss of 82268630332.224838, Test MSE of 82268630046.573166
Epoch 26: training loss 83461990189.176
Test Loss of 76769597433.841309, Test MSE of 76769596242.188553
Epoch 27: training loss 79386229127.529
Test Loss of 76029961828.315521, Test MSE of 76029962043.601578
Epoch 28: training loss 77436589327.059
Test Loss of 72624043738.751785, Test MSE of 72624043351.169724
Epoch 29: training loss 73690101443.765
Test Loss of 66613172939.828819, Test MSE of 66613174733.224350
Epoch 30: training loss 70227837680.941
Test Loss of 66830754589.786720, Test MSE of 66830754163.077202
Epoch 31: training loss 68348522752.000
Test Loss of 65183668647.054359, Test MSE of 65183670625.794472
Epoch 32: training loss 64460082507.294
Test Loss of 62174001388.398796, Test MSE of 62174002856.406593
Epoch 33: training loss 62711351070.118
Test Loss of 62158101133.294472, Test MSE of 62158099104.329315
Epoch 34: training loss 59825283809.882
Test Loss of 59162742608.714317, Test MSE of 59162742789.289864
Epoch 35: training loss 57622260540.235
Test Loss of 56943396474.107796, Test MSE of 56943397625.138199
Epoch 36: training loss 54723118908.235
Test Loss of 54496232230.314133, Test MSE of 54496232777.122940
Epoch 37: training loss 52449446354.824
Test Loss of 52831915751.542908, Test MSE of 52831915688.052551
Epoch 38: training loss 49432121524.706
Test Loss of 49223086023.624336, Test MSE of 49223085350.143402
Epoch 39: training loss 47861821967.059
Test Loss of 48361624530.757347, Test MSE of 48361623423.066978
Epoch 40: training loss 45553126174.118
Test Loss of 43397087850.711082, Test MSE of 43397088814.891914
Epoch 41: training loss 43124022144.000
Test Loss of 45522633281.969002, Test MSE of 45522632657.200516
Epoch 42: training loss 40596026224.941
Test Loss of 42402715627.155220, Test MSE of 42402715123.827461
Epoch 43: training loss 39461430753.882
Test Loss of 40130132414.267868, Test MSE of 40130132813.205788
Epoch 44: training loss 36854394736.941
Test Loss of 41958102667.162621, Test MSE of 41958104185.604797
Epoch 45: training loss 35494689438.118
Test Loss of 37857688270.671295, Test MSE of 37857688069.765961
Epoch 46: training loss 34368588912.941
Test Loss of 33867224981.881100, Test MSE of 33867225644.034779
Epoch 47: training loss 31847165899.294
Test Loss of 36466891046.669441, Test MSE of 36466890223.769272
Epoch 48: training loss 30413997718.588
Test Loss of 34325291215.026604, Test MSE of 34325291315.724670
Epoch 49: training loss 28937086441.412
Test Loss of 32808149899.695583, Test MSE of 32808150031.438858
Epoch 50: training loss 27393391420.235
Test Loss of 32987337153.584084, Test MSE of 32987337599.695843
Epoch 51: training loss 27136221921.882
Test Loss of 35358479902.674995, Test MSE of 35358479113.743340
Epoch 52: training loss 25421211640.471
Test Loss of 29006488509.438816, Test MSE of 29006488375.020618
Epoch 53: training loss 24228334388.706
Test Loss of 29331274716.232246, Test MSE of 29331275429.043003
Epoch 54: training loss 23276537152.000
Test Loss of 29179299984.965996, Test MSE of 29179299183.578960
Epoch 55: training loss 22071192899.765
Test Loss of 28268199416.301643, Test MSE of 28268199341.638355
Epoch 56: training loss 21127333232.941
Test Loss of 26942135974.166088, Test MSE of 26942135889.506630
Epoch 57: training loss 20272547478.588
Test Loss of 27932703860.778164, Test MSE of 27932703544.536289
Epoch 58: training loss 19327171606.588
Test Loss of 27410481951.207958, Test MSE of 27410482518.819221
Epoch 59: training loss 18437719860.706
Test Loss of 26895588819.823273, Test MSE of 26895588321.784996
Epoch 60: training loss 18074767363.765
Test Loss of 30706841461.192692, Test MSE of 30706841451.435249
Epoch 61: training loss 17253612638.118
Test Loss of 28812741919.800140, Test MSE of 28812742691.209633
Epoch 62: training loss 16604176030.118
Test Loss of 26188402287.211658, Test MSE of 26188402295.844028
Epoch 63: training loss 16246987749.647
Test Loss of 26888176264.793892, Test MSE of 26888176760.762341
Epoch 64: training loss 15618526027.294
Test Loss of 26533090201.434189, Test MSE of 26533090377.762180
Epoch 65: training loss 15390629778.824
Test Loss of 25335879752.009254, Test MSE of 25335879881.013844
Epoch 66: training loss 14955048978.824
Test Loss of 23276274039.206108, Test MSE of 23276273996.349842
Epoch 67: training loss 14303061861.647
Test Loss of 24915058994.513069, Test MSE of 24915058589.475960
Epoch 68: training loss 13928344094.118
Test Loss of 27911142246.269722, Test MSE of 27911142481.328636
Epoch 69: training loss 13636657716.706
Test Loss of 24888819077.181587, Test MSE of 24888819226.195786
Epoch 70: training loss 13378797914.353
Test Loss of 24524448256.592182, Test MSE of 24524447536.141068
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22905189011.786133, 'MSE - std': 1619258524.3549366, 'R2 - mean': 0.8295779384284889, 'R2 - std': 0.00466621135999401} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003558 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926332295.529
Test Loss of 447258191747.405029, Test MSE of 447258192558.862122
Epoch 2: training loss 421904004156.235
Test Loss of 447238407682.724060, Test MSE of 447238410756.335388
Epoch 3: training loss 421875700916.706
Test Loss of 447212893955.493896, Test MSE of 447212896176.133118
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896583890.824
Test Loss of 447222242227.016418, Test MSE of 447222252769.103516
Epoch 2: training loss 421885571553.882
Test Loss of 447222703571.349548, Test MSE of 447222694099.875549
Epoch 3: training loss 421885061963.294
Test Loss of 447222652497.839478, Test MSE of 447222655154.962708
Epoch 4: training loss 421884638388.706
Test Loss of 447221931419.921326, Test MSE of 447221930833.472717
Epoch 5: training loss 415242058691.765
Test Loss of 425711597266.224365, Test MSE of 425711596966.304749
Epoch 6: training loss 370421028382.118
Test Loss of 358994806403.582703, Test MSE of 358994805414.820007
Epoch 7: training loss 292256910155.294
Test Loss of 271957866205.594269, Test MSE of 271957870042.567841
Epoch 8: training loss 214049504256.000
Test Loss of 200137901957.300018, Test MSE of 200137901016.971893
Epoch 9: training loss 155741638686.118
Test Loss of 150393418102.258606, Test MSE of 150393417564.677246
Epoch 10: training loss 135039845918.118
Test Loss of 140018412536.183197, Test MSE of 140018413917.574615
Epoch 11: training loss 130899255446.588
Test Loss of 136165899295.030304, Test MSE of 136165901691.527359
Epoch 12: training loss 128025501635.765
Test Loss of 133697367657.526718, Test MSE of 133697367865.822571
Epoch 13: training loss 125393811696.941
Test Loss of 129215243022.626877, Test MSE of 129215242358.655319
Epoch 14: training loss 122031036205.176
Test Loss of 125787166363.269958, Test MSE of 125787167705.221893
Epoch 15: training loss 117844341278.118
Test Loss of 123007957133.649780, Test MSE of 123007958244.155426
Epoch 16: training loss 114960184470.588
Test Loss of 120034441118.882263, Test MSE of 120034439539.665543
Epoch 17: training loss 110658137961.412
Test Loss of 116562369743.263474, Test MSE of 116562370064.092850
Epoch 18: training loss 106067775759.059
Test Loss of 113091960936.223923, Test MSE of 113091960660.938324
Epoch 19: training loss 104174618593.882
Test Loss of 109387076013.213043, Test MSE of 109387076351.358231
Epoch 20: training loss 100724701153.882
Test Loss of 105772669295.152435, Test MSE of 105772669514.118607
Epoch 21: training loss 95801758177.882
Test Loss of 101680654561.976410, Test MSE of 101680654447.272324
Epoch 22: training loss 92562317899.294
Test Loss of 99317274720.170258, Test MSE of 99317274203.197906
Epoch 23: training loss 89913350234.353
Test Loss of 94387803888.544067, Test MSE of 94387805199.384277
Epoch 24: training loss 85650361765.647
Test Loss of 92158384438.303024, Test MSE of 92158383859.183975
Epoch 25: training loss 83475108743.529
Test Loss of 89553339823.344894, Test MSE of 89553339539.124252
Epoch 26: training loss 79155116498.824
Test Loss of 83132018010.544525, Test MSE of 83132018158.240402
Epoch 27: training loss 77296787998.118
Test Loss of 83397888506.433502, Test MSE of 83397889715.818420
Epoch 28: training loss 75009782031.059
Test Loss of 79541723033.671066, Test MSE of 79541723787.964615
Epoch 29: training loss 71333175040.000
Test Loss of 73971711088.514450, Test MSE of 73971709926.013580
Epoch 30: training loss 67492990509.176
Test Loss of 72644259488.481140, Test MSE of 72644257424.038742
Epoch 31: training loss 64963311495.529
Test Loss of 68195635102.408516, Test MSE of 68195635686.937843
Epoch 32: training loss 62692608843.294
Test Loss of 68768001654.791580, Test MSE of 68768001632.399368
Epoch 33: training loss 59577052536.471
Test Loss of 62975306609.639603, Test MSE of 62975305938.770905
Epoch 34: training loss 56876803870.118
Test Loss of 60789367638.636131, Test MSE of 60789368218.432312
Epoch 35: training loss 54806982648.471
Test Loss of 56104228115.009018, Test MSE of 56104229107.287476
Epoch 36: training loss 53003043840.000
Test Loss of 57526581950.563957, Test MSE of 57526582810.748222
Epoch 37: training loss 50540064353.882
Test Loss of 53221615514.144806, Test MSE of 53221616316.769905
Epoch 38: training loss 47899027907.765
Test Loss of 51127625859.227386, Test MSE of 51127626513.273247
Epoch 39: training loss 45407455676.235
Test Loss of 48511631940.811470, Test MSE of 48511631702.394531
Epoch 40: training loss 43347358117.647
Test Loss of 46499621882.078186, Test MSE of 46499622437.853790
Epoch 41: training loss 41817558053.647
Test Loss of 44869798548.874390, Test MSE of 44869799110.390656
Epoch 42: training loss 39935879280.941
Test Loss of 43502575826.342819, Test MSE of 43502575176.470314
Epoch 43: training loss 38035565214.118
Test Loss of 40212557454.478836, Test MSE of 40212556802.195236
Epoch 44: training loss 35698303450.353
Test Loss of 40434820518.817490, Test MSE of 40434819914.895981
Epoch 45: training loss 34195333993.412
Test Loss of 36069724636.587555, Test MSE of 36069724757.234314
Epoch 46: training loss 32927332630.588
Test Loss of 36824768129.213974, Test MSE of 36824769487.634949
Epoch 47: training loss 31109093142.588
Test Loss of 34573502138.537125, Test MSE of 34573502174.251930
Epoch 48: training loss 29497496816.941
Test Loss of 33179915884.842934, Test MSE of 33179915482.925610
Epoch 49: training loss 28577227241.412
Test Loss of 33422993102.434422, Test MSE of 33422992532.172607
Epoch 50: training loss 26986030900.706
Test Loss of 30617591776.732826, Test MSE of 30617591704.064857
Epoch 51: training loss 26580618563.765
Test Loss of 30321157814.747166, Test MSE of 30321157732.902431
Epoch 52: training loss 24858614061.176
Test Loss of 31237759557.758965, Test MSE of 31237759994.181145
Epoch 53: training loss 23880686757.647
Test Loss of 30228925977.937542, Test MSE of 30228925789.798828
Epoch 54: training loss 22617421989.647
Test Loss of 27433483131.114502, Test MSE of 27433482975.241978
Epoch 55: training loss 22202928271.059
Test Loss of 27472228868.855888, Test MSE of 27472228808.047569
Epoch 56: training loss 21162936670.118
Test Loss of 24731466752.710617, Test MSE of 24731467514.535007
Epoch 57: training loss 20540000666.353
Test Loss of 24467792886.288227, Test MSE of 24467792266.786861
Epoch 58: training loss 19515607973.647
Test Loss of 26126474566.647236, Test MSE of 26126474198.367916
Epoch 59: training loss 18969982290.824
Test Loss of 21394477055.526257, Test MSE of 21394477449.358692
Epoch 60: training loss 18549717891.765
Test Loss of 24743637930.489014, Test MSE of 24743638286.322628
Epoch 61: training loss 17689153050.353
Test Loss of 23135847038.134628, Test MSE of 23135847411.532379
Epoch 62: training loss 17016903168.000
Test Loss of 21851094021.448067, Test MSE of 21851093965.753750
Epoch 63: training loss 16752489938.824
Test Loss of 23333838739.512375, Test MSE of 23333838953.698978
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23048072325.75708, 'MSE - std': 1337471518.1839921, 'R2 - mean': 0.8346080169465209, 'R2 - std': 0.008069638494539835} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005575 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109073648.941
Test Loss of 410762423920.066650, Test MSE of 410762417069.594788
Epoch 2: training loss 430086549985.882
Test Loss of 410742858512.703369, Test MSE of 410742857912.181641
Epoch 3: training loss 430058184944.941
Test Loss of 410718369156.086975, Test MSE of 410718366068.981018
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430073464952.471
Test Loss of 410721674660.309143, Test MSE of 410721672497.226196
Epoch 2: training loss 430061561976.471
Test Loss of 410722936220.727417, Test MSE of 410722930389.842712
Epoch 3: training loss 430060992391.529
Test Loss of 410722834565.627014, Test MSE of 410722840158.724609
Epoch 4: training loss 430060582068.706
Test Loss of 410722972339.827881, Test MSE of 410722975739.241089
Epoch 5: training loss 423125432922.353
Test Loss of 389000869562.935669, Test MSE of 389000877932.416992
Epoch 6: training loss 377402825667.765
Test Loss of 322372246805.205017, Test MSE of 322372247660.919434
Epoch 7: training loss 298055685180.235
Test Loss of 237393511855.681641, Test MSE of 237393515401.624481
Epoch 8: training loss 219490908521.412
Test Loss of 167506230719.792694, Test MSE of 167506234271.757690
Epoch 9: training loss 162103585370.353
Test Loss of 120872001191.033783, Test MSE of 120872000637.991516
Epoch 10: training loss 142289364781.176
Test Loss of 112329786405.434525, Test MSE of 112329786307.562561
Epoch 11: training loss 137044192948.706
Test Loss of 108925928057.069870, Test MSE of 108925926159.843552
Epoch 12: training loss 135182981270.588
Test Loss of 106236648133.834335, Test MSE of 106236646292.815643
Epoch 13: training loss 132020984530.824
Test Loss of 103684075893.397507, Test MSE of 103684075841.379883
Epoch 14: training loss 127923443410.824
Test Loss of 100096094031.726059, Test MSE of 100096092404.444305
Epoch 15: training loss 123837293628.235
Test Loss of 96497257739.727905, Test MSE of 96497258149.543411
Epoch 16: training loss 120182470475.294
Test Loss of 93587158059.120773, Test MSE of 93587159830.638992
Epoch 17: training loss 116594160730.353
Test Loss of 91120250643.072647, Test MSE of 91120249271.159439
Epoch 18: training loss 113600459384.471
Test Loss of 89211250724.960663, Test MSE of 89211251407.987045
Epoch 19: training loss 108391515346.824
Test Loss of 86399875266.280426, Test MSE of 86399875669.138062
Epoch 20: training loss 104897604035.765
Test Loss of 82351352845.741791, Test MSE of 82351354943.835815
Epoch 21: training loss 100454922330.353
Test Loss of 81395606720.858856, Test MSE of 81395606538.190414
Epoch 22: training loss 98524683173.647
Test Loss of 77256105294.067566, Test MSE of 77256103752.494888
Epoch 23: training loss 94517527868.235
Test Loss of 75712832257.540024, Test MSE of 75712830934.982010
Epoch 24: training loss 92000284114.824
Test Loss of 73875805033.788055, Test MSE of 73875805551.011475
Epoch 25: training loss 88443937189.647
Test Loss of 71383889841.340118, Test MSE of 71383889205.651413
Epoch 26: training loss 84777917078.588
Test Loss of 68150511546.817215, Test MSE of 68150511998.170090
Epoch 27: training loss 82028366727.529
Test Loss of 66610699040.814438, Test MSE of 66610699269.411125
Epoch 28: training loss 78662576956.235
Test Loss of 62138320250.609901, Test MSE of 62138320547.769348
Epoch 29: training loss 75926966904.471
Test Loss of 60402843420.549744, Test MSE of 60402842858.588585
Epoch 30: training loss 72254732845.176
Test Loss of 58722038944.636742, Test MSE of 58722038248.654724
Epoch 31: training loss 69114475911.529
Test Loss of 57081949311.466911, Test MSE of 57081949508.656982
Epoch 32: training loss 66328672843.294
Test Loss of 53734678283.490974, Test MSE of 53734678906.317009
Epoch 33: training loss 63831219651.765
Test Loss of 52413293923.864876, Test MSE of 52413293330.195564
Epoch 34: training loss 60939101929.412
Test Loss of 50543415500.705231, Test MSE of 50543416497.607399
Epoch 35: training loss 58250036359.529
Test Loss of 46612899293.171677, Test MSE of 46612899950.269302
Epoch 36: training loss 55864900156.235
Test Loss of 45659589331.102264, Test MSE of 45659588287.339790
Epoch 37: training loss 53218088801.882
Test Loss of 45948569263.563164, Test MSE of 45948569279.495163
Epoch 38: training loss 51070189440.000
Test Loss of 42415822204.505325, Test MSE of 42415821503.621758
Epoch 39: training loss 48659459094.588
Test Loss of 39406745000.099953, Test MSE of 39406744600.679100
Epoch 40: training loss 46808293865.412
Test Loss of 38284727494.545120, Test MSE of 38284727162.018349
Epoch 41: training loss 44973302234.353
Test Loss of 36232008834.310043, Test MSE of 36232008529.020699
Epoch 42: training loss 42230559013.647
Test Loss of 31059996188.668209, Test MSE of 31059995726.001560
Epoch 43: training loss 40423569686.588
Test Loss of 34550390240.014809, Test MSE of 34550389883.392204
Epoch 44: training loss 39012259870.118
Test Loss of 33131625726.459972, Test MSE of 33131625735.965714
Epoch 45: training loss 36384636408.471
Test Loss of 31124548722.198982, Test MSE of 31124549163.068371
Epoch 46: training loss 34854158750.118
Test Loss of 28959605035.476170, Test MSE of 28959605404.676319
Epoch 47: training loss 33386822166.588
Test Loss of 29606691850.898659, Test MSE of 29606692005.535870
Epoch 48: training loss 31861269737.412
Test Loss of 28787906984.573807, Test MSE of 28787906565.370106
Epoch 49: training loss 30356456922.353
Test Loss of 26262019020.349838, Test MSE of 26262019660.701862
Epoch 50: training loss 28902031841.882
Test Loss of 23686516502.863487, Test MSE of 23686516815.123634
Epoch 51: training loss 27841028118.588
Test Loss of 25960627366.322998, Test MSE of 25960628140.565647
Epoch 52: training loss 26864101112.471
Test Loss of 24772703541.427116, Test MSE of 24772703958.874481
Epoch 53: training loss 25579178435.765
Test Loss of 22208575139.242943, Test MSE of 22208575156.609257
Epoch 54: training loss 24248345336.471
Test Loss of 24326272870.471077, Test MSE of 24326272528.478905
Epoch 55: training loss 23347774851.765
Test Loss of 23677249605.182785, Test MSE of 23677249535.184380
Epoch 56: training loss 22870524355.765
Test Loss of 23784412164.264690, Test MSE of 23784412406.287411
Epoch 57: training loss 21865971301.647
Test Loss of 22987167930.698750, Test MSE of 22987168101.638260
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23032846269.727375, 'MSE - std': 1158584500.505554, 'R2 - mean': 0.8285247431210983, 'R2 - std': 0.01264349478505187} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005411 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043831416.471
Test Loss of 431612416369.606689, Test MSE of 431612430285.711182
Epoch 2: training loss 424024712011.294
Test Loss of 431592906005.205017, Test MSE of 431592908573.702332
Epoch 3: training loss 423998320519.529
Test Loss of 431566508280.299866, Test MSE of 431566506628.470947
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012047661.176
Test Loss of 431567799287.944458, Test MSE of 431567791672.788086
Epoch 2: training loss 424000293345.882
Test Loss of 431569044396.127747, Test MSE of 431569048732.877258
Epoch 3: training loss 423999628709.647
Test Loss of 431569316563.102295, Test MSE of 431569321940.163574
Epoch 4: training loss 423999204412.235
Test Loss of 431568911216.421997, Test MSE of 431568911580.907532
Epoch 5: training loss 416697237263.059
Test Loss of 408455298382.067566, Test MSE of 408455299741.329041
Epoch 6: training loss 370152773270.588
Test Loss of 339812085413.138367, Test MSE of 339812085956.524353
Epoch 7: training loss 291717147407.059
Test Loss of 253065618465.643677, Test MSE of 253065616015.279816
Epoch 8: training loss 214582754484.706
Test Loss of 182381465491.961121, Test MSE of 182381463305.763550
Epoch 9: training loss 157923205059.765
Test Loss of 134502138526.978256, Test MSE of 134502138569.106491
Epoch 10: training loss 139396696184.471
Test Loss of 124598606753.702911, Test MSE of 124598607625.159012
Epoch 11: training loss 135609929637.647
Test Loss of 121174079761.888016, Test MSE of 121174080929.098801
Epoch 12: training loss 132655030844.235
Test Loss of 118545619195.142990, Test MSE of 118545620415.841873
Epoch 13: training loss 128985418571.294
Test Loss of 115365176308.153625, Test MSE of 115365176434.968674
Epoch 14: training loss 125950121773.176
Test Loss of 111309732860.683014, Test MSE of 111309734184.403793
Epoch 15: training loss 122289214885.647
Test Loss of 109497364394.706161, Test MSE of 109497364103.037750
Epoch 16: training loss 118122003636.706
Test Loss of 104351518998.626556, Test MSE of 104351518203.716629
Epoch 17: training loss 114674332852.706
Test Loss of 102704262343.492828, Test MSE of 102704261524.017761
Epoch 18: training loss 112197090996.706
Test Loss of 99444954280.218414, Test MSE of 99444955832.528397
Epoch 19: training loss 108515414648.471
Test Loss of 97899961348.264694, Test MSE of 97899960619.603043
Epoch 20: training loss 104936467787.294
Test Loss of 92454910730.069412, Test MSE of 92454910811.563004
Epoch 21: training loss 100863531572.706
Test Loss of 89622156482.754288, Test MSE of 89622154899.730759
Epoch 22: training loss 98183876751.059
Test Loss of 87113168514.073120, Test MSE of 87113168770.696960
Epoch 23: training loss 95039828043.294
Test Loss of 84929180813.682556, Test MSE of 84929182748.959488
Epoch 24: training loss 91000597744.941
Test Loss of 81730787488.162888, Test MSE of 81730787734.803558
Epoch 25: training loss 88442709647.059
Test Loss of 78006316932.797775, Test MSE of 78006318842.388458
Epoch 26: training loss 86720566272.000
Test Loss of 75767073964.956970, Test MSE of 75767074216.694290
Epoch 27: training loss 83257671574.588
Test Loss of 71634490680.270248, Test MSE of 71634490908.164780
Epoch 28: training loss 79484114477.176
Test Loss of 69765314316.438690, Test MSE of 69765314076.024277
Epoch 29: training loss 77511758245.647
Test Loss of 66265956456.248032, Test MSE of 66265957453.618217
Epoch 30: training loss 74042105916.235
Test Loss of 62849060484.916245, Test MSE of 62849060416.131111
Epoch 31: training loss 70904081257.412
Test Loss of 62950802255.252197, Test MSE of 62950801836.808784
Epoch 32: training loss 68102741880.471
Test Loss of 58787074849.762146, Test MSE of 58787074174.317986
Epoch 33: training loss 65614411166.118
Test Loss of 55104198605.771400, Test MSE of 55104199860.329620
Epoch 34: training loss 62992733199.059
Test Loss of 54082271217.784363, Test MSE of 54082270887.080849
Epoch 35: training loss 60653006290.824
Test Loss of 51245755813.256828, Test MSE of 51245756040.011513
Epoch 36: training loss 58133936956.235
Test Loss of 48689871292.475708, Test MSE of 48689871323.843605
Epoch 37: training loss 55413798128.941
Test Loss of 48568783401.462288, Test MSE of 48568781583.152855
Epoch 38: training loss 52468241287.529
Test Loss of 42915419366.767235, Test MSE of 42915419707.281601
Epoch 39: training loss 50713333654.588
Test Loss of 41278299064.447937, Test MSE of 41278298980.160789
Epoch 40: training loss 48955457219.765
Test Loss of 40449135905.525223, Test MSE of 40449136546.750450
Epoch 41: training loss 46413456489.412
Test Loss of 40600706939.320686, Test MSE of 40600707176.057487
Epoch 42: training loss 44404952033.882
Test Loss of 36847204771.835258, Test MSE of 36847204592.685028
Epoch 43: training loss 42857644634.353
Test Loss of 34539247363.909302, Test MSE of 34539247799.353264
Epoch 44: training loss 40690177957.647
Test Loss of 32489567135.807495, Test MSE of 32489567020.327618
Epoch 45: training loss 38317794785.882
Test Loss of 32013300179.220730, Test MSE of 32013299774.250286
Epoch 46: training loss 36982992368.941
Test Loss of 34188528449.036556, Test MSE of 34188529131.939655
Epoch 47: training loss 35049333933.176
Test Loss of 31979377768.721889, Test MSE of 31979377876.262203
Epoch 48: training loss 34060198919.529
Test Loss of 31275861564.890327, Test MSE of 31275861378.169865
Epoch 49: training loss 32865213876.706
Test Loss of 26375058977.406757, Test MSE of 26375058711.286686
Epoch 50: training loss 31760066356.706
Test Loss of 25448962742.670986, Test MSE of 25448962470.884907
Epoch 51: training loss 30118943664.941
Test Loss of 25836307158.893105, Test MSE of 25836307342.543804
Epoch 52: training loss 28390828935.529
Test Loss of 27308422524.505322, Test MSE of 27308422805.284641
Epoch 53: training loss 27402464745.412
Test Loss of 23384818964.257290, Test MSE of 23384818773.106510
Epoch 54: training loss 25746758324.706
Test Loss of 24073215830.360020, Test MSE of 24073216194.865376
Epoch 55: training loss 25421901127.529
Test Loss of 21175137742.008331, Test MSE of 21175137974.020359
Epoch 56: training loss 24638645989.647
Test Loss of 22618892096.562702, Test MSE of 22618892197.439552
Epoch 57: training loss 23484591815.529
Test Loss of 23371773146.447014, Test MSE of 23371773351.340584
Epoch 58: training loss 22505210183.529
Test Loss of 24213324846.911613, Test MSE of 24213324535.018425
Epoch 59: training loss 21814388276.706
Test Loss of 23714769161.358631, Test MSE of 23714768767.404202
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23169230769.26274, 'MSE - std': 1071567712.6077528, 'R2 - mean': 0.8273992397731117, 'R2 - std': 0.011530542027546817} 
 

Saving model.....
Results After CV: {'MSE - mean': 23169230769.26274, 'MSE - std': 1071567712.6077528, 'R2 - mean': 0.8273992397731117, 'R2 - std': 0.011530542027546817}
Train time: 94.86549676379946
Inference time: 0.07207940359985514
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 24 finished with value: 23169230769.26274 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005391 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525889325.176
Test Loss of 418111055072.792053, Test MSE of 418111056662.861206
Epoch 2: training loss 427505629424.941
Test Loss of 418092649735.165405, Test MSE of 418092651126.852051
Epoch 3: training loss 427478481859.765
Test Loss of 418068337554.328003, Test MSE of 418068343103.188538
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490491211.294
Test Loss of 418072503782.299316, Test MSE of 418072500507.321472
Epoch 2: training loss 427480842721.882
Test Loss of 418073370212.789246, Test MSE of 418073371953.947510
Epoch 3: training loss 427480322529.882
Test Loss of 418073002851.901001, Test MSE of 418072997754.678223
Epoch 4: training loss 422039880523.294
Test Loss of 400777453084.780029, Test MSE of 400777452518.505066
Epoch 5: training loss 385185253737.412
Test Loss of 345959878202.152222, Test MSE of 345959875607.643616
Epoch 6: training loss 317385101552.941
Test Loss of 268715151330.627808, Test MSE of 268715151287.179749
Epoch 7: training loss 225963255747.765
Test Loss of 169633527237.137177, Test MSE of 169633526257.919769
Epoch 8: training loss 161194006046.118
Test Loss of 130820040897.761734, Test MSE of 130820043141.667648
Epoch 9: training loss 141317569520.941
Test Loss of 120169323583.244965, Test MSE of 120169323424.736908
Epoch 10: training loss 135867821929.412
Test Loss of 116168205709.235260, Test MSE of 116168204889.473709
Epoch 11: training loss 133160080007.529
Test Loss of 113104475287.835297, Test MSE of 113104474495.869354
Epoch 12: training loss 130377803625.412
Test Loss of 110289716829.683090, Test MSE of 110289719376.495560
Epoch 13: training loss 125858291651.765
Test Loss of 106682793855.141342, Test MSE of 106682793522.297195
Epoch 14: training loss 122729208365.176
Test Loss of 103806465897.822815, Test MSE of 103806467903.298721
Epoch 15: training loss 118442235211.294
Test Loss of 101022786873.856125, Test MSE of 101022788762.204346
Epoch 16: training loss 115378993182.118
Test Loss of 97432987199.363403, Test MSE of 97432988988.113861
Epoch 17: training loss 111035304734.118
Test Loss of 94764860593.417542, Test MSE of 94764858989.266541
Epoch 18: training loss 108968103544.471
Test Loss of 90299045197.753418, Test MSE of 90299044969.732040
Epoch 19: training loss 103773022960.941
Test Loss of 87538082743.280136, Test MSE of 87538083375.261551
Epoch 20: training loss 99191714725.647
Test Loss of 83485209954.598190, Test MSE of 83485210810.445938
Epoch 21: training loss 95534741496.471
Test Loss of 81919599397.366638, Test MSE of 81919600687.619675
Epoch 22: training loss 92486772540.235
Test Loss of 76039143132.173035, Test MSE of 76039142198.672684
Epoch 23: training loss 88240412551.529
Test Loss of 72335658101.251907, Test MSE of 72335658680.933670
Epoch 24: training loss 85920840448.000
Test Loss of 72124857386.400192, Test MSE of 72124856091.372131
Epoch 25: training loss 82513549643.294
Test Loss of 70768112157.016891, Test MSE of 70768111341.777542
Epoch 26: training loss 78206976670.118
Test Loss of 67493864945.669212, Test MSE of 67493863706.379326
Epoch 27: training loss 74329776278.588
Test Loss of 63935914713.567429, Test MSE of 63935915554.396080
Epoch 28: training loss 70989799431.529
Test Loss of 60912447831.938934, Test MSE of 60912446921.087990
Epoch 29: training loss 68155126866.824
Test Loss of 58215164186.825813, Test MSE of 58215165349.888298
Epoch 30: training loss 64878896956.235
Test Loss of 56035263494.869301, Test MSE of 56035263060.237099
Epoch 31: training loss 61873370112.000
Test Loss of 49212385070.130928, Test MSE of 49212384109.750656
Epoch 32: training loss 59370009223.529
Test Loss of 51766523076.367340, Test MSE of 51766523510.277138
Epoch 33: training loss 56216205654.588
Test Loss of 49344189421.287071, Test MSE of 49344189457.846664
Epoch 34: training loss 53719145095.529
Test Loss of 44211951793.654404, Test MSE of 44211951517.956589
Epoch 35: training loss 51260911344.941
Test Loss of 41572815097.189911, Test MSE of 41572816072.445831
Epoch 36: training loss 49121273347.765
Test Loss of 41441952244.274811, Test MSE of 41441952783.368652
Epoch 37: training loss 45702070663.529
Test Loss of 39932625820.750404, Test MSE of 39932625918.575508
Epoch 38: training loss 43553771041.882
Test Loss of 39892776623.167244, Test MSE of 39892777032.084579
Epoch 39: training loss 42194708894.118
Test Loss of 33628201717.044643, Test MSE of 33628202494.844402
Epoch 40: training loss 39993416192.000
Test Loss of 35994533709.871849, Test MSE of 35994533584.140396
Epoch 41: training loss 37793703585.882
Test Loss of 34122580990.341892, Test MSE of 34122581323.406330
Epoch 42: training loss 35594531659.294
Test Loss of 32521735486.119823, Test MSE of 32521735097.908112
Epoch 43: training loss 34809226217.412
Test Loss of 29131846009.101086, Test MSE of 29131845734.041546
Epoch 44: training loss 32790571956.706
Test Loss of 28432321159.609531, Test MSE of 28432320960.936108
Epoch 45: training loss 31055023104.000
Test Loss of 27615657744.758732, Test MSE of 27615657625.298771
Epoch 46: training loss 30438398177.882
Test Loss of 29792202903.598427, Test MSE of 29792203150.732258
Epoch 47: training loss 28824101760.000
Test Loss of 25956110696.993755, Test MSE of 25956110649.404560
Epoch 48: training loss 27138080948.706
Test Loss of 24871030790.158688, Test MSE of 24871030764.474445
Epoch 49: training loss 25991394831.059
Test Loss of 23741495033.545223, Test MSE of 23741495384.798336
Epoch 50: training loss 25094016542.118
Test Loss of 25312122575.381912, Test MSE of 25312122611.036316
Epoch 51: training loss 24332941756.235
Test Loss of 21648931859.660419, Test MSE of 21648931695.316029
Epoch 52: training loss 23117283655.529
Test Loss of 21374875051.081192, Test MSE of 21374875575.054302
Epoch 53: training loss 22505842251.294
Test Loss of 21470323850.096691, Test MSE of 21470324588.745358
Epoch 54: training loss 21329279943.529
Test Loss of 20137540416.843857, Test MSE of 20137540455.565659
Epoch 55: training loss 21055883817.412
Test Loss of 21706949899.429100, Test MSE of 21706950114.034245
Epoch 56: training loss 20003017679.059
Test Loss of 21131211914.570438, Test MSE of 21131211851.858715
Epoch 57: training loss 19488922849.882
Test Loss of 21112020977.787647, Test MSE of 21112020970.556019
Epoch 58: training loss 18527869793.882
Test Loss of 19759137928.201714, Test MSE of 19759137956.004288
Epoch 59: training loss 17890544498.824
Test Loss of 19693693080.545918, Test MSE of 19693693138.093735
Epoch 60: training loss 17188413376.000
Test Loss of 18603115009.065926, Test MSE of 18603114873.055771
Epoch 61: training loss 16820660749.176
Test Loss of 20871956865.154755, Test MSE of 20871956640.734276
Epoch 62: training loss 16491366034.824
Test Loss of 19895572165.433266, Test MSE of 19895572301.164131
Epoch 63: training loss 16456754386.824
Test Loss of 18824821593.952347, Test MSE of 18824821665.037552
Epoch 64: training loss 15612829560.471
Test Loss of 19491833318.773075, Test MSE of 19491833714.819050
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19491833714.81905, 'MSE - std': 0.0, 'R2 - mean': 0.8482149760148329, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005800 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917020220.235
Test Loss of 424556411180.828125, Test MSE of 424556407783.925049
Epoch 2: training loss 427895217212.235
Test Loss of 424538890859.658569, Test MSE of 424538884377.869568
Epoch 3: training loss 427866412212.706
Test Loss of 424515777640.460815, Test MSE of 424515775716.764648
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888257385.412
Test Loss of 424523769447.158020, Test MSE of 424523766074.402527
Epoch 2: training loss 427874931410.824
Test Loss of 424524244427.769592, Test MSE of 424524244378.932129
Epoch 3: training loss 427874338695.529
Test Loss of 424524289983.096924, Test MSE of 424524291526.116699
Epoch 4: training loss 422128981413.647
Test Loss of 407086789535.592896, Test MSE of 407086788969.247498
Epoch 5: training loss 384635257795.765
Test Loss of 351950257747.734436, Test MSE of 351950258969.603699
Epoch 6: training loss 315671329731.765
Test Loss of 275216307689.141785, Test MSE of 275216303315.288940
Epoch 7: training loss 223168643734.588
Test Loss of 179900511575.228302, Test MSE of 179900509019.394348
Epoch 8: training loss 158757112892.235
Test Loss of 141190344015.174652, Test MSE of 141190344631.369965
Epoch 9: training loss 139304815766.588
Test Loss of 132009220211.120056, Test MSE of 132009219317.103897
Epoch 10: training loss 133335732886.588
Test Loss of 126770235424.688416, Test MSE of 126770236628.379547
Epoch 11: training loss 129997920195.765
Test Loss of 124377118632.357162, Test MSE of 124377119105.728806
Epoch 12: training loss 126189354797.176
Test Loss of 120880639646.586166, Test MSE of 120880640705.426544
Epoch 13: training loss 123411322277.647
Test Loss of 117520975646.971085, Test MSE of 117520976601.871277
Epoch 14: training loss 120085767107.765
Test Loss of 113977477023.829742, Test MSE of 113977475816.854782
Epoch 15: training loss 115118442496.000
Test Loss of 110687109447.831604, Test MSE of 110687113973.070267
Epoch 16: training loss 112032810676.706
Test Loss of 107919303777.828354, Test MSE of 107919303845.546616
Epoch 17: training loss 107025707640.471
Test Loss of 103639211877.559097, Test MSE of 103639212732.677948
Epoch 18: training loss 103807889648.941
Test Loss of 98652440249.589630, Test MSE of 98652440067.519379
Epoch 19: training loss 99160151612.235
Test Loss of 94082381814.051346, Test MSE of 94082382417.845917
Epoch 20: training loss 95204302998.588
Test Loss of 91052336326.262314, Test MSE of 91052336925.760757
Epoch 21: training loss 91623075282.824
Test Loss of 88424416194.650009, Test MSE of 88424415132.274323
Epoch 22: training loss 88730385648.941
Test Loss of 83530551103.659500, Test MSE of 83530551107.651215
Epoch 23: training loss 83690032941.176
Test Loss of 80705796740.767059, Test MSE of 80705794681.004944
Epoch 24: training loss 80252994816.000
Test Loss of 74867070688.436737, Test MSE of 74867071106.596115
Epoch 25: training loss 76729810311.529
Test Loss of 71976646611.941711, Test MSE of 71976647127.211288
Epoch 26: training loss 73804826112.000
Test Loss of 67843904568.138794, Test MSE of 67843904117.961372
Epoch 27: training loss 68226107738.353
Test Loss of 67923888411.773308, Test MSE of 67923887526.867554
Epoch 28: training loss 64892650480.941
Test Loss of 63788705796.263702, Test MSE of 63788707583.589447
Epoch 29: training loss 62781070215.529
Test Loss of 60891816774.528801, Test MSE of 60891817718.410866
Epoch 30: training loss 59720801641.412
Test Loss of 58308880840.216515, Test MSE of 58308881413.733192
Epoch 31: training loss 55446161317.647
Test Loss of 51454499981.412910, Test MSE of 51454501746.417778
Epoch 32: training loss 52815235644.235
Test Loss of 48218550765.642380, Test MSE of 48218551910.823288
Epoch 33: training loss 50473096101.647
Test Loss of 48383730129.691422, Test MSE of 48383731055.044395
Epoch 34: training loss 47044684400.941
Test Loss of 47056047040.755028, Test MSE of 47056047603.702736
Epoch 35: training loss 45520315798.588
Test Loss of 45244635561.423088, Test MSE of 45244635486.923378
Epoch 36: training loss 43177599924.706
Test Loss of 43241989121.184364, Test MSE of 43241989964.346504
Epoch 37: training loss 40217950607.059
Test Loss of 41028563037.327782, Test MSE of 41028564011.566246
Epoch 38: training loss 38277470923.294
Test Loss of 42467627898.167015, Test MSE of 42467627212.324730
Epoch 39: training loss 36249048199.529
Test Loss of 35770051791.737221, Test MSE of 35770052033.935493
Epoch 40: training loss 34318063073.882
Test Loss of 32745313528.479298, Test MSE of 32745313925.415989
Epoch 41: training loss 32320133805.176
Test Loss of 34015612229.699745, Test MSE of 34015612256.194469
Epoch 42: training loss 30504374226.824
Test Loss of 32845008953.796902, Test MSE of 32845008925.996544
Epoch 43: training loss 29249711917.176
Test Loss of 26700053640.675457, Test MSE of 26700054337.347416
Epoch 44: training loss 28236338650.353
Test Loss of 30810758554.026371, Test MSE of 30810757927.414173
Epoch 45: training loss 26014550106.353
Test Loss of 28824206854.987740, Test MSE of 28824206701.002003
Epoch 46: training loss 25316413530.353
Test Loss of 33849003208.631042, Test MSE of 33849003778.674374
Epoch 47: training loss 23786867817.412
Test Loss of 28837657621.081657, Test MSE of 28837658292.584084
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24164746003.70157, 'MSE - std': 4672912288.882517, 'R2 - mean': 0.8211666209726614, 'R2 - std': 0.027048355042171457} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003544 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927031868.235
Test Loss of 447258216745.985657, Test MSE of 447258214502.128601
Epoch 2: training loss 421906366223.059
Test Loss of 447239682367.777954, Test MSE of 447239686422.845703
Epoch 3: training loss 421878953863.529
Test Loss of 447215083324.817017, Test MSE of 447215082717.260376
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900672301.176
Test Loss of 447224821158.817505, Test MSE of 447224815291.079224
Epoch 2: training loss 421888072884.706
Test Loss of 447225706690.709229, Test MSE of 447225705576.543213
Epoch 3: training loss 421887561246.118
Test Loss of 447225268404.496887, Test MSE of 447225273034.456299
Epoch 4: training loss 416529663879.529
Test Loss of 430084827993.004883, Test MSE of 430084837593.791443
Epoch 5: training loss 380129299516.235
Test Loss of 374416775012.137878, Test MSE of 374416782568.073242
Epoch 6: training loss 312168508114.824
Test Loss of 295863509740.517212, Test MSE of 295863513450.258057
Epoch 7: training loss 220121450556.235
Test Loss of 195840589175.916718, Test MSE of 195840590377.873627
Epoch 8: training loss 156832110320.941
Test Loss of 154022787505.476746, Test MSE of 154022787483.297607
Epoch 9: training loss 136715763862.588
Test Loss of 142657527275.510529, Test MSE of 142657525992.643097
Epoch 10: training loss 132969488082.824
Test Loss of 137196972707.086746, Test MSE of 137196975539.496506
Epoch 11: training loss 128851624086.588
Test Loss of 134778572164.234100, Test MSE of 134778574078.885986
Epoch 12: training loss 125926465475.765
Test Loss of 131583668560.359009, Test MSE of 131583669716.054855
Epoch 13: training loss 122303241908.706
Test Loss of 127756017415.757568, Test MSE of 127756016342.169220
Epoch 14: training loss 119758498996.706
Test Loss of 123740863996.565353, Test MSE of 123740863890.865784
Epoch 15: training loss 114811302520.471
Test Loss of 121104215521.088135, Test MSE of 121104215675.814560
Epoch 16: training loss 111284302486.588
Test Loss of 115949253948.698593, Test MSE of 115949253401.853668
Epoch 17: training loss 107051931437.176
Test Loss of 113214344712.882721, Test MSE of 113214344417.746002
Epoch 18: training loss 102330511631.059
Test Loss of 109633353718.051346, Test MSE of 109633353002.044357
Epoch 19: training loss 99667469010.824
Test Loss of 104904137256.860519, Test MSE of 104904140910.646835
Epoch 20: training loss 95972724374.588
Test Loss of 103718956016.603287, Test MSE of 103718959911.352341
Epoch 21: training loss 92022989492.706
Test Loss of 97794641062.521393, Test MSE of 97794640841.969437
Epoch 22: training loss 88839910400.000
Test Loss of 93669113742.301178, Test MSE of 93669114331.820068
Epoch 23: training loss 84162845455.059
Test Loss of 89754281906.779556, Test MSE of 89754281663.320679
Epoch 24: training loss 81509931866.353
Test Loss of 86802317679.626190, Test MSE of 86802318184.327988
Epoch 25: training loss 77207268984.471
Test Loss of 84629180854.687943, Test MSE of 84629181179.468018
Epoch 26: training loss 73748672286.118
Test Loss of 83164286391.398560, Test MSE of 83164288322.248932
Epoch 27: training loss 70111941300.706
Test Loss of 77522058089.112198, Test MSE of 77522058704.267258
Epoch 28: training loss 66821170311.529
Test Loss of 71200248892.876236, Test MSE of 71200250491.126678
Epoch 29: training loss 64157365097.412
Test Loss of 68642380819.660423, Test MSE of 68642380143.179985
Epoch 30: training loss 61889827117.176
Test Loss of 63782502598.972939, Test MSE of 63782501731.519760
Epoch 31: training loss 58545367341.176
Test Loss of 64912185360.581078, Test MSE of 64912186264.661354
Epoch 32: training loss 54851741319.529
Test Loss of 59935367003.136711, Test MSE of 59935366926.301239
Epoch 33: training loss 53383747621.647
Test Loss of 58550687608.982651, Test MSE of 58550687859.018631
Epoch 34: training loss 50079015695.059
Test Loss of 57493046045.786720, Test MSE of 57493045379.255959
Epoch 35: training loss 46942724555.294
Test Loss of 53023027163.047882, Test MSE of 53023026219.657516
Epoch 36: training loss 45236945739.294
Test Loss of 53247435809.399025, Test MSE of 53247435964.581154
Epoch 37: training loss 43179367183.059
Test Loss of 47705534657.287994, Test MSE of 47705535053.708908
Epoch 38: training loss 40977631488.000
Test Loss of 43397348896.569977, Test MSE of 43397349404.064613
Epoch 39: training loss 38889217076.706
Test Loss of 43807447731.194077, Test MSE of 43807448067.789902
Epoch 40: training loss 36804954767.059
Test Loss of 42366800216.886421, Test MSE of 42366800234.637177
Epoch 41: training loss 34873988291.765
Test Loss of 41339438465.865372, Test MSE of 41339438175.924110
Epoch 42: training loss 33902353882.353
Test Loss of 40886375482.033775, Test MSE of 40886375310.077133
Epoch 43: training loss 32208723606.588
Test Loss of 36053953704.416374, Test MSE of 36053953973.342865
Epoch 44: training loss 30092680651.294
Test Loss of 38045636109.146423, Test MSE of 38045636164.359337
Epoch 45: training loss 28860050447.059
Test Loss of 30261663967.133934, Test MSE of 30261664325.279308
Epoch 46: training loss 27733199198.118
Test Loss of 30749936418.997917, Test MSE of 30749936761.723190
Epoch 47: training loss 26523985584.941
Test Loss of 34054696490.044876, Test MSE of 34054695921.485428
Epoch 48: training loss 25733292822.588
Test Loss of 31790502359.850105, Test MSE of 31790502411.008240
Epoch 49: training loss 24390084717.176
Test Loss of 31977905883.225536, Test MSE of 31977905951.228863
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26769132652.877335, 'MSE - std': 5303118498.569445, 'R2 - mean': 0.809819477461129, 'R2 - std': 0.027299407906903873} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005302 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111160440.471
Test Loss of 410764889875.072632, Test MSE of 410764893304.742004
Epoch 2: training loss 430090108205.176
Test Loss of 410747006811.098572, Test MSE of 410747007365.931763
Epoch 3: training loss 430061937362.824
Test Loss of 410723805414.293396, Test MSE of 410723807587.377869
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078223540.706
Test Loss of 410728775763.398438, Test MSE of 410728773707.482605
Epoch 2: training loss 430067015439.059
Test Loss of 410729639968.222107, Test MSE of 410729640420.407043
Epoch 3: training loss 430066571384.471
Test Loss of 410729214393.632568, Test MSE of 410729214984.138855
Epoch 4: training loss 424589264534.588
Test Loss of 393789996934.693176, Test MSE of 393789996418.991211
Epoch 5: training loss 387896583589.647
Test Loss of 338842898665.136536, Test MSE of 338842894579.892700
Epoch 6: training loss 319283279631.059
Test Loss of 261463965195.609436, Test MSE of 261463962598.179474
Epoch 7: training loss 227588466989.176
Test Loss of 164145951267.776031, Test MSE of 164145949922.329346
Epoch 8: training loss 163288518957.176
Test Loss of 124944779452.594162, Test MSE of 124944778705.732208
Epoch 9: training loss 144095531760.941
Test Loss of 114318599004.520126, Test MSE of 114318598992.906662
Epoch 10: training loss 139084084254.118
Test Loss of 109636837694.430359, Test MSE of 109636836893.722595
Epoch 11: training loss 135878599800.471
Test Loss of 108299306298.165665, Test MSE of 108299306610.996323
Epoch 12: training loss 131268663898.353
Test Loss of 105005297749.293839, Test MSE of 105005295306.345673
Epoch 13: training loss 129189022147.765
Test Loss of 101837272474.832016, Test MSE of 101837272606.520874
Epoch 14: training loss 125097397127.529
Test Loss of 99602837828.116608, Test MSE of 99602839480.513321
Epoch 15: training loss 122228610499.765
Test Loss of 96631665828.427582, Test MSE of 96631667530.124756
Epoch 16: training loss 117460739975.529
Test Loss of 92598273793.066177, Test MSE of 92598274738.927689
Epoch 17: training loss 113142173033.412
Test Loss of 89991049176.196213, Test MSE of 89991050049.197220
Epoch 18: training loss 110281386496.000
Test Loss of 86808609397.279037, Test MSE of 86808611578.057114
Epoch 19: training loss 105673194736.941
Test Loss of 84096653976.344284, Test MSE of 84096654616.742996
Epoch 20: training loss 101261647194.353
Test Loss of 81377721988.916245, Test MSE of 81377721249.588028
Epoch 21: training loss 97933244521.412
Test Loss of 77948502175.215179, Test MSE of 77948502065.974945
Epoch 22: training loss 94426096986.353
Test Loss of 75302312763.350296, Test MSE of 75302313579.065964
Epoch 23: training loss 90132623570.824
Test Loss of 73052366380.779266, Test MSE of 73052366246.890945
Epoch 24: training loss 86643804882.824
Test Loss of 68361048062.104584, Test MSE of 68361047515.910080
Epoch 25: training loss 83675176688.941
Test Loss of 66113001529.810272, Test MSE of 66113002406.976051
Epoch 26: training loss 79301815461.647
Test Loss of 63663130528.281349, Test MSE of 63663130187.987808
Epoch 27: training loss 76733108073.412
Test Loss of 61870726732.053680, Test MSE of 61870727478.103233
Epoch 28: training loss 72843723248.941
Test Loss of 57128331266.843124, Test MSE of 57128330902.201141
Epoch 29: training loss 69682417543.529
Test Loss of 53812959153.340118, Test MSE of 53812958370.322723
Epoch 30: training loss 66393101793.882
Test Loss of 52930045168.244331, Test MSE of 52930045165.841255
Epoch 31: training loss 62645270648.471
Test Loss of 53395333447.907448, Test MSE of 53395334035.501808
Epoch 32: training loss 60122749522.824
Test Loss of 46628396203.061546, Test MSE of 46628397258.680344
Epoch 33: training loss 57120294836.706
Test Loss of 47268059517.926888, Test MSE of 47268059939.830521
Epoch 34: training loss 55003420370.824
Test Loss of 44883635204.264694, Test MSE of 44883635778.437912
Epoch 35: training loss 51912085353.412
Test Loss of 41871017397.367882, Test MSE of 41871016726.321144
Epoch 36: training loss 49267402149.647
Test Loss of 38930745621.678856, Test MSE of 38930746010.128662
Epoch 37: training loss 46678825758.118
Test Loss of 38506191910.382233, Test MSE of 38506192210.744087
Epoch 38: training loss 44380701402.353
Test Loss of 34844160557.253120, Test MSE of 34844160929.443039
Epoch 39: training loss 42649162563.765
Test Loss of 34945559292.801483, Test MSE of 34945559678.615845
Epoch 40: training loss 40818052374.588
Test Loss of 32307220061.112450, Test MSE of 32307220245.110760
Epoch 41: training loss 38454590464.000
Test Loss of 31996081344.858860, Test MSE of 31996081429.364555
Epoch 42: training loss 36676714081.882
Test Loss of 30139521438.149006, Test MSE of 30139522621.536575
Epoch 43: training loss 35099375344.941
Test Loss of 26413681093.952801, Test MSE of 26413681707.181564
Epoch 44: training loss 33061621240.471
Test Loss of 28955607330.946785, Test MSE of 28955607548.958160
Epoch 45: training loss 31717996585.412
Test Loss of 26394693841.443775, Test MSE of 26394693662.666203
Epoch 46: training loss 30332108016.941
Test Loss of 26480726871.781582, Test MSE of 26480726653.887848
Epoch 47: training loss 28793202763.294
Test Loss of 26541471857.725128, Test MSE of 26541471285.831516
Epoch 48: training loss 27764555907.765
Test Loss of 24165868108.527534, Test MSE of 24165868172.142326
Epoch 49: training loss 26492015055.059
Test Loss of 23134438817.465988, Test MSE of 23134438515.725281
Epoch 50: training loss 25821260141.176
Test Loss of 23013566795.698288, Test MSE of 23013567098.745426
Epoch 51: training loss 24636301985.882
Test Loss of 21794317204.434982, Test MSE of 21794316266.547707
Epoch 52: training loss 23744120146.824
Test Loss of 21909764346.195278, Test MSE of 21909764486.592403
Epoch 53: training loss 22844313671.529
Test Loss of 22341908766.208237, Test MSE of 22341908631.612358
Epoch 54: training loss 21873775092.706
Test Loss of 21441134991.933365, Test MSE of 21441134372.376228
Epoch 55: training loss 21183275448.471
Test Loss of 21255652876.557148, Test MSE of 21255653168.178925
Epoch 56: training loss 20566384139.294
Test Loss of 19674669093.908375, Test MSE of 19674669158.667141
Epoch 57: training loss 19724655597.176
Test Loss of 20270566937.825081, Test MSE of 20270566952.854431
Epoch 58: training loss 19268295966.118
Test Loss of 22082477547.387321, Test MSE of 22082477634.849453
Epoch 59: training loss 18572244544.000
Test Loss of 20015859456.118465, Test MSE of 20015859776.897789
Epoch 60: training loss 17784793103.059
Test Loss of 19816107981.771400, Test MSE of 19816108240.890846
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25030876549.880714, 'MSE - std': 5491530041.456305, 'R2 - mean': 0.8164764409228145, 'R2 - std': 0.02630377808683164} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043121603.765
Test Loss of 431611375587.568726, Test MSE of 431611375680.145752
Epoch 2: training loss 424023201671.529
Test Loss of 431592493804.690430, Test MSE of 431592492385.372498
Epoch 3: training loss 423996038806.588
Test Loss of 431566140569.055054, Test MSE of 431566142782.762817
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011337607.529
Test Loss of 431568629845.293823, Test MSE of 431568632449.439392
Epoch 2: training loss 423999989157.647
Test Loss of 431571189070.067566, Test MSE of 431571200788.994446
Epoch 3: training loss 423999436920.471
Test Loss of 431570465990.545105, Test MSE of 431570463805.883423
Epoch 4: training loss 418661035791.059
Test Loss of 414153251935.244812, Test MSE of 414153256828.499329
Epoch 5: training loss 382400009396.706
Test Loss of 358756588836.368347, Test MSE of 358756597214.242371
Epoch 6: training loss 314857432064.000
Test Loss of 280462726604.112915, Test MSE of 280462721141.686279
Epoch 7: training loss 223215593592.471
Test Loss of 180467902065.014343, Test MSE of 180467904533.026733
Epoch 8: training loss 159401151518.118
Test Loss of 138374597050.580292, Test MSE of 138374592685.888092
Epoch 9: training loss 141086955098.353
Test Loss of 126529893737.551132, Test MSE of 126529891719.704605
Epoch 10: training loss 135330409607.529
Test Loss of 121190577869.889862, Test MSE of 121190578748.817734
Epoch 11: training loss 131519581214.118
Test Loss of 117285356016.599716, Test MSE of 117285355534.788574
Epoch 12: training loss 129029686482.824
Test Loss of 114992969770.646927, Test MSE of 114992969492.012695
Epoch 13: training loss 125490425072.941
Test Loss of 111817959468.542343, Test MSE of 111817962665.537766
Epoch 14: training loss 121722892122.353
Test Loss of 107589615149.726974, Test MSE of 107589615360.619324
Epoch 15: training loss 117429346183.529
Test Loss of 104084507407.755676, Test MSE of 104084505810.364349
Epoch 16: training loss 113633623913.412
Test Loss of 99724907278.807953, Test MSE of 99724905746.229980
Epoch 17: training loss 109010248372.706
Test Loss of 97049580712.218414, Test MSE of 97049581663.052673
Epoch 18: training loss 106071634002.824
Test Loss of 93356946789.760300, Test MSE of 93356947805.844696
Epoch 19: training loss 102705611821.176
Test Loss of 89366558417.680710, Test MSE of 89366559329.838486
Epoch 20: training loss 98075081050.353
Test Loss of 85021122404.575653, Test MSE of 85021124332.269089
Epoch 21: training loss 94267340092.235
Test Loss of 81212482931.028229, Test MSE of 81212481243.104553
Epoch 22: training loss 91060723621.647
Test Loss of 76890950918.041641, Test MSE of 76890948332.455795
Epoch 23: training loss 86710653839.059
Test Loss of 73441863358.726517, Test MSE of 73441864303.286316
Epoch 24: training loss 83545711043.765
Test Loss of 69616239592.781113, Test MSE of 69616239980.806168
Epoch 25: training loss 79641588314.353
Test Loss of 67376780653.341972, Test MSE of 67376781474.060287
Epoch 26: training loss 76178848888.471
Test Loss of 65078098984.277649, Test MSE of 65078098731.730942
Epoch 27: training loss 72406460536.471
Test Loss of 61961323940.309113, Test MSE of 61961323591.162178
Epoch 28: training loss 69277182177.882
Test Loss of 60024535075.065247, Test MSE of 60024535633.686172
Epoch 29: training loss 66267513216.000
Test Loss of 57447303620.057381, Test MSE of 57447303768.606018
Epoch 30: training loss 63171673313.882
Test Loss of 49622346877.571495, Test MSE of 49622345526.402931
Epoch 31: training loss 60272837097.412
Test Loss of 45807144664.314667, Test MSE of 45807144944.251305
Epoch 32: training loss 57702420208.941
Test Loss of 47359846427.483574, Test MSE of 47359846408.091217
Epoch 33: training loss 54652696786.824
Test Loss of 45327670858.158257, Test MSE of 45327671063.452103
Epoch 34: training loss 52063842906.353
Test Loss of 40566067495.685333, Test MSE of 40566067681.031181
Epoch 35: training loss 49534627764.706
Test Loss of 40707134287.252197, Test MSE of 40707134586.010742
Epoch 36: training loss 46937655446.588
Test Loss of 41746168661.886162, Test MSE of 41746169192.607559
Epoch 37: training loss 44645545095.529
Test Loss of 40540992541.852844, Test MSE of 40540992161.878036
Epoch 38: training loss 42454922691.765
Test Loss of 34964014851.909302, Test MSE of 34964015281.856735
Epoch 39: training loss 40747079085.176
Test Loss of 35502402149.167976, Test MSE of 35502403175.030815
Epoch 40: training loss 38673087465.412
Test Loss of 29594311634.983803, Test MSE of 29594311887.818836
Epoch 41: training loss 36449766113.882
Test Loss of 31740638660.531235, Test MSE of 31740638112.632286
Epoch 42: training loss 35118640730.353
Test Loss of 28413247394.176769, Test MSE of 28413247048.538269
Epoch 43: training loss 33419940306.824
Test Loss of 30666131730.835724, Test MSE of 30666131869.860653
Epoch 44: training loss 32362239653.647
Test Loss of 27112651918.630264, Test MSE of 27112651536.525162
Epoch 45: training loss 30413979821.176
Test Loss of 28948040812.512726, Test MSE of 28948040618.873314
Epoch 46: training loss 28990229504.000
Test Loss of 27843879278.763535, Test MSE of 27843879541.685318
Epoch 47: training loss 27784077590.588
Test Loss of 24328403017.921333, Test MSE of 24328403492.793457
Epoch 48: training loss 26839150915.765
Test Loss of 27025804379.453957, Test MSE of 27025804077.951057
Epoch 49: training loss 25966952884.706
Test Loss of 25544976826.580288, Test MSE of 25544976373.497013
Epoch 50: training loss 24477819745.882
Test Loss of 26283861615.118927, Test MSE of 26283861734.489365
Epoch 51: training loss 23774525522.824
Test Loss of 24788909726.504395, Test MSE of 24788909572.763737
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24982483154.457317, 'MSE - std': 4912727291.419308, 'R2 - mean': 0.8161562533316233, 'R2 - std': 0.02353552790330479} 
 

Saving model.....
Results After CV: {'MSE - mean': 24982483154.457317, 'MSE - std': 4912727291.419308, 'R2 - mean': 0.8161562533316233, 'R2 - std': 0.02353552790330479}
Train time: 82.96671098100015
Inference time: 0.097206797599938
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 25 finished with value: 24982483154.457317 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005796 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427523150185.412
Test Loss of 418110504399.085815, Test MSE of 418110506309.551208
Epoch 2: training loss 427500640015.059
Test Loss of 418091970445.353699, Test MSE of 418091967570.125244
Epoch 3: training loss 427473039480.471
Test Loss of 418068059719.890808, Test MSE of 418068063650.497986
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492604385.882
Test Loss of 418070884036.485779, Test MSE of 418070877957.294434
Epoch 2: training loss 427480390354.824
Test Loss of 418072438845.113098, Test MSE of 418072437344.984741
Epoch 3: training loss 423552131192.471
Test Loss of 405799141186.501953, Test MSE of 405799141338.740662
Epoch 4: training loss 396717704131.765
Test Loss of 364018857507.886169, Test MSE of 364018858483.197327
Epoch 5: training loss 332432418936.471
Test Loss of 279667388543.911194, Test MSE of 279667384825.179138
Epoch 6: training loss 250937073664.000
Test Loss of 202756359822.952576, Test MSE of 202756367026.952087
Epoch 7: training loss 183078878659.765
Test Loss of 145793590553.641449, Test MSE of 145793590367.382080
Epoch 8: training loss 148555578142.118
Test Loss of 123381725632.636597, Test MSE of 123381728080.425888
Epoch 9: training loss 137827540028.235
Test Loss of 117845826899.912094, Test MSE of 117845828234.350220
Epoch 10: training loss 135455219862.588
Test Loss of 114872692282.152206, Test MSE of 114872693531.662262
Epoch 11: training loss 132433742546.824
Test Loss of 112277587401.874619, Test MSE of 112277586714.110367
Epoch 12: training loss 130160455770.353
Test Loss of 109353513728.414520, Test MSE of 109353514665.703125
Epoch 13: training loss 126720953856.000
Test Loss of 107060853533.312973, Test MSE of 107060853005.584351
Epoch 14: training loss 123970900464.941
Test Loss of 104449756364.421005, Test MSE of 104449756971.626526
Epoch 15: training loss 120288870415.059
Test Loss of 101220719899.062683, Test MSE of 101220721774.786407
Epoch 16: training loss 116438209264.941
Test Loss of 98510162653.831131, Test MSE of 98510163111.159821
Epoch 17: training loss 113426373270.588
Test Loss of 96290709727.607681, Test MSE of 96290707086.286667
Epoch 18: training loss 110933067038.118
Test Loss of 94203989528.042557, Test MSE of 94203991005.042480
Epoch 19: training loss 107962643471.059
Test Loss of 91572004258.080032, Test MSE of 91572004089.312393
Epoch 20: training loss 104631624960.000
Test Loss of 89712543996.506134, Test MSE of 89712544089.172531
Epoch 21: training loss 101652489743.059
Test Loss of 86399403952.884567, Test MSE of 86399403284.694214
Epoch 22: training loss 97975593577.412
Test Loss of 82508296388.130463, Test MSE of 82508297475.052444
Epoch 23: training loss 95463509097.412
Test Loss of 79800421127.520706, Test MSE of 79800420370.054123
Epoch 24: training loss 91604309165.176
Test Loss of 77874179695.211655, Test MSE of 77874178952.394485
Epoch 25: training loss 88769642962.824
Test Loss of 74575701063.298630, Test MSE of 74575701263.960007
Epoch 26: training loss 85454109989.647
Test Loss of 73428930153.526718, Test MSE of 73428929974.671280
Epoch 27: training loss 82274779271.529
Test Loss of 68415278826.385384, Test MSE of 68415279518.478706
Epoch 28: training loss 79184994168.471
Test Loss of 64449101886.534348, Test MSE of 64449101937.881256
Epoch 29: training loss 76254730398.118
Test Loss of 65252721955.116356, Test MSE of 65252722155.958130
Epoch 30: training loss 72932181880.471
Test Loss of 63655250526.867455, Test MSE of 63655249514.690002
Epoch 31: training loss 69622141492.706
Test Loss of 59387006601.741386, Test MSE of 59387006213.221146
Epoch 32: training loss 67651800591.059
Test Loss of 56183817844.422852, Test MSE of 56183817963.791939
Epoch 33: training loss 64262311664.941
Test Loss of 54593320084.045341, Test MSE of 54593320341.637184
Epoch 34: training loss 62428379301.647
Test Loss of 54177727413.385147, Test MSE of 54177727945.905670
Epoch 35: training loss 59796051712.000
Test Loss of 49339684462.737915, Test MSE of 49339684973.450974
Epoch 36: training loss 57071062573.176
Test Loss of 50620575167.925980, Test MSE of 50620575305.607201
Epoch 37: training loss 55073107087.059
Test Loss of 50674355876.271111, Test MSE of 50674355705.072044
Epoch 38: training loss 52365914616.471
Test Loss of 47410033014.258614, Test MSE of 47410032684.592110
Epoch 39: training loss 50016724841.412
Test Loss of 45250568343.598427, Test MSE of 45250568271.298332
Epoch 40: training loss 47265603041.882
Test Loss of 41895869058.872078, Test MSE of 41895869190.405266
Epoch 41: training loss 44639079582.118
Test Loss of 39666194374.676842, Test MSE of 39666194488.870667
Epoch 42: training loss 43091548333.176
Test Loss of 38887245171.653015, Test MSE of 38887244777.188080
Epoch 43: training loss 41608038927.059
Test Loss of 35685035644.476524, Test MSE of 35685035857.323143
Epoch 44: training loss 39240515508.706
Test Loss of 35053277090.198471, Test MSE of 35053277091.106033
Epoch 45: training loss 37328781357.176
Test Loss of 33869093658.470505, Test MSE of 33869094219.965206
Epoch 46: training loss 35668091745.882
Test Loss of 31462372269.805229, Test MSE of 31462372787.949757
Epoch 47: training loss 34032268754.824
Test Loss of 29010213570.353920, Test MSE of 29010213529.621593
Epoch 48: training loss 32826760606.118
Test Loss of 26639060081.461948, Test MSE of 26639060425.254642
Epoch 49: training loss 31067734686.118
Test Loss of 27055657182.660191, Test MSE of 27055657584.410450
Epoch 50: training loss 29939211068.235
Test Loss of 26408085922.316910, Test MSE of 26408085786.043240
Epoch 51: training loss 28190077891.765
Test Loss of 26215400667.107101, Test MSE of 26215401154.896839
Epoch 52: training loss 27388335201.882
Test Loss of 24277373363.371731, Test MSE of 24277373253.342136
Epoch 53: training loss 25995483787.294
Test Loss of 25182158895.848255, Test MSE of 25182158786.216053
Epoch 54: training loss 25256537244.235
Test Loss of 27310876662.051353, Test MSE of 27310876197.467739
Epoch 55: training loss 24107993709.176
Test Loss of 24752509291.362480, Test MSE of 24752509185.619202
Epoch 56: training loss 23352740544.000
Test Loss of 22234208842.496414, Test MSE of 22234208834.626476
Epoch 57: training loss 22270637974.588
Test Loss of 25413888679.824196, Test MSE of 25413888300.613167
Epoch 58: training loss 21539956193.882
Test Loss of 24198918711.546612, Test MSE of 24198919005.703007
Epoch 59: training loss 20664951092.706
Test Loss of 21971081233.528568, Test MSE of 21971081405.146454
Epoch 60: training loss 19800980739.765
Test Loss of 21629405656.797596, Test MSE of 21629405765.980595
Epoch 61: training loss 19276489754.353
Test Loss of 21899529285.403656, Test MSE of 21899529546.373520
Epoch 62: training loss 18795312459.294
Test Loss of 18867197321.445293, Test MSE of 18867197746.860050
Epoch 63: training loss 18142983947.294
Test Loss of 20055711094.495491, Test MSE of 20055710999.883385
Epoch 64: training loss 17347722812.235
Test Loss of 20328318965.103863, Test MSE of 20328318975.704067
Epoch 65: training loss 17173653044.706
Test Loss of 21026986129.439739, Test MSE of 21026986154.567963
Epoch 66: training loss 16428990893.176
Test Loss of 20014461377.584084, Test MSE of 20014461812.689617
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20014461812.689617, 'MSE - std': 0.0, 'R2 - mean': 0.8441452143119975, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005376 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916271856.941
Test Loss of 424555357730.938721, Test MSE of 424555369395.653015
Epoch 2: training loss 427894450296.471
Test Loss of 424539083109.203796, Test MSE of 424539083592.407715
Epoch 3: training loss 427866868796.235
Test Loss of 424517006823.246826, Test MSE of 424517003995.448425
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888589402.353
Test Loss of 424522794017.399048, Test MSE of 424522802801.866394
Epoch 2: training loss 427875998057.412
Test Loss of 424523115186.246582, Test MSE of 424523112029.482056
Epoch 3: training loss 424192130108.235
Test Loss of 413025123014.143860, Test MSE of 413025123465.102966
Epoch 4: training loss 397815207333.647
Test Loss of 372474647199.770508, Test MSE of 372474655592.205261
Epoch 5: training loss 333492990192.941
Test Loss of 290402242382.819336, Test MSE of 290402245336.501709
Epoch 6: training loss 251244318358.588
Test Loss of 213705750847.777924, Test MSE of 213705748193.950287
Epoch 7: training loss 182191447100.235
Test Loss of 155900066881.139954, Test MSE of 155900063533.069794
Epoch 8: training loss 147886823966.118
Test Loss of 135904887494.617630, Test MSE of 135904885436.338196
Epoch 9: training loss 136907786360.471
Test Loss of 130257556004.596802, Test MSE of 130257556414.930206
Epoch 10: training loss 133379698477.176
Test Loss of 127356114325.762665, Test MSE of 127356112638.387299
Epoch 11: training loss 130505825942.588
Test Loss of 124738376730.766602, Test MSE of 124738377841.747452
Epoch 12: training loss 127817456519.529
Test Loss of 121782081717.444366, Test MSE of 121782077781.209961
Epoch 13: training loss 125265564852.706
Test Loss of 119457844466.320618, Test MSE of 119457844873.869324
Epoch 14: training loss 122259701609.412
Test Loss of 117182129054.645386, Test MSE of 117182130232.395203
Epoch 15: training loss 119424039062.588
Test Loss of 114723078168.634750, Test MSE of 114723076296.074463
Epoch 16: training loss 116904466371.765
Test Loss of 111280036739.641922, Test MSE of 111280038428.532059
Epoch 17: training loss 112541817765.647
Test Loss of 109045060561.809860, Test MSE of 109045060475.983551
Epoch 18: training loss 109601894279.529
Test Loss of 104819057732.456161, Test MSE of 104819058759.698990
Epoch 19: training loss 105871105204.706
Test Loss of 102700796771.427246, Test MSE of 102700798578.051987
Epoch 20: training loss 104107209125.647
Test Loss of 99742974031.589172, Test MSE of 99742971984.527496
Epoch 21: training loss 100455445654.588
Test Loss of 95993858602.755493, Test MSE of 95993858181.603790
Epoch 22: training loss 97839630893.176
Test Loss of 93503641279.274582, Test MSE of 93503640168.251450
Epoch 23: training loss 94136795587.765
Test Loss of 90170638598.454773, Test MSE of 90170640646.024200
Epoch 24: training loss 91103496161.882
Test Loss of 86232442339.456863, Test MSE of 86232442707.390869
Epoch 25: training loss 87152352948.706
Test Loss of 83128079293.912567, Test MSE of 83128079553.869125
Epoch 26: training loss 84248606087.529
Test Loss of 83912333624.198013, Test MSE of 83912333488.393265
Epoch 27: training loss 81496006716.235
Test Loss of 79763611824.233170, Test MSE of 79763609823.835861
Epoch 28: training loss 78731775593.412
Test Loss of 75702982558.882263, Test MSE of 75702981323.221085
Epoch 29: training loss 75625003896.471
Test Loss of 76030235975.120987, Test MSE of 76030237203.365051
Epoch 30: training loss 71991656960.000
Test Loss of 71316204543.763123, Test MSE of 71316205451.304764
Epoch 31: training loss 68914215981.176
Test Loss of 69476702288.062912, Test MSE of 69476702932.341492
Epoch 32: training loss 66122569788.235
Test Loss of 68375302420.667130, Test MSE of 68375301115.994347
Epoch 33: training loss 63863992214.588
Test Loss of 65487265127.809395, Test MSE of 65487267534.469582
Epoch 34: training loss 60568603873.882
Test Loss of 61535109900.731895, Test MSE of 61535112483.113586
Epoch 35: training loss 58093902983.529
Test Loss of 62678428631.968544, Test MSE of 62678428223.911362
Epoch 36: training loss 56504700664.471
Test Loss of 56316939036.365486, Test MSE of 56316938566.423706
Epoch 37: training loss 53594378428.235
Test Loss of 56489298215.143188, Test MSE of 56489297747.789505
Epoch 38: training loss 50706144316.235
Test Loss of 55992650996.215591, Test MSE of 55992651022.959587
Epoch 39: training loss 48705804446.118
Test Loss of 51290047577.300949, Test MSE of 51290047603.278839
Epoch 40: training loss 45532359777.882
Test Loss of 49920299764.570900, Test MSE of 49920300439.320358
Epoch 41: training loss 44033392534.588
Test Loss of 49154458892.376587, Test MSE of 49154458907.783798
Epoch 42: training loss 43137060562.824
Test Loss of 44482303961.152901, Test MSE of 44482302991.590370
Epoch 43: training loss 40142045974.588
Test Loss of 45423604289.495255, Test MSE of 45423604380.912979
Epoch 44: training loss 38199580528.941
Test Loss of 41590707780.574600, Test MSE of 41590707179.655724
Epoch 45: training loss 36299259384.471
Test Loss of 41102359060.489471, Test MSE of 41102358994.303093
Epoch 46: training loss 34251095936.000
Test Loss of 40032817328.233170, Test MSE of 40032817578.235786
Epoch 47: training loss 33108838791.529
Test Loss of 37396742088.098083, Test MSE of 37396742854.405724
Epoch 48: training loss 31720457923.765
Test Loss of 36214050151.809395, Test MSE of 36214049416.195915
Epoch 49: training loss 29656795301.647
Test Loss of 34103121859.834373, Test MSE of 34103122072.831757
Epoch 50: training loss 28240644954.353
Test Loss of 35294548310.517693, Test MSE of 35294547956.738937
Epoch 51: training loss 27276679348.706
Test Loss of 33131395167.222763, Test MSE of 33131395608.890614
Epoch 52: training loss 24866562183.529
Test Loss of 36237983554.975708, Test MSE of 36237983619.193039
Epoch 53: training loss 24425268024.471
Test Loss of 33033246032.122139, Test MSE of 33033245642.014008
Epoch 54: training loss 23446917515.294
Test Loss of 31367353205.429562, Test MSE of 31367352828.990082
Epoch 55: training loss 22333057347.765
Test Loss of 31916427619.782558, Test MSE of 31916426772.072830
Epoch 56: training loss 21221886234.353
Test Loss of 29761093523.512375, Test MSE of 29761093145.690685
Epoch 57: training loss 20569545584.941
Test Loss of 29493602087.024750, Test MSE of 29493601247.198479
Epoch 58: training loss 19608612156.235
Test Loss of 31265603642.744389, Test MSE of 31265603483.953499
Epoch 59: training loss 18980668201.412
Test Loss of 29293144296.135090, Test MSE of 29293143779.312981
Epoch 60: training loss 18219908833.882
Test Loss of 30433377011.149666, Test MSE of 30433377469.231689
Epoch 61: training loss 17373031521.882
Test Loss of 29093892552.216515, Test MSE of 29093893097.214626
Epoch 62: training loss 16990303051.294
Test Loss of 28200433006.441822, Test MSE of 28200432400.863312
Epoch 63: training loss 16401050774.588
Test Loss of 28499257542.499191, Test MSE of 28499256656.726387
Epoch 64: training loss 15841758102.588
Test Loss of 27412268604.520935, Test MSE of 27412268475.287502
Epoch 65: training loss 15588792903.529
Test Loss of 28880338509.102013, Test MSE of 28880339315.214806
Epoch 66: training loss 14828417464.471
Test Loss of 30185200074.585243, Test MSE of 30185200705.671703
Epoch 67: training loss 14093682330.353
Test Loss of 29891831148.073097, Test MSE of 29891832030.782314
Epoch 68: training loss 14136192075.294
Test Loss of 25484888346.352070, Test MSE of 25484888291.756824
Epoch 69: training loss 13577097200.941
Test Loss of 28792310218.111496, Test MSE of 28792310090.898026
Epoch 70: training loss 13326273916.235
Test Loss of 29762483454.637981, Test MSE of 29762483609.356522
Epoch 71: training loss 12770611365.647
Test Loss of 26671658301.646080, Test MSE of 26671658769.219353
Epoch 72: training loss 12548411550.118
Test Loss of 25757433217.391624, Test MSE of 25757433446.990284
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22885947629.83995, 'MSE - std': 2871485817.1503334, 'R2 - mean': 0.830127120295679, 'R2 - std': 0.014018094016318428} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005409 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926537697.882
Test Loss of 447258053459.319946, Test MSE of 447258052943.499878
Epoch 2: training loss 421905041287.529
Test Loss of 447238609960.505188, Test MSE of 447238612906.842468
Epoch 3: training loss 421877606159.059
Test Loss of 447213636590.708313, Test MSE of 447213626448.498962
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898099410.824
Test Loss of 447222154906.322449, Test MSE of 447222162882.454407
Epoch 2: training loss 421886732167.529
Test Loss of 447222612369.735840, Test MSE of 447222611031.988525
Epoch 3: training loss 418439306661.647
Test Loss of 435996739821.346313, Test MSE of 435996740755.655457
Epoch 4: training loss 392710657927.529
Test Loss of 394746984457.001160, Test MSE of 394746993039.522705
Epoch 5: training loss 329164347271.529
Test Loss of 310923714057.830200, Test MSE of 310923720224.278381
Epoch 6: training loss 247196737837.176
Test Loss of 231194860379.847321, Test MSE of 231194859578.133514
Epoch 7: training loss 180030522307.765
Test Loss of 170676560055.813080, Test MSE of 170676559784.341522
Epoch 8: training loss 143682869458.824
Test Loss of 147372790662.958130, Test MSE of 147372791233.873444
Epoch 9: training loss 133164861259.294
Test Loss of 140105371133.512848, Test MSE of 140105372713.508667
Epoch 10: training loss 131283380464.941
Test Loss of 136615951459.249588, Test MSE of 136615952542.149063
Epoch 11: training loss 129009787151.059
Test Loss of 134099227705.086288, Test MSE of 134099228515.892639
Epoch 12: training loss 125329208711.529
Test Loss of 130473225515.170013, Test MSE of 130473225231.197266
Epoch 13: training loss 123574889532.235
Test Loss of 128367386663.794586, Test MSE of 128367386851.429031
Epoch 14: training loss 119185857054.118
Test Loss of 124525681909.163086, Test MSE of 124525681715.076477
Epoch 15: training loss 117747547979.294
Test Loss of 123350521904.795746, Test MSE of 123350525093.772461
Epoch 16: training loss 114127398640.941
Test Loss of 120356634555.780701, Test MSE of 120356635917.850388
Epoch 17: training loss 111675963723.294
Test Loss of 116200704032.688416, Test MSE of 116200706707.376648
Epoch 18: training loss 108704577626.353
Test Loss of 114056173639.535507, Test MSE of 114056174823.025940
Epoch 19: training loss 104771636193.882
Test Loss of 110369354272.806854, Test MSE of 110369355780.937485
Epoch 20: training loss 102642803922.824
Test Loss of 108239425017.722885, Test MSE of 108239425887.823120
Epoch 21: training loss 98172957048.471
Test Loss of 104427710888.949341, Test MSE of 104427713042.102875
Epoch 22: training loss 94980275832.471
Test Loss of 101516843187.549393, Test MSE of 101516843233.163528
Epoch 23: training loss 92078570435.765
Test Loss of 97977291416.190613, Test MSE of 97977291465.685867
Epoch 24: training loss 88376980751.059
Test Loss of 95936847422.889664, Test MSE of 95936849514.633255
Epoch 25: training loss 86426601321.412
Test Loss of 94792900323.752945, Test MSE of 94792898900.080017
Epoch 26: training loss 83389845850.353
Test Loss of 90751538523.018280, Test MSE of 90751539506.437408
Epoch 27: training loss 79997331124.706
Test Loss of 88476846690.657410, Test MSE of 88476846376.518509
Epoch 28: training loss 77280556468.706
Test Loss of 83848264915.053436, Test MSE of 83848263175.804062
Epoch 29: training loss 75126587663.059
Test Loss of 82530693535.948181, Test MSE of 82530693296.034714
Epoch 30: training loss 71618952643.765
Test Loss of 77075323144.823502, Test MSE of 77075323537.756927
Epoch 31: training loss 68651949266.824
Test Loss of 74478108632.205414, Test MSE of 74478107667.201065
Epoch 32: training loss 66162225829.647
Test Loss of 71510759290.877625, Test MSE of 71510759674.313354
Epoch 33: training loss 64069880907.294
Test Loss of 66773284476.950264, Test MSE of 66773283329.796585
Epoch 34: training loss 60962397349.647
Test Loss of 66865468692.903999, Test MSE of 66865468157.075066
Epoch 35: training loss 57717763629.176
Test Loss of 62710076658.794357, Test MSE of 62710075634.238472
Epoch 36: training loss 55589164190.118
Test Loss of 61630363230.630577, Test MSE of 61630364275.505219
Epoch 37: training loss 53023105008.941
Test Loss of 58485390647.487396, Test MSE of 58485390646.404083
Epoch 38: training loss 50521640244.706
Test Loss of 55737311677.320381, Test MSE of 55737311647.892281
Epoch 39: training loss 48860996833.882
Test Loss of 56196083610.618553, Test MSE of 56196085710.666977
Epoch 40: training loss 46345279277.176
Test Loss of 51196516874.540825, Test MSE of 51196517092.429611
Epoch 41: training loss 44228766569.412
Test Loss of 50804128498.675919, Test MSE of 50804129113.295197
Epoch 42: training loss 42860189003.294
Test Loss of 52224546042.374275, Test MSE of 52224547022.852531
Epoch 43: training loss 40280237123.765
Test Loss of 46131714463.474442, Test MSE of 46131714059.052048
Epoch 44: training loss 38983126159.059
Test Loss of 44694318767.640991, Test MSE of 44694319670.593582
Epoch 45: training loss 36764981955.765
Test Loss of 42938312947.978722, Test MSE of 42938313516.190704
Epoch 46: training loss 35342687947.294
Test Loss of 42305785589.755264, Test MSE of 42305784932.063889
Epoch 47: training loss 33296724547.765
Test Loss of 39959420366.138329, Test MSE of 39959420562.256149
Epoch 48: training loss 31940298834.824
Test Loss of 39955397284.034233, Test MSE of 39955397161.514442
Epoch 49: training loss 31063045744.941
Test Loss of 38542456975.544762, Test MSE of 38542456955.921379
Epoch 50: training loss 29898392312.471
Test Loss of 36394387582.489937, Test MSE of 36394387000.641136
Epoch 51: training loss 28241916769.882
Test Loss of 34355680655.130234, Test MSE of 34355680642.088856
Epoch 52: training loss 26988661251.765
Test Loss of 34325130321.247280, Test MSE of 34325130716.201321
Epoch 53: training loss 25963854162.824
Test Loss of 32484380969.511913, Test MSE of 32484381116.602245
Epoch 54: training loss 24620166945.882
Test Loss of 36207635843.286606, Test MSE of 36207636049.909546
Epoch 55: training loss 23590439145.412
Test Loss of 33556913293.886654, Test MSE of 33556913552.319332
Epoch 56: training loss 22497331568.941
Test Loss of 30117287209.748787, Test MSE of 30117287536.800140
Epoch 57: training loss 21796126584.471
Test Loss of 29519699579.529030, Test MSE of 29519700028.502220
Epoch 58: training loss 20854327348.706
Test Loss of 28222757199.885265, Test MSE of 28222757217.934155
Epoch 59: training loss 20270663175.529
Test Loss of 32160312426.829517, Test MSE of 32160311968.028084
Epoch 60: training loss 19338700416.000
Test Loss of 29087970483.312515, Test MSE of 29087970569.176178
Epoch 61: training loss 18826029884.235
Test Loss of 28053030271.022900, Test MSE of 28053030210.895058
Epoch 62: training loss 18128029210.353
Test Loss of 27284890575.914875, Test MSE of 27284890159.625450
Epoch 63: training loss 17549476201.412
Test Loss of 27160866697.089985, Test MSE of 27160867016.975166
Epoch 64: training loss 16907094038.588
Test Loss of 23987183207.157993, Test MSE of 23987183631.308662
Epoch 65: training loss 16634629312.000
Test Loss of 24486329015.931530, Test MSE of 24486329155.587887
Epoch 66: training loss 15731138055.529
Test Loss of 25340479505.054825, Test MSE of 25340479420.536976
Epoch 67: training loss 15358121697.882
Test Loss of 25263719518.038399, Test MSE of 25263719428.671261
Epoch 68: training loss 15105981409.882
Test Loss of 24608631332.123062, Test MSE of 24608632146.463722
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23460175802.04787, 'MSE - std': 2481215398.330326, 'R2 - mean': 0.8321454001395429, 'R2 - std': 0.011796251394257468} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005659 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430105945027.765
Test Loss of 410763228866.517334, Test MSE of 410763229679.029724
Epoch 2: training loss 430083056820.706
Test Loss of 410743115106.917175, Test MSE of 410743115147.980652
Epoch 3: training loss 430056881935.059
Test Loss of 410719443176.188782, Test MSE of 410719440339.304138
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430071867030.588
Test Loss of 410722417285.390076, Test MSE of 410722416337.973999
Epoch 2: training loss 430061483429.647
Test Loss of 410723218380.349854, Test MSE of 410723224489.483032
Epoch 3: training loss 426478601878.588
Test Loss of 399364578406.826477, Test MSE of 399364574733.409851
Epoch 4: training loss 400342273084.235
Test Loss of 358498962968.877380, Test MSE of 358498968293.777161
Epoch 5: training loss 335853804122.353
Test Loss of 275585555225.232788, Test MSE of 275585553755.816650
Epoch 6: training loss 254865871209.412
Test Loss of 199118639337.136505, Test MSE of 199118638274.849548
Epoch 7: training loss 187033815461.647
Test Loss of 137522474971.987030, Test MSE of 137522478816.651581
Epoch 8: training loss 151906873494.588
Test Loss of 118645688993.821381, Test MSE of 118645689329.714859
Epoch 9: training loss 140786373933.176
Test Loss of 112456690258.213791, Test MSE of 112456690201.605469
Epoch 10: training loss 137609526151.529
Test Loss of 109720978947.080063, Test MSE of 109720978456.245331
Epoch 11: training loss 134184165677.176
Test Loss of 107117940844.038864, Test MSE of 107117940264.867081
Epoch 12: training loss 131749018051.765
Test Loss of 105400084054.478485, Test MSE of 105400083024.167984
Epoch 13: training loss 129117980912.941
Test Loss of 102834756888.995834, Test MSE of 102834756107.774231
Epoch 14: training loss 125962330081.882
Test Loss of 100456093010.806107, Test MSE of 100456091747.747162
Epoch 15: training loss 123644164306.824
Test Loss of 98099469210.121246, Test MSE of 98099467991.126450
Epoch 16: training loss 120787575747.765
Test Loss of 95347142328.092545, Test MSE of 95347141655.707123
Epoch 17: training loss 117150789451.294
Test Loss of 92921192334.274872, Test MSE of 92921192862.221939
Epoch 18: training loss 114596407868.235
Test Loss of 90486747880.899582, Test MSE of 90486749406.066956
Epoch 19: training loss 110887547030.588
Test Loss of 88616210407.359558, Test MSE of 88616210632.869766
Epoch 20: training loss 108032877492.706
Test Loss of 86511179185.577042, Test MSE of 86511179164.052719
Epoch 21: training loss 105016938676.706
Test Loss of 82717659217.503006, Test MSE of 82717660455.817871
Epoch 22: training loss 101723572133.647
Test Loss of 81585565387.046738, Test MSE of 81585564989.634445
Epoch 23: training loss 98383752914.824
Test Loss of 78100931045.227203, Test MSE of 78100930308.976028
Epoch 24: training loss 95888984545.882
Test Loss of 75736403511.204071, Test MSE of 75736403324.917770
Epoch 25: training loss 91058722695.529
Test Loss of 74574346362.254517, Test MSE of 74574345818.191406
Epoch 26: training loss 88895977773.176
Test Loss of 70481879631.370667, Test MSE of 70481878022.257950
Epoch 27: training loss 85676897475.765
Test Loss of 68572677449.329018, Test MSE of 68572676271.739487
Epoch 28: training loss 82072555896.471
Test Loss of 65870244491.076355, Test MSE of 65870244706.499405
Epoch 29: training loss 80334117737.412
Test Loss of 63059289212.623787, Test MSE of 63059288221.012260
Epoch 30: training loss 76272042917.647
Test Loss of 59905885926.530312, Test MSE of 59905886159.465660
Epoch 31: training loss 74404453240.471
Test Loss of 58882715013.034706, Test MSE of 58882715866.123886
Epoch 32: training loss 70564336549.647
Test Loss of 56860543824.199905, Test MSE of 56860543265.731895
Epoch 33: training loss 68376642996.706
Test Loss of 56048005641.240166, Test MSE of 56048005403.072273
Epoch 34: training loss 65616744809.412
Test Loss of 55153950651.764923, Test MSE of 55153951206.165863
Epoch 35: training loss 62696738183.529
Test Loss of 51928708793.040260, Test MSE of 51928709765.213081
Epoch 36: training loss 60182838949.647
Test Loss of 48831725185.125404, Test MSE of 48831725002.901466
Epoch 37: training loss 57574970985.412
Test Loss of 46674965228.690422, Test MSE of 46674965494.471596
Epoch 38: training loss 54964704225.882
Test Loss of 44358767103.763069, Test MSE of 44358767374.151176
Epoch 39: training loss 53028174885.647
Test Loss of 43360964484.797775, Test MSE of 43360964666.313049
Epoch 40: training loss 50723624591.059
Test Loss of 40034752892.031464, Test MSE of 40034753204.642616
Epoch 41: training loss 48101555568.941
Test Loss of 36926278163.664970, Test MSE of 36926278612.627884
Epoch 42: training loss 46038250601.412
Test Loss of 37573076039.552063, Test MSE of 37573077278.314651
Epoch 43: training loss 44335853974.588
Test Loss of 35677885208.285049, Test MSE of 35677885404.328575
Epoch 44: training loss 42440099998.118
Test Loss of 34403603781.538177, Test MSE of 34403604010.465538
Epoch 45: training loss 39870415036.235
Test Loss of 34096700556.734844, Test MSE of 34096700176.066181
Epoch 46: training loss 38664953321.412
Test Loss of 31137712066.398888, Test MSE of 31137712310.093693
Epoch 47: training loss 36623366354.824
Test Loss of 31993997223.863026, Test MSE of 31993997094.152275
Epoch 48: training loss 35032716461.176
Test Loss of 29722330020.546043, Test MSE of 29722329707.284527
Epoch 49: training loss 33666452751.059
Test Loss of 28026637342.800556, Test MSE of 28026637274.310265
Epoch 50: training loss 31903681039.059
Test Loss of 29873432249.040260, Test MSE of 29873432511.107048
Epoch 51: training loss 30636474842.353
Test Loss of 27408496437.190189, Test MSE of 27408496298.775581
Epoch 52: training loss 29059372830.118
Test Loss of 26193320382.371124, Test MSE of 26193320603.950672
Epoch 53: training loss 28048768395.294
Test Loss of 26823075139.642757, Test MSE of 26823074899.326168
Epoch 54: training loss 26789071021.176
Test Loss of 25628060059.305878, Test MSE of 25628059486.767338
Epoch 55: training loss 25788725808.941
Test Loss of 25050905039.429893, Test MSE of 25050904715.666840
Epoch 56: training loss 24655948295.529
Test Loss of 23507310665.921333, Test MSE of 23507310175.125114
Epoch 57: training loss 24005109616.941
Test Loss of 22145889950.504395, Test MSE of 22145890167.340397
Epoch 58: training loss 22859061485.176
Test Loss of 24789960555.209625, Test MSE of 24789959921.967205
Epoch 59: training loss 21592792941.176
Test Loss of 21566096423.329941, Test MSE of 21566095948.882015
Epoch 60: training loss 21263532227.765
Test Loss of 21608776190.815365, Test MSE of 21608776420.429352
Epoch 61: training loss 20499662565.647
Test Loss of 20858155680.873669, Test MSE of 20858155885.224705
Epoch 62: training loss 19607519062.588
Test Loss of 21289415366.782043, Test MSE of 21289414800.138367
Epoch 63: training loss 19258804107.294
Test Loss of 20202837030.382229, Test MSE of 20202836853.717854
Epoch 64: training loss 18831967258.353
Test Loss of 20761938543.592781, Test MSE of 20761938514.389694
Epoch 65: training loss 18014290262.588
Test Loss of 20292613696.681168, Test MSE of 20292613683.642029
Epoch 66: training loss 17545166157.176
Test Loss of 20466987924.434982, Test MSE of 20466988133.363010
Epoch 67: training loss 16999501191.529
Test Loss of 20978720985.025452, Test MSE of 20978720783.846622
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22839812047.49756, 'MSE - std': 2402472883.00262, 'R2 - mean': 0.8308219711253324, 'R2 - std': 0.010469864052811438} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003807 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043092088.471
Test Loss of 431611901437.393799, Test MSE of 431611899302.186279
Epoch 2: training loss 424022674853.647
Test Loss of 431591680518.397034, Test MSE of 431591682561.486694
Epoch 3: training loss 423994487024.941
Test Loss of 431564508416.355408, Test MSE of 431564504425.291260
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010289995.294
Test Loss of 431567976896.266541, Test MSE of 431567974867.081726
Epoch 2: training loss 423999016237.176
Test Loss of 431570916524.009277, Test MSE of 431570917358.709961
Epoch 3: training loss 420173725696.000
Test Loss of 419399985837.193909, Test MSE of 419399981250.528992
Epoch 4: training loss 393724751631.059
Test Loss of 377167888330.928284, Test MSE of 377167886049.086304
Epoch 5: training loss 329520345449.412
Test Loss of 292552849098.098999, Test MSE of 292552848627.305786
Epoch 6: training loss 248938914695.529
Test Loss of 214374984228.723724, Test MSE of 214374983595.745758
Epoch 7: training loss 181818279875.765
Test Loss of 153784163301.937988, Test MSE of 153784161569.122955
Epoch 8: training loss 147636773526.588
Test Loss of 131211139042.621002, Test MSE of 131211138598.372345
Epoch 9: training loss 137938676916.706
Test Loss of 123715634910.948639, Test MSE of 123715635182.157303
Epoch 10: training loss 134164483373.176
Test Loss of 120983288987.898193, Test MSE of 120983288967.426498
Epoch 11: training loss 131021482465.882
Test Loss of 118840085627.676071, Test MSE of 118840084383.909683
Epoch 12: training loss 129255458710.588
Test Loss of 115090177430.093475, Test MSE of 115090178294.516968
Epoch 13: training loss 125305836001.882
Test Loss of 112330958005.486343, Test MSE of 112330957203.996094
Epoch 14: training loss 123706768052.706
Test Loss of 110368777696.962524, Test MSE of 110368777851.828659
Epoch 15: training loss 120622428250.353
Test Loss of 108343872442.343353, Test MSE of 108343873101.961075
Epoch 16: training loss 118859250447.059
Test Loss of 106060840019.872284, Test MSE of 106060839774.577393
Epoch 17: training loss 114600403034.353
Test Loss of 101308064388.442383, Test MSE of 101308067006.228683
Epoch 18: training loss 111158911367.529
Test Loss of 97291369578.617310, Test MSE of 97291371493.992920
Epoch 19: training loss 107432887747.765
Test Loss of 95616670493.023605, Test MSE of 95616670380.350006
Epoch 20: training loss 105024217984.000
Test Loss of 93524408070.752426, Test MSE of 93524407404.217514
Epoch 21: training loss 101874445628.235
Test Loss of 91743016987.483566, Test MSE of 91743015749.340790
Epoch 22: training loss 98193195369.412
Test Loss of 86461986123.224426, Test MSE of 86461985708.130005
Epoch 23: training loss 94784014320.941
Test Loss of 84829597580.379456, Test MSE of 84829598459.079041
Epoch 24: training loss 91334419094.588
Test Loss of 83634340628.494217, Test MSE of 83634341500.479843
Epoch 25: training loss 90296908664.471
Test Loss of 81156192628.449799, Test MSE of 81156192097.371155
Epoch 26: training loss 86429104775.529
Test Loss of 76778040517.597412, Test MSE of 76778039586.466217
Epoch 27: training loss 83256524167.529
Test Loss of 75441471139.242950, Test MSE of 75441471619.722610
Epoch 28: training loss 80213490010.353
Test Loss of 73206507369.314209, Test MSE of 73206508003.623276
Epoch 29: training loss 77380460604.235
Test Loss of 70508456223.155945, Test MSE of 70508455620.954330
Epoch 30: training loss 74781936173.176
Test Loss of 67725905190.263763, Test MSE of 67725907389.340469
Epoch 31: training loss 72113821025.882
Test Loss of 63944355462.811661, Test MSE of 63944356059.559402
Epoch 32: training loss 69109254076.235
Test Loss of 61285728599.070801, Test MSE of 61285727669.771507
Epoch 33: training loss 66046060257.882
Test Loss of 58339875925.767700, Test MSE of 58339875491.311584
Epoch 34: training loss 63436401633.882
Test Loss of 57136135141.937988, Test MSE of 57136135125.572739
Epoch 35: training loss 60998941259.294
Test Loss of 55428869596.223969, Test MSE of 55428869091.081100
Epoch 36: training loss 58411363297.882
Test Loss of 52341787371.742714, Test MSE of 52341787409.076591
Epoch 37: training loss 55496634992.941
Test Loss of 50319009667.850067, Test MSE of 50319010476.309174
Epoch 38: training loss 53742394112.000
Test Loss of 44465017415.788986, Test MSE of 44465017549.611900
Epoch 39: training loss 51702537140.706
Test Loss of 47303761641.373436, Test MSE of 47303761194.239021
Epoch 40: training loss 49304431352.471
Test Loss of 44376581187.761223, Test MSE of 44376581859.036980
Epoch 41: training loss 47116020540.235
Test Loss of 44557433265.103195, Test MSE of 44557433365.457672
Epoch 42: training loss 44512405165.176
Test Loss of 41580169978.906059, Test MSE of 41580169184.594299
Epoch 43: training loss 43037855480.471
Test Loss of 38089458812.623787, Test MSE of 38089459112.274239
Epoch 44: training loss 40949656944.941
Test Loss of 39410724407.204071, Test MSE of 39410723885.326736
Epoch 45: training loss 38480777991.529
Test Loss of 33541250701.445625, Test MSE of 33541250575.078094
Epoch 46: training loss 37714997248.000
Test Loss of 33932206733.919483, Test MSE of 33932206252.462894
Epoch 47: training loss 35444040387.765
Test Loss of 32037647237.271633, Test MSE of 32037647717.464947
Epoch 48: training loss 34388356570.353
Test Loss of 33381499429.671448, Test MSE of 33381498753.517700
Epoch 49: training loss 32903760956.235
Test Loss of 29020623828.879223, Test MSE of 29020623773.275467
Epoch 50: training loss 31856116088.471
Test Loss of 30555294653.186489, Test MSE of 30555294649.471882
Epoch 51: training loss 30254030411.294
Test Loss of 30372127833.558537, Test MSE of 30372127765.344753
Epoch 52: training loss 28921157150.118
Test Loss of 26468453908.138824, Test MSE of 26468454309.051193
Epoch 53: training loss 27490686682.353
Test Loss of 25449623188.079594, Test MSE of 25449623520.493912
Epoch 54: training loss 26332505216.000
Test Loss of 24748329498.298935, Test MSE of 24748329522.471069
Epoch 55: training loss 25630598155.294
Test Loss of 27465103719.181862, Test MSE of 27465103577.730843
Epoch 56: training loss 24445639913.412
Test Loss of 25817601243.394726, Test MSE of 25817601201.896214
Epoch 57: training loss 23276142593.882
Test Loss of 23701431190.330402, Test MSE of 23701430987.257992
Epoch 58: training loss 22976955410.824
Test Loss of 23761132276.272095, Test MSE of 23761132265.752125
Epoch 59: training loss 21627975664.941
Test Loss of 21702021272.581211, Test MSE of 21702021950.547306
Epoch 60: training loss 20921444355.765
Test Loss of 22129764525.430820, Test MSE of 22129765007.797417
Epoch 61: training loss 20157321626.353
Test Loss of 21205367289.129108, Test MSE of 21205367390.076939
Epoch 62: training loss 19665707561.412
Test Loss of 20575851670.211941, Test MSE of 20575852003.010899
Epoch 63: training loss 19142594733.176
Test Loss of 19684381221.671448, Test MSE of 19684381315.143101
Epoch 64: training loss 18640284743.529
Test Loss of 22206257519.711246, Test MSE of 22206257639.271770
Epoch 65: training loss 18199401321.412
Test Loss of 21356562981.671448, Test MSE of 21356563033.124317
Epoch 66: training loss 17601909910.588
Test Loss of 21394447824.851460, Test MSE of 21394447278.717979
Epoch 67: training loss 16668744828.235
Test Loss of 18519768598.981953, Test MSE of 18519768754.078197
Epoch 68: training loss 16563963621.647
Test Loss of 20013577478.041649, Test MSE of 20013577105.692562
Epoch 69: training loss 16142510362.353
Test Loss of 21670093076.731144, Test MSE of 21670093275.880032
Epoch 70: training loss 15701402529.882
Test Loss of 19754034995.294769, Test MSE of 19754034834.911850
Epoch 71: training loss 15359097084.235
Test Loss of 21339972321.317909, Test MSE of 21339972274.787571
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22539844092.95556, 'MSE - std': 2231014087.328625, 'R2 - mean': 0.8327840358517979, 'R2 - std': 0.010153483864705997} 
 

Saving model.....
Results After CV: {'MSE - mean': 22539844092.95556, 'MSE - std': 2231014087.328625, 'R2 - mean': 0.8327840358517979, 'R2 - std': 0.010153483864705997}
Train time: 105.04703691940048
Inference time: 0.10006731579996994
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 26 finished with value: 22539844092.95556 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005414 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427526307237.647
Test Loss of 418111404757.303711, Test MSE of 418111402184.588867
Epoch 2: training loss 427506834130.824
Test Loss of 418093266505.785767, Test MSE of 418093268350.513916
Epoch 3: training loss 427480764536.471
Test Loss of 418069552790.058777, Test MSE of 418069553782.663330
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492337664.000
Test Loss of 418072074789.070557, Test MSE of 418072079218.657410
Epoch 2: training loss 427481527476.706
Test Loss of 418072361022.060608, Test MSE of 418072358463.532837
Epoch 3: training loss 427481233648.941
Test Loss of 418072383064.471924, Test MSE of 418072395667.849426
Epoch 4: training loss 427480999815.529
Test Loss of 418072288057.737671, Test MSE of 418072280669.936523
Epoch 5: training loss 427480803809.882
Test Loss of 418072426527.740906, Test MSE of 418072425219.674805
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418072425219.6748, 'MSE - std': 0.0, 'R2 - mean': -2.2555753356987034, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005494 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918271668.706
Test Loss of 424556570548.674561, Test MSE of 424556572089.193054
Epoch 2: training loss 427897536752.941
Test Loss of 424539358139.070068, Test MSE of 424539352592.478088
Epoch 3: training loss 427869449758.118
Test Loss of 424516392432.721741, Test MSE of 424516404205.781433
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884128858.353
Test Loss of 424524092116.829956, Test MSE of 424524089643.179871
Epoch 2: training loss 427874841780.706
Test Loss of 424523786630.839722, Test MSE of 424523787256.470337
Epoch 3: training loss 427874474345.412
Test Loss of 424523034437.344421, Test MSE of 424523035162.821655
Epoch 4: training loss 427874188830.118
Test Loss of 424522335879.372681, Test MSE of 424522335118.153687
Epoch 5: training loss 427873995836.235
Test Loss of 424521956102.573242, Test MSE of 424521961928.598633
Epoch 6: training loss 418973803700.706
Test Loss of 397332276494.271545, Test MSE of 397332272900.233887
Epoch 7: training loss 363719804084.706
Test Loss of 320812418092.768921, Test MSE of 320812417563.677246
Epoch 8: training loss 275750210499.765
Test Loss of 230818733047.235718, Test MSE of 230818732854.219757
Epoch 9: training loss 197633380713.412
Test Loss of 168406013659.699280, Test MSE of 168406013940.630585
Epoch 10: training loss 154408608346.353
Test Loss of 140811985098.762909, Test MSE of 140811984852.547638
Epoch 11: training loss 138111101801.412
Test Loss of 129258190299.876938, Test MSE of 129258188837.870316
Epoch 12: training loss 132155056820.706
Test Loss of 126051875073.243576, Test MSE of 126051873889.348236
Epoch 13: training loss 128569271536.941
Test Loss of 123616558778.063385, Test MSE of 123616556311.652969
Epoch 14: training loss 125489675956.706
Test Loss of 119801032144.033310, Test MSE of 119801036265.446014
Epoch 15: training loss 122665147241.412
Test Loss of 116782030986.333557, Test MSE of 116782030776.438217
Epoch 16: training loss 119046571730.824
Test Loss of 112395796999.224609, Test MSE of 112395798082.909683
Epoch 17: training loss 113773418496.000
Test Loss of 108236048007.372665, Test MSE of 108236048679.423874
Epoch 18: training loss 109635878159.059
Test Loss of 105005998020.308121, Test MSE of 105005996326.321472
Epoch 19: training loss 106353542083.765
Test Loss of 101906347817.393478, Test MSE of 101906349176.013885
Epoch 20: training loss 102497988065.882
Test Loss of 97718522837.599808, Test MSE of 97718523328.675735
Epoch 21: training loss 98749408015.059
Test Loss of 94178877839.840851, Test MSE of 94178878535.192764
Epoch 22: training loss 94298374445.176
Test Loss of 88150222809.626648, Test MSE of 88150223426.133942
Epoch 23: training loss 90145263495.529
Test Loss of 84087061860.019424, Test MSE of 84087061285.761505
Epoch 24: training loss 86025857852.235
Test Loss of 78473927728.085129, Test MSE of 78473929392.950027
Epoch 25: training loss 82687883670.588
Test Loss of 76803683783.032150, Test MSE of 76803685729.118225
Epoch 26: training loss 78304910802.824
Test Loss of 73412273805.057602, Test MSE of 73412275968.440033
Epoch 27: training loss 75009523079.529
Test Loss of 70059481297.869080, Test MSE of 70059481780.980911
Epoch 28: training loss 71096247175.529
Test Loss of 67075234489.826508, Test MSE of 67075235778.144981
Epoch 29: training loss 67611322036.706
Test Loss of 60212967147.806618, Test MSE of 60212966677.372002
Epoch 30: training loss 64356721136.941
Test Loss of 60221963031.628036, Test MSE of 60221964264.083435
Epoch 31: training loss 61042019734.588
Test Loss of 54458715147.133011, Test MSE of 54458715950.172882
Epoch 32: training loss 58050990004.706
Test Loss of 54080008414.186447, Test MSE of 54080008043.963219
Epoch 33: training loss 54354680560.941
Test Loss of 52117754833.336113, Test MSE of 52117754358.097267
Epoch 34: training loss 52081716781.176
Test Loss of 48051405996.206337, Test MSE of 48051406289.374756
Epoch 35: training loss 49234534317.176
Test Loss of 45169748026.744392, Test MSE of 45169747367.843544
Epoch 36: training loss 46689328692.706
Test Loss of 44239373121.791351, Test MSE of 44239371986.709846
Epoch 37: training loss 44394774859.294
Test Loss of 41841308714.400185, Test MSE of 41841308613.910004
Epoch 38: training loss 41289036800.000
Test Loss of 39937866322.550079, Test MSE of 39937867005.097191
Epoch 39: training loss 39098609513.412
Test Loss of 40259401658.359474, Test MSE of 40259403101.172470
Epoch 40: training loss 36945252374.588
Test Loss of 37898850573.087212, Test MSE of 37898850475.373573
Epoch 41: training loss 35235148393.412
Test Loss of 36016442100.097153, Test MSE of 36016442502.941940
Epoch 42: training loss 33206420811.294
Test Loss of 35204449249.443443, Test MSE of 35204450144.793114
Epoch 43: training loss 31383868559.059
Test Loss of 30182128861.475826, Test MSE of 30182128861.236237
Epoch 44: training loss 29609570183.529
Test Loss of 31641441807.041405, Test MSE of 31641442060.872211
Epoch 45: training loss 27666355200.000
Test Loss of 32146412498.283600, Test MSE of 32146413083.507004
Epoch 46: training loss 26596089008.941
Test Loss of 32222877524.030533, Test MSE of 32222876291.202633
Epoch 47: training loss 25070066876.235
Test Loss of 27560342028.435810, Test MSE of 27560342919.956852
Epoch 48: training loss 23935489792.000
Test Loss of 31847856919.154289, Test MSE of 31847856270.979073
Epoch 49: training loss 22687025505.882
Test Loss of 28123070310.032848, Test MSE of 28123071249.975735
Epoch 50: training loss 21702051960.471
Test Loss of 26669997982.408512, Test MSE of 26669998429.270611
Epoch 51: training loss 20927398238.118
Test Loss of 27536947874.612999, Test MSE of 27536947662.984062
Epoch 52: training loss 19638563079.529
Test Loss of 27702527556.574600, Test MSE of 27702526959.006283
Epoch 53: training loss 18528276047.059
Test Loss of 27871985258.474209, Test MSE of 27871984688.769215
Epoch 54: training loss 18176486415.059
Test Loss of 27638876838.876705, Test MSE of 27638876488.484718
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 222855650854.07977, 'MSE - std': 195216774365.59503, 'R2 - mean': -0.7264492817960981, 'R2 - std': 1.5291260539026053} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005543 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421928038640.941
Test Loss of 447258630018.457581, Test MSE of 447258621566.097961
Epoch 2: training loss 421908993686.588
Test Loss of 447240723090.979431, Test MSE of 447240721893.132996
Epoch 3: training loss 421883335619.765
Test Loss of 447217360545.428650, Test MSE of 447217359791.230164
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899335559.529
Test Loss of 447224411206.351135, Test MSE of 447224409173.330688
Epoch 2: training loss 421888827873.882
Test Loss of 447224786397.298157, Test MSE of 447224782847.772278
Epoch 3: training loss 421888555851.294
Test Loss of 447225147247.033997, Test MSE of 447225154364.052185
Epoch 4: training loss 421888373458.824
Test Loss of 447225247271.676147, Test MSE of 447225254142.062378
Epoch 5: training loss 421888203474.824
Test Loss of 447225116639.311584, Test MSE of 447225111938.676514
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 297645471215.612, 'MSE - std': 191294085433.6388, 'R2 - mean': -1.143348873172477, 'R2 - std': 1.3807346586975906} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110716867.765
Test Loss of 410763985881.617798, Test MSE of 410763983309.537842
Epoch 2: training loss 430090361916.235
Test Loss of 410745732725.752869, Test MSE of 410745731796.887817
Epoch 3: training loss 430063474085.647
Test Loss of 410721951740.209167, Test MSE of 410721953924.435669
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077921159.529
Test Loss of 410728922681.099487, Test MSE of 410728931380.459778
Epoch 2: training loss 430064944067.765
Test Loss of 410728581247.940796, Test MSE of 410728585870.283569
Epoch 3: training loss 430064681923.765
Test Loss of 410728628057.203125, Test MSE of 410728626898.408813
Epoch 4: training loss 430064508446.118
Test Loss of 410728654866.006470, Test MSE of 410728644729.323486
Epoch 5: training loss 430064348220.235
Test Loss of 410728671515.365112, Test MSE of 410728679678.158386
Epoch 6: training loss 422040018462.118
Test Loss of 385442161136.599731, Test MSE of 385442158982.707886
Epoch 7: training loss 368864926780.235
Test Loss of 309044928227.687195, Test MSE of 309044927754.482849
Epoch 8: training loss 281932664711.529
Test Loss of 218812658613.604797, Test MSE of 218812656053.111206
Epoch 9: training loss 203127923290.353
Test Loss of 153841026820.857025, Test MSE of 153841028690.637787
Epoch 10: training loss 159282466514.824
Test Loss of 122297263789.667740, Test MSE of 122297266128.645279
Epoch 11: training loss 142394472719.059
Test Loss of 112063186184.884781, Test MSE of 112063187143.080399
Epoch 12: training loss 137248220129.882
Test Loss of 108860490478.585846, Test MSE of 108860489192.592178
Epoch 13: training loss 132966944466.824
Test Loss of 105873119615.822311, Test MSE of 105873118746.912048
Epoch 14: training loss 130247384907.294
Test Loss of 102823319738.698746, Test MSE of 102823318846.546829
Epoch 15: training loss 127907243068.235
Test Loss of 100744253297.843597, Test MSE of 100744255598.718475
Epoch 16: training loss 122365036544.000
Test Loss of 96199267010.991211, Test MSE of 96199266370.108246
Epoch 17: training loss 117913311021.176
Test Loss of 93719247186.332260, Test MSE of 93719247526.963333
Epoch 18: training loss 115205774275.765
Test Loss of 90479200709.952805, Test MSE of 90479200305.987915
Epoch 19: training loss 110417632888.471
Test Loss of 86484175434.632111, Test MSE of 86484176706.175323
Epoch 20: training loss 106380739915.294
Test Loss of 84671805406.830170, Test MSE of 84671804634.007294
Epoch 21: training loss 103227796871.529
Test Loss of 80405903800.684875, Test MSE of 80405903860.767380
Epoch 22: training loss 98599518328.471
Test Loss of 78818540472.447937, Test MSE of 78818541527.707825
Epoch 23: training loss 95044272624.941
Test Loss of 73322292864.177689, Test MSE of 73322293401.479095
Epoch 24: training loss 91227851279.059
Test Loss of 70834622181.582596, Test MSE of 70834622991.570557
Epoch 25: training loss 87076554480.941
Test Loss of 69668094971.735306, Test MSE of 69668096527.312820
Epoch 26: training loss 84463352892.235
Test Loss of 66346185460.272095, Test MSE of 66346187059.106010
Epoch 27: training loss 78758218736.941
Test Loss of 62449089418.957893, Test MSE of 62449088599.141647
Epoch 28: training loss 76106305114.353
Test Loss of 60816455548.268394, Test MSE of 60816456630.999863
Epoch 29: training loss 73243790336.000
Test Loss of 55438506087.300323, Test MSE of 55438505096.199318
Epoch 30: training loss 69723310923.294
Test Loss of 55804400584.558998, Test MSE of 55804399173.630608
Epoch 31: training loss 65690909726.118
Test Loss of 51026826527.629799, Test MSE of 51026825930.399567
Epoch 32: training loss 63126057185.882
Test Loss of 49617882421.427116, Test MSE of 49617882574.481812
Epoch 33: training loss 59719661296.941
Test Loss of 46946038087.907448, Test MSE of 46946038133.834961
Epoch 34: training loss 57097692476.235
Test Loss of 43475705154.695045, Test MSE of 43475705969.819496
Epoch 35: training loss 54161341711.059
Test Loss of 40360127996.446091, Test MSE of 40360127851.467125
Epoch 36: training loss 51720460815.059
Test Loss of 40981083285.264229, Test MSE of 40981083459.230713
Epoch 37: training loss 48706659056.941
Test Loss of 35759524511.925957, Test MSE of 35759524645.594894
Epoch 38: training loss 46667421492.706
Test Loss of 35974672945.043961, Test MSE of 35974673172.472893
Epoch 39: training loss 43824036705.882
Test Loss of 33389707521.776955, Test MSE of 33389707183.976696
Epoch 40: training loss 41266111826.824
Test Loss of 36054844466.702454, Test MSE of 36054844909.499992
Epoch 41: training loss 39425592320.000
Test Loss of 31973588326.708004, Test MSE of 31973588201.506622
Epoch 42: training loss 38346975924.706
Test Loss of 32043494618.447014, Test MSE of 32043495112.484753
Epoch 43: training loss 35908019260.235
Test Loss of 28485444717.460434, Test MSE of 28485444643.030178
Epoch 44: training loss 33663225660.235
Test Loss of 26207693391.370663, Test MSE of 26207693520.125252
Epoch 45: training loss 32000780114.824
Test Loss of 29281072656.347988, Test MSE of 29281072751.469028
Epoch 46: training loss 30628319653.647
Test Loss of 28195120665.825081, Test MSE of 28195120260.101051
Epoch 47: training loss 29467372709.647
Test Loss of 26434935628.409069, Test MSE of 26434935430.824272
Epoch 48: training loss 27572856357.647
Test Loss of 25792394500.146229, Test MSE of 25792394453.185253
Epoch 49: training loss 26421101349.647
Test Loss of 25064202719.540955, Test MSE of 25064202504.602497
Epoch 50: training loss 25071286535.529
Test Loss of 24502849790.933826, Test MSE of 24502849858.389648
Epoch 51: training loss 23941243260.235
Test Loss of 24244832310.019436, Test MSE of 24244831952.701298
Epoch 52: training loss 23048760137.412
Test Loss of 22508419547.750114, Test MSE of 22508419664.534733
Epoch 53: training loss 22291308920.471
Test Loss of 20720953962.854233, Test MSE of 20720954424.485783
Epoch 54: training loss 21331253221.647
Test Loss of 21386613765.212402, Test MSE of 21386613539.314651
Epoch 55: training loss 20794834695.529
Test Loss of 21996228562.509949, Test MSE of 21996228215.830997
Epoch 56: training loss 19907162872.471
Test Loss of 18370161022.874596, Test MSE of 18370161016.689953
Epoch 57: training loss 19229743800.471
Test Loss of 20820571118.467377, Test MSE of 20820570978.243855
Epoch 58: training loss 18455043154.824
Test Loss of 19756280789.826931, Test MSE of 19756280683.986668
Epoch 59: training loss 18062839826.824
Test Loss of 17857447234.695049, Test MSE of 17857447351.174828
Epoch 60: training loss 17358174753.882
Test Loss of 19667974097.562241, Test MSE of 19667973815.843678
Epoch 61: training loss 17168180414.118
Test Loss of 17995197486.911613, Test MSE of 17995197683.181313
Epoch 62: training loss 16446695781.647
Test Loss of 18284880205.593708, Test MSE of 18284880309.278503
Epoch 63: training loss 15900975695.059
Test Loss of 18047177406.252663, Test MSE of 18047177060.156563
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 227745897676.74817, 'MSE - std': 205189964384.6266, 'R2 - mean': -0.6447498443193154, 'R2 - std': 1.4749997027174317} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005306 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043135216.941
Test Loss of 431611915160.225830, Test MSE of 431611918249.003113
Epoch 2: training loss 424024097370.353
Test Loss of 431592481385.906555, Test MSE of 431592481650.560303
Epoch 3: training loss 423996783194.353
Test Loss of 431566035441.547424, Test MSE of 431566032792.250488
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011086426.353
Test Loss of 431567661665.377136, Test MSE of 431567666615.648560
Epoch 2: training loss 423999884830.118
Test Loss of 431569949860.427551, Test MSE of 431569947789.233215
Epoch 3: training loss 423999477760.000
Test Loss of 431570125906.924561, Test MSE of 431570122560.246399
Epoch 4: training loss 423999153091.765
Test Loss of 431569853588.316528, Test MSE of 431569846548.424255
Epoch 5: training loss 423998918535.529
Test Loss of 431569293560.299866, Test MSE of 431569290248.753906
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 268510576191.14932, 'MSE - std': 200821744950.2453, 'R2 - mean': -0.9603949719661273, 'R2 - std': 1.462541173725341} 
 

Saving model.....
Results After CV: {'MSE - mean': 268510576191.14932, 'MSE - std': 200821744950.2453, 'R2 - mean': -0.9603949719661273, 'R2 - std': 1.462541173725341}
Train time: 44.68785049739963
Inference time: 0.07132729799996014
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 27 finished with value: 268510576191.14932 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005867 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525816440.471
Test Loss of 418111285146.144836, Test MSE of 418111293293.768066
Epoch 2: training loss 427505334633.412
Test Loss of 418094016440.464478, Test MSE of 418094020068.690247
Epoch 3: training loss 427477902396.235
Test Loss of 418070817568.155457, Test MSE of 418070816588.280334
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493087834.353
Test Loss of 418074283653.951416, Test MSE of 418074283763.993713
Epoch 2: training loss 427483563911.529
Test Loss of 418076150808.397888, Test MSE of 418076146468.098206
Epoch 3: training loss 427483060103.529
Test Loss of 418075451446.954407, Test MSE of 418075449648.279419
Epoch 4: training loss 427482673754.353
Test Loss of 418075113958.536194, Test MSE of 418075112386.417908
Epoch 5: training loss 420789525684.706
Test Loss of 396924208563.134888, Test MSE of 396924212475.107971
Epoch 6: training loss 376005478881.882
Test Loss of 330589680949.118652, Test MSE of 330589674241.205078
Epoch 7: training loss 296668060852.706
Test Loss of 244768710506.296539, Test MSE of 244768714396.433075
Epoch 8: training loss 218248755802.353
Test Loss of 172745145564.528351, Test MSE of 172745143911.297638
Epoch 9: training loss 160672883260.235
Test Loss of 127222316574.674988, Test MSE of 127222316820.536575
Epoch 10: training loss 140260931192.471
Test Loss of 118729994290.216980, Test MSE of 118729994242.197861
Epoch 11: training loss 136063777671.529
Test Loss of 115546665925.492477, Test MSE of 115546667126.869919
Epoch 12: training loss 132473868227.765
Test Loss of 113083709528.590332, Test MSE of 113083710912.222366
Epoch 13: training loss 131311240327.529
Test Loss of 109507615204.641220, Test MSE of 109507613955.057632
Epoch 14: training loss 125682276201.412
Test Loss of 106122484469.518387, Test MSE of 106122487184.429672
Epoch 15: training loss 122213021952.000
Test Loss of 102788822207.629883, Test MSE of 102788821813.831329
Epoch 16: training loss 118856440455.529
Test Loss of 100556779316.289612, Test MSE of 100556777705.580383
Epoch 17: training loss 116956151672.471
Test Loss of 96999323427.708542, Test MSE of 96999323185.456848
Epoch 18: training loss 112349044404.706
Test Loss of 93584324827.343979, Test MSE of 93584323561.891403
Epoch 19: training loss 108980623600.941
Test Loss of 92561951040.725418, Test MSE of 92561951499.822937
Epoch 20: training loss 104391138469.647
Test Loss of 87735487601.461945, Test MSE of 87735487211.695251
Epoch 21: training loss 100693395983.059
Test Loss of 85311531694.456635, Test MSE of 85311530659.089294
Epoch 22: training loss 97741588495.059
Test Loss of 82301425383.069168, Test MSE of 82301426398.052948
Epoch 23: training loss 94079376293.647
Test Loss of 80816816029.461029, Test MSE of 80816815744.701401
Epoch 24: training loss 91594474074.353
Test Loss of 78104932951.761276, Test MSE of 78104933044.031830
Epoch 25: training loss 88852039966.118
Test Loss of 74568651389.660889, Test MSE of 74568651129.947006
Epoch 26: training loss 85308305234.824
Test Loss of 72768665182.630585, Test MSE of 72768667317.030487
Epoch 27: training loss 82254491866.353
Test Loss of 70988380386.923889, Test MSE of 70988381241.129715
Epoch 28: training loss 80118962040.471
Test Loss of 67633833218.901688, Test MSE of 67633832224.999725
Epoch 29: training loss 76143953494.588
Test Loss of 63289989415.143188, Test MSE of 63289989430.048340
Epoch 30: training loss 73274724472.471
Test Loss of 61919486385.950500, Test MSE of 61919486380.197189
Epoch 31: training loss 70325682816.000
Test Loss of 60444783968.703217, Test MSE of 60444783858.250793
Epoch 32: training loss 67942300995.765
Test Loss of 56128682834.372429, Test MSE of 56128682735.439987
Epoch 33: training loss 65932948239.059
Test Loss of 54182885914.648163, Test MSE of 54182886715.724953
Epoch 34: training loss 61796650966.588
Test Loss of 52837036885.688644, Test MSE of 52837037607.869026
Epoch 35: training loss 59269782900.706
Test Loss of 49141758910.386307, Test MSE of 49141758623.893654
Epoch 36: training loss 57112682044.235
Test Loss of 47498730527.977791, Test MSE of 47498730623.639000
Epoch 37: training loss 54076142573.176
Test Loss of 45074080001.717323, Test MSE of 45074080467.511955
Epoch 38: training loss 52400444431.059
Test Loss of 41550670375.913025, Test MSE of 41550669636.684143
Epoch 39: training loss 50009865758.118
Test Loss of 43279088071.269028, Test MSE of 43279088559.741341
Epoch 40: training loss 47626873569.882
Test Loss of 40235049820.321075, Test MSE of 40235050447.068672
Epoch 41: training loss 45792626582.588
Test Loss of 41021224421.351837, Test MSE of 41021223711.243263
Epoch 42: training loss 44095803181.176
Test Loss of 38639240616.712471, Test MSE of 38639241654.818901
Epoch 43: training loss 41331029827.765
Test Loss of 39485708891.077492, Test MSE of 39485709187.950851
Epoch 44: training loss 39564104892.235
Test Loss of 33860871934.756420, Test MSE of 33860871877.647221
Epoch 45: training loss 38202247484.235
Test Loss of 30195769739.103401, Test MSE of 30195770386.503208
Epoch 46: training loss 36269034789.647
Test Loss of 29700790577.565578, Test MSE of 29700791050.996040
Epoch 47: training loss 34708936011.294
Test Loss of 29098144187.425400, Test MSE of 29098144810.704262
Epoch 48: training loss 32925631819.294
Test Loss of 28305352412.646774, Test MSE of 28305353099.863281
Epoch 49: training loss 31570918113.882
Test Loss of 30065591975.587322, Test MSE of 30065592157.661232
Epoch 50: training loss 30281932807.529
Test Loss of 27932623646.734211, Test MSE of 27932624255.905766
Epoch 51: training loss 28795796852.706
Test Loss of 24467567024.055515, Test MSE of 24467567067.184036
Epoch 52: training loss 27731337031.529
Test Loss of 25366656484.878094, Test MSE of 25366656769.136040
Epoch 53: training loss 26820080071.529
Test Loss of 25140049767.454082, Test MSE of 25140049826.097359
Epoch 54: training loss 25158352064.000
Test Loss of 23668648525.812630, Test MSE of 23668648231.207813
Epoch 55: training loss 24148356001.882
Test Loss of 23501025442.020821, Test MSE of 23501026014.481213
Epoch 56: training loss 23645918524.235
Test Loss of 22757991581.046497, Test MSE of 22757991932.240589
Epoch 57: training loss 22820277884.235
Test Loss of 20562188040.705067, Test MSE of 20562187904.290951
Epoch 58: training loss 21899772265.412
Test Loss of 20795429096.608837, Test MSE of 20795429357.966660
Epoch 59: training loss 20977358701.176
Test Loss of 21447921821.757114, Test MSE of 21447921757.919018
Epoch 60: training loss 20283813244.235
Test Loss of 19202956591.433727, Test MSE of 19202956703.106495
Epoch 61: training loss 19839452788.706
Test Loss of 20011816966.632431, Test MSE of 20011816821.240040
Epoch 62: training loss 19058675184.941
Test Loss of 21329693709.501736, Test MSE of 21329693730.045876
Epoch 63: training loss 18078695341.176
Test Loss of 21307247503.959286, Test MSE of 21307247464.342896
Epoch 64: training loss 18021110505.412
Test Loss of 21969451722.881332, Test MSE of 21969451940.588547
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21969451940.588547, 'MSE - std': 0.0, 'R2 - mean': 0.8289214940712332, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005351 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917877729.882
Test Loss of 424556781732.863281, Test MSE of 424556784285.177307
Epoch 2: training loss 427897128116.706
Test Loss of 424540271656.031433, Test MSE of 424540269501.211914
Epoch 3: training loss 427869112199.529
Test Loss of 424517953767.661316, Test MSE of 424517952129.374512
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888613496.471
Test Loss of 424524066231.398560, Test MSE of 424524067753.947815
Epoch 2: training loss 427877253120.000
Test Loss of 424524834403.368042, Test MSE of 424524835506.548767
Epoch 3: training loss 427876591736.471
Test Loss of 424524101872.188782, Test MSE of 424524104742.399231
Epoch 4: training loss 427876154910.118
Test Loss of 424523278503.231995, Test MSE of 424523277932.499817
Epoch 5: training loss 421094928143.059
Test Loss of 403394945689.138123, Test MSE of 403394952223.654541
Epoch 6: training loss 375408869014.588
Test Loss of 337451885940.600525, Test MSE of 337451886268.469666
Epoch 7: training loss 296491954176.000
Test Loss of 253571111711.207947, Test MSE of 253571111015.458832
Epoch 8: training loss 216633256598.588
Test Loss of 184592551326.763824, Test MSE of 184592552127.989655
Epoch 9: training loss 158461345370.353
Test Loss of 138673706791.498505, Test MSE of 138673705267.817200
Epoch 10: training loss 138074746819.765
Test Loss of 130336912929.991211, Test MSE of 130336914937.583160
Epoch 11: training loss 133550597270.588
Test Loss of 126605934101.200089, Test MSE of 126605932859.115448
Epoch 12: training loss 130347207981.176
Test Loss of 124602552677.203796, Test MSE of 124602553990.536530
Epoch 13: training loss 127100462682.353
Test Loss of 120899514147.234787, Test MSE of 120899513287.666077
Epoch 14: training loss 122672502031.059
Test Loss of 118066982234.544525, Test MSE of 118066981445.117706
Epoch 15: training loss 120934963139.765
Test Loss of 115349765243.884338, Test MSE of 115349769695.530716
Epoch 16: training loss 117536242537.412
Test Loss of 112799058604.561646, Test MSE of 112799056866.995056
Epoch 17: training loss 112464735533.176
Test Loss of 109403098653.727509, Test MSE of 109403097702.274887
Epoch 18: training loss 108734153065.412
Test Loss of 104980437485.168640, Test MSE of 104980437639.726593
Epoch 19: training loss 105542415269.647
Test Loss of 101166074309.374039, Test MSE of 101166073536.634216
Epoch 20: training loss 101749998652.235
Test Loss of 100716030762.577835, Test MSE of 100716030482.038818
Epoch 21: training loss 99018932886.588
Test Loss of 95338634393.256531, Test MSE of 95338633157.157288
Epoch 22: training loss 95393678697.412
Test Loss of 91830669854.911865, Test MSE of 91830670229.806763
Epoch 23: training loss 92079523599.059
Test Loss of 89067133411.693726, Test MSE of 89067131802.756363
Epoch 24: training loss 89042732333.176
Test Loss of 86793709194.215134, Test MSE of 86793708313.080811
Epoch 25: training loss 85081513381.647
Test Loss of 85843753997.027985, Test MSE of 85843754077.809586
Epoch 26: training loss 81978397545.412
Test Loss of 78822721021.986588, Test MSE of 78822720232.339478
Epoch 27: training loss 79148272067.765
Test Loss of 77530585891.945404, Test MSE of 77530585516.182816
Epoch 28: training loss 76022070693.647
Test Loss of 73265329192.031464, Test MSE of 73265327070.756256
Epoch 29: training loss 74102186646.588
Test Loss of 73516262977.258392, Test MSE of 73516260702.633896
Epoch 30: training loss 70399651674.353
Test Loss of 69753271084.946564, Test MSE of 69753272221.642975
Epoch 31: training loss 67062464030.118
Test Loss of 65054319151.966690, Test MSE of 65054320488.844414
Epoch 32: training loss 64052092988.235
Test Loss of 61918786024.904930, Test MSE of 61918785828.544792
Epoch 33: training loss 61254077876.706
Test Loss of 60355264180.378441, Test MSE of 60355264208.889114
Epoch 34: training loss 58990116412.235
Test Loss of 59560847212.665276, Test MSE of 59560846476.921135
Epoch 35: training loss 56423097328.941
Test Loss of 54127474151.720566, Test MSE of 54127474564.302002
Epoch 36: training loss 53582862772.706
Test Loss of 52992397377.850563, Test MSE of 52992397248.863632
Epoch 37: training loss 50840071966.118
Test Loss of 51902339278.552856, Test MSE of 51902339650.779480
Epoch 38: training loss 48601269775.059
Test Loss of 48874473693.238953, Test MSE of 48874473921.624176
Epoch 39: training loss 46103483776.000
Test Loss of 47232142278.439972, Test MSE of 47232140650.694977
Epoch 40: training loss 44346073426.824
Test Loss of 45659362441.622948, Test MSE of 45659361791.844971
Epoch 41: training loss 42206486851.765
Test Loss of 44675143048.024055, Test MSE of 44675144341.920593
Epoch 42: training loss 40003293605.647
Test Loss of 41566365574.721260, Test MSE of 41566367424.233963
Epoch 43: training loss 38499910942.118
Test Loss of 41823262174.245667, Test MSE of 41823261090.822815
Epoch 44: training loss 36047657615.059
Test Loss of 39014748400.188759, Test MSE of 39014747161.322777
Epoch 45: training loss 34670517760.000
Test Loss of 36552583006.926674, Test MSE of 36552582946.219948
Epoch 46: training loss 32991283478.588
Test Loss of 37876322273.680313, Test MSE of 37876322417.980743
Epoch 47: training loss 30868084306.824
Test Loss of 34456209974.125374, Test MSE of 34456209171.622681
Epoch 48: training loss 29424700107.294
Test Loss of 37354913725.675690, Test MSE of 37354913607.309128
Epoch 49: training loss 28628678603.294
Test Loss of 32095804686.745316, Test MSE of 32095803870.193325
Epoch 50: training loss 26950099696.941
Test Loss of 34757404252.735603, Test MSE of 34757404806.660400
Epoch 51: training loss 25544599119.059
Test Loss of 33365996327.735371, Test MSE of 33365995986.969662
Epoch 52: training loss 24675924973.176
Test Loss of 32553048645.758965, Test MSE of 32553048489.508442
Epoch 53: training loss 23293192719.059
Test Loss of 31408659956.511681, Test MSE of 31408660230.715084
Epoch 54: training loss 22510170774.588
Test Loss of 29130686016.784641, Test MSE of 29130686063.723183
Epoch 55: training loss 21686190076.235
Test Loss of 31707565674.474209, Test MSE of 31707564864.984024
Epoch 56: training loss 20489667949.176
Test Loss of 30018165322.496414, Test MSE of 30018165361.650322
Epoch 57: training loss 19949156811.294
Test Loss of 29631330537.556328, Test MSE of 29631330347.764038
Epoch 58: training loss 18856762567.529
Test Loss of 29042400672.658802, Test MSE of 29042401354.594986
Epoch 59: training loss 18156532555.294
Test Loss of 27492020288.903076, Test MSE of 27492020435.424236
Epoch 60: training loss 17553671126.588
Test Loss of 26979942287.011799, Test MSE of 26979941242.742310
Epoch 61: training loss 16926721464.471
Test Loss of 30201326659.508675, Test MSE of 30201326867.420265
Epoch 62: training loss 16498408161.882
Test Loss of 28776915974.869305, Test MSE of 28776915441.025227
Epoch 63: training loss 15950866277.647
Test Loss of 27990492790.791580, Test MSE of 27990492668.007687
Epoch 64: training loss 15080925319.529
Test Loss of 25723998416.210964, Test MSE of 25723998956.852997
Epoch 65: training loss 14811449114.353
Test Loss of 26668070332.846634, Test MSE of 26668070141.112476
Epoch 66: training loss 14358020141.176
Test Loss of 26612956928.177654, Test MSE of 26612957336.855045
Epoch 67: training loss 13977742464.000
Test Loss of 28375373496.879021, Test MSE of 28375373730.879814
Epoch 68: training loss 13738500498.824
Test Loss of 29824680827.114502, Test MSE of 29824679739.842918
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25897065840.215733, 'MSE - std': 3927613899.627186, 'R2 - mean': 0.8079965410899191, 'R2 - std': 0.020924952981314027} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003586 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926946936.471
Test Loss of 447258769701.248230, Test MSE of 447258763361.887207
Epoch 2: training loss 421906674386.824
Test Loss of 447240932857.722900, Test MSE of 447240934297.182251
Epoch 3: training loss 421880748032.000
Test Loss of 447217334606.937805, Test MSE of 447217334324.574951
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902096865.882
Test Loss of 447226013222.728638, Test MSE of 447226022744.782532
Epoch 2: training loss 421889358305.882
Test Loss of 447226358321.387939, Test MSE of 447226352322.922668
Epoch 3: training loss 421888902565.647
Test Loss of 447226046346.984985, Test MSE of 447226049765.586731
Epoch 4: training loss 421888586812.235
Test Loss of 447225350495.045105, Test MSE of 447225347017.856628
Epoch 5: training loss 414865712670.118
Test Loss of 425049741925.499878, Test MSE of 425049746618.920288
Epoch 6: training loss 369101603177.412
Test Loss of 357541479499.088623, Test MSE of 357541479802.702881
Epoch 7: training loss 290690802748.235
Test Loss of 270494359019.510529, Test MSE of 270494359233.424469
Epoch 8: training loss 212423962804.706
Test Loss of 199551547859.586395, Test MSE of 199551548005.665894
Epoch 9: training loss 155896325692.235
Test Loss of 150516112515.464264, Test MSE of 150516111967.368134
Epoch 10: training loss 134975531339.294
Test Loss of 140966113976.405273, Test MSE of 140966118690.458435
Epoch 11: training loss 131174295160.471
Test Loss of 137666596554.170715, Test MSE of 137666598244.552338
Epoch 12: training loss 128908399043.765
Test Loss of 134174751672.701370, Test MSE of 134174749068.155991
Epoch 13: training loss 126305027132.235
Test Loss of 130405895472.854965, Test MSE of 130405896393.535110
Epoch 14: training loss 122317019136.000
Test Loss of 126311715294.482529, Test MSE of 126311713800.689713
Epoch 15: training loss 118950670607.059
Test Loss of 125193722746.167007, Test MSE of 125193722389.119202
Epoch 16: training loss 115801218951.529
Test Loss of 121803827503.670593, Test MSE of 121803829587.431198
Epoch 17: training loss 111812833370.353
Test Loss of 116087230514.216980, Test MSE of 116087230077.748779
Epoch 18: training loss 108353288402.824
Test Loss of 114585478725.048340, Test MSE of 114585478223.134415
Epoch 19: training loss 105023948920.471
Test Loss of 109955189538.050430, Test MSE of 109955193310.898163
Epoch 20: training loss 101473331320.471
Test Loss of 106784583068.868835, Test MSE of 106784583116.053146
Epoch 21: training loss 98206730571.294
Test Loss of 103975422200.716171, Test MSE of 103975421393.224319
Epoch 22: training loss 94262078283.294
Test Loss of 102524500647.824203, Test MSE of 102524500890.305405
Epoch 23: training loss 90982266398.118
Test Loss of 95969383623.209808, Test MSE of 95969383811.456284
Epoch 24: training loss 88430775070.118
Test Loss of 93355486865.321304, Test MSE of 93355488559.949036
Epoch 25: training loss 84613285616.941
Test Loss of 90545114316.421005, Test MSE of 90545113905.211823
Epoch 26: training loss 81629216286.118
Test Loss of 87824974968.568130, Test MSE of 87824975001.558441
Epoch 27: training loss 78982254140.235
Test Loss of 84095964209.506363, Test MSE of 84095965927.248611
Epoch 28: training loss 76022369295.059
Test Loss of 81214073262.634277, Test MSE of 81214073710.274551
Epoch 29: training loss 72132129912.471
Test Loss of 77298821342.186447, Test MSE of 77298822394.879211
Epoch 30: training loss 70387880643.765
Test Loss of 73278155059.460556, Test MSE of 73278155011.894547
Epoch 31: training loss 66621259008.000
Test Loss of 69483256944.514450, Test MSE of 69483257670.371246
Epoch 32: training loss 64517882443.294
Test Loss of 69129712873.319458, Test MSE of 69129713704.874405
Epoch 33: training loss 62263520090.353
Test Loss of 69468603379.919495, Test MSE of 69468603389.940796
Epoch 34: training loss 58913226608.941
Test Loss of 65588539536.965996, Test MSE of 65588539468.696075
Epoch 35: training loss 56903469040.941
Test Loss of 64384186143.918571, Test MSE of 64384187771.861458
Epoch 36: training loss 54404096225.882
Test Loss of 57640074547.697433, Test MSE of 57640075150.688919
Epoch 37: training loss 52083320395.294
Test Loss of 57199537045.644226, Test MSE of 57199536118.454529
Epoch 38: training loss 49220792146.824
Test Loss of 53693219695.270874, Test MSE of 53693219826.191505
Epoch 39: training loss 47405381895.529
Test Loss of 52797153624.886421, Test MSE of 52797153071.442543
Epoch 40: training loss 44613971222.588
Test Loss of 49550090829.338882, Test MSE of 49550089706.712082
Epoch 41: training loss 42579093345.882
Test Loss of 46595441161.119591, Test MSE of 46595441386.563889
Epoch 42: training loss 42028596487.529
Test Loss of 46699738523.447609, Test MSE of 46699737708.382790
Epoch 43: training loss 39651238550.588
Test Loss of 40483202881.080734, Test MSE of 40483203275.225685
Epoch 44: training loss 37764871401.412
Test Loss of 40081025466.004166, Test MSE of 40081025584.405853
Epoch 45: training loss 36267736636.235
Test Loss of 43989795153.306503, Test MSE of 43989796417.425240
Epoch 46: training loss 34452579041.882
Test Loss of 37540955613.771919, Test MSE of 37540956177.364227
Epoch 47: training loss 32564394624.000
Test Loss of 38771707665.232475, Test MSE of 38771708315.626991
Epoch 48: training loss 31166037270.588
Test Loss of 38482434011.521629, Test MSE of 38482434124.817253
Epoch 49: training loss 30104635444.706
Test Loss of 35678833974.539902, Test MSE of 35678834978.973412
Epoch 50: training loss 28495349135.059
Test Loss of 33689033277.705296, Test MSE of 33689033816.630669
Epoch 51: training loss 27680403855.059
Test Loss of 34896383786.814713, Test MSE of 34896384010.140587
Epoch 52: training loss 26322149737.412
Test Loss of 31601615741.483231, Test MSE of 31601615875.434452
Epoch 53: training loss 24932380318.118
Test Loss of 33736094241.754337, Test MSE of 33736095147.313549
Epoch 54: training loss 23965828374.588
Test Loss of 31447314897.217674, Test MSE of 31447314771.738754
Epoch 55: training loss 22954030042.353
Test Loss of 28560742264.035160, Test MSE of 28560741824.661205
Epoch 56: training loss 22098352139.294
Test Loss of 29994010367.703911, Test MSE of 29994010458.918583
Epoch 57: training loss 21113407838.118
Test Loss of 28536017016.331253, Test MSE of 28536017000.302650
Epoch 58: training loss 20403716924.235
Test Loss of 25374755327.644691, Test MSE of 25374754925.783821
Epoch 59: training loss 20063789665.882
Test Loss of 25427670613.629425, Test MSE of 25427670379.342873
Epoch 60: training loss 18884319525.647
Test Loss of 25112112441.382374, Test MSE of 25112112856.078049
Epoch 61: training loss 18817646320.941
Test Loss of 25578257671.639141, Test MSE of 25578257766.202740
Epoch 62: training loss 18015218187.294
Test Loss of 26070000969.963451, Test MSE of 26070001268.346474
Epoch 63: training loss 17381879570.824
Test Loss of 24512120339.778858, Test MSE of 24512120930.087341
Epoch 64: training loss 16813737468.235
Test Loss of 25638922654.526947, Test MSE of 25638922976.271187
Epoch 65: training loss 16392666010.353
Test Loss of 25600833077.414757, Test MSE of 25600833162.240376
Epoch 66: training loss 15872171056.941
Test Loss of 25547787214.967384, Test MSE of 25547787293.881687
Epoch 67: training loss 15855834586.353
Test Loss of 27033670239.814945, Test MSE of 27033670706.330841
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26275934128.920765, 'MSE - std': 3251335570.2032685, 'R2 - mean': 0.8120105734508275, 'R2 - std': 0.018003537146043688} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005296 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110105359.059
Test Loss of 410766708006.263794, Test MSE of 410766713614.746399
Epoch 2: training loss 430090212773.647
Test Loss of 410749510826.113831, Test MSE of 410749508396.243225
Epoch 3: training loss 430063664911.059
Test Loss of 410727388756.583069, Test MSE of 410727388226.844971
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430083284028.235
Test Loss of 410730001720.270264, Test MSE of 410730000753.949036
Epoch 2: training loss 430070468608.000
Test Loss of 410731776044.542358, Test MSE of 410731775145.510437
Epoch 3: training loss 430070052020.706
Test Loss of 410732484127.511353, Test MSE of 410732483295.159119
Epoch 4: training loss 430069739279.059
Test Loss of 410733078955.416931, Test MSE of 410733075941.185120
Epoch 5: training loss 423438851132.235
Test Loss of 389763125253.686279, Test MSE of 389763128007.666443
Epoch 6: training loss 378262297539.765
Test Loss of 323637773849.825073, Test MSE of 323637778067.227905
Epoch 7: training loss 299457139651.765
Test Loss of 238762290640.851471, Test MSE of 238762295023.355713
Epoch 8: training loss 220353640809.412
Test Loss of 168285625393.754730, Test MSE of 168285621809.602142
Epoch 9: training loss 163394652702.118
Test Loss of 121329094108.223969, Test MSE of 121329093471.720200
Epoch 10: training loss 141984321295.059
Test Loss of 113108564810.513657, Test MSE of 113108564379.519867
Epoch 11: training loss 139246199205.647
Test Loss of 110127800217.647385, Test MSE of 110127799787.531097
Epoch 12: training loss 135581374012.235
Test Loss of 107628267007.289215, Test MSE of 107628267273.861588
Epoch 13: training loss 133017254640.941
Test Loss of 105296885480.425735, Test MSE of 105296887608.868576
Epoch 14: training loss 128536223081.412
Test Loss of 101483829245.156876, Test MSE of 101483829076.470215
Epoch 15: training loss 124410636077.176
Test Loss of 99730592310.730225, Test MSE of 99730592883.070145
Epoch 16: training loss 120787754947.765
Test Loss of 96114299541.501160, Test MSE of 96114299515.780304
Epoch 17: training loss 118453080515.765
Test Loss of 92957288242.347061, Test MSE of 92957289801.869217
Epoch 18: training loss 113847136406.588
Test Loss of 91089787798.330399, Test MSE of 91089786964.376801
Epoch 19: training loss 109964472922.353
Test Loss of 88203928928.074036, Test MSE of 88203929644.049942
Epoch 20: training loss 106582679431.529
Test Loss of 84790699746.739471, Test MSE of 84790699737.448471
Epoch 21: training loss 104284288000.000
Test Loss of 82975345469.245712, Test MSE of 82975347580.291946
Epoch 22: training loss 100233427395.765
Test Loss of 79785292054.626556, Test MSE of 79785291031.373550
Epoch 23: training loss 97897345776.941
Test Loss of 76707630446.289688, Test MSE of 76707631566.784256
Epoch 24: training loss 94016184425.412
Test Loss of 75722534201.217957, Test MSE of 75722535052.036179
Epoch 25: training loss 90603277101.176
Test Loss of 72071219422.711700, Test MSE of 72071219718.271545
Epoch 26: training loss 86599538748.235
Test Loss of 69106929730.339661, Test MSE of 69106930723.660568
Epoch 27: training loss 83819642895.059
Test Loss of 66298458254.156410, Test MSE of 66298457919.916847
Epoch 28: training loss 80727055646.118
Test Loss of 64155475532.053680, Test MSE of 64155475918.545456
Epoch 29: training loss 78582414878.118
Test Loss of 61398045344.873672, Test MSE of 61398045600.957428
Epoch 30: training loss 75205538529.882
Test Loss of 58897699250.050903, Test MSE of 58897700628.425896
Epoch 31: training loss 72975417720.471
Test Loss of 56634656949.012497, Test MSE of 56634657817.619827
Epoch 32: training loss 70140033581.176
Test Loss of 54302296059.261452, Test MSE of 54302296046.798813
Epoch 33: training loss 66842385483.294
Test Loss of 52701206329.928734, Test MSE of 52701206290.992821
Epoch 34: training loss 64407684848.941
Test Loss of 50381170014.178619, Test MSE of 50381170278.213699
Epoch 35: training loss 61795748193.882
Test Loss of 47926569656.566406, Test MSE of 47926570250.213928
Epoch 36: training loss 59636310256.941
Test Loss of 46558930952.529381, Test MSE of 46558930913.203697
Epoch 37: training loss 56584215529.412
Test Loss of 43297016496.984726, Test MSE of 43297016757.025093
Epoch 38: training loss 53982817588.706
Test Loss of 42598323138.872742, Test MSE of 42598323280.496643
Epoch 39: training loss 51953524894.118
Test Loss of 40721871894.271172, Test MSE of 40721872603.489868
Epoch 40: training loss 49102085775.059
Test Loss of 39805229130.869041, Test MSE of 39805228429.684303
Epoch 41: training loss 47137687875.765
Test Loss of 39369882804.538643, Test MSE of 39369882730.820717
Epoch 42: training loss 45456237575.529
Test Loss of 35486844093.068024, Test MSE of 35486843509.866425
Epoch 43: training loss 42918717891.765
Test Loss of 35045868287.644608, Test MSE of 35045867669.157829
Epoch 44: training loss 41530967913.412
Test Loss of 33509181222.974548, Test MSE of 33509180677.961201
Epoch 45: training loss 39383196265.412
Test Loss of 33951887329.673298, Test MSE of 33951887303.444035
Epoch 46: training loss 37536368007.529
Test Loss of 30545554413.519669, Test MSE of 30545555412.999680
Epoch 47: training loss 35927396284.235
Test Loss of 30024213945.158722, Test MSE of 30024214058.153454
Epoch 48: training loss 34302494418.824
Test Loss of 30136813676.986580, Test MSE of 30136814305.064716
Epoch 49: training loss 32402879307.294
Test Loss of 26934847814.959740, Test MSE of 26934847808.396988
Epoch 50: training loss 31462938458.353
Test Loss of 26899965194.780193, Test MSE of 26899965122.142853
Epoch 51: training loss 30148794352.941
Test Loss of 26417377914.965294, Test MSE of 26417378013.956585
Epoch 52: training loss 28379981903.059
Test Loss of 25040903823.341045, Test MSE of 25040903777.730953
Epoch 53: training loss 27620844871.529
Test Loss of 24950719932.949562, Test MSE of 24950720081.521454
Epoch 54: training loss 26482930763.294
Test Loss of 21715504571.054142, Test MSE of 21715504529.732082
Epoch 55: training loss 25505582689.882
Test Loss of 22868233454.348911, Test MSE of 22868233356.913681
Epoch 56: training loss 24406128018.824
Test Loss of 20719243504.718185, Test MSE of 20719243534.898895
Epoch 57: training loss 23468569494.588
Test Loss of 21048929375.244793, Test MSE of 21048929419.272652
Epoch 58: training loss 22604081859.765
Test Loss of 21814464011.609440, Test MSE of 21814463934.608162
Epoch 59: training loss 21835974983.529
Test Loss of 20718669775.666821, Test MSE of 20718669862.857548
Epoch 60: training loss 21018753750.588
Test Loss of 20066651715.524296, Test MSE of 20066651423.690136
Epoch 61: training loss 20321279198.118
Test Loss of 21009034315.342896, Test MSE of 21009034636.232204
Epoch 62: training loss 19611485212.235
Test Loss of 19734780807.167053, Test MSE of 19734780837.215164
Epoch 63: training loss 18918755196.235
Test Loss of 19897750404.323925, Test MSE of 19897750308.265446
Epoch 64: training loss 18548842639.059
Test Loss of 20982250602.617306, Test MSE of 20982250416.300434
Epoch 65: training loss 17684632527.059
Test Loss of 18804807965.260529, Test MSE of 18804808093.433537
Epoch 66: training loss 17621895190.588
Test Loss of 21719136171.653866, Test MSE of 21719136194.068222
Epoch 67: training loss 17265737680.941
Test Loss of 20108872713.003239, Test MSE of 20108872582.872646
Epoch 68: training loss 16191409618.824
Test Loss of 18654123998.830170, Test MSE of 18654124140.269768
Epoch 69: training loss 15696718377.412
Test Loss of 20902963274.869041, Test MSE of 20902963084.051655
Epoch 70: training loss 15202510415.059
Test Loss of 18534373317.715874, Test MSE of 18534373238.457970
Epoch 71: training loss 14970380303.059
Test Loss of 18929150908.238777, Test MSE of 18929151308.119591
Epoch 72: training loss 14552490063.059
Test Loss of 19919206125.164276, Test MSE of 19919206140.672916
Epoch 73: training loss 14391491817.412
Test Loss of 19782671108.383156, Test MSE of 19782670971.611912
Epoch 74: training loss 13976533372.235
Test Loss of 18609697121.495605, Test MSE of 18609697538.017879
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24359374981.195045, 'MSE - std': 4352928226.23406, 'R2 - mean': 0.8206090469813014, 'R2 - std': 0.021561464548808182} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005253 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042640685.176
Test Loss of 431613018703.370667, Test MSE of 431613019098.389954
Epoch 2: training loss 424022731956.706
Test Loss of 431592980351.111511, Test MSE of 431592981849.534180
Epoch 3: training loss 423995403806.118
Test Loss of 431565715449.366028, Test MSE of 431565710281.492615
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008923256.471
Test Loss of 431571146279.093018, Test MSE of 431571149009.994019
Epoch 2: training loss 424000080474.353
Test Loss of 431574003039.126343, Test MSE of 431574007711.689697
Epoch 3: training loss 423999479567.059
Test Loss of 431573956881.888000, Test MSE of 431573957648.195740
Epoch 4: training loss 423999029248.000
Test Loss of 431572678687.274414, Test MSE of 431572678639.276489
Epoch 5: training loss 417235357936.941
Test Loss of 409218719725.045837, Test MSE of 409218723437.254944
Epoch 6: training loss 371912125741.176
Test Loss of 341328140300.320251, Test MSE of 341328136645.181030
Epoch 7: training loss 294033457152.000
Test Loss of 253793476717.934296, Test MSE of 253793476396.324738
Epoch 8: training loss 214821926098.824
Test Loss of 183136507332.531250, Test MSE of 183136511069.663391
Epoch 9: training loss 158901103796.706
Test Loss of 134418698025.817673, Test MSE of 134418697427.117279
Epoch 10: training loss 138417676920.471
Test Loss of 124546695804.860718, Test MSE of 124546695380.539139
Epoch 11: training loss 133624851727.059
Test Loss of 121495868356.768158, Test MSE of 121495869776.605438
Epoch 12: training loss 133169851482.353
Test Loss of 118722350563.805649, Test MSE of 118722351274.848221
Epoch 13: training loss 128615691595.294
Test Loss of 115085593018.580292, Test MSE of 115085594272.259399
Epoch 14: training loss 124143908261.647
Test Loss of 111601479132.223969, Test MSE of 111601479025.535110
Epoch 15: training loss 121495995090.824
Test Loss of 108026652830.267471, Test MSE of 108026653359.241150
Epoch 16: training loss 117826313999.059
Test Loss of 105192671583.600189, Test MSE of 105192670484.939011
Epoch 17: training loss 113464579674.353
Test Loss of 101031090157.993515, Test MSE of 101031090887.603821
Epoch 18: training loss 111309030881.882
Test Loss of 97974453584.910690, Test MSE of 97974455183.862259
Epoch 19: training loss 106916633630.118
Test Loss of 96355656191.763077, Test MSE of 96355656621.833206
Epoch 20: training loss 104811606377.412
Test Loss of 92454842699.698288, Test MSE of 92454843438.965088
Epoch 21: training loss 101053064320.000
Test Loss of 88019032716.971771, Test MSE of 88019033082.529495
Epoch 22: training loss 97716900939.294
Test Loss of 86363857017.306808, Test MSE of 86363856380.022202
Epoch 23: training loss 94307546383.059
Test Loss of 83054497149.453033, Test MSE of 83054497627.007919
Epoch 24: training loss 91246362503.529
Test Loss of 80687606232.906982, Test MSE of 80687606334.723694
Epoch 25: training loss 87955674330.353
Test Loss of 77437289501.852844, Test MSE of 77437290327.775314
Epoch 26: training loss 84904260833.882
Test Loss of 73763811631.267014, Test MSE of 73763812101.118759
Epoch 27: training loss 81818082680.471
Test Loss of 69792521663.792694, Test MSE of 69792520219.751358
Epoch 28: training loss 79649957767.529
Test Loss of 67301042641.325310, Test MSE of 67301043248.565842
Epoch 29: training loss 75375427584.000
Test Loss of 65293153035.017120, Test MSE of 65293153354.573204
Epoch 30: training loss 72441141948.235
Test Loss of 63834711335.211479, Test MSE of 63834712999.895226
Epoch 31: training loss 70055422381.176
Test Loss of 60478762581.530769, Test MSE of 60478762216.778023
Epoch 32: training loss 67667530631.529
Test Loss of 55876314213.878761, Test MSE of 55876315027.096863
Epoch 33: training loss 64634338454.588
Test Loss of 54489402232.003700, Test MSE of 54489402573.642929
Epoch 34: training loss 61418888975.059
Test Loss of 50158650682.165665, Test MSE of 50158649799.706398
Epoch 35: training loss 59582207774.118
Test Loss of 49654094715.794540, Test MSE of 49654094414.346558
Epoch 36: training loss 56789389839.059
Test Loss of 47991215363.198517, Test MSE of 47991214470.149208
Epoch 37: training loss 54334940845.176
Test Loss of 45809071672.151779, Test MSE of 45809071606.018158
Epoch 38: training loss 51627559160.471
Test Loss of 45013251297.080978, Test MSE of 45013250175.413597
Epoch 39: training loss 49967759036.235
Test Loss of 46261652812.172142, Test MSE of 46261652744.272591
Epoch 40: training loss 47883671725.176
Test Loss of 41782315554.828323, Test MSE of 41782314682.343651
Epoch 41: training loss 45779500958.118
Test Loss of 38250286119.329941, Test MSE of 38250285187.741554
Epoch 42: training loss 44061715915.294
Test Loss of 37201890945.125404, Test MSE of 37201890160.589645
Epoch 43: training loss 41695157586.824
Test Loss of 35563627846.485886, Test MSE of 35563627776.288322
Epoch 44: training loss 40286834093.176
Test Loss of 35284058192.555298, Test MSE of 35284058651.981812
Epoch 45: training loss 37870567514.353
Test Loss of 33500967525.167976, Test MSE of 33500967571.192589
Epoch 46: training loss 36448074774.588
Test Loss of 30327064178.909763, Test MSE of 30327064388.362747
Epoch 47: training loss 34767864267.294
Test Loss of 31761457169.058769, Test MSE of 31761457301.557915
Epoch 48: training loss 33228212487.529
Test Loss of 28188258587.365108, Test MSE of 28188258629.073940
Epoch 49: training loss 32042107248.941
Test Loss of 26288016201.565941, Test MSE of 26288015994.985741
Epoch 50: training loss 30608972845.176
Test Loss of 27642575874.369274, Test MSE of 27642576201.972916
Epoch 51: training loss 29116858631.529
Test Loss of 26557161879.515038, Test MSE of 26557161770.861935
Epoch 52: training loss 27918273701.647
Test Loss of 26371933820.386856, Test MSE of 26371933502.406239
Epoch 53: training loss 26695779154.824
Test Loss of 24853662509.134659, Test MSE of 24853662622.708973
Epoch 54: training loss 25721111544.471
Test Loss of 22531288843.490978, Test MSE of 22531289375.875114
Epoch 55: training loss 24946760929.882
Test Loss of 23156962287.415085, Test MSE of 23156962557.162296
Epoch 56: training loss 23623377110.588
Test Loss of 22972828146.495140, Test MSE of 22972828279.275703
Epoch 57: training loss 22964435550.118
Test Loss of 22307783621.715874, Test MSE of 22307783968.563599
Epoch 58: training loss 22369575032.471
Test Loss of 22656801304.403519, Test MSE of 22656800952.716423
Epoch 59: training loss 21318188186.353
Test Loss of 23659921371.987041, Test MSE of 23659921665.579464
Epoch 60: training loss 20366262784.000
Test Loss of 21990651778.902359, Test MSE of 21990651721.553020
Epoch 61: training loss 19834035060.706
Test Loss of 22486238407.492828, Test MSE of 22486237864.872261
Epoch 62: training loss 19457096470.588
Test Loss of 22207246536.914391, Test MSE of 22207246270.736149
Epoch 63: training loss 18782402744.471
Test Loss of 19680789534.800556, Test MSE of 19680789613.889057
Epoch 64: training loss 18400437985.882
Test Loss of 20398088109.549282, Test MSE of 20398088237.993587
Epoch 65: training loss 17806314940.235
Test Loss of 21270036679.018974, Test MSE of 21270036280.718716
Epoch 66: training loss 17271013647.059
Test Loss of 22240224788.138824, Test MSE of 22240225024.832787
Epoch 67: training loss 16676344018.824
Test Loss of 20170008693.515965, Test MSE of 20170008444.958549
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23521501673.947746, 'MSE - std': 4238692490.637445, 'R2 - mean': 0.8263611632003108, 'R2 - std': 0.022455840371261516} 
 

Saving model.....
Results After CV: {'MSE - mean': 23521501673.947746, 'MSE - std': 4238692490.637445, 'R2 - mean': 0.8263611632003108, 'R2 - std': 0.022455840371261516}
Train time: 103.5464379832003
Inference time: 0.09894298640028865
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 28 finished with value: 23521501673.947746 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005587 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524573304.471
Test Loss of 418110628436.918823, Test MSE of 418110627675.122498
Epoch 2: training loss 427502759936.000
Test Loss of 418091899709.527649, Test MSE of 418091899579.723938
Epoch 3: training loss 427474644751.059
Test Loss of 418067803071.333801, Test MSE of 418067799140.243958
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427488982076.235
Test Loss of 418074680872.149902, Test MSE of 418074678981.528992
Epoch 2: training loss 427480473359.059
Test Loss of 418076469491.031250, Test MSE of 418076471265.896362
Epoch 3: training loss 427480141583.059
Test Loss of 418076607817.015991, Test MSE of 418076603097.076782
Epoch 4: training loss 422117747772.235
Test Loss of 401173111168.681030, Test MSE of 401173113785.625427
Epoch 5: training loss 385765192282.353
Test Loss of 346287728098.509338, Test MSE of 346287725472.231812
Epoch 6: training loss 317145162029.176
Test Loss of 267656377304.442291, Test MSE of 267656370997.642334
Epoch 7: training loss 227758285281.882
Test Loss of 172768336944.085114, Test MSE of 172768338494.827332
Epoch 8: training loss 163546056101.647
Test Loss of 132912895427.242188, Test MSE of 132912892395.817871
Epoch 9: training loss 143669599081.412
Test Loss of 120731252896.599579, Test MSE of 120731254196.683548
Epoch 10: training loss 136087615247.059
Test Loss of 116031734156.050888, Test MSE of 116031735152.415665
Epoch 11: training loss 133376605906.824
Test Loss of 113029731980.110107, Test MSE of 113029732128.629272
Epoch 12: training loss 129346634330.353
Test Loss of 110593408821.237106, Test MSE of 110593407558.715698
Epoch 13: training loss 127108373232.941
Test Loss of 107722441168.270187, Test MSE of 107722442721.356430
Epoch 14: training loss 123985916687.059
Test Loss of 104631938301.690491, Test MSE of 104631935258.375443
Epoch 15: training loss 120469999465.412
Test Loss of 102095351411.001617, Test MSE of 102095349809.150192
Epoch 16: training loss 116527261575.529
Test Loss of 99011760871.779785, Test MSE of 99011757692.169861
Epoch 17: training loss 112712548803.765
Test Loss of 95747196121.212128, Test MSE of 95747197389.296661
Epoch 18: training loss 110203563248.941
Test Loss of 93926677055.600281, Test MSE of 93926676701.968475
Epoch 19: training loss 107132493296.941
Test Loss of 91311337850.048584, Test MSE of 91311336892.428619
Epoch 20: training loss 104040965029.647
Test Loss of 86536478109.816330, Test MSE of 86536480518.151550
Epoch 21: training loss 100774508122.353
Test Loss of 84473106844.158218, Test MSE of 84473105076.203064
Epoch 22: training loss 98263949884.235
Test Loss of 82233344073.904236, Test MSE of 82233343869.913605
Epoch 23: training loss 94221778281.412
Test Loss of 81448961290.481613, Test MSE of 81448960534.708832
Epoch 24: training loss 90750268370.824
Test Loss of 77320205048.834610, Test MSE of 77320206137.241104
Epoch 25: training loss 88077227038.118
Test Loss of 74343971378.335419, Test MSE of 74343970812.167847
Epoch 26: training loss 84703135013.647
Test Loss of 70569983530.044876, Test MSE of 70569983141.122803
Epoch 27: training loss 81315881517.176
Test Loss of 69689949717.200089, Test MSE of 69689948663.148270
Epoch 28: training loss 78213356144.941
Test Loss of 67908749242.359474, Test MSE of 67908749265.693893
Epoch 29: training loss 75100163230.118
Test Loss of 64386661456.299789, Test MSE of 64386659453.706734
Epoch 30: training loss 72460401152.000
Test Loss of 61511146591.459633, Test MSE of 61511146280.026749
Epoch 31: training loss 69615252894.118
Test Loss of 58069409570.287300, Test MSE of 58069409347.933441
Epoch 32: training loss 67215228536.471
Test Loss of 55360531963.617859, Test MSE of 55360532405.663994
Epoch 33: training loss 64586695461.647
Test Loss of 55663865372.780014, Test MSE of 55663865434.073273
Epoch 34: training loss 60969279290.353
Test Loss of 51094228465.195465, Test MSE of 51094228810.348640
Epoch 35: training loss 57728021101.176
Test Loss of 49651138945.628502, Test MSE of 49651139871.576805
Epoch 36: training loss 56476977603.765
Test Loss of 47651602688.769836, Test MSE of 47651602505.026779
Epoch 37: training loss 53869656101.647
Test Loss of 44298353108.533890, Test MSE of 44298353008.844162
Epoch 38: training loss 51189565718.588
Test Loss of 43463448692.067543, Test MSE of 43463448545.590897
Epoch 39: training loss 49101886960.941
Test Loss of 42136217468.772614, Test MSE of 42136217567.265182
Epoch 40: training loss 46692949677.176
Test Loss of 39256216246.510292, Test MSE of 39256216573.296799
Epoch 41: training loss 44683088429.176
Test Loss of 38507323313.358315, Test MSE of 38507322860.325951
Epoch 42: training loss 42970218627.765
Test Loss of 37188059768.923431, Test MSE of 37188059824.486893
Epoch 43: training loss 40832536756.706
Test Loss of 34451710756.419151, Test MSE of 34451711434.215141
Epoch 44: training loss 38600544105.412
Test Loss of 29451282272.347908, Test MSE of 29451282600.913277
Epoch 45: training loss 37104933782.588
Test Loss of 31899868182.502892, Test MSE of 31899868081.814011
Epoch 46: training loss 35167047834.353
Test Loss of 30301696783.574371, Test MSE of 30301696916.684395
Epoch 47: training loss 33647009483.294
Test Loss of 28987718772.304417, Test MSE of 28987718709.059578
Epoch 48: training loss 32569483414.588
Test Loss of 29385861061.729355, Test MSE of 29385861252.789104
Epoch 49: training loss 30453533854.118
Test Loss of 26243300324.285912, Test MSE of 26243300649.213566
Epoch 50: training loss 29336511420.235
Test Loss of 24955848366.219753, Test MSE of 24955848439.054207
Epoch 51: training loss 28044122386.824
Test Loss of 24282359122.964607, Test MSE of 24282359204.444386
Epoch 52: training loss 26854117888.000
Test Loss of 22899148277.932919, Test MSE of 22899148360.625385
Epoch 53: training loss 26158352707.765
Test Loss of 24146501505.983807, Test MSE of 24146501725.353741
Epoch 54: training loss 24852183732.706
Test Loss of 20844553485.797825, Test MSE of 20844553299.183296
Epoch 55: training loss 23827681584.941
Test Loss of 22098152641.051121, Test MSE of 22098152939.262726
Epoch 56: training loss 23491239585.882
Test Loss of 20629363131.425400, Test MSE of 20629363456.396008
Epoch 57: training loss 21986183424.000
Test Loss of 25470064066.294704, Test MSE of 25470064599.027500
Epoch 58: training loss 21360011105.882
Test Loss of 20741150286.996994, Test MSE of 20741150220.028404
Epoch 59: training loss 20172430851.765
Test Loss of 20784619419.566044, Test MSE of 20784619210.559162
Epoch 60: training loss 19625703744.000
Test Loss of 22229874328.427483, Test MSE of 22229873531.774391
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22229873531.77439, 'MSE - std': 0.0, 'R2 - mean': 0.8268935628851413, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005464 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918501044.706
Test Loss of 424557265415.224609, Test MSE of 424557264270.528870
Epoch 2: training loss 427898392576.000
Test Loss of 424541780265.038147, Test MSE of 424541777766.399170
Epoch 3: training loss 427871035632.941
Test Loss of 424520485764.589417, Test MSE of 424520479530.173401
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427893698319.059
Test Loss of 424525081887.800110, Test MSE of 424525090881.898743
Epoch 2: training loss 427880057434.353
Test Loss of 424526355018.970154, Test MSE of 424526348138.148743
Epoch 3: training loss 427879530977.882
Test Loss of 424525462748.528320, Test MSE of 424525462120.456055
Epoch 4: training loss 422713491456.000
Test Loss of 408325597389.131592, Test MSE of 408325598304.177979
Epoch 5: training loss 386624457788.235
Test Loss of 354493360981.214905, Test MSE of 354493360798.372803
Epoch 6: training loss 318077111717.647
Test Loss of 278188855675.943542, Test MSE of 278188858793.568359
Epoch 7: training loss 228598268566.588
Test Loss of 184739246461.838531, Test MSE of 184739246048.046234
Epoch 8: training loss 162719656478.118
Test Loss of 144080235737.685852, Test MSE of 144080238081.076508
Epoch 9: training loss 141525862550.588
Test Loss of 132671824874.444595, Test MSE of 132671825466.539673
Epoch 10: training loss 135665880425.412
Test Loss of 129080872168.845703, Test MSE of 129080871104.030716
Epoch 11: training loss 133498188739.765
Test Loss of 125352957470.201248, Test MSE of 125352952627.443253
Epoch 12: training loss 128768772608.000
Test Loss of 122973968728.412674, Test MSE of 122973970320.255035
Epoch 13: training loss 126712224195.765
Test Loss of 119726753205.029846, Test MSE of 119726752708.992615
Epoch 14: training loss 123053578480.941
Test Loss of 116823033833.733978, Test MSE of 116823033182.041748
Epoch 15: training loss 120604956461.176
Test Loss of 113221461711.381912, Test MSE of 113221462784.971436
Epoch 16: training loss 115983469206.588
Test Loss of 110386875406.922974, Test MSE of 110386876231.702850
Epoch 17: training loss 113545428269.176
Test Loss of 108080074684.728195, Test MSE of 108080074913.300247
Epoch 18: training loss 111161692129.882
Test Loss of 104636649488.107330, Test MSE of 104636650736.012039
Epoch 19: training loss 106476910682.353
Test Loss of 102148867646.415909, Test MSE of 102148867675.335373
Epoch 20: training loss 104615836702.118
Test Loss of 98588378924.235947, Test MSE of 98588379393.564087
Epoch 21: training loss 100409095680.000
Test Loss of 93816128768.059219, Test MSE of 93816129605.789871
Epoch 22: training loss 96593860909.176
Test Loss of 93494267535.426331, Test MSE of 93494268340.624069
Epoch 23: training loss 93555058989.176
Test Loss of 88809026848.273880, Test MSE of 88809026697.859497
Epoch 24: training loss 89533403617.882
Test Loss of 86534553375.444824, Test MSE of 86534555049.085739
Epoch 25: training loss 87793965643.294
Test Loss of 84362314378.925751, Test MSE of 84362315752.528778
Epoch 26: training loss 84582367322.353
Test Loss of 82542169934.819336, Test MSE of 82542167801.713516
Epoch 27: training loss 80354534565.647
Test Loss of 78459456673.783951, Test MSE of 78459456614.286758
Epoch 28: training loss 79482356976.941
Test Loss of 74004221529.419388, Test MSE of 74004222786.362976
Epoch 29: training loss 75416963132.235
Test Loss of 73878821974.932220, Test MSE of 73878824638.717422
Epoch 30: training loss 71943116619.294
Test Loss of 68521697171.038628, Test MSE of 68521697973.787880
Epoch 31: training loss 69477246208.000
Test Loss of 68011239210.340965, Test MSE of 68011241383.797562
Epoch 32: training loss 66974969690.353
Test Loss of 64966946388.681931, Test MSE of 64966944737.800629
Epoch 33: training loss 64216856410.353
Test Loss of 63257965680.040710, Test MSE of 63257964637.566536
Epoch 34: training loss 62006514703.059
Test Loss of 58484428276.985428, Test MSE of 58484428356.685966
Epoch 35: training loss 59390744997.647
Test Loss of 56766060448.777237, Test MSE of 56766062116.464272
Epoch 36: training loss 55934640308.706
Test Loss of 55286575182.641685, Test MSE of 55286575253.992363
Epoch 37: training loss 53565975777.882
Test Loss of 55933987903.718712, Test MSE of 55933988234.925400
Epoch 38: training loss 51178589214.118
Test Loss of 50080566515.741844, Test MSE of 50080567763.815674
Epoch 39: training loss 48964071280.941
Test Loss of 48876444724.348831, Test MSE of 48876444141.092064
Epoch 40: training loss 46827199126.588
Test Loss of 47348086629.559105, Test MSE of 47348084551.303596
Epoch 41: training loss 45230524717.176
Test Loss of 45693109416.416374, Test MSE of 45693110070.962479
Epoch 42: training loss 42291906032.941
Test Loss of 44328179255.309738, Test MSE of 44328179442.797722
Epoch 43: training loss 40884265464.471
Test Loss of 41272498863.404114, Test MSE of 41272499467.192390
Epoch 44: training loss 38756141259.294
Test Loss of 40171727962.248436, Test MSE of 40171727561.312340
Epoch 45: training loss 36830169148.235
Test Loss of 40885922790.180893, Test MSE of 40885923673.910858
Epoch 46: training loss 35229082255.059
Test Loss of 40171895702.117973, Test MSE of 40171897495.137161
Epoch 47: training loss 33426050273.882
Test Loss of 38408599557.684944, Test MSE of 38408601431.401268
Epoch 48: training loss 32348571414.588
Test Loss of 35192429313.362015, Test MSE of 35192429193.897316
Epoch 49: training loss 30165889972.706
Test Loss of 35048907529.415680, Test MSE of 35048906461.677757
Epoch 50: training loss 28822064180.706
Test Loss of 33562270124.739300, Test MSE of 33562269867.884354
Epoch 51: training loss 27461885176.471
Test Loss of 32241315386.152210, Test MSE of 32241315414.899540
Epoch 52: training loss 26506910836.706
Test Loss of 31123056204.154522, Test MSE of 31123055628.898621
Epoch 53: training loss 24922085714.824
Test Loss of 30190679696.847561, Test MSE of 30190679450.362072
Epoch 54: training loss 24035222625.882
Test Loss of 29761654740.889198, Test MSE of 29761654808.358761
Epoch 55: training loss 22934094256.941
Test Loss of 28823117472.007401, Test MSE of 28823117297.272419
Epoch 56: training loss 21881849709.176
Test Loss of 29041476143.492943, Test MSE of 29041476780.117920
Epoch 57: training loss 20921641645.176
Test Loss of 29214517576.305344, Test MSE of 29214517680.577953
Epoch 58: training loss 20048875324.235
Test Loss of 27554885911.035854, Test MSE of 27554885650.177700
Epoch 59: training loss 19095822226.824
Test Loss of 28250458611.801064, Test MSE of 28250459421.415932
Epoch 60: training loss 18566349771.294
Test Loss of 26033773261.250057, Test MSE of 26033772897.960640
Epoch 61: training loss 17885839600.941
Test Loss of 26541391823.678001, Test MSE of 26541391727.324203
Epoch 62: training loss 17214072342.588
Test Loss of 26126415669.710850, Test MSE of 26126415865.795933
Epoch 63: training loss 16755706224.941
Test Loss of 26417402817.465649, Test MSE of 26417402810.870304
Epoch 64: training loss 16108279905.882
Test Loss of 26466835870.290077, Test MSE of 26466835834.839184
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24348354683.306786, 'MSE - std': 2118481151.5323963, 'R2 - mean': 0.8189689635538169, 'R2 - std': 0.007924599331324389} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005429 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927342080.000
Test Loss of 447258378865.343506, Test MSE of 447258383137.820740
Epoch 2: training loss 421906803531.294
Test Loss of 447239538334.112427, Test MSE of 447239541186.762939
Epoch 3: training loss 421878840018.824
Test Loss of 447214340826.041199, Test MSE of 447214346546.292725
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897613432.471
Test Loss of 447224202879.792725, Test MSE of 447224206125.998108
Epoch 2: training loss 421887325003.294
Test Loss of 447223848427.510498, Test MSE of 447223842747.159851
Epoch 3: training loss 421886822038.588
Test Loss of 447222779203.804749, Test MSE of 447222773530.083252
Epoch 4: training loss 416493300314.353
Test Loss of 430183617479.387451, Test MSE of 430183618720.764771
Epoch 5: training loss 380044500630.588
Test Loss of 374818091984.862366, Test MSE of 374818090686.729797
Epoch 6: training loss 312066176903.529
Test Loss of 296200008016.359009, Test MSE of 296200013486.151855
Epoch 7: training loss 223045498759.529
Test Loss of 200089425290.392792, Test MSE of 200089425379.772644
Epoch 8: training loss 159707788860.235
Test Loss of 156046378078.038391, Test MSE of 156046381503.081512
Epoch 9: training loss 138563710313.412
Test Loss of 143305174390.969238, Test MSE of 143305175346.480865
Epoch 10: training loss 131938427904.000
Test Loss of 138309241759.119141, Test MSE of 138309240585.890839
Epoch 11: training loss 129616167785.412
Test Loss of 135387149442.990509, Test MSE of 135387147976.525589
Epoch 12: training loss 127569970176.000
Test Loss of 132125152775.461487, Test MSE of 132125153443.553436
Epoch 13: training loss 123736470738.824
Test Loss of 130256947615.474442, Test MSE of 130256945873.062836
Epoch 14: training loss 119462511555.765
Test Loss of 124917627556.034241, Test MSE of 124917628633.691223
Epoch 15: training loss 117539646765.176
Test Loss of 122507187226.766602, Test MSE of 122507185980.016602
Epoch 16: training loss 113059957037.176
Test Loss of 119751491163.314362, Test MSE of 119751490591.370712
Epoch 17: training loss 110813002270.118
Test Loss of 116620465580.502426, Test MSE of 116620466742.108841
Epoch 18: training loss 107525140841.412
Test Loss of 114017621390.182739, Test MSE of 114017622166.384949
Epoch 19: training loss 104558395753.412
Test Loss of 111377176442.877625, Test MSE of 111377176536.580887
Epoch 20: training loss 100900390490.353
Test Loss of 108007754215.483688, Test MSE of 108007754355.244705
Epoch 21: training loss 97901472436.706
Test Loss of 104682108967.320847, Test MSE of 104682108420.056900
Epoch 22: training loss 95011503676.235
Test Loss of 100367937716.970627, Test MSE of 100367937186.474075
Epoch 23: training loss 91633045007.059
Test Loss of 98233111341.894058, Test MSE of 98233113206.172012
Epoch 24: training loss 88881437033.412
Test Loss of 95601651548.084198, Test MSE of 95601650147.761627
Epoch 25: training loss 85959574392.471
Test Loss of 90245374103.598434, Test MSE of 90245372780.259979
Epoch 26: training loss 82316745878.588
Test Loss of 88539370839.228317, Test MSE of 88539372252.514542
Epoch 27: training loss 78937866496.000
Test Loss of 84174177743.559570, Test MSE of 84174176850.529327
Epoch 28: training loss 76070016195.765
Test Loss of 82466954866.054123, Test MSE of 82466954682.247253
Epoch 29: training loss 73602463653.647
Test Loss of 77906538493.157532, Test MSE of 77906538276.451996
Epoch 30: training loss 70998261504.000
Test Loss of 77710697308.321075, Test MSE of 77710698212.320496
Epoch 31: training loss 67831568459.294
Test Loss of 74739037085.697891, Test MSE of 74739038411.593201
Epoch 32: training loss 65441509707.294
Test Loss of 71712525774.138336, Test MSE of 71712526720.117676
Epoch 33: training loss 63249568557.176
Test Loss of 67761060955.669670, Test MSE of 67761061145.106972
Epoch 34: training loss 59886779015.529
Test Loss of 66166317010.520470, Test MSE of 66166317467.166061
Epoch 35: training loss 57103058160.941
Test Loss of 64829985712.647697, Test MSE of 64829987207.609001
Epoch 36: training loss 55303518494.118
Test Loss of 57420861158.595421, Test MSE of 57420861254.181236
Epoch 37: training loss 52539581214.118
Test Loss of 58699510621.268562, Test MSE of 58699511528.431381
Epoch 38: training loss 49988054151.529
Test Loss of 55232171741.120522, Test MSE of 55232172149.324921
Epoch 39: training loss 48633705825.882
Test Loss of 53549324152.982651, Test MSE of 53549324331.694847
Epoch 40: training loss 45910182482.824
Test Loss of 52946636215.398567, Test MSE of 52946635409.515556
Epoch 41: training loss 44364445033.412
Test Loss of 50866047381.052048, Test MSE of 50866047734.820557
Epoch 42: training loss 41516569336.471
Test Loss of 46915242227.031227, Test MSE of 46915241954.636734
Epoch 43: training loss 39919441754.353
Test Loss of 43693745189.188988, Test MSE of 43693744930.495689
Epoch 44: training loss 37978901406.118
Test Loss of 40484330042.862823, Test MSE of 40484330322.525780
Epoch 45: training loss 36707055284.706
Test Loss of 46680918025.001160, Test MSE of 46680917951.104309
Epoch 46: training loss 34973563723.294
Test Loss of 42696317124.604210, Test MSE of 42696317808.249710
Epoch 47: training loss 33437960568.471
Test Loss of 39016627392.103630, Test MSE of 39016627517.619087
Epoch 48: training loss 31768325504.000
Test Loss of 34701983184.033310, Test MSE of 34701982524.407082
Epoch 49: training loss 30542045086.118
Test Loss of 37653081689.419388, Test MSE of 37653081033.966156
Epoch 50: training loss 29087268751.059
Test Loss of 37250220392.046265, Test MSE of 37250219952.312271
Epoch 51: training loss 27363090209.882
Test Loss of 34288887983.759426, Test MSE of 34288887662.628094
Epoch 52: training loss 26703035595.294
Test Loss of 30899657949.475826, Test MSE of 30899657144.444366
Epoch 53: training loss 24889947474.824
Test Loss of 29679616441.530418, Test MSE of 29679616483.779739
Epoch 54: training loss 24432365010.824
Test Loss of 31123215285.385151, Test MSE of 31123215202.792530
Epoch 55: training loss 23170293240.471
Test Loss of 30973846937.552624, Test MSE of 30973847239.736294
Epoch 56: training loss 22346456722.824
Test Loss of 26759394323.660419, Test MSE of 26759394416.251961
Epoch 57: training loss 21539618823.529
Test Loss of 29387069015.524403, Test MSE of 29387068906.551003
Epoch 58: training loss 20678698642.824
Test Loss of 24897435354.278049, Test MSE of 24897435437.660896
Epoch 59: training loss 20192337212.235
Test Loss of 27953512622.338192, Test MSE of 27953512961.831184
Epoch 60: training loss 19386367706.353
Test Loss of 26949881914.270645, Test MSE of 26949882223.400360
Epoch 61: training loss 18747656647.529
Test Loss of 24387624847.248669, Test MSE of 24387624687.573601
Epoch 62: training loss 18065643083.294
Test Loss of 23404095801.619247, Test MSE of 23404095733.560444
Epoch 63: training loss 17629606644.706
Test Loss of 25190673455.611382, Test MSE of 25190673673.874348
Epoch 64: training loss 17284217720.471
Test Loss of 23449972052.859589, Test MSE of 23449971580.832222
Epoch 65: training loss 16291406983.529
Test Loss of 22703562410.903538, Test MSE of 22703562338.304607
Epoch 66: training loss 16073611207.529
Test Loss of 21489218420.008327, Test MSE of 21489218528.004040
Epoch 67: training loss 15716427335.529
Test Loss of 23224798455.058060, Test MSE of 23224798814.884048
Epoch 68: training loss 15059578947.765
Test Loss of 24634841998.538052, Test MSE of 24634841473.548504
Epoch 69: training loss 14612472368.941
Test Loss of 22301663935.511452, Test MSE of 22301664449.511887
Epoch 70: training loss 14290362928.941
Test Loss of 21668000028.720795, Test MSE of 21668000047.480228
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23454903138.031265, 'MSE - std': 2142075221.3159509, 'R2 - mean': 0.8312318269177531, 'R2 - std': 0.0185100464290744} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005471 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111076352.000
Test Loss of 410764423283.620544, Test MSE of 410764421794.606323
Epoch 2: training loss 430090052547.765
Test Loss of 410746121438.237854, Test MSE of 410746119940.927551
Epoch 3: training loss 430061999766.588
Test Loss of 410722846408.203613, Test MSE of 410722847790.320862
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077375909.647
Test Loss of 410727686731.579834, Test MSE of 410727680515.318054
Epoch 2: training loss 430066399352.471
Test Loss of 410728001568.222107, Test MSE of 410727995086.295105
Epoch 3: training loss 430065912892.235
Test Loss of 410727873907.975952, Test MSE of 410727868343.130859
Epoch 4: training loss 424859056007.529
Test Loss of 394457839662.437744, Test MSE of 394457842857.372375
Epoch 5: training loss 388845224658.824
Test Loss of 340091739307.061523, Test MSE of 340091733212.173889
Epoch 6: training loss 321096535100.235
Test Loss of 263404792743.389160, Test MSE of 263404787101.135712
Epoch 7: training loss 232077403075.765
Test Loss of 169209253590.419250, Test MSE of 169209249575.302429
Epoch 8: training loss 166333520745.412
Test Loss of 127123284570.743179, Test MSE of 127123286748.974350
Epoch 9: training loss 145723712421.647
Test Loss of 114985228414.045349, Test MSE of 114985229861.698151
Epoch 10: training loss 139621786232.471
Test Loss of 110833875676.579361, Test MSE of 110833875716.090179
Epoch 11: training loss 135890352700.235
Test Loss of 108341333733.582596, Test MSE of 108341333115.025299
Epoch 12: training loss 133034221869.176
Test Loss of 105641904358.293381, Test MSE of 105641905485.215271
Epoch 13: training loss 128955717421.176
Test Loss of 103018017319.566864, Test MSE of 103018018562.961685
Epoch 14: training loss 127000796069.647
Test Loss of 100528850495.733459, Test MSE of 100528851436.079971
Epoch 15: training loss 122878298864.941
Test Loss of 97995228697.825089, Test MSE of 97995228722.896606
Epoch 16: training loss 120367605760.000
Test Loss of 95872921613.267929, Test MSE of 95872921064.253876
Epoch 17: training loss 116371077903.059
Test Loss of 92310179557.582596, Test MSE of 92310178605.385880
Epoch 18: training loss 113743308257.882
Test Loss of 89065390466.665436, Test MSE of 89065391160.067062
Epoch 19: training loss 111094525349.647
Test Loss of 87412263070.267471, Test MSE of 87412261830.141998
Epoch 20: training loss 107073786307.765
Test Loss of 84549924478.282272, Test MSE of 84549925372.128738
Epoch 21: training loss 102278926095.059
Test Loss of 81465427155.813049, Test MSE of 81465429188.253738
Epoch 22: training loss 101501330597.647
Test Loss of 79606164493.741791, Test MSE of 79606164625.648499
Epoch 23: training loss 97175306752.000
Test Loss of 77229555063.766769, Test MSE of 77229553990.340286
Epoch 24: training loss 93846232033.882
Test Loss of 72990336158.741318, Test MSE of 72990335232.517456
Epoch 25: training loss 91270015623.529
Test Loss of 71370641735.907455, Test MSE of 71370642138.509521
Epoch 26: training loss 88077225065.412
Test Loss of 69402026260.731140, Test MSE of 69402025481.511856
Epoch 27: training loss 84118799043.765
Test Loss of 68393248619.209625, Test MSE of 68393249786.512581
Epoch 28: training loss 81123618575.059
Test Loss of 63571875260.949562, Test MSE of 63571875419.470177
Epoch 29: training loss 77686596005.647
Test Loss of 59437750382.881996, Test MSE of 59437750829.087051
Epoch 30: training loss 74937261552.941
Test Loss of 60135452355.465065, Test MSE of 60135451547.827370
Epoch 31: training loss 72740082221.176
Test Loss of 57585292281.839890, Test MSE of 57585291378.961433
Epoch 32: training loss 69715772777.412
Test Loss of 55530031891.546509, Test MSE of 55530032484.531174
Epoch 33: training loss 66910591691.294
Test Loss of 53149780493.978714, Test MSE of 53149780167.784386
Epoch 34: training loss 64375063160.471
Test Loss of 49219096229.138359, Test MSE of 49219096591.729713
Epoch 35: training loss 60879959326.118
Test Loss of 47804154936.388710, Test MSE of 47804154670.139114
Epoch 36: training loss 58823667998.118
Test Loss of 46963573596.520126, Test MSE of 46963573316.321121
Epoch 37: training loss 55338252272.941
Test Loss of 44654463556.945862, Test MSE of 44654464059.005745
Epoch 38: training loss 54074256414.118
Test Loss of 43934678509.282738, Test MSE of 43934677873.819168
Epoch 39: training loss 51301188826.353
Test Loss of 39136317201.177231, Test MSE of 39136317205.025154
Epoch 40: training loss 48894392176.941
Test Loss of 39794235578.698753, Test MSE of 39794236143.467659
Epoch 41: training loss 47333268186.353
Test Loss of 38925124827.394722, Test MSE of 38925125135.511398
Epoch 42: training loss 45302363376.941
Test Loss of 36214994776.255440, Test MSE of 36214994808.406670
Epoch 43: training loss 43118221891.765
Test Loss of 36983413589.886162, Test MSE of 36983413469.061371
Epoch 44: training loss 41181391864.471
Test Loss of 34119286603.935215, Test MSE of 34119286797.204201
Epoch 45: training loss 38957001916.235
Test Loss of 34857852683.490974, Test MSE of 34857852730.690926
Epoch 46: training loss 37325970168.471
Test Loss of 34449314491.409531, Test MSE of 34449315026.319122
Epoch 47: training loss 35551063920.941
Test Loss of 30004591855.296623, Test MSE of 30004592153.935432
Epoch 48: training loss 33754708600.471
Test Loss of 30219840849.858398, Test MSE of 30219840237.977673
Epoch 49: training loss 32732643945.412
Test Loss of 28912009151.555759, Test MSE of 28912008689.825340
Epoch 50: training loss 31233959092.706
Test Loss of 27638212318.000927, Test MSE of 27638212552.582748
Epoch 51: training loss 29899408143.059
Test Loss of 25937007968.074039, Test MSE of 25937008323.902519
Epoch 52: training loss 28632900389.647
Test Loss of 23769147494.352615, Test MSE of 23769147811.044273
Epoch 53: training loss 26951949037.176
Test Loss of 24823984417.051365, Test MSE of 24823983993.921295
Epoch 54: training loss 26253987550.118
Test Loss of 25406884478.756130, Test MSE of 25406884290.659229
Epoch 55: training loss 24813568399.059
Test Loss of 23302100701.053215, Test MSE of 23302100718.539658
Epoch 56: training loss 24186919924.706
Test Loss of 24066977393.014347, Test MSE of 24066977961.183617
Epoch 57: training loss 23244543661.176
Test Loss of 22300975784.455345, Test MSE of 22300976057.036373
Epoch 58: training loss 22704734294.588
Test Loss of 24689797736.958817, Test MSE of 24689798247.536846
Epoch 59: training loss 21473876709.647
Test Loss of 23039872687.563164, Test MSE of 23039872716.282661
Epoch 60: training loss 21062897848.471
Test Loss of 22440188208.214714, Test MSE of 22440188047.393337
Epoch 61: training loss 20324418480.941
Test Loss of 21989047785.491901, Test MSE of 21989047586.470222
Epoch 62: training loss 19543793114.353
Test Loss of 21529359066.683941, Test MSE of 21529359174.777279
Epoch 63: training loss 19006139184.941
Test Loss of 23235674039.026375, Test MSE of 23235673531.055740
Epoch 64: training loss 18170263687.529
Test Loss of 22123292019.975937, Test MSE of 22123291781.313824
Epoch 65: training loss 17951846211.765
Test Loss of 20597200424.988430, Test MSE of 20597200330.922314
Epoch 66: training loss 17413138119.529
Test Loss of 21193574832.155483, Test MSE of 21193574462.248257
Epoch 67: training loss 16528798825.412
Test Loss of 18601614490.950485, Test MSE of 18601614321.186817
Epoch 68: training loss 16062845084.235
Test Loss of 20813414086.308189, Test MSE of 20813414238.067619
Epoch 69: training loss 15810815405.176
Test Loss of 20525214493.971310, Test MSE of 20525214533.919968
Epoch 70: training loss 15609726055.529
Test Loss of 19058799386.180473, Test MSE of 19058799026.052681
Epoch 71: training loss 14966481528.471
Test Loss of 20551169918.163815, Test MSE of 20551170276.348625
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22728969922.610603, 'MSE - std': 2241049260.9679003, 'R2 - mean': 0.8310189904992331, 'R2 - std': 0.016034408692808498} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005558 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043234484.706
Test Loss of 431612063806.548828, Test MSE of 431612065382.033691
Epoch 2: training loss 424023375631.059
Test Loss of 431591989395.842651, Test MSE of 431591982373.954529
Epoch 3: training loss 423995614750.118
Test Loss of 431564537135.740845, Test MSE of 431564539236.743164
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008172242.824
Test Loss of 431565096678.530334, Test MSE of 431565099114.748291
Epoch 2: training loss 423998540498.824
Test Loss of 431567019497.491882, Test MSE of 431567016306.813599
Epoch 3: training loss 423998016451.765
Test Loss of 431567491032.196228, Test MSE of 431567485245.536621
Epoch 4: training loss 418471705419.294
Test Loss of 413958610834.065735, Test MSE of 413958607135.916504
Epoch 5: training loss 381759726772.706
Test Loss of 357965229217.584473, Test MSE of 357965234893.706848
Epoch 6: training loss 313843641524.706
Test Loss of 279375587110.974548, Test MSE of 279375585082.005676
Epoch 7: training loss 225790078012.235
Test Loss of 183530135752.914398, Test MSE of 183530133781.115906
Epoch 8: training loss 162558368677.647
Test Loss of 140025381876.153625, Test MSE of 140025379952.545990
Epoch 9: training loss 141401289818.353
Test Loss of 127601485553.902817, Test MSE of 127601486516.151459
Epoch 10: training loss 136048643072.000
Test Loss of 121832816185.099487, Test MSE of 121832814456.233322
Epoch 11: training loss 133140202014.118
Test Loss of 119095364932.590469, Test MSE of 119095369424.888397
Epoch 12: training loss 131178205424.941
Test Loss of 115343762029.697357, Test MSE of 115343759966.730957
Epoch 13: training loss 128749262607.059
Test Loss of 113045751932.149933, Test MSE of 113045747789.792389
Epoch 14: training loss 122730827625.412
Test Loss of 110665225630.622864, Test MSE of 110665227815.380615
Epoch 15: training loss 119807552301.176
Test Loss of 106261417489.295700, Test MSE of 106261418406.574371
Epoch 16: training loss 117047510241.882
Test Loss of 104441530875.024521, Test MSE of 104441530512.115631
Epoch 17: training loss 113785564461.176
Test Loss of 101005855863.885239, Test MSE of 101005858084.761902
Epoch 18: training loss 109777139998.118
Test Loss of 99205189531.542801, Test MSE of 99205189936.326263
Epoch 19: training loss 106610615265.882
Test Loss of 94267698132.879227, Test MSE of 94267696682.471252
Epoch 20: training loss 103738041569.882
Test Loss of 93798133674.706161, Test MSE of 93798134247.787766
Epoch 21: training loss 100231218206.118
Test Loss of 89407716523.061539, Test MSE of 89407715202.551041
Epoch 22: training loss 97182256504.471
Test Loss of 84087116740.768158, Test MSE of 84087117566.707275
Epoch 23: training loss 94131258804.706
Test Loss of 84096219588.057388, Test MSE of 84096218316.396439
Epoch 24: training loss 91191365391.059
Test Loss of 80526595179.565018, Test MSE of 80526594348.308487
Epoch 25: training loss 87941100709.647
Test Loss of 76909265256.129562, Test MSE of 76909264466.766312
Epoch 26: training loss 84075463898.353
Test Loss of 72899885358.319290, Test MSE of 72899885668.394928
Epoch 27: training loss 80948682646.588
Test Loss of 72016645209.558533, Test MSE of 72016645735.118225
Epoch 28: training loss 79257013172.706
Test Loss of 69243288745.166122, Test MSE of 69243290147.429092
Epoch 29: training loss 75792615183.059
Test Loss of 59863764430.956039, Test MSE of 59863764088.462547
Epoch 30: training loss 73721076856.471
Test Loss of 61551996554.602501, Test MSE of 61551995936.457962
Epoch 31: training loss 70096189184.000
Test Loss of 61231226680.981026, Test MSE of 61231226043.174957
Epoch 32: training loss 67575913155.765
Test Loss of 56140024415.481720, Test MSE of 56140025357.460030
Epoch 33: training loss 64638092604.235
Test Loss of 50321304921.440071, Test MSE of 50321305327.177635
Epoch 34: training loss 62323197786.353
Test Loss of 52133961845.515968, Test MSE of 52133962416.039177
Epoch 35: training loss 59806140958.118
Test Loss of 52630605816.418327, Test MSE of 52630606849.635971
Epoch 36: training loss 57098072591.059
Test Loss of 46967730344.692268, Test MSE of 46967730486.212700
Epoch 37: training loss 54812642304.000
Test Loss of 41448957590.922722, Test MSE of 41448957478.292885
Epoch 38: training loss 52232115207.529
Test Loss of 40666812325.019897, Test MSE of 40666812170.249512
Epoch 39: training loss 49613739023.059
Test Loss of 40780453214.178619, Test MSE of 40780452889.518890
Epoch 40: training loss 47850565699.765
Test Loss of 40703891686.293381, Test MSE of 40703892105.020187
Epoch 41: training loss 45751430023.529
Test Loss of 39881498116.027763, Test MSE of 39881497564.928307
Epoch 42: training loss 43680483900.235
Test Loss of 36839174981.775101, Test MSE of 36839174769.710754
Epoch 43: training loss 41426121441.882
Test Loss of 34169775758.393337, Test MSE of 34169776218.311707
Epoch 44: training loss 39528061906.824
Test Loss of 35416239080.781120, Test MSE of 35416239425.479347
Epoch 45: training loss 38588511179.294
Test Loss of 34530881018.550674, Test MSE of 34530881318.347694
Epoch 46: training loss 36545343420.235
Test Loss of 32021490502.722813, Test MSE of 32021490352.369122
Epoch 47: training loss 34553435858.824
Test Loss of 31078142123.535400, Test MSE of 31078142236.625214
Epoch 48: training loss 32964050522.353
Test Loss of 27764405156.546043, Test MSE of 27764405920.271748
Epoch 49: training loss 32319895619.765
Test Loss of 27382891428.546043, Test MSE of 27382891664.810482
Epoch 50: training loss 30262954842.353
Test Loss of 26973864886.552521, Test MSE of 26973865342.328297
Epoch 51: training loss 29898695145.412
Test Loss of 25982537166.956039, Test MSE of 25982537102.968765
Epoch 52: training loss 28184860777.412
Test Loss of 24413176552.899582, Test MSE of 24413176200.047428
Epoch 53: training loss 26446128429.176
Test Loss of 26061250698.365570, Test MSE of 26061250797.219677
Epoch 54: training loss 25661902106.353
Test Loss of 26482547110.678391, Test MSE of 26482547846.264687
Epoch 55: training loss 24969449449.412
Test Loss of 23703551478.759834, Test MSE of 23703551290.994362
Epoch 56: training loss 23725004242.824
Test Loss of 29084902603.283665, Test MSE of 29084903039.375462
Epoch 57: training loss 22853950339.765
Test Loss of 26448973288.544193, Test MSE of 26448973455.071941
Epoch 58: training loss 22013869884.235
Test Loss of 23265131981.060619, Test MSE of 23265132112.267414
Epoch 59: training loss 21183504333.176
Test Loss of 23231684371.072651, Test MSE of 23231684208.791058
Epoch 60: training loss 20539111627.294
Test Loss of 22789120178.169365, Test MSE of 22789120348.793465
Epoch 61: training loss 19919985003.294
Test Loss of 22723338199.248497, Test MSE of 22723337996.516304
Epoch 62: training loss 19240781854.118
Test Loss of 22024433103.903748, Test MSE of 22024432825.758331
Epoch 63: training loss 18889714420.706
Test Loss of 22961449325.815826, Test MSE of 22961449306.606297
Epoch 64: training loss 18159958810.353
Test Loss of 22490419420.816288, Test MSE of 22490419873.601028
Epoch 65: training loss 17824634386.824
Test Loss of 21772027469.949097, Test MSE of 21772027786.324829
Epoch 66: training loss 17251441878.588
Test Loss of 21409739996.342434, Test MSE of 21409740238.876980
Epoch 67: training loss 16565428487.529
Test Loss of 22671394692.797779, Test MSE of 22671394436.753643
Epoch 68: training loss 16255935375.059
Test Loss of 22330861513.980564, Test MSE of 22330861868.561943
Epoch 69: training loss 15728774177.882
Test Loss of 20929970262.241554, Test MSE of 20929970535.348316
Epoch 70: training loss 15303918008.471
Test Loss of 21657554461.615917, Test MSE of 21657554160.602657
Epoch 71: training loss 15163859248.941
Test Loss of 23571584344.492363, Test MSE of 23571584478.716225
Epoch 72: training loss 14581376131.765
Test Loss of 20167238218.632114, Test MSE of 20167238616.751682
Epoch 73: training loss 14503986661.647
Test Loss of 21038853383.937065, Test MSE of 21038853553.275032
Epoch 74: training loss 14117527055.059
Test Loss of 19493873879.130032, Test MSE of 19493873835.693878
Epoch 75: training loss 13686236107.294
Test Loss of 18043902803.043037, Test MSE of 18043902951.361435
Epoch 76: training loss 13458316574.118
Test Loss of 19665609641.284592, Test MSE of 19665609321.140190
Epoch 77: training loss 13098994876.235
Test Loss of 19065960616.692272, Test MSE of 19065960576.737244
Epoch 78: training loss 13031056481.882
Test Loss of 20244344218.832024, Test MSE of 20244344617.659451
Epoch 79: training loss 12553071360.000
Test Loss of 20220048787.250347, Test MSE of 20220048785.007370
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22227185695.08996, 'MSE - std': 2241649186.6529403, 'R2 - mean': 0.8346143773911875, 'R2 - std': 0.01604334869389049} 
 

Saving model.....
Results After CV: {'MSE - mean': 22227185695.08996, 'MSE - std': 2241649186.6529403, 'R2 - mean': 0.8346143773911875, 'R2 - std': 0.01604334869389049}
Train time: 104.50465853120004
Inference time: 0.07164167119990453
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 29 finished with value: 22227185695.08996 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525491410.824
Test Loss of 418111985106.402039, Test MSE of 418111991061.066467
Epoch 2: training loss 427504785889.882
Test Loss of 418093537995.828796, Test MSE of 418093544675.596191
Epoch 3: training loss 427476886347.294
Test Loss of 418069435862.192017, Test MSE of 418069434178.681030
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493024948.706
Test Loss of 418074680432.514465, Test MSE of 418074682315.049133
Epoch 2: training loss 427481919728.941
Test Loss of 418077455388.661560, Test MSE of 418077455344.099976
Epoch 3: training loss 427481480011.294
Test Loss of 418077097032.719849, Test MSE of 418077097089.765442
Epoch 4: training loss 427481096432.941
Test Loss of 418077458435.789978, Test MSE of 418077452183.718262
Epoch 5: training loss 427480884766.118
Test Loss of 418076050510.878540, Test MSE of 418076045597.678406
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418076045597.6784, 'MSE - std': 0.0, 'R2 - mean': -2.2556035279750253, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005568 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918488756.706
Test Loss of 424557371700.881775, Test MSE of 424557362972.324341
Epoch 2: training loss 427898154285.176
Test Loss of 424541429883.173706, Test MSE of 424541436618.098206
Epoch 3: training loss 427870408704.000
Test Loss of 424519346896.566284, Test MSE of 424519354815.328674
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886577784.471
Test Loss of 424529360222.808228, Test MSE of 424529352540.976624
Epoch 2: training loss 427878635399.529
Test Loss of 424528852747.073792, Test MSE of 424528854950.063232
Epoch 3: training loss 427878144361.412
Test Loss of 424528367973.203796, Test MSE of 424528369881.508545
Epoch 4: training loss 427877762469.647
Test Loss of 424527611497.052979, Test MSE of 424527615867.101807
Epoch 5: training loss 427877451896.471
Test Loss of 424527176841.622925, Test MSE of 424527182995.140747
Epoch 6: training loss 427877230230.588
Test Loss of 424526277499.114502, Test MSE of 424526287664.299194
Epoch 7: training loss 427877051452.235
Test Loss of 424526155288.042542, Test MSE of 424526154246.963562
Epoch 8: training loss 417329056105.412
Test Loss of 391037744926.023621, Test MSE of 391037744447.770447
Epoch 9: training loss 350402141244.235
Test Loss of 300196203100.972473, Test MSE of 300196200070.091736
Epoch 10: training loss 251600175104.000
Test Loss of 206530657564.957672, Test MSE of 206530660233.304413
Epoch 11: training loss 176365572999.529
Test Loss of 152478649988.056458, Test MSE of 152478649982.282684
Epoch 12: training loss 145159632022.588
Test Loss of 134147239756.924362, Test MSE of 134147241127.942657
Epoch 13: training loss 137066751307.294
Test Loss of 129914559107.819565, Test MSE of 129914560519.538864
Epoch 14: training loss 134261045940.706
Test Loss of 127479532274.675919, Test MSE of 127479531085.710297
Epoch 15: training loss 129795636675.765
Test Loss of 123293979470.108719, Test MSE of 123293979445.750580
Epoch 16: training loss 125736808327.529
Test Loss of 119346964044.628265, Test MSE of 119346964446.562088
Epoch 17: training loss 121627057814.588
Test Loss of 116698502231.405975, Test MSE of 116698501307.953354
Epoch 18: training loss 119092362450.824
Test Loss of 113519766309.840393, Test MSE of 113519765718.502731
Epoch 19: training loss 114864168779.294
Test Loss of 109854772242.712936, Test MSE of 109854773963.608749
Epoch 20: training loss 111378728869.647
Test Loss of 106452743572.815170, Test MSE of 106452742502.922791
Epoch 21: training loss 107384291448.471
Test Loss of 102778920517.522095, Test MSE of 102778921948.815094
Epoch 22: training loss 103900712538.353
Test Loss of 98530774553.937546, Test MSE of 98530775448.921646
Epoch 23: training loss 99323075794.824
Test Loss of 94768848806.935928, Test MSE of 94768851623.778809
Epoch 24: training loss 95553232414.118
Test Loss of 92893686095.885269, Test MSE of 92893686527.360641
Epoch 25: training loss 90952205703.529
Test Loss of 87590393676.450607, Test MSE of 87590395093.922852
Epoch 26: training loss 87550993709.176
Test Loss of 83031898475.362473, Test MSE of 83031900883.637726
Epoch 27: training loss 84061045519.059
Test Loss of 80018249280.310898, Test MSE of 80018247816.770905
Epoch 28: training loss 80376441208.471
Test Loss of 77288339122.009720, Test MSE of 77288337538.543762
Epoch 29: training loss 76790669357.176
Test Loss of 75002073964.428406, Test MSE of 75002073865.193863
Epoch 30: training loss 73430681012.706
Test Loss of 73132482769.395325, Test MSE of 73132483589.870193
Epoch 31: training loss 69183425114.353
Test Loss of 69752012312.042557, Test MSE of 69752011930.427429
Epoch 32: training loss 66169282183.529
Test Loss of 64699107752.001854, Test MSE of 64699106246.735039
Epoch 33: training loss 62792290876.235
Test Loss of 62733468767.933380, Test MSE of 62733468120.003166
Epoch 34: training loss 60074793396.706
Test Loss of 59717942433.783943, Test MSE of 59717942989.605240
Epoch 35: training loss 57479712240.941
Test Loss of 54867373745.535973, Test MSE of 54867374569.336098
Epoch 36: training loss 54077920391.529
Test Loss of 57832974951.868607, Test MSE of 57832974813.517326
Epoch 37: training loss 50991498541.176
Test Loss of 49504797414.358551, Test MSE of 49504799119.814850
Epoch 38: training loss 47532360056.471
Test Loss of 46566506954.111496, Test MSE of 46566507647.381653
Epoch 39: training loss 45596312109.176
Test Loss of 47136064269.205643, Test MSE of 47136063037.052269
Epoch 40: training loss 43346035779.765
Test Loss of 46078009547.236641, Test MSE of 46078007506.268654
Epoch 41: training loss 41262984975.059
Test Loss of 42831767662.382607, Test MSE of 42831767745.168839
Epoch 42: training loss 38560364875.294
Test Loss of 44167003207.298637, Test MSE of 44167002300.731766
Epoch 43: training loss 37427027011.765
Test Loss of 37801508409.204720, Test MSE of 37801508639.385254
Epoch 44: training loss 34390311612.235
Test Loss of 38604009660.076797, Test MSE of 38604009440.020195
Epoch 45: training loss 33212109191.529
Test Loss of 34194645097.645153, Test MSE of 34194644458.685215
Epoch 46: training loss 31159147648.000
Test Loss of 34908892325.573906, Test MSE of 34908892151.110413
Epoch 47: training loss 29258335070.118
Test Loss of 33435463680.000000, Test MSE of 33435463552.850899
Epoch 48: training loss 27736494750.118
Test Loss of 29595118481.854267, Test MSE of 29595118423.140236
Epoch 49: training loss 26353390381.176
Test Loss of 30873378373.285217, Test MSE of 30873378461.585083
Epoch 50: training loss 25374458048.000
Test Loss of 29851163297.665508, Test MSE of 29851163683.244766
Epoch 51: training loss 24372144685.176
Test Loss of 30621667107.708534, Test MSE of 30621666563.970894
Epoch 52: training loss 22434390268.235
Test Loss of 30048662549.081657, Test MSE of 30048662681.144981
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 224062354139.41168, 'MSE - std': 194013691458.26672, 'R2 - mean': -0.7350655146793436, 'R2 - std': 1.5205380132956816} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005551 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927152097.882
Test Loss of 447259765414.876709, Test MSE of 447259756784.827454
Epoch 2: training loss 421906895691.294
Test Loss of 447241586246.943298, Test MSE of 447241586353.517517
Epoch 3: training loss 421880059181.176
Test Loss of 447217062548.400635, Test MSE of 447217062398.717590
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898083388.235
Test Loss of 447225785628.010193, Test MSE of 447225788579.430237
Epoch 2: training loss 421889589970.824
Test Loss of 447226960879.655823, Test MSE of 447226959053.682251
Epoch 3: training loss 421889075561.412
Test Loss of 447226897541.833008, Test MSE of 447226902287.279907
Epoch 4: training loss 421888645240.471
Test Loss of 447226743078.195679, Test MSE of 447226753350.409546
Epoch 5: training loss 421888375988.706
Test Loss of 447226655393.665527, Test MSE of 447226651589.273987
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 298450453289.3658, 'MSE - std': 190161475871.94366, 'R2 - mean': -1.1490964448787804, 'R2 - std': 1.3726618139859048} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005450 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110617118.118
Test Loss of 410764589019.039307, Test MSE of 410764590540.944153
Epoch 2: training loss 430090597556.706
Test Loss of 410746957757.660339, Test MSE of 410746961234.818176
Epoch 3: training loss 430064414238.118
Test Loss of 410724398316.453491, Test MSE of 410724402666.916138
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078425690.353
Test Loss of 410730251066.402588, Test MSE of 410730253129.399475
Epoch 2: training loss 430067678027.294
Test Loss of 410729976882.702454, Test MSE of 410729981387.227661
Epoch 3: training loss 430067192289.882
Test Loss of 410729677497.514099, Test MSE of 410729677086.714172
Epoch 4: training loss 430066838829.176
Test Loss of 410729687641.795471, Test MSE of 410729689416.900146
Epoch 5: training loss 430066614753.882
Test Loss of 410729409030.397034, Test MSE of 410729406991.699890
Epoch 6: training loss 430066485850.353
Test Loss of 410729513596.860718, Test MSE of 410729513323.169739
Epoch 7: training loss 430066380800.000
Test Loss of 410729299050.617310, Test MSE of 410729299737.932434
Epoch 8: training loss 419871421620.706
Test Loss of 376742625848.625610, Test MSE of 376742623889.276123
Epoch 9: training loss 353346143774.118
Test Loss of 285364735505.295715, Test MSE of 285364737208.489502
Epoch 10: training loss 255536244976.941
Test Loss of 190993642626.310028, Test MSE of 190993641981.425751
Epoch 11: training loss 181204556559.059
Test Loss of 136872157589.619614, Test MSE of 136872156926.738602
Epoch 12: training loss 149251632158.118
Test Loss of 117658950955.950027, Test MSE of 117658951280.911697
Epoch 13: training loss 140991483904.000
Test Loss of 112212633443.627945, Test MSE of 112212635733.416336
Epoch 14: training loss 136488888711.529
Test Loss of 110355458790.056458, Test MSE of 110355459085.675034
Epoch 15: training loss 135086631002.353
Test Loss of 106269705568.074036, Test MSE of 106269704146.364624
Epoch 16: training loss 130061632301.176
Test Loss of 102942254276.649704, Test MSE of 102942254116.788208
Epoch 17: training loss 125144999002.353
Test Loss of 100257145593.958359, Test MSE of 100257145505.538498
Epoch 18: training loss 121768154262.588
Test Loss of 96368571344.140671, Test MSE of 96368571339.175995
Epoch 19: training loss 118660774550.588
Test Loss of 94103083721.625168, Test MSE of 94103082731.032806
Epoch 20: training loss 116050952523.294
Test Loss of 90956004735.348450, Test MSE of 90956005202.864960
Epoch 21: training loss 110220514755.765
Test Loss of 89211968072.736694, Test MSE of 89211966329.217865
Epoch 22: training loss 107737235636.706
Test Loss of 85181269848.255432, Test MSE of 85181270452.642319
Epoch 23: training loss 103322256293.647
Test Loss of 81221391562.335953, Test MSE of 81221392615.746262
Epoch 24: training loss 98807784899.765
Test Loss of 77544005662.326706, Test MSE of 77544004688.966644
Epoch 25: training loss 94862558027.294
Test Loss of 74559824088.077744, Test MSE of 74559825217.357193
Epoch 26: training loss 91435520451.765
Test Loss of 71933164006.174911, Test MSE of 71933163901.503647
Epoch 27: training loss 87401293929.412
Test Loss of 67906176825.928734, Test MSE of 67906178814.121979
Epoch 28: training loss 83717504286.118
Test Loss of 66088938983.596481, Test MSE of 66088938731.680313
Epoch 29: training loss 80228112640.000
Test Loss of 63086134218.454422, Test MSE of 63086134048.056274
Epoch 30: training loss 76684296011.294
Test Loss of 59488926758.382233, Test MSE of 59488926993.910103
Epoch 31: training loss 72715778544.941
Test Loss of 56614706434.250809, Test MSE of 56614707090.313019
Epoch 32: training loss 68799041189.647
Test Loss of 53684933352.899582, Test MSE of 53684933693.098091
Epoch 33: training loss 66386814509.176
Test Loss of 52134992818.761681, Test MSE of 52134992905.951744
Epoch 34: training loss 63443409648.941
Test Loss of 51416810626.310043, Test MSE of 51416810786.007607
Epoch 35: training loss 60189720184.471
Test Loss of 46651074490.817215, Test MSE of 46651075489.078682
Epoch 36: training loss 57291167924.706
Test Loss of 45559050473.136513, Test MSE of 45559049615.366211
Epoch 37: training loss 54240627154.824
Test Loss of 43484972461.786209, Test MSE of 43484972225.815277
Epoch 38: training loss 52028484698.353
Test Loss of 39063097683.279961, Test MSE of 39063097722.127129
Epoch 39: training loss 49351651282.824
Test Loss of 36385674328.610825, Test MSE of 36385673907.336105
Epoch 40: training loss 47563674721.882
Test Loss of 36522180609.895416, Test MSE of 36522180254.873177
Epoch 41: training loss 44229214908.235
Test Loss of 34733175005.764000, Test MSE of 34733175231.355392
Epoch 42: training loss 41776933232.941
Test Loss of 34765166589.156876, Test MSE of 34765167074.674759
Epoch 43: training loss 39790064941.176
Test Loss of 31714631926.404442, Test MSE of 31714631517.633251
Epoch 44: training loss 37921454110.118
Test Loss of 30741164295.463211, Test MSE of 30741164219.633923
Epoch 45: training loss 35465378906.353
Test Loss of 29120668074.469227, Test MSE of 29120667921.167248
Epoch 46: training loss 34417279600.941
Test Loss of 27717195446.197132, Test MSE of 27717195195.207176
Epoch 47: training loss 32566334832.941
Test Loss of 27646071571.546505, Test MSE of 27646071793.437321
Epoch 48: training loss 30665896798.118
Test Loss of 22933116934.633965, Test MSE of 22933117082.125320
Epoch 49: training loss 29485273904.941
Test Loss of 23903252534.967144, Test MSE of 23903252578.019672
Epoch 50: training loss 27959201987.765
Test Loss of 23578484260.249886, Test MSE of 23578483925.438995
Epoch 51: training loss 26714494422.588
Test Loss of 23442570686.844978, Test MSE of 23442571006.476189
Epoch 52: training loss 26029099791.059
Test Loss of 20059213808.836651, Test MSE of 20059213732.319050
Epoch 53: training loss 24441200052.706
Test Loss of 20764447048.855160, Test MSE of 20764447104.701138
Epoch 54: training loss 23743395772.235
Test Loss of 19618057872.288754, Test MSE of 19618057586.567268
Epoch 55: training loss 22453825355.294
Test Loss of 19307377861.597408, Test MSE of 19307377796.910870
Epoch 56: training loss 21553554648.471
Test Loss of 19274226542.526608, Test MSE of 19274226470.755386
Epoch 57: training loss 20999835124.706
Test Loss of 20826411688.455345, Test MSE of 20826412118.908978
Epoch 58: training loss 20395668163.765
Test Loss of 18306944156.845905, Test MSE of 18306944345.728779
Epoch 59: training loss 19409563414.588
Test Loss of 19136507814.915318, Test MSE of 19136507612.074451
Epoch 60: training loss 18759778251.294
Test Loss of 17855393320.040722, Test MSE of 17855393534.502270
Epoch 61: training loss 18162595956.706
Test Loss of 18269288474.535862, Test MSE of 18269288753.827316
Epoch 62: training loss 17804703472.941
Test Loss of 18362601663.437298, Test MSE of 18362601959.063549
Epoch 63: training loss 17163369897.412
Test Loss of 18254271714.028690, Test MSE of 18254271769.026100
Epoch 64: training loss 16564759691.294
Test Loss of 17688787976.529385, Test MSE of 17688787948.958168
Epoch 65: training loss 16360489460.706
Test Loss of 19061093485.460434, Test MSE of 19061093513.965809
Epoch 66: training loss 15693274989.176
Test Loss of 18107081533.719574, Test MSE of 18107081966.901947
Epoch 67: training loss 15108241411.765
Test Loss of 17573296445.008793, Test MSE of 17573296412.295135
Epoch 68: training loss 15011591625.412
Test Loss of 18192645603.805645, Test MSE of 18192645936.127331
Epoch 69: training loss 14215146959.059
Test Loss of 18464707847.937065, Test MSE of 18464707905.393288
Epoch 70: training loss 14045885485.176
Test Loss of 17552314324.405369, Test MSE of 17552314340.104359
Epoch 71: training loss 13764667998.118
Test Loss of 17229914650.772789, Test MSE of 17229914301.267052
Epoch 72: training loss 13241171983.059
Test Loss of 17157113435.217030, Test MSE of 17157113094.346867
Epoch 73: training loss 12745346194.824
Test Loss of 18440059010.783897, Test MSE of 18440058595.358624
Epoch 74: training loss 12622702085.647
Test Loss of 17427348151.144840, Test MSE of 17427348409.443710
Epoch 75: training loss 12202347789.176
Test Loss of 19904304925.497456, Test MSE of 19904304999.534061
Epoch 76: training loss 11875694273.882
Test Loss of 18204130428.623787, Test MSE of 18204130579.536880
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 228388872611.90857, 'MSE - std': 204565186286.47055, 'R2 - mean': -0.6493843778974122, 'R2 - std': 1.470471622079692} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005431 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043746725.647
Test Loss of 431612279383.900024, Test MSE of 431612288222.313843
Epoch 2: training loss 424024190494.118
Test Loss of 431592719207.892639, Test MSE of 431592716424.374756
Epoch 3: training loss 423997553965.176
Test Loss of 431565685410.769104, Test MSE of 431565680571.878723
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010615024.941
Test Loss of 431567388588.601562, Test MSE of 431567392225.402100
Epoch 2: training loss 423999740385.882
Test Loss of 431570648269.179077, Test MSE of 431570648848.841736
Epoch 3: training loss 423999181763.765
Test Loss of 431570891671.278137, Test MSE of 431570889061.497803
Epoch 4: training loss 423998802040.471
Test Loss of 431570669937.606689, Test MSE of 431570666818.706055
Epoch 5: training loss 423998553750.588
Test Loss of 431570105117.971313, Test MSE of 431570100442.562317
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 269025118178.0393, 'MSE - std': 200206768534.87808, 'R2 - mean': -0.964103808940093, 'R2 - std': 1.458088719583503} 
 

Saving model.....
Results After CV: {'MSE - mean': 269025118178.0393, 'MSE - std': 200206768534.87808, 'R2 - mean': -0.964103808940093, 'R2 - std': 1.458088719583503}
Train time: 46.29544788280073
Inference time: 0.07066486019975855
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 30 finished with value: 269025118178.0393 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 7, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003888 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524861590.588
Test Loss of 418110016714.526001, Test MSE of 418110017974.914124
Epoch 2: training loss 427503417705.412
Test Loss of 418090779730.668518, Test MSE of 418090775238.662476
Epoch 3: training loss 427475772837.647
Test Loss of 418066876441.108459, Test MSE of 418066878744.760559
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490479646.118
Test Loss of 418071650866.809143, Test MSE of 418071647794.942383
Epoch 2: training loss 427479717767.529
Test Loss of 418074278272.207275, Test MSE of 418074274696.967224
Epoch 3: training loss 427479289615.059
Test Loss of 418073248373.133484, Test MSE of 418073248181.089966
Epoch 4: training loss 422224274010.353
Test Loss of 401252729487.663208, Test MSE of 401252734300.806030
Epoch 5: training loss 385973876374.588
Test Loss of 346701282346.400208, Test MSE of 346701277094.175537
Epoch 6: training loss 317859919149.176
Test Loss of 269461254935.154297, Test MSE of 269461252312.112762
Epoch 7: training loss 226001355535.059
Test Loss of 171580143276.324768, Test MSE of 171580144081.506256
Epoch 8: training loss 160992054211.765
Test Loss of 130943765661.046494, Test MSE of 130943764435.381332
Epoch 9: training loss 141263961840.941
Test Loss of 120560342046.556564, Test MSE of 120560342331.179596
Epoch 10: training loss 136346208225.882
Test Loss of 115600500907.021973, Test MSE of 115600499775.749741
Epoch 11: training loss 133016179215.059
Test Loss of 112750807226.181824, Test MSE of 112750805577.557449
Epoch 12: training loss 128913041603.765
Test Loss of 109859339363.249588, Test MSE of 109859339381.647461
Epoch 13: training loss 125462555934.118
Test Loss of 106637152245.577606, Test MSE of 106637151644.892395
Epoch 14: training loss 121865195851.294
Test Loss of 102860224776.823502, Test MSE of 102860226765.107407
Epoch 15: training loss 117983733007.059
Test Loss of 99352482446.715714, Test MSE of 99352481068.886276
Epoch 16: training loss 114840817182.118
Test Loss of 96738511941.877396, Test MSE of 96738512090.257202
Epoch 17: training loss 109790819267.765
Test Loss of 93157987352.871613, Test MSE of 93157989309.920639
Epoch 18: training loss 106624814080.000
Test Loss of 90613554637.190842, Test MSE of 90613555938.942703
Epoch 19: training loss 101965704523.294
Test Loss of 85733163214.552856, Test MSE of 85733162143.682800
Epoch 20: training loss 98393653534.118
Test Loss of 82962855918.471436, Test MSE of 82962855173.537125
Epoch 21: training loss 94199455209.412
Test Loss of 80223874508.243347, Test MSE of 80223875663.109207
Epoch 22: training loss 91310840786.824
Test Loss of 76485893414.432571, Test MSE of 76485892618.432678
Epoch 23: training loss 88285889385.412
Test Loss of 73738557701.033539, Test MSE of 73738556953.027588
Epoch 24: training loss 83059517936.941
Test Loss of 70152084175.618790, Test MSE of 70152084679.015717
Epoch 25: training loss 79474926486.588
Test Loss of 68676749325.738609, Test MSE of 68676749354.457756
Epoch 26: training loss 75668405180.235
Test Loss of 64472021506.960907, Test MSE of 64472022710.164589
Epoch 27: training loss 73578209340.235
Test Loss of 61862723053.405502, Test MSE of 61862722955.249237
Epoch 28: training loss 69779623506.824
Test Loss of 59135736326.750870, Test MSE of 59135735955.753578
Epoch 29: training loss 65799643632.941
Test Loss of 56107497039.470741, Test MSE of 56107497669.355225
Epoch 30: training loss 62210821308.235
Test Loss of 51915603819.244041, Test MSE of 51915604031.551224
Epoch 31: training loss 59933173579.294
Test Loss of 50178925050.907242, Test MSE of 50178925870.838814
Epoch 32: training loss 56799400011.294
Test Loss of 47325950801.898682, Test MSE of 47325950164.176758
Epoch 33: training loss 54960074164.706
Test Loss of 45575582453.992134, Test MSE of 45575582260.819321
Epoch 34: training loss 51279977818.353
Test Loss of 42548391952.107330, Test MSE of 42548392805.807877
Epoch 35: training loss 49015136737.882
Test Loss of 43360130548.511681, Test MSE of 43360130294.478302
Epoch 36: training loss 46027213180.235
Test Loss of 38837825003.510529, Test MSE of 38837824820.607101
Epoch 37: training loss 44405195576.471
Test Loss of 38712158415.974091, Test MSE of 38712158934.536469
Epoch 38: training loss 42003437052.235
Test Loss of 38579377352.157295, Test MSE of 38579377833.127174
Epoch 39: training loss 39576346198.588
Test Loss of 34755524675.745544, Test MSE of 34755525145.060684
Epoch 40: training loss 37986603572.706
Test Loss of 33343493903.811241, Test MSE of 33343494335.648357
Epoch 41: training loss 35611672591.059
Test Loss of 31201460077.139023, Test MSE of 31201460101.356728
Epoch 42: training loss 34594068171.294
Test Loss of 30495190839.132084, Test MSE of 30495190688.214317
Epoch 43: training loss 32838346646.588
Test Loss of 31998125175.620632, Test MSE of 31998124612.188065
Epoch 44: training loss 31384386477.176
Test Loss of 29627806324.185982, Test MSE of 29627806591.688694
Epoch 45: training loss 29644945174.588
Test Loss of 26389452620.450611, Test MSE of 26389452666.227318
Epoch 46: training loss 28561508638.118
Test Loss of 27307670668.939163, Test MSE of 27307670785.759312
Epoch 47: training loss 27061506145.882
Test Loss of 25014205040.869766, Test MSE of 25014204935.501995
Epoch 48: training loss 26137295139.765
Test Loss of 27267209226.896137, Test MSE of 27267209664.904724
Epoch 49: training loss 25155308329.412
Test Loss of 24249739352.116585, Test MSE of 24249738932.500648
Epoch 50: training loss 24311716299.294
Test Loss of 22600581812.141567, Test MSE of 22600582018.759495
Epoch 51: training loss 23074239420.235
Test Loss of 24132225104.062920, Test MSE of 24132224848.593990
Epoch 52: training loss 21831220235.294
Test Loss of 22414716804.826279, Test MSE of 22414716747.658978
Epoch 53: training loss 21436315215.059
Test Loss of 25604418297.308350, Test MSE of 25604418456.625816
Epoch 54: training loss 20436775401.412
Test Loss of 23697793788.624565, Test MSE of 23697793527.505829
Epoch 55: training loss 19803268928.000
Test Loss of 21197991104.577377, Test MSE of 21197991178.449890
Epoch 56: training loss 19164639706.353
Test Loss of 22885700135.913025, Test MSE of 22885700424.198128
Epoch 57: training loss 18504616907.294
Test Loss of 22152318423.139488, Test MSE of 22152318205.595695
Epoch 58: training loss 17959734750.118
Test Loss of 19522048617.289845, Test MSE of 19522048688.561581
Epoch 59: training loss 17336311085.176
Test Loss of 20614639460.137867, Test MSE of 20614639770.876030
Epoch 60: training loss 16802723376.941
Test Loss of 20752571561.837612, Test MSE of 20752571342.809109
Epoch 61: training loss 16357196254.118
Test Loss of 21558735141.248207, Test MSE of 21558734918.721516
Epoch 62: training loss 15970848527.059
Test Loss of 21084861084.928059, Test MSE of 21084861106.936138
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21084861106.93614, 'MSE - std': 0.0, 'R2 - mean': 0.8358098988702598, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005480 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917408376.471
Test Loss of 424554720486.950745, Test MSE of 424554719692.418579
Epoch 2: training loss 427895506100.706
Test Loss of 424537990956.235962, Test MSE of 424537995782.149292
Epoch 3: training loss 427867329716.706
Test Loss of 424514940999.061768, Test MSE of 424514947396.767395
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886393464.471
Test Loss of 424521791057.365723, Test MSE of 424521789159.526489
Epoch 2: training loss 427874580118.588
Test Loss of 424523079196.780029, Test MSE of 424523074736.515015
Epoch 3: training loss 427873961622.588
Test Loss of 424522233095.639160, Test MSE of 424522236712.988464
Epoch 4: training loss 422635046550.588
Test Loss of 407881996916.659729, Test MSE of 407881995748.760681
Epoch 5: training loss 386496044453.647
Test Loss of 354328658280.046265, Test MSE of 354328662577.688293
Epoch 6: training loss 317718599559.529
Test Loss of 277815134725.803406, Test MSE of 277815135679.841309
Epoch 7: training loss 225304211998.118
Test Loss of 180753800130.176270, Test MSE of 180753802760.525055
Epoch 8: training loss 159563774012.235
Test Loss of 141634686074.936859, Test MSE of 141634686021.663086
Epoch 9: training loss 139344274522.353
Test Loss of 131313281600.547775, Test MSE of 131313281227.691879
Epoch 10: training loss 133858013123.765
Test Loss of 127445545106.860977, Test MSE of 127445546880.952698
Epoch 11: training loss 130914425675.294
Test Loss of 123912511105.213974, Test MSE of 123912513305.926620
Epoch 12: training loss 126621948506.353
Test Loss of 120968642834.772156, Test MSE of 120968641988.648544
Epoch 13: training loss 123465064508.235
Test Loss of 117456070523.825119, Test MSE of 117456071236.703766
Epoch 14: training loss 118904909101.176
Test Loss of 114320584485.129776, Test MSE of 114320583772.691559
Epoch 15: training loss 114882097423.059
Test Loss of 110325003086.108719, Test MSE of 110325002892.160995
Epoch 16: training loss 112037568903.529
Test Loss of 105821899068.935455, Test MSE of 105821901234.392441
Epoch 17: training loss 108660500239.059
Test Loss of 102645815390.275269, Test MSE of 102645813997.525070
Epoch 18: training loss 103889788626.824
Test Loss of 99996536566.702759, Test MSE of 99996534101.110489
Epoch 19: training loss 100102928323.765
Test Loss of 94233436737.969009, Test MSE of 94233437755.872391
Epoch 20: training loss 96104198174.118
Test Loss of 91314817325.775620, Test MSE of 91314815871.300415
Epoch 21: training loss 91474999536.941
Test Loss of 84892490977.265793, Test MSE of 84892489714.162445
Epoch 22: training loss 88243076577.882
Test Loss of 82299243174.876709, Test MSE of 82299242295.034790
Epoch 23: training loss 84408592896.000
Test Loss of 78196058076.469116, Test MSE of 78196056776.144653
Epoch 24: training loss 80453118539.294
Test Loss of 75938110958.352997, Test MSE of 75938110110.847534
Epoch 25: training loss 77529882699.294
Test Loss of 72224744921.034470, Test MSE of 72224744127.250824
Epoch 26: training loss 72280992256.000
Test Loss of 67103242582.517693, Test MSE of 67103242471.409119
Epoch 27: training loss 69949516122.353
Test Loss of 66294459949.834839, Test MSE of 66294460452.998398
Epoch 28: training loss 66562322778.353
Test Loss of 60682195035.906548, Test MSE of 60682194923.704872
Epoch 29: training loss 62653699538.824
Test Loss of 60120691794.668518, Test MSE of 60120691999.319267
Epoch 30: training loss 59821616971.294
Test Loss of 59134113263.774231, Test MSE of 59134114251.751572
Epoch 31: training loss 56627324325.647
Test Loss of 52444256200.098083, Test MSE of 52444256727.272614
Epoch 32: training loss 53284631175.529
Test Loss of 54918923118.323387, Test MSE of 54918924015.143517
Epoch 33: training loss 50930570398.118
Test Loss of 48799446408.497803, Test MSE of 48799447482.272758
Epoch 34: training loss 48044022558.118
Test Loss of 43299470165.214897, Test MSE of 43299471081.971260
Epoch 35: training loss 46202139768.471
Test Loss of 46718349015.435577, Test MSE of 46718349111.432755
Epoch 36: training loss 43263126076.235
Test Loss of 44117453174.732361, Test MSE of 44117452810.634979
Epoch 37: training loss 41047275128.471
Test Loss of 39575646971.203331, Test MSE of 39575646813.119553
Epoch 38: training loss 39182599521.882
Test Loss of 37162977852.757805, Test MSE of 37162977932.880234
Epoch 39: training loss 36631678539.294
Test Loss of 33187246083.316216, Test MSE of 33187245960.128334
Epoch 40: training loss 34608964050.824
Test Loss of 41423473655.709457, Test MSE of 41423473927.230911
Epoch 41: training loss 32816308156.235
Test Loss of 35035862994.046730, Test MSE of 35035863637.377090
Epoch 42: training loss 30924983913.412
Test Loss of 34586625435.921349, Test MSE of 34586627181.627129
Epoch 43: training loss 30102466100.706
Test Loss of 33773028979.475365, Test MSE of 33773029553.624126
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 27428945330.280132, 'MSE - std': 6344084223.343994, 'R2 - mean': 0.7973464453649668, 'R2 - std': 0.038463453505293066} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005712 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421925911491.765
Test Loss of 447257366976.162842, Test MSE of 447257366708.766479
Epoch 2: training loss 421903749360.941
Test Loss of 447237590673.321289, Test MSE of 447237597971.220032
Epoch 3: training loss 421875569362.824
Test Loss of 447211822488.131409, Test MSE of 447211825126.463135
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899851535.059
Test Loss of 447219828552.660645, Test MSE of 447219825226.407837
Epoch 2: training loss 421884991247.059
Test Loss of 447220468858.936829, Test MSE of 447220463541.228516
Epoch 3: training loss 421884437564.235
Test Loss of 447220429428.185974, Test MSE of 447220426794.971680
Epoch 4: training loss 416856474081.882
Test Loss of 430685685091.545715, Test MSE of 430685682687.699463
Epoch 5: training loss 381400210853.647
Test Loss of 376317745405.453613, Test MSE of 376317751990.504150
Epoch 6: training loss 313628744402.824
Test Loss of 296957493145.671082, Test MSE of 296957494326.185181
Epoch 7: training loss 220901564235.294
Test Loss of 195937344597.984741, Test MSE of 195937344540.186707
Epoch 8: training loss 157413946127.059
Test Loss of 154549388984.642151, Test MSE of 154549392878.102386
Epoch 9: training loss 137492304835.765
Test Loss of 142719389480.919739, Test MSE of 142719391975.991455
Epoch 10: training loss 131968335058.824
Test Loss of 136832417064.327545, Test MSE of 136832416774.508728
Epoch 11: training loss 128919000967.529
Test Loss of 133491799337.748779, Test MSE of 133491799257.276016
Epoch 12: training loss 125116928903.529
Test Loss of 130253666824.645844, Test MSE of 130253667099.840714
Epoch 13: training loss 120920550851.765
Test Loss of 126483918483.216278, Test MSE of 126483918928.284958
Epoch 14: training loss 118183971900.235
Test Loss of 122811308876.687485, Test MSE of 122811310503.114517
Epoch 15: training loss 113206241611.294
Test Loss of 118845767941.507294, Test MSE of 118845769358.881622
Epoch 16: training loss 109910979282.824
Test Loss of 115525608706.901688, Test MSE of 115525609404.070496
Epoch 17: training loss 106282545483.294
Test Loss of 112212310059.110809, Test MSE of 112212310611.528595
Epoch 18: training loss 103226111277.176
Test Loss of 108477045796.715240, Test MSE of 108477045091.700195
Epoch 19: training loss 98575771919.059
Test Loss of 106182764093.468430, Test MSE of 106182764998.462570
Epoch 20: training loss 95002179960.471
Test Loss of 98343791583.548462, Test MSE of 98343790475.804337
Epoch 21: training loss 90805682416.941
Test Loss of 97555288665.656250, Test MSE of 97555289888.197403
Epoch 22: training loss 87164053308.235
Test Loss of 92488632293.944016, Test MSE of 92488632599.257980
Epoch 23: training loss 83825229206.588
Test Loss of 88414181928.860519, Test MSE of 88414180798.640106
Epoch 24: training loss 79106985517.176
Test Loss of 83668449755.166321, Test MSE of 83668451469.992599
Epoch 25: training loss 76063600564.706
Test Loss of 80419450691.449463, Test MSE of 80419450069.144409
Epoch 26: training loss 73702643636.706
Test Loss of 78114745122.050430, Test MSE of 78114743815.781067
Epoch 27: training loss 69935477037.176
Test Loss of 75491256345.108490, Test MSE of 75491256713.246918
Epoch 28: training loss 66271621632.000
Test Loss of 73058611655.742767, Test MSE of 73058613410.929855
Epoch 29: training loss 63390997187.765
Test Loss of 69343498580.148972, Test MSE of 69343497989.774567
Epoch 30: training loss 59616198144.000
Test Loss of 62824470468.781868, Test MSE of 62824470989.509155
Epoch 31: training loss 57219232798.118
Test Loss of 61199158751.193153, Test MSE of 61199158942.568695
Epoch 32: training loss 53750188092.235
Test Loss of 60254038643.712234, Test MSE of 60254038935.944138
Epoch 33: training loss 51371953558.588
Test Loss of 53904954243.168167, Test MSE of 53904954199.630867
Epoch 34: training loss 48617287175.529
Test Loss of 51542245768.260933, Test MSE of 51542245787.642403
Epoch 35: training loss 46205927823.059
Test Loss of 51567901767.772377, Test MSE of 51567901900.243301
Epoch 36: training loss 43977239664.941
Test Loss of 45646090465.502663, Test MSE of 45646090398.239449
Epoch 37: training loss 41618076227.765
Test Loss of 42638198433.902382, Test MSE of 42638198117.022797
Epoch 38: training loss 39504344854.588
Test Loss of 45017832250.685173, Test MSE of 45017832347.783981
Epoch 39: training loss 37549661711.059
Test Loss of 42193699055.715012, Test MSE of 42193699596.075157
Epoch 40: training loss 35638290281.412
Test Loss of 35178034169.604439, Test MSE of 35178034179.564827
Epoch 41: training loss 34008913490.824
Test Loss of 37456281853.927368, Test MSE of 37456282009.097260
Epoch 42: training loss 32348108099.765
Test Loss of 35508000217.981956, Test MSE of 35508001008.875435
Epoch 43: training loss 31194476747.294
Test Loss of 34560597989.470276, Test MSE of 34560597659.071045
Epoch 44: training loss 29425172833.882
Test Loss of 34784431956.504280, Test MSE of 34784432640.773987
Epoch 45: training loss 27764250774.588
Test Loss of 31961911115.976868, Test MSE of 31961910944.087849
Epoch 46: training loss 26586635862.588
Test Loss of 32628998276.648624, Test MSE of 32628998222.143883
Epoch 47: training loss 25107503973.647
Test Loss of 31169913335.117279, Test MSE of 31169913415.224102
Epoch 48: training loss 24538988032.000
Test Loss of 29169547146.984962, Test MSE of 29169547101.365768
Epoch 49: training loss 23108359849.412
Test Loss of 26857462490.514919, Test MSE of 26857462534.253231
Epoch 50: training loss 22550047190.588
Test Loss of 26573818719.637287, Test MSE of 26573818784.564938
Epoch 51: training loss 21273178944.000
Test Loss of 25757635976.497803, Test MSE of 25757636201.761486
Epoch 52: training loss 20581503386.353
Test Loss of 25945877536.451538, Test MSE of 25945877810.307728
Epoch 53: training loss 20077792643.765
Test Loss of 25528585955.752949, Test MSE of 25528586255.945484
Epoch 54: training loss 19327648071.529
Test Loss of 24439575267.516075, Test MSE of 24439575274.688587
Epoch 55: training loss 18480818270.118
Test Loss of 23837174464.932686, Test MSE of 23837174530.972351
Epoch 56: training loss 18208636382.118
Test Loss of 24474308838.240112, Test MSE of 24474308667.021713
Epoch 57: training loss 17311239239.529
Test Loss of 23834510851.671524, Test MSE of 23834510392.262836
Epoch 58: training loss 17001780480.000
Test Loss of 22596906110.726810, Test MSE of 22596906174.357014
Epoch 59: training loss 16337453613.176
Test Loss of 23773114511.544762, Test MSE of 23773114571.465824
Epoch 60: training loss 15850325854.118
Test Loss of 24285141379.049732, Test MSE of 24285141424.564728
Epoch 61: training loss 15567221665.882
Test Loss of 22783811147.680779, Test MSE of 22783811059.854671
Epoch 62: training loss 14955603361.882
Test Loss of 21663559498.081886, Test MSE of 21663559563.418186
Epoch 63: training loss 14508243783.529
Test Loss of 23378335731.682629, Test MSE of 23378335706.666260
Epoch 64: training loss 14068009317.647
Test Loss of 23869225014.006939, Test MSE of 23869225338.446629
Epoch 65: training loss 13852384259.765
Test Loss of 22579052245.540596, Test MSE of 22579052432.146687
Epoch 66: training loss 13441785931.294
Test Loss of 22461032436.156372, Test MSE of 22461031858.681633
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25772974173.08063, 'MSE - std': 5684723743.115527, 'R2 - mean': 0.8150570946479562, 'R2 - std': 0.04016996016618219} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005415 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109633355.294
Test Loss of 410764921399.677917, Test MSE of 410764923770.055298
Epoch 2: training loss 430087276664.471
Test Loss of 410744959349.397522, Test MSE of 410744959287.837830
Epoch 3: training loss 430058455040.000
Test Loss of 410720031089.606689, Test MSE of 410720034894.502991
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076440094.118
Test Loss of 410724903453.142090, Test MSE of 410724909388.007812
Epoch 2: training loss 430063343977.412
Test Loss of 410726047285.308655, Test MSE of 410726049524.192627
Epoch 3: training loss 430062853903.059
Test Loss of 410725375670.197144, Test MSE of 410725374646.190613
Epoch 4: training loss 424279318287.059
Test Loss of 392786656969.151306, Test MSE of 392786656148.710876
Epoch 5: training loss 386774845680.941
Test Loss of 336791209331.028198, Test MSE of 336791217289.234802
Epoch 6: training loss 318299838945.882
Test Loss of 260390630583.381775, Test MSE of 260390637989.228394
Epoch 7: training loss 226816687284.706
Test Loss of 162690969715.620544, Test MSE of 162690971788.152252
Epoch 8: training loss 162015793694.118
Test Loss of 123515868900.634888, Test MSE of 123515869101.648926
Epoch 9: training loss 144655508178.824
Test Loss of 113924929627.927811, Test MSE of 113924928543.511795
Epoch 10: training loss 139526779542.588
Test Loss of 110205592207.814896, Test MSE of 110205590331.273102
Epoch 11: training loss 135222076596.706
Test Loss of 107036697311.422485, Test MSE of 107036695497.559814
Epoch 12: training loss 131103032621.176
Test Loss of 104913428006.145309, Test MSE of 104913427703.680847
Epoch 13: training loss 127479196250.353
Test Loss of 101270242060.912537, Test MSE of 101270241574.950317
Epoch 14: training loss 124007434631.529
Test Loss of 98905473585.043961, Test MSE of 98905471945.250626
Epoch 15: training loss 120193915994.353
Test Loss of 95185462976.621933, Test MSE of 95185462106.099442
Epoch 16: training loss 116035323813.647
Test Loss of 91212447812.235077, Test MSE of 91212449768.689621
Epoch 17: training loss 111025084235.294
Test Loss of 88738076494.778351, Test MSE of 88738077261.694305
Epoch 18: training loss 108029746296.471
Test Loss of 85389324305.058762, Test MSE of 85389324655.243896
Epoch 19: training loss 104998910614.588
Test Loss of 81684449320.751511, Test MSE of 81684449642.879700
Epoch 20: training loss 99902118008.471
Test Loss of 78857024858.387787, Test MSE of 78857024031.964401
Epoch 21: training loss 96489966923.294
Test Loss of 76405892675.524292, Test MSE of 76405891414.337738
Epoch 22: training loss 92407547602.824
Test Loss of 74734215751.788986, Test MSE of 74734215608.909561
Epoch 23: training loss 88213303491.765
Test Loss of 71722642896.851456, Test MSE of 71722641834.868561
Epoch 24: training loss 84704077763.765
Test Loss of 69785629197.031006, Test MSE of 69785629783.448944
Epoch 25: training loss 80838044370.824
Test Loss of 66134767575.248497, Test MSE of 66134766992.770218
Epoch 26: training loss 77182208933.647
Test Loss of 63331046048.873672, Test MSE of 63331047134.102180
Epoch 27: training loss 73454959992.471
Test Loss of 58630161415.107819, Test MSE of 58630160669.379959
Epoch 28: training loss 70694526704.941
Test Loss of 58244692681.625175, Test MSE of 58244693337.976547
Epoch 29: training loss 67240317831.529
Test Loss of 54903396887.455811, Test MSE of 54903396819.755310
Epoch 30: training loss 64233078452.706
Test Loss of 52420165544.810738, Test MSE of 52420165561.689789
Epoch 31: training loss 60690529889.882
Test Loss of 51144747846.722816, Test MSE of 51144748018.511497
Epoch 32: training loss 58218641566.118
Test Loss of 46076345008.984726, Test MSE of 46076345181.807869
Epoch 33: training loss 54856910667.294
Test Loss of 45607923351.870430, Test MSE of 45607923988.336136
Epoch 34: training loss 52555235252.706
Test Loss of 42330564942.541412, Test MSE of 42330565448.464294
Epoch 35: training loss 49585295894.588
Test Loss of 41117450765.031006, Test MSE of 41117449964.977097
Epoch 36: training loss 47043923154.824
Test Loss of 40678157680.185097, Test MSE of 40678157557.632187
Epoch 37: training loss 44884157191.529
Test Loss of 37197603485.082832, Test MSE of 37197603273.282471
Epoch 38: training loss 42033024918.588
Test Loss of 36575500832.932899, Test MSE of 36575501780.799988
Epoch 39: training loss 40567701428.706
Test Loss of 33247694438.589542, Test MSE of 33247695096.976688
Epoch 40: training loss 38238085978.353
Test Loss of 30817130508.320221, Test MSE of 30817130306.968433
Epoch 41: training loss 36547884498.824
Test Loss of 32946880532.849606, Test MSE of 32946879907.397659
Epoch 42: training loss 35085628212.706
Test Loss of 29623623788.038872, Test MSE of 29623624071.917599
Epoch 43: training loss 33091773771.294
Test Loss of 28256467697.428967, Test MSE of 28256467940.925911
Epoch 44: training loss 31871128267.294
Test Loss of 26157883606.182323, Test MSE of 26157883804.025204
Epoch 45: training loss 29700561152.000
Test Loss of 28337792933.019897, Test MSE of 28337792603.405525
Epoch 46: training loss 28767311111.529
Test Loss of 25055548572.845905, Test MSE of 25055548573.959633
Epoch 47: training loss 27729504101.647
Test Loss of 25725072127.644608, Test MSE of 25725072773.072834
Epoch 48: training loss 26406536594.824
Test Loss of 24155205515.431744, Test MSE of 24155205417.887146
Epoch 49: training loss 25363015898.353
Test Loss of 24520829319.403980, Test MSE of 24520829562.958973
Epoch 50: training loss 24329382618.353
Test Loss of 24261116496.318371, Test MSE of 24261116864.379135
Epoch 51: training loss 23467428800.000
Test Loss of 21294482656.133270, Test MSE of 21294482815.280182
Epoch 52: training loss 22017312041.412
Test Loss of 20549424095.304028, Test MSE of 20549424449.461941
Epoch 53: training loss 22090833825.882
Test Loss of 22753078346.395187, Test MSE of 22753078098.330208
Epoch 54: training loss 20904618763.294
Test Loss of 21220083570.791302, Test MSE of 21220083615.542351
Epoch 55: training loss 20344577630.118
Test Loss of 20256703516.905136, Test MSE of 20256703931.252865
Epoch 56: training loss 19169813248.000
Test Loss of 19597090009.499306, Test MSE of 19597089646.654724
Epoch 57: training loss 18650361347.765
Test Loss of 21926341564.238777, Test MSE of 21926341817.687004
Epoch 58: training loss 18226827783.529
Test Loss of 22023768463.933365, Test MSE of 22023768541.929562
Epoch 59: training loss 17425726283.294
Test Loss of 18249437421.875057, Test MSE of 18249437233.892048
Epoch 60: training loss 17023323949.176
Test Loss of 18457679035.172604, Test MSE of 18457679269.166191
Epoch 61: training loss 16645569991.529
Test Loss of 18546058338.087921, Test MSE of 18546058040.201900
Epoch 62: training loss 16072550234.353
Test Loss of 19564656114.968994, Test MSE of 19564656184.290051
Epoch 63: training loss 15823888577.882
Test Loss of 19042394213.404903, Test MSE of 19042393856.094570
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24090329093.834118, 'MSE - std': 5721096609.461651, 'R2 - mean': 0.8220011208247797, 'R2 - std': 0.036808664380112016} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005450 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043895506.824
Test Loss of 431612073056.192505, Test MSE of 431612072606.390015
Epoch 2: training loss 424023204080.941
Test Loss of 431591169498.328552, Test MSE of 431591178134.097961
Epoch 3: training loss 423995196837.647
Test Loss of 431562923724.942139, Test MSE of 431562929527.946655
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011635290.353
Test Loss of 431564750347.609436, Test MSE of 431564752719.889587
Epoch 2: training loss 423997509993.412
Test Loss of 431568801918.993042, Test MSE of 431568807396.500427
Epoch 3: training loss 423996853910.588
Test Loss of 431569711280.273926, Test MSE of 431569712625.621582
Epoch 4: training loss 418770883764.706
Test Loss of 414667210888.470154, Test MSE of 414667211151.967468
Epoch 5: training loss 382802483200.000
Test Loss of 358635315029.412292, Test MSE of 358635315134.434998
Epoch 6: training loss 315343505769.412
Test Loss of 279895816685.756592, Test MSE of 279895812556.320374
Epoch 7: training loss 224068794910.118
Test Loss of 180662357809.399353, Test MSE of 180662356907.454193
Epoch 8: training loss 160043476992.000
Test Loss of 137895350091.935211, Test MSE of 137895350392.116486
Epoch 9: training loss 140485059809.882
Test Loss of 126616229225.551132, Test MSE of 126616228477.463211
Epoch 10: training loss 135120755410.824
Test Loss of 121534213956.353546, Test MSE of 121534216738.992050
Epoch 11: training loss 131823109752.471
Test Loss of 118071748379.602036, Test MSE of 118071747528.114792
Epoch 12: training loss 128253500777.412
Test Loss of 115399241745.058762, Test MSE of 115399239427.471146
Epoch 13: training loss 124390166016.000
Test Loss of 111350358614.952332, Test MSE of 111350360295.466721
Epoch 14: training loss 120643139403.294
Test Loss of 107944638681.025452, Test MSE of 107944636822.883072
Epoch 15: training loss 117995065976.471
Test Loss of 103538727864.447937, Test MSE of 103538728245.692642
Epoch 16: training loss 113270071296.000
Test Loss of 100003680341.293839, Test MSE of 100003680584.358063
Epoch 17: training loss 109863848448.000
Test Loss of 97867580616.440536, Test MSE of 97867581602.334396
Epoch 18: training loss 105941425453.176
Test Loss of 92806055612.831100, Test MSE of 92806057687.186920
Epoch 19: training loss 102340506654.118
Test Loss of 88864995522.280426, Test MSE of 88864996212.367798
Epoch 20: training loss 98760528474.353
Test Loss of 85817280317.719574, Test MSE of 85817280149.080658
Epoch 21: training loss 94745489950.118
Test Loss of 83429366659.376221, Test MSE of 83429367922.121445
Epoch 22: training loss 89650172167.529
Test Loss of 79983054813.882462, Test MSE of 79983054601.772568
Epoch 23: training loss 86964481942.588
Test Loss of 77383158153.299393, Test MSE of 77383157349.464890
Epoch 24: training loss 83607383115.294
Test Loss of 73390842758.219345, Test MSE of 73390841624.416092
Epoch 25: training loss 79802003222.588
Test Loss of 69758022210.102737, Test MSE of 69758022338.501801
Epoch 26: training loss 77594588054.588
Test Loss of 66965255383.603889, Test MSE of 66965254818.171272
Epoch 27: training loss 73485825423.059
Test Loss of 64897646455.529846, Test MSE of 64897646363.159691
Epoch 28: training loss 70007759164.235
Test Loss of 59845675370.972694, Test MSE of 59845676298.600670
Epoch 29: training loss 67260856877.176
Test Loss of 56721724019.383621, Test MSE of 56721724504.118027
Epoch 30: training loss 63826975653.647
Test Loss of 58111569353.269783, Test MSE of 58111568448.310951
Epoch 31: training loss 60724339666.824
Test Loss of 53397958127.178162, Test MSE of 53397958287.823242
Epoch 32: training loss 57374540099.765
Test Loss of 50967572614.574738, Test MSE of 50967572566.077248
Epoch 33: training loss 55285884566.588
Test Loss of 47127911775.126328, Test MSE of 47127911699.796265
Epoch 34: training loss 52291117876.706
Test Loss of 48210178881.984268, Test MSE of 48210177980.108444
Epoch 35: training loss 49744386846.118
Test Loss of 41162229649.118004, Test MSE of 41162230051.400200
Epoch 36: training loss 46913885824.000
Test Loss of 43167526730.039795, Test MSE of 43167525769.694672
Epoch 37: training loss 44815461801.412
Test Loss of 39439146577.739937, Test MSE of 39439146943.632492
Epoch 38: training loss 42630632094.118
Test Loss of 34264706220.956963, Test MSE of 34264706211.903343
Epoch 39: training loss 40870845312.000
Test Loss of 33769756957.260529, Test MSE of 33769756821.393703
Epoch 40: training loss 39067240711.529
Test Loss of 32115766836.360943, Test MSE of 32115766709.678005
Epoch 41: training loss 36760346522.353
Test Loss of 31824034118.959740, Test MSE of 31824034394.605103
Epoch 42: training loss 35132663830.588
Test Loss of 30858741578.039795, Test MSE of 30858741338.928516
Epoch 43: training loss 33537174656.000
Test Loss of 30740897246.119389, Test MSE of 30740897724.101112
Epoch 44: training loss 32176010036.706
Test Loss of 27991705408.562702, Test MSE of 27991705116.175987
Epoch 45: training loss 30613933199.059
Test Loss of 27627065604.620083, Test MSE of 27627065685.196865
Epoch 46: training loss 29411293387.294
Test Loss of 27912426993.073578, Test MSE of 27912426907.791145
Epoch 47: training loss 27974522864.941
Test Loss of 24629037771.520592, Test MSE of 24629038298.680439
Epoch 48: training loss 26664621718.588
Test Loss of 27277776741.523369, Test MSE of 27277777031.411549
Epoch 49: training loss 25743604672.000
Test Loss of 26171965355.653866, Test MSE of 26171965826.015324
Epoch 50: training loss 24840049502.118
Test Loss of 23173762844.549744, Test MSE of 23173762755.956985
Epoch 51: training loss 23701422584.471
Test Loss of 23658152606.978252, Test MSE of 23658152471.088951
Epoch 52: training loss 22776929453.176
Test Loss of 22325843348.198055, Test MSE of 22325843474.374474
Epoch 53: training loss 22332487085.176
Test Loss of 24252973255.018974, Test MSE of 24252972744.987392
Epoch 54: training loss 21505634669.176
Test Loss of 23401531104.844055, Test MSE of 23401530901.680138
Epoch 55: training loss 20934543608.471
Test Loss of 21712751479.055992, Test MSE of 21712751837.726032
Epoch 56: training loss 19699862098.824
Test Loss of 21286969972.331329, Test MSE of 21286969967.611950
Epoch 57: training loss 19211779166.118
Test Loss of 20867356599.974087, Test MSE of 20867356630.762276
Epoch 58: training loss 18905022418.824
Test Loss of 21760889520.037022, Test MSE of 21760889529.017723
Epoch 59: training loss 18154558241.882
Test Loss of 21769435538.776493, Test MSE of 21769435566.212894
Epoch 60: training loss 17745592470.588
Test Loss of 21423215424.088848, Test MSE of 21423215263.219921
Epoch 61: training loss 17150390136.471
Test Loss of 20601446233.203148, Test MSE of 20601446135.719433
Epoch 62: training loss 16764028336.941
Test Loss of 20720117379.020824, Test MSE of 20720117316.328308
Epoch 63: training loss 16197041505.882
Test Loss of 18949163606.004627, Test MSE of 18949163662.107635
Epoch 64: training loss 15861464105.412
Test Loss of 20338268354.280426, Test MSE of 20338267931.227592
Epoch 65: training loss 15394676133.647
Test Loss of 17883634180.501621, Test MSE of 17883633917.025204
Epoch 66: training loss 14922234985.412
Test Loss of 18589001312.903286, Test MSE of 18589001313.790478
Epoch 67: training loss 14845410066.824
Test Loss of 19286787272.440536, Test MSE of 19286787330.489559
Epoch 68: training loss 14437754409.412
Test Loss of 21335890340.782970, Test MSE of 21335890715.487194
Epoch 69: training loss 13834579536.941
Test Loss of 19638524295.403980, Test MSE of 19638523819.387695
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23199968038.94483, 'MSE - std': 5418092686.923057, 'R2 - mean': 0.8282686516583574, 'R2 - std': 0.035228255559388286} 
 

Saving model.....
Results After CV: {'MSE - mean': 23199968038.94483, 'MSE - std': 5418092686.923057, 'R2 - mean': 0.8282686516583574, 'R2 - std': 0.035228255559388286}
Train time: 92.83417722459926
Inference time: 0.07133435860014288
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 31 finished with value: 23199968038.94483 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005515 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525596220.235
Test Loss of 418111382169.138123, Test MSE of 418111385885.604187
Epoch 2: training loss 427505221632.000
Test Loss of 418094163915.414307, Test MSE of 418094170142.475708
Epoch 3: training loss 427478191164.235
Test Loss of 418070653144.501526, Test MSE of 418070653373.086487
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493669345.882
Test Loss of 418075382757.707153, Test MSE of 418075383268.871277
Epoch 2: training loss 427483117327.059
Test Loss of 418076524619.799194, Test MSE of 418076522213.786133
Epoch 3: training loss 427482749048.471
Test Loss of 418076208916.074951, Test MSE of 418076209015.545410
Epoch 4: training loss 421878714247.529
Test Loss of 400619663378.002319, Test MSE of 400619658636.856567
Epoch 5: training loss 384937188050.824
Test Loss of 345458443801.463806, Test MSE of 345458447243.615356
Epoch 6: training loss 316807782881.882
Test Loss of 267688123303.646545, Test MSE of 267688127170.406952
Epoch 7: training loss 225731768621.176
Test Loss of 170151943026.587097, Test MSE of 170151943488.905273
Epoch 8: training loss 160251440429.176
Test Loss of 131222692440.708771, Test MSE of 131222692843.295624
Epoch 9: training loss 141460470784.000
Test Loss of 120510716089.234329, Test MSE of 120510717760.688461
Epoch 10: training loss 136435559122.824
Test Loss of 116227070978.605591, Test MSE of 116227070922.892868
Epoch 11: training loss 132123163934.118
Test Loss of 113403581131.355072, Test MSE of 113403580080.252426
Epoch 12: training loss 130413259565.176
Test Loss of 110465506422.199402, Test MSE of 110465507556.401749
Epoch 13: training loss 126251000312.471
Test Loss of 107442611007.896362, Test MSE of 107442611984.262817
Epoch 14: training loss 121943229485.176
Test Loss of 103128208929.991211, Test MSE of 103128208196.565903
Epoch 15: training loss 118429345355.294
Test Loss of 100258351616.118439, Test MSE of 100258352516.912582
Epoch 16: training loss 114874762029.176
Test Loss of 96819123071.378204, Test MSE of 96819124150.951889
Epoch 17: training loss 111715092208.941
Test Loss of 93612945695.089523, Test MSE of 93612945989.154999
Epoch 18: training loss 106531567329.882
Test Loss of 90692537739.814011, Test MSE of 90692538757.882843
Epoch 19: training loss 103795817637.647
Test Loss of 86305744047.522552, Test MSE of 86305743915.848862
Epoch 20: training loss 99121868769.882
Test Loss of 82987037754.981262, Test MSE of 82987037513.825027
Epoch 21: training loss 95544767382.588
Test Loss of 82204667112.845703, Test MSE of 82204667199.268494
Epoch 22: training loss 91710973545.412
Test Loss of 78534617207.857513, Test MSE of 78534617577.438934
Epoch 23: training loss 88248604235.294
Test Loss of 73254822238.571365, Test MSE of 73254821858.821701
Epoch 24: training loss 84677670174.118
Test Loss of 72444753640.964142, Test MSE of 72444753635.603134
Epoch 25: training loss 79842336572.235
Test Loss of 68148528421.011337, Test MSE of 68148529061.675453
Epoch 26: training loss 76881039668.706
Test Loss of 64829209433.952347, Test MSE of 64829210202.504089
Epoch 27: training loss 73600497377.882
Test Loss of 63560073778.809158, Test MSE of 63560073048.893227
Epoch 28: training loss 69790598535.529
Test Loss of 58602790023.254219, Test MSE of 58602789696.466217
Epoch 29: training loss 66572370526.118
Test Loss of 56809047459.975014, Test MSE of 56809046847.234398
Epoch 30: training loss 63638413974.588
Test Loss of 54228844833.931992, Test MSE of 54228844826.935417
Epoch 31: training loss 61102734426.353
Test Loss of 48836764104.690262, Test MSE of 48836764036.406761
Epoch 32: training loss 58746223119.059
Test Loss of 51531881846.021744, Test MSE of 51531880843.419258
Epoch 33: training loss 55233696112.941
Test Loss of 48440905272.257225, Test MSE of 48440906065.387894
Epoch 34: training loss 52393894535.529
Test Loss of 44900824847.811241, Test MSE of 44900825333.959251
Epoch 35: training loss 50375096022.588
Test Loss of 42058691587.079346, Test MSE of 42058691601.817986
Epoch 36: training loss 47855068980.706
Test Loss of 41605020641.443443, Test MSE of 41605020414.073578
Epoch 37: training loss 45010939535.059
Test Loss of 40840218480.455238, Test MSE of 40840219041.892967
Epoch 38: training loss 42676844777.412
Test Loss of 36237731983.544762, Test MSE of 36237731471.414948
Epoch 39: training loss 40787193072.941
Test Loss of 35643727680.606987, Test MSE of 35643727672.052925
Epoch 40: training loss 39274918102.588
Test Loss of 33747419529.208420, Test MSE of 33747420367.848248
Epoch 41: training loss 36596752813.176
Test Loss of 28881617878.547306, Test MSE of 28881618336.616154
Epoch 42: training loss 35599436340.706
Test Loss of 31240693599.400417, Test MSE of 31240693837.898457
Epoch 43: training loss 33251301775.059
Test Loss of 28384768415.000694, Test MSE of 28384768996.197239
Epoch 44: training loss 32110798475.294
Test Loss of 28398215234.324310, Test MSE of 28398215645.027290
Epoch 45: training loss 30519230296.471
Test Loss of 28448823709.579460, Test MSE of 28448823618.678154
Epoch 46: training loss 29315447533.176
Test Loss of 30059121664.710617, Test MSE of 30059121300.033680
Epoch 47: training loss 28091271853.176
Test Loss of 24127621498.522324, Test MSE of 24127621570.279457
Epoch 48: training loss 26605107787.294
Test Loss of 23046767560.808697, Test MSE of 23046767019.067822
Epoch 49: training loss 26008359649.882
Test Loss of 25081220370.061531, Test MSE of 25081220647.951950
Epoch 50: training loss 24311594902.588
Test Loss of 24365894899.741844, Test MSE of 24365895134.999893
Epoch 51: training loss 23394492807.529
Test Loss of 23804054513.550774, Test MSE of 23804054812.721207
Epoch 52: training loss 22500778296.471
Test Loss of 21847714733.331482, Test MSE of 21847714754.398045
Epoch 53: training loss 22025718283.294
Test Loss of 22838931672.027760, Test MSE of 22838932270.501137
Epoch 54: training loss 21092208813.176
Test Loss of 22997416942.234558, Test MSE of 22997417189.075130
Epoch 55: training loss 20316239194.353
Test Loss of 20307452365.427711, Test MSE of 20307452377.108559
Epoch 56: training loss 19340602198.588
Test Loss of 20044607952.270184, Test MSE of 20044607698.732121
Epoch 57: training loss 19106213504.000
Test Loss of 20487392471.317142, Test MSE of 20487392364.454853
Epoch 58: training loss 18139674560.000
Test Loss of 20201757417.201019, Test MSE of 20201757707.161972
Epoch 59: training loss 18079011290.353
Test Loss of 21091508467.031227, Test MSE of 21091508700.657112
Epoch 60: training loss 17010048960.000
Test Loss of 20083409801.800602, Test MSE of 20083409721.189518
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20083409721.189518, 'MSE - std': 0.0, 'R2 - mean': 0.8436083094677176, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917833637.647
Test Loss of 424555556643.234802, Test MSE of 424555559378.174744
Epoch 2: training loss 427896468901.647
Test Loss of 424539061468.528320, Test MSE of 424539060232.231812
Epoch 3: training loss 427868368655.059
Test Loss of 424516934662.395569, Test MSE of 424516936423.027344
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886474661.647
Test Loss of 424522127492.174866, Test MSE of 424522126459.105591
Epoch 2: training loss 427876329351.529
Test Loss of 424522868899.442078, Test MSE of 424522864012.250732
Epoch 3: training loss 427875789161.412
Test Loss of 424521957739.836243, Test MSE of 424521961025.798523
Epoch 4: training loss 422348547011.765
Test Loss of 407311346820.411743, Test MSE of 407311346550.917236
Epoch 5: training loss 385385072278.588
Test Loss of 353140114878.504761, Test MSE of 353140115072.465942
Epoch 6: training loss 317231007744.000
Test Loss of 275701025444.981750, Test MSE of 275701023146.347534
Epoch 7: training loss 224513299456.000
Test Loss of 180703382218.881348, Test MSE of 180703385221.587830
Epoch 8: training loss 159055283440.941
Test Loss of 141807785784.553314, Test MSE of 141807790051.652222
Epoch 9: training loss 138728878290.824
Test Loss of 130999441909.696045, Test MSE of 130999443991.758759
Epoch 10: training loss 134876155000.471
Test Loss of 127204387718.484390, Test MSE of 127204387209.432846
Epoch 11: training loss 131328852540.235
Test Loss of 123834773199.855652, Test MSE of 123834772378.712540
Epoch 12: training loss 128122811060.706
Test Loss of 121896490149.100159, Test MSE of 121896492332.120392
Epoch 13: training loss 123527686384.941
Test Loss of 117524508367.855652, Test MSE of 117524508746.483871
Epoch 14: training loss 118933958656.000
Test Loss of 114250914731.436508, Test MSE of 114250913444.291641
Epoch 15: training loss 115414956483.765
Test Loss of 111309880527.263474, Test MSE of 111309880468.641479
Epoch 16: training loss 111274405586.824
Test Loss of 106976142363.714081, Test MSE of 106976143714.146484
Epoch 17: training loss 107885500897.882
Test Loss of 105167652593.491562, Test MSE of 105167654391.983200
Epoch 18: training loss 104684375250.824
Test Loss of 100419952587.888046, Test MSE of 100419953294.098740
Epoch 19: training loss 99951525074.824
Test Loss of 95985532521.526718, Test MSE of 95985531932.335251
Epoch 20: training loss 96608905697.882
Test Loss of 91559602383.026596, Test MSE of 91559600161.599777
Epoch 21: training loss 91796364950.588
Test Loss of 89649113161.430481, Test MSE of 89649114683.167450
Epoch 22: training loss 86732187331.765
Test Loss of 82305304310.229004, Test MSE of 82305305657.128387
Epoch 23: training loss 84645479168.000
Test Loss of 79058609504.703217, Test MSE of 79058610174.619415
Epoch 24: training loss 80492278166.588
Test Loss of 78113993410.827667, Test MSE of 78113993182.172012
Epoch 25: training loss 76486164570.353
Test Loss of 75744831633.439743, Test MSE of 75744833361.814835
Epoch 26: training loss 73027701338.353
Test Loss of 69244794127.219055, Test MSE of 69244793882.353088
Epoch 27: training loss 70377020732.235
Test Loss of 67946262455.990746, Test MSE of 67946261303.925400
Epoch 28: training loss 67364370944.000
Test Loss of 63980696122.389084, Test MSE of 63980696321.243858
Epoch 29: training loss 62724039393.882
Test Loss of 60464629666.672218, Test MSE of 60464629442.569427
Epoch 30: training loss 59934585208.471
Test Loss of 58310445189.122368, Test MSE of 58310444941.223221
Epoch 31: training loss 57587852694.588
Test Loss of 55716299551.207954, Test MSE of 55716298962.922096
Epoch 32: training loss 54159887314.824
Test Loss of 53005328531.808464, Test MSE of 53005328466.803535
Epoch 33: training loss 50968116570.353
Test Loss of 48166798884.359932, Test MSE of 48166798902.291161
Epoch 34: training loss 48702674228.706
Test Loss of 47388049137.965302, Test MSE of 47388048851.395325
Epoch 35: training loss 46120602970.353
Test Loss of 47564442974.808235, Test MSE of 47564443626.075394
Epoch 36: training loss 43220170337.882
Test Loss of 43876194788.641220, Test MSE of 43876194540.176674
Epoch 37: training loss 41316367186.824
Test Loss of 42367222065.565582, Test MSE of 42367222693.993385
Epoch 38: training loss 38291025889.882
Test Loss of 41059265688.782791, Test MSE of 41059265343.719406
Epoch 39: training loss 36621390599.529
Test Loss of 36292414477.501732, Test MSE of 36292415532.017220
Epoch 40: training loss 35015514857.412
Test Loss of 36168771126.835991, Test MSE of 36168770899.512375
Epoch 41: training loss 32681272259.765
Test Loss of 36605726363.269951, Test MSE of 36605726858.317284
Epoch 42: training loss 31341601347.765
Test Loss of 33904837720.827202, Test MSE of 33904837625.519112
Epoch 43: training loss 29557645808.941
Test Loss of 32200912941.479527, Test MSE of 32200912939.653252
Epoch 44: training loss 27990383751.529
Test Loss of 30456531452.802219, Test MSE of 30456531628.492115
Epoch 45: training loss 26564594612.706
Test Loss of 29511768813.464722, Test MSE of 29511768525.438911
Epoch 46: training loss 25846333345.882
Test Loss of 32004608340.622715, Test MSE of 32004608264.026726
Epoch 47: training loss 24574670863.059
Test Loss of 30213799087.522552, Test MSE of 30213799062.326687
Epoch 48: training loss 23693064922.353
Test Loss of 30055408820.733749, Test MSE of 30055409090.551376
Epoch 49: training loss 22233867433.412
Test Loss of 30230489927.476288, Test MSE of 30230489808.486561
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25156949764.83804, 'MSE - std': 5073540043.648521, 'R2 - mean': 0.8138913415575129, 'R2 - std': 0.029716967910204783} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005510 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926671661.176
Test Loss of 447257637245.838562, Test MSE of 447257638425.074341
Epoch 2: training loss 421905755075.765
Test Loss of 447239619211.873230, Test MSE of 447239612000.250549
Epoch 3: training loss 421878858330.353
Test Loss of 447215618815.940796, Test MSE of 447215623232.022339
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898011708.235
Test Loss of 447223299989.881104, Test MSE of 447223296937.194153
Epoch 2: training loss 421888090473.412
Test Loss of 447224469268.548706, Test MSE of 447224471400.402893
Epoch 3: training loss 421887587629.176
Test Loss of 447224650868.304443, Test MSE of 447224650017.845154
Epoch 4: training loss 416219944598.588
Test Loss of 428964018078.882263, Test MSE of 428964018659.634888
Epoch 5: training loss 378995176508.235
Test Loss of 372585310238.793457, Test MSE of 372585313255.246155
Epoch 6: training loss 310120319337.412
Test Loss of 294048289557.259338, Test MSE of 294048292319.591614
Epoch 7: training loss 218834882138.353
Test Loss of 194942379380.600494, Test MSE of 194942377862.957886
Epoch 8: training loss 156108792470.588
Test Loss of 154381446594.768433, Test MSE of 154381443796.811584
Epoch 9: training loss 136874222772.706
Test Loss of 142407402400.540375, Test MSE of 142407404105.680328
Epoch 10: training loss 131414691599.059
Test Loss of 136702657501.179733, Test MSE of 136702656063.583969
Epoch 11: training loss 128213865682.824
Test Loss of 133654262183.528107, Test MSE of 133654262506.279556
Epoch 12: training loss 126198026059.294
Test Loss of 130393503946.289154, Test MSE of 130393503939.774429
Epoch 13: training loss 122156485511.529
Test Loss of 126987382297.937546, Test MSE of 126987383212.695267
Epoch 14: training loss 118682755614.118
Test Loss of 123430615302.928528, Test MSE of 123430614010.022690
Epoch 15: training loss 112880027166.118
Test Loss of 119455558212.337723, Test MSE of 119455554639.497940
Epoch 16: training loss 110178405526.588
Test Loss of 115836045185.510056, Test MSE of 115836045857.518112
Epoch 17: training loss 107016980449.882
Test Loss of 111623849751.391159, Test MSE of 111623850818.097778
Epoch 18: training loss 102807561155.765
Test Loss of 108589530340.108261, Test MSE of 108589529071.955765
Epoch 19: training loss 98549300254.118
Test Loss of 103415783994.389084, Test MSE of 103415784085.499451
Epoch 20: training loss 94666739169.882
Test Loss of 99748812585.393478, Test MSE of 99748812109.014267
Epoch 21: training loss 92061719431.529
Test Loss of 98047417528.523712, Test MSE of 98047417191.535858
Epoch 22: training loss 87658099275.294
Test Loss of 95015330201.315750, Test MSE of 95015329442.089111
Epoch 23: training loss 83013109263.059
Test Loss of 89483156442.811005, Test MSE of 89483156174.163361
Epoch 24: training loss 80065278313.412
Test Loss of 86800499980.376587, Test MSE of 86800500681.823700
Epoch 25: training loss 76818633848.471
Test Loss of 82084908090.507523, Test MSE of 82084908179.355789
Epoch 26: training loss 72492428589.176
Test Loss of 80888915083.517929, Test MSE of 80888913801.341583
Epoch 27: training loss 69771523147.294
Test Loss of 72721032414.897064, Test MSE of 72721032377.521973
Epoch 28: training loss 66259992229.647
Test Loss of 71653579301.070557, Test MSE of 71653579601.654877
Epoch 29: training loss 63169409716.706
Test Loss of 65358950179.234787, Test MSE of 65358949770.317284
Epoch 30: training loss 60524944474.353
Test Loss of 61234520173.908859, Test MSE of 61234521456.633919
Epoch 31: training loss 57653009844.706
Test Loss of 61796065099.976868, Test MSE of 61796065389.723022
Epoch 32: training loss 54222919107.765
Test Loss of 56735627778.960907, Test MSE of 56735628526.651642
Epoch 33: training loss 51335547550.118
Test Loss of 57231806760.564423, Test MSE of 57231807959.131493
Epoch 34: training loss 49582219964.235
Test Loss of 51745401753.434189, Test MSE of 51745402193.419006
Epoch 35: training loss 46926295634.824
Test Loss of 53045452428.583855, Test MSE of 53045452534.840530
Epoch 36: training loss 44571728271.059
Test Loss of 47719146769.114044, Test MSE of 47719147307.808022
Epoch 37: training loss 41929198960.941
Test Loss of 43074706093.509140, Test MSE of 43074705735.290184
Epoch 38: training loss 39749842213.647
Test Loss of 44807054640.381218, Test MSE of 44807054407.043915
Epoch 39: training loss 38313289389.176
Test Loss of 43500642565.507286, Test MSE of 43500643725.297173
Epoch 40: training loss 35352012973.176
Test Loss of 38884210438.336342, Test MSE of 38884210250.053886
Epoch 41: training loss 34610176158.118
Test Loss of 39900418243.182976, Test MSE of 39900419427.517494
Epoch 42: training loss 32629080553.412
Test Loss of 36573913481.208420, Test MSE of 36573913300.227356
Epoch 43: training loss 30968089185.882
Test Loss of 28535232165.692345, Test MSE of 28535232959.102818
Epoch 44: training loss 29475186793.412
Test Loss of 31107529422.671291, Test MSE of 31107529240.955605
Epoch 45: training loss 28009179452.235
Test Loss of 30144022025.356464, Test MSE of 30144021909.549892
Epoch 46: training loss 27261534034.824
Test Loss of 30121298979.767754, Test MSE of 30121299049.342499
Epoch 47: training loss 25675947742.118
Test Loss of 28692113796.944714, Test MSE of 28692114140.438030
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26335337890.038036, 'MSE - std': 4465169223.769101, 'R2 - mean': 0.8122603909314727, 'R2 - std': 0.024373184467222486} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005332 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109260137.412
Test Loss of 410764041622.093445, Test MSE of 410764041770.404663
Epoch 2: training loss 430087682409.412
Test Loss of 410745149270.360046, Test MSE of 410745150588.799377
Epoch 3: training loss 430059500001.882
Test Loss of 410721079498.809814, Test MSE of 410721082100.240417
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430072833927.529
Test Loss of 410725206629.167969, Test MSE of 410725207823.829102
Epoch 2: training loss 430064198113.882
Test Loss of 410726323167.304016, Test MSE of 410726314996.844421
Epoch 3: training loss 430063728278.588
Test Loss of 410726894340.383179, Test MSE of 410726893713.133972
Epoch 4: training loss 424936437157.647
Test Loss of 394520001058.828308, Test MSE of 394519999996.868591
Epoch 5: training loss 389028192978.824
Test Loss of 340412241401.602966, Test MSE of 340412243794.505554
Epoch 6: training loss 320862494358.588
Test Loss of 263231501139.043030, Test MSE of 263231505020.121765
Epoch 7: training loss 229849765948.235
Test Loss of 164892725822.311890, Test MSE of 164892725011.818024
Epoch 8: training loss 163488773752.471
Test Loss of 125064164997.863953, Test MSE of 125064165015.703278
Epoch 9: training loss 144946855092.706
Test Loss of 114610666215.951874, Test MSE of 114610666544.381424
Epoch 10: training loss 138598786168.471
Test Loss of 110372910032.140671, Test MSE of 110372909204.773270
Epoch 11: training loss 133988039258.353
Test Loss of 107214926815.304031, Test MSE of 107214922753.301361
Epoch 12: training loss 133078395904.000
Test Loss of 104224343953.117996, Test MSE of 104224344159.610657
Epoch 13: training loss 129150064579.765
Test Loss of 101879833250.769089, Test MSE of 101879833951.152466
Epoch 14: training loss 124195231653.647
Test Loss of 98065659298.413696, Test MSE of 98065660504.509262
Epoch 15: training loss 120167535555.765
Test Loss of 95547194406.382233, Test MSE of 95547194330.469559
Epoch 16: training loss 116683355196.235
Test Loss of 92763552082.332260, Test MSE of 92763551966.491913
Epoch 17: training loss 113596152681.412
Test Loss of 89174947196.505325, Test MSE of 89174946765.498138
Epoch 18: training loss 109073183472.941
Test Loss of 85583931148.438690, Test MSE of 85583932394.050125
Epoch 19: training loss 105095669775.059
Test Loss of 82525940681.506714, Test MSE of 82525941121.852921
Epoch 20: training loss 101585413135.059
Test Loss of 79126248960.236923, Test MSE of 79126249621.235077
Epoch 21: training loss 96575316208.941
Test Loss of 77904267145.536331, Test MSE of 77904266884.458282
Epoch 22: training loss 93918003501.176
Test Loss of 73285398131.857468, Test MSE of 73285398389.901520
Epoch 23: training loss 89538285477.647
Test Loss of 71851529952.844055, Test MSE of 71851530352.115341
Epoch 24: training loss 85733639845.647
Test Loss of 69042070671.577972, Test MSE of 69042070717.118149
Epoch 25: training loss 82192786883.765
Test Loss of 64381248363.683479, Test MSE of 64381248342.017426
Epoch 26: training loss 78205097261.176
Test Loss of 60031641178.269318, Test MSE of 60031641858.905090
Epoch 27: training loss 75277765933.176
Test Loss of 58293767416.773712, Test MSE of 58293767758.634323
Epoch 28: training loss 71473068920.471
Test Loss of 55895802210.443314, Test MSE of 55895801721.091293
Epoch 29: training loss 67952654336.000
Test Loss of 55696195278.837578, Test MSE of 55696194455.930054
Epoch 30: training loss 65215930774.588
Test Loss of 51148610511.192963, Test MSE of 51148611319.607582
Epoch 31: training loss 61447726004.706
Test Loss of 51018462042.624710, Test MSE of 51018461964.364227
Epoch 32: training loss 59515789071.059
Test Loss of 48295707124.390556, Test MSE of 48295706012.746941
Epoch 33: training loss 56775177818.353
Test Loss of 46018114161.488197, Test MSE of 46018113944.409721
Epoch 34: training loss 53689782347.294
Test Loss of 43263392867.035629, Test MSE of 43263393165.075775
Epoch 35: training loss 51702614964.706
Test Loss of 44079256461.801018, Test MSE of 44079255565.829865
Epoch 36: training loss 47926488673.882
Test Loss of 38191760512.414619, Test MSE of 38191761074.108498
Epoch 37: training loss 45166061560.471
Test Loss of 36863252312.255440, Test MSE of 36863252941.160156
Epoch 38: training loss 43845604269.176
Test Loss of 34352974355.664970, Test MSE of 34352974243.652779
Epoch 39: training loss 42010842443.294
Test Loss of 33563833383.329941, Test MSE of 33563833139.779762
Epoch 40: training loss 39360632899.765
Test Loss of 35480316585.403053, Test MSE of 35480316592.678719
Epoch 41: training loss 37276948412.235
Test Loss of 31056067266.043499, Test MSE of 31056066957.218796
Epoch 42: training loss 36089068408.471
Test Loss of 30612982488.788525, Test MSE of 30612983121.561005
Epoch 43: training loss 33779266620.235
Test Loss of 28588337807.814899, Test MSE of 28588337727.771233
Epoch 44: training loss 32516786055.529
Test Loss of 27994195744.340584, Test MSE of 27994195736.299820
Epoch 45: training loss 31373200557.176
Test Loss of 26938112380.031467, Test MSE of 26938112086.661312
Epoch 46: training loss 30066098808.471
Test Loss of 27119699182.348911, Test MSE of 27119698769.929585
Epoch 47: training loss 28502781680.941
Test Loss of 22966500479.466915, Test MSE of 22966500560.910595
Epoch 48: training loss 27269828720.941
Test Loss of 24305539362.946785, Test MSE of 24305539658.418648
Epoch 49: training loss 26097090785.882
Test Loss of 24168097250.857937, Test MSE of 24168097494.921013
Epoch 50: training loss 24681037153.882
Test Loss of 22961152225.554836, Test MSE of 22961152138.147774
Epoch 51: training loss 23727168609.882
Test Loss of 20833997858.591393, Test MSE of 20833997560.047722
Epoch 52: training loss 23170547520.000
Test Loss of 21560770878.430355, Test MSE of 21560770825.692863
Epoch 53: training loss 21826797161.412
Test Loss of 21141396939.165203, Test MSE of 21141396924.787151
Epoch 54: training loss 21352811896.471
Test Loss of 19619258755.613144, Test MSE of 19619259457.078438
Epoch 55: training loss 20735597639.529
Test Loss of 20331918763.890793, Test MSE of 20331919218.667881
Epoch 56: training loss 19780241566.118
Test Loss of 20634727579.898197, Test MSE of 20634727390.086868
Epoch 57: training loss 19354523617.882
Test Loss of 19812650894.274872, Test MSE of 19812650835.515690
Epoch 58: training loss 18383782268.235
Test Loss of 20129070666.158260, Test MSE of 20129070338.297886
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24783771002.102997, 'MSE - std': 4709074385.787542, 'R2 - mean': 0.8176613662021979, 'R2 - std': 0.023087890601566387} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005363 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043772385.882
Test Loss of 431613161769.580750, Test MSE of 431613156044.726746
Epoch 2: training loss 424023601392.941
Test Loss of 431592825093.567810, Test MSE of 431592825026.149902
Epoch 3: training loss 423995855088.941
Test Loss of 431565363113.758423, Test MSE of 431565373807.117065
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010543344.941
Test Loss of 431566971247.711243, Test MSE of 431566974438.121338
Epoch 2: training loss 423999364999.529
Test Loss of 431568821115.794556, Test MSE of 431568812866.319519
Epoch 3: training loss 423998781440.000
Test Loss of 431567921853.304932, Test MSE of 431567922009.832581
Epoch 4: training loss 418170330895.059
Test Loss of 412806630883.805664, Test MSE of 412806629506.828003
Epoch 5: training loss 380767741108.706
Test Loss of 356604231111.374390, Test MSE of 356604226184.101685
Epoch 6: training loss 312384388517.647
Test Loss of 277727939726.630249, Test MSE of 277727943036.227295
Epoch 7: training loss 222394460521.412
Test Loss of 178212060554.720947, Test MSE of 178212059576.691925
Epoch 8: training loss 159589510927.059
Test Loss of 137915559532.275787, Test MSE of 137915558992.869904
Epoch 9: training loss 140773958475.294
Test Loss of 125301384324.205460, Test MSE of 125301382922.268799
Epoch 10: training loss 135436320105.412
Test Loss of 121667481526.078674, Test MSE of 121667485963.341095
Epoch 11: training loss 133127236306.824
Test Loss of 118346814988.557144, Test MSE of 118346815241.900101
Epoch 12: training loss 127854791243.294
Test Loss of 115680763813.019897, Test MSE of 115680763031.485474
Epoch 13: training loss 124880646565.647
Test Loss of 112146841296.732986, Test MSE of 112146844196.866196
Epoch 14: training loss 121524854663.529
Test Loss of 108071076825.617767, Test MSE of 108071078991.289246
Epoch 15: training loss 118314887198.118
Test Loss of 105419248743.774185, Test MSE of 105419248873.310791
Epoch 16: training loss 114203433803.294
Test Loss of 101984038443.831558, Test MSE of 101984038446.435074
Epoch 17: training loss 108914777148.235
Test Loss of 99049851076.175842, Test MSE of 99049851699.289810
Epoch 18: training loss 105567017622.588
Test Loss of 93827928513.214249, Test MSE of 93827927853.380386
Epoch 19: training loss 102593932288.000
Test Loss of 91671649454.852386, Test MSE of 91671650316.110107
Epoch 20: training loss 98042208060.235
Test Loss of 85701368388.945862, Test MSE of 85701369192.884155
Epoch 21: training loss 94166415284.706
Test Loss of 82177539940.575653, Test MSE of 82177540349.680252
Epoch 22: training loss 91095490921.412
Test Loss of 77311569711.503937, Test MSE of 77311570336.564346
Epoch 23: training loss 87122626966.588
Test Loss of 76717123969.717728, Test MSE of 76717123687.269714
Epoch 24: training loss 83078735420.235
Test Loss of 70768136847.814896, Test MSE of 70768136709.524124
Epoch 25: training loss 80124984681.412
Test Loss of 69015672906.869049, Test MSE of 69015674142.334244
Epoch 26: training loss 76713433630.118
Test Loss of 66074252967.981491, Test MSE of 66074254105.337181
Epoch 27: training loss 72946507218.824
Test Loss of 62609101939.146690, Test MSE of 62609102688.783676
Epoch 28: training loss 69639244739.765
Test Loss of 59979082877.571495, Test MSE of 59979083113.165749
Epoch 29: training loss 66794962951.529
Test Loss of 56344094175.067101, Test MSE of 56344094589.871964
Epoch 30: training loss 62660282684.235
Test Loss of 57856634019.479874, Test MSE of 57856634771.326195
Epoch 31: training loss 60305621835.294
Test Loss of 49365333279.629799, Test MSE of 49365334582.906364
Epoch 32: training loss 57286151544.471
Test Loss of 47078914342.737625, Test MSE of 47078914377.042648
Epoch 33: training loss 54585515474.824
Test Loss of 46567148794.669136, Test MSE of 46567148553.385887
Epoch 34: training loss 52071538936.471
Test Loss of 44434239071.007866, Test MSE of 44434238395.508400
Epoch 35: training loss 49786231024.941
Test Loss of 41146396944.940308, Test MSE of 41146397347.934486
Epoch 36: training loss 46698663439.059
Test Loss of 39054088465.888016, Test MSE of 39054089527.912544
Epoch 37: training loss 44995648030.118
Test Loss of 36837179285.856544, Test MSE of 36837178180.909813
Epoch 38: training loss 42297178729.412
Test Loss of 36330962416.125870, Test MSE of 36330962840.649193
Epoch 39: training loss 40794766441.412
Test Loss of 34396329267.531700, Test MSE of 34396328497.600601
Epoch 40: training loss 38587978428.235
Test Loss of 33280612596.509022, Test MSE of 33280612301.146980
Epoch 41: training loss 36343915632.941
Test Loss of 32478145855.851921, Test MSE of 32478146135.699547
Epoch 42: training loss 34937481426.824
Test Loss of 27454055361.925034, Test MSE of 27454054492.167465
Epoch 43: training loss 33177146315.294
Test Loss of 30109198399.496529, Test MSE of 30109197983.761852
Epoch 44: training loss 31680632545.882
Test Loss of 27473154163.620544, Test MSE of 27473153864.794777
Epoch 45: training loss 30436977423.059
Test Loss of 27340747196.475704, Test MSE of 27340746927.990128
Epoch 46: training loss 29302102460.235
Test Loss of 23735358491.957428, Test MSE of 23735358685.800392
Epoch 47: training loss 27930473886.118
Test Loss of 23726255417.217957, Test MSE of 23726254907.091415
Epoch 48: training loss 26810991348.706
Test Loss of 26099603330.902359, Test MSE of 26099602873.782631
Epoch 49: training loss 25984432399.059
Test Loss of 25494961175.218880, Test MSE of 25494961083.176647
Epoch 50: training loss 24512942181.647
Test Loss of 23531292976.214714, Test MSE of 23531293053.270592
Epoch 51: training loss 23680378322.824
Test Loss of 21468916497.651089, Test MSE of 21468916716.318005
Epoch 52: training loss 22655850454.588
Test Loss of 24071390131.709393, Test MSE of 24071390234.294785
Epoch 53: training loss 21912376602.353
Test Loss of 22644824471.515038, Test MSE of 22644824630.635204
Epoch 54: training loss 20818905776.941
Test Loss of 22143521346.102730, Test MSE of 22143521602.186222
Epoch 55: training loss 20610979452.235
Test Loss of 22890835728.229523, Test MSE of 22890835595.297852
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24405183920.741966, 'MSE - std': 4279441315.101553, 'R2 - mean': 0.8199391709419219, 'R2 - std': 0.021146965070032057} 
 

Saving model.....
Results After CV: {'MSE - mean': 24405183920.741966, 'MSE - std': 4279441315.101553, 'R2 - mean': 0.8199391709419219, 'R2 - std': 0.021146965070032057}
Train time: 83.01335586620007
Inference time: 0.07242213319987059
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 32 finished with value: 24405183920.741966 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005791 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524867132.235
Test Loss of 418110509227.495728, Test MSE of 418110506557.453979
Epoch 2: training loss 427504292803.765
Test Loss of 418092069658.944275, Test MSE of 418092073937.867249
Epoch 3: training loss 427476470844.235
Test Loss of 418067886905.263916, Test MSE of 418067889471.130249
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427489748751.059
Test Loss of 418075505438.734192, Test MSE of 418075507585.406189
Epoch 2: training loss 427480606117.647
Test Loss of 418075265041.528564, Test MSE of 418075265987.647034
Epoch 3: training loss 423610892167.529
Test Loss of 406072953020.076782, Test MSE of 406072954693.367615
Epoch 4: training loss 396890119710.118
Test Loss of 364518861918.512146, Test MSE of 364518868401.344788
Epoch 5: training loss 330580723109.647
Test Loss of 277832395794.712952, Test MSE of 277832392188.664246
Epoch 6: training loss 247388124521.412
Test Loss of 200877354783.444824, Test MSE of 200877353377.075653
Epoch 7: training loss 178986258040.471
Test Loss of 141137946427.395782, Test MSE of 141137950090.240509
Epoch 8: training loss 146696673159.529
Test Loss of 122690724304.270187, Test MSE of 122690729094.361664
Epoch 9: training loss 137421268510.118
Test Loss of 116605650667.806610, Test MSE of 116605649345.070068
Epoch 10: training loss 133874078418.824
Test Loss of 114043670259.860275, Test MSE of 114043667880.511002
Epoch 11: training loss 130536980118.588
Test Loss of 110599490180.767059, Test MSE of 110599489630.140167
Epoch 12: training loss 127612627727.059
Test Loss of 107799377330.187363, Test MSE of 107799379714.279800
Epoch 13: training loss 123304531937.882
Test Loss of 103525951903.474442, Test MSE of 103525949804.417328
Epoch 14: training loss 118826872048.941
Test Loss of 101284083213.146423, Test MSE of 101284081759.312195
Epoch 15: training loss 115456150543.059
Test Loss of 98540796388.641220, Test MSE of 98540796877.355591
Epoch 16: training loss 112228624820.706
Test Loss of 93721345083.455002, Test MSE of 93721344780.442719
Epoch 17: training loss 107895875282.824
Test Loss of 89187794534.684250, Test MSE of 89187793293.213715
Epoch 18: training loss 103807181839.059
Test Loss of 88558606629.248215, Test MSE of 88558607991.008194
Epoch 19: training loss 100551458838.588
Test Loss of 84271424230.832291, Test MSE of 84271424751.106720
Epoch 20: training loss 97227650816.000
Test Loss of 81825647270.876709, Test MSE of 81825647932.094925
Epoch 21: training loss 93071289374.118
Test Loss of 78819642477.671982, Test MSE of 78819642563.367371
Epoch 22: training loss 89519475501.176
Test Loss of 76171784431.715012, Test MSE of 76171785773.893738
Epoch 23: training loss 85590951168.000
Test Loss of 71305306360.242432, Test MSE of 71305305181.292328
Epoch 24: training loss 82503128274.824
Test Loss of 70668405716.178574, Test MSE of 70668404525.449692
Epoch 25: training loss 78166677436.235
Test Loss of 65497295109.981033, Test MSE of 65497294964.180359
Epoch 26: training loss 76125768975.059
Test Loss of 66682874974.512146, Test MSE of 66682874497.973541
Epoch 27: training loss 71306274620.235
Test Loss of 60375046973.527641, Test MSE of 60375046710.659767
Epoch 28: training loss 69075392858.353
Test Loss of 58286368258.724037, Test MSE of 58286368195.088661
Epoch 29: training loss 65427865027.765
Test Loss of 54589386850.538979, Test MSE of 54589387211.264404
Epoch 30: training loss 63102003553.882
Test Loss of 54619573803.939857, Test MSE of 54619572952.044876
Epoch 31: training loss 60262188717.176
Test Loss of 52512684425.682167, Test MSE of 52512685418.313721
Epoch 32: training loss 57245891064.471
Test Loss of 46859104137.563728, Test MSE of 46859103753.795174
Epoch 33: training loss 55174226627.765
Test Loss of 46364186001.025215, Test MSE of 46364185696.253242
Epoch 34: training loss 52489806908.235
Test Loss of 43507983416.138794, Test MSE of 43507983967.710487
Epoch 35: training loss 49847341507.765
Test Loss of 41715024190.356697, Test MSE of 41715024392.698517
Epoch 36: training loss 48347150400.000
Test Loss of 38957710274.176270, Test MSE of 38957710782.249695
Epoch 37: training loss 45625277967.059
Test Loss of 38078945908.659729, Test MSE of 38078945863.911812
Epoch 38: training loss 43265222226.824
Test Loss of 35128672988.409897, Test MSE of 35128673401.454750
Epoch 39: training loss 41687977366.588
Test Loss of 33726512220.617165, Test MSE of 33726512439.734776
Epoch 40: training loss 40149895966.118
Test Loss of 33432921832.016655, Test MSE of 33432921927.675343
Epoch 41: training loss 38214311604.706
Test Loss of 32456917728.199860, Test MSE of 32456918070.243782
Epoch 42: training loss 35963828939.294
Test Loss of 29212332403.416145, Test MSE of 29212332431.177238
Epoch 43: training loss 34624799495.529
Test Loss of 28861012331.362480, Test MSE of 28861013063.265945
Epoch 44: training loss 32827095356.235
Test Loss of 29736499338.807308, Test MSE of 29736499747.666351
Epoch 45: training loss 31357721472.000
Test Loss of 26496475162.529724, Test MSE of 26496475477.570110
Epoch 46: training loss 30053769306.353
Test Loss of 26197080118.480686, Test MSE of 26197080562.310410
Epoch 47: training loss 29162350458.353
Test Loss of 25010684370.165165, Test MSE of 25010683966.567986
Epoch 48: training loss 27593947798.588
Test Loss of 22352809160.157299, Test MSE of 22352809726.386589
Epoch 49: training loss 26177498209.882
Test Loss of 22880728850.653713, Test MSE of 22880729025.859604
Epoch 50: training loss 25542817020.235
Test Loss of 24382839417.870922, Test MSE of 24382839495.850746
Epoch 51: training loss 24285479706.353
Test Loss of 24396531425.857967, Test MSE of 24396531134.077332
Epoch 52: training loss 23513986861.176
Test Loss of 21321583037.320381, Test MSE of 21321582539.973305
Epoch 53: training loss 22257198757.647
Test Loss of 21563569304.782791, Test MSE of 21563569400.850765
Epoch 54: training loss 21981893888.000
Test Loss of 21680989411.634514, Test MSE of 21680989889.053417
Epoch 55: training loss 20830750923.294
Test Loss of 20257700668.580154, Test MSE of 20257701028.461926
Epoch 56: training loss 19954469948.235
Test Loss of 18762550694.343742, Test MSE of 18762550581.908165
Epoch 57: training loss 19710888184.471
Test Loss of 20931394662.092064, Test MSE of 20931394673.218464
Epoch 58: training loss 18999312594.824
Test Loss of 17718873733.003933, Test MSE of 17718873553.269524
Epoch 59: training loss 18249025253.647
Test Loss of 19223877985.413834, Test MSE of 19223877954.431305
Epoch 60: training loss 17924119672.471
Test Loss of 17796331419.092297, Test MSE of 17796331498.510147
Epoch 61: training loss 16779293465.412
Test Loss of 20106972815.426323, Test MSE of 20106972698.528843
Epoch 62: training loss 16440531649.882
Test Loss of 17848576713.933842, Test MSE of 17848577103.401283
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17848577103.401283, 'MSE - std': 0.0, 'R2 - mean': 0.8610111935399292, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005532 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917858213.647
Test Loss of 424555468147.653015, Test MSE of 424555482391.404602
Epoch 2: training loss 427897449050.353
Test Loss of 424539545370.707397, Test MSE of 424539547757.985107
Epoch 3: training loss 427870058375.529
Test Loss of 424517788457.156616, Test MSE of 424517789111.100281
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888236664.471
Test Loss of 424525079659.303284, Test MSE of 424525083495.752686
Epoch 2: training loss 427877208064.000
Test Loss of 424525668171.976868, Test MSE of 424525660947.815674
Epoch 3: training loss 424181442198.588
Test Loss of 413022117423.729797, Test MSE of 413022119665.519897
Epoch 4: training loss 397691503073.882
Test Loss of 372392676224.799438, Test MSE of 372392680083.213623
Epoch 5: training loss 331498128444.235
Test Loss of 286451951991.679871, Test MSE of 286451955691.732239
Epoch 6: training loss 247906816000.000
Test Loss of 210577308881.158447, Test MSE of 210577310864.819916
Epoch 7: training loss 177801989722.353
Test Loss of 152412495301.610931, Test MSE of 152412495928.199921
Epoch 8: training loss 144306405225.412
Test Loss of 133758817892.315521, Test MSE of 133758821762.496155
Epoch 9: training loss 134575098729.412
Test Loss of 128945473869.990280, Test MSE of 128945472324.185318
Epoch 10: training loss 132063910219.294
Test Loss of 125675042402.420547, Test MSE of 125675043215.329498
Epoch 11: training loss 127928041291.294
Test Loss of 122509245380.071243, Test MSE of 122509245595.409988
Epoch 12: training loss 125585672583.529
Test Loss of 119479154988.828125, Test MSE of 119479154763.479385
Epoch 13: training loss 121737759593.412
Test Loss of 116397860002.257690, Test MSE of 116397860352.060013
Epoch 14: training loss 117837888421.647
Test Loss of 113478860807.579926, Test MSE of 113478860562.765503
Epoch 15: training loss 113049409837.176
Test Loss of 109571262993.173264, Test MSE of 109571264835.285797
Epoch 16: training loss 109882095465.412
Test Loss of 106553152508.446915, Test MSE of 106553151348.296173
Epoch 17: training loss 106765928568.471
Test Loss of 102074847826.076340, Test MSE of 102074847619.814255
Epoch 18: training loss 101878522096.941
Test Loss of 97597187639.783478, Test MSE of 97597187708.174622
Epoch 19: training loss 98057812239.059
Test Loss of 92848992298.637054, Test MSE of 92848992410.155685
Epoch 20: training loss 94717918177.882
Test Loss of 90108509331.571594, Test MSE of 90108509089.874634
Epoch 21: training loss 91356400609.882
Test Loss of 86284131305.023361, Test MSE of 86284134010.799759
Epoch 22: training loss 86998843632.941
Test Loss of 84328485117.927368, Test MSE of 84328484920.409439
Epoch 23: training loss 82990967371.294
Test Loss of 81582974793.608139, Test MSE of 81582975259.898743
Epoch 24: training loss 79702425705.412
Test Loss of 78742529894.980331, Test MSE of 78742531186.244949
Epoch 25: training loss 76681902652.235
Test Loss of 75987199594.000458, Test MSE of 75987198887.765732
Epoch 26: training loss 74021847160.471
Test Loss of 71430943130.263245, Test MSE of 71430944016.546524
Epoch 27: training loss 70060990373.647
Test Loss of 66398061623.191299, Test MSE of 66398060463.659233
Epoch 28: training loss 66270731233.882
Test Loss of 63244298578.727737, Test MSE of 63244298506.667068
Epoch 29: training loss 63067041475.765
Test Loss of 60975768164.078651, Test MSE of 60975768895.871864
Epoch 30: training loss 60806511841.882
Test Loss of 58423955875.738144, Test MSE of 58423957721.586311
Epoch 31: training loss 57542520907.294
Test Loss of 58395987928.205414, Test MSE of 58395988403.277428
Epoch 32: training loss 55255667049.412
Test Loss of 55861825626.959053, Test MSE of 55861826556.197952
Epoch 33: training loss 52674907437.176
Test Loss of 52512726251.688179, Test MSE of 52512725522.180878
Epoch 34: training loss 50218053760.000
Test Loss of 51474316298.659264, Test MSE of 51474315061.503761
Epoch 35: training loss 48049864990.118
Test Loss of 45631256900.989128, Test MSE of 45631256547.024803
Epoch 36: training loss 45885188193.882
Test Loss of 46465801316.197083, Test MSE of 46465802216.363464
Epoch 37: training loss 43799633980.235
Test Loss of 42698905252.744850, Test MSE of 42698904628.945633
Epoch 38: training loss 41950824651.294
Test Loss of 41260982372.907700, Test MSE of 41260982684.265137
Epoch 39: training loss 39173677869.176
Test Loss of 40495045792.836456, Test MSE of 40495045179.831367
Epoch 40: training loss 37034043715.765
Test Loss of 38006976407.302338, Test MSE of 38006976056.528076
Epoch 41: training loss 35276452525.176
Test Loss of 37248185824.140640, Test MSE of 37248185758.875481
Epoch 42: training loss 33606070113.882
Test Loss of 37819634554.167015, Test MSE of 37819632800.832710
Epoch 43: training loss 31883766174.118
Test Loss of 35480974560.318298, Test MSE of 35480974855.301285
Epoch 44: training loss 30391540487.529
Test Loss of 34839461801.778397, Test MSE of 34839462855.779205
Epoch 45: training loss 29081863499.294
Test Loss of 37319539834.463104, Test MSE of 37319540025.404396
Epoch 46: training loss 27976703503.059
Test Loss of 33064646273.687717, Test MSE of 33064647571.373787
Epoch 47: training loss 26382456790.588
Test Loss of 31554795746.923897, Test MSE of 31554796485.740242
Epoch 48: training loss 25405365025.882
Test Loss of 31349663126.710155, Test MSE of 31349662740.328518
Epoch 49: training loss 23478346691.765
Test Loss of 32501337439.281979, Test MSE of 32501336819.584328
Epoch 50: training loss 22968005022.118
Test Loss of 29316665939.023827, Test MSE of 29316667103.456509
Epoch 51: training loss 21862285270.588
Test Loss of 29413473982.563961, Test MSE of 29413473769.706238
Epoch 52: training loss 20944004468.706
Test Loss of 28618832326.558407, Test MSE of 28618831990.239582
Epoch 53: training loss 20007672169.412
Test Loss of 29299617551.337498, Test MSE of 29299617920.471703
Epoch 54: training loss 19438231367.529
Test Loss of 30509618186.659264, Test MSE of 30509617951.887115
Epoch 55: training loss 18205610138.353
Test Loss of 31453973097.289845, Test MSE of 31453972653.240334
Epoch 56: training loss 18082046475.294
Test Loss of 25967834771.690029, Test MSE of 25967835005.369156
Epoch 57: training loss 17350153607.529
Test Loss of 25727526443.466110, Test MSE of 25727526993.747780
Epoch 58: training loss 16651781165.176
Test Loss of 27261595580.017582, Test MSE of 27261595664.161499
Epoch 59: training loss 16015699049.412
Test Loss of 25131334113.325005, Test MSE of 25131333793.964771
Epoch 60: training loss 15312522104.471
Test Loss of 26590090817.495258, Test MSE of 26590091050.633083
Epoch 61: training loss 14895650981.647
Test Loss of 26452449956.034237, Test MSE of 26452449960.871311
Epoch 62: training loss 14380087386.353
Test Loss of 26577355107.545685, Test MSE of 26577354962.057781
Epoch 63: training loss 14106563279.059
Test Loss of 27130846922.170715, Test MSE of 27130847114.451839
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22489712108.92656, 'MSE - std': 4641135005.525278, 'R2 - mean': 0.8336574790409578, 'R2 - std': 0.027353714498971393} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005478 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927496764.235
Test Loss of 447260622218.866516, Test MSE of 447260622878.463745
Epoch 2: training loss 421906076852.706
Test Loss of 447240748923.351379, Test MSE of 447240751000.521912
Epoch 3: training loss 421878907723.294
Test Loss of 447215708880.566284, Test MSE of 447215701598.235107
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899047514.353
Test Loss of 447224042049.021484, Test MSE of 447224048741.272156
Epoch 2: training loss 421887931813.647
Test Loss of 447224505697.650696, Test MSE of 447224504358.283691
Epoch 3: training loss 418109446625.882
Test Loss of 435269352028.024963, Test MSE of 435269365704.289612
Epoch 4: training loss 391615204291.765
Test Loss of 393349193642.725891, Test MSE of 393349198771.503113
Epoch 5: training loss 325552667105.882
Test Loss of 305898607237.240784, Test MSE of 305898609579.421326
Epoch 6: training loss 243539369743.059
Test Loss of 226444950047.385620, Test MSE of 226444953980.077087
Epoch 7: training loss 175784235008.000
Test Loss of 165714321182.497345, Test MSE of 165714322974.204315
Epoch 8: training loss 141672452035.765
Test Loss of 145443255169.273193, Test MSE of 145443255045.859955
Epoch 9: training loss 132708297577.412
Test Loss of 138797515938.257690, Test MSE of 138797514506.677643
Epoch 10: training loss 130024875188.706
Test Loss of 135976120576.769836, Test MSE of 135976121491.534637
Epoch 11: training loss 127147714258.824
Test Loss of 132254069231.537354, Test MSE of 132254070397.575470
Epoch 12: training loss 121942662806.588
Test Loss of 129653724925.808929, Test MSE of 129653727480.708740
Epoch 13: training loss 120189811290.353
Test Loss of 126106519349.710846, Test MSE of 126106520698.631302
Epoch 14: training loss 116501103585.882
Test Loss of 122704142531.182968, Test MSE of 122704146013.107071
Epoch 15: training loss 111621152015.059
Test Loss of 120230336670.704605, Test MSE of 120230337989.901443
Epoch 16: training loss 108506142629.647
Test Loss of 115764621045.755264, Test MSE of 115764621579.547943
Epoch 17: training loss 105163034518.588
Test Loss of 112350865753.360168, Test MSE of 112350866614.621933
Epoch 18: training loss 101347123410.824
Test Loss of 109225109678.811935, Test MSE of 109225110196.232391
Epoch 19: training loss 97367497637.647
Test Loss of 104420151243.888046, Test MSE of 104420150204.715393
Epoch 20: training loss 93421524736.000
Test Loss of 100920441286.321533, Test MSE of 100920440632.089233
Epoch 21: training loss 90817160312.471
Test Loss of 98167260816.847565, Test MSE of 98167261476.497711
Epoch 22: training loss 87249299425.882
Test Loss of 94963213739.554932, Test MSE of 94963212924.568497
Epoch 23: training loss 82192050236.235
Test Loss of 90454915427.782562, Test MSE of 90454916377.196045
Epoch 24: training loss 79911737298.824
Test Loss of 86400446925.427719, Test MSE of 86400448848.651779
Epoch 25: training loss 75695350347.294
Test Loss of 78829615700.445068, Test MSE of 78829614778.369965
Epoch 26: training loss 72617829150.118
Test Loss of 78681604018.305801, Test MSE of 78681603428.133102
Epoch 27: training loss 68345830038.588
Test Loss of 75491030840.553314, Test MSE of 75491031310.552628
Epoch 28: training loss 65948544421.647
Test Loss of 70124455511.998154, Test MSE of 70124455760.778915
Epoch 29: training loss 62933756897.882
Test Loss of 71092698775.953735, Test MSE of 71092699169.436493
Epoch 30: training loss 60658140611.765
Test Loss of 66299712146.505669, Test MSE of 66299712248.680023
Epoch 31: training loss 58155415838.118
Test Loss of 62377150868.104553, Test MSE of 62377150432.238388
Epoch 32: training loss 55947191296.000
Test Loss of 59396847164.520935, Test MSE of 59396846574.119476
Epoch 33: training loss 52745184941.176
Test Loss of 54983057644.872543, Test MSE of 54983057078.124809
Epoch 34: training loss 50812258951.529
Test Loss of 53307569804.110107, Test MSE of 53307569420.115677
Epoch 35: training loss 48094384112.941
Test Loss of 52130695420.506126, Test MSE of 52130695086.566826
Epoch 36: training loss 45828920756.706
Test Loss of 51515522444.761505, Test MSE of 51515522191.962135
Epoch 37: training loss 43830700400.941
Test Loss of 48765524316.202637, Test MSE of 48765525702.507538
Epoch 38: training loss 41179786202.353
Test Loss of 45202036479.703911, Test MSE of 45202036413.342560
Epoch 39: training loss 39720901398.588
Test Loss of 44761176709.714554, Test MSE of 44761177924.836151
Epoch 40: training loss 38024305287.529
Test Loss of 42579144407.435577, Test MSE of 42579144127.939682
Epoch 41: training loss 36108909583.059
Test Loss of 44987268452.966919, Test MSE of 44987268656.198280
Epoch 42: training loss 34686564178.824
Test Loss of 38869563732.622719, Test MSE of 38869564355.774651
Epoch 43: training loss 33014737920.000
Test Loss of 38137265535.496643, Test MSE of 38137265959.880112
Epoch 44: training loss 31242128075.294
Test Loss of 36105327396.419151, Test MSE of 36105328417.739792
Epoch 45: training loss 30156932464.941
Test Loss of 36506177603.271805, Test MSE of 36506177563.729195
Epoch 46: training loss 28504558147.765
Test Loss of 33440322944.917881, Test MSE of 33440322452.695538
Epoch 47: training loss 27451954319.059
Test Loss of 33232354651.492020, Test MSE of 33232354629.348541
Epoch 48: training loss 26392375209.412
Test Loss of 32987065126.787880, Test MSE of 32987065646.695133
Epoch 49: training loss 25152897547.294
Test Loss of 30150047300.574600, Test MSE of 30150047088.758854
Epoch 50: training loss 23942896512.000
Test Loss of 28352797077.052048, Test MSE of 28352797261.819538
Epoch 51: training loss 23273945287.529
Test Loss of 27793632148.696739, Test MSE of 27793631904.051651
Epoch 52: training loss 22221135480.471
Test Loss of 31826087387.403191, Test MSE of 31826088253.219059
Epoch 53: training loss 21580337325.176
Test Loss of 30106471280.218369, Test MSE of 30106470865.130985
Epoch 54: training loss 20569242590.118
Test Loss of 25848964609.776543, Test MSE of 25848964908.720425
Epoch 55: training loss 20094258706.824
Test Loss of 27284602861.523941, Test MSE of 27284602878.777470
Epoch 56: training loss 18894102215.529
Test Loss of 26603259806.882259, Test MSE of 26603259838.224602
Epoch 57: training loss 18678612201.412
Test Loss of 25396161770.740688, Test MSE of 25396161988.310268
Epoch 58: training loss 18001325782.588
Test Loss of 27611268248.782791, Test MSE of 27611268218.137882
Epoch 59: training loss 17613608922.353
Test Loss of 25791554117.285217, Test MSE of 25791554741.550900
Epoch 60: training loss 16807983830.588
Test Loss of 24393905273.515614, Test MSE of 24393904908.220303
Epoch 61: training loss 16498915858.824
Test Loss of 23446865688.812397, Test MSE of 23446865221.049957
Epoch 62: training loss 15826720222.118
Test Loss of 24920505141.947723, Test MSE of 24920505534.744881
Epoch 63: training loss 15451829782.588
Test Loss of 25078829409.887577, Test MSE of 25078829683.360226
Epoch 64: training loss 14770045812.706
Test Loss of 27564613611.628960, Test MSE of 27564613413.786808
Epoch 65: training loss 14769654196.706
Test Loss of 23554347904.325699, Test MSE of 23554348222.598763
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22844590813.48396, 'MSE - std': 3822560295.9584684, 'R2 - mean': 0.8368384054712538, 'R2 - std': 0.022782750430330748} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005370 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111060208.941
Test Loss of 410765614440.603455, Test MSE of 410765610775.638977
Epoch 2: training loss 430091248579.765
Test Loss of 410748704969.862122, Test MSE of 410748705935.189209
Epoch 3: training loss 430064608195.765
Test Loss of 410725938078.859802, Test MSE of 410725944702.528198
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430081278192.941
Test Loss of 410729188919.204102, Test MSE of 410729191918.453857
Epoch 2: training loss 430068828280.471
Test Loss of 410729225735.818604, Test MSE of 410729221486.088257
Epoch 3: training loss 426472881453.176
Test Loss of 398912904962.013855, Test MSE of 398912911757.111877
Epoch 4: training loss 400300314503.529
Test Loss of 358339766365.823242, Test MSE of 358339765850.580994
Epoch 5: training loss 334770814735.059
Test Loss of 271239029369.069885, Test MSE of 271239029162.735535
Epoch 6: training loss 251832184711.529
Test Loss of 196055124421.952789, Test MSE of 196055124492.256714
Epoch 7: training loss 182991637805.176
Test Loss of 135777841017.425262, Test MSE of 135777842445.179398
Epoch 8: training loss 147166863209.412
Test Loss of 117168446688.133270, Test MSE of 117168445952.429611
Epoch 9: training loss 141507448530.824
Test Loss of 111983108911.030075, Test MSE of 111983111845.941666
Epoch 10: training loss 137455552391.529
Test Loss of 108998587147.017120, Test MSE of 108998586139.551849
Epoch 11: training loss 134188707267.765
Test Loss of 106020614506.024994, Test MSE of 106020614536.084320
Epoch 12: training loss 130275008722.824
Test Loss of 103711575254.656174, Test MSE of 103711575547.750153
Epoch 13: training loss 126130227983.059
Test Loss of 100159825865.980560, Test MSE of 100159826233.546707
Epoch 14: training loss 123426571203.765
Test Loss of 97819571622.204529, Test MSE of 97819568825.208496
Epoch 15: training loss 119880495495.529
Test Loss of 95401496854.626556, Test MSE of 95401496125.096252
Epoch 16: training loss 115479164355.765
Test Loss of 93020708045.179077, Test MSE of 93020708717.179321
Epoch 17: training loss 111779485696.000
Test Loss of 89840325345.317902, Test MSE of 89840326937.246323
Epoch 18: training loss 107265007826.824
Test Loss of 85446423098.521057, Test MSE of 85446421897.869843
Epoch 19: training loss 104567308679.529
Test Loss of 82025274042.461823, Test MSE of 82025273435.563309
Epoch 20: training loss 100785544131.765
Test Loss of 80725080205.208694, Test MSE of 80725080161.476257
Epoch 21: training loss 97077831966.118
Test Loss of 77249661670.056458, Test MSE of 77249661405.813950
Epoch 22: training loss 92252236544.000
Test Loss of 75029477433.810272, Test MSE of 75029478692.983398
Epoch 23: training loss 90000452442.353
Test Loss of 70973537746.746872, Test MSE of 70973537805.994904
Epoch 24: training loss 85975066473.412
Test Loss of 68731525083.039337, Test MSE of 68731523987.878235
Epoch 25: training loss 82178319902.118
Test Loss of 67245773616.925499, Test MSE of 67245774484.469315
Epoch 26: training loss 78266232289.882
Test Loss of 66082260536.151779, Test MSE of 66082261297.501144
Epoch 27: training loss 76044635196.235
Test Loss of 59796334120.988434, Test MSE of 59796333907.061493
Epoch 28: training loss 72255541955.765
Test Loss of 58249180131.094864, Test MSE of 58249179611.726143
Epoch 29: training loss 69618026059.294
Test Loss of 57898073285.597412, Test MSE of 57898074954.786972
Epoch 30: training loss 66358037504.000
Test Loss of 54149251951.474319, Test MSE of 54149252425.328232
Epoch 31: training loss 63961770887.529
Test Loss of 54484814962.198982, Test MSE of 54484815619.431038
Epoch 32: training loss 61549372235.294
Test Loss of 47746609675.135582, Test MSE of 47746610544.614876
Epoch 33: training loss 58978142034.824
Test Loss of 47306104312.655251, Test MSE of 47306104454.376114
Epoch 34: training loss 56355404453.647
Test Loss of 45300695538.495140, Test MSE of 45300694956.804039
Epoch 35: training loss 54356240173.176
Test Loss of 44707087924.360947, Test MSE of 44707088021.916687
Epoch 36: training loss 51510847646.118
Test Loss of 41545199820.705231, Test MSE of 41545200647.400925
Epoch 37: training loss 48932328282.353
Test Loss of 39843299271.137436, Test MSE of 39843299591.069656
Epoch 38: training loss 47295202183.529
Test Loss of 40930771197.986115, Test MSE of 40930771210.281494
Epoch 39: training loss 44682859422.118
Test Loss of 36760045783.130035, Test MSE of 36760045601.639297
Epoch 40: training loss 42632206125.176
Test Loss of 34862968301.282738, Test MSE of 34862967683.530487
Epoch 41: training loss 41177326200.471
Test Loss of 33522574066.376678, Test MSE of 33522574583.609352
Epoch 42: training loss 39223539584.000
Test Loss of 32243196574.978252, Test MSE of 32243197106.540085
Epoch 43: training loss 36915169016.471
Test Loss of 31384081388.571957, Test MSE of 31384081284.420193
Epoch 44: training loss 35658762526.118
Test Loss of 27678665288.736694, Test MSE of 27678665401.397732
Epoch 45: training loss 33903246215.529
Test Loss of 27192478857.891716, Test MSE of 27192479236.460464
Epoch 46: training loss 32946740367.059
Test Loss of 29851549188.027763, Test MSE of 29851549087.775970
Epoch 47: training loss 30949728512.000
Test Loss of 27894023298.310043, Test MSE of 27894022821.889156
Epoch 48: training loss 29629462083.765
Test Loss of 29300827183.859325, Test MSE of 29300827164.619251
Epoch 49: training loss 28834662987.294
Test Loss of 25266887514.624710, Test MSE of 25266887846.341976
Epoch 50: training loss 26993467444.706
Test Loss of 26479549406.356316, Test MSE of 26479549569.617153
Epoch 51: training loss 26272750095.059
Test Loss of 25316573636.057381, Test MSE of 25316573607.349541
Epoch 52: training loss 24910363045.647
Test Loss of 25420972285.038406, Test MSE of 25420972170.873798
Epoch 53: training loss 24379944651.294
Test Loss of 24830801374.119389, Test MSE of 24830801537.665707
Epoch 54: training loss 23212251486.118
Test Loss of 21671686538.247108, Test MSE of 21671686364.277065
Epoch 55: training loss 22105589360.941
Test Loss of 22099701915.424339, Test MSE of 22099702356.863686
Epoch 56: training loss 21553049822.118
Test Loss of 24970955272.292458, Test MSE of 24970955614.254765
Epoch 57: training loss 20824658808.471
Test Loss of 21631222831.859325, Test MSE of 21631222490.545502
Epoch 58: training loss 20120352896.000
Test Loss of 21419809597.245720, Test MSE of 21419809770.748539
Epoch 59: training loss 19592998644.706
Test Loss of 21499249529.899120, Test MSE of 21499249637.021763
Epoch 60: training loss 18754156694.588
Test Loss of 21567254319.503933, Test MSE of 21567254818.366180
Epoch 61: training loss 18304472783.059
Test Loss of 22312889599.407681, Test MSE of 22312889853.516483
Epoch 62: training loss 17824405025.882
Test Loss of 20855112275.161499, Test MSE of 20855112336.917206
Epoch 63: training loss 17243009202.824
Test Loss of 20709964558.807961, Test MSE of 20709964438.754715
Epoch 64: training loss 16826765379.765
Test Loss of 18164682370.073112, Test MSE of 18164682069.402412
Epoch 65: training loss 16492678132.706
Test Loss of 20177922915.154095, Test MSE of 20177922931.409657
Epoch 66: training loss 15699080530.824
Test Loss of 19540779302.737621, Test MSE of 19540779396.901505
Epoch 67: training loss 15443550174.118
Test Loss of 20483292755.635353, Test MSE of 20483292897.439812
Epoch 68: training loss 14900813819.294
Test Loss of 18827162770.421101, Test MSE of 18827163124.709538
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21840233891.290356, 'MSE - std': 3739675640.166298, 'R2 - mean': 0.8387812067912415, 'R2 - std': 0.020015337088964774} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005257 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043410853.647
Test Loss of 431611725739.653870, Test MSE of 431611734623.520142
Epoch 2: training loss 424023708852.706
Test Loss of 431592348236.527527, Test MSE of 431592339742.839050
Epoch 3: training loss 423996575623.529
Test Loss of 431565517082.417419, Test MSE of 431565510310.433594
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011773590.588
Test Loss of 431566544338.746887, Test MSE of 431566539134.774292
Epoch 2: training loss 423999907840.000
Test Loss of 431569606289.236450, Test MSE of 431569611471.542664
Epoch 3: training loss 420044816865.882
Test Loss of 418982032847.903748, Test MSE of 418982033856.456787
Epoch 4: training loss 393160473178.353
Test Loss of 376436079722.617310, Test MSE of 376436084385.712524
Epoch 5: training loss 327132039047.529
Test Loss of 287909974135.411377, Test MSE of 287909976049.922791
Epoch 6: training loss 244797815808.000
Test Loss of 209733914318.837585, Test MSE of 209733909948.755829
Epoch 7: training loss 177461364133.647
Test Loss of 148934951109.123566, Test MSE of 148934950239.346619
Epoch 8: training loss 144932917112.471
Test Loss of 128924243021.238312, Test MSE of 128924241367.856720
Epoch 9: training loss 136520226243.765
Test Loss of 123085075707.616837, Test MSE of 123085075721.901611
Epoch 10: training loss 134568027949.176
Test Loss of 119916217953.850998, Test MSE of 119916218703.831528
Epoch 11: training loss 130057942889.412
Test Loss of 116031064870.500687, Test MSE of 116031066487.682297
Epoch 12: training loss 127352443361.882
Test Loss of 112068155483.927811, Test MSE of 112068155416.431366
Epoch 13: training loss 123627237436.235
Test Loss of 110714698073.913925, Test MSE of 110714697389.178970
Epoch 14: training loss 120154684144.941
Test Loss of 105710259763.413239, Test MSE of 105710262267.550446
Epoch 15: training loss 115306098778.353
Test Loss of 102740052152.803329, Test MSE of 102740052324.477158
Epoch 16: training loss 111736005451.294
Test Loss of 98338329927.433594, Test MSE of 98338328086.307541
Epoch 17: training loss 107740506307.765
Test Loss of 95779176038.589539, Test MSE of 95779174048.261978
Epoch 18: training loss 104542407152.941
Test Loss of 92141730312.292450, Test MSE of 92141729026.964767
Epoch 19: training loss 100036951431.529
Test Loss of 87869615477.871353, Test MSE of 87869615370.962906
Epoch 20: training loss 97626417212.235
Test Loss of 85111871576.136978, Test MSE of 85111872174.631241
Epoch 21: training loss 93227224003.765
Test Loss of 81599928878.200836, Test MSE of 81599927786.429337
Epoch 22: training loss 88965163851.294
Test Loss of 76009452223.200363, Test MSE of 76009453352.880783
Epoch 23: training loss 86283686731.294
Test Loss of 74432653036.216568, Test MSE of 74432653180.169998
Epoch 24: training loss 81684438309.647
Test Loss of 71133141876.212860, Test MSE of 71133142130.163925
Epoch 25: training loss 78843928402.824
Test Loss of 69356576555.713089, Test MSE of 69356575565.014664
Epoch 26: training loss 75955467444.706
Test Loss of 64046724381.734383, Test MSE of 64046724096.752747
Epoch 27: training loss 72381211083.294
Test Loss of 62730045399.248497, Test MSE of 62730045601.709679
Epoch 28: training loss 69562281027.765
Test Loss of 59148017834.587692, Test MSE of 59148017535.004211
Epoch 29: training loss 66743580973.176
Test Loss of 54797493382.100876, Test MSE of 54797492857.904907
Epoch 30: training loss 63855717744.941
Test Loss of 54742176009.358627, Test MSE of 54742176208.595184
Epoch 31: training loss 61415489566.118
Test Loss of 50170873913.810272, Test MSE of 50170874096.393417
Epoch 32: training loss 58473097645.176
Test Loss of 46768487822.511803, Test MSE of 46768488342.571495
Epoch 33: training loss 56238206960.941
Test Loss of 47883356272.777420, Test MSE of 47883355855.777298
Epoch 34: training loss 53808744222.118
Test Loss of 45260378083.568718, Test MSE of 45260379018.274689
Epoch 35: training loss 50905428510.118
Test Loss of 43224700684.438683, Test MSE of 43224700949.122269
Epoch 36: training loss 48386267941.647
Test Loss of 43836704052.479408, Test MSE of 43836704146.122856
Epoch 37: training loss 45948250752.000
Test Loss of 41323347623.033783, Test MSE of 41323347042.012405
Epoch 38: training loss 43969624794.353
Test Loss of 39177648993.258675, Test MSE of 39177649910.239311
Epoch 39: training loss 42683759691.294
Test Loss of 38293609388.127716, Test MSE of 38293609658.062538
Epoch 40: training loss 40639464003.765
Test Loss of 37474291101.675148, Test MSE of 37474291259.701233
Epoch 41: training loss 38659084468.706
Test Loss of 34567513985.006943, Test MSE of 34567514027.402641
Epoch 42: training loss 36525513355.294
Test Loss of 36595221387.905602, Test MSE of 36595220797.661613
Epoch 43: training loss 35623308995.765
Test Loss of 31478141490.939381, Test MSE of 31478141134.777550
Epoch 44: training loss 33525336357.647
Test Loss of 31339199602.198982, Test MSE of 31339200428.499313
Epoch 45: training loss 31858311627.294
Test Loss of 31104553503.985191, Test MSE of 31104553436.267483
Epoch 46: training loss 30560964773.647
Test Loss of 25907685863.122627, Test MSE of 25907686223.287937
Epoch 47: training loss 29337377679.059
Test Loss of 25321376200.795929, Test MSE of 25321376228.406170
Epoch 48: training loss 28673116611.765
Test Loss of 28631125534.563629, Test MSE of 28631125426.565220
Epoch 49: training loss 27572224033.882
Test Loss of 26504420343.944469, Test MSE of 26504420915.262470
Epoch 50: training loss 26289064203.294
Test Loss of 25388282876.683018, Test MSE of 25388282713.313339
Epoch 51: training loss 24996623792.941
Test Loss of 28140112609.317909, Test MSE of 28140112346.237724
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23100209582.279827, 'MSE - std': 4187874649.8027105, 'R2 - mean': 0.8289946846797237, 'R2 - std': 0.02652536588220431} 
 

Saving model.....
Results After CV: {'MSE - mean': 23100209582.279827, 'MSE - std': 4187874649.8027105, 'R2 - mean': 0.8289946846797237, 'R2 - std': 0.02652536588220431}
Train time: 94.42189417619993
Inference time: 0.0707505427992146
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 33 finished with value: 23100209582.279827 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003858 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427523910234.353
Test Loss of 418110805170.365051, Test MSE of 418110804476.080872
Epoch 2: training loss 427501125391.059
Test Loss of 418091504227.131165, Test MSE of 418091507117.255005
Epoch 3: training loss 427472899011.765
Test Loss of 418066402756.663452, Test MSE of 418066399346.042114
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490332672.000
Test Loss of 418071241937.632202, Test MSE of 418071241094.876038
Epoch 2: training loss 427478831344.941
Test Loss of 418072840429.346313, Test MSE of 418072837572.601868
Epoch 3: training loss 427478413071.059
Test Loss of 418073425720.553345, Test MSE of 418073431468.495239
Epoch 4: training loss 422120256090.353
Test Loss of 401001213242.329834, Test MSE of 401001213747.744141
Epoch 5: training loss 385813572065.882
Test Loss of 346359305068.428406, Test MSE of 346359301716.027527
Epoch 6: training loss 317692300227.765
Test Loss of 268804380671.526245, Test MSE of 268804378628.801514
Epoch 7: training loss 226172315226.353
Test Loss of 171042270787.627106, Test MSE of 171042271524.613708
Epoch 8: training loss 160978954691.765
Test Loss of 130185340894.364105, Test MSE of 130185338762.001648
Epoch 9: training loss 141198229187.765
Test Loss of 120604276775.320847, Test MSE of 120604276146.659332
Epoch 10: training loss 136486581744.941
Test Loss of 115678622494.734207, Test MSE of 115678623857.170151
Epoch 11: training loss 133013782106.353
Test Loss of 113374262469.314835, Test MSE of 113374261885.876007
Epoch 12: training loss 129556410849.882
Test Loss of 109704462955.658569, Test MSE of 109704463494.605743
Epoch 13: training loss 126770327853.176
Test Loss of 106061649883.521622, Test MSE of 106061650163.686264
Epoch 14: training loss 122190335789.176
Test Loss of 103064191258.352066, Test MSE of 103064192833.282867
Epoch 15: training loss 118354892288.000
Test Loss of 99622835401.104782, Test MSE of 99622834303.702606
Epoch 16: training loss 114971245025.882
Test Loss of 96043601151.585480, Test MSE of 96043601185.880234
Epoch 17: training loss 110355988901.647
Test Loss of 93239550421.955124, Test MSE of 93239552271.558060
Epoch 18: training loss 106773489272.471
Test Loss of 88817483474.698120, Test MSE of 88817482224.227585
Epoch 19: training loss 102417431265.882
Test Loss of 85822676840.401566, Test MSE of 85822676765.117981
Epoch 20: training loss 99245096056.471
Test Loss of 83512708567.613235, Test MSE of 83512708699.310989
Epoch 21: training loss 95504380039.529
Test Loss of 77537662782.238266, Test MSE of 77537663572.738007
Epoch 22: training loss 91500485978.353
Test Loss of 76299808128.444138, Test MSE of 76299807924.448822
Epoch 23: training loss 87566589153.882
Test Loss of 73876769776.603287, Test MSE of 73876768112.168411
Epoch 24: training loss 84524749748.706
Test Loss of 69706951037.364792, Test MSE of 69706951418.311066
Epoch 25: training loss 80293811049.412
Test Loss of 66841703253.688644, Test MSE of 66841703712.094147
Epoch 26: training loss 76452774381.176
Test Loss of 64772442279.232018, Test MSE of 64772442054.958855
Epoch 27: training loss 73059854095.059
Test Loss of 61718149227.540131, Test MSE of 61718148692.517075
Epoch 28: training loss 70577872557.176
Test Loss of 60636932513.369423, Test MSE of 60636932061.753960
Epoch 29: training loss 67141024632.471
Test Loss of 55701744925.905159, Test MSE of 55701745261.855804
Epoch 30: training loss 63101171275.294
Test Loss of 51759840491.688179, Test MSE of 51759839864.465584
Epoch 31: training loss 60792082228.706
Test Loss of 51321256962.605598, Test MSE of 51321256522.912704
Epoch 32: training loss 58137518569.412
Test Loss of 48333688705.036316, Test MSE of 48333688836.789284
Epoch 33: training loss 55238026051.765
Test Loss of 46958532369.706223, Test MSE of 46958533218.484268
Epoch 34: training loss 52599680986.353
Test Loss of 41175046411.665970, Test MSE of 41175046472.397148
Epoch 35: training loss 50236634552.471
Test Loss of 38403067242.651863, Test MSE of 38403067367.767143
Epoch 36: training loss 47307871616.000
Test Loss of 41734250482.735138, Test MSE of 41734250050.769951
Epoch 37: training loss 44831386503.529
Test Loss of 38692829409.028915, Test MSE of 38692829286.613823
Epoch 38: training loss 42345765605.647
Test Loss of 36197078343.120979, Test MSE of 36197078186.265373
Epoch 39: training loss 40320851531.294
Test Loss of 36930445006.908165, Test MSE of 36930444912.704895
Epoch 40: training loss 38537878859.294
Test Loss of 33606624522.481609, Test MSE of 33606623737.618595
Epoch 41: training loss 37385948574.118
Test Loss of 31551688548.611610, Test MSE of 31551688101.553226
Epoch 42: training loss 34811459817.412
Test Loss of 29847444122.559334, Test MSE of 29847444129.949219
Epoch 43: training loss 33296479201.882
Test Loss of 28539719922.557484, Test MSE of 28539719803.598442
Epoch 44: training loss 32028036340.706
Test Loss of 27611516127.370808, Test MSE of 27611516411.977047
Epoch 45: training loss 30215383243.294
Test Loss of 27506603851.503124, Test MSE of 27506603764.699665
Epoch 46: training loss 28923663954.824
Test Loss of 27083659871.814945, Test MSE of 27083659886.030529
Epoch 47: training loss 27500548295.529
Test Loss of 23495346989.894054, Test MSE of 23495346752.318428
Epoch 48: training loss 26488467757.176
Test Loss of 25993220361.297249, Test MSE of 25993220250.910645
Epoch 49: training loss 25132676833.882
Test Loss of 24179362162.468655, Test MSE of 24179362381.514767
Epoch 50: training loss 24594553283.765
Test Loss of 23860349237.355541, Test MSE of 23860349464.847946
Epoch 51: training loss 23637030531.765
Test Loss of 21653369335.117279, Test MSE of 21653369447.616261
Epoch 52: training loss 22722138944.000
Test Loss of 21645650567.609531, Test MSE of 21645650702.239681
Epoch 53: training loss 21984388073.412
Test Loss of 20879258891.902843, Test MSE of 20879258257.394493
Epoch 54: training loss 20732981225.412
Test Loss of 23475485902.315983, Test MSE of 23475485744.497444
Epoch 55: training loss 20261672173.176
Test Loss of 23071279524.211891, Test MSE of 23071279309.763382
Epoch 56: training loss 19584602608.941
Test Loss of 21168722979.294010, Test MSE of 21168723146.084225
Epoch 57: training loss 18831389827.765
Test Loss of 22593914028.680084, Test MSE of 22593913739.870483
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22593913739.870483, 'MSE - std': 0.0, 'R2 - mean': 0.8240587422866348, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005330 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916352572.235
Test Loss of 424555714813.453613, Test MSE of 424555711453.432251
Epoch 2: training loss 427894132976.941
Test Loss of 424538977596.698608, Test MSE of 424538976698.117188
Epoch 3: training loss 427866274635.294
Test Loss of 424516624044.798523, Test MSE of 424516616987.919067
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888101737.412
Test Loss of 424521500102.084656, Test MSE of 424521496110.188477
Epoch 2: training loss 427875849396.706
Test Loss of 424522125043.860291, Test MSE of 424522128211.922791
Epoch 3: training loss 427875326072.471
Test Loss of 424522046227.838074, Test MSE of 424522041480.778076
Epoch 4: training loss 422605988562.824
Test Loss of 408089586168.538513, Test MSE of 408089583824.416626
Epoch 5: training loss 386255292054.588
Test Loss of 354401755524.470947, Test MSE of 354401755612.854309
Epoch 6: training loss 317635767476.706
Test Loss of 278023449126.018066, Test MSE of 278023450367.027588
Epoch 7: training loss 226297564220.235
Test Loss of 181149780121.967163, Test MSE of 181149779888.697601
Epoch 8: training loss 159816211395.765
Test Loss of 141904217401.856110, Test MSE of 141904216050.196381
Epoch 9: training loss 138848302832.941
Test Loss of 131657952339.142258, Test MSE of 131657948728.689941
Epoch 10: training loss 135007827305.412
Test Loss of 127908510214.750870, Test MSE of 127908508648.563202
Epoch 11: training loss 132051704380.235
Test Loss of 124645675238.240112, Test MSE of 124645677915.961319
Epoch 12: training loss 128246548058.353
Test Loss of 120671343999.022903, Test MSE of 120671346784.945724
Epoch 13: training loss 124386262919.529
Test Loss of 117917635349.259308, Test MSE of 117917637697.552582
Epoch 14: training loss 119811286407.529
Test Loss of 114999581228.650467, Test MSE of 114999583128.592804
Epoch 15: training loss 116446831194.353
Test Loss of 110265966046.245667, Test MSE of 110265963529.045776
Epoch 16: training loss 112146261775.059
Test Loss of 107390510640.203568, Test MSE of 107390508797.043777
Epoch 17: training loss 108536927894.588
Test Loss of 103957669252.707840, Test MSE of 103957671103.177338
Epoch 18: training loss 105753645327.059
Test Loss of 99716362559.304184, Test MSE of 99716361884.232773
Epoch 19: training loss 100407198388.706
Test Loss of 96079610779.802917, Test MSE of 96079610755.456741
Epoch 20: training loss 96968801731.765
Test Loss of 93397955766.154984, Test MSE of 93397954219.691467
Epoch 21: training loss 93096072041.412
Test Loss of 87795151254.473282, Test MSE of 87795151099.389587
Epoch 22: training loss 88241932950.588
Test Loss of 86280138769.054825, Test MSE of 86280138039.581345
Epoch 23: training loss 84863110113.882
Test Loss of 81580661507.730743, Test MSE of 81580661081.773483
Epoch 24: training loss 81494790324.706
Test Loss of 76381767941.033539, Test MSE of 76381769425.115097
Epoch 25: training loss 77716352301.176
Test Loss of 71911304903.328247, Test MSE of 71911303513.950241
Epoch 26: training loss 74099644807.529
Test Loss of 73563160352.392319, Test MSE of 73563159287.858063
Epoch 27: training loss 70445201859.765
Test Loss of 64709725050.167015, Test MSE of 64709725100.428787
Epoch 28: training loss 67724487439.059
Test Loss of 65030266106.848022, Test MSE of 65030265424.160484
Epoch 29: training loss 63723451587.765
Test Loss of 61545062580.970619, Test MSE of 61545060373.604683
Epoch 30: training loss 61324761840.941
Test Loss of 54856219913.534119, Test MSE of 54856217880.787025
Epoch 31: training loss 58003314748.235
Test Loss of 54930794284.235947, Test MSE of 54930794298.473511
Epoch 32: training loss 55034322055.529
Test Loss of 53818718547.675224, Test MSE of 53818718675.641655
Epoch 33: training loss 52260895201.882
Test Loss of 50456023733.562805, Test MSE of 50456023482.901932
Epoch 34: training loss 48914635956.706
Test Loss of 49485592754.128151, Test MSE of 49485592598.022263
Epoch 35: training loss 46928255457.882
Test Loss of 46941456554.074486, Test MSE of 46941456480.990211
Epoch 36: training loss 43870684212.706
Test Loss of 42033152861.268562, Test MSE of 42033153626.004387
Epoch 37: training loss 41896845025.882
Test Loss of 39529777710.071709, Test MSE of 39529777451.343018
Epoch 38: training loss 39543101312.000
Test Loss of 42979613868.206337, Test MSE of 42979614620.265503
Epoch 39: training loss 37343660024.471
Test Loss of 40333299457.598892, Test MSE of 40333298487.861595
Epoch 40: training loss 35136219851.294
Test Loss of 34522730360.982651, Test MSE of 34522730262.656662
Epoch 41: training loss 33516231408.941
Test Loss of 35472224797.964378, Test MSE of 35472224536.946281
Epoch 42: training loss 31541373003.294
Test Loss of 35171498576.655098, Test MSE of 35171499769.841644
Epoch 43: training loss 30476408613.647
Test Loss of 35269912432.928986, Test MSE of 35269911415.829262
Epoch 44: training loss 28757443260.235
Test Loss of 31674015205.351837, Test MSE of 31674014483.360474
Epoch 45: training loss 27093047845.647
Test Loss of 33431056687.670601, Test MSE of 33431056238.467064
Epoch 46: training loss 26455867324.235
Test Loss of 30726040521.756187, Test MSE of 30726040484.947903
Epoch 47: training loss 24439183480.471
Test Loss of 29942907509.844090, Test MSE of 29942907239.073738
Epoch 48: training loss 23800440839.529
Test Loss of 30025530306.176266, Test MSE of 30025530128.352646
Epoch 49: training loss 22703503589.647
Test Loss of 30059105907.001617, Test MSE of 30059105129.465546
Epoch 50: training loss 21846487808.000
Test Loss of 29138005728.673607, Test MSE of 29138005248.557446
Epoch 51: training loss 20502189214.118
Test Loss of 27132731466.377979, Test MSE of 27132731719.302521
Epoch 52: training loss 19485740197.647
Test Loss of 32011209776.321999, Test MSE of 32011210066.488708
Epoch 53: training loss 18970188427.294
Test Loss of 28029989866.918343, Test MSE of 28029989906.238892
Epoch 54: training loss 18400371094.588
Test Loss of 27193221305.708073, Test MSE of 27193220790.977360
Epoch 55: training loss 17742644167.529
Test Loss of 27941361633.206570, Test MSE of 27941362340.428413
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25267638040.14945, 'MSE - std': 2673724300.278965, 'R2 - mean': 0.8122879831196017, 'R2 - std': 0.011770759167033074} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005362 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926081475.765
Test Loss of 447255963477.925537, Test MSE of 447255969116.675903
Epoch 2: training loss 421904088244.706
Test Loss of 447236839791.152466, Test MSE of 447236849581.085205
Epoch 3: training loss 421876665645.176
Test Loss of 447211793638.950745, Test MSE of 447211795104.664490
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897468867.765
Test Loss of 447219898090.859131, Test MSE of 447219905666.760437
Epoch 2: training loss 421885029074.824
Test Loss of 447221822936.086975, Test MSE of 447221823838.038513
Epoch 3: training loss 421884457441.882
Test Loss of 447221711966.038391, Test MSE of 447221713725.479858
Epoch 4: training loss 416222390994.824
Test Loss of 429563792817.713623, Test MSE of 429563793231.222900
Epoch 5: training loss 379241347794.824
Test Loss of 373417577712.425659, Test MSE of 373417572527.535828
Epoch 6: training loss 310941501680.941
Test Loss of 294422029419.303284, Test MSE of 294422034601.270203
Epoch 7: training loss 219914372758.588
Test Loss of 194904961655.975952, Test MSE of 194904958482.318817
Epoch 8: training loss 155381633415.529
Test Loss of 154214240097.769135, Test MSE of 154214239393.340576
Epoch 9: training loss 136786104681.412
Test Loss of 142338212757.170471, Test MSE of 142338212286.712616
Epoch 10: training loss 132185712609.882
Test Loss of 137317552299.495728, Test MSE of 137317554577.125412
Epoch 11: training loss 129259176688.941
Test Loss of 133947647795.815872, Test MSE of 133947647894.792709
Epoch 12: training loss 126505588585.412
Test Loss of 130869000140.361786, Test MSE of 130868999292.483078
Epoch 13: training loss 120376293737.412
Test Loss of 127747629872.736526, Test MSE of 127747629780.979965
Epoch 14: training loss 117363271378.824
Test Loss of 123628919617.317596, Test MSE of 123628918702.782013
Epoch 15: training loss 114417985385.412
Test Loss of 120772882446.212357, Test MSE of 120772879389.271973
Epoch 16: training loss 110525347358.118
Test Loss of 116678603725.072403, Test MSE of 116678603278.883194
Epoch 17: training loss 105967970695.529
Test Loss of 111548306401.917191, Test MSE of 111548304842.699799
Epoch 18: training loss 101786069263.059
Test Loss of 108176110025.637756, Test MSE of 108176109222.305557
Epoch 19: training loss 98721091252.706
Test Loss of 105016509511.298630, Test MSE of 105016510694.189072
Epoch 20: training loss 94657972434.824
Test Loss of 100821185553.291702, Test MSE of 100821186520.057571
Epoch 21: training loss 91144366185.412
Test Loss of 95768724052.918808, Test MSE of 95768723486.947510
Epoch 22: training loss 86678567198.118
Test Loss of 92724305702.077255, Test MSE of 92724306068.902939
Epoch 23: training loss 83533292694.588
Test Loss of 89331855800.582932, Test MSE of 89331856945.433411
Epoch 24: training loss 80041234160.941
Test Loss of 86132222543.707611, Test MSE of 86132223604.783310
Epoch 25: training loss 76186357007.059
Test Loss of 79580441728.621796, Test MSE of 79580442163.546356
Epoch 26: training loss 73215613093.647
Test Loss of 76805026266.692581, Test MSE of 76805027780.994827
Epoch 27: training loss 69694194703.059
Test Loss of 72950559899.625259, Test MSE of 72950558943.904663
Epoch 28: training loss 66178012626.824
Test Loss of 71405802060.391388, Test MSE of 71405801177.802582
Epoch 29: training loss 62633985972.706
Test Loss of 62914523786.925743, Test MSE of 62914524958.982216
Epoch 30: training loss 60691991461.647
Test Loss of 66159562000.640297, Test MSE of 66159563308.693069
Epoch 31: training loss 57054422437.647
Test Loss of 63295412100.352531, Test MSE of 63295412251.115601
Epoch 32: training loss 54076719540.706
Test Loss of 57621150239.385612, Test MSE of 57621150831.351074
Epoch 33: training loss 51687260265.412
Test Loss of 52853576391.565117, Test MSE of 52853576335.747513
Epoch 34: training loss 49302477967.059
Test Loss of 52721098229.932915, Test MSE of 52721098616.336411
Epoch 35: training loss 46615699651.765
Test Loss of 52802814182.240112, Test MSE of 52802814973.700462
Epoch 36: training loss 43871322164.706
Test Loss of 48219292020.363640, Test MSE of 48219293240.147644
Epoch 37: training loss 41678932464.941
Test Loss of 46379796238.863754, Test MSE of 46379796778.003464
Epoch 38: training loss 39671806848.000
Test Loss of 43022341096.549622, Test MSE of 43022341247.763809
Epoch 39: training loss 37377491011.765
Test Loss of 43150325178.714783, Test MSE of 43150325855.952789
Epoch 40: training loss 35672338492.235
Test Loss of 38590872453.536896, Test MSE of 38590873083.030045
Epoch 41: training loss 34023270430.118
Test Loss of 40677295206.565811, Test MSE of 40677296433.926781
Epoch 42: training loss 32660040365.176
Test Loss of 35191873775.715012, Test MSE of 35191873343.646065
Epoch 43: training loss 30502857321.412
Test Loss of 33366848139.873238, Test MSE of 33366848212.002163
Epoch 44: training loss 29639144583.529
Test Loss of 38085799754.081886, Test MSE of 38085800125.236481
Epoch 45: training loss 28012253409.882
Test Loss of 35026288605.890350, Test MSE of 35026287983.522057
Epoch 46: training loss 26673612065.882
Test Loss of 30269378922.888733, Test MSE of 30269379045.717342
Epoch 47: training loss 25475490925.176
Test Loss of 33562426099.623409, Test MSE of 33562426524.886250
Epoch 48: training loss 24418085609.412
Test Loss of 30828312902.410362, Test MSE of 30828312944.844334
Epoch 49: training loss 23332882582.588
Test Loss of 32930329546.229935, Test MSE of 32930328225.859962
Epoch 50: training loss 22424760907.294
Test Loss of 30495711243.369881, Test MSE of 30495711091.570076
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 27010329057.289658, 'MSE - std': 3292386933.271541, 'R2 - mean': 0.8071893433472651, 'R2 - std': 0.01201496716467839} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005388 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106339689.412
Test Loss of 410762270209.184631, Test MSE of 410762273215.459534
Epoch 2: training loss 430084771117.176
Test Loss of 410744348933.567810, Test MSE of 410744347586.352722
Epoch 3: training loss 430058621771.294
Test Loss of 410721514459.513184, Test MSE of 410721509835.915100
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077439759.059
Test Loss of 410724146832.288757, Test MSE of 410724150179.135681
Epoch 2: training loss 430063560824.471
Test Loss of 410724688755.265137, Test MSE of 410724686438.101929
Epoch 3: training loss 430063230253.176
Test Loss of 410724593529.425293, Test MSE of 410724595343.393127
Epoch 4: training loss 424982030697.412
Test Loss of 394497684291.879700, Test MSE of 394497688195.237976
Epoch 5: training loss 389116787892.706
Test Loss of 339920154080.014832, Test MSE of 339920153988.341492
Epoch 6: training loss 321315653872.941
Test Loss of 263528527759.222595, Test MSE of 263528532624.529602
Epoch 7: training loss 229664359905.882
Test Loss of 165165768002.695038, Test MSE of 165165768611.497833
Epoch 8: training loss 163987579934.118
Test Loss of 124692972930.191574, Test MSE of 124692970697.677002
Epoch 9: training loss 144597910317.176
Test Loss of 114202670627.302170, Test MSE of 114202669965.192535
Epoch 10: training loss 138492226048.000
Test Loss of 110365481544.736694, Test MSE of 110365483363.393021
Epoch 11: training loss 135175350121.412
Test Loss of 107767826916.279495, Test MSE of 107767830091.092224
Epoch 12: training loss 132931327909.647
Test Loss of 104408047089.073578, Test MSE of 104408047160.228012
Epoch 13: training loss 128794663243.294
Test Loss of 101948980137.284592, Test MSE of 101948980199.103806
Epoch 14: training loss 124251455969.882
Test Loss of 99072804991.940765, Test MSE of 99072805465.428619
Epoch 15: training loss 121869855021.176
Test Loss of 95349349323.402130, Test MSE of 95349350366.234695
Epoch 16: training loss 116781143762.824
Test Loss of 92291321437.112442, Test MSE of 92291321074.617767
Epoch 17: training loss 113309147648.000
Test Loss of 89759967500.201752, Test MSE of 89759966832.869385
Epoch 18: training loss 108816589793.882
Test Loss of 87494321576.099960, Test MSE of 87494322538.623322
Epoch 19: training loss 106273009874.824
Test Loss of 84554864333.889862, Test MSE of 84554863431.447708
Epoch 20: training loss 101691904180.706
Test Loss of 79076382846.045349, Test MSE of 79076382556.893967
Epoch 21: training loss 98071553054.118
Test Loss of 79080065667.494675, Test MSE of 79080066647.852570
Epoch 22: training loss 94165814904.471
Test Loss of 74604424918.419250, Test MSE of 74604425816.294418
Epoch 23: training loss 90529025415.529
Test Loss of 71441163426.532166, Test MSE of 71441163235.216248
Epoch 24: training loss 86859638889.412
Test Loss of 71383465362.302643, Test MSE of 71383467294.959183
Epoch 25: training loss 82734337370.353
Test Loss of 66868365618.110138, Test MSE of 66868365294.505074
Epoch 26: training loss 79236689377.882
Test Loss of 63889332262.856087, Test MSE of 63889331777.176880
Epoch 27: training loss 75990844385.882
Test Loss of 62529142518.641373, Test MSE of 62529141559.665916
Epoch 28: training loss 72734388299.294
Test Loss of 56571804266.854233, Test MSE of 56571806023.893524
Epoch 29: training loss 69540357511.529
Test Loss of 57753228016.481262, Test MSE of 57753228451.939629
Epoch 30: training loss 66443456888.471
Test Loss of 53873380032.148079, Test MSE of 53873379916.284348
Epoch 31: training loss 63463025656.471
Test Loss of 51003756610.339661, Test MSE of 51003755681.261635
Epoch 32: training loss 60224858292.706
Test Loss of 52530466184.825546, Test MSE of 52530467333.157898
Epoch 33: training loss 57467078881.882
Test Loss of 48681306188.764458, Test MSE of 48681305793.420502
Epoch 34: training loss 54064701944.471
Test Loss of 43567965881.040260, Test MSE of 43567966568.371284
Epoch 35: training loss 51606563832.471
Test Loss of 43921760981.945396, Test MSE of 43921761095.287262
Epoch 36: training loss 50171708950.588
Test Loss of 39713779583.111519, Test MSE of 39713779163.754677
Epoch 37: training loss 46299838403.765
Test Loss of 37101919624.825546, Test MSE of 37101919863.054359
Epoch 38: training loss 44789063273.412
Test Loss of 38864659700.035172, Test MSE of 38864658966.881493
Epoch 39: training loss 42424306288.941
Test Loss of 35654794793.936142, Test MSE of 35654795166.762390
Epoch 40: training loss 40179154974.118
Test Loss of 33998738497.865803, Test MSE of 33998738868.865471
Epoch 41: training loss 38235778176.000
Test Loss of 33659623622.071262, Test MSE of 33659623015.649162
Epoch 42: training loss 36993675791.059
Test Loss of 31819066374.160110, Test MSE of 31819065925.369347
Epoch 43: training loss 34890296282.353
Test Loss of 30669416157.053215, Test MSE of 30669415442.724731
Epoch 44: training loss 33520583258.353
Test Loss of 29405921450.587692, Test MSE of 29405921142.986942
Epoch 45: training loss 32016040545.882
Test Loss of 25903257740.260990, Test MSE of 25903257462.410374
Epoch 46: training loss 30147771550.118
Test Loss of 28121857340.061081, Test MSE of 28121857266.042171
Epoch 47: training loss 29169003151.059
Test Loss of 25663096549.582600, Test MSE of 25663096874.102535
Epoch 48: training loss 27992133820.235
Test Loss of 25740241804.853310, Test MSE of 25740241502.898312
Epoch 49: training loss 26879933033.412
Test Loss of 24157392781.327164, Test MSE of 24157392843.720657
Epoch 50: training loss 25302586375.529
Test Loss of 24460678734.896809, Test MSE of 24460679103.414162
Epoch 51: training loss 24694074341.647
Test Loss of 24715315875.716797, Test MSE of 24715316136.390835
Epoch 52: training loss 23570465965.176
Test Loss of 23440807098.224895, Test MSE of 23440806549.757759
Epoch 53: training loss 23062478437.647
Test Loss of 22344311827.428043, Test MSE of 22344311730.030937
Epoch 54: training loss 21925496365.176
Test Loss of 22215254524.919945, Test MSE of 22215254416.953907
Epoch 55: training loss 20994821485.176
Test Loss of 23601436195.302174, Test MSE of 23601436276.320236
Epoch 56: training loss 20083052133.647
Test Loss of 22552326110.830170, Test MSE of 22552325831.994503
Epoch 57: training loss 19262048884.706
Test Loss of 22731609931.935215, Test MSE of 22731610046.611557
Epoch 58: training loss 19100428111.059
Test Loss of 22723444803.287369, Test MSE of 22723444714.008221
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25938607971.4693, 'MSE - std': 3402295849.299234, 'R2 - mean': 0.8085048995617612, 'R2 - std': 0.010651837467626072} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005299 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043119435.294
Test Loss of 431612353077.782532, Test MSE of 431612352855.762817
Epoch 2: training loss 424022676660.706
Test Loss of 431592048593.562256, Test MSE of 431592051861.771729
Epoch 3: training loss 423995721125.647
Test Loss of 431564164557.060608, Test MSE of 431564160375.373474
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424006393856.000
Test Loss of 431568119719.863037, Test MSE of 431568114488.898865
Epoch 2: training loss 423998224504.471
Test Loss of 431570711591.329956, Test MSE of 431570707643.631165
Epoch 3: training loss 423997667087.059
Test Loss of 431569733555.235535, Test MSE of 431569727122.123352
Epoch 4: training loss 418507615412.706
Test Loss of 413830936228.190674, Test MSE of 413830943916.912903
Epoch 5: training loss 381998925342.118
Test Loss of 357913800395.046753, Test MSE of 357913799357.848389
Epoch 6: training loss 313910482462.118
Test Loss of 278873318918.870911, Test MSE of 278873316391.483032
Epoch 7: training loss 223558358076.235
Test Loss of 178436909589.560394, Test MSE of 178436909397.634369
Epoch 8: training loss 158932707659.294
Test Loss of 137564649651.117065, Test MSE of 137564647884.971466
Epoch 9: training loss 139863566667.294
Test Loss of 126624089584.125870, Test MSE of 126624090188.986267
Epoch 10: training loss 134311560553.412
Test Loss of 120806985934.600647, Test MSE of 120806984523.675491
Epoch 11: training loss 131630940521.412
Test Loss of 117488751427.879684, Test MSE of 117488748693.293030
Epoch 12: training loss 128210119439.059
Test Loss of 114126390089.565948, Test MSE of 114126389795.875458
Epoch 13: training loss 124740675463.529
Test Loss of 110050843276.971771, Test MSE of 110050844543.022766
Epoch 14: training loss 120619953603.765
Test Loss of 107468056863.155945, Test MSE of 107468059336.993347
Epoch 15: training loss 117385366317.176
Test Loss of 103738352150.508102, Test MSE of 103738350801.698227
Epoch 16: training loss 114465628702.118
Test Loss of 99764914501.064316, Test MSE of 99764915563.476059
Epoch 17: training loss 108923875749.647
Test Loss of 96815185101.179077, Test MSE of 96815186105.608200
Epoch 18: training loss 106277007721.412
Test Loss of 91725588217.484497, Test MSE of 91725587923.816238
Epoch 19: training loss 102063751514.353
Test Loss of 90505198057.965759, Test MSE of 90505198229.760529
Epoch 20: training loss 97290691885.176
Test Loss of 84843509901.208694, Test MSE of 84843507853.914536
Epoch 21: training loss 94230408463.059
Test Loss of 80693229311.170761, Test MSE of 80693229001.229401
Epoch 22: training loss 90278966076.235
Test Loss of 76586450022.826462, Test MSE of 76586449888.268951
Epoch 23: training loss 86592424583.529
Test Loss of 73342449221.419708, Test MSE of 73342446672.751099
Epoch 24: training loss 82715597944.471
Test Loss of 68488936203.964828, Test MSE of 68488937378.158379
Epoch 25: training loss 80598669613.176
Test Loss of 67494162801.606667, Test MSE of 67494163057.659363
Epoch 26: training loss 76464364852.706
Test Loss of 67106153316.101807, Test MSE of 67106153616.015617
Epoch 27: training loss 72714895299.765
Test Loss of 62833328309.960205, Test MSE of 62833330158.787636
Epoch 28: training loss 69618431736.471
Test Loss of 58914297627.602036, Test MSE of 58914297786.681931
Epoch 29: training loss 66835753125.647
Test Loss of 54640399541.486351, Test MSE of 54640399949.761406
Epoch 30: training loss 62767758900.706
Test Loss of 52693005933.223511, Test MSE of 52693006173.932137
Epoch 31: training loss 60495627971.765
Test Loss of 48757966193.132812, Test MSE of 48757966815.771904
Epoch 32: training loss 57951183420.235
Test Loss of 49509437565.097641, Test MSE of 49509437111.963951
Epoch 33: training loss 54941426974.118
Test Loss of 44967799107.168900, Test MSE of 44967798843.491516
Epoch 34: training loss 52371726863.059
Test Loss of 41162919638.419250, Test MSE of 41162920159.677887
Epoch 35: training loss 49611890522.353
Test Loss of 40169884854.434059, Test MSE of 40169884764.784363
Epoch 36: training loss 46652661895.529
Test Loss of 40970429314.428505, Test MSE of 40970430129.812813
Epoch 37: training loss 45359329242.353
Test Loss of 34459171384.625633, Test MSE of 34459171932.489456
Epoch 38: training loss 42087888240.941
Test Loss of 33351472575.792690, Test MSE of 33351473043.986031
Epoch 39: training loss 39845298319.059
Test Loss of 34758611228.312820, Test MSE of 34758611196.594833
Epoch 40: training loss 38352960090.353
Test Loss of 31289019683.420639, Test MSE of 31289020281.945347
Epoch 41: training loss 36415109857.882
Test Loss of 30114121373.556686, Test MSE of 30114121246.219509
Epoch 42: training loss 34876525756.235
Test Loss of 28777801132.838501, Test MSE of 28777801455.621853
Epoch 43: training loss 33265540295.529
Test Loss of 29339118924.172142, Test MSE of 29339118313.149460
Epoch 44: training loss 31535394913.882
Test Loss of 27196116636.608978, Test MSE of 27196116047.805210
Epoch 45: training loss 29554978115.765
Test Loss of 25818924368.910690, Test MSE of 25818923965.964191
Epoch 46: training loss 28899720003.765
Test Loss of 25451520711.729755, Test MSE of 25451520829.465446
Epoch 47: training loss 27940399853.176
Test Loss of 26460599621.538177, Test MSE of 26460599837.097446
Epoch 48: training loss 26831609799.529
Test Loss of 23300878535.018974, Test MSE of 23300878553.794670
Epoch 49: training loss 25652208545.882
Test Loss of 23010378790.382229, Test MSE of 23010378510.178234
Epoch 50: training loss 24192692623.059
Test Loss of 21943774487.100418, Test MSE of 21943774490.495251
Epoch 51: training loss 23276921656.471
Test Loss of 22584423486.074966, Test MSE of 22584423629.559021
Epoch 52: training loss 23067515922.824
Test Loss of 21769246176.014809, Test MSE of 21769245910.744621
Epoch 53: training loss 22145163407.059
Test Loss of 22730704137.832485, Test MSE of 22730703885.678673
Epoch 54: training loss 21230915388.235
Test Loss of 21594318728.588615, Test MSE of 21594318507.803329
Epoch 55: training loss 20563421782.588
Test Loss of 20981202047.466915, Test MSE of 20981202506.033569
Epoch 56: training loss 19689669594.353
Test Loss of 19275113126.559925, Test MSE of 19275113148.343548
Epoch 57: training loss 19253617285.647
Test Loss of 19968554261.204998, Test MSE of 19968554284.593468
Epoch 58: training loss 18845374034.824
Test Loss of 19215885176.003700, Test MSE of 19215884987.244289
Epoch 59: training loss 17840100664.471
Test Loss of 19160385666.783897, Test MSE of 19160385632.168507
Epoch 60: training loss 17522576888.471
Test Loss of 18442741119.348450, Test MSE of 18442741186.032486
Epoch 61: training loss 17032935175.529
Test Loss of 18377706614.937531, Test MSE of 18377706901.333965
Epoch 62: training loss 16738826575.059
Test Loss of 18437271764.286903, Test MSE of 18437271788.350555
Epoch 63: training loss 16421335548.235
Test Loss of 20652105041.858398, Test MSE of 20652105463.681107
Epoch 64: training loss 15560603535.059
Test Loss of 20982123781.093937, Test MSE of 20982123633.425541
Epoch 65: training loss 15715403531.294
Test Loss of 19254203904.236927, Test MSE of 19254203928.827625
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24601727162.940964, 'MSE - std': 4050863466.1973653, 'R2 - mean': 0.8180456976902303, 'R2 - std': 0.021327836947571035} 
 

Saving model.....
Results After CV: {'MSE - mean': 24601727162.940964, 'MSE - std': 4050863466.1973653, 'R2 - mean': 0.8180456976902303, 'R2 - std': 0.021327836947571035}
Train time: 88.21114439400007
Inference time: 0.09493019760047901
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 34 finished with value: 24601727162.940964 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003628 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525349737.412
Test Loss of 418113147995.195923, Test MSE of 418113147748.773865
Epoch 2: training loss 427505035504.941
Test Loss of 418094681002.489014, Test MSE of 418094681224.203552
Epoch 3: training loss 427477974678.588
Test Loss of 418071041941.407349, Test MSE of 418071045084.883850
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495149929.412
Test Loss of 418075009166.123535, Test MSE of 418075012445.981506
Epoch 2: training loss 427483433441.882
Test Loss of 418076400142.330811, Test MSE of 418076405202.413574
Epoch 3: training loss 427482881084.235
Test Loss of 418077094463.126526, Test MSE of 418077096549.893799
Epoch 4: training loss 427482526418.824
Test Loss of 418076709380.382141, Test MSE of 418076706321.661499
Epoch 5: training loss 420846255766.588
Test Loss of 396479028980.807800, Test MSE of 396479036946.266174
Epoch 6: training loss 375717257697.882
Test Loss of 330380939715.952820, Test MSE of 330380940227.651978
Epoch 7: training loss 297471421921.882
Test Loss of 244773593370.352081, Test MSE of 244773592635.612305
Epoch 8: training loss 218513251207.529
Test Loss of 174512196257.191772, Test MSE of 174512198757.994354
Epoch 9: training loss 160712292608.000
Test Loss of 126833346823.876007, Test MSE of 126833348763.913010
Epoch 10: training loss 140177396043.294
Test Loss of 119148467382.154984, Test MSE of 119148465224.782364
Epoch 11: training loss 136555967849.412
Test Loss of 115650194589.046494, Test MSE of 115650194937.566498
Epoch 12: training loss 133810844099.765
Test Loss of 113459493293.686798, Test MSE of 113459493708.464615
Epoch 13: training loss 129066014629.647
Test Loss of 110134558439.542908, Test MSE of 110134557463.112671
Epoch 14: training loss 125884330827.294
Test Loss of 106762794253.324081, Test MSE of 106762793997.078415
Epoch 15: training loss 123051219546.353
Test Loss of 103846399813.344437, Test MSE of 103846400219.539719
Epoch 16: training loss 118336011550.118
Test Loss of 100229449908.496872, Test MSE of 100229448731.052307
Epoch 17: training loss 115447586168.471
Test Loss of 97043919144.564423, Test MSE of 97043919439.368240
Epoch 18: training loss 113372352060.235
Test Loss of 94943371851.443909, Test MSE of 94943372996.253174
Epoch 19: training loss 108720141658.353
Test Loss of 92561900185.138092, Test MSE of 92561899748.625595
Epoch 20: training loss 104634241746.824
Test Loss of 88683280322.886887, Test MSE of 88683281427.247803
Epoch 21: training loss 102421694253.176
Test Loss of 85537215823.648392, Test MSE of 85537216623.571304
Epoch 22: training loss 98478492024.471
Test Loss of 83412912640.118439, Test MSE of 83412913606.512604
Epoch 23: training loss 95157515640.471
Test Loss of 80238618349.701599, Test MSE of 80238618129.903046
Epoch 24: training loss 91087099489.882
Test Loss of 77767712506.492706, Test MSE of 77767712034.909439
Epoch 25: training loss 88439108562.824
Test Loss of 76166786614.362244, Test MSE of 76166786517.215530
Epoch 26: training loss 85812946718.118
Test Loss of 72257522757.877396, Test MSE of 72257522817.252701
Epoch 27: training loss 82951934177.882
Test Loss of 69646086122.207733, Test MSE of 69646085231.735779
Epoch 28: training loss 79862718042.353
Test Loss of 67055788932.115662, Test MSE of 67055789604.030624
Epoch 29: training loss 76567838208.000
Test Loss of 65526369524.215591, Test MSE of 65526370036.368439
Epoch 30: training loss 73277951352.471
Test Loss of 62393540668.639374, Test MSE of 62393541640.491776
Epoch 31: training loss 70463554590.118
Test Loss of 57443946436.308121, Test MSE of 57443946558.043343
Epoch 32: training loss 67822015887.059
Test Loss of 56925880530.342819, Test MSE of 56925881207.199066
Epoch 33: training loss 64449573330.824
Test Loss of 54215872896.681007, Test MSE of 54215872742.131805
Epoch 34: training loss 62842335495.529
Test Loss of 52328867956.067543, Test MSE of 52328868351.519608
Epoch 35: training loss 60156097046.588
Test Loss of 49844693899.221840, Test MSE of 49844693695.566879
Epoch 36: training loss 56792854174.118
Test Loss of 47859069053.068703, Test MSE of 47859069690.108948
Epoch 37: training loss 54780008688.941
Test Loss of 47966528328.423782, Test MSE of 47966528136.283501
Epoch 38: training loss 52209419678.118
Test Loss of 44362833924.737450, Test MSE of 44362834054.082291
Epoch 39: training loss 50703012352.000
Test Loss of 41093833926.262321, Test MSE of 41093834545.704491
Epoch 40: training loss 47500873863.529
Test Loss of 40835319392.525558, Test MSE of 40835319189.522499
Epoch 41: training loss 45634345991.529
Test Loss of 36571152453.166779, Test MSE of 36571152884.427780
Epoch 42: training loss 43704954706.824
Test Loss of 37673922685.068703, Test MSE of 37673922334.171013
Epoch 43: training loss 41880582279.529
Test Loss of 34815089886.423317, Test MSE of 34815089890.676453
Epoch 44: training loss 40261686369.882
Test Loss of 31646756452.789268, Test MSE of 31646756591.290325
Epoch 45: training loss 38403212747.294
Test Loss of 31255471632.936386, Test MSE of 31255472526.099960
Epoch 46: training loss 36447387301.647
Test Loss of 30540299638.021744, Test MSE of 30540299210.677711
Epoch 47: training loss 34929113404.235
Test Loss of 28730716556.050892, Test MSE of 28730716849.379612
Epoch 48: training loss 33369978962.824
Test Loss of 28706553725.246357, Test MSE of 28706553715.057022
Epoch 49: training loss 31906896745.412
Test Loss of 26029821026.302105, Test MSE of 26029820749.267075
Epoch 50: training loss 30967452912.941
Test Loss of 25583637307.158916, Test MSE of 25583636780.447727
Epoch 51: training loss 29309407555.765
Test Loss of 26327897924.870693, Test MSE of 26327897486.172710
Epoch 52: training loss 28309884641.882
Test Loss of 24755106228.792969, Test MSE of 24755106630.578186
Epoch 53: training loss 26860551386.353
Test Loss of 23902756316.350681, Test MSE of 23902756179.351044
Epoch 54: training loss 25670334014.118
Test Loss of 23194624935.646542, Test MSE of 23194625156.976200
Epoch 55: training loss 24718472079.059
Test Loss of 22706313827.131161, Test MSE of 22706313993.389095
Epoch 56: training loss 23616467907.765
Test Loss of 22296065565.727505, Test MSE of 22296066076.198658
Epoch 57: training loss 22820736233.412
Test Loss of 20475885465.197315, Test MSE of 20475885457.987198
Epoch 58: training loss 21829974991.059
Test Loss of 20126540850.453850, Test MSE of 20126540682.309067
Epoch 59: training loss 21137052547.765
Test Loss of 19185121683.157066, Test MSE of 19185121989.534588
Epoch 60: training loss 20406669059.765
Test Loss of 20739540228.322926, Test MSE of 20739540291.908886
Epoch 61: training loss 19464121016.471
Test Loss of 22253445158.847095, Test MSE of 22253445133.269665
Epoch 62: training loss 19274010627.765
Test Loss of 19369164200.712467, Test MSE of 19369164366.624249
Epoch 63: training loss 18298106740.706
Test Loss of 19453678767.996300, Test MSE of 19453678658.017788
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19453678658.017788, 'MSE - std': 0.0, 'R2 - mean': 0.8485120935819365, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005402 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917870260.706
Test Loss of 424556736730.870239, Test MSE of 424556732595.542847
Epoch 2: training loss 427897043064.471
Test Loss of 424540038869.066833, Test MSE of 424540044300.349548
Epoch 3: training loss 427869360609.882
Test Loss of 424517226784.984497, Test MSE of 424517228097.049866
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889128146.824
Test Loss of 424523990075.928772, Test MSE of 424523991653.092590
Epoch 2: training loss 427876489577.412
Test Loss of 424524926704.070312, Test MSE of 424524933796.026733
Epoch 3: training loss 427875900958.118
Test Loss of 424524428604.698608, Test MSE of 424524425372.905273
Epoch 4: training loss 427875473528.471
Test Loss of 424524028400.011108, Test MSE of 424524025932.774292
Epoch 5: training loss 420646446260.706
Test Loss of 402240056607.800110, Test MSE of 402240058110.806641
Epoch 6: training loss 374247077767.529
Test Loss of 335767997460.607910, Test MSE of 335768001611.239746
Epoch 7: training loss 295243146902.588
Test Loss of 251822842502.188293, Test MSE of 251822835957.579468
Epoch 8: training loss 216135095175.529
Test Loss of 184287262884.389557, Test MSE of 184287263231.825684
Epoch 9: training loss 157597005281.882
Test Loss of 138854710026.363159, Test MSE of 138854707414.328217
Epoch 10: training loss 138519926723.765
Test Loss of 130424019822.797134, Test MSE of 130424020874.089233
Epoch 11: training loss 132815216549.647
Test Loss of 127208575996.446915, Test MSE of 127208574781.910950
Epoch 12: training loss 130729488956.235
Test Loss of 124904384845.990280, Test MSE of 124904384238.031403
Epoch 13: training loss 127547047092.706
Test Loss of 121557661621.622025, Test MSE of 121557663582.735916
Epoch 14: training loss 124089081313.882
Test Loss of 117884697689.774689, Test MSE of 117884696077.248871
Epoch 15: training loss 120463089332.706
Test Loss of 115109337906.157761, Test MSE of 115109340023.844711
Epoch 16: training loss 116787297189.647
Test Loss of 110646943018.222534, Test MSE of 110646945221.546982
Epoch 17: training loss 112636442955.294
Test Loss of 107813753470.608368, Test MSE of 107813755886.525085
Epoch 18: training loss 109405244175.059
Test Loss of 105110220355.627106, Test MSE of 105110220264.021347
Epoch 19: training loss 105702354130.824
Test Loss of 102726314512.225769, Test MSE of 102726315246.628677
Epoch 20: training loss 102121220336.941
Test Loss of 97747424367.093216, Test MSE of 97747424160.443726
Epoch 21: training loss 99752692193.882
Test Loss of 94767945516.235947, Test MSE of 94767946462.005096
Epoch 22: training loss 94575650032.941
Test Loss of 91666871411.830673, Test MSE of 91666871895.620956
Epoch 23: training loss 91798089968.941
Test Loss of 89651888774.188293, Test MSE of 89651890261.251938
Epoch 24: training loss 88959837635.765
Test Loss of 84911730098.661118, Test MSE of 84911732365.143311
Epoch 25: training loss 85012121118.118
Test Loss of 81044036558.493637, Test MSE of 81044035224.520645
Epoch 26: training loss 81150179749.647
Test Loss of 79513262237.757111, Test MSE of 79513261176.857010
Epoch 27: training loss 78191670482.824
Test Loss of 75214939392.059219, Test MSE of 75214938819.280792
Epoch 28: training loss 75680537193.412
Test Loss of 72162490441.430481, Test MSE of 72162488795.477844
Epoch 29: training loss 73128333989.647
Test Loss of 69203114764.258148, Test MSE of 69203115355.392120
Epoch 30: training loss 69759260069.647
Test Loss of 64933721877.496185, Test MSE of 64933722654.390915
Epoch 31: training loss 67070134874.353
Test Loss of 64403194564.722649, Test MSE of 64403197017.798439
Epoch 32: training loss 64779643015.529
Test Loss of 61920954413.953270, Test MSE of 61920955833.554047
Epoch 33: training loss 61212871845.647
Test Loss of 58779600593.750633, Test MSE of 58779601443.515709
Epoch 34: training loss 58911787806.118
Test Loss of 57561851387.144112, Test MSE of 57561853686.316528
Epoch 35: training loss 56057631457.882
Test Loss of 56250250120.853111, Test MSE of 56250249887.096565
Epoch 36: training loss 53475515678.118
Test Loss of 52934414069.755264, Test MSE of 52934414354.660728
Epoch 37: training loss 51163615947.294
Test Loss of 51328358208.133240, Test MSE of 51328359392.309196
Epoch 38: training loss 48735347937.882
Test Loss of 48751346190.330788, Test MSE of 48751347314.052826
Epoch 39: training loss 46479825822.118
Test Loss of 45582859698.661118, Test MSE of 45582860994.682098
Epoch 40: training loss 44891566426.353
Test Loss of 45210865985.672913, Test MSE of 45210865775.986305
Epoch 41: training loss 42212161182.118
Test Loss of 42424567101.646080, Test MSE of 42424566334.526054
Epoch 42: training loss 40758757963.294
Test Loss of 41426422913.095535, Test MSE of 41426422667.622314
Epoch 43: training loss 38424288504.471
Test Loss of 39371879726.012489, Test MSE of 39371879497.031128
Epoch 44: training loss 36315134509.176
Test Loss of 38313118358.769371, Test MSE of 38313118861.469566
Epoch 45: training loss 34735703363.765
Test Loss of 37425965693.660881, Test MSE of 37425965710.047997
Epoch 46: training loss 32546956905.412
Test Loss of 32959499719.032154, Test MSE of 32959499063.312798
Epoch 47: training loss 31091689502.118
Test Loss of 35442419326.371498, Test MSE of 35442420085.960785
Epoch 48: training loss 29635839375.059
Test Loss of 33125956355.256996, Test MSE of 33125957070.781929
Epoch 49: training loss 28248736173.176
Test Loss of 34572270185.289848, Test MSE of 34572269219.077469
Epoch 50: training loss 26981432869.647
Test Loss of 28968477967.692806, Test MSE of 28968478154.160007
Epoch 51: training loss 26068289219.765
Test Loss of 30591043236.271107, Test MSE of 30591043265.236950
Epoch 52: training loss 24533183593.412
Test Loss of 29655144318.430721, Test MSE of 29655144647.667679
Epoch 53: training loss 23105128395.294
Test Loss of 32776177070.397408, Test MSE of 32776177720.952652
Epoch 54: training loss 22506843523.765
Test Loss of 28575293526.221607, Test MSE of 28575293199.916023
Epoch 55: training loss 21290916653.176
Test Loss of 27537176005.137173, Test MSE of 27537176137.512779
Epoch 56: training loss 20655095232.000
Test Loss of 29576256875.836224, Test MSE of 29576256310.417469
Epoch 57: training loss 19740410048.000
Test Loss of 28334066731.347675, Test MSE of 28334065594.104710
Epoch 58: training loss 19002098868.706
Test Loss of 26083147325.468426, Test MSE of 26083147418.802746
Epoch 59: training loss 18007367593.412
Test Loss of 27018180937.252834, Test MSE of 27018181116.536362
Epoch 60: training loss 17496974309.647
Test Loss of 26546362182.528801, Test MSE of 26546362608.522648
Epoch 61: training loss 16737855168.000
Test Loss of 27333945360.344204, Test MSE of 27333945375.269939
Epoch 62: training loss 16672528470.588
Test Loss of 25397599972.463566, Test MSE of 25397599826.654789
Epoch 63: training loss 15961719126.588
Test Loss of 28158368815.374508, Test MSE of 28158369644.554630
Epoch 64: training loss 15295262957.176
Test Loss of 26681453636.929909, Test MSE of 26681453652.186035
Epoch 65: training loss 14763566471.529
Test Loss of 27376050758.706455, Test MSE of 27376050067.219929
Epoch 66: training loss 14292288564.706
Test Loss of 25064166619.817719, Test MSE of 25064166453.875263
Epoch 67: training loss 13862363290.353
Test Loss of 23508454792.734676, Test MSE of 23508455191.710957
Epoch 68: training loss 13494746070.588
Test Loss of 25432245252.974323, Test MSE of 25432245296.926521
Epoch 69: training loss 13060574889.412
Test Loss of 25071885596.010178, Test MSE of 25071885278.438091
Epoch 70: training loss 12651623770.353
Test Loss of 26008632455.964840, Test MSE of 26008632298.118443
Epoch 71: training loss 12475517436.235
Test Loss of 23655799692.406200, Test MSE of 23655799650.080376
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21554739154.04908, 'MSE - std': 2101060496.0312939, 'R2 - mean': 0.8398126949184637, 'R2 - std': 0.008699398663472857} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927209441.882
Test Loss of 447260221604.863281, Test MSE of 447260231984.570190
Epoch 2: training loss 421906237801.412
Test Loss of 447242284372.859558, Test MSE of 447242281035.911316
Epoch 3: training loss 421878969404.235
Test Loss of 447218111018.281738, Test MSE of 447218112606.888489
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900636762.353
Test Loss of 447225684269.301880, Test MSE of 447225685276.521545
Epoch 2: training loss 421890343273.412
Test Loss of 447227122756.456177, Test MSE of 447227131696.451782
Epoch 3: training loss 421889816335.059
Test Loss of 447226595209.089966, Test MSE of 447226595734.861084
Epoch 4: training loss 421889458657.882
Test Loss of 447226860430.538025, Test MSE of 447226869645.275940
Epoch 5: training loss 415023019670.588
Test Loss of 425228558553.922729, Test MSE of 425228560375.649353
Epoch 6: training loss 369319240643.765
Test Loss of 357310810788.981750, Test MSE of 357310807102.266541
Epoch 7: training loss 290617564822.588
Test Loss of 270163849813.866302, Test MSE of 270163850842.167542
Epoch 8: training loss 212371605564.235
Test Loss of 198662578083.145966, Test MSE of 198662577642.510559
Epoch 9: training loss 154754567830.588
Test Loss of 150236550110.127228, Test MSE of 150236550104.029266
Epoch 10: training loss 134829281430.588
Test Loss of 141013316731.410583, Test MSE of 141013319493.830414
Epoch 11: training loss 130997346514.824
Test Loss of 137134169264.943787, Test MSE of 137134169494.818298
Epoch 12: training loss 129575801765.647
Test Loss of 133499998462.874863, Test MSE of 133499999354.234909
Epoch 13: training loss 124596149217.882
Test Loss of 130147230838.673141, Test MSE of 130147230649.652573
Epoch 14: training loss 121243453168.941
Test Loss of 128021394365.912567, Test MSE of 128021393627.364319
Epoch 15: training loss 117823409965.176
Test Loss of 123258305585.980103, Test MSE of 123258305054.657303
Epoch 16: training loss 115793884160.000
Test Loss of 119884801461.029846, Test MSE of 119884801523.410416
Epoch 17: training loss 111465526422.588
Test Loss of 117733172445.002075, Test MSE of 117733173668.052917
Epoch 18: training loss 108458589274.353
Test Loss of 112859192838.040253, Test MSE of 112859194606.022873
Epoch 19: training loss 103729796999.529
Test Loss of 108625277912.442291, Test MSE of 108625278886.627457
Epoch 20: training loss 100838028679.529
Test Loss of 106795879175.520706, Test MSE of 106795880008.126511
Epoch 21: training loss 96780066770.824
Test Loss of 101680930355.519775, Test MSE of 101680931404.099121
Epoch 22: training loss 93447169641.412
Test Loss of 98876191289.441589, Test MSE of 98876190285.828018
Epoch 23: training loss 91283631661.176
Test Loss of 95395280897.421234, Test MSE of 95395277872.419922
Epoch 24: training loss 87683527228.235
Test Loss of 91050704778.511215, Test MSE of 91050703273.866776
Epoch 25: training loss 83757977298.824
Test Loss of 87727430721.139954, Test MSE of 87727429408.911957
Epoch 26: training loss 81786474511.059
Test Loss of 86821417936.862366, Test MSE of 86821418676.332047
Epoch 27: training loss 78255446392.471
Test Loss of 83145103139.708542, Test MSE of 83145103351.006287
Epoch 28: training loss 74174181767.529
Test Loss of 79676479569.010406, Test MSE of 79676479916.002548
Epoch 29: training loss 72214319450.353
Test Loss of 75398011743.637283, Test MSE of 75398011718.896072
Epoch 30: training loss 69178813349.647
Test Loss of 72254653530.248444, Test MSE of 72254651678.971863
Epoch 31: training loss 65987066428.235
Test Loss of 67839548094.563957, Test MSE of 67839548599.481087
Epoch 32: training loss 63213919262.118
Test Loss of 63314092219.603050, Test MSE of 63314093478.040398
Epoch 33: training loss 60937874959.059
Test Loss of 63856997565.734909, Test MSE of 63856997642.435844
Epoch 34: training loss 58800366953.412
Test Loss of 60849421238.806381, Test MSE of 60849420862.905518
Epoch 35: training loss 55804269206.588
Test Loss of 58689666821.862595, Test MSE of 58689666132.364319
Epoch 36: training loss 53208097189.647
Test Loss of 61525381132.317375, Test MSE of 61525380834.045250
Epoch 37: training loss 51303567360.000
Test Loss of 53468887683.819572, Test MSE of 53468887472.320030
Epoch 38: training loss 49105002774.588
Test Loss of 55851658666.133705, Test MSE of 55851659258.064362
Epoch 39: training loss 46968766908.235
Test Loss of 52009080758.569511, Test MSE of 52009081234.611626
Epoch 40: training loss 44558035651.765
Test Loss of 49870371782.913719, Test MSE of 49870371737.398163
Epoch 41: training loss 42402342648.471
Test Loss of 44485962781.135323, Test MSE of 44485962742.922676
Epoch 42: training loss 40474868495.059
Test Loss of 40755935726.116119, Test MSE of 40755935248.292961
Epoch 43: training loss 39211317639.529
Test Loss of 40819040998.832291, Test MSE of 40819041848.065079
Epoch 44: training loss 36510167213.176
Test Loss of 40886190910.948875, Test MSE of 40886190170.827766
Epoch 45: training loss 34360666526.118
Test Loss of 37364365144.767982, Test MSE of 37364364895.098007
Epoch 46: training loss 33791013910.588
Test Loss of 37130718307.249596, Test MSE of 37130719055.835915
Epoch 47: training loss 32012111646.118
Test Loss of 37884470320.558868, Test MSE of 37884470740.797813
Epoch 48: training loss 30470359936.000
Test Loss of 34041055493.033543, Test MSE of 34041055488.616642
Epoch 49: training loss 29245162089.412
Test Loss of 31799415813.921814, Test MSE of 31799415535.584747
Epoch 50: training loss 27929794281.412
Test Loss of 28771856474.959057, Test MSE of 28771856589.855228
Epoch 51: training loss 26518902407.529
Test Loss of 32267320510.208652, Test MSE of 32267320974.298931
Epoch 52: training loss 25227408225.882
Test Loss of 31479830074.625954, Test MSE of 31479829731.513779
Epoch 53: training loss 24337426232.471
Test Loss of 30427681711.700207, Test MSE of 30427682028.474148
Epoch 54: training loss 23313033976.471
Test Loss of 29604070575.522552, Test MSE of 29604070719.802807
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24237849675.96699, 'MSE - std': 4164268757.4541454, 'R2 - mean': 0.8275176815837689, 'R2 - std': 0.018782644396836023} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005445 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110230889.412
Test Loss of 410764827963.113403, Test MSE of 410764829206.544250
Epoch 2: training loss 430088990479.059
Test Loss of 410746516079.592773, Test MSE of 410746514064.678650
Epoch 3: training loss 430061303446.588
Test Loss of 410722820342.404419, Test MSE of 410722823663.502319
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077292062.118
Test Loss of 410729145643.002319, Test MSE of 410729149006.952454
Epoch 2: training loss 430065960839.529
Test Loss of 410729984176.273926, Test MSE of 410729984753.027283
Epoch 3: training loss 430065546661.647
Test Loss of 410729764474.491455, Test MSE of 410729758503.924561
Epoch 4: training loss 430065210307.765
Test Loss of 410730223423.614990, Test MSE of 410730218826.903198
Epoch 5: training loss 423412859843.765
Test Loss of 389544320337.858398, Test MSE of 389544323253.114014
Epoch 6: training loss 378278462042.353
Test Loss of 323524696349.260498, Test MSE of 323524701193.372070
Epoch 7: training loss 299521361438.118
Test Loss of 237635956707.568726, Test MSE of 237635952208.847687
Epoch 8: training loss 220309854991.059
Test Loss of 167934454786.369263, Test MSE of 167934452308.583466
Epoch 9: training loss 161960930635.294
Test Loss of 120974074837.826935, Test MSE of 120974074429.116898
Epoch 10: training loss 141839724303.059
Test Loss of 112768673756.934753, Test MSE of 112768674284.372055
Epoch 11: training loss 138443613605.647
Test Loss of 109113034868.568253, Test MSE of 109113034481.938248
Epoch 12: training loss 135662019674.353
Test Loss of 107058390557.615921, Test MSE of 107058389029.342117
Epoch 13: training loss 131728774053.647
Test Loss of 103782692186.387787, Test MSE of 103782692994.435593
Epoch 14: training loss 126984625694.118
Test Loss of 101984727697.710312, Test MSE of 101984726489.232590
Epoch 15: training loss 124639729844.706
Test Loss of 98912848083.339188, Test MSE of 98912848769.072876
Epoch 16: training loss 121317155840.000
Test Loss of 95699771001.543732, Test MSE of 95699771067.358459
Epoch 17: training loss 116703307535.059
Test Loss of 91743116546.250809, Test MSE of 91743114091.885666
Epoch 18: training loss 113850536628.706
Test Loss of 89253677108.124023, Test MSE of 89253677137.307587
Epoch 19: training loss 110208596419.765
Test Loss of 87313857824.577515, Test MSE of 87313858258.614960
Epoch 20: training loss 106364794217.412
Test Loss of 85284837475.509491, Test MSE of 85284836383.050995
Epoch 21: training loss 103178151137.882
Test Loss of 81758411378.435913, Test MSE of 81758411974.708069
Epoch 22: training loss 99426505607.529
Test Loss of 79562827824.333176, Test MSE of 79562828181.625778
Epoch 23: training loss 96385325025.882
Test Loss of 76303972815.429901, Test MSE of 76303972427.734650
Epoch 24: training loss 92270901549.176
Test Loss of 74052063400.692276, Test MSE of 74052061249.507355
Epoch 25: training loss 89865748208.941
Test Loss of 70921455247.341049, Test MSE of 70921454398.803238
Epoch 26: training loss 86131035542.588
Test Loss of 68577041158.278572, Test MSE of 68577040043.665115
Epoch 27: training loss 83257048003.765
Test Loss of 66346271626.010178, Test MSE of 66346271289.727760
Epoch 28: training loss 79601347072.000
Test Loss of 64856028453.316055, Test MSE of 64856028338.097214
Epoch 29: training loss 76515645093.647
Test Loss of 59477813319.078201, Test MSE of 59477813214.711220
Epoch 30: training loss 73035591920.941
Test Loss of 58635754712.551598, Test MSE of 58635753408.021782
Epoch 31: training loss 70983095446.588
Test Loss of 56337501554.080521, Test MSE of 56337501284.895851
Epoch 32: training loss 68571601603.765
Test Loss of 54619964893.171677, Test MSE of 54619965388.533409
Epoch 33: training loss 65353889822.118
Test Loss of 52820146205.378990, Test MSE of 52820146450.609932
Epoch 34: training loss 62391721953.882
Test Loss of 50693528165.167976, Test MSE of 50693529454.056618
Epoch 35: training loss 59659183186.824
Test Loss of 47449765223.655716, Test MSE of 47449765251.266342
Epoch 36: training loss 57204111616.000
Test Loss of 46408418692.086998, Test MSE of 46408418509.865463
Epoch 37: training loss 55109479439.059
Test Loss of 44375718893.519669, Test MSE of 44375719185.882881
Epoch 38: training loss 52156810202.353
Test Loss of 40181112153.440071, Test MSE of 40181111554.929085
Epoch 39: training loss 50303151450.353
Test Loss of 42130094719.703842, Test MSE of 42130094803.396431
Epoch 40: training loss 48432349003.294
Test Loss of 40227147573.190193, Test MSE of 40227147561.518219
Epoch 41: training loss 46093342034.824
Test Loss of 38409326698.143456, Test MSE of 38409327160.472321
Epoch 42: training loss 43786878727.529
Test Loss of 36465029146.535866, Test MSE of 36465029180.103920
Epoch 43: training loss 41599808956.235
Test Loss of 35489350719.970383, Test MSE of 35489351133.988892
Epoch 44: training loss 39401740950.588
Test Loss of 34041720128.799629, Test MSE of 34041720047.212112
Epoch 45: training loss 37777143875.765
Test Loss of 35817487845.701065, Test MSE of 35817487761.162064
Epoch 46: training loss 36359676024.471
Test Loss of 32214276860.801479, Test MSE of 32214277065.469910
Epoch 47: training loss 34683539200.000
Test Loss of 29253226535.803795, Test MSE of 29253226431.470413
Epoch 48: training loss 33076598656.000
Test Loss of 30466655582.178619, Test MSE of 30466655523.937931
Epoch 49: training loss 31808123429.647
Test Loss of 26960133946.876446, Test MSE of 26960134199.069153
Epoch 50: training loss 30596483659.294
Test Loss of 27647433525.664043, Test MSE of 27647433907.292068
Epoch 51: training loss 29076432534.588
Test Loss of 23765242055.018974, Test MSE of 23765242359.681450
Epoch 52: training loss 27301155433.412
Test Loss of 24507633648.836651, Test MSE of 24507633428.592613
Epoch 53: training loss 26157548626.824
Test Loss of 25620452218.846828, Test MSE of 25620452411.598507
Epoch 54: training loss 25547700909.176
Test Loss of 25795599769.884312, Test MSE of 25795599797.843922
Epoch 55: training loss 24390314447.059
Test Loss of 22946768081.917629, Test MSE of 22946767768.083218
Epoch 56: training loss 23502802100.706
Test Loss of 21035251942.293385, Test MSE of 21035252054.745010
Epoch 57: training loss 22775232289.882
Test Loss of 22878243533.416012, Test MSE of 22878243644.969833
Epoch 58: training loss 22095027169.882
Test Loss of 22879449066.202682, Test MSE of 22879449078.261539
Epoch 59: training loss 21225125869.176
Test Loss of 20860163191.885239, Test MSE of 20860163263.580128
Epoch 60: training loss 20107544105.412
Test Loss of 22691688915.220730, Test MSE of 22691688814.182693
Epoch 61: training loss 19451525413.647
Test Loss of 20922372517.730679, Test MSE of 20922372287.952736
Epoch 62: training loss 18612398592.000
Test Loss of 21362235286.804256, Test MSE of 21362234966.121552
Epoch 63: training loss 18425698179.765
Test Loss of 19963929067.861176, Test MSE of 19963928921.828342
Epoch 64: training loss 18189081995.294
Test Loss of 20046042935.085609, Test MSE of 20046043016.051640
Epoch 65: training loss 17413101891.765
Test Loss of 19956661925.138363, Test MSE of 19956661790.153893
Epoch 66: training loss 16995984628.706
Test Loss of 20023801557.471542, Test MSE of 20023801294.743080
Epoch 67: training loss 16473668065.882
Test Loss of 19968732013.578899, Test MSE of 19968731975.513405
Epoch 68: training loss 16033073694.118
Test Loss of 21552710547.487274, Test MSE of 21552710669.502789
Epoch 69: training loss 15547845808.941
Test Loss of 18052969147.409534, Test MSE of 18052969067.432041
Epoch 70: training loss 15001410313.412
Test Loss of 22339718080.029617, Test MSE of 22339718426.391026
Epoch 71: training loss 14534728749.176
Test Loss of 19181423989.871357, Test MSE of 19181423821.088539
Epoch 72: training loss 14274185772.235
Test Loss of 19207186310.693199, Test MSE of 19207186165.324699
Epoch 73: training loss 13951432760.471
Test Loss of 20681537314.236000, Test MSE of 20681537622.577923
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23348771662.619724, 'MSE - std': 3921381115.7147284, 'R2 - mean': 0.8279643840860478, 'R2 - std': 0.01628463776938631} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005385 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042781274.353
Test Loss of 431612222703.296631, Test MSE of 431612217372.351318
Epoch 2: training loss 424022466921.412
Test Loss of 431591514964.938477, Test MSE of 431591519936.663635
Epoch 3: training loss 423995224064.000
Test Loss of 431564303498.839417, Test MSE of 431564307058.220886
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424007102464.000
Test Loss of 431568872716.675598, Test MSE of 431568871434.585266
Epoch 2: training loss 423998913475.765
Test Loss of 431571875226.832031, Test MSE of 431571873340.340820
Epoch 3: training loss 423998402921.412
Test Loss of 431571238335.792664, Test MSE of 431571237702.692993
Epoch 4: training loss 423997978624.000
Test Loss of 431570413661.823242, Test MSE of 431570403578.737122
Epoch 5: training loss 417206354160.941
Test Loss of 409628014668.764465, Test MSE of 409628016259.991516
Epoch 6: training loss 371933449517.176
Test Loss of 342262413707.194824, Test MSE of 342262414857.351013
Epoch 7: training loss 293942073223.529
Test Loss of 255686761332.212860, Test MSE of 255686762735.883942
Epoch 8: training loss 216287635817.412
Test Loss of 183994740427.046722, Test MSE of 183994739960.163818
Epoch 9: training loss 158220601615.059
Test Loss of 134556836476.860718, Test MSE of 134556836483.439117
Epoch 10: training loss 138441358215.529
Test Loss of 124602519459.124481, Test MSE of 124602517679.681976
Epoch 11: training loss 134819514127.059
Test Loss of 121257714353.932434, Test MSE of 121257713007.094711
Epoch 12: training loss 131548968056.471
Test Loss of 118303876413.956497, Test MSE of 118303875256.806076
Epoch 13: training loss 129587576801.882
Test Loss of 114468856505.514114, Test MSE of 114468858056.442108
Epoch 14: training loss 124914042940.235
Test Loss of 111430365997.134659, Test MSE of 111430367943.939667
Epoch 15: training loss 122244498160.941
Test Loss of 107659540734.933823, Test MSE of 107659539947.833557
Epoch 16: training loss 117104250940.235
Test Loss of 104330960585.151321, Test MSE of 104330960297.469376
Epoch 17: training loss 114558879262.118
Test Loss of 101863229257.092087, Test MSE of 101863228813.134277
Epoch 18: training loss 110585906477.176
Test Loss of 99770469211.572418, Test MSE of 99770470810.876007
Epoch 19: training loss 106278322176.000
Test Loss of 94502858909.319763, Test MSE of 94502859267.860565
Epoch 20: training loss 104065697460.706
Test Loss of 91397942535.937073, Test MSE of 91397942491.776611
Epoch 21: training loss 100848465392.941
Test Loss of 89626596311.722351, Test MSE of 89626596950.087357
Epoch 22: training loss 97089017833.412
Test Loss of 86763204412.771866, Test MSE of 86763204630.202133
Epoch 23: training loss 94361887969.882
Test Loss of 82540833775.415085, Test MSE of 82540833624.730652
Epoch 24: training loss 89290444860.235
Test Loss of 80173392755.265152, Test MSE of 80173392273.097580
Epoch 25: training loss 87382749289.412
Test Loss of 76945879002.091629, Test MSE of 76945880455.101227
Epoch 26: training loss 85580739553.882
Test Loss of 73095027803.453949, Test MSE of 73095027674.987076
Epoch 27: training loss 82104446509.176
Test Loss of 70442458283.061539, Test MSE of 70442457844.724121
Epoch 28: training loss 79075311811.765
Test Loss of 67731053348.605278, Test MSE of 67731053992.238602
Epoch 29: training loss 75211673133.176
Test Loss of 64369496992.755203, Test MSE of 64369497274.443199
Epoch 30: training loss 72378417272.471
Test Loss of 63965668944.792229, Test MSE of 63965668717.576599
Epoch 31: training loss 70282557063.529
Test Loss of 58562725953.391945, Test MSE of 58562725038.600159
Epoch 32: training loss 66687157413.647
Test Loss of 57960391220.834801, Test MSE of 57960391259.447952
Epoch 33: training loss 64448575201.882
Test Loss of 54311760760.951408, Test MSE of 54311760753.178856
Epoch 34: training loss 61843803632.941
Test Loss of 54205960626.050903, Test MSE of 54205961156.926224
Epoch 35: training loss 59151996709.647
Test Loss of 48178165480.425728, Test MSE of 48178165192.340721
Epoch 36: training loss 56080228035.765
Test Loss of 47164072697.958351, Test MSE of 47164073020.820900
Epoch 37: training loss 53694456824.471
Test Loss of 46541230534.900513, Test MSE of 46541230038.280045
Epoch 38: training loss 52110371425.882
Test Loss of 45710623038.430359, Test MSE of 45710622017.223160
Epoch 39: training loss 49572081121.882
Test Loss of 41073897098.602501, Test MSE of 41073897414.105331
Epoch 40: training loss 47375769291.294
Test Loss of 39302829101.016197, Test MSE of 39302828748.179070
Epoch 41: training loss 45242966663.529
Test Loss of 38712855918.289680, Test MSE of 38712856684.801743
Epoch 42: training loss 43683563264.000
Test Loss of 36339142549.856544, Test MSE of 36339143587.621605
Epoch 43: training loss 41413381481.412
Test Loss of 37156200317.216103, Test MSE of 37156200293.272980
Epoch 44: training loss 39510629504.000
Test Loss of 31777052860.594170, Test MSE of 31777053121.457527
Epoch 45: training loss 37907517402.353
Test Loss of 32588626338.887550, Test MSE of 32588626047.483238
Epoch 46: training loss 36448850838.588
Test Loss of 33152165037.904675, Test MSE of 33152165119.513783
Epoch 47: training loss 34407919713.882
Test Loss of 31240208111.059696, Test MSE of 31240208300.433086
Epoch 48: training loss 32639913306.353
Test Loss of 26797067538.361870, Test MSE of 26797068107.924473
Epoch 49: training loss 31371538665.412
Test Loss of 30378180690.450718, Test MSE of 30378180135.960514
Epoch 50: training loss 30626240120.471
Test Loss of 29245198261.130959, Test MSE of 29245198091.913494
Epoch 51: training loss 28796319856.941
Test Loss of 24947894337.391949, Test MSE of 24947894067.493011
Epoch 52: training loss 27652553332.706
Test Loss of 27514427690.528458, Test MSE of 27514427831.881935
Epoch 53: training loss 26377025536.000
Test Loss of 26572844154.728367, Test MSE of 26572843936.842999
Epoch 54: training loss 25709350460.235
Test Loss of 26032134899.324387, Test MSE of 26032135136.920078
Epoch 55: training loss 24514296760.471
Test Loss of 27752575508.612679, Test MSE of 27752575514.613052
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24229532433.01839, 'MSE - std': 3924887517.1704316, 'R2 - mean': 0.8209200544049178, 'R2 - std': 0.020264300245749683} 
 

Saving model.....
Results After CV: {'MSE - mean': 24229532433.01839, 'MSE - std': 3924887517.1704316, 'R2 - mean': 0.8209200544049178, 'R2 - std': 0.020264300245749683}
Train time: 96.08548332999962
Inference time: 0.07066131079955085
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 35 finished with value: 24229532433.01839 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005335 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524729434.353
Test Loss of 418112239056.507080, Test MSE of 418112236995.561157
Epoch 2: training loss 427503890432.000
Test Loss of 418093074491.218140, Test MSE of 418093073805.923096
Epoch 3: training loss 427475819941.647
Test Loss of 418068175525.929199, Test MSE of 418068167978.299194
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492964592.941
Test Loss of 418072511415.280151, Test MSE of 418072517089.255005
Epoch 2: training loss 427481289306.353
Test Loss of 418074550057.867249, Test MSE of 418074557540.766235
Epoch 3: training loss 427480694543.059
Test Loss of 418073539783.209778, Test MSE of 418073542629.417908
Epoch 4: training loss 427480272534.588
Test Loss of 418073992827.529053, Test MSE of 418073992416.619995
Epoch 5: training loss 427480061831.529
Test Loss of 418073326226.505676, Test MSE of 418073326006.924927
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418073326006.9249, 'MSE - std': 0.0, 'R2 - mean': -2.2555823502267547, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917720636.235
Test Loss of 424556294381.820007, Test MSE of 424556297617.733398
Epoch 2: training loss 427896266029.176
Test Loss of 424539642689.317627, Test MSE of 424539648851.154724
Epoch 3: training loss 427868500449.882
Test Loss of 424517147339.828796, Test MSE of 424517155528.921143
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888978281.412
Test Loss of 424523986596.744873, Test MSE of 424523983962.076538
Epoch 2: training loss 427876371516.235
Test Loss of 424525389185.391602, Test MSE of 424525393764.492371
Epoch 3: training loss 427875838313.412
Test Loss of 424525120863.755737, Test MSE of 424525121768.275513
Epoch 4: training loss 427875424858.353
Test Loss of 424525367491.656738, Test MSE of 424525372026.115234
Epoch 5: training loss 427875146812.235
Test Loss of 424524978929.728455, Test MSE of 424524978871.461304
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 421299152439.1931, 'MSE - std': 3225826432.2681885, 'R2 - mean': -2.143204422595315, 'R2 - std': 0.11237792763143961} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926756111.059
Test Loss of 447259050340.256287, Test MSE of 447259040440.381592
Epoch 2: training loss 421906797387.294
Test Loss of 447241072722.431641, Test MSE of 447241077810.252625
Epoch 3: training loss 421879947866.353
Test Loss of 447217394367.037720, Test MSE of 447217401580.193054
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901215864.471
Test Loss of 447225879255.198730, Test MSE of 447225884531.544312
Epoch 2: training loss 421889524434.824
Test Loss of 447227252100.234070, Test MSE of 447227251220.974487
Epoch 3: training loss 421889018217.412
Test Loss of 447226860642.775879, Test MSE of 447226874618.797119
Epoch 4: training loss 421888676683.294
Test Loss of 447226939761.994934, Test MSE of 447226944612.822144
Epoch 5: training loss 421888490315.294
Test Loss of 447226912595.556763, Test MSE of 447226915841.112061
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 429941740239.83276, 'MSE - std': 12503037706.797922, 'R2 - mean': -2.0878563031917774, 'R2 - std': 0.12060691407284511} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005445 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110314255.059
Test Loss of 410764375594.883850, Test MSE of 410764389936.315796
Epoch 2: training loss 430089586085.647
Test Loss of 410746081194.706177, Test MSE of 410746083213.128418
Epoch 3: training loss 430062596096.000
Test Loss of 410723360840.025940, Test MSE of 410723360180.085571
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074913731.765
Test Loss of 410727385193.195740, Test MSE of 410727381764.463562
Epoch 2: training loss 430066755463.529
Test Loss of 410728329915.409546, Test MSE of 410728326922.989258
Epoch 3: training loss 430066328515.765
Test Loss of 410728258859.950012, Test MSE of 410728262782.139587
Epoch 4: training loss 430065964694.588
Test Loss of 410727839790.437744, Test MSE of 410727835633.046509
Epoch 5: training loss 430065731945.412
Test Loss of 410727684820.997681, Test MSE of 410727686287.861084
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 425138226751.83984, 'MSE - std': 13655244026.058184, 'R2 - mean': -2.163379639926047, 'R2 - std': 0.16739427698121834} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005290 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042856929.882
Test Loss of 431612981598.652466, Test MSE of 431612977106.361206
Epoch 2: training loss 424022432587.294
Test Loss of 431592420823.485413, Test MSE of 431592422232.689148
Epoch 3: training loss 423994925417.412
Test Loss of 431564635318.434082, Test MSE of 431564636364.237366
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008935062.588
Test Loss of 431565952528.347961, Test MSE of 431565955650.238647
Epoch 2: training loss 423998768549.647
Test Loss of 431568749453.801025, Test MSE of 431568752726.234741
Epoch 3: training loss 423998185472.000
Test Loss of 431568536272.259155, Test MSE of 431568537893.066528
Epoch 4: training loss 423997825385.412
Test Loss of 431568038338.161987, Test MSE of 431568044947.032715
Epoch 5: training loss 423997595648.000
Test Loss of 431568566280.529358, Test MSE of 431568563677.990540
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 426424294137.06995, 'MSE - std': 12481523497.008245, 'R2 - mean': -2.1752977232400257, 'R2 - std': 0.15160751304723516} 
 

Saving model.....
Results After CV: {'MSE - mean': 426424294137.06995, 'MSE - std': 12481523497.008245, 'R2 - mean': -2.1752977232400257, 'R2 - std': 0.15160751304723516}
Train time: 11.98737780599986
Inference time: 0.06882004239996604
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 36 finished with value: 426424294137.06995 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 6, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005608 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.20575e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524917970.824
Test Loss of 418109074310.484375, Test MSE of 418109068435.900208
Epoch 2: training loss 427504176308.706
Test Loss of 418089967261.638672, Test MSE of 418089965884.762512
Epoch 3: training loss 427477476171.294
Test Loss of 418065933665.650696, Test MSE of 418065935596.212708
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427489952707.765
Test Loss of 418067807732.985413, Test MSE of 418067814904.531311
Epoch 2: training loss 427477752771.765
Test Loss of 418069152100.256287, Test MSE of 418069145076.696594
Epoch 3: training loss 423848442096.941
Test Loss of 406448900678.232727, Test MSE of 406448899030.301392
Epoch 4: training loss 397667418232.471
Test Loss of 365604565510.277100, Test MSE of 365604568116.339233
Epoch 5: training loss 333689426040.471
Test Loss of 281913034159.344910, Test MSE of 281913042690.605347
Epoch 6: training loss 252404256888.471
Test Loss of 205190718419.704834, Test MSE of 205190714892.279724
Epoch 7: training loss 184401591446.588
Test Loss of 146293989836.480225, Test MSE of 146293988812.399994
Epoch 8: training loss 148783515994.353
Test Loss of 124782646332.876236, Test MSE of 124782643595.629425
Epoch 9: training loss 138250063314.824
Test Loss of 118842146774.310440, Test MSE of 118842148017.507980
Epoch 10: training loss 134637119879.529
Test Loss of 115807523487.770523, Test MSE of 115807522937.977463
Epoch 11: training loss 132660226590.118
Test Loss of 113256574306.124451, Test MSE of 113256574429.839096
Epoch 12: training loss 130198011467.294
Test Loss of 110333008036.389542, Test MSE of 110333006633.963562
Epoch 13: training loss 127165178488.471
Test Loss of 107874688868.611618, Test MSE of 107874690733.213654
Epoch 14: training loss 123303686972.235
Test Loss of 104925914052.544998, Test MSE of 104925914194.518341
Epoch 15: training loss 121580086000.941
Test Loss of 103145913232.906784, Test MSE of 103145915107.031342
Epoch 16: training loss 118762480429.176
Test Loss of 100568170452.415451, Test MSE of 100568170097.815536
Epoch 17: training loss 114741414038.588
Test Loss of 98329426224.618088, Test MSE of 98329424798.258392
Epoch 18: training loss 112152696380.235
Test Loss of 94828492503.909317, Test MSE of 94828491359.619476
Epoch 19: training loss 108824156310.588
Test Loss of 92336517859.989822, Test MSE of 92336519590.979233
Epoch 20: training loss 106123490770.824
Test Loss of 91160825532.905853, Test MSE of 91160825752.279297
Epoch 21: training loss 102633575273.412
Test Loss of 87203291126.525101, Test MSE of 87203293033.453598
Epoch 22: training loss 100329957150.118
Test Loss of 85481862900.334030, Test MSE of 85481863409.125198
Epoch 23: training loss 96807865554.824
Test Loss of 82538457768.771683, Test MSE of 82538457652.588867
Epoch 24: training loss 92557571448.471
Test Loss of 78840870049.310196, Test MSE of 78840869984.142563
Epoch 25: training loss 89512099599.059
Test Loss of 79015284594.823959, Test MSE of 79015285935.920258
Epoch 26: training loss 87427282416.941
Test Loss of 73863505941.318527, Test MSE of 73863506158.670395
Epoch 27: training loss 83945455224.471
Test Loss of 70452136063.437424, Test MSE of 70452138089.784180
Epoch 28: training loss 79933072165.647
Test Loss of 68755109798.462173, Test MSE of 68755109639.378113
Epoch 29: training loss 78233967841.882
Test Loss of 65946530459.980568, Test MSE of 65946530992.490128
Epoch 30: training loss 74851017200.941
Test Loss of 64090666378.392784, Test MSE of 64090667290.142769
Epoch 31: training loss 72570866669.176
Test Loss of 62701043552.821655, Test MSE of 62701043738.044167
Epoch 32: training loss 68869525865.412
Test Loss of 60638581218.272499, Test MSE of 60638581845.547997
Epoch 33: training loss 66479481871.059
Test Loss of 57709745152.236870, Test MSE of 57709745838.185516
Epoch 34: training loss 63873610232.471
Test Loss of 55256555614.749016, Test MSE of 55256555570.333122
Epoch 35: training loss 61593024026.353
Test Loss of 51955571771.455009, Test MSE of 51955571090.549225
Epoch 36: training loss 58790151115.294
Test Loss of 50088745900.383995, Test MSE of 50088745746.612183
Epoch 37: training loss 55459324184.471
Test Loss of 48058201459.653015, Test MSE of 48058201015.738625
Epoch 38: training loss 53409633618.824
Test Loss of 48247494184.623642, Test MSE of 48247494269.185089
Epoch 39: training loss 51872118426.353
Test Loss of 45619125834.259544, Test MSE of 45619126184.454041
Epoch 40: training loss 49312464301.176
Test Loss of 44435722897.558174, Test MSE of 44435723627.652092
Epoch 41: training loss 47424658202.353
Test Loss of 41730270908.432106, Test MSE of 41730271585.300644
Epoch 42: training loss 44789062896.941
Test Loss of 40598074188.213737, Test MSE of 40598073852.347046
Epoch 43: training loss 42363579858.824
Test Loss of 36642837534.556557, Test MSE of 36642837674.319023
Epoch 44: training loss 40861268126.118
Test Loss of 36734850837.496185, Test MSE of 36734850366.672089
Epoch 45: training loss 38656871476.706
Test Loss of 32132787182.708305, Test MSE of 32132786976.421753
Epoch 46: training loss 36910511081.412
Test Loss of 34348399334.358547, Test MSE of 34348399556.214306
Epoch 47: training loss 35499779471.059
Test Loss of 32481465186.006016, Test MSE of 32481465281.570389
Epoch 48: training loss 33745296007.529
Test Loss of 29775258461.268562, Test MSE of 29775258996.073864
Epoch 49: training loss 32215109888.000
Test Loss of 28064340516.123062, Test MSE of 28064340866.260410
Epoch 50: training loss 30936221327.059
Test Loss of 26904660594.764748, Test MSE of 26904660652.984501
Epoch 51: training loss 29483515783.529
Test Loss of 28090794109.068703, Test MSE of 28090794181.916740
Epoch 52: training loss 28464767823.059
Test Loss of 24682726293.170483, Test MSE of 24682726626.464001
Epoch 53: training loss 27113223070.118
Test Loss of 23289798276.056442, Test MSE of 23289798104.804195
Epoch 54: training loss 26204232222.118
Test Loss of 23568787211.073792, Test MSE of 23568787397.002392
Epoch 55: training loss 24666612517.647
Test Loss of 24356050671.122833, Test MSE of 24356050831.988895
Epoch 56: training loss 23961003753.412
Test Loss of 23257699443.830673, Test MSE of 23257699459.320827
Epoch 57: training loss 22718063623.529
Test Loss of 22569518493.816330, Test MSE of 22569518791.531822
Epoch 58: training loss 22179873600.000
Test Loss of 21452394147.797363, Test MSE of 21452394170.075001
Epoch 59: training loss 21720143273.412
Test Loss of 22663642295.576221, Test MSE of 22663642499.753593
Epoch 60: training loss 20331799055.059
Test Loss of 18931100243.497570, Test MSE of 18931100359.555168
Epoch 61: training loss 19944588344.471
Test Loss of 23169159175.106174, Test MSE of 23169158975.069870
Epoch 62: training loss 19226950177.882
Test Loss of 20906590721.065926, Test MSE of 20906590798.958500
Epoch 63: training loss 18712516303.059
Test Loss of 21609914864.011105, Test MSE of 21609914748.707897
Epoch 64: training loss 17738279356.235
Test Loss of 19399539773.823734, Test MSE of 19399539384.245560
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19399539384.24556, 'MSE - std': 0.0, 'R2 - mean': 0.8489336819808673, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005360 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[131]	valid_0's l2: 1.88763e+10
Model Interpreting...
[(27,), (26,), (26,), (26,), (26,)]

Train embedding model...
Epoch 1: training loss 427916936854.588
Test Loss of 424555807477.992126, Test MSE of 424555810898.687012
Epoch 2: training loss 427896732009.412
Test Loss of 424538972533.311096, Test MSE of 424538966351.150696
Epoch 3: training loss 427870740480.000
Test Loss of 424517168267.991699, Test MSE of 424517170986.708008
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888267264.000
Test Loss of 424522537838.797119, Test MSE of 424522537979.074585
Epoch 2: training loss 427875832289.882
Test Loss of 424522873871.870483, Test MSE of 424522870723.061279
Epoch 3: training loss 424231717225.412
Test Loss of 412399153757.446228, Test MSE of 412399154091.387695
Epoch 4: training loss 397995793347.765
Test Loss of 372606143849.704346, Test MSE of 372606152461.341675
Epoch 5: training loss 334110212698.353
Test Loss of 290800636606.090210, Test MSE of 290800638705.878784
Epoch 6: training loss 252536476370.824
Test Loss of 214981342918.854492, Test MSE of 214981349123.579224
Epoch 7: training loss 182708052449.882
Test Loss of 154653915029.407349, Test MSE of 154653915828.884979
Epoch 8: training loss 146384725594.353
Test Loss of 135295401519.729813, Test MSE of 135295404328.273132
Epoch 9: training loss 136401006742.588
Test Loss of 129499463088.766129, Test MSE of 129499463769.407471
Epoch 10: training loss 133754079503.059
Test Loss of 126623475685.944016, Test MSE of 126623478635.772400
Epoch 11: training loss 130755696941.176
Test Loss of 124142997724.765213, Test MSE of 124142998058.090637
Epoch 12: training loss 129017318400.000
Test Loss of 121415986899.645615, Test MSE of 121415984553.346085
Epoch 13: training loss 125552415171.765
Test Loss of 119517191413.163086, Test MSE of 119517193009.786179
Epoch 14: training loss 122431052739.765
Test Loss of 115944095548.817032, Test MSE of 115944094964.611832
Epoch 15: training loss 118256553773.176
Test Loss of 113620878464.148041, Test MSE of 113620881331.805435
Epoch 16: training loss 116639083640.471
Test Loss of 110288246161.972702, Test MSE of 110288246498.352905
Epoch 17: training loss 112941132047.059
Test Loss of 105832518000.573669, Test MSE of 105832515386.711197
Epoch 18: training loss 110477825054.118
Test Loss of 104745038623.681702, Test MSE of 104745036569.182541
Epoch 19: training loss 107188066002.824
Test Loss of 100436627618.968307, Test MSE of 100436628552.529785
Epoch 20: training loss 104300624941.176
Test Loss of 97111926676.459869, Test MSE of 97111925018.477539
Epoch 21: training loss 101697216361.412
Test Loss of 94527871441.691422, Test MSE of 94527870547.047256
Epoch 22: training loss 97478740419.765
Test Loss of 92888069204.563492, Test MSE of 92888069943.909760
Epoch 23: training loss 94073716916.706
Test Loss of 87586951338.548233, Test MSE of 87586952414.583466
Epoch 24: training loss 91480738544.941
Test Loss of 86347766842.507523, Test MSE of 86347768845.678986
Epoch 25: training loss 88105974723.765
Test Loss of 84990450206.438126, Test MSE of 84990447691.190048
Epoch 26: training loss 84399130352.941
Test Loss of 82354294550.917419, Test MSE of 82354293136.126587
Epoch 27: training loss 81145339407.059
Test Loss of 77463108840.845703, Test MSE of 77463107420.853363
Epoch 28: training loss 78553613161.412
Test Loss of 73821718991.796432, Test MSE of 73821718802.543289
Epoch 29: training loss 75390426428.235
Test Loss of 72224661487.892670, Test MSE of 72224661351.535507
Epoch 30: training loss 73485420679.529
Test Loss of 70029920798.438126, Test MSE of 70029919440.833740
Epoch 31: training loss 69532839213.176
Test Loss of 67708994612.348831, Test MSE of 67708992819.273613
Epoch 32: training loss 67569456368.941
Test Loss of 64018223935.896370, Test MSE of 64018222282.359840
Epoch 33: training loss 63664050763.294
Test Loss of 62442771039.341202, Test MSE of 62442770691.206017
Epoch 34: training loss 61602963290.353
Test Loss of 58949369058.213280, Test MSE of 58949368844.352753
Epoch 35: training loss 59118193859.765
Test Loss of 55872580141.597961, Test MSE of 55872580909.482277
Epoch 36: training loss 56196824982.588
Test Loss of 54133928092.335876, Test MSE of 54133926371.966606
Epoch 37: training loss 53941587049.412
Test Loss of 51634139084.361786, Test MSE of 51634138945.214508
Epoch 38: training loss 51028810992.941
Test Loss of 48163074547.327316, Test MSE of 48163072700.837288
Epoch 39: training loss 48362185547.294
Test Loss of 49394951002.426094, Test MSE of 49394951818.534088
Epoch 40: training loss 47037432026.353
Test Loss of 45818171847.742775, Test MSE of 45818171944.467712
Epoch 41: training loss 44343162044.235
Test Loss of 42567496216.753181, Test MSE of 42567495823.655960
Epoch 42: training loss 41452533714.824
Test Loss of 43070293181.261162, Test MSE of 43070291986.793060
Epoch 43: training loss 40626295823.059
Test Loss of 40566945469.379601, Test MSE of 40566945665.523697
Epoch 44: training loss 38803314560.000
Test Loss of 41040301898.555634, Test MSE of 41040301779.166588
Epoch 45: training loss 36827858612.706
Test Loss of 36410399478.465881, Test MSE of 36410398289.690308
Epoch 46: training loss 34814175661.176
Test Loss of 35771735632.891975, Test MSE of 35771735639.247414
Epoch 47: training loss 33011606384.941
Test Loss of 35571117918.926674, Test MSE of 35571117535.209618
Epoch 48: training loss 31753168195.765
Test Loss of 35231005074.683319, Test MSE of 35231005004.487823
Epoch 49: training loss 30146333583.059
Test Loss of 32944659316.482071, Test MSE of 32944659010.676846
Epoch 50: training loss 28666762646.588
Test Loss of 34762452298.437195, Test MSE of 34762452299.609657
Epoch 51: training loss 27761344677.647
Test Loss of 31793446998.695351, Test MSE of 31793446831.845978
Epoch 52: training loss 26009295047.529
Test Loss of 30626905464.864216, Test MSE of 30626906845.856796
Epoch 53: training loss 24782950949.647
Test Loss of 32010667535.041405, Test MSE of 32010668731.800976
Epoch 54: training loss 23602471311.059
Test Loss of 29320530109.971779, Test MSE of 29320530007.726833
Epoch 55: training loss 22356578642.824
Test Loss of 30418139820.087902, Test MSE of 30418139564.159309
Epoch 56: training loss 21724527954.824
Test Loss of 29528656927.504047, Test MSE of 29528657958.450779
Epoch 57: training loss 20962493793.882
Test Loss of 26609099069.882950, Test MSE of 26609099425.656445
Epoch 58: training loss 19988444160.000
Test Loss of 26630221792.259079, Test MSE of 26630221415.797642
Epoch 59: training loss 19090554816.000
Test Loss of 25704508538.699978, Test MSE of 25704508171.412754
Epoch 60: training loss 18265927706.353
Test Loss of 25340258162.350220, Test MSE of 25340258397.605888
Epoch 61: training loss 17775792839.529
Test Loss of 23374034410.799908, Test MSE of 23374034142.906353
Epoch 62: training loss 17013527168.000
Test Loss of 26341305048.619942, Test MSE of 26341305523.811901
Epoch 63: training loss 16351844419.765
Test Loss of 24400259907.449455, Test MSE of 24400260068.211029
Epoch 64: training loss 15772944613.647
Test Loss of 23136332993.998611, Test MSE of 23136332785.043324
Epoch 65: training loss 15241048658.824
Test Loss of 25339098371.375435, Test MSE of 25339097642.475418
Epoch 66: training loss 14847514164.706
Test Loss of 26006043702.243813, Test MSE of 26006043423.141678
Epoch 67: training loss 14188852329.412
Test Loss of 22581723641.486004, Test MSE of 22581723520.610855
Epoch 68: training loss 13965111853.176
Test Loss of 28031217372.173027, Test MSE of 28031216844.168228
Epoch 69: training loss 13604569795.765
Test Loss of 25060031383.539207, Test MSE of 25060031500.872334
Epoch 70: training loss 13033859162.353
Test Loss of 22828177252.848484, Test MSE of 22828177467.643715
Epoch 71: training loss 13047768820.706
Test Loss of 23198511755.399490, Test MSE of 23198511056.528790
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21299025220.387177, 'MSE - std': 1899485836.141615, 'R2 - mean': 0.8416558575934723, 'R2 - std': 0.007277824387394938} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005361 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.45492e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926705272.471
Test Loss of 447256547328.947510, Test MSE of 447256545430.193481
Epoch 2: training loss 421905816877.176
Test Loss of 447237901389.457336, Test MSE of 447237898108.988770
Epoch 3: training loss 421879100476.235
Test Loss of 447213467497.822815, Test MSE of 447213463424.463135
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897838471.529
Test Loss of 447221972912.410828, Test MSE of 447221975347.116760
Epoch 2: training loss 421885551676.235
Test Loss of 447222306110.593567, Test MSE of 447222310324.274353
Epoch 3: training loss 418166772073.412
Test Loss of 435451748645.721924, Test MSE of 435451752090.998169
Epoch 4: training loss 391852558095.059
Test Loss of 393589603211.221863, Test MSE of 393589603475.142395
Epoch 5: training loss 327826416820.706
Test Loss of 309402704519.609558, Test MSE of 309402704964.567139
Epoch 6: training loss 246260308269.176
Test Loss of 230329698003.171875, Test MSE of 230329702491.255829
Epoch 7: training loss 178270571038.118
Test Loss of 169863021718.887817, Test MSE of 169863024872.364197
Epoch 8: training loss 144012089584.941
Test Loss of 147398834033.402740, Test MSE of 147398833568.015320
Epoch 9: training loss 134372648176.941
Test Loss of 140051258988.369202, Test MSE of 140051261061.610809
Epoch 10: training loss 130254476438.588
Test Loss of 137126533854.778625, Test MSE of 137126534006.739914
Epoch 11: training loss 128078496888.471
Test Loss of 134204113680.521866, Test MSE of 134204113626.411514
Epoch 12: training loss 124965961035.294
Test Loss of 131247704585.593338, Test MSE of 131247701531.389023
Epoch 13: training loss 122125629319.529
Test Loss of 128142315107.841782, Test MSE of 128142316859.360718
Epoch 14: training loss 120112086106.353
Test Loss of 125569303664.988205, Test MSE of 125569303974.027802
Epoch 15: training loss 117387709801.412
Test Loss of 123004986001.558182, Test MSE of 123004989785.419098
Epoch 16: training loss 114079840828.235
Test Loss of 120947545110.976639, Test MSE of 120947544206.372025
Epoch 17: training loss 110738004148.706
Test Loss of 117698715388.387695, Test MSE of 117698718012.228409
Epoch 18: training loss 108035152052.706
Test Loss of 113301578014.142029, Test MSE of 113301576820.847549
Epoch 19: training loss 104431351024.941
Test Loss of 111127770739.475357, Test MSE of 111127772995.250092
Epoch 20: training loss 101338403448.471
Test Loss of 108551912808.756882, Test MSE of 108551913535.521362
Epoch 21: training loss 98255788935.529
Test Loss of 104942621605.277817, Test MSE of 104942620910.892227
Epoch 22: training loss 94927269857.882
Test Loss of 101773485324.376587, Test MSE of 101773484266.978226
Epoch 23: training loss 93178076099.765
Test Loss of 98629775624.349762, Test MSE of 98629773653.708359
Epoch 24: training loss 90227884544.000
Test Loss of 94015329981.379593, Test MSE of 94015332654.545517
Epoch 25: training loss 86414474450.824
Test Loss of 93270784048.795746, Test MSE of 93270783913.996964
Epoch 26: training loss 83496694814.118
Test Loss of 87336917764.678238, Test MSE of 87336918353.946289
Epoch 27: training loss 80167692754.824
Test Loss of 85348875634.231781, Test MSE of 85348875453.681503
Epoch 28: training loss 77512161566.118
Test Loss of 79779817279.185745, Test MSE of 79779817230.651230
Epoch 29: training loss 72480926554.353
Test Loss of 79933366829.124222, Test MSE of 79933367691.261383
Epoch 30: training loss 71157690624.000
Test Loss of 77899314757.995834, Test MSE of 77899314882.764389
Epoch 31: training loss 68464548728.471
Test Loss of 72974333403.876938, Test MSE of 72974333918.276367
Epoch 32: training loss 65113504888.471
Test Loss of 70986954848.170258, Test MSE of 70986955712.009354
Epoch 33: training loss 63215661327.059
Test Loss of 65657443809.561882, Test MSE of 65657445321.765816
Epoch 34: training loss 59691800380.235
Test Loss of 63895191127.524406, Test MSE of 63895192737.090492
Epoch 35: training loss 57312610477.176
Test Loss of 64441007919.552162, Test MSE of 64441007965.642281
Epoch 36: training loss 55245694027.294
Test Loss of 61086691834.907242, Test MSE of 61086692074.535858
Epoch 37: training loss 52622701402.353
Test Loss of 54122059834.270645, Test MSE of 54122060997.226341
Epoch 38: training loss 49885325025.882
Test Loss of 54185946771.216286, Test MSE of 54185946588.746445
Epoch 39: training loss 48094631883.294
Test Loss of 50871877810.128151, Test MSE of 50871878579.895905
Epoch 40: training loss 46009504609.882
Test Loss of 50104870546.742538, Test MSE of 50104871424.014282
Epoch 41: training loss 44136726415.059
Test Loss of 49644962204.158226, Test MSE of 49644962693.845840
Epoch 42: training loss 41563186319.059
Test Loss of 48959012608.177658, Test MSE of 48959013295.985428
Epoch 43: training loss 39794745291.294
Test Loss of 44106277595.462410, Test MSE of 44106277109.044922
Epoch 44: training loss 38021422832.941
Test Loss of 44569490606.575066, Test MSE of 44569490649.251877
Epoch 45: training loss 36038811422.118
Test Loss of 40315002694.291924, Test MSE of 40315003252.495476
Epoch 46: training loss 34682235309.176
Test Loss of 40719620122.055977, Test MSE of 40719620802.820541
Epoch 47: training loss 32884434680.471
Test Loss of 39417227910.662041, Test MSE of 39417227821.255249
Epoch 48: training loss 31663532280.471
Test Loss of 37153404777.822807, Test MSE of 37153404534.881653
Epoch 49: training loss 30247191446.588
Test Loss of 35059938811.144112, Test MSE of 35059937942.988289
Epoch 50: training loss 28764536350.118
Test Loss of 32988045132.924358, Test MSE of 32988045275.107182
Epoch 51: training loss 27419582328.471
Test Loss of 34608888362.755493, Test MSE of 34608888142.166916
Epoch 52: training loss 26364380800.000
Test Loss of 30940254718.697201, Test MSE of 30940254582.141590
Epoch 53: training loss 25257316291.765
Test Loss of 28834776480.658802, Test MSE of 28834776422.015774
Epoch 54: training loss 24293556525.176
Test Loss of 33887941005.945869, Test MSE of 33887941147.347706
Epoch 55: training loss 23241968997.647
Test Loss of 27525409020.979874, Test MSE of 27525409413.909775
Epoch 56: training loss 22024270125.176
Test Loss of 28432448458.229935, Test MSE of 28432448933.623020
Epoch 57: training loss 21208956992.000
Test Loss of 28164101342.423317, Test MSE of 28164101536.412350
Epoch 58: training loss 20083381248.000
Test Loss of 28804206705.698822, Test MSE of 28804206708.843899
Epoch 59: training loss 19630732468.706
Test Loss of 26017153171.808468, Test MSE of 26017153402.663643
Epoch 60: training loss 18912387828.706
Test Loss of 25742541369.915337, Test MSE of 25742541747.929993
Epoch 61: training loss 18443472530.824
Test Loss of 24615570824.024055, Test MSE of 24615571091.604824
Epoch 62: training loss 18274379459.765
Test Loss of 26069771199.333797, Test MSE of 26069771425.110897
Epoch 63: training loss 17000188634.353
Test Loss of 27672482296.538513, Test MSE of 27672482346.789818
Epoch 64: training loss 16734710147.765
Test Loss of 23389309920.022205, Test MSE of 23389309963.772404
Epoch 65: training loss 16139756939.294
Test Loss of 27404411539.926903, Test MSE of 27404411014.462593
Epoch 66: training loss 15677821481.412
Test Loss of 24480747872.466343, Test MSE of 24480747884.040112
Epoch 67: training loss 15459254253.176
Test Loss of 25448385904.810547, Test MSE of 25448385514.585159
Epoch 68: training loss 14901432636.235
Test Loss of 24673430963.608604, Test MSE of 24673431302.815395
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22423827247.86325, 'MSE - std': 2221648823.8461494, 'R2 - mean': 0.8396874371022495, 'R2 - std': 0.006562050772157472} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005342 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[151]	valid_0's l2: 1.28279e+10
Model Interpreting...
[(31,), (30,), (30,), (30,), (30,)]

Train embedding model...
Epoch 1: training loss 430108484788.706
Test Loss of 410761919905.466003, Test MSE of 410761922316.541748
Epoch 2: training loss 430086382290.824
Test Loss of 410742205078.448853, Test MSE of 410742203791.606323
Epoch 3: training loss 430058700077.176
Test Loss of 410717895559.167053, Test MSE of 410717902664.166748
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076193129.412
Test Loss of 410721794268.816284, Test MSE of 410721796370.318115
Epoch 2: training loss 430060676517.647
Test Loss of 410723082198.300781, Test MSE of 410723085147.059204
Epoch 3: training loss 426480739026.824
Test Loss of 399426122336.429443, Test MSE of 399426123967.173645
Epoch 4: training loss 400447823992.471
Test Loss of 358519379237.316040, Test MSE of 358519381801.356262
Epoch 5: training loss 336458478772.706
Test Loss of 275885166590.104553, Test MSE of 275885170669.795715
Epoch 6: training loss 254566698646.588
Test Loss of 198427473526.700592, Test MSE of 198427472874.613434
Epoch 7: training loss 186488387523.765
Test Loss of 139045527760.496063, Test MSE of 139045527607.422821
Epoch 8: training loss 151073153596.235
Test Loss of 117982001380.397964, Test MSE of 117981999548.290421
Epoch 9: training loss 140755047092.706
Test Loss of 112380300278.996765, Test MSE of 112380301342.956696
Epoch 10: training loss 136375332472.471
Test Loss of 109470919000.018509, Test MSE of 109470919098.353561
Epoch 11: training loss 134756492257.882
Test Loss of 107170303562.158264, Test MSE of 107170300625.031418
Epoch 12: training loss 132118833453.176
Test Loss of 104833353203.916702, Test MSE of 104833352683.128296
Epoch 13: training loss 129005382957.176
Test Loss of 102158343843.242950, Test MSE of 102158343107.401001
Epoch 14: training loss 125554571294.118
Test Loss of 99034780627.931519, Test MSE of 99034781404.840500
Epoch 15: training loss 122933121174.588
Test Loss of 97279035152.229523, Test MSE of 97279035919.719406
Epoch 16: training loss 120181379252.706
Test Loss of 94220444986.165665, Test MSE of 94220443693.212692
Epoch 17: training loss 117235985859.765
Test Loss of 92600523306.409988, Test MSE of 92600523510.472214
Epoch 18: training loss 113545602198.588
Test Loss of 89385131497.491898, Test MSE of 89385133094.913681
Epoch 19: training loss 112084853579.294
Test Loss of 87195806438.056458, Test MSE of 87195806175.031921
Epoch 20: training loss 107261246674.824
Test Loss of 85320219151.874130, Test MSE of 85320220210.489044
Epoch 21: training loss 102783742674.824
Test Loss of 81824214003.205917, Test MSE of 81824215497.370163
Epoch 22: training loss 100292535612.235
Test Loss of 80562625885.230911, Test MSE of 80562626073.061356
Epoch 23: training loss 97858229775.059
Test Loss of 77963894742.300781, Test MSE of 77963895054.071854
Epoch 24: training loss 94931130880.000
Test Loss of 73557464880.451645, Test MSE of 73557464908.419113
Epoch 25: training loss 90547325500.235
Test Loss of 71335566309.937988, Test MSE of 71335567647.175705
Epoch 26: training loss 87645551134.118
Test Loss of 70752978533.641830, Test MSE of 70752979838.000519
Epoch 27: training loss 84443150260.706
Test Loss of 67735090658.857933, Test MSE of 67735090295.718536
Epoch 28: training loss 82399943634.824
Test Loss of 64853304346.062012, Test MSE of 64853303753.878502
Epoch 29: training loss 78849882096.941
Test Loss of 63329026506.691345, Test MSE of 63329027393.925529
Epoch 30: training loss 75275711578.353
Test Loss of 60129157430.848679, Test MSE of 60129157148.723511
Epoch 31: training loss 72733083979.294
Test Loss of 60439039175.492828, Test MSE of 60439038273.395020
Epoch 32: training loss 69952685161.412
Test Loss of 53394543880.884773, Test MSE of 53394543302.497322
Epoch 33: training loss 66795351280.941
Test Loss of 53606468487.167053, Test MSE of 53606468798.190292
Epoch 34: training loss 65147479235.765
Test Loss of 53514781733.434525, Test MSE of 53514781265.538109
Epoch 35: training loss 62153301263.059
Test Loss of 51159441207.085609, Test MSE of 51159442044.456543
Epoch 36: training loss 59229638761.412
Test Loss of 48524577824.695976, Test MSE of 48524576993.641838
Epoch 37: training loss 56836598166.588
Test Loss of 48880417705.284592, Test MSE of 48880417729.368484
Epoch 38: training loss 54632143909.647
Test Loss of 43510135878.604347, Test MSE of 43510135534.241684
Epoch 39: training loss 52387644092.235
Test Loss of 44890871429.863953, Test MSE of 44890872237.782608
Epoch 40: training loss 48950330345.412
Test Loss of 42333003569.399353, Test MSE of 42333003827.334541
Epoch 41: training loss 47088940566.588
Test Loss of 37452255938.991211, Test MSE of 37452256687.173607
Epoch 42: training loss 45350464813.176
Test Loss of 38015937989.478943, Test MSE of 38015938657.552582
Epoch 43: training loss 43304499870.118
Test Loss of 38791162029.430817, Test MSE of 38791162065.569962
Epoch 44: training loss 41091903653.647
Test Loss of 36186879640.818138, Test MSE of 36186879456.849075
Epoch 45: training loss 39116138428.235
Test Loss of 36557411033.736237, Test MSE of 36557410927.122665
Epoch 46: training loss 37296831766.588
Test Loss of 33694285376.681168, Test MSE of 33694285533.774597
Epoch 47: training loss 36194278264.471
Test Loss of 32631798086.959740, Test MSE of 32631797647.317184
Epoch 48: training loss 34038985404.235
Test Loss of 30260335175.788986, Test MSE of 30260335125.576199
Epoch 49: training loss 32871426823.529
Test Loss of 29066756863.644608, Test MSE of 29066756896.761124
Epoch 50: training loss 31021717865.412
Test Loss of 30026342799.933365, Test MSE of 30026342761.327816
Epoch 51: training loss 30047560967.529
Test Loss of 28773358594.369274, Test MSE of 28773358078.576202
Epoch 52: training loss 28811798637.176
Test Loss of 27516818096.037022, Test MSE of 27516818304.836475
Epoch 53: training loss 27115143002.353
Test Loss of 24660519206.737621, Test MSE of 24660519364.699055
Epoch 54: training loss 26117804604.235
Test Loss of 26917096674.976398, Test MSE of 26917097256.879868
Epoch 55: training loss 24516981895.529
Test Loss of 26433011019.224434, Test MSE of 26433011254.109436
Epoch 56: training loss 23864139742.118
Test Loss of 22209067982.245258, Test MSE of 22209067813.673519
Epoch 57: training loss 23005230991.059
Test Loss of 24574681231.104118, Test MSE of 24574680923.029499
Epoch 58: training loss 22295820129.882
Test Loss of 24374968787.220730, Test MSE of 24374968796.907135
Epoch 59: training loss 21449196382.118
Test Loss of 21075143271.063396, Test MSE of 21075143761.599289
Epoch 60: training loss 20686047996.235
Test Loss of 23220090067.813049, Test MSE of 23220090488.958447
Epoch 61: training loss 19971339134.118
Test Loss of 20735437620.716335, Test MSE of 20735437760.657944
Epoch 62: training loss 19200728933.647
Test Loss of 23612543171.228134, Test MSE of 23612542980.341167
Epoch 63: training loss 18771044054.588
Test Loss of 22923820818.124943, Test MSE of 22923820362.081944
Epoch 64: training loss 18176066176.000
Test Loss of 21068022632.840351, Test MSE of 21068022453.979351
Epoch 65: training loss 17568811734.588
Test Loss of 20901918657.451180, Test MSE of 20901918750.087166
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22043350123.419228, 'MSE - std': 2033735761.8550906, 'R2 - mean': 0.83663697065007, 'R2 - std': 0.0077596017603930485} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005254 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043797443.765
Test Loss of 431613254190.200806, Test MSE of 431613257719.197571
Epoch 2: training loss 424024752248.471
Test Loss of 431593491841.243896, Test MSE of 431593491969.760986
Epoch 3: training loss 423998377020.235
Test Loss of 431567252224.592346, Test MSE of 431567254126.240967
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008647378.824
Test Loss of 431567410711.929688, Test MSE of 431567409475.281616
Epoch 2: training loss 424000701500.235
Test Loss of 431568286198.285950, Test MSE of 431568286714.509399
Epoch 3: training loss 420317932001.882
Test Loss of 419539168500.982849, Test MSE of 419539171276.983948
Epoch 4: training loss 393998544775.529
Test Loss of 377401097383.270691, Test MSE of 377401103962.326355
Epoch 5: training loss 330212202857.412
Test Loss of 293092432005.627014, Test MSE of 293092431641.706055
Epoch 6: training loss 248960815405.176
Test Loss of 214081836590.674683, Test MSE of 214081836785.890961
Epoch 7: training loss 182702463638.588
Test Loss of 152953267900.831085, Test MSE of 152953270150.497681
Epoch 8: training loss 147413821349.647
Test Loss of 131138146715.305878, Test MSE of 131138147633.564148
Epoch 9: training loss 138030834808.471
Test Loss of 123735417370.298935, Test MSE of 123735417007.191315
Epoch 10: training loss 133416113543.529
Test Loss of 121060233687.959274, Test MSE of 121060235855.367126
Epoch 11: training loss 131043088128.000
Test Loss of 118621847131.217026, Test MSE of 118621847891.845734
Epoch 12: training loss 128224876935.529
Test Loss of 115517817908.597870, Test MSE of 115517819468.973923
Epoch 13: training loss 126504005391.059
Test Loss of 113710612821.175385, Test MSE of 113710613727.057800
Epoch 14: training loss 122863015393.882
Test Loss of 110344889987.968536, Test MSE of 110344886597.779907
Epoch 15: training loss 120074768745.412
Test Loss of 108572261403.009720, Test MSE of 108572262349.478943
Epoch 16: training loss 117413673170.824
Test Loss of 103875922548.331329, Test MSE of 103875923777.822510
Epoch 17: training loss 114090658123.294
Test Loss of 103254130346.824615, Test MSE of 103254128732.791321
Epoch 18: training loss 109585374780.235
Test Loss of 99219608251.409531, Test MSE of 99219609266.373199
Epoch 19: training loss 106808824229.647
Test Loss of 97034255618.250809, Test MSE of 97034256357.454071
Epoch 20: training loss 104243372227.765
Test Loss of 93246931596.024063, Test MSE of 93246930361.498474
Epoch 21: training loss 101735026507.294
Test Loss of 89250003460.501617, Test MSE of 89250003538.267807
Epoch 22: training loss 97532795949.176
Test Loss of 87690250019.183716, Test MSE of 87690250728.905014
Epoch 23: training loss 94920497950.118
Test Loss of 84759924555.935211, Test MSE of 84759924538.467880
Epoch 24: training loss 91094408478.118
Test Loss of 81512548225.954651, Test MSE of 81512545435.072906
Epoch 25: training loss 88466959149.176
Test Loss of 77953445215.126328, Test MSE of 77953445476.356720
Epoch 26: training loss 84897594925.176
Test Loss of 77183529574.115692, Test MSE of 77183530677.585876
Epoch 27: training loss 83544872244.706
Test Loss of 73629174771.205917, Test MSE of 73629173649.892334
Epoch 28: training loss 79871946526.118
Test Loss of 68703321018.343361, Test MSE of 68703320094.704796
Epoch 29: training loss 76310831164.235
Test Loss of 66050024074.128647, Test MSE of 66050024795.910950
Epoch 30: training loss 73446633283.765
Test Loss of 61161401499.898193, Test MSE of 61161401231.715828
Epoch 31: training loss 71191423548.235
Test Loss of 60846865465.336418, Test MSE of 60846865281.842339
Epoch 32: training loss 68085448741.647
Test Loss of 59806588346.106430, Test MSE of 59806589110.161705
Epoch 33: training loss 65470094742.588
Test Loss of 57308517995.328087, Test MSE of 57308518310.867638
Epoch 34: training loss 62957729189.647
Test Loss of 54331802691.761223, Test MSE of 54331803314.433563
Epoch 35: training loss 60432100864.000
Test Loss of 53649576360.573807, Test MSE of 53649576960.364098
Epoch 36: training loss 57428998761.412
Test Loss of 50870442582.004631, Test MSE of 50870443647.662094
Epoch 37: training loss 55732466823.529
Test Loss of 47614485674.113838, Test MSE of 47614485087.879707
Epoch 38: training loss 52887544666.353
Test Loss of 45133851089.325310, Test MSE of 45133851003.953308
Epoch 39: training loss 50528443881.412
Test Loss of 45229043075.139290, Test MSE of 45229043772.707542
Epoch 40: training loss 47939279642.353
Test Loss of 41264568722.776489, Test MSE of 41264569173.668159
Epoch 41: training loss 46115861270.588
Test Loss of 41588255655.389175, Test MSE of 41588255494.169579
Epoch 42: training loss 44105730736.941
Test Loss of 39948682118.219345, Test MSE of 39948681974.413559
Epoch 43: training loss 42014023454.118
Test Loss of 39164381232.333176, Test MSE of 39164381424.522713
Epoch 44: training loss 40209417931.294
Test Loss of 33774355870.622860, Test MSE of 33774355842.468678
Epoch 45: training loss 38543204924.235
Test Loss of 34882624097.377144, Test MSE of 34882624425.772873
Epoch 46: training loss 36231514503.529
Test Loss of 32317564459.831558, Test MSE of 32317564026.348286
Epoch 47: training loss 35150754544.941
Test Loss of 31096342594.339657, Test MSE of 31096342832.408211
Epoch 48: training loss 33435542136.471
Test Loss of 29841086976.710781, Test MSE of 29841086962.993919
Epoch 49: training loss 32096937306.353
Test Loss of 31177048656.318371, Test MSE of 31177048324.358780
Epoch 50: training loss 30545427945.412
Test Loss of 25623767501.534473, Test MSE of 25623767847.877068
Epoch 51: training loss 29386937886.118
Test Loss of 26030323882.587692, Test MSE of 26030323582.258846
Epoch 52: training loss 27926137411.765
Test Loss of 24887289815.248497, Test MSE of 24887289068.390751
Epoch 53: training loss 27234661372.235
Test Loss of 27596521851.083759, Test MSE of 27596521778.770100
Epoch 54: training loss 25836964547.765
Test Loss of 23501595153.769550, Test MSE of 23501595494.661713
Epoch 55: training loss 25098802812.235
Test Loss of 25680646730.632114, Test MSE of 25680647110.803383
Epoch 56: training loss 23739879567.059
Test Loss of 23564713165.179085, Test MSE of 23564712987.385021
Epoch 57: training loss 23083086490.353
Test Loss of 26067859148.942158, Test MSE of 26067858833.257614
Epoch 58: training loss 22026538887.529
Test Loss of 25120109650.924572, Test MSE of 25120110055.002930
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22658702109.73597, 'MSE - std': 2196246158.2196283, 'R2 - mean': 0.8317899936138435, 'R2 - std': 0.011922327002005212} 
 

Saving model.....
Results After CV: {'MSE - mean': 22658702109.73597, 'MSE - std': 2196246158.2196283, 'R2 - mean': 0.8317899936138435, 'R2 - std': 0.011922327002005212}
Train time: 101.94619093700021
Inference time: 0.07088421240005119
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 37 finished with value: 22658702109.73597 and parameters: {'n_trees': 200, 'maxleaf': 128, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005428 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525127107.765
Test Loss of 418111829983.311584, Test MSE of 418111835032.258423
Epoch 2: training loss 427504350750.118
Test Loss of 418093764264.061096, Test MSE of 418093763195.002319
Epoch 3: training loss 427477299922.824
Test Loss of 418070618481.521179, Test MSE of 418070618747.263367
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494758159.059
Test Loss of 418072757360.988220, Test MSE of 418072755134.094849
Epoch 2: training loss 427483059621.647
Test Loss of 418073908455.424500, Test MSE of 418073912123.164795
Epoch 3: training loss 427482585208.471
Test Loss of 418073290153.659973, Test MSE of 418073289619.954651
Epoch 4: training loss 427482199702.588
Test Loss of 418073672318.134644, Test MSE of 418073675231.361023
Epoch 5: training loss 420422117135.059
Test Loss of 395977201690.055969, Test MSE of 395977207170.009705
Epoch 6: training loss 374382901850.353
Test Loss of 329140393892.567200, Test MSE of 329140399193.292114
Epoch 7: training loss 295939756995.765
Test Loss of 243276468384.125824, Test MSE of 243276469444.356750
Epoch 8: training loss 217215483964.235
Test Loss of 173834365110.391846, Test MSE of 173834364449.330322
Epoch 9: training loss 160120923256.471
Test Loss of 125835016155.521622, Test MSE of 125835014379.291199
Epoch 10: training loss 139690917436.235
Test Loss of 117986556714.577835, Test MSE of 117986556112.002701
Epoch 11: training loss 135727740777.412
Test Loss of 115308347615.607681, Test MSE of 115308348632.413742
Epoch 12: training loss 132421727412.706
Test Loss of 112061392487.157990, Test MSE of 112061390922.369659
Epoch 13: training loss 128954089261.176
Test Loss of 108740205091.175568, Test MSE of 108740202964.102280
Epoch 14: training loss 124349228574.118
Test Loss of 106258394513.498962, Test MSE of 106258392402.075302
Epoch 15: training loss 121388428897.882
Test Loss of 101894326501.766373, Test MSE of 101894328070.161697
Epoch 16: training loss 116758377938.824
Test Loss of 97926687901.046494, Test MSE of 97926689540.717896
Epoch 17: training loss 112933773206.588
Test Loss of 94823131386.611145, Test MSE of 94823132487.232071
Epoch 18: training loss 109829599171.765
Test Loss of 91177974248.431183, Test MSE of 91177974381.090683
Epoch 19: training loss 104860701861.647
Test Loss of 88246526600.083282, Test MSE of 88246525725.851547
Epoch 20: training loss 101772532525.176
Test Loss of 83769914193.898682, Test MSE of 83769913878.815262
Epoch 21: training loss 97880495164.235
Test Loss of 82865547525.981033, Test MSE of 82865548130.293716
Epoch 22: training loss 92714005052.235
Test Loss of 79328564735.881561, Test MSE of 79328565901.249542
Epoch 23: training loss 90631928508.235
Test Loss of 76504315969.850571, Test MSE of 76504316350.472794
Epoch 24: training loss 86074661857.882
Test Loss of 72314044112.803146, Test MSE of 72314044324.606323
Epoch 25: training loss 82974166339.765
Test Loss of 69661515908.648621, Test MSE of 69661516107.232803
Epoch 26: training loss 78089304696.471
Test Loss of 67416811723.236641, Test MSE of 67416811885.355064
Epoch 27: training loss 75868222418.824
Test Loss of 62500866696.557022, Test MSE of 62500866451.709045
Epoch 28: training loss 71952718230.588
Test Loss of 60923204081.432335, Test MSE of 60923205235.566429
Epoch 29: training loss 68456963297.882
Test Loss of 58545944368.973396, Test MSE of 58545944740.077408
Epoch 30: training loss 65118897219.765
Test Loss of 55719703192.190605, Test MSE of 55719702306.526024
Epoch 31: training loss 61875797187.765
Test Loss of 54987404672.917885, Test MSE of 54987404887.119545
Epoch 32: training loss 59257301345.882
Test Loss of 48677181036.842934, Test MSE of 48677181551.311478
Epoch 33: training loss 56430693187.765
Test Loss of 47996338409.082581, Test MSE of 47996338731.366348
Epoch 34: training loss 53023636762.353
Test Loss of 46527431519.163544, Test MSE of 46527431489.563988
Epoch 35: training loss 50461950441.412
Test Loss of 44402959406.663887, Test MSE of 44402960226.829102
Epoch 36: training loss 48427442285.176
Test Loss of 39912508029.187141, Test MSE of 39912508575.468292
Epoch 37: training loss 45971793483.294
Test Loss of 37562429782.991440, Test MSE of 37562430801.928635
Epoch 38: training loss 43385406516.706
Test Loss of 35797355789.797829, Test MSE of 35797355762.828300
Epoch 39: training loss 41266550859.294
Test Loss of 33744636242.727734, Test MSE of 33744635939.751057
Epoch 40: training loss 39194088485.647
Test Loss of 32095921126.654636, Test MSE of 32095921967.759941
Epoch 41: training loss 36977778288.941
Test Loss of 31187316310.576916, Test MSE of 31187316404.284237
Epoch 42: training loss 35574708239.059
Test Loss of 29287747701.488781, Test MSE of 29287747566.691284
Epoch 43: training loss 33311791762.824
Test Loss of 31779510282.896137, Test MSE of 31779510336.821751
Epoch 44: training loss 32153211565.176
Test Loss of 30521705917.557251, Test MSE of 30521705511.003170
Epoch 45: training loss 30534434703.059
Test Loss of 26678219725.546150, Test MSE of 26678219465.386108
Epoch 46: training loss 28950888914.824
Test Loss of 25548872274.550079, Test MSE of 25548871602.597458
Epoch 47: training loss 27444758106.353
Test Loss of 26889504428.324776, Test MSE of 26889504505.198704
Epoch 48: training loss 26513224101.647
Test Loss of 24025553444.123062, Test MSE of 24025553670.800739
Epoch 49: training loss 25116530492.235
Test Loss of 24596875064.553318, Test MSE of 24596875206.249943
Epoch 50: training loss 23860963068.235
Test Loss of 24031771907.849178, Test MSE of 24031771883.409496
Epoch 51: training loss 23327365699.765
Test Loss of 22469948935.461487, Test MSE of 22469948932.105583
Epoch 52: training loss 22279914270.118
Test Loss of 18845047417.397179, Test MSE of 18845047302.308750
Epoch 53: training loss 21868359736.471
Test Loss of 20038159493.832985, Test MSE of 20038159308.811996
Epoch 54: training loss 20544077037.176
Test Loss of 20376324510.053204, Test MSE of 20376324954.440941
Epoch 55: training loss 19873217600.000
Test Loss of 19811888423.853806, Test MSE of 19811888338.803329
Epoch 56: training loss 19309271977.412
Test Loss of 21352681027.627110, Test MSE of 21352681191.605808
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21352681191.60581, 'MSE - std': 0.0, 'R2 - mean': 0.8337243548126789, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005205 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918325157.647
Test Loss of 424557147638.406677, Test MSE of 424557154938.648071
Epoch 2: training loss 427897297739.294
Test Loss of 424540165044.437683, Test MSE of 424540168093.202026
Epoch 3: training loss 427868575382.588
Test Loss of 424516793160.423767, Test MSE of 424516794319.232178
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887445172.706
Test Loss of 424525004966.758240, Test MSE of 424524994543.463806
Epoch 2: training loss 427876583062.588
Test Loss of 424525554927.951904, Test MSE of 424525552328.659668
Epoch 3: training loss 427876030343.529
Test Loss of 424524680599.183899, Test MSE of 424524677718.562134
Epoch 4: training loss 427875560990.118
Test Loss of 424524100848.899353, Test MSE of 424524098217.919312
Epoch 5: training loss 421396158584.471
Test Loss of 404206102710.391846, Test MSE of 404206099697.805786
Epoch 6: training loss 376678739004.235
Test Loss of 339020461342.378906, Test MSE of 339020467830.315186
Epoch 7: training loss 297674433837.176
Test Loss of 254389605668.063843, Test MSE of 254389605490.605377
Epoch 8: training loss 218096377374.118
Test Loss of 184570046115.797363, Test MSE of 184570047438.507690
Epoch 9: training loss 157171047152.941
Test Loss of 137069044441.567429, Test MSE of 137069042735.853638
Epoch 10: training loss 137649875968.000
Test Loss of 129570661354.444595, Test MSE of 129570658394.026276
Epoch 11: training loss 133462774482.824
Test Loss of 126856882887.091370, Test MSE of 126856881615.963104
Epoch 12: training loss 130008500705.882
Test Loss of 124121938552.449692, Test MSE of 124121937350.987427
Epoch 13: training loss 127089358275.765
Test Loss of 119855986702.686096, Test MSE of 119855985943.784332
Epoch 14: training loss 123268792862.118
Test Loss of 116584468139.851028, Test MSE of 116584467001.187363
Epoch 15: training loss 118299697242.353
Test Loss of 113793188766.645386, Test MSE of 113793187761.312469
Epoch 16: training loss 115017820491.294
Test Loss of 110286052791.398560, Test MSE of 110286051574.230362
Epoch 17: training loss 111481431371.294
Test Loss of 107254184141.131622, Test MSE of 107254185550.393188
Epoch 18: training loss 106874307855.059
Test Loss of 102329470742.917419, Test MSE of 102329470516.580887
Epoch 19: training loss 103440530221.176
Test Loss of 100250551741.557251, Test MSE of 100250554721.388596
Epoch 20: training loss 99822366177.882
Test Loss of 96379485563.706680, Test MSE of 96379486689.617661
Epoch 21: training loss 95490886445.176
Test Loss of 93522552255.452225, Test MSE of 93522551656.039612
Epoch 22: training loss 92248351412.706
Test Loss of 89737145865.830215, Test MSE of 89737144413.982269
Epoch 23: training loss 88271172156.235
Test Loss of 83259329399.324539, Test MSE of 83259328809.857330
Epoch 24: training loss 84382362940.235
Test Loss of 80401211437.479523, Test MSE of 80401212228.528793
Epoch 25: training loss 80095212739.765
Test Loss of 75667565715.571594, Test MSE of 75667567265.074677
Epoch 26: training loss 76669260664.471
Test Loss of 73293607214.249359, Test MSE of 73293607885.061966
Epoch 27: training loss 73056777276.235
Test Loss of 69089219954.705536, Test MSE of 69089222323.934998
Epoch 28: training loss 69768202586.353
Test Loss of 68857726354.920197, Test MSE of 68857724917.995743
Epoch 29: training loss 66522931004.235
Test Loss of 64930972438.206802, Test MSE of 64930971963.560471
Epoch 30: training loss 63786362322.824
Test Loss of 60918991742.904465, Test MSE of 60918991247.999718
Epoch 31: training loss 59464113874.824
Test Loss of 59391659752.490402, Test MSE of 59391660392.903008
Epoch 32: training loss 57768140257.882
Test Loss of 57254666288.321999, Test MSE of 57254665707.397522
Epoch 33: training loss 54640475105.882
Test Loss of 53375872506.670364, Test MSE of 53375872907.941154
Epoch 34: training loss 51881014942.118
Test Loss of 49196694477.546150, Test MSE of 49196694757.819260
Epoch 35: training loss 48737635937.882
Test Loss of 45717492361.504509, Test MSE of 45717490493.528030
Epoch 36: training loss 45841451286.588
Test Loss of 44855263297.850563, Test MSE of 44855263305.318497
Epoch 37: training loss 43075779779.765
Test Loss of 43311623557.181587, Test MSE of 43311623281.350189
Epoch 38: training loss 40555620186.353
Test Loss of 42087994155.525330, Test MSE of 42087994382.915451
Epoch 39: training loss 39108021775.059
Test Loss of 36347681443.086746, Test MSE of 36347682410.417145
Epoch 40: training loss 37383844743.529
Test Loss of 37169466041.352768, Test MSE of 37169466182.631706
Epoch 41: training loss 35301100845.176
Test Loss of 34844942780.846634, Test MSE of 34844941983.866364
Epoch 42: training loss 33096783337.412
Test Loss of 34639502018.353920, Test MSE of 34639502111.459198
Epoch 43: training loss 31398198219.294
Test Loss of 31792101400.160999, Test MSE of 31792100971.772820
Epoch 44: training loss 29356468404.706
Test Loss of 31353092774.402962, Test MSE of 31353092374.880859
Epoch 45: training loss 27804304609.882
Test Loss of 29881197572.974323, Test MSE of 29881197295.416782
Epoch 46: training loss 26467024790.588
Test Loss of 29233545432.501503, Test MSE of 29233545982.524975
Epoch 47: training loss 24860429590.588
Test Loss of 29800956636.883644, Test MSE of 29800956082.401306
Epoch 48: training loss 23903148920.471
Test Loss of 26917339752.342354, Test MSE of 26917340209.797413
Epoch 49: training loss 22859958377.412
Test Loss of 28693502813.979179, Test MSE of 28693502243.173187
Epoch 50: training loss 21522624591.059
Test Loss of 27190270544.181355, Test MSE of 27190270117.093513
Epoch 51: training loss 21191044468.706
Test Loss of 26489480694.406662, Test MSE of 26489480673.202263
Epoch 52: training loss 19762639589.647
Test Loss of 26158777008.351608, Test MSE of 26158776883.436455
Epoch 53: training loss 19292029334.588
Test Loss of 26418999462.995144, Test MSE of 26419000010.201630
Epoch 54: training loss 18061009434.353
Test Loss of 24125107326.016193, Test MSE of 24125107299.622643
Epoch 55: training loss 17742472960.000
Test Loss of 25299531845.640526, Test MSE of 25299532073.557983
Epoch 56: training loss 16981925910.588
Test Loss of 23665104419.886189, Test MSE of 23665104756.302055
Epoch 57: training loss 16293328858.353
Test Loss of 24425812629.821884, Test MSE of 24425812506.712345
Epoch 58: training loss 15611388724.706
Test Loss of 22719200236.339581, Test MSE of 22719200082.163269
Epoch 59: training loss 15118967175.529
Test Loss of 25196740322.331715, Test MSE of 25196740045.285595
Epoch 60: training loss 14813279198.118
Test Loss of 24051934366.467731, Test MSE of 24051934763.919792
Epoch 61: training loss 14118210081.882
Test Loss of 25262315720.157299, Test MSE of 25262316133.007278
Epoch 62: training loss 13817090213.647
Test Loss of 22845753691.728893, Test MSE of 22845754149.551903
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22099217670.578857, 'MSE - std': 746536478.9730473, 'R2 - mean': 0.8353104190646261, 'R2 - std': 0.001586064251947128} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005375 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926368677.647
Test Loss of 447257638822.462158, Test MSE of 447257643353.428162
Epoch 2: training loss 421906092754.824
Test Loss of 447239927669.192688, Test MSE of 447239925500.082520
Epoch 3: training loss 421880267474.824
Test Loss of 447216725529.937561, Test MSE of 447216731645.867859
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899691670.588
Test Loss of 447224616940.576477, Test MSE of 447224617671.920715
Epoch 2: training loss 421888645722.353
Test Loss of 447225290083.071960, Test MSE of 447225289948.664307
Epoch 3: training loss 421888274191.059
Test Loss of 447224750635.229248, Test MSE of 447224747509.137634
Epoch 4: training loss 421887947715.765
Test Loss of 447224076052.548706, Test MSE of 447224086615.640625
Epoch 5: training loss 414892624233.412
Test Loss of 424897432388.160095, Test MSE of 424897433996.854614
Epoch 6: training loss 369251673509.647
Test Loss of 356935722769.706238, Test MSE of 356935724447.745361
Epoch 7: training loss 290236602006.588
Test Loss of 270386324545.613708, Test MSE of 270386330939.535461
Epoch 8: training loss 211732506262.588
Test Loss of 198227494042.677765, Test MSE of 198227492498.708832
Epoch 9: training loss 153708190569.412
Test Loss of 148368633013.681244, Test MSE of 148368630337.140167
Epoch 10: training loss 134166863390.118
Test Loss of 140190488495.937073, Test MSE of 140190487988.387390
Epoch 11: training loss 129714455311.059
Test Loss of 136257350761.882019, Test MSE of 136257353768.300522
Epoch 12: training loss 127944210251.294
Test Loss of 133397498373.803375, Test MSE of 133397500817.658997
Epoch 13: training loss 124827224786.824
Test Loss of 129452066185.445297, Test MSE of 129452063469.526764
Epoch 14: training loss 120266016587.294
Test Loss of 126253717412.567200, Test MSE of 126253717524.696396
Epoch 15: training loss 116237870772.706
Test Loss of 122455371744.732819, Test MSE of 122455370860.390457
Epoch 16: training loss 113494845711.059
Test Loss of 119635124608.681015, Test MSE of 119635123563.099808
Epoch 17: training loss 108815953950.118
Test Loss of 113667934077.720108, Test MSE of 113667934616.409149
Epoch 18: training loss 104903243866.353
Test Loss of 111166614004.748550, Test MSE of 111166615238.537506
Epoch 19: training loss 101043778288.941
Test Loss of 108623614745.049271, Test MSE of 108623614346.390732
Epoch 20: training loss 98403485214.118
Test Loss of 103455682108.047195, Test MSE of 103455685161.456284
Epoch 21: training loss 93382476890.353
Test Loss of 100635988971.392090, Test MSE of 100635989974.552597
Epoch 22: training loss 90694746895.059
Test Loss of 96752691308.487625, Test MSE of 96752690765.457703
Epoch 23: training loss 85532994951.529
Test Loss of 91886108867.182968, Test MSE of 91886108374.585327
Epoch 24: training loss 82428520207.059
Test Loss of 88540376910.819336, Test MSE of 88540378512.966309
Epoch 25: training loss 78562431141.647
Test Loss of 85377369940.030533, Test MSE of 85377368414.149689
Epoch 26: training loss 74526725466.353
Test Loss of 79830324743.461487, Test MSE of 79830324648.792969
Epoch 27: training loss 71059955260.235
Test Loss of 76653616378.848022, Test MSE of 76653617905.005005
Epoch 28: training loss 68424289551.059
Test Loss of 71681991376.092529, Test MSE of 71681991984.127487
Epoch 29: training loss 65248255984.941
Test Loss of 72176569957.736755, Test MSE of 72176569380.491592
Epoch 30: training loss 61914362608.941
Test Loss of 67575949916.972473, Test MSE of 67575950801.459549
Epoch 31: training loss 58891270746.353
Test Loss of 61045659505.402733, Test MSE of 61045659719.867210
Epoch 32: training loss 56038051689.412
Test Loss of 60150141366.687950, Test MSE of 60150142437.761261
Epoch 33: training loss 52643643136.000
Test Loss of 54886840897.495255, Test MSE of 54886839988.900940
Epoch 34: training loss 51102442036.706
Test Loss of 57476868666.862823, Test MSE of 57476867772.235474
Epoch 35: training loss 47367084574.118
Test Loss of 50082608049.595192, Test MSE of 50082607599.061905
Epoch 36: training loss 45373839698.824
Test Loss of 49306307267.301414, Test MSE of 49306307505.288513
Epoch 37: training loss 42908262430.118
Test Loss of 44713098499.375435, Test MSE of 44713098209.943939
Epoch 38: training loss 41328902151.529
Test Loss of 42482335037.172333, Test MSE of 42482334847.709595
Epoch 39: training loss 38579220886.588
Test Loss of 43986374979.094147, Test MSE of 43986374883.460052
Epoch 40: training loss 36513831024.941
Test Loss of 42303512770.235481, Test MSE of 42303512880.403946
Epoch 41: training loss 35145439427.765
Test Loss of 38668706434.398331, Test MSE of 38668707555.022011
Epoch 42: training loss 32949706014.118
Test Loss of 38987810689.510063, Test MSE of 38987810934.157654
Epoch 43: training loss 31549882375.529
Test Loss of 39008268609.199165, Test MSE of 39008268854.099106
Epoch 44: training loss 29056717575.529
Test Loss of 32434043544.664352, Test MSE of 32434043373.716484
Epoch 45: training loss 27880429537.882
Test Loss of 34185665335.842701, Test MSE of 34185665295.463840
Epoch 46: training loss 26982573063.529
Test Loss of 31882985254.077263, Test MSE of 31882985426.860825
Epoch 47: training loss 25689552384.000
Test Loss of 30654089008.025909, Test MSE of 30654089466.690254
Epoch 48: training loss 24570241050.353
Test Loss of 31406256610.272495, Test MSE of 31406256699.844936
Epoch 49: training loss 23440918863.059
Test Loss of 29550948522.548229, Test MSE of 29550948811.988571
Epoch 50: training loss 22290121479.529
Test Loss of 28059078725.640526, Test MSE of 28059078639.671024
Epoch 51: training loss 21043551284.706
Test Loss of 28227400303.922276, Test MSE of 28227400685.153904
Epoch 52: training loss 20871018089.412
Test Loss of 27322504986.233635, Test MSE of 27322505094.342117
Epoch 53: training loss 20135363802.353
Test Loss of 22288589816.183205, Test MSE of 22288590223.281834
Epoch 54: training loss 18956088557.176
Test Loss of 25615961112.397873, Test MSE of 25615961878.025616
Epoch 55: training loss 18565321423.059
Test Loss of 22956433882.218830, Test MSE of 22956434366.009296
Epoch 56: training loss 17837435418.353
Test Loss of 27543592442.196621, Test MSE of 27543592318.292603
Epoch 57: training loss 17238418823.529
Test Loss of 24291377373.712700, Test MSE of 24291377039.319759
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22829937460.15916, 'MSE - std': 1199769685.4230855, 'R2 - mean': 0.8363049147559307, 'R2 - std': 0.0019118341719187827} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005413 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109848515.765
Test Loss of 410764761593.129089, Test MSE of 410764755347.736450
Epoch 2: training loss 430087577840.941
Test Loss of 410744804243.487305, Test MSE of 410744798983.322449
Epoch 3: training loss 430059388446.118
Test Loss of 410719565362.465515, Test MSE of 410719558600.138367
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074905539.765
Test Loss of 410723151115.727905, Test MSE of 410723145676.131897
Epoch 2: training loss 430062979915.294
Test Loss of 410724190624.044434, Test MSE of 410724194128.162170
Epoch 3: training loss 430062502610.824
Test Loss of 410723830949.375305, Test MSE of 410723833691.091248
Epoch 4: training loss 430062134211.765
Test Loss of 410724389931.120789, Test MSE of 410724389645.481995
Epoch 5: training loss 423348463495.529
Test Loss of 389271888853.826904, Test MSE of 389271886255.740173
Epoch 6: training loss 378194414170.353
Test Loss of 323266975377.236450, Test MSE of 323266978216.265991
Epoch 7: training loss 299592496911.059
Test Loss of 237953220107.609436, Test MSE of 237953223026.038025
Epoch 8: training loss 220228857193.412
Test Loss of 168520289968.510864, Test MSE of 168520287850.249725
Epoch 9: training loss 161194090465.882
Test Loss of 120075644740.353546, Test MSE of 120075643726.541580
Epoch 10: training loss 141143616662.588
Test Loss of 112412919875.287369, Test MSE of 112412918344.730331
Epoch 11: training loss 138103779689.412
Test Loss of 109514183840.636749, Test MSE of 109514180281.299774
Epoch 12: training loss 135449163113.412
Test Loss of 106883087242.957886, Test MSE of 106883088545.517090
Epoch 13: training loss 130262959856.941
Test Loss of 103788983054.807953, Test MSE of 103788984049.040909
Epoch 14: training loss 126816287593.412
Test Loss of 100121207568.703384, Test MSE of 100121205898.185303
Epoch 15: training loss 122893693620.706
Test Loss of 97094100361.773254, Test MSE of 97094100610.157059
Epoch 16: training loss 118974648320.000
Test Loss of 94770247718.856079, Test MSE of 94770249205.490372
Epoch 17: training loss 115808456794.353
Test Loss of 90901864417.673294, Test MSE of 90901864300.153259
Epoch 18: training loss 110462970428.235
Test Loss of 87452985650.110138, Test MSE of 87452983932.993164
Epoch 19: training loss 105985959092.706
Test Loss of 83821253462.833878, Test MSE of 83821253995.321320
Epoch 20: training loss 101272175375.059
Test Loss of 80827826176.000000, Test MSE of 80827824940.987976
Epoch 21: training loss 98344817664.000
Test Loss of 76212153820.223969, Test MSE of 76212153092.528610
Epoch 22: training loss 94086752813.176
Test Loss of 75476777070.882004, Test MSE of 75476777579.113632
Epoch 23: training loss 91259999066.353
Test Loss of 72101467345.917633, Test MSE of 72101467539.874710
Epoch 24: training loss 87585807073.882
Test Loss of 68144569211.794540, Test MSE of 68144569313.298340
Epoch 25: training loss 83991382603.294
Test Loss of 65663841948.608978, Test MSE of 65663841953.507263
Epoch 26: training loss 79248189168.941
Test Loss of 63660970554.521057, Test MSE of 63660972193.606140
Epoch 27: training loss 74937493714.824
Test Loss of 58732554314.869041, Test MSE of 58732554739.951080
Epoch 28: training loss 71802534595.765
Test Loss of 57724755899.764923, Test MSE of 57724755696.352028
Epoch 29: training loss 69612201547.294
Test Loss of 53708370274.443314, Test MSE of 53708369996.393753
Epoch 30: training loss 65513205835.294
Test Loss of 52989091745.229057, Test MSE of 52989092050.191292
Epoch 31: training loss 62244985946.353
Test Loss of 49225031471.503937, Test MSE of 49225031827.091042
Epoch 32: training loss 59539429210.353
Test Loss of 47288369405.986115, Test MSE of 47288368800.090073
Epoch 33: training loss 56103300050.824
Test Loss of 46278352298.943085, Test MSE of 46278351822.866493
Epoch 34: training loss 53988499975.529
Test Loss of 42156202654.030540, Test MSE of 42156202375.501747
Epoch 35: training loss 51354580080.941
Test Loss of 40730323371.416939, Test MSE of 40730323669.614777
Epoch 36: training loss 48086188973.176
Test Loss of 38869283786.454422, Test MSE of 38869284266.878151
Epoch 37: training loss 45812422844.235
Test Loss of 37075541229.401199, Test MSE of 37075542031.287323
Epoch 38: training loss 43173081231.059
Test Loss of 34829324096.562706, Test MSE of 34829324550.153404
Epoch 39: training loss 41548398531.765
Test Loss of 35772214609.858398, Test MSE of 35772214492.898643
Epoch 40: training loss 39284812619.294
Test Loss of 33984085913.647385, Test MSE of 33984085850.077763
Epoch 41: training loss 37024014599.529
Test Loss of 29448532094.993057, Test MSE of 29448532106.569427
Epoch 42: training loss 35006463051.294
Test Loss of 27897030899.561314, Test MSE of 27897030777.119080
Epoch 43: training loss 32601177645.176
Test Loss of 26621577459.561314, Test MSE of 26621577434.776123
Epoch 44: training loss 31460086452.706
Test Loss of 26468830331.676075, Test MSE of 26468830294.007015
Epoch 45: training loss 29861064749.176
Test Loss of 26428245157.375290, Test MSE of 26428244854.033649
Epoch 46: training loss 28824149112.471
Test Loss of 25381925482.854233, Test MSE of 25381924981.432793
Epoch 47: training loss 27546361946.353
Test Loss of 24926618796.956963, Test MSE of 24926618836.073158
Epoch 48: training loss 25829859561.412
Test Loss of 23254985676.349838, Test MSE of 23254985752.940624
Epoch 49: training loss 24716258537.412
Test Loss of 23947253753.366035, Test MSE of 23947253748.622936
Epoch 50: training loss 24349530891.294
Test Loss of 22933456113.192039, Test MSE of 22933455844.870125
Epoch 51: training loss 22750388976.941
Test Loss of 20989973614.408146, Test MSE of 20989973792.310928
Epoch 52: training loss 21832314187.294
Test Loss of 19234923429.493752, Test MSE of 19234923405.014046
Epoch 53: training loss 21228500912.941
Test Loss of 21658531537.680702, Test MSE of 21658531650.697327
Epoch 54: training loss 20206787602.824
Test Loss of 19657695186.509949, Test MSE of 19657694815.619282
Epoch 55: training loss 19491151849.412
Test Loss of 19888528502.937531, Test MSE of 19888528468.234489
Epoch 56: training loss 18986225140.706
Test Loss of 20701210858.084221, Test MSE of 20701210832.752907
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22297755803.307594, 'MSE - std': 1388969913.7177088, 'R2 - mean': 0.8345142156510488, 'R2 - std': 0.003515841617360239} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005232 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042864519.529
Test Loss of 431612763398.041626, Test MSE of 431612766716.891846
Epoch 2: training loss 424022485955.765
Test Loss of 431592235360.547913, Test MSE of 431592236044.225952
Epoch 3: training loss 423994968786.824
Test Loss of 431564436693.708496, Test MSE of 431564432460.540161
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424013033833.412
Test Loss of 431567971073.540039, Test MSE of 431567969501.456360
Epoch 2: training loss 423998924197.647
Test Loss of 431570938783.807495, Test MSE of 431570948768.387085
Epoch 3: training loss 423998276909.176
Test Loss of 431570613590.596924, Test MSE of 431570609788.048706
Epoch 4: training loss 423997862851.765
Test Loss of 431570721932.734863, Test MSE of 431570721951.385925
Epoch 5: training loss 417270318501.647
Test Loss of 409498043634.139771, Test MSE of 409498055567.704163
Epoch 6: training loss 372257515158.588
Test Loss of 342528450730.587708, Test MSE of 342528448206.736145
Epoch 7: training loss 294127753637.647
Test Loss of 255322344689.192047, Test MSE of 255322351570.905273
Epoch 8: training loss 216248587294.118
Test Loss of 183567583450.920868, Test MSE of 183567587707.990631
Epoch 9: training loss 156305501680.941
Test Loss of 132738003912.559006, Test MSE of 132738003414.511749
Epoch 10: training loss 138313123719.529
Test Loss of 123585012716.571960, Test MSE of 123585015280.836426
Epoch 11: training loss 135344217765.647
Test Loss of 120467620681.565948, Test MSE of 120467620971.859375
Epoch 12: training loss 130371742177.882
Test Loss of 117497991207.803787, Test MSE of 117497992370.307587
Epoch 13: training loss 126925800899.765
Test Loss of 113950812545.717728, Test MSE of 113950809684.233566
Epoch 14: training loss 123658119047.529
Test Loss of 109036540675.435440, Test MSE of 109036538851.184875
Epoch 15: training loss 119655113697.882
Test Loss of 107597041688.640442, Test MSE of 107597038480.779007
Epoch 16: training loss 116708537765.647
Test Loss of 103372733713.414154, Test MSE of 103372733413.841476
Epoch 17: training loss 112748307034.353
Test Loss of 99316433339.528000, Test MSE of 99316433159.192383
Epoch 18: training loss 109098054053.647
Test Loss of 94443785074.317444, Test MSE of 94443785133.362732
Epoch 19: training loss 104242725526.588
Test Loss of 92944644654.200836, Test MSE of 92944646092.497864
Epoch 20: training loss 101005786774.588
Test Loss of 87983389609.284592, Test MSE of 87983389645.588760
Epoch 21: training loss 96975915324.235
Test Loss of 83585773983.096710, Test MSE of 83585775358.228882
Epoch 22: training loss 92570297931.294
Test Loss of 79796898697.062469, Test MSE of 79796899387.060760
Epoch 23: training loss 89841250499.765
Test Loss of 76708325814.789444, Test MSE of 76708324900.366531
Epoch 24: training loss 86431579632.941
Test Loss of 74441992700.919937, Test MSE of 74441993168.185074
Epoch 25: training loss 81207672741.647
Test Loss of 68769573698.931976, Test MSE of 68769574054.660812
Epoch 26: training loss 78415998162.824
Test Loss of 67396827832.566406, Test MSE of 67396826106.237534
Epoch 27: training loss 74988924611.765
Test Loss of 61675958315.594635, Test MSE of 61675959125.541832
Epoch 28: training loss 71245391420.235
Test Loss of 60015525122.250809, Test MSE of 60015525009.050842
Epoch 29: training loss 69219938032.941
Test Loss of 58837390521.277184, Test MSE of 58837391295.495468
Epoch 30: training loss 65146260359.529
Test Loss of 58297871678.904213, Test MSE of 58297871451.115646
Epoch 31: training loss 62975263713.882
Test Loss of 53168203496.425728, Test MSE of 53168203494.146011
Epoch 32: training loss 59096774543.059
Test Loss of 48825079511.366959, Test MSE of 48825079856.051064
Epoch 33: training loss 56631075802.353
Test Loss of 48239769235.131882, Test MSE of 48239769782.277924
Epoch 34: training loss 54132591947.294
Test Loss of 44480755106.413696, Test MSE of 44480754785.933334
Epoch 35: training loss 51147155433.412
Test Loss of 41743481320.070335, Test MSE of 41743481198.002167
Epoch 36: training loss 47855464192.000
Test Loss of 41214291160.551598, Test MSE of 41214290379.283745
Epoch 37: training loss 45741864244.706
Test Loss of 40436841190.056458, Test MSE of 40436840555.727852
Epoch 38: training loss 43617276461.176
Test Loss of 36961042356.183250, Test MSE of 36961042536.047852
Epoch 39: training loss 41835566373.647
Test Loss of 37313580225.806572, Test MSE of 37313580624.034584
Epoch 40: training loss 39007438328.471
Test Loss of 30709252249.528923, Test MSE of 30709252251.889122
Epoch 41: training loss 37649913046.588
Test Loss of 33493872500.686718, Test MSE of 33493873162.559284
Epoch 42: training loss 35860647717.647
Test Loss of 30800817116.460899, Test MSE of 30800816757.247715
Epoch 43: training loss 34401409212.235
Test Loss of 29277698232.803333, Test MSE of 29277698594.132500
Epoch 44: training loss 32088560640.000
Test Loss of 26881858700.734844, Test MSE of 26881858566.500954
Epoch 45: training loss 30869316005.647
Test Loss of 25361668934.722813, Test MSE of 25361669180.835365
Epoch 46: training loss 29618765274.353
Test Loss of 26421102506.706154, Test MSE of 26421102295.308670
Epoch 47: training loss 28156190795.294
Test Loss of 24516115027.161499, Test MSE of 24516114930.856808
Epoch 48: training loss 26829170401.882
Test Loss of 23072568390.604351, Test MSE of 23072568455.187748
Epoch 49: training loss 25789477123.765
Test Loss of 22719634060.024063, Test MSE of 22719634188.559086
Epoch 50: training loss 24169031111.529
Test Loss of 25026539570.228600, Test MSE of 25026540183.981010
Epoch 51: training loss 23404892645.647
Test Loss of 22946338055.937065, Test MSE of 22946337683.786831
Epoch 52: training loss 22703434281.412
Test Loss of 22676130868.597870, Test MSE of 22676131271.348579
Epoch 53: training loss 21701194435.765
Test Loss of 22523596613.301250, Test MSE of 22523596897.164574
Epoch 54: training loss 21173676943.059
Test Loss of 21672116459.031929, Test MSE of 21672115981.681767
Epoch 55: training loss 20318767363.765
Test Loss of 21411661276.223969, Test MSE of 21411661114.470440
Epoch 56: training loss 19875398912.000
Test Loss of 21073192798.889404, Test MSE of 21073192110.939739
Epoch 57: training loss 18910179045.647
Test Loss of 21612605487.385471, Test MSE of 21612605613.444725
Epoch 58: training loss 18316900449.882
Test Loss of 20761366550.745026, Test MSE of 20761366395.727444
Epoch 59: training loss 18006235260.235
Test Loss of 20603187071.111523, Test MSE of 20603186622.382580
Epoch 60: training loss 17586218435.765
Test Loss of 20700812237.297546, Test MSE of 20700812430.634045
Epoch 61: training loss 16926481859.765
Test Loss of 18611994610.732067, Test MSE of 18611994719.410667
Epoch 62: training loss 16259833799.529
Test Loss of 19011657800.973625, Test MSE of 19011657943.192894
Epoch 63: training loss 16154023792.941
Test Loss of 20647349176.447941, Test MSE of 20647349339.781139
Epoch 64: training loss 15682886422.588
Test Loss of 18730124284.683018, Test MSE of 18730124272.954536
Epoch 65: training loss 15206795738.353
Test Loss of 20683267807.896343, Test MSE of 20683268052.696640
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21974858253.185402, 'MSE - std': 1400157579.8709354, 'R2 - mean': 0.8367186897748089, 'R2 - std': 0.005415509068591954} 
 

Saving model.....
Results After CV: {'MSE - mean': 21974858253.185402, 'MSE - std': 1400157579.8709354, 'R2 - mean': 0.8367186897748089, 'R2 - std': 0.005415509068591954}
Train time: 90.16892770100021
Inference time: 0.07056186599947978
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 38 finished with value: 21974858253.185402 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005855 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525493579.294
Test Loss of 418110709822.534363, Test MSE of 418110707797.644287
Epoch 2: training loss 427505342825.412
Test Loss of 418093166632.742065, Test MSE of 418093168037.156799
Epoch 3: training loss 427479140833.882
Test Loss of 418070962996.526489, Test MSE of 418070957930.730957
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495967201.882
Test Loss of 418077003299.412415, Test MSE of 418077006991.216431
Epoch 2: training loss 427482981436.235
Test Loss of 418077838280.808716, Test MSE of 418077835532.466675
Epoch 3: training loss 427482613157.647
Test Loss of 418077421453.353699, Test MSE of 418077419423.533630
Epoch 4: training loss 427482303186.824
Test Loss of 418077116695.035828, Test MSE of 418077116168.733765
Epoch 5: training loss 427482075979.294
Test Loss of 418076197288.475586, Test MSE of 418076194026.443420
Epoch 6: training loss 419851610955.294
Test Loss of 393414721221.433289, Test MSE of 393414724496.030518
Epoch 7: training loss 367377198019.765
Test Loss of 317387556873.948669, Test MSE of 317387555491.322449
Epoch 8: training loss 280545266206.118
Test Loss of 226462087988.052734, Test MSE of 226462091240.983887
Epoch 9: training loss 201046872184.471
Test Loss of 160274065669.981018, Test MSE of 160274065040.556091
Epoch 10: training loss 157438629195.294
Test Loss of 130408895514.766602, Test MSE of 130408895674.178574
Epoch 11: training loss 140567797082.353
Test Loss of 117783016889.767288, Test MSE of 117783018359.505722
Epoch 12: training loss 135298371523.765
Test Loss of 114218350799.974091, Test MSE of 114218353257.349380
Epoch 13: training loss 130617131068.235
Test Loss of 111202198332.106415, Test MSE of 111202197779.103806
Epoch 14: training loss 128201499301.647
Test Loss of 106959007013.485077, Test MSE of 106959006928.995392
Epoch 15: training loss 124260208700.235
Test Loss of 104920158792.601440, Test MSE of 104920158530.395676
Epoch 16: training loss 120546742091.294
Test Loss of 101149715097.611847, Test MSE of 101149715225.355789
Epoch 17: training loss 115965201227.294
Test Loss of 97192746475.510529, Test MSE of 97192747434.982346
Epoch 18: training loss 112830392922.353
Test Loss of 95231593335.324539, Test MSE of 95231594420.294586
Epoch 19: training loss 109168856937.412
Test Loss of 92023991379.142258, Test MSE of 92023992287.382172
Epoch 20: training loss 104981231947.294
Test Loss of 89674930039.087677, Test MSE of 89674929176.737503
Epoch 21: training loss 101213301820.235
Test Loss of 85965415311.485550, Test MSE of 85965415606.891861
Epoch 22: training loss 97199205360.941
Test Loss of 82188933113.841309, Test MSE of 82188934606.434967
Epoch 23: training loss 93840641069.176
Test Loss of 77121849391.848251, Test MSE of 77121849020.626358
Epoch 24: training loss 89356370055.529
Test Loss of 74470941435.677078, Test MSE of 74470941210.927460
Epoch 25: training loss 84416500600.471
Test Loss of 73989841682.416840, Test MSE of 73989843177.037994
Epoch 26: training loss 81407064771.765
Test Loss of 67521446649.782097, Test MSE of 67521447261.557251
Epoch 27: training loss 78848704854.588
Test Loss of 66726896739.723343, Test MSE of 66726897616.237129
Epoch 28: training loss 74640608146.824
Test Loss of 63458512210.964607, Test MSE of 63458513576.185661
Epoch 29: training loss 71146705370.353
Test Loss of 60096049539.523476, Test MSE of 60096049053.311409
Epoch 30: training loss 67942266142.118
Test Loss of 56933411710.904465, Test MSE of 56933410919.583199
Epoch 31: training loss 65197508352.000
Test Loss of 56525010687.703911, Test MSE of 56525011928.422195
Epoch 32: training loss 61859716570.353
Test Loss of 47429377822.497337, Test MSE of 47429377952.765022
Epoch 33: training loss 58458172732.235
Test Loss of 50487342652.520935, Test MSE of 50487342155.800758
Epoch 34: training loss 56133689645.176
Test Loss of 48018137545.637749, Test MSE of 48018137404.529129
Epoch 35: training loss 52432438768.941
Test Loss of 42670662173.016884, Test MSE of 42670662839.455826
Epoch 36: training loss 49584858134.588
Test Loss of 43141290123.754799, Test MSE of 43141290606.632530
Epoch 37: training loss 47906879292.235
Test Loss of 39860703620.470970, Test MSE of 39860703633.523277
Epoch 38: training loss 45034442337.882
Test Loss of 38711898231.383759, Test MSE of 38711897880.056099
Epoch 39: training loss 43500781266.824
Test Loss of 37633318133.163078, Test MSE of 37633317994.525826
Epoch 40: training loss 40888103589.647
Test Loss of 32593071788.798519, Test MSE of 32593072080.421402
Epoch 41: training loss 38717481701.647
Test Loss of 31661297794.753643, Test MSE of 31661297438.739765
Epoch 42: training loss 36946151936.000
Test Loss of 31986958437.381447, Test MSE of 31986958725.231205
Epoch 43: training loss 34899982859.294
Test Loss of 29914344200.468193, Test MSE of 29914344542.302402
Epoch 44: training loss 33300820389.647
Test Loss of 28376835167.933380, Test MSE of 28376835496.051517
Epoch 45: training loss 31768755666.824
Test Loss of 27236154283.436501, Test MSE of 27236154124.453438
Epoch 46: training loss 30413907629.176
Test Loss of 25941654318.367802, Test MSE of 25941653738.048100
Epoch 47: training loss 28553458025.412
Test Loss of 25128893011.023827, Test MSE of 25128892885.381081
Epoch 48: training loss 27688970533.647
Test Loss of 24112145603.419846, Test MSE of 24112145500.333458
Epoch 49: training loss 26152937185.882
Test Loss of 25064259550.364098, Test MSE of 25064259932.366116
Epoch 50: training loss 25324235617.882
Test Loss of 20221284632.457088, Test MSE of 20221284367.507885
Epoch 51: training loss 24028196295.529
Test Loss of 21872458278.491787, Test MSE of 21872458658.695118
Epoch 52: training loss 23173225479.529
Test Loss of 20862364337.772842, Test MSE of 20862364558.430099
Epoch 53: training loss 22014845364.706
Test Loss of 20606145759.844551, Test MSE of 20606145928.425831
Epoch 54: training loss 21194370849.882
Test Loss of 18680087897.597038, Test MSE of 18680087945.324120
Epoch 55: training loss 20189405477.647
Test Loss of 20010647592.742077, Test MSE of 20010647776.214035
Epoch 56: training loss 19416183269.647
Test Loss of 19254509352.919731, Test MSE of 19254509751.023865
Epoch 57: training loss 19087383582.118
Test Loss of 20454731089.069626, Test MSE of 20454731324.086124
Epoch 58: training loss 18253964553.412
Test Loss of 18100194117.581310, Test MSE of 18100194123.783310
Epoch 59: training loss 17578148333.176
Test Loss of 19264770687.555862, Test MSE of 19264770481.049053
Epoch 60: training loss 16995425046.588
Test Loss of 18927029988.700439, Test MSE of 18927029685.971382
Epoch 61: training loss 16587122104.471
Test Loss of 20871483976.127689, Test MSE of 20871483865.276970
Epoch 62: training loss 16191728116.706
Test Loss of 18599361512.075874, Test MSE of 18599361730.438469
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18599361730.43847, 'MSE - std': 0.0, 'R2 - mean': 0.8551647521896781, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003938 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917625223.529
Test Loss of 424556113956.952087, Test MSE of 424556122862.316772
Epoch 2: training loss 427896884043.294
Test Loss of 424539345798.958130, Test MSE of 424539350554.768188
Epoch 3: training loss 427868572732.235
Test Loss of 424516461698.043030, Test MSE of 424516463277.171875
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887047619.765
Test Loss of 424524042278.847107, Test MSE of 424524040021.808899
Epoch 2: training loss 427875654716.235
Test Loss of 424524698563.597473, Test MSE of 424524702660.548645
Epoch 3: training loss 427875106575.059
Test Loss of 424524334575.774231, Test MSE of 424524327729.241821
Epoch 4: training loss 427874702034.824
Test Loss of 424524220535.857483, Test MSE of 424524226237.610229
Epoch 5: training loss 427874422061.176
Test Loss of 424524419235.915771, Test MSE of 424524421345.961121
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221561891538.1998, 'MSE - std': 202962529807.76132, 'R2 - mean': -0.5878288812061415, 'R2 - std': 1.4429936333958198} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005399 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926786349.176
Test Loss of 447258737744.299805, Test MSE of 447258737127.776550
Epoch 2: training loss 421906433807.059
Test Loss of 447240214675.334717, Test MSE of 447240222371.782471
Epoch 3: training loss 421879706563.765
Test Loss of 447215624121.885742, Test MSE of 447215629850.838196
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899168225.882
Test Loss of 447222695930.788818, Test MSE of 447222699469.422302
Epoch 2: training loss 421888315873.882
Test Loss of 447224310120.520020, Test MSE of 447224310614.095764
Epoch 3: training loss 421887867241.412
Test Loss of 447224584528.832764, Test MSE of 447224586663.317566
Epoch 4: training loss 421887531008.000
Test Loss of 447224307194.670349, Test MSE of 447224309787.148682
Epoch 5: training loss 421887311510.588
Test Loss of 447223923938.687012, Test MSE of 447223929982.655334
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 296782571019.685, 'MSE - std': 196923403672.4861, 'R2 - mean': -1.0509326500447624, 'R2 - std': 1.3479925638168961} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005389 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110293052.235
Test Loss of 410764908905.077271, Test MSE of 410764907344.564880
Epoch 2: training loss 430088650029.176
Test Loss of 410746366948.042603, Test MSE of 410746366325.906311
Epoch 3: training loss 430060214633.412
Test Loss of 410722504747.594604, Test MSE of 410722500302.300598
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076893545.412
Test Loss of 410726718339.376221, Test MSE of 410726722139.489380
Epoch 2: training loss 430065835791.059
Test Loss of 410726434890.869019, Test MSE of 410726434902.317749
Epoch 3: training loss 430065310539.294
Test Loss of 410726808424.840332, Test MSE of 410726809432.798584
Epoch 4: training loss 430064931779.765
Test Loss of 410726933962.217468, Test MSE of 410726929093.000305
Epoch 5: training loss 430064649396.706
Test Loss of 410726612423.374390, Test MSE of 410726606063.942322
Epoch 6: training loss 421949569385.412
Test Loss of 385002092491.875977, Test MSE of 385002103162.880737
Epoch 7: training loss 368642364235.294
Test Loss of 308718198272.236938, Test MSE of 308718200365.802185
Epoch 8: training loss 281543615548.235
Test Loss of 217923773056.651550, Test MSE of 217923775963.420074
Epoch 9: training loss 203570683904.000
Test Loss of 153638240468.286896, Test MSE of 153638238843.884705
Epoch 10: training loss 159257635900.235
Test Loss of 123901671978.409988, Test MSE of 123901670829.133331
Epoch 11: training loss 142606682834.824
Test Loss of 112290631306.602493, Test MSE of 112290633946.996872
Epoch 12: training loss 136778731520.000
Test Loss of 108978925164.275803, Test MSE of 108978924035.242508
Epoch 13: training loss 134726864112.941
Test Loss of 106166718553.084686, Test MSE of 106166717292.695328
Epoch 14: training loss 129820530898.824
Test Loss of 103936475780.442383, Test MSE of 103936476513.101364
Epoch 15: training loss 126289968941.176
Test Loss of 100898820086.522903, Test MSE of 100898819015.682007
Epoch 16: training loss 122427781180.235
Test Loss of 98293773874.465530, Test MSE of 98293771974.833649
Epoch 17: training loss 117694939015.529
Test Loss of 94331213495.144836, Test MSE of 94331213317.020554
Epoch 18: training loss 114843700615.529
Test Loss of 90970319279.207779, Test MSE of 90970316401.197739
Epoch 19: training loss 110414968048.941
Test Loss of 88711823166.667282, Test MSE of 88711824285.588181
Epoch 20: training loss 106215511040.000
Test Loss of 85265099634.317444, Test MSE of 85265098435.829300
Epoch 21: training loss 102542647898.353
Test Loss of 81337374675.457657, Test MSE of 81337375015.740417
Epoch 22: training loss 98052182738.824
Test Loss of 79690858181.360474, Test MSE of 79690858417.726028
Epoch 23: training loss 94340284687.059
Test Loss of 75115056266.839432, Test MSE of 75115056852.455200
Epoch 24: training loss 90464596555.294
Test Loss of 72443221653.027298, Test MSE of 72443222997.735733
Epoch 25: training loss 87324865415.529
Test Loss of 67673229466.476631, Test MSE of 67673228798.058235
Epoch 26: training loss 83889748359.529
Test Loss of 66809713403.853775, Test MSE of 66809713644.165749
Epoch 27: training loss 79565737622.588
Test Loss of 62622603939.716797, Test MSE of 62622603763.165115
Epoch 28: training loss 76003294117.647
Test Loss of 61074336065.273483, Test MSE of 61074336734.198456
Epoch 29: training loss 73381922258.824
Test Loss of 58932550584.447937, Test MSE of 58932549292.536888
Epoch 30: training loss 69290816617.412
Test Loss of 54509401176.610825, Test MSE of 54509400538.135094
Epoch 31: training loss 66651046957.176
Test Loss of 50834658151.892639, Test MSE of 50834657713.474304
Epoch 32: training loss 62704167815.529
Test Loss of 48929175346.347061, Test MSE of 48929176017.230492
Epoch 33: training loss 59068556182.588
Test Loss of 48675036135.833412, Test MSE of 48675037494.286484
Epoch 34: training loss 56347843922.824
Test Loss of 45268320257.895416, Test MSE of 45268321345.820412
Epoch 35: training loss 53562449310.118
Test Loss of 44656564151.026375, Test MSE of 44656563336.977760
Epoch 36: training loss 50848589462.588
Test Loss of 40876797802.261917, Test MSE of 40876798392.120834
Epoch 37: training loss 48095107704.471
Test Loss of 37336363138.783897, Test MSE of 37336363040.328194
Epoch 38: training loss 45827825957.647
Test Loss of 36903105289.121704, Test MSE of 36903105100.367249
Epoch 39: training loss 43382786153.412
Test Loss of 35875871858.198982, Test MSE of 35875872557.630188
Epoch 40: training loss 41172802560.000
Test Loss of 33742011210.513653, Test MSE of 33742011182.427986
Epoch 41: training loss 39050584560.941
Test Loss of 32793203141.005089, Test MSE of 32793202646.559940
Epoch 42: training loss 36975579136.000
Test Loss of 32160286923.757519, Test MSE of 32160286816.170544
Epoch 43: training loss 35496874209.882
Test Loss of 28224531471.637203, Test MSE of 28224531764.294811
Epoch 44: training loss 32739233182.118
Test Loss of 28055970112.325775, Test MSE of 28055970164.590569
Epoch 45: training loss 31478427105.882
Test Loss of 27469226916.546043, Test MSE of 27469227674.019058
Epoch 46: training loss 30166518204.235
Test Loss of 26607283861.975010, Test MSE of 26607283928.279305
Epoch 47: training loss 29004971200.000
Test Loss of 27634711772.342434, Test MSE of 27634712058.551701
Epoch 48: training loss 27628084314.353
Test Loss of 25041490777.677002, Test MSE of 25041490961.669548
Epoch 49: training loss 26401077707.294
Test Loss of 25102353847.263306, Test MSE of 25102354104.233685
Epoch 50: training loss 25017987316.706
Test Loss of 24167568460.290607, Test MSE of 24167568658.468662
Epoch 51: training loss 24156567875.765
Test Loss of 23578897615.074501, Test MSE of 23578897326.365738
Epoch 52: training loss 23012032470.588
Test Loss of 22235436153.780659, Test MSE of 22235435984.366276
Epoch 53: training loss 21714796464.941
Test Loss of 21771102522.639519, Test MSE of 21771102327.507172
Epoch 54: training loss 20719569103.059
Test Loss of 20503980504.906986, Test MSE of 20503980342.519550
Epoch 55: training loss 20454779821.176
Test Loss of 20327221418.587692, Test MSE of 20327221681.866585
Epoch 56: training loss 19450536293.647
Test Loss of 20095800955.439148, Test MSE of 20095800550.808834
Epoch 57: training loss 19057779102.118
Test Loss of 20230779005.097641, Test MSE of 20230779061.008877
Epoch 58: training loss 18481599269.647
Test Loss of 17877053869.312355, Test MSE of 17877053893.770691
Epoch 59: training loss 17632152786.824
Test Loss of 19066808672.547894, Test MSE of 19066809097.930145
Epoch 60: training loss 16875857266.824
Test Loss of 18973995972.768162, Test MSE of 18973995895.391354
Epoch 61: training loss 16682828732.235
Test Loss of 19270414987.076355, Test MSE of 19270415460.060966
Epoch 62: training loss 16143375190.588
Test Loss of 19797393235.516891, Test MSE of 19797393576.107365
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 227536276658.7906, 'MSE - std': 208492848878.83948, 'R2 - mean': -0.579049039236152, 'R2 - std': 1.4250738193260561} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005296 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043684562.824
Test Loss of 431612393200.007385, Test MSE of 431612384190.010010
Epoch 2: training loss 424023620909.176
Test Loss of 431592806852.057373, Test MSE of 431592802244.134033
Epoch 3: training loss 423996200237.176
Test Loss of 431565986418.435913, Test MSE of 431565978523.146484
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012284747.294
Test Loss of 431568174201.780640, Test MSE of 431568178321.712952
Epoch 2: training loss 424000079872.000
Test Loss of 431570713350.278564, Test MSE of 431570720858.317200
Epoch 3: training loss 423999496794.353
Test Loss of 431571438795.283691, Test MSE of 431571442058.008057
Epoch 4: training loss 423999071412.706
Test Loss of 431571245947.794556, Test MSE of 431571256145.288513
Epoch 5: training loss 423998840952.471
Test Loss of 431571689839.711243, Test MSE of 431571688292.475586
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 268343358985.5276, 'MSE - std': 203559048678.69678, 'R2 - mean': -0.9078379096355059, 'R2 - std': 1.4342513021143732} 
 

Saving model.....
Results After CV: {'MSE - mean': 268343358985.5276, 'MSE - std': 203559048678.69678, 'R2 - mean': -0.9078379096355059, 'R2 - std': 1.4342513021143732}
Train time: 44.905507626600226
Inference time: 0.06849358319996099
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 39 finished with value: 268343358985.5276 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005496 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427523855179.294
Test Loss of 418110308374.739746, Test MSE of 418110312493.646423
Epoch 2: training loss 427501348743.529
Test Loss of 418091776914.801758, Test MSE of 418091778972.593994
Epoch 3: training loss 427473563407.059
Test Loss of 418068745915.010864, Test MSE of 418068744748.463013
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493370940.235
Test Loss of 418071467993.626648, Test MSE of 418071473562.039551
Epoch 2: training loss 427480775378.824
Test Loss of 418075030367.163574, Test MSE of 418075033629.631592
Epoch 3: training loss 427480453963.294
Test Loss of 418075120916.903992, Test MSE of 418075124153.261597
Epoch 4: training loss 427480148690.824
Test Loss of 418074423056.284973, Test MSE of 418074424687.890808
Epoch 5: training loss 427479900641.882
Test Loss of 418073467038.704590, Test MSE of 418073467639.937805
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418073467639.9378, 'MSE - std': 0.0, 'R2 - mean': -2.2555834531383927, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005514 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916290891.294
Test Loss of 424554521392.736511, Test MSE of 424554524755.148621
Epoch 2: training loss 427895000124.235
Test Loss of 424537336323.908386, Test MSE of 424537346077.788330
Epoch 3: training loss 427868379979.294
Test Loss of 424515461694.179016, Test MSE of 424515463040.210938
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884961069.176
Test Loss of 424515736367.789062, Test MSE of 424515743351.778015
Epoch 2: training loss 427874619151.059
Test Loss of 424517566509.479553, Test MSE of 424517564798.512939
Epoch 3: training loss 427874147629.176
Test Loss of 424517850771.690002, Test MSE of 424517852071.378418
Epoch 4: training loss 427873799710.118
Test Loss of 424518058954.229919, Test MSE of 424518059562.480103
Epoch 5: training loss 427873534192.941
Test Loss of 424517849498.263245, Test MSE of 424517850532.973572
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 421295659086.4557, 'MSE - std': 3222191446.5178833, 'R2 - mean': -2.143179528249086, 'R2 - std': 0.11240392488930673} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005432 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926496015.059
Test Loss of 447259412584.697693, Test MSE of 447259415626.310913
Epoch 2: training loss 421905375111.529
Test Loss of 447240267143.550293, Test MSE of 447240263299.021423
Epoch 3: training loss 421878326452.706
Test Loss of 447215392843.325439, Test MSE of 447215394320.715210
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900373654.588
Test Loss of 447221563399.579895, Test MSE of 447221559485.901550
Epoch 2: training loss 421887932536.471
Test Loss of 447223879596.147095, Test MSE of 447223882548.274109
Epoch 3: training loss 421887478362.353
Test Loss of 447224783956.563477, Test MSE of 447224789572.573730
Epoch 4: training loss 421887130925.176
Test Loss of 447224689495.583618, Test MSE of 447224688367.157715
Epoch 5: training loss 421886917451.294
Test Loss of 447225161027.330994, Test MSE of 447225156639.573547
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 429938824937.49493, 'MSE - std': 12503212000.754793, 'R2 - mean': -2.087835803331443, 'R2 - std': 0.12061903109598193} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106612193.882
Test Loss of 410761936039.270691, Test MSE of 410761942231.710632
Epoch 2: training loss 430084066846.118
Test Loss of 410742315391.822327, Test MSE of 410742324114.834045
Epoch 3: training loss 430057248888.471
Test Loss of 410718624810.173096, Test MSE of 410718626127.638611
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074771576.471
Test Loss of 410720117945.277161, Test MSE of 410720116022.400513
Epoch 2: training loss 430060984440.471
Test Loss of 410721210525.319763, Test MSE of 410721203711.607422
Epoch 3: training loss 430060554601.412
Test Loss of 410721590018.487732, Test MSE of 410721584655.468079
Epoch 4: training loss 430060306672.941
Test Loss of 410721924643.302185, Test MSE of 410721920814.895447
Epoch 5: training loss 430060120907.294
Test Loss of 410722261891.376221, Test MSE of 410722256829.262146
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 425134682910.43677, 'MSE - std': 13656027043.042381, 'R2 - mean': -2.163353061992937, 'R2 - std': 0.16739259859426608} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005390 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042743567.059
Test Loss of 431613255964.312805, Test MSE of 431613254919.310608
Epoch 2: training loss 424022911819.294
Test Loss of 431593004369.858398, Test MSE of 431593002804.146973
Epoch 3: training loss 423995502351.059
Test Loss of 431565359626.187866, Test MSE of 431565366390.426697
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008912173.176
Test Loss of 431569983113.654785, Test MSE of 431569986294.117554
Epoch 2: training loss 423999616542.118
Test Loss of 431571702804.375732, Test MSE of 431571706489.205017
Epoch 3: training loss 423998962266.353
Test Loss of 431571354896.466431, Test MSE of 431571349513.775696
Epoch 4: training loss 423998463156.706
Test Loss of 431571527637.353088, Test MSE of 431571535981.049500
Epoch 5: training loss 423998174750.118
Test Loss of 431570379847.552063, Test MSE of 431570386161.710205
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 426421823560.69147, 'MSE - std': 12482651316.70336, 'R2 - mean': -2.1752791829687537, 'R2 - std': 0.15160855878687107} 
 

Saving model.....
Results After CV: {'MSE - mean': 426421823560.69147, 'MSE - std': 12482651316.70336, 'R2 - mean': -2.1752791829687537, 'R2 - std': 0.15160855878687107}
Train time: 12.968006622199756
Inference time: 0.09382337539973377
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 40 finished with value: 426421823560.69147 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 6, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005425 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525494784.000
Test Loss of 418112082788.848511, Test MSE of 418112088538.490662
Epoch 2: training loss 427505115979.294
Test Loss of 418093570362.093018, Test MSE of 418093568393.220764
Epoch 3: training loss 427477284743.529
Test Loss of 418068788006.314148, Test MSE of 418068781888.564392
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493544056.471
Test Loss of 418074501432.671753, Test MSE of 418074499188.418762
Epoch 2: training loss 427481487119.059
Test Loss of 418075062733.427734, Test MSE of 418075059590.316589
Epoch 3: training loss 427480950061.176
Test Loss of 418074530433.450867, Test MSE of 418074527484.003601
Epoch 4: training loss 427480624790.588
Test Loss of 418074324532.467285, Test MSE of 418074321798.656921
Epoch 5: training loss 420659927762.824
Test Loss of 396545575391.903748, Test MSE of 396545574053.863403
Epoch 6: training loss 375446081656.471
Test Loss of 329994011101.535034, Test MSE of 329994004980.370667
Epoch 7: training loss 296812982091.294
Test Loss of 244712364402.942413, Test MSE of 244712365020.399567
Epoch 8: training loss 217029565741.176
Test Loss of 174377164650.296539, Test MSE of 174377163231.262054
Epoch 9: training loss 158648415593.412
Test Loss of 126389608677.292618, Test MSE of 126389609652.901108
Epoch 10: training loss 139519427343.059
Test Loss of 118611387294.645386, Test MSE of 118611388004.947662
Epoch 11: training loss 135879030497.882
Test Loss of 115636657668.145264, Test MSE of 115636658453.607132
Epoch 12: training loss 133772851591.529
Test Loss of 113020677140.607910, Test MSE of 113020674854.976990
Epoch 13: training loss 128965688500.706
Test Loss of 108626517434.241043, Test MSE of 108626519024.579407
Epoch 14: training loss 125493432651.294
Test Loss of 106628884004.833679, Test MSE of 106628884372.284485
Epoch 15: training loss 121381895469.176
Test Loss of 103041259428.567200, Test MSE of 103041257390.687134
Epoch 16: training loss 117492221259.294
Test Loss of 98740828132.996536, Test MSE of 98740827116.234131
Epoch 17: training loss 114334494268.235
Test Loss of 96546228078.560257, Test MSE of 96546230205.184937
Epoch 18: training loss 110278335006.118
Test Loss of 92881324161.332413, Test MSE of 92881323947.067841
Epoch 19: training loss 105910285703.529
Test Loss of 90035412453.825577, Test MSE of 90035412202.211746
Epoch 20: training loss 101933620344.471
Test Loss of 87195312543.000687, Test MSE of 87195311554.714890
Epoch 21: training loss 98833968052.706
Test Loss of 84156131049.911636, Test MSE of 84156132056.589661
Epoch 22: training loss 95000904402.824
Test Loss of 81170050900.504272, Test MSE of 81170048841.863098
Epoch 23: training loss 90603922861.176
Test Loss of 78281871268.093460, Test MSE of 78281870991.397781
Epoch 24: training loss 88130422467.765
Test Loss of 74742791909.647934, Test MSE of 74742793141.756531
Epoch 25: training loss 83673895243.294
Test Loss of 69103568379.144119, Test MSE of 69103569104.359100
Epoch 26: training loss 79845296128.000
Test Loss of 66460610386.609299, Test MSE of 66460610999.099899
Epoch 27: training loss 76771958806.588
Test Loss of 66350912976.507057, Test MSE of 66350913882.119186
Epoch 28: training loss 73419658368.000
Test Loss of 63280326017.865372, Test MSE of 63280325207.323631
Epoch 29: training loss 70862671969.882
Test Loss of 57296831371.695580, Test MSE of 57296830354.882904
Epoch 30: training loss 67051744384.000
Test Loss of 56670632994.820267, Test MSE of 56670633480.751389
Epoch 31: training loss 63996110351.059
Test Loss of 53723950675.734444, Test MSE of 53723949641.359268
Epoch 32: training loss 61041089261.176
Test Loss of 52773565658.159615, Test MSE of 52773566596.032127
Epoch 33: training loss 57619621308.235
Test Loss of 50566968919.050659, Test MSE of 50566969725.948563
Epoch 34: training loss 55090652280.471
Test Loss of 44430146034.853577, Test MSE of 44430146629.233734
Epoch 35: training loss 52149312824.471
Test Loss of 46030249081.041870, Test MSE of 46030248705.539978
Epoch 36: training loss 50560423559.529
Test Loss of 43315097392.499657, Test MSE of 43315097675.332726
Epoch 37: training loss 47276618465.882
Test Loss of 39165173308.994682, Test MSE of 39165173592.803070
Epoch 38: training loss 45482535491.765
Test Loss of 37206587256.035164, Test MSE of 37206587423.994461
Epoch 39: training loss 43005699207.529
Test Loss of 35569023591.394867, Test MSE of 35569024220.109398
Epoch 40: training loss 40900162078.118
Test Loss of 36877420374.873001, Test MSE of 36877419696.709457
Epoch 41: training loss 38812006159.059
Test Loss of 34163922666.859127, Test MSE of 34163923096.735931
Epoch 42: training loss 36638600320.000
Test Loss of 30248248151.583622, Test MSE of 30248247944.688137
Epoch 43: training loss 35170293270.588
Test Loss of 31870097123.279205, Test MSE of 31870097570.577316
Epoch 44: training loss 33371397289.412
Test Loss of 29703812514.080036, Test MSE of 29703812359.694191
Epoch 45: training loss 31908987550.118
Test Loss of 27075100867.182976, Test MSE of 27075100835.397217
Epoch 46: training loss 30142621440.000
Test Loss of 26288868362.185520, Test MSE of 26288868012.323265
Epoch 47: training loss 28909605067.294
Test Loss of 25956450636.569050, Test MSE of 25956450893.310184
Epoch 48: training loss 27763833863.529
Test Loss of 22350928900.026833, Test MSE of 22350928917.491158
Epoch 49: training loss 26254188423.529
Test Loss of 26703348916.260006, Test MSE of 26703349642.076675
Epoch 50: training loss 25320133632.000
Test Loss of 22924445840.965996, Test MSE of 22924445861.695271
Epoch 51: training loss 23983916449.882
Test Loss of 22137457260.132317, Test MSE of 22137457059.363350
Epoch 52: training loss 23128030128.941
Test Loss of 19843575158.969234, Test MSE of 19843575012.354961
Epoch 53: training loss 22376845168.941
Test Loss of 20509796289.939392, Test MSE of 20509796082.805027
Epoch 54: training loss 21697695491.765
Test Loss of 19090711546.078186, Test MSE of 19090711918.524334
Epoch 55: training loss 20729360338.824
Test Loss of 20542169852.150822, Test MSE of 20542169889.996681
Epoch 56: training loss 19892635034.353
Test Loss of 18507678837.251907, Test MSE of 18507679039.704918
Epoch 57: training loss 19558258631.529
Test Loss of 21142636368.714317, Test MSE of 21142636616.413261
Epoch 58: training loss 18824844216.471
Test Loss of 18760657319.054359, Test MSE of 18760657119.676205
Epoch 59: training loss 18223599992.471
Test Loss of 18014244207.863056, Test MSE of 18014244546.175194
Epoch 60: training loss 17816214806.588
Test Loss of 18586014372.034237, Test MSE of 18586014016.080181
Epoch 61: training loss 16916366849.882
Test Loss of 18588587881.822807, Test MSE of 18588587297.056160
Epoch 62: training loss 16523653790.118
Test Loss of 18988829818.463104, Test MSE of 18988829914.034992
Epoch 63: training loss 16027975311.059
Test Loss of 18280831940.071247, Test MSE of 18280831950.685585
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18280831950.685585, 'MSE - std': 0.0, 'R2 - mean': 0.8576451781448321, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005521 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917453914.353
Test Loss of 424557487802.537109, Test MSE of 424557486217.708252
Epoch 2: training loss 427895591152.941
Test Loss of 424539590554.618530, Test MSE of 424539597628.151733
Epoch 3: training loss 427866703510.588
Test Loss of 424515923129.471191, Test MSE of 424515935111.045776
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887909225.412
Test Loss of 424523959278.708313, Test MSE of 424523959708.661499
Epoch 2: training loss 427875127055.059
Test Loss of 424524084135.172791, Test MSE of 424524089467.983337
Epoch 3: training loss 427874518196.706
Test Loss of 424522056665.389771, Test MSE of 424522059386.217957
Epoch 4: training loss 427874040410.353
Test Loss of 424522105843.919495, Test MSE of 424522102321.246826
Epoch 5: training loss 421204215446.588
Test Loss of 403376034141.623901, Test MSE of 403376038871.720215
Epoch 6: training loss 375673345204.706
Test Loss of 338018421943.576233, Test MSE of 338018427759.414734
Epoch 7: training loss 296160673551.059
Test Loss of 253680384691.194092, Test MSE of 253680387057.795746
Epoch 8: training loss 217503934162.824
Test Loss of 184669816474.322449, Test MSE of 184669815395.766296
Epoch 9: training loss 156655363192.471
Test Loss of 137393655399.631744, Test MSE of 137393655985.462753
Epoch 10: training loss 136644891497.412
Test Loss of 129745313303.095078, Test MSE of 129745312245.048157
Epoch 11: training loss 133568751013.647
Test Loss of 127336826213.440674, Test MSE of 127336825656.846634
Epoch 12: training loss 130146194853.647
Test Loss of 124242412067.649323, Test MSE of 124242414943.328018
Epoch 13: training loss 126399400749.176
Test Loss of 120106096671.740921, Test MSE of 120106098336.148453
Epoch 14: training loss 122007477428.706
Test Loss of 117750647392.762436, Test MSE of 117750646972.270737
Epoch 15: training loss 118543228717.176
Test Loss of 113602897770.770294, Test MSE of 113602900716.584076
Epoch 16: training loss 114256759115.294
Test Loss of 109756691910.084671, Test MSE of 109756691860.087753
Epoch 17: training loss 110349867851.294
Test Loss of 105848818950.217911, Test MSE of 105848819734.144226
Epoch 18: training loss 107581275738.353
Test Loss of 100570794732.043488, Test MSE of 100570795338.837418
Epoch 19: training loss 102275024715.294
Test Loss of 97523867077.610916, Test MSE of 97523866778.667084
Epoch 20: training loss 98872049061.647
Test Loss of 92629055145.482300, Test MSE of 92629054606.760147
Epoch 21: training loss 93679815710.118
Test Loss of 89385228160.562576, Test MSE of 89385227699.554062
Epoch 22: training loss 90545554522.353
Test Loss of 85365991512.590332, Test MSE of 85365992126.558502
Epoch 23: training loss 86275260912.941
Test Loss of 84079821077.614624, Test MSE of 84079822608.077255
Epoch 24: training loss 82780443151.059
Test Loss of 80880827941.307419, Test MSE of 80880828314.908691
Epoch 25: training loss 79235761347.765
Test Loss of 74712778452.119354, Test MSE of 74712778120.675171
Epoch 26: training loss 74429361076.706
Test Loss of 69702273687.716858, Test MSE of 69702273342.925323
Epoch 27: training loss 72128230942.118
Test Loss of 67920138114.931297, Test MSE of 67920137207.028709
Epoch 28: training loss 68751436167.529
Test Loss of 64493500808.734673, Test MSE of 64493499405.013809
Epoch 29: training loss 64585319002.353
Test Loss of 61273400171.480919, Test MSE of 61273398916.641258
Epoch 30: training loss 61702840033.882
Test Loss of 57968168869.040947, Test MSE of 57968168834.964539
Epoch 31: training loss 58316335254.588
Test Loss of 53334414472.201714, Test MSE of 53334414192.286980
Epoch 32: training loss 55027246125.176
Test Loss of 54165431013.411057, Test MSE of 54165431675.360123
Epoch 33: training loss 52440541018.353
Test Loss of 52121823016.445984, Test MSE of 52121822608.999222
Epoch 34: training loss 48880139512.471
Test Loss of 49974514596.804070, Test MSE of 49974512783.490822
Epoch 35: training loss 46929974543.059
Test Loss of 45124821093.381447, Test MSE of 45124822384.720932
Epoch 36: training loss 44787572668.235
Test Loss of 42438673994.259544, Test MSE of 42438674763.673080
Epoch 37: training loss 42663250288.941
Test Loss of 42033384686.530647, Test MSE of 42033385184.997208
Epoch 38: training loss 39366470957.176
Test Loss of 38324269096.268333, Test MSE of 38324269044.864517
Epoch 39: training loss 37637672606.118
Test Loss of 36780563608.309044, Test MSE of 36780562474.914787
Epoch 40: training loss 35116455484.235
Test Loss of 35682082006.369652, Test MSE of 35682081717.219246
Epoch 41: training loss 33455086576.941
Test Loss of 35780250651.240341, Test MSE of 35780250682.388977
Epoch 42: training loss 31343628604.235
Test Loss of 33167520491.569744, Test MSE of 33167520665.777679
Epoch 43: training loss 29913900928.000
Test Loss of 30829359309.605366, Test MSE of 30829360044.869492
Epoch 44: training loss 28506441524.706
Test Loss of 29611031459.382835, Test MSE of 29611031579.395210
Epoch 45: training loss 26759465938.824
Test Loss of 30518381204.400646, Test MSE of 30518379998.149826
Epoch 46: training loss 25370752421.647
Test Loss of 27922981712.003700, Test MSE of 27922981628.881680
Epoch 47: training loss 24657448335.059
Test Loss of 31276764459.170021, Test MSE of 31276764662.434467
Epoch 48: training loss 22464983341.176
Test Loss of 30243983896.279434, Test MSE of 30243984177.217648
Epoch 49: training loss 21905421549.176
Test Loss of 27774668558.390007, Test MSE of 27774668586.730537
Epoch 50: training loss 20668108916.706
Test Loss of 27723950763.140411, Test MSE of 27723951688.759190
Epoch 51: training loss 19964802300.235
Test Loss of 26971419943.380058, Test MSE of 26971420571.578484
Epoch 52: training loss 19178213677.176
Test Loss of 27393752433.758038, Test MSE of 27393752549.689800
Epoch 53: training loss 18624152124.235
Test Loss of 23828991127.124683, Test MSE of 23828991207.497002
Epoch 54: training loss 17651761219.765
Test Loss of 25826951443.719639, Test MSE of 25826951799.939209
Epoch 55: training loss 16934582151.529
Test Loss of 25060207116.198936, Test MSE of 25060207220.629982
Epoch 56: training loss 16197481212.235
Test Loss of 26084333283.279205, Test MSE of 26084333469.135063
Epoch 57: training loss 15434389202.824
Test Loss of 25107103483.440205, Test MSE of 25107103720.496197
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21693967835.59089, 'MSE - std': 3413135884.905306, 'R2 - mean': 0.8391985634495018, 'R2 - std': 0.01844661469533032} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003544 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926397470.118
Test Loss of 447259813971.852905, Test MSE of 447259817783.619568
Epoch 2: training loss 421904465438.118
Test Loss of 447240595824.099915, Test MSE of 447240588766.909180
Epoch 3: training loss 421876753588.706
Test Loss of 447216070776.331238, Test MSE of 447216071174.413574
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898831149.176
Test Loss of 447223974906.078186, Test MSE of 447223977333.751465
Epoch 2: training loss 421888729328.941
Test Loss of 447225208917.274109, Test MSE of 447225208934.928650
Epoch 3: training loss 421888202992.941
Test Loss of 447224958052.197083, Test MSE of 447224954898.404236
Epoch 4: training loss 421887800380.235
Test Loss of 447224949562.685181, Test MSE of 447224948055.094971
Epoch 5: training loss 415292892461.176
Test Loss of 425754836025.559998, Test MSE of 425754835019.573914
Epoch 6: training loss 370439742403.765
Test Loss of 358912241102.138306, Test MSE of 358912244475.978210
Epoch 7: training loss 292419858311.529
Test Loss of 271313566751.030304, Test MSE of 271313566556.977509
Epoch 8: training loss 214142500562.824
Test Loss of 200144603104.259094, Test MSE of 200144599391.537048
Epoch 9: training loss 153929967495.529
Test Loss of 147925832863.415222, Test MSE of 147925834834.081879
Epoch 10: training loss 134699768380.235
Test Loss of 140059465526.658325, Test MSE of 140059466365.502472
Epoch 11: training loss 131996110908.235
Test Loss of 137057510859.769608, Test MSE of 137057511610.719604
Epoch 12: training loss 127644186172.235
Test Loss of 133678502616.619934, Test MSE of 133678502974.553268
Epoch 13: training loss 124290525665.882
Test Loss of 129187697724.876236, Test MSE of 129187699325.539673
Epoch 14: training loss 119788532946.824
Test Loss of 126801316604.150818, Test MSE of 126801314784.259033
Epoch 15: training loss 116203239213.176
Test Loss of 123069655180.702286, Test MSE of 123069655308.544846
Epoch 16: training loss 112774736203.294
Test Loss of 120300005437.113113, Test MSE of 120300006045.852371
Epoch 17: training loss 108686457163.294
Test Loss of 115379715618.464951, Test MSE of 115379716309.607880
Epoch 18: training loss 105193248978.824
Test Loss of 111960829674.385376, Test MSE of 111960831736.398422
Epoch 19: training loss 101071326659.765
Test Loss of 105495024905.534119, Test MSE of 105495027837.041382
Epoch 20: training loss 96848055130.353
Test Loss of 105612490200.086975, Test MSE of 105612489582.027359
Epoch 21: training loss 92454533120.000
Test Loss of 99646827644.831833, Test MSE of 99646830737.474472
Epoch 22: training loss 89851943047.529
Test Loss of 95550210362.566742, Test MSE of 95550210495.154755
Epoch 23: training loss 85586647476.706
Test Loss of 94643566815.133942, Test MSE of 94643565596.600937
Epoch 24: training loss 81802729592.471
Test Loss of 86576675482.559326, Test MSE of 86576674277.108841
Epoch 25: training loss 77788341835.294
Test Loss of 83809927766.813782, Test MSE of 83809928414.321716
Epoch 26: training loss 74750506571.294
Test Loss of 80036995166.749023, Test MSE of 80036997049.093369
Epoch 27: training loss 71110936154.353
Test Loss of 76409467388.802216, Test MSE of 76409467713.138596
Epoch 28: training loss 68317430272.000
Test Loss of 71520193785.900528, Test MSE of 71520193473.110275
Epoch 29: training loss 64792985178.353
Test Loss of 68466604399.152443, Test MSE of 68466603905.602043
Epoch 30: training loss 61182718554.353
Test Loss of 64777311611.232941, Test MSE of 64777313614.662033
Epoch 31: training loss 57960234405.647
Test Loss of 61918794162.897987, Test MSE of 61918795454.080025
Epoch 32: training loss 56048986684.235
Test Loss of 60305960578.161461, Test MSE of 60305961518.996971
Epoch 33: training loss 53144966912.000
Test Loss of 56352021663.888969, Test MSE of 56352020683.879257
Epoch 34: training loss 50070686305.882
Test Loss of 53014200966.188293, Test MSE of 53014200496.120529
Epoch 35: training loss 47347089656.471
Test Loss of 50838217838.145729, Test MSE of 50838217745.224792
Epoch 36: training loss 44552117428.706
Test Loss of 45896877145.300949, Test MSE of 45896876683.362907
Epoch 37: training loss 42731655649.882
Test Loss of 45644746616.508904, Test MSE of 45644746583.707626
Epoch 38: training loss 40054387019.294
Test Loss of 42926337184.836456, Test MSE of 42926336865.896149
Epoch 39: training loss 38297765421.176
Test Loss of 45401364612.174881, Test MSE of 45401364631.099701
Epoch 40: training loss 36492770665.412
Test Loss of 36448699055.877861, Test MSE of 36448699091.331062
Epoch 41: training loss 33841444999.529
Test Loss of 38788375987.134857, Test MSE of 38788375935.845261
Epoch 42: training loss 32438731339.294
Test Loss of 37849089228.894753, Test MSE of 37849088755.424042
Epoch 43: training loss 30938665750.588
Test Loss of 32131998523.158916, Test MSE of 32131998263.963364
Epoch 44: training loss 29334278836.706
Test Loss of 31643952172.768909, Test MSE of 31643952026.550358
Epoch 45: training loss 27596582840.471
Test Loss of 31256176296.534813, Test MSE of 31256175884.803661
Epoch 46: training loss 26722082326.588
Test Loss of 32329258892.879944, Test MSE of 32329259518.095543
Epoch 47: training loss 25260306469.647
Test Loss of 29965478533.240807, Test MSE of 29965478693.494888
Epoch 48: training loss 24051024154.353
Test Loss of 27685663309.338886, Test MSE of 27685663386.554165
Epoch 49: training loss 23334373146.353
Test Loss of 31436501062.114273, Test MSE of 31436501248.521614
Epoch 50: training loss 22141902320.941
Test Loss of 24003507461.270412, Test MSE of 24003507673.100857
Epoch 51: training loss 21134431653.647
Test Loss of 24388157931.747398, Test MSE of 24388158273.555588
Epoch 52: training loss 20386529287.529
Test Loss of 27590595791.737221, Test MSE of 27590595585.349575
Epoch 53: training loss 19557789040.941
Test Loss of 22303749798.166088, Test MSE of 22303749719.981106
Epoch 54: training loss 18955848417.882
Test Loss of 23657658262.354847, Test MSE of 23657658089.690022
Epoch 55: training loss 18193409449.412
Test Loss of 23039937148.239647, Test MSE of 23039936833.114727
Epoch 56: training loss 17793031017.412
Test Loss of 24411036618.703678, Test MSE of 24411036474.758087
Epoch 57: training loss 17267615450.353
Test Loss of 23681568689.358315, Test MSE of 23681569247.896908
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22356501639.692898, 'MSE - std': 2940107673.035343, 'R2 - mean': 0.8402501610870953, 'R2 - std': 0.015134842064854912} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005336 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110917089.882
Test Loss of 410765907457.184631, Test MSE of 410765905020.391846
Epoch 2: training loss 430090572739.765
Test Loss of 410747093454.482178, Test MSE of 410747091283.835754
Epoch 3: training loss 430062817039.059
Test Loss of 410722690999.026367, Test MSE of 410722688617.313721
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075010108.235
Test Loss of 410724890048.740417, Test MSE of 410724887511.442688
Epoch 2: training loss 430065704960.000
Test Loss of 410726643341.445618, Test MSE of 410726636039.076538
Epoch 3: training loss 430065306202.353
Test Loss of 410727112450.013855, Test MSE of 410727111267.160522
Epoch 4: training loss 430064991774.118
Test Loss of 410726671135.866699, Test MSE of 410726673141.971252
Epoch 5: training loss 423480864768.000
Test Loss of 389827596665.188354, Test MSE of 389827598295.929443
Epoch 6: training loss 378656126373.647
Test Loss of 323639902526.430359, Test MSE of 323639903431.543274
Epoch 7: training loss 300427766844.235
Test Loss of 239249858465.229065, Test MSE of 239249860168.965607
Epoch 8: training loss 221792230339.765
Test Loss of 169236899569.902832, Test MSE of 169236899857.939667
Epoch 9: training loss 162267108924.235
Test Loss of 120373684757.560394, Test MSE of 120373682877.359695
Epoch 10: training loss 142667440820.706
Test Loss of 112681189444.235077, Test MSE of 112681190535.609131
Epoch 11: training loss 137749960161.882
Test Loss of 109839551369.062469, Test MSE of 109839552400.512634
Epoch 12: training loss 133462593234.824
Test Loss of 107118494715.735306, Test MSE of 107118494792.953613
Epoch 13: training loss 131852142682.353
Test Loss of 104404754250.039795, Test MSE of 104404752241.085663
Epoch 14: training loss 127729526362.353
Test Loss of 102407227959.677933, Test MSE of 102407228149.510986
Epoch 15: training loss 123664544527.059
Test Loss of 98438599541.160568, Test MSE of 98438598224.039871
Epoch 16: training loss 121720237778.824
Test Loss of 95040280667.927811, Test MSE of 95040283770.985214
Epoch 17: training loss 116285071450.353
Test Loss of 92239148533.812119, Test MSE of 92239147481.031525
Epoch 18: training loss 113206998618.353
Test Loss of 88756229436.061081, Test MSE of 88756229096.684662
Epoch 19: training loss 109594703781.647
Test Loss of 85008159362.546967, Test MSE of 85008159383.313461
Epoch 20: training loss 105194208256.000
Test Loss of 82342232230.796860, Test MSE of 82342232581.199097
Epoch 21: training loss 101552968493.176
Test Loss of 79948248171.565018, Test MSE of 79948246636.977783
Epoch 22: training loss 98014003094.588
Test Loss of 76147683288.196213, Test MSE of 76147683622.381409
Epoch 23: training loss 93746718584.471
Test Loss of 74054968740.309113, Test MSE of 74054969854.838181
Epoch 24: training loss 88902127043.765
Test Loss of 71761040677.316055, Test MSE of 71761039806.493866
Epoch 25: training loss 87270381161.412
Test Loss of 68520304719.607590, Test MSE of 68520304635.084236
Epoch 26: training loss 82252186774.588
Test Loss of 65055236399.267006, Test MSE of 65055236380.629776
Epoch 27: training loss 79237314590.118
Test Loss of 62822008630.137901, Test MSE of 62822010202.332207
Epoch 28: training loss 75714114770.824
Test Loss of 57834867771.231834, Test MSE of 57834868235.832146
Epoch 29: training loss 71608425020.235
Test Loss of 57414104004.768166, Test MSE of 57414103795.489227
Epoch 30: training loss 68659180062.118
Test Loss of 55986622321.843590, Test MSE of 55986620932.225197
Epoch 31: training loss 66093959047.529
Test Loss of 52531423769.351227, Test MSE of 52531425097.006981
Epoch 32: training loss 63426851523.765
Test Loss of 51943084164.679314, Test MSE of 51943083483.973549
Epoch 33: training loss 60071770910.118
Test Loss of 48587358894.141602, Test MSE of 48587358598.492638
Epoch 34: training loss 56964824139.294
Test Loss of 43678710649.899124, Test MSE of 43678710692.625450
Epoch 35: training loss 54551844668.235
Test Loss of 45621406561.258675, Test MSE of 45621406410.391953
Epoch 36: training loss 51966200312.471
Test Loss of 40045970793.551132, Test MSE of 40045970931.759384
Epoch 37: training loss 49689146277.647
Test Loss of 41276130682.609901, Test MSE of 41276131515.089668
Epoch 38: training loss 46427898307.765
Test Loss of 38634316081.162422, Test MSE of 38634315261.080620
Epoch 39: training loss 44198886332.235
Test Loss of 36666300384.725594, Test MSE of 36666300222.390175
Epoch 40: training loss 41831342878.118
Test Loss of 34000702022.841278, Test MSE of 34000702199.810879
Epoch 41: training loss 39840716619.294
Test Loss of 32699780469.871357, Test MSE of 32699780345.987015
Epoch 42: training loss 38077736463.059
Test Loss of 32733507544.196205, Test MSE of 32733507031.859779
Epoch 43: training loss 36335733172.706
Test Loss of 29380738221.904675, Test MSE of 29380738096.831104
Epoch 44: training loss 34468009773.176
Test Loss of 28494586711.781582, Test MSE of 28494587122.741520
Epoch 45: training loss 33228622305.882
Test Loss of 27251776722.865341, Test MSE of 27251776673.179272
Epoch 46: training loss 31471978496.000
Test Loss of 28223005625.395649, Test MSE of 28223005539.353603
Epoch 47: training loss 29918582317.176
Test Loss of 27022267860.642296, Test MSE of 27022267285.954285
Epoch 48: training loss 28773162853.647
Test Loss of 24890004671.911152, Test MSE of 24890004903.397907
Epoch 49: training loss 27610561106.824
Test Loss of 26532497039.341045, Test MSE of 26532496923.137894
Epoch 50: training loss 26307177509.647
Test Loss of 23054949904.347988, Test MSE of 23054949985.874191
Epoch 51: training loss 24873170492.235
Test Loss of 23864744750.082371, Test MSE of 23864744732.407394
Epoch 52: training loss 23841215909.647
Test Loss of 22539343016.218418, Test MSE of 22539343225.794281
Epoch 53: training loss 23499921999.059
Test Loss of 20545664483.805645, Test MSE of 20545664693.180096
Epoch 54: training loss 22145785848.471
Test Loss of 20107722696.085144, Test MSE of 20107722812.328850
Epoch 55: training loss 21562587670.588
Test Loss of 22308772927.022675, Test MSE of 22308773208.500664
Epoch 56: training loss 20538941304.471
Test Loss of 19717591738.461823, Test MSE of 19717591467.277760
Epoch 57: training loss 19825644400.941
Test Loss of 19793400677.049515, Test MSE of 19793400755.556637
Epoch 58: training loss 19375045138.824
Test Loss of 19999982959.237389, Test MSE of 19999983059.229900
Epoch 59: training loss 18832435843.765
Test Loss of 19389975168.651550, Test MSE of 19389975634.739979
Epoch 60: training loss 18087031589.647
Test Loss of 17885877437.068024, Test MSE of 17885877338.408924
Epoch 61: training loss 17577943491.765
Test Loss of 19375061092.457195, Test MSE of 19375060845.907669
Epoch 62: training loss 17116548039.529
Test Loss of 18451596704.518280, Test MSE of 18451596936.599964
Epoch 63: training loss 16461770142.118
Test Loss of 18400424491.357704, Test MSE of 18400424609.098766
Epoch 64: training loss 16166122782.118
Test Loss of 20354887403.268856, Test MSE of 20354887134.640053
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21856098013.429688, 'MSE - std': 2689681433.6112537, 'R2 - mean': 0.8381877478921428, 'R2 - std': 0.013585220933286962} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005225 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043014746.353
Test Loss of 431614451912.914368, Test MSE of 431614448652.500610
Epoch 2: training loss 424023136737.882
Test Loss of 431594517581.238342, Test MSE of 431594525487.302429
Epoch 3: training loss 423995964958.118
Test Loss of 431567655933.630737, Test MSE of 431567658521.954773
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010925959.529
Test Loss of 431574369264.362793, Test MSE of 431574365237.835999
Epoch 2: training loss 424001530578.824
Test Loss of 431576200269.238342, Test MSE of 431576192268.330261
Epoch 3: training loss 424001029541.647
Test Loss of 431576246153.536316, Test MSE of 431576248692.405518
Epoch 4: training loss 424000581391.059
Test Loss of 431576083208.174011, Test MSE of 431576073300.789246
Epoch 5: training loss 417324567732.706
Test Loss of 409931966391.026367, Test MSE of 409931958747.912720
Epoch 6: training loss 372399710689.882
Test Loss of 342750121337.662170, Test MSE of 342750121779.864258
Epoch 7: training loss 294460203429.647
Test Loss of 256058501888.592316, Test MSE of 256058498523.809174
Epoch 8: training loss 216159263171.765
Test Loss of 184105667347.546509, Test MSE of 184105666170.988800
Epoch 9: training loss 156787384832.000
Test Loss of 132750040561.073578, Test MSE of 132750040336.784973
Epoch 10: training loss 138230957598.118
Test Loss of 124209086096.288757, Test MSE of 124209085446.575745
Epoch 11: training loss 134686336030.118
Test Loss of 121116140945.354935, Test MSE of 121116139088.131714
Epoch 12: training loss 130687916092.235
Test Loss of 117645773325.978714, Test MSE of 117645773208.136337
Epoch 13: training loss 128405881675.294
Test Loss of 112985102117.079132, Test MSE of 112985103047.576523
Epoch 14: training loss 123810194913.882
Test Loss of 109952980792.033325, Test MSE of 109952979349.722992
Epoch 15: training loss 119585121867.294
Test Loss of 107468490724.516434, Test MSE of 107468489947.612152
Epoch 16: training loss 116334062441.412
Test Loss of 103566344461.149475, Test MSE of 103566342298.229172
Epoch 17: training loss 112141299531.294
Test Loss of 100269319705.825089, Test MSE of 100269320684.976974
Epoch 18: training loss 108348916344.471
Test Loss of 96547112011.342896, Test MSE of 96547115488.582657
Epoch 19: training loss 104375963888.941
Test Loss of 95110740620.971771, Test MSE of 95110739430.214615
Epoch 20: training loss 100382544564.706
Test Loss of 90915525421.134659, Test MSE of 90915522876.934692
Epoch 21: training loss 95692165571.765
Test Loss of 84229848454.930130, Test MSE of 84229849339.083405
Epoch 22: training loss 92636546273.882
Test Loss of 81020716945.117996, Test MSE of 81020716489.545410
Epoch 23: training loss 88742950490.353
Test Loss of 80458953924.649704, Test MSE of 80458954487.001602
Epoch 24: training loss 85052080353.882
Test Loss of 74766573439.111526, Test MSE of 74766574642.583649
Epoch 25: training loss 81390817957.647
Test Loss of 70053956140.305420, Test MSE of 70053953800.506561
Epoch 26: training loss 77778098808.471
Test Loss of 66140700575.333641, Test MSE of 66140700875.814278
Epoch 27: training loss 75027502064.941
Test Loss of 65732695534.230446, Test MSE of 65732694592.696671
Epoch 28: training loss 71186960037.647
Test Loss of 59070390560.103653, Test MSE of 59070389523.192245
Epoch 29: training loss 67345968843.294
Test Loss of 58793897093.153168, Test MSE of 58793897798.131317
Epoch 30: training loss 64694090593.882
Test Loss of 55588818454.981956, Test MSE of 55588817549.144829
Epoch 31: training loss 62284367405.176
Test Loss of 53329856306.347061, Test MSE of 53329855647.485596
Epoch 32: training loss 59214986277.647
Test Loss of 51232248660.938454, Test MSE of 51232248535.870956
Epoch 33: training loss 55911010364.235
Test Loss of 47281888629.871353, Test MSE of 47281888717.920143
Epoch 34: training loss 53176731768.471
Test Loss of 45072291209.773254, Test MSE of 45072291533.421494
Epoch 35: training loss 49448992677.647
Test Loss of 41514177823.629799, Test MSE of 41514176954.720833
Epoch 36: training loss 47411159085.176
Test Loss of 39255297251.924110, Test MSE of 39255297540.259377
Epoch 37: training loss 45585952730.353
Test Loss of 36428220460.542343, Test MSE of 36428221112.652306
Epoch 38: training loss 43452706213.647
Test Loss of 36088977211.824158, Test MSE of 36088976811.848999
Epoch 39: training loss 40701744926.118
Test Loss of 36157481709.638130, Test MSE of 36157481751.620445
Epoch 40: training loss 38758312124.235
Test Loss of 32405061914.417400, Test MSE of 32405062245.155376
Epoch 41: training loss 36732511593.412
Test Loss of 30788152981.975010, Test MSE of 30788153102.486076
Epoch 42: training loss 35591353057.882
Test Loss of 31692828938.306339, Test MSE of 31692829078.815838
Epoch 43: training loss 33115784015.059
Test Loss of 29879875911.907452, Test MSE of 29879876099.792225
Epoch 44: training loss 31840219452.235
Test Loss of 28297574796.142525, Test MSE of 28297574984.276485
Epoch 45: training loss 30056581925.647
Test Loss of 27180388943.370663, Test MSE of 27180389383.575130
Epoch 46: training loss 28886466017.882
Test Loss of 25158680302.111984, Test MSE of 25158680624.844372
Epoch 47: training loss 27201154025.412
Test Loss of 27302583054.807961, Test MSE of 27302582945.219406
Epoch 48: training loss 26227967856.941
Test Loss of 23671585790.578438, Test MSE of 23671585938.324219
Epoch 49: training loss 25037849588.706
Test Loss of 24020397750.197132, Test MSE of 24020397871.860973
Epoch 50: training loss 24034382354.824
Test Loss of 28048971643.320686, Test MSE of 28048971892.062111
Epoch 51: training loss 22972795271.529
Test Loss of 23544481290.661732, Test MSE of 23544481201.177197
Epoch 52: training loss 21921427147.294
Test Loss of 23722418588.253586, Test MSE of 23722418490.668507
Epoch 53: training loss 21050029492.706
Test Loss of 22382332864.977325, Test MSE of 22382333186.610893
Epoch 54: training loss 20660433249.882
Test Loss of 22766863476.094402, Test MSE of 22766863655.883446
Epoch 55: training loss 20044457065.412
Test Loss of 21978706008.136974, Test MSE of 21978705965.922062
Epoch 56: training loss 19012040624.941
Test Loss of 22780934727.788986, Test MSE of 22780934687.079727
Epoch 57: training loss 18777451783.529
Test Loss of 22201560858.654327, Test MSE of 22201561312.302143
Epoch 58: training loss 18239146834.824
Test Loss of 20179648935.626099, Test MSE of 20179648920.909325
Epoch 59: training loss 17715423875.765
Test Loss of 18343227345.562241, Test MSE of 18343227587.348328
Epoch 60: training loss 16585421187.765
Test Loss of 21040864202.454418, Test MSE of 21040863959.762688
Epoch 61: training loss 16477072259.765
Test Loss of 20655384218.713558, Test MSE of 20655384278.182526
Epoch 62: training loss 16227963169.882
Test Loss of 21475885997.075428, Test MSE of 21475885860.556461
Epoch 63: training loss 15584558927.059
Test Loss of 22911744191.437298, Test MSE of 22911744095.531361
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22067227229.85002, 'MSE - std': 2442501004.2076807, 'R2 - mean': 0.8363290472026741, 'R2 - std': 0.012706913679454162} 
 

Saving model.....
Results After CV: {'MSE - mean': 22067227229.85002, 'MSE - std': 2442501004.2076807, 'R2 - mean': 0.8363290472026741, 'R2 - std': 0.012706913679454162}
Train time: 91.7413119224002
Inference time: 0.0705066374001035
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 41 finished with value: 22067227229.85002 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005664 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526023168.000
Test Loss of 418111352402.786926, Test MSE of 418111353135.827209
Epoch 2: training loss 427506808350.118
Test Loss of 418094007026.202148, Test MSE of 418094007992.320984
Epoch 3: training loss 427481374117.647
Test Loss of 418071635957.103882, Test MSE of 418071640443.429626
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495748065.882
Test Loss of 418075024174.367798, Test MSE of 418075025363.442871
Epoch 2: training loss 427483835813.647
Test Loss of 418076228934.647217, Test MSE of 418076229336.693359
Epoch 3: training loss 427483392963.765
Test Loss of 418075472966.351135, Test MSE of 418075479179.741150
Epoch 4: training loss 422195320109.176
Test Loss of 401292027862.784180, Test MSE of 401292027476.518005
Epoch 5: training loss 386081285541.647
Test Loss of 346672036538.773987, Test MSE of 346672031945.623779
Epoch 6: training loss 318207269586.824
Test Loss of 268809560062.105011, Test MSE of 268809559950.493652
Epoch 7: training loss 226127522002.824
Test Loss of 170881098014.142029, Test MSE of 170881097130.079803
Epoch 8: training loss 161537212807.529
Test Loss of 130823500066.879486, Test MSE of 130823500811.658310
Epoch 9: training loss 142159038855.529
Test Loss of 120258667104.288681, Test MSE of 120258665858.691330
Epoch 10: training loss 136168342648.471
Test Loss of 116212155133.572052, Test MSE of 116212156192.634827
Epoch 11: training loss 133026182874.353
Test Loss of 112927875001.885727, Test MSE of 112927874289.444748
Epoch 12: training loss 130655568730.353
Test Loss of 111415374912.666199, Test MSE of 111415376177.097641
Epoch 13: training loss 127022824026.353
Test Loss of 107420344138.081894, Test MSE of 107420345078.381195
Epoch 14: training loss 121919316178.824
Test Loss of 104337705623.716858, Test MSE of 104337706043.252060
Epoch 15: training loss 119522985923.765
Test Loss of 101317157621.992142, Test MSE of 101317157862.024429
Epoch 16: training loss 115463273863.529
Test Loss of 98718924352.784637, Test MSE of 98718925530.330429
Epoch 17: training loss 112573745453.176
Test Loss of 94421856148.933609, Test MSE of 94421857273.418167
Epoch 18: training loss 107853106115.765
Test Loss of 92547330242.235489, Test MSE of 92547329851.846954
Epoch 19: training loss 104458905795.765
Test Loss of 88289666196.992828, Test MSE of 88289666752.922104
Epoch 20: training loss 101340592926.118
Test Loss of 85126365792.288681, Test MSE of 85126364644.447891
Epoch 21: training loss 97960490330.353
Test Loss of 81781502631.350449, Test MSE of 81781502783.765472
Epoch 22: training loss 93207197168.941
Test Loss of 78100909101.242661, Test MSE of 78100910182.021912
Epoch 23: training loss 89398658710.588
Test Loss of 76838729549.398102, Test MSE of 76838732009.451859
Epoch 24: training loss 86782797733.647
Test Loss of 72057250277.114960, Test MSE of 72057249960.752289
Epoch 25: training loss 81629452762.353
Test Loss of 69547894869.747864, Test MSE of 69547895048.498581
Epoch 26: training loss 78476615198.118
Test Loss of 67830095898.529724, Test MSE of 67830095249.818039
Epoch 27: training loss 75615636448.000
Test Loss of 63240588998.380753, Test MSE of 63240589025.311920
Epoch 28: training loss 73116267926.588
Test Loss of 61049640830.667595, Test MSE of 61049641226.006287
Epoch 29: training loss 68506006000.941
Test Loss of 56847214467.405045, Test MSE of 56847214629.736458
Epoch 30: training loss 65840001054.118
Test Loss of 56899773183.467033, Test MSE of 56899774357.020164
Epoch 31: training loss 62867958851.765
Test Loss of 52044600991.059914, Test MSE of 52044600541.114136
Epoch 32: training loss 59917994160.941
Test Loss of 51085235096.960442, Test MSE of 51085234299.929268
Epoch 33: training loss 56881168820.706
Test Loss of 49430939189.888504, Test MSE of 49430940059.068069
Epoch 34: training loss 54294319661.176
Test Loss of 46107788983.694656, Test MSE of 46107789233.139038
Epoch 35: training loss 52077035979.294
Test Loss of 45635811844.855888, Test MSE of 45635812077.083504
Epoch 36: training loss 50235859531.294
Test Loss of 40035639820.909554, Test MSE of 40035639415.675438
Epoch 37: training loss 47381272944.941
Test Loss of 40327282349.746010, Test MSE of 40327282027.854675
Epoch 38: training loss 44758929965.176
Test Loss of 39107974978.028221, Test MSE of 39107975714.405548
Epoch 39: training loss 42647511567.059
Test Loss of 34767357758.238258, Test MSE of 34767358385.437599
Epoch 40: training loss 40939748992.000
Test Loss of 34188624561.772842, Test MSE of 34188623856.240761
Epoch 41: training loss 38797191337.412
Test Loss of 34113419687.054359, Test MSE of 34113419843.815407
Epoch 42: training loss 36278084205.176
Test Loss of 31403785542.647236, Test MSE of 31403786075.777840
Epoch 43: training loss 35079442023.529
Test Loss of 29959660492.124912, Test MSE of 29959659841.745811
Epoch 44: training loss 33352432771.765
Test Loss of 28880338089.363869, Test MSE of 28880338427.875374
Epoch 45: training loss 31537221906.824
Test Loss of 27351009747.586399, Test MSE of 27351009993.594612
Epoch 46: training loss 30305571968.000
Test Loss of 27886610648.501503, Test MSE of 27886610260.161301
Epoch 47: training loss 29152909952.000
Test Loss of 24416594351.581772, Test MSE of 24416594166.652500
Epoch 48: training loss 28001793069.176
Test Loss of 23299706104.005550, Test MSE of 23299706285.938610
Epoch 49: training loss 26769450586.353
Test Loss of 24389354485.340736, Test MSE of 24389354447.356873
Epoch 50: training loss 26333871939.765
Test Loss of 21596303135.918575, Test MSE of 21596303263.993393
Epoch 51: training loss 24797017618.824
Test Loss of 23206489528.346054, Test MSE of 23206489752.262482
Epoch 52: training loss 23812862012.235
Test Loss of 22426108313.789497, Test MSE of 22426108235.416985
Epoch 53: training loss 22810953611.294
Test Loss of 21590355898.359474, Test MSE of 21590355859.548359
Epoch 54: training loss 22235294578.824
Test Loss of 24010846263.428173, Test MSE of 24010846322.312904
Epoch 55: training loss 21424048576.000
Test Loss of 21242217951.193153, Test MSE of 21242218005.046257
Epoch 56: training loss 20737735638.588
Test Loss of 23127066753.332409, Test MSE of 23127066513.722633
Epoch 57: training loss 19577748634.353
Test Loss of 22426842421.592411, Test MSE of 22426842705.964172
Epoch 58: training loss 19278860521.412
Test Loss of 22312167754.200325, Test MSE of 22312167633.283245
Epoch 59: training loss 18639381319.529
Test Loss of 20934809722.699978, Test MSE of 20934809369.140362
Epoch 60: training loss 17650521773.176
Test Loss of 20079751949.205643, Test MSE of 20079752245.054512
Epoch 61: training loss 17491869545.412
Test Loss of 19764593999.648392, Test MSE of 19764594076.147827
Epoch 62: training loss 16673550083.765
Test Loss of 20641305139.282906, Test MSE of 20641305239.393276
Epoch 63: training loss 16179513345.882
Test Loss of 19125225677.131622, Test MSE of 19125225533.670776
Epoch 64: training loss 15875628950.588
Test Loss of 19725010379.769604, Test MSE of 19725010664.463135
Epoch 65: training loss 15383323066.353
Test Loss of 21367676794.403885, Test MSE of 21367677404.496490
Epoch 66: training loss 15135788235.294
Test Loss of 18174815211.392090, Test MSE of 18174815204.626850
Epoch 67: training loss 15000104448.000
Test Loss of 19047877021.105713, Test MSE of 19047877175.965733
Epoch 68: training loss 14300669861.647
Test Loss of 19770993692.661579, Test MSE of 19770993851.694317
Epoch 69: training loss 13978453729.882
Test Loss of 18433601876.148972, Test MSE of 18433601795.705502
Epoch 70: training loss 13734618650.353
Test Loss of 19081512691.386536, Test MSE of 19081512679.994438
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19081512679.99444, 'MSE - std': 0.0, 'R2 - mean': 0.8514101904325058, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918264681.412
Test Loss of 424555089318.343750, Test MSE of 424555092660.591003
Epoch 2: training loss 427897452423.529
Test Loss of 424539893516.495056, Test MSE of 424539899234.597290
Epoch 3: training loss 427869999344.941
Test Loss of 424518490827.828796, Test MSE of 424518488301.545776
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890339599.059
Test Loss of 424525005148.676392, Test MSE of 424525001999.878296
Epoch 2: training loss 427878089426.824
Test Loss of 424525412433.010437, Test MSE of 424525414586.874878
Epoch 3: training loss 427877528756.706
Test Loss of 424525282725.396240, Test MSE of 424525279142.288818
Epoch 4: training loss 422298552922.353
Test Loss of 406820852482.309509, Test MSE of 406820854348.141174
Epoch 5: training loss 384906083508.706
Test Loss of 352921556237.324097, Test MSE of 352921547859.236450
Epoch 6: training loss 316157456143.059
Test Loss of 276154350679.879700, Test MSE of 276154354471.820679
Epoch 7: training loss 224632374332.235
Test Loss of 178456795268.174866, Test MSE of 178456797577.352814
Epoch 8: training loss 158531415642.353
Test Loss of 141062799528.653259, Test MSE of 141062800362.266174
Epoch 9: training loss 140137173142.588
Test Loss of 131813264889.722885, Test MSE of 131813268258.931412
Epoch 10: training loss 134336424327.529
Test Loss of 127784798574.204956, Test MSE of 127784800417.623383
Epoch 11: training loss 130849007104.000
Test Loss of 124613899954.009720, Test MSE of 124613901058.836319
Epoch 12: training loss 128305253827.765
Test Loss of 121657669938.513077, Test MSE of 121657671403.806686
Epoch 13: training loss 123725102832.941
Test Loss of 118343478309.662735, Test MSE of 118343479235.563553
Epoch 14: training loss 120274495427.765
Test Loss of 114931502771.430954, Test MSE of 114931501741.184540
Epoch 15: training loss 115264795949.176
Test Loss of 111143704118.835999, Test MSE of 111143705138.263260
Epoch 16: training loss 111346887710.118
Test Loss of 108211760882.912796, Test MSE of 108211761966.403946
Epoch 17: training loss 108883420943.059
Test Loss of 102749228991.570663, Test MSE of 102749229920.537445
Epoch 18: training loss 104733856496.941
Test Loss of 99638752646.602829, Test MSE of 99638751779.179001
Epoch 19: training loss 100101152737.882
Test Loss of 96234411644.002777, Test MSE of 96234411467.533081
Epoch 20: training loss 96031569167.059
Test Loss of 92677863862.214203, Test MSE of 92677865172.912338
Epoch 21: training loss 92450877741.176
Test Loss of 87858736956.817032, Test MSE of 87858737639.469025
Epoch 22: training loss 88488265758.118
Test Loss of 83706615449.374969, Test MSE of 83706613256.713821
Epoch 23: training loss 84685574610.824
Test Loss of 80339428145.684021, Test MSE of 80339426849.141281
Epoch 24: training loss 81580742445.176
Test Loss of 77363208575.733521, Test MSE of 77363207447.107330
Epoch 25: training loss 78204753272.471
Test Loss of 71070735171.212585, Test MSE of 71070735130.964172
Epoch 26: training loss 73953742336.000
Test Loss of 69577834454.310440, Test MSE of 69577836120.609970
Epoch 27: training loss 70307398023.529
Test Loss of 64713197162.000465, Test MSE of 64713196772.956352
Epoch 28: training loss 67399949236.706
Test Loss of 64991682717.283371, Test MSE of 64991683894.280029
Epoch 29: training loss 63538649750.588
Test Loss of 62749645489.299095, Test MSE of 62749645152.212952
Epoch 30: training loss 61141444472.471
Test Loss of 54296384225.147354, Test MSE of 54296383664.042534
Epoch 31: training loss 57271627520.000
Test Loss of 55180418572.198936, Test MSE of 55180418865.679359
Epoch 32: training loss 54560998550.588
Test Loss of 54009903093.103867, Test MSE of 54009904080.993309
Epoch 33: training loss 52292166068.706
Test Loss of 47576947844.411751, Test MSE of 47576947440.101372
Epoch 34: training loss 49329546744.471
Test Loss of 46285058984.594032, Test MSE of 46285059110.202843
Epoch 35: training loss 46628845402.353
Test Loss of 45446142760.682861, Test MSE of 45446142055.469543
Epoch 36: training loss 43635952865.882
Test Loss of 43380443752.342354, Test MSE of 43380445242.596939
Epoch 37: training loss 42329583766.588
Test Loss of 41595526547.393936, Test MSE of 41595526322.497726
Epoch 38: training loss 39722391356.235
Test Loss of 39085920751.537361, Test MSE of 39085919860.709885
Epoch 39: training loss 37445690428.235
Test Loss of 39114223048.927132, Test MSE of 39114223603.055847
Epoch 40: training loss 35344040907.294
Test Loss of 35347154333.816330, Test MSE of 35347154115.672699
Epoch 41: training loss 33880564886.588
Test Loss of 33724348344.938236, Test MSE of 33724349145.403263
Epoch 42: training loss 31892750117.647
Test Loss of 31443671014.891510, Test MSE of 31443671530.164600
Epoch 43: training loss 30033222912.000
Test Loss of 31495246067.031227, Test MSE of 31495245327.599449
Epoch 44: training loss 28485203064.471
Test Loss of 32397761296.521858, Test MSE of 32397761405.880077
Epoch 45: training loss 27427381970.824
Test Loss of 29551029621.548000, Test MSE of 29551028736.963299
Epoch 46: training loss 26184595471.059
Test Loss of 30322185606.602821, Test MSE of 30322186859.866856
Epoch 47: training loss 24654833626.353
Test Loss of 29235160790.961834, Test MSE of 29235161125.399841
Epoch 48: training loss 23501144271.059
Test Loss of 28333476992.621792, Test MSE of 28333476801.424885
Epoch 49: training loss 22431233351.529
Test Loss of 29436744031.518852, Test MSE of 29436744476.394646
Epoch 50: training loss 21578080813.176
Test Loss of 27694892881.188065, Test MSE of 27694892811.681499
Epoch 51: training loss 20377314285.176
Test Loss of 25820973224.890121, Test MSE of 25820972938.073257
Epoch 52: training loss 19879822836.706
Test Loss of 26941195834.152210, Test MSE of 26941195245.809406
Epoch 53: training loss 18922179934.118
Test Loss of 26201906988.235947, Test MSE of 26201907044.285213
Epoch 54: training loss 18493792572.235
Test Loss of 25143917266.224380, Test MSE of 25143917436.508644
Epoch 55: training loss 17655327096.471
Test Loss of 26517497776.647697, Test MSE of 26517498397.386871
Epoch 56: training loss 16880296274.824
Test Loss of 24438669317.211197, Test MSE of 24438669441.024502
Epoch 57: training loss 16581980224.000
Test Loss of 25634499218.505669, Test MSE of 25634499178.689827
Epoch 58: training loss 15791472282.353
Test Loss of 23741995454.741615, Test MSE of 23741995647.984711
Epoch 59: training loss 15237141590.588
Test Loss of 27128654722.457554, Test MSE of 27128654223.548435
Epoch 60: training loss 14645661665.882
Test Loss of 24931140198.684246, Test MSE of 24931140237.738613
Epoch 61: training loss 14322546345.412
Test Loss of 22949798780.772610, Test MSE of 22949798695.939255
Epoch 62: training loss 13648121434.353
Test Loss of 24441514779.654869, Test MSE of 24441514672.713856
Epoch 63: training loss 13577026304.000
Test Loss of 22433547507.504974, Test MSE of 22433547242.450729
Epoch 64: training loss 12994115403.294
Test Loss of 22299686619.225536, Test MSE of 22299686771.553226
Epoch 65: training loss 12846534720.000
Test Loss of 21829255008.111034, Test MSE of 21829254787.331924
Epoch 66: training loss 12381278919.529
Test Loss of 24037831857.891277, Test MSE of 24037832531.064056
Epoch 67: training loss 11942169298.824
Test Loss of 23681249216.044415, Test MSE of 23681248960.783756
Epoch 68: training loss 11496400368.941
Test Loss of 24932281907.519779, Test MSE of 24932281689.774281
Epoch 69: training loss 11487684453.647
Test Loss of 21897418132.578300, Test MSE of 21897418192.372185
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20489465436.18331, 'MSE - std': 1407952756.1888733, 'R2 - mean': 0.8475385814280967, 'R2 - std': 0.0038716090044090823} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005473 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926710211.765
Test Loss of 447259486246.373352, Test MSE of 447259495870.746033
Epoch 2: training loss 421905391375.059
Test Loss of 447240438601.371277, Test MSE of 447240432587.992798
Epoch 3: training loss 421877632542.118
Test Loss of 447215454104.249817, Test MSE of 447215453786.934998
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897538981.647
Test Loss of 447225600056.375671, Test MSE of 447225596913.774231
Epoch 2: training loss 421887966750.118
Test Loss of 447227083947.258850, Test MSE of 447227083730.115173
Epoch 3: training loss 421887438848.000
Test Loss of 447226126315.155212, Test MSE of 447226125474.827393
Epoch 4: training loss 416678795143.529
Test Loss of 430494441212.387695, Test MSE of 430494436421.050842
Epoch 5: training loss 380832634639.059
Test Loss of 375463028183.139465, Test MSE of 375463022939.400085
Epoch 6: training loss 312506485579.294
Test Loss of 296989014923.221863, Test MSE of 296989016963.978149
Epoch 7: training loss 220718367442.824
Test Loss of 196402110121.008545, Test MSE of 196402109987.844330
Epoch 8: training loss 156454514748.235
Test Loss of 152952430127.729828, Test MSE of 152952432319.674988
Epoch 9: training loss 137287411169.882
Test Loss of 142383265266.616699, Test MSE of 142383266151.290527
Epoch 10: training loss 131524786115.765
Test Loss of 137296060713.511917, Test MSE of 137296061847.233917
Epoch 11: training loss 128498699565.176
Test Loss of 133628227025.928284, Test MSE of 133628228718.197403
Epoch 12: training loss 124667587975.529
Test Loss of 131337015577.404587, Test MSE of 131337017541.818436
Epoch 13: training loss 121959307625.412
Test Loss of 126340980631.776077, Test MSE of 126340980088.649689
Epoch 14: training loss 117860897280.000
Test Loss of 123663607391.578064, Test MSE of 123663605512.995071
Epoch 15: training loss 115363677033.412
Test Loss of 119286729513.630356, Test MSE of 119286731513.193222
Epoch 16: training loss 109982316634.353
Test Loss of 115947732313.597046, Test MSE of 115947734548.427032
Epoch 17: training loss 107417634424.471
Test Loss of 112754858180.604218, Test MSE of 112754860618.102325
Epoch 18: training loss 102126376990.118
Test Loss of 109094824914.283600, Test MSE of 109094825343.706345
Epoch 19: training loss 98996940047.059
Test Loss of 103163617460.023132, Test MSE of 103163618705.129486
Epoch 20: training loss 94898665908.706
Test Loss of 99836624469.629425, Test MSE of 99836623857.251938
Epoch 21: training loss 90760552553.412
Test Loss of 97457591866.625961, Test MSE of 97457591581.324280
Epoch 22: training loss 87940280124.235
Test Loss of 94810365101.153824, Test MSE of 94810364819.502991
Epoch 23: training loss 84340132261.647
Test Loss of 88334004478.874863, Test MSE of 88334005916.531723
Epoch 24: training loss 80834422061.176
Test Loss of 84314847178.466812, Test MSE of 84314848082.908081
Epoch 25: training loss 76766824417.882
Test Loss of 82703139763.490173, Test MSE of 82703139336.520996
Epoch 26: training loss 73751703341.176
Test Loss of 79215936950.924820, Test MSE of 79215935635.764771
Epoch 27: training loss 70744540370.824
Test Loss of 72899468868.574600, Test MSE of 72899468641.099121
Epoch 28: training loss 66426477793.882
Test Loss of 69719117783.257919, Test MSE of 69719117453.443726
Epoch 29: training loss 63245888165.647
Test Loss of 67701670981.403656, Test MSE of 67701672188.240776
Epoch 30: training loss 60596160331.294
Test Loss of 64597027455.082115, Test MSE of 64597027570.701195
Epoch 31: training loss 57467846339.765
Test Loss of 58923840688.470047, Test MSE of 58923841230.272812
Epoch 32: training loss 54419390343.529
Test Loss of 54329469318.839691, Test MSE of 54329469966.899193
Epoch 33: training loss 52435814144.000
Test Loss of 56829871946.555634, Test MSE of 56829871466.237144
Epoch 34: training loss 49363565146.353
Test Loss of 53582964958.186447, Test MSE of 53582965803.311562
Epoch 35: training loss 46712502166.588
Test Loss of 49158266394.885033, Test MSE of 49158265116.098686
Epoch 36: training loss 44678663115.294
Test Loss of 44769405951.763130, Test MSE of 44769405385.436760
Epoch 37: training loss 42219559913.412
Test Loss of 44183206388.748558, Test MSE of 44183207247.117256
Epoch 38: training loss 40264093537.882
Test Loss of 44671209720.242424, Test MSE of 44671210444.200470
Epoch 39: training loss 37798474684.235
Test Loss of 41318766408.660652, Test MSE of 41318766644.681252
Epoch 40: training loss 35820938970.353
Test Loss of 40291761391.478142, Test MSE of 40291761831.012604
Epoch 41: training loss 34024447472.941
Test Loss of 36626701660.439507, Test MSE of 36626701480.109718
Epoch 42: training loss 33100958441.412
Test Loss of 35944135495.950035, Test MSE of 35944136222.206642
Epoch 43: training loss 31265647296.000
Test Loss of 35450194161.136246, Test MSE of 35450195245.510010
Epoch 44: training loss 29518437202.824
Test Loss of 36890728320.562569, Test MSE of 36890728373.482452
Epoch 45: training loss 28246307471.059
Test Loss of 30120900028.609760, Test MSE of 30120900157.509052
Epoch 46: training loss 26984062870.588
Test Loss of 30787936943.877861, Test MSE of 30787936445.892666
Epoch 47: training loss 25752652739.765
Test Loss of 29813913341.808929, Test MSE of 29813913368.443089
Epoch 48: training loss 24942010578.824
Test Loss of 28902962982.551006, Test MSE of 28902963305.959187
Epoch 49: training loss 23258545999.059
Test Loss of 26114888247.072865, Test MSE of 26114888384.884724
Epoch 50: training loss 22968179727.059
Test Loss of 30209445772.406200, Test MSE of 30209445420.619076
Epoch 51: training loss 21508776429.176
Test Loss of 27109229329.469349, Test MSE of 27109229729.708023
Epoch 52: training loss 21119696278.588
Test Loss of 27330070195.904697, Test MSE of 27330070109.674099
Epoch 53: training loss 20318822080.000
Test Loss of 29243641245.816330, Test MSE of 29243641159.871494
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23407524010.746037, 'MSE - std': 4283886747.444585, 'R2 - mean': 0.8334680578549286, 'R2 - std': 0.02014825479873733} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005459 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110539776.000
Test Loss of 410765527493.952820, Test MSE of 410765526013.332581
Epoch 2: training loss 430089604638.118
Test Loss of 410747799808.829224, Test MSE of 410747799502.697327
Epoch 3: training loss 430061543905.882
Test Loss of 410724017291.787109, Test MSE of 410724016818.797791
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078196796.235
Test Loss of 410728153414.012024, Test MSE of 410728147379.514221
Epoch 2: training loss 430067166268.235
Test Loss of 410729036906.617310, Test MSE of 410729042837.295349
Epoch 3: training loss 430066668001.882
Test Loss of 410728804058.210083, Test MSE of 410728800660.075195
Epoch 4: training loss 424843465667.765
Test Loss of 394278351888.584900, Test MSE of 394278353645.515625
Epoch 5: training loss 388523500965.647
Test Loss of 339670370454.685791, Test MSE of 339670366068.062805
Epoch 6: training loss 321174081054.118
Test Loss of 262984345580.098114, Test MSE of 262984345624.808136
Epoch 7: training loss 229088935996.235
Test Loss of 165370158410.276733, Test MSE of 165370159224.985138
Epoch 8: training loss 163363766121.412
Test Loss of 125503663674.994904, Test MSE of 125503663400.532776
Epoch 9: training loss 144138550934.588
Test Loss of 114499454861.327164, Test MSE of 114499456072.949371
Epoch 10: training loss 139307136331.294
Test Loss of 110618511783.626099, Test MSE of 110618512618.207016
Epoch 11: training loss 135796977694.118
Test Loss of 107522380172.142532, Test MSE of 107522380499.415741
Epoch 12: training loss 131443998117.647
Test Loss of 104917404090.580292, Test MSE of 104917404508.853745
Epoch 13: training loss 129214875256.471
Test Loss of 102047474312.233231, Test MSE of 102047475437.477524
Epoch 14: training loss 123978333093.647
Test Loss of 98169868585.580750, Test MSE of 98169866909.034714
Epoch 15: training loss 121024112338.824
Test Loss of 95766160800.518280, Test MSE of 95766161697.005936
Epoch 16: training loss 117451598034.824
Test Loss of 92394803478.626556, Test MSE of 92394803183.473312
Epoch 17: training loss 112715916047.059
Test Loss of 89786480591.666824, Test MSE of 89786481630.992676
Epoch 18: training loss 108410683994.353
Test Loss of 88121711298.043503, Test MSE of 88121713079.467728
Epoch 19: training loss 104868250473.412
Test Loss of 84837517462.685791, Test MSE of 84837516923.363235
Epoch 20: training loss 101416110531.765
Test Loss of 80210465190.678391, Test MSE of 80210466678.786484
Epoch 21: training loss 97974421473.882
Test Loss of 78296653634.458115, Test MSE of 78296654672.519470
Epoch 22: training loss 94129697942.588
Test Loss of 73461964644.575653, Test MSE of 73461965322.168762
Epoch 23: training loss 88804808478.118
Test Loss of 71717732958.534012, Test MSE of 71717733399.139862
Epoch 24: training loss 86702592030.118
Test Loss of 69020257987.465057, Test MSE of 69020257873.843857
Epoch 25: training loss 83332343958.588
Test Loss of 66441085640.677467, Test MSE of 66441085785.033722
Epoch 26: training loss 79265111898.353
Test Loss of 63541267439.415085, Test MSE of 63541267019.493355
Epoch 27: training loss 76495876457.412
Test Loss of 60931054387.768623, Test MSE of 60931053254.969894
Epoch 28: training loss 72363040090.353
Test Loss of 57510993218.695045, Test MSE of 57510992619.278923
Epoch 29: training loss 69125969874.824
Test Loss of 56128142867.191116, Test MSE of 56128143903.326019
Epoch 30: training loss 66612129551.059
Test Loss of 53382712371.650162, Test MSE of 53382713054.251503
Epoch 31: training loss 63322668830.118
Test Loss of 47688824338.243408, Test MSE of 47688825253.833252
Epoch 32: training loss 60284799894.588
Test Loss of 46679250799.000465, Test MSE of 46679250229.792801
Epoch 33: training loss 57507477880.471
Test Loss of 44970624530.243408, Test MSE of 44970624333.128479
Epoch 34: training loss 54003070479.059
Test Loss of 41499365804.364647, Test MSE of 41499365939.300735
Epoch 35: training loss 51443266176.000
Test Loss of 41180788230.870895, Test MSE of 41180787387.189774
Epoch 36: training loss 49389926942.118
Test Loss of 39359192403.753815, Test MSE of 39359192340.562416
Epoch 37: training loss 47263319348.706
Test Loss of 34883075669.530769, Test MSE of 34883075509.187096
Epoch 38: training loss 44751092683.294
Test Loss of 39650423574.389633, Test MSE of 39650423205.783821
Epoch 39: training loss 42672988973.176
Test Loss of 35184480487.241089, Test MSE of 35184479987.778847
Epoch 40: training loss 40318694068.706
Test Loss of 32916677394.598797, Test MSE of 32916677892.731823
Epoch 41: training loss 38724415021.176
Test Loss of 31466571875.509487, Test MSE of 31466572181.816441
Epoch 42: training loss 36210640873.412
Test Loss of 28752998530.783897, Test MSE of 28752998854.656532
Epoch 43: training loss 34659729046.588
Test Loss of 29393179765.989819, Test MSE of 29393179149.369663
Epoch 44: training loss 33289371083.294
Test Loss of 26212854622.415550, Test MSE of 26212855057.220413
Epoch 45: training loss 31304688907.294
Test Loss of 26823709430.167515, Test MSE of 26823709530.938011
Epoch 46: training loss 30406129603.765
Test Loss of 24367332900.249886, Test MSE of 24367333223.678333
Epoch 47: training loss 29201734761.412
Test Loss of 25166548159.911152, Test MSE of 25166548637.453999
Epoch 48: training loss 27745453108.706
Test Loss of 25453510786.783897, Test MSE of 25453510146.489796
Epoch 49: training loss 26797128557.176
Test Loss of 24081574167.574272, Test MSE of 24081574662.546391
Epoch 50: training loss 25375553908.706
Test Loss of 24341985488.496067, Test MSE of 24341985846.088356
Epoch 51: training loss 24706866319.059
Test Loss of 22725467859.576122, Test MSE of 22725467312.838718
Epoch 52: training loss 23327037566.118
Test Loss of 22822369555.783432, Test MSE of 22822369778.354263
Epoch 53: training loss 22387029131.294
Test Loss of 20278732053.204998, Test MSE of 20278731874.755611
Epoch 54: training loss 21596921562.353
Test Loss of 20055827542.715408, Test MSE of 20055827744.591087
Epoch 55: training loss 20682839683.765
Test Loss of 20443217242.387783, Test MSE of 20443217726.532227
Epoch 56: training loss 20529119107.765
Test Loss of 19914028503.485424, Test MSE of 19914028842.850815
Epoch 57: training loss 19618370654.118
Test Loss of 19978221718.685795, Test MSE of 19978222000.720501
Epoch 58: training loss 19143659350.588
Test Loss of 18887551169.332718, Test MSE of 18887551364.248875
Epoch 59: training loss 18340793103.059
Test Loss of 20920593192.396114, Test MSE of 20920593048.485870
Epoch 60: training loss 17581404613.647
Test Loss of 19039533563.024525, Test MSE of 19039533972.408817
Epoch 61: training loss 17443702663.529
Test Loss of 19146777335.115223, Test MSE of 19146777736.406605
Epoch 62: training loss 16862900920.471
Test Loss of 19256423952.347988, Test MSE of 19256424482.067497
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22369749128.5764, 'MSE - std': 4122462172.8871098, 'R2 - mean': 0.8353677166548739, 'R2 - std': 0.01775641403112578} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005500 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043408685.176
Test Loss of 431612873969.192017, Test MSE of 431612874040.804016
Epoch 2: training loss 424022910976.000
Test Loss of 431592483963.676086, Test MSE of 431592488813.804016
Epoch 3: training loss 423994510275.765
Test Loss of 431563664939.831543, Test MSE of 431563671197.491577
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010408056.471
Test Loss of 431565809735.078186, Test MSE of 431565801871.001404
Epoch 2: training loss 423998318230.588
Test Loss of 431569336457.417847, Test MSE of 431569339574.252319
Epoch 3: training loss 423997619019.294
Test Loss of 431569388482.872742, Test MSE of 431569397720.858154
Epoch 4: training loss 418768381229.176
Test Loss of 414626958546.865356, Test MSE of 414626955306.341980
Epoch 5: training loss 382832584342.588
Test Loss of 359333757473.880615, Test MSE of 359333754904.967773
Epoch 6: training loss 314988712056.471
Test Loss of 279293910740.523804, Test MSE of 279293915470.236206
Epoch 7: training loss 224408000180.706
Test Loss of 180354866165.101349, Test MSE of 180354864210.692200
Epoch 8: training loss 159577904941.176
Test Loss of 138412690855.152252, Test MSE of 138412691956.370209
Epoch 9: training loss 140963636284.235
Test Loss of 126366892532.390564, Test MSE of 126366894695.912125
Epoch 10: training loss 135159026055.529
Test Loss of 122008476350.252655, Test MSE of 122008474526.018982
Epoch 11: training loss 131844157620.706
Test Loss of 117670099369.521515, Test MSE of 117670100873.881531
Epoch 12: training loss 128696287533.176
Test Loss of 114660280694.345215, Test MSE of 114660279484.440903
Epoch 13: training loss 124504078576.941
Test Loss of 110804631552.000000, Test MSE of 110804633413.694763
Epoch 14: training loss 123026403568.941
Test Loss of 107214253843.072647, Test MSE of 107214256533.695541
Epoch 15: training loss 118072880459.294
Test Loss of 104758257845.012497, Test MSE of 104758256536.147949
Epoch 16: training loss 114111629221.647
Test Loss of 100991642137.351227, Test MSE of 100991640778.307037
Epoch 17: training loss 109254912391.529
Test Loss of 98566679368.618225, Test MSE of 98566679989.899368
Epoch 18: training loss 106634442631.529
Test Loss of 93263879547.557617, Test MSE of 93263879681.221802
Epoch 19: training loss 102835841084.235
Test Loss of 92728073280.918091, Test MSE of 92728073135.377487
Epoch 20: training loss 99507086155.294
Test Loss of 87457646602.424805, Test MSE of 87457646058.102859
Epoch 21: training loss 94188587685.647
Test Loss of 83913229387.342896, Test MSE of 83913228806.574097
Epoch 22: training loss 91320425848.471
Test Loss of 79465323649.362335, Test MSE of 79465324667.026855
Epoch 23: training loss 87199726004.706
Test Loss of 76581811375.800095, Test MSE of 76581811220.965729
Epoch 24: training loss 83344468751.059
Test Loss of 72257197294.348907, Test MSE of 72257198386.187927
Epoch 25: training loss 80125760466.824
Test Loss of 70767205588.760757, Test MSE of 70767205960.345230
Epoch 26: training loss 76775660604.235
Test Loss of 65674075652.027763, Test MSE of 65674075353.146095
Epoch 27: training loss 73612512677.647
Test Loss of 65262713456.540489, Test MSE of 65262713244.876015
Epoch 28: training loss 70132324728.471
Test Loss of 56905892694.833870, Test MSE of 56905894870.821510
Epoch 29: training loss 66352408244.706
Test Loss of 58114309338.447014, Test MSE of 58114309375.201630
Epoch 30: training loss 63923456331.294
Test Loss of 54730521568.251732, Test MSE of 54730521348.490799
Epoch 31: training loss 60961981214.118
Test Loss of 52222592289.525223, Test MSE of 52222593142.786926
Epoch 32: training loss 58374980141.176
Test Loss of 48826448111.770477, Test MSE of 48826447815.941963
Epoch 33: training loss 54801383905.882
Test Loss of 45266849266.968994, Test MSE of 45266847960.940544
Epoch 34: training loss 52584547380.706
Test Loss of 44515140374.389633, Test MSE of 44515140377.715385
Epoch 35: training loss 50309879928.471
Test Loss of 43568552657.206848, Test MSE of 43568551591.532234
Epoch 36: training loss 47681318716.235
Test Loss of 38821823296.088844, Test MSE of 38821823745.805038
Epoch 37: training loss 45232446832.941
Test Loss of 37554775467.890793, Test MSE of 37554775590.737999
Epoch 38: training loss 42497896914.824
Test Loss of 38402067564.512726, Test MSE of 38402067199.477776
Epoch 39: training loss 40155143747.765
Test Loss of 32496683874.206387, Test MSE of 32496683693.930523
Epoch 40: training loss 38961285639.529
Test Loss of 30153624094.089775, Test MSE of 30153624163.759613
Epoch 41: training loss 36716636009.412
Test Loss of 32261543290.609901, Test MSE of 32261543460.702370
Epoch 42: training loss 35157613394.824
Test Loss of 29425354527.392872, Test MSE of 29425354150.350048
Epoch 43: training loss 33779878309.647
Test Loss of 29157295821.889866, Test MSE of 29157295708.949593
Epoch 44: training loss 32233135194.353
Test Loss of 26813968905.240166, Test MSE of 26813969111.472267
Epoch 45: training loss 30574398087.529
Test Loss of 25013382961.399353, Test MSE of 25013383176.084698
Epoch 46: training loss 29325324480.000
Test Loss of 24950253371.824154, Test MSE of 24950252940.430470
Epoch 47: training loss 28318983589.647
Test Loss of 23299536987.927811, Test MSE of 23299537303.309242
Epoch 48: training loss 27445539305.412
Test Loss of 23872697835.387321, Test MSE of 23872698010.007275
Epoch 49: training loss 26178911378.824
Test Loss of 21583418560.858860, Test MSE of 21583419143.294395
Epoch 50: training loss 24859130654.118
Test Loss of 24000760224.992134, Test MSE of 24000760317.597176
Epoch 51: training loss 23612840975.059
Test Loss of 22471176837.390099, Test MSE of 22471176410.126480
Epoch 52: training loss 22993180024.471
Test Loss of 23844210366.726517, Test MSE of 23844210822.822464
Epoch 53: training loss 22248075934.118
Test Loss of 21880223156.420177, Test MSE of 21880222899.461956
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22271843882.753513, 'MSE - std': 3692437845.400604, 'R2 - mean': 0.8356137099308147, 'R2 - std': 0.01588943806945485} 
 

Saving model.....
Results After CV: {'MSE - mean': 22271843882.753513, 'MSE - std': 3692437845.400604, 'R2 - mean': 0.8356137099308147, 'R2 - std': 0.01588943806945485}
Train time: 93.4572391608006
Inference time: 0.07058880259937723
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 42 finished with value: 22271843882.753513 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005469 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525435632.941
Test Loss of 418111184249.811707, Test MSE of 418111189613.685791
Epoch 2: training loss 427504473509.647
Test Loss of 418092934884.226685, Test MSE of 418092936631.167114
Epoch 3: training loss 427476672512.000
Test Loss of 418069044298.614868, Test MSE of 418069046427.997314
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493306849.882
Test Loss of 418075630810.633362, Test MSE of 418075627699.685791
Epoch 2: training loss 427481541571.765
Test Loss of 418076910043.640076, Test MSE of 418076908388.194336
Epoch 3: training loss 427481094505.412
Test Loss of 418076973351.143188, Test MSE of 418076976888.423645
Epoch 4: training loss 427480678881.882
Test Loss of 418076292477.127930, Test MSE of 418076288969.091492
Epoch 5: training loss 420270543209.412
Test Loss of 395442636529.491577, Test MSE of 395442635862.648987
Epoch 6: training loss 373823026236.235
Test Loss of 328105206262.169800, Test MSE of 328105206192.056458
Epoch 7: training loss 294966867425.882
Test Loss of 242748699115.747406, Test MSE of 242748697859.793762
Epoch 8: training loss 216598465686.588
Test Loss of 173466763771.854736, Test MSE of 173466760426.910950
Epoch 9: training loss 157692955843.765
Test Loss of 125254152069.773773, Test MSE of 125254152197.252823
Epoch 10: training loss 139348978718.118
Test Loss of 117826472546.657410, Test MSE of 117826470600.330444
Epoch 11: training loss 136616746420.706
Test Loss of 115311006542.108719, Test MSE of 115311009859.863007
Epoch 12: training loss 132480696169.412
Test Loss of 112420727498.881332, Test MSE of 112420727708.942841
Epoch 13: training loss 128127938304.000
Test Loss of 108878589141.895905, Test MSE of 108878590803.829178
Epoch 14: training loss 125470607510.588
Test Loss of 106109727725.287064, Test MSE of 106109726907.434219
Epoch 15: training loss 121475004084.706
Test Loss of 102197917951.348602, Test MSE of 102197918813.614151
Epoch 16: training loss 117698647732.706
Test Loss of 99301949591.598434, Test MSE of 99301947748.538940
Epoch 17: training loss 113985769321.412
Test Loss of 95438109901.368500, Test MSE of 95438109929.787231
Epoch 18: training loss 110508127698.824
Test Loss of 92885241025.051117, Test MSE of 92885242019.540619
Epoch 19: training loss 105977944816.941
Test Loss of 89173628279.442978, Test MSE of 89173628520.349823
Epoch 20: training loss 102715566305.882
Test Loss of 85991221557.355545, Test MSE of 85991222280.249207
Epoch 21: training loss 98510780732.235
Test Loss of 82513759298.087433, Test MSE of 82513758662.113129
Epoch 22: training loss 93993640207.059
Test Loss of 79260503189.466568, Test MSE of 79260503869.261795
Epoch 23: training loss 89997819986.824
Test Loss of 75457440121.574829, Test MSE of 75457441729.886627
Epoch 24: training loss 86888486806.588
Test Loss of 72679294870.354843, Test MSE of 72679295255.238876
Epoch 25: training loss 83244672045.176
Test Loss of 68655797469.949570, Test MSE of 68655797691.869972
Epoch 26: training loss 80057366482.824
Test Loss of 64229477650.535278, Test MSE of 64229478471.250870
Epoch 27: training loss 75623471736.471
Test Loss of 62818501589.599815, Test MSE of 62818501743.587181
Epoch 28: training loss 72048040922.353
Test Loss of 60963298096.262779, Test MSE of 60963297170.596375
Epoch 29: training loss 69126845199.059
Test Loss of 58239750355.290306, Test MSE of 58239750675.740044
Epoch 30: training loss 66934883019.294
Test Loss of 54629745336.405273, Test MSE of 54629744790.050591
Epoch 31: training loss 62946426721.882
Test Loss of 53905200832.222069, Test MSE of 53905201139.035973
Epoch 32: training loss 59985412645.647
Test Loss of 49147608345.167709, Test MSE of 49147608356.188942
Epoch 33: training loss 57193739685.647
Test Loss of 47081240360.445984, Test MSE of 47081240912.520515
Epoch 34: training loss 54772455100.235
Test Loss of 42910515071.141335, Test MSE of 42910515034.795296
Epoch 35: training loss 51346688933.647
Test Loss of 40083783950.982185, Test MSE of 40083784685.118439
Epoch 36: training loss 49224809178.353
Test Loss of 39657240513.939392, Test MSE of 39657240230.153915
Epoch 37: training loss 46516356186.353
Test Loss of 39350277666.938698, Test MSE of 39350278412.030380
Epoch 38: training loss 44022006290.824
Test Loss of 34011553100.332176, Test MSE of 34011553630.718346
Epoch 39: training loss 42085234014.118
Test Loss of 33690407690.836918, Test MSE of 33690407516.512909
Epoch 40: training loss 39822264975.059
Test Loss of 32421243937.872772, Test MSE of 32421243713.177162
Epoch 41: training loss 38158102738.824
Test Loss of 29390676920.227619, Test MSE of 29390677222.324043
Epoch 42: training loss 35903469236.706
Test Loss of 28908789857.591488, Test MSE of 28908790100.155563
Epoch 43: training loss 34075704286.118
Test Loss of 26838664547.545685, Test MSE of 26838664592.458889
Epoch 44: training loss 32308785920.000
Test Loss of 27149941656.486698, Test MSE of 27149941590.055092
Epoch 45: training loss 31043102098.824
Test Loss of 25836723001.737682, Test MSE of 25836723233.474236
Epoch 46: training loss 29342792387.765
Test Loss of 23793322524.069397, Test MSE of 23793322432.589001
Epoch 47: training loss 27617625490.824
Test Loss of 23391793297.439739, Test MSE of 23391793402.834518
Epoch 48: training loss 26703427794.824
Test Loss of 22332476894.245663, Test MSE of 22332476895.820053
Epoch 49: training loss 25728321389.176
Test Loss of 21206582359.405968, Test MSE of 21206582659.087437
Epoch 50: training loss 24340189575.529
Test Loss of 19907192391.417072, Test MSE of 19907192376.510395
Epoch 51: training loss 23670510309.647
Test Loss of 21839182624.629192, Test MSE of 21839182639.416332
Epoch 52: training loss 22279022505.412
Test Loss of 20192431256.309044, Test MSE of 20192431477.406120
Epoch 53: training loss 21579948886.588
Test Loss of 21620471600.499653, Test MSE of 21620471619.890110
Epoch 54: training loss 20909947994.353
Test Loss of 19461607007.578072, Test MSE of 19461607092.267353
Epoch 55: training loss 20196001231.059
Test Loss of 18233762722.435345, Test MSE of 18233762562.015259
Epoch 56: training loss 19269333334.588
Test Loss of 19791341959.313438, Test MSE of 19791341888.078678
Epoch 57: training loss 18644446486.588
Test Loss of 18773327268.922508, Test MSE of 18773327177.203987
Epoch 58: training loss 18078085857.882
Test Loss of 18604490634.511219, Test MSE of 18604491140.515823
Epoch 59: training loss 17448418710.588
Test Loss of 17425300598.199398, Test MSE of 17425300456.709602
Epoch 60: training loss 16987912139.294
Test Loss of 18561883932.602360, Test MSE of 18561884157.067677
Epoch 61: training loss 16285240316.235
Test Loss of 17822640334.079113, Test MSE of 17822640166.982643
Epoch 62: training loss 16215305426.824
Test Loss of 17955183103.170944, Test MSE of 17955183352.882408
Epoch 63: training loss 15488203241.412
Test Loss of 18656046068.393246, Test MSE of 18656045924.373306
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18656045924.373306, 'MSE - std': 0.0, 'R2 - mean': 0.8547233462213194, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005725 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918139632.941
Test Loss of 424556908408.508911, Test MSE of 424556909357.709717
Epoch 2: training loss 427897452905.412
Test Loss of 424540785142.643555, Test MSE of 424540779924.940491
Epoch 3: training loss 427869595527.529
Test Loss of 424518861349.544312, Test MSE of 424518865378.569397
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889089596.235
Test Loss of 424527704707.345825, Test MSE of 424527715347.062256
Epoch 2: training loss 427878437104.941
Test Loss of 424528392479.089539, Test MSE of 424528400554.334473
Epoch 3: training loss 427877854027.294
Test Loss of 424528533154.849854, Test MSE of 424528529236.779053
Epoch 4: training loss 427877403949.176
Test Loss of 424527789420.546814, Test MSE of 424527787007.740784
Epoch 5: training loss 420865199646.118
Test Loss of 402698149905.054810, Test MSE of 402698152490.548340
Epoch 6: training loss 374990944376.471
Test Loss of 336455249039.781616, Test MSE of 336455253745.770020
Epoch 7: training loss 295691105701.647
Test Loss of 252957281433.256531, Test MSE of 252957287377.751678
Epoch 8: training loss 216429726780.235
Test Loss of 183979979524.204498, Test MSE of 183979977099.364594
Epoch 9: training loss 157427599781.647
Test Loss of 137495679014.847107, Test MSE of 137495677883.876709
Epoch 10: training loss 136854914198.588
Test Loss of 129986453176.168396, Test MSE of 129986455194.813110
Epoch 11: training loss 133133951367.529
Test Loss of 126786646162.860977, Test MSE of 126786645490.184891
Epoch 12: training loss 130287230102.588
Test Loss of 124373046097.898682, Test MSE of 124373048586.073257
Epoch 13: training loss 126721381135.059
Test Loss of 121529650018.479767, Test MSE of 121529650699.513870
Epoch 14: training loss 122267866021.647
Test Loss of 117134557689.959747, Test MSE of 117134557782.225159
Epoch 15: training loss 119925407262.118
Test Loss of 114529742401.732132, Test MSE of 114529741775.006790
Epoch 16: training loss 115985248828.235
Test Loss of 110522212425.904236, Test MSE of 110522215296.704239
Epoch 17: training loss 112030579681.882
Test Loss of 107071872622.264175, Test MSE of 107071873057.740341
Epoch 18: training loss 107461430753.882
Test Loss of 103534210840.812393, Test MSE of 103534209162.319778
Epoch 19: training loss 104649205458.824
Test Loss of 98288389421.065002, Test MSE of 98288389973.856842
Epoch 20: training loss 100585459952.941
Test Loss of 95312514194.624100, Test MSE of 95312513315.925171
Epoch 21: training loss 95502767013.647
Test Loss of 91378669878.303024, Test MSE of 91378669365.658737
Epoch 22: training loss 92258686192.941
Test Loss of 87090019641.382370, Test MSE of 87090018667.309540
Epoch 23: training loss 88358335849.412
Test Loss of 85328457403.010880, Test MSE of 85328460073.694565
Epoch 24: training loss 85225264685.176
Test Loss of 78422776846.922974, Test MSE of 78422776720.801666
Epoch 25: training loss 81449775856.941
Test Loss of 77903896160.051819, Test MSE of 77903895344.320038
Epoch 26: training loss 76496122819.765
Test Loss of 75703705045.718246, Test MSE of 75703703076.831650
Epoch 27: training loss 74074976602.353
Test Loss of 70052939239.246826, Test MSE of 70052939082.566162
Epoch 28: training loss 70318961769.412
Test Loss of 67425774735.781631, Test MSE of 67425775441.395874
Epoch 29: training loss 67644960271.059
Test Loss of 62972629081.064079, Test MSE of 62972627870.610161
Epoch 30: training loss 64089545833.412
Test Loss of 59787679024.381218, Test MSE of 59787675878.775955
Epoch 31: training loss 60886420419.765
Test Loss of 56587418176.547768, Test MSE of 56587417745.816864
Epoch 32: training loss 57273341229.176
Test Loss of 56474550816.806847, Test MSE of 56474553519.680534
Epoch 33: training loss 54239929856.000
Test Loss of 55227938876.165627, Test MSE of 55227938565.861427
Epoch 34: training loss 52416389647.059
Test Loss of 48301859745.014114, Test MSE of 48301860188.135941
Epoch 35: training loss 48932628239.059
Test Loss of 48839545613.679390, Test MSE of 48839545237.071709
Epoch 36: training loss 46680664997.647
Test Loss of 44394932307.379135, Test MSE of 44394932328.044899
Epoch 37: training loss 44141303491.765
Test Loss of 45758272654.834145, Test MSE of 45758273818.832932
Epoch 38: training loss 41482224730.353
Test Loss of 44651606605.812630, Test MSE of 44651605983.052368
Epoch 39: training loss 39313029067.294
Test Loss of 38256742755.782555, Test MSE of 38256742974.173691
Epoch 40: training loss 37282652280.471
Test Loss of 39358899317.725655, Test MSE of 39358899701.765640
Epoch 41: training loss 35098103702.588
Test Loss of 37664294637.701599, Test MSE of 37664294928.251755
Epoch 42: training loss 33778519845.647
Test Loss of 34529785646.130928, Test MSE of 34529786612.264015
Epoch 43: training loss 31154353912.471
Test Loss of 34997667662.819336, Test MSE of 34997668773.172859
Epoch 44: training loss 30447307678.118
Test Loss of 35135258919.143188, Test MSE of 35135259440.967468
Epoch 45: training loss 28165454132.706
Test Loss of 33200559204.197086, Test MSE of 33200558158.052120
Epoch 46: training loss 26984430433.882
Test Loss of 30307594957.250057, Test MSE of 30307595453.194557
Epoch 47: training loss 25418715328.000
Test Loss of 29176268661.192692, Test MSE of 29176268065.059505
Epoch 48: training loss 24436167823.059
Test Loss of 29146436190.867455, Test MSE of 29146435698.832050
Epoch 49: training loss 23547654806.588
Test Loss of 27998495463.542912, Test MSE of 27998494789.774773
Epoch 50: training loss 22064247175.529
Test Loss of 29777893169.920887, Test MSE of 29777892553.885475
Epoch 51: training loss 21216771015.529
Test Loss of 29218036879.307888, Test MSE of 29218036931.701939
Epoch 52: training loss 20555793295.059
Test Loss of 32467184663.924126, Test MSE of 32467184631.183037
Epoch 53: training loss 19408146804.706
Test Loss of 29329140799.008095, Test MSE of 29329140608.472530
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23992593266.42292, 'MSE - std': 5336547342.049612, 'R2 - mean': 0.8226663773627101, 'R2 - std': 0.03205696885860937} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005493 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926752015.059
Test Loss of 447258284995.597473, Test MSE of 447258285230.231995
Epoch 2: training loss 421906308397.176
Test Loss of 447239830797.797852, Test MSE of 447239829708.573120
Epoch 3: training loss 421878690153.412
Test Loss of 447215073516.398804, Test MSE of 447215068418.665405
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897159740.235
Test Loss of 447222412623.648376, Test MSE of 447222420606.645935
Epoch 2: training loss 421887887360.000
Test Loss of 447223150392.553345, Test MSE of 447223154386.520752
Epoch 3: training loss 421887402104.471
Test Loss of 447223191657.645142, Test MSE of 447223194657.305786
Epoch 4: training loss 421887002021.647
Test Loss of 447222652801.036316, Test MSE of 447222656289.272400
Epoch 5: training loss 415051603245.176
Test Loss of 425194658447.189453, Test MSE of 425194657945.274414
Epoch 6: training loss 370141143521.882
Test Loss of 358125381147.358765, Test MSE of 358125376785.981628
Epoch 7: training loss 291249475584.000
Test Loss of 271574559080.520020, Test MSE of 271574559116.990173
Epoch 8: training loss 212716886256.941
Test Loss of 198955521297.114044, Test MSE of 198955523381.709869
Epoch 9: training loss 154193710471.529
Test Loss of 148490083069.098297, Test MSE of 148490081704.617645
Epoch 10: training loss 134817783868.235
Test Loss of 140419811603.245880, Test MSE of 140419810675.633392
Epoch 11: training loss 129970188167.529
Test Loss of 137067684107.192230, Test MSE of 137067684363.040131
Epoch 12: training loss 127454695604.706
Test Loss of 133763544308.689331, Test MSE of 133763544542.192398
Epoch 13: training loss 124576821127.529
Test Loss of 129216630921.386078, Test MSE of 129216632725.269287
Epoch 14: training loss 120676229662.118
Test Loss of 126763038175.666901, Test MSE of 126763038050.832230
Epoch 15: training loss 117384925123.765
Test Loss of 122866490143.207962, Test MSE of 122866489046.851471
Epoch 16: training loss 114618590328.471
Test Loss of 119629669695.541061, Test MSE of 119629670562.132401
Epoch 17: training loss 109387467294.118
Test Loss of 114627874570.600052, Test MSE of 114627876659.276855
Epoch 18: training loss 105883348886.588
Test Loss of 111498122482.794357, Test MSE of 111498123646.272110
Epoch 19: training loss 101568409268.706
Test Loss of 106528570145.102936, Test MSE of 106528569277.084167
Epoch 20: training loss 98661195535.059
Test Loss of 102964616270.404816, Test MSE of 102964617119.026291
Epoch 21: training loss 94301362386.824
Test Loss of 100603386477.079803, Test MSE of 100603386602.526062
Epoch 22: training loss 90818338575.059
Test Loss of 96030642366.445526, Test MSE of 96030641809.224472
Epoch 23: training loss 87427997846.588
Test Loss of 90193603642.507523, Test MSE of 90193602722.308334
Epoch 24: training loss 83928156641.882
Test Loss of 90735814340.012024, Test MSE of 90735816702.189011
Epoch 25: training loss 79463190211.765
Test Loss of 83320405000.290543, Test MSE of 83320405686.558517
Epoch 26: training loss 76490856613.647
Test Loss of 83381002658.790649, Test MSE of 83381003805.828247
Epoch 27: training loss 72922031841.882
Test Loss of 78961228452.034241, Test MSE of 78961228262.687943
Epoch 28: training loss 69454805519.059
Test Loss of 70557830428.010178, Test MSE of 70557829733.738022
Epoch 29: training loss 66625770827.294
Test Loss of 66508853228.102707, Test MSE of 66508853407.338997
Epoch 30: training loss 62922179433.412
Test Loss of 64689273905.980110, Test MSE of 64689274952.491531
Epoch 31: training loss 60115070885.647
Test Loss of 60518222734.064308, Test MSE of 60518223144.435577
Epoch 32: training loss 57417262501.647
Test Loss of 59706633916.905853, Test MSE of 59706633357.881737
Epoch 33: training loss 53318350554.353
Test Loss of 55655603153.336113, Test MSE of 55655603380.994911
Epoch 34: training loss 51157474002.824
Test Loss of 55618931080.734673, Test MSE of 55618931264.049995
Epoch 35: training loss 48856220822.588
Test Loss of 50052525302.110573, Test MSE of 50052525542.865730
Epoch 36: training loss 46462339011.765
Test Loss of 48523954598.106873, Test MSE of 48523955427.519386
Epoch 37: training loss 43979899218.824
Test Loss of 42949636721.343513, Test MSE of 42949636722.310570
Epoch 38: training loss 41826101933.176
Test Loss of 42780783746.990517, Test MSE of 42780783791.472855
Epoch 39: training loss 39322586443.294
Test Loss of 40233024621.671989, Test MSE of 40233024441.199196
Epoch 40: training loss 37509332412.235
Test Loss of 38877365779.778854, Test MSE of 38877366018.603638
Epoch 41: training loss 35560287457.882
Test Loss of 36995936598.991440, Test MSE of 36995936417.078804
Epoch 42: training loss 33923932461.176
Test Loss of 33532453527.243118, Test MSE of 33532453312.357967
Epoch 43: training loss 32138627019.294
Test Loss of 36340570841.093681, Test MSE of 36340571604.945396
Epoch 44: training loss 30394618300.235
Test Loss of 32863894675.571594, Test MSE of 32863894546.173897
Epoch 45: training loss 28660790034.824
Test Loss of 30301195146.274345, Test MSE of 30301195052.105534
Epoch 46: training loss 27525849600.000
Test Loss of 32080931963.884338, Test MSE of 32080932308.133888
Epoch 47: training loss 26279220449.882
Test Loss of 31113930910.704605, Test MSE of 31113931753.723888
Epoch 48: training loss 25212962055.529
Test Loss of 27681198339.612305, Test MSE of 27681197903.957912
Epoch 49: training loss 23772960342.588
Test Loss of 25497007359.111729, Test MSE of 25497007644.278316
Epoch 50: training loss 23115777882.353
Test Loss of 26322742911.318993, Test MSE of 26322742501.526630
Epoch 51: training loss 22012934102.588
Test Loss of 25670098275.071941, Test MSE of 25670098325.387768
Epoch 52: training loss 21084441592.471
Test Loss of 24909529498.736988, Test MSE of 24909529811.839363
Epoch 53: training loss 20280827422.118
Test Loss of 29034933687.398567, Test MSE of 29034933748.113678
Epoch 54: training loss 19436242605.176
Test Loss of 22698390494.127228, Test MSE of 22698390239.035492
Epoch 55: training loss 19134850763.294
Test Loss of 24594652103.861206, Test MSE of 24594652355.299393
Epoch 56: training loss 18264576933.647
Test Loss of 22499646107.033077, Test MSE of 22499645982.799091
Epoch 57: training loss 17856754228.706
Test Loss of 21890391607.309738, Test MSE of 21890391920.705231
Epoch 58: training loss 17173874608.941
Test Loss of 20560162952.912331, Test MSE of 20560162755.172428
Epoch 59: training loss 16591615442.824
Test Loss of 23041929434.396484, Test MSE of 23041929500.973312
Epoch 60: training loss 16030742942.118
Test Loss of 22575109846.961834, Test MSE of 22575110074.603786
Epoch 61: training loss 15505054388.706
Test Loss of 21455981635.982418, Test MSE of 21455981408.089748
Epoch 62: training loss 15191699809.882
Test Loss of 20692526723.582699, Test MSE of 20692526962.110329
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22892571164.98539, 'MSE - std': 4626653463.34979, 'R2 - mean': 0.8358613227749384, 'R2 - std': 0.03214518113908539} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005434 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110154752.000
Test Loss of 410765301651.013428, Test MSE of 410765293397.005066
Epoch 2: training loss 430089308280.471
Test Loss of 410747021049.484497, Test MSE of 410747014779.273071
Epoch 3: training loss 430061354767.059
Test Loss of 410722775913.788086, Test MSE of 410722776846.893921
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078121140.706
Test Loss of 410730939088.259155, Test MSE of 410730933254.388855
Epoch 2: training loss 430066145641.412
Test Loss of 410730505613.564087, Test MSE of 410730507784.357239
Epoch 3: training loss 430065722066.824
Test Loss of 410730054822.322998, Test MSE of 410730053240.733154
Epoch 4: training loss 430065373906.824
Test Loss of 410729383328.518250, Test MSE of 410729383743.251404
Epoch 5: training loss 423444629624.471
Test Loss of 389701015291.379944, Test MSE of 389701020279.641235
Epoch 6: training loss 378421946006.588
Test Loss of 323224411489.021729, Test MSE of 323224408123.626648
Epoch 7: training loss 299260601042.824
Test Loss of 237701610314.513641, Test MSE of 237701606536.860931
Epoch 8: training loss 221074000655.059
Test Loss of 168416419100.786682, Test MSE of 168416420971.304138
Epoch 9: training loss 162198019252.706
Test Loss of 119745172561.029160, Test MSE of 119745173300.398239
Epoch 10: training loss 142402860875.294
Test Loss of 112515930351.770477, Test MSE of 112515929206.387131
Epoch 11: training loss 137356455966.118
Test Loss of 109322005411.598328, Test MSE of 109322002756.773041
Epoch 12: training loss 134752644336.941
Test Loss of 107390835045.286438, Test MSE of 107390836001.970551
Epoch 13: training loss 130344593257.412
Test Loss of 104098883814.293381, Test MSE of 104098884815.905869
Epoch 14: training loss 127923645530.353
Test Loss of 100968598458.343353, Test MSE of 100968598336.014435
Epoch 15: training loss 124676013718.588
Test Loss of 97889905457.873199, Test MSE of 97889906747.039398
Epoch 16: training loss 120372045281.882
Test Loss of 94035113973.575195, Test MSE of 94035114747.522720
Epoch 17: training loss 115762965263.059
Test Loss of 91137827282.273026, Test MSE of 91137826113.970474
Epoch 18: training loss 112263056444.235
Test Loss of 88461706110.637665, Test MSE of 88461705939.198364
Epoch 19: training loss 107106776425.412
Test Loss of 84956673207.381775, Test MSE of 84956673563.514709
Epoch 20: training loss 103970668182.588
Test Loss of 82494749605.493759, Test MSE of 82494750145.236588
Epoch 21: training loss 99972089283.765
Test Loss of 78461245243.824158, Test MSE of 78461245211.173965
Epoch 22: training loss 95771684577.882
Test Loss of 74555855983.355850, Test MSE of 74555856594.879700
Epoch 23: training loss 92904777441.882
Test Loss of 72543571492.249878, Test MSE of 72543570523.332382
Epoch 24: training loss 88669814031.059
Test Loss of 70530125685.634430, Test MSE of 70530125276.135559
Epoch 25: training loss 86051052423.529
Test Loss of 65326142878.622856, Test MSE of 65326142916.133347
Epoch 26: training loss 81837779486.118
Test Loss of 64085187425.258675, Test MSE of 64085188821.150795
Epoch 27: training loss 77635885914.353
Test Loss of 62791361387.683479, Test MSE of 62791360788.806168
Epoch 28: training loss 74375383672.471
Test Loss of 57872193790.933830, Test MSE of 57872194593.085915
Epoch 29: training loss 70745936941.176
Test Loss of 56642107165.497452, Test MSE of 56642106488.066246
Epoch 30: training loss 66804153268.706
Test Loss of 53850979540.760757, Test MSE of 53850980043.136864
Epoch 31: training loss 63975319928.471
Test Loss of 52608419613.497452, Test MSE of 52608419451.468956
Epoch 32: training loss 61003725477.647
Test Loss of 49477862111.422493, Test MSE of 49477862248.193504
Epoch 33: training loss 58144782027.294
Test Loss of 46047440063.911156, Test MSE of 46047438795.032120
Epoch 34: training loss 55848195297.882
Test Loss of 44994050283.979637, Test MSE of 44994050387.587044
Epoch 35: training loss 53061482782.118
Test Loss of 43564327023.829712, Test MSE of 43564327702.458099
Epoch 36: training loss 49678099998.118
Test Loss of 40445186330.417397, Test MSE of 40445186662.947754
Epoch 37: training loss 47193250846.118
Test Loss of 40058281574.589539, Test MSE of 40058282080.504654
Epoch 38: training loss 45169814400.000
Test Loss of 35793816549.937988, Test MSE of 35793817290.737465
Epoch 39: training loss 42368589643.294
Test Loss of 35199942216.262840, Test MSE of 35199942749.150063
Epoch 40: training loss 40680199408.941
Test Loss of 32486834232.862564, Test MSE of 32486834927.047077
Epoch 41: training loss 38213974648.471
Test Loss of 30418682822.663582, Test MSE of 30418683114.514359
Epoch 42: training loss 36933315478.588
Test Loss of 29164198269.926888, Test MSE of 29164198574.065342
Epoch 43: training loss 34906877266.824
Test Loss of 29183059635.354004, Test MSE of 29183059221.674290
Epoch 44: training loss 32907376858.353
Test Loss of 27866907715.761223, Test MSE of 27866907809.001865
Epoch 45: training loss 31420436472.471
Test Loss of 25935205946.047199, Test MSE of 25935206267.021221
Epoch 46: training loss 30266025155.765
Test Loss of 24308399628.557148, Test MSE of 24308399536.423500
Epoch 47: training loss 28283655936.000
Test Loss of 24940167230.548820, Test MSE of 24940167087.113941
Epoch 48: training loss 27349014825.412
Test Loss of 23155386459.927811, Test MSE of 23155386393.736248
Epoch 49: training loss 26024708280.471
Test Loss of 22906507288.640446, Test MSE of 22906506963.471714
Epoch 50: training loss 25141804080.941
Test Loss of 20755351615.496529, Test MSE of 20755351491.875202
Epoch 51: training loss 23675609144.471
Test Loss of 21086764137.669598, Test MSE of 21086764260.393852
Epoch 52: training loss 22884423691.294
Test Loss of 22442936377.810272, Test MSE of 22442935891.688259
Epoch 53: training loss 22100299843.765
Test Loss of 21840125945.366035, Test MSE of 21840126337.438477
Epoch 54: training loss 21257901153.882
Test Loss of 19819957495.826008, Test MSE of 19819957389.016369
Epoch 55: training loss 20195652182.588
Test Loss of 19426963218.124943, Test MSE of 19426963479.183987
Epoch 56: training loss 19937338624.000
Test Loss of 19022224861.171680, Test MSE of 19022224900.463135
Epoch 57: training loss 19110344380.235
Test Loss of 16848313135.030079, Test MSE of 16848312903.686939
Epoch 58: training loss 18469181078.588
Test Loss of 17530291996.549744, Test MSE of 17530292195.593834
Epoch 59: training loss 18171810066.824
Test Loss of 19728568260.294308, Test MSE of 19728568213.675484
Epoch 60: training loss 17591191766.588
Test Loss of 18196796042.128643, Test MSE of 18196796117.179611
Epoch 61: training loss 17186182106.353
Test Loss of 17043119317.708467, Test MSE of 17043119560.733091
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21430208263.922314, 'MSE - std': 4740248671.289491, 'R2 - mean': 0.8417295550957998, 'R2 - std': 0.029636007736590697} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005647 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043996340.706
Test Loss of 431613469509.775085, Test MSE of 431613470909.490784
Epoch 2: training loss 424025173534.118
Test Loss of 431594627379.057861, Test MSE of 431594631899.830750
Epoch 3: training loss 423998236792.471
Test Loss of 431567927205.967590, Test MSE of 431567927342.107910
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424014833904.941
Test Loss of 431571638542.097168, Test MSE of 431571639387.923157
Epoch 2: training loss 424002027158.588
Test Loss of 431574174915.701965, Test MSE of 431574182178.866150
Epoch 3: training loss 424001500461.176
Test Loss of 431573149039.237366, Test MSE of 431573146818.333496
Epoch 4: training loss 424001065441.882
Test Loss of 431572528024.225830, Test MSE of 431572526497.676758
Epoch 5: training loss 417087475832.471
Test Loss of 409020832196.057373, Test MSE of 409020837609.655029
Epoch 6: training loss 371572768527.059
Test Loss of 340357865718.404419, Test MSE of 340357857572.950500
Epoch 7: training loss 293240060566.588
Test Loss of 255411881929.980560, Test MSE of 255411878188.342560
Epoch 8: training loss 215631956359.529
Test Loss of 183342741648.999542, Test MSE of 183342740965.104858
Epoch 9: training loss 157694309978.353
Test Loss of 133169044268.186951, Test MSE of 133169045557.926941
Epoch 10: training loss 138421931083.294
Test Loss of 124646438897.784363, Test MSE of 124646437561.025085
Epoch 11: training loss 136531809822.118
Test Loss of 121372952578.843124, Test MSE of 121372950682.150482
Epoch 12: training loss 132232680960.000
Test Loss of 117798255370.069412, Test MSE of 117798255646.723862
Epoch 13: training loss 129274072244.706
Test Loss of 114952246781.393799, Test MSE of 114952244701.870010
Epoch 14: training loss 124524035584.000
Test Loss of 111026081539.909302, Test MSE of 111026083642.615662
Epoch 15: training loss 121544177558.588
Test Loss of 108120998684.075897, Test MSE of 108120999065.830612
Epoch 16: training loss 117907105641.412
Test Loss of 104856652540.801483, Test MSE of 104856653974.333328
Epoch 17: training loss 114702557680.941
Test Loss of 100510169403.587234, Test MSE of 100510171996.053284
Epoch 18: training loss 109882857381.647
Test Loss of 95965286178.236008, Test MSE of 95965285706.778046
Epoch 19: training loss 106830925402.353
Test Loss of 94136682438.189728, Test MSE of 94136680514.328979
Epoch 20: training loss 102911940909.176
Test Loss of 90429960550.234146, Test MSE of 90429959801.983109
Epoch 21: training loss 97888186759.529
Test Loss of 86346027252.509018, Test MSE of 86346026732.434433
Epoch 22: training loss 94636100442.353
Test Loss of 82533079154.672836, Test MSE of 82533077910.109100
Epoch 23: training loss 90533862144.000
Test Loss of 78866268190.326706, Test MSE of 78866267031.713089
Epoch 24: training loss 87661523184.941
Test Loss of 77231608983.159653, Test MSE of 77231608178.789948
Epoch 25: training loss 84018251640.471
Test Loss of 73379691353.677002, Test MSE of 73379691698.559189
Epoch 26: training loss 80620729178.353
Test Loss of 71543354413.016190, Test MSE of 71543355185.845474
Epoch 27: training loss 77067901560.471
Test Loss of 66299359446.182320, Test MSE of 66299359441.244415
Epoch 28: training loss 73911332382.118
Test Loss of 65017436316.372047, Test MSE of 65017436651.233467
Epoch 29: training loss 70344915712.000
Test Loss of 60756858934.019432, Test MSE of 60756859449.319565
Epoch 30: training loss 67037720033.882
Test Loss of 57503181549.638130, Test MSE of 57503182389.184624
Epoch 31: training loss 64376793298.824
Test Loss of 55411074168.832947, Test MSE of 55411074196.566856
Epoch 32: training loss 62185865539.765
Test Loss of 56601239599.385468, Test MSE of 56601240451.306198
Epoch 33: training loss 57862913084.235
Test Loss of 51324736352.310966, Test MSE of 51324737509.937790
Epoch 34: training loss 55612951597.176
Test Loss of 46520288625.132812, Test MSE of 46520288573.892570
Epoch 35: training loss 52949325010.824
Test Loss of 42415127565.741783, Test MSE of 42415127329.998657
Epoch 36: training loss 50137466447.059
Test Loss of 42895244684.616379, Test MSE of 42895243962.680878
Epoch 37: training loss 48385222272.000
Test Loss of 38638272751.770477, Test MSE of 38638272570.385857
Epoch 38: training loss 45872249871.059
Test Loss of 34721361963.594635, Test MSE of 34721362402.217987
Epoch 39: training loss 43207906959.059
Test Loss of 37094817486.837578, Test MSE of 37094817563.356827
Epoch 40: training loss 41367658721.882
Test Loss of 35692093465.588150, Test MSE of 35692093794.184731
Epoch 41: training loss 39463920862.118
Test Loss of 35431214438.708008, Test MSE of 35431214958.361595
Epoch 42: training loss 36805048436.706
Test Loss of 29988099715.968533, Test MSE of 29988099740.883640
Epoch 43: training loss 35490287201.882
Test Loss of 31623436069.079128, Test MSE of 31623436690.200138
Epoch 44: training loss 34455860864.000
Test Loss of 28676353341.008793, Test MSE of 28676353577.869259
Epoch 45: training loss 32291710908.235
Test Loss of 30299110321.813976, Test MSE of 30299110191.390293
Epoch 46: training loss 30970804363.294
Test Loss of 27599975560.944008, Test MSE of 27599975590.913113
Epoch 47: training loss 29449675896.471
Test Loss of 25688122912.459045, Test MSE of 25688123311.754715
Epoch 48: training loss 28485289088.000
Test Loss of 25863013649.414162, Test MSE of 25863013827.976482
Epoch 49: training loss 26888507576.471
Test Loss of 23743952003.257751, Test MSE of 23743951967.261353
Epoch 50: training loss 25931085037.176
Test Loss of 23424663979.416935, Test MSE of 23424663740.340324
Epoch 51: training loss 24465018326.588
Test Loss of 23110408896.148079, Test MSE of 23110408707.091156
Epoch 52: training loss 23802095770.353
Test Loss of 24351015583.925961, Test MSE of 24351015282.860859
Epoch 53: training loss 22844924933.647
Test Loss of 23350057639.507637, Test MSE of 23350057602.232685
Epoch 54: training loss 22042804261.647
Test Loss of 21117407872.651550, Test MSE of 21117408189.221901
Epoch 55: training loss 21192666093.176
Test Loss of 22838758445.016197, Test MSE of 22838758991.837833
Epoch 56: training loss 20466024741.647
Test Loss of 21224865741.771400, Test MSE of 21224865698.048889
Epoch 57: training loss 19839841754.353
Test Loss of 22489853708.438686, Test MSE of 22489854246.258015
Epoch 58: training loss 19049701590.588
Test Loss of 21433970909.764000, Test MSE of 21433970816.395554
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21430960774.41696, 'MSE - std': 4239807570.8241296, 'R2 - mean': 0.8413697061084451, 'R2 - std': 0.02651701960676222} 
 

Saving model.....
Results After CV: {'MSE - mean': 21430960774.41696, 'MSE - std': 4239807570.8241296, 'R2 - mean': 0.8413697061084451, 'R2 - std': 0.02651701960676222}
Train time: 90.65549319159909
Inference time: 0.07067805379920174
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 43 finished with value: 21430960774.41696 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005732 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525792707.765
Test Loss of 418110394952.601440, Test MSE of 418110394710.975586
Epoch 2: training loss 427506064805.647
Test Loss of 418093526565.307434, Test MSE of 418093527896.248047
Epoch 3: training loss 427479382497.882
Test Loss of 418071203643.869507, Test MSE of 418071204897.871826
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493226134.588
Test Loss of 418075837371.070068, Test MSE of 418075842085.555847
Epoch 2: training loss 427483534998.588
Test Loss of 418075873064.919739, Test MSE of 418075874321.975037
Epoch 3: training loss 427483057694.118
Test Loss of 418075656908.302551, Test MSE of 418075662711.701843
Epoch 4: training loss 421700005526.588
Test Loss of 400183037001.667358, Test MSE of 400183037252.713013
Epoch 5: training loss 384298219399.529
Test Loss of 344828424607.948181, Test MSE of 344828425354.854370
Epoch 6: training loss 316165042416.941
Test Loss of 267365341524.622711, Test MSE of 267365339471.757629
Epoch 7: training loss 224579004897.882
Test Loss of 170153277949.275970, Test MSE of 170153276366.286621
Epoch 8: training loss 160448641897.412
Test Loss of 130942370125.516541, Test MSE of 130942370676.502167
Epoch 9: training loss 141804953569.882
Test Loss of 120575544109.420303, Test MSE of 120575545174.221771
Epoch 10: training loss 135832289400.471
Test Loss of 115725799155.860275, Test MSE of 115725800795.556747
Epoch 11: training loss 131933949590.588
Test Loss of 112907624773.936615, Test MSE of 112907625113.581131
Epoch 12: training loss 130060713080.471
Test Loss of 110820247397.085358, Test MSE of 110820248152.663910
Epoch 13: training loss 126236415156.706
Test Loss of 106850039229.557251, Test MSE of 106850039257.230331
Epoch 14: training loss 123129232474.353
Test Loss of 103760240037.396255, Test MSE of 103760242375.935165
Epoch 15: training loss 118708761720.471
Test Loss of 99791428021.266708, Test MSE of 99791427435.401718
Epoch 16: training loss 114503128079.059
Test Loss of 97278142748.720795, Test MSE of 97278143296.988449
Epoch 17: training loss 111689823744.000
Test Loss of 92528537473.983810, Test MSE of 92528537377.853424
Epoch 18: training loss 106924314940.235
Test Loss of 89726749083.684479, Test MSE of 89726748908.375641
Epoch 19: training loss 102533015732.706
Test Loss of 86619919193.478607, Test MSE of 86619920297.792313
Epoch 20: training loss 100224114981.647
Test Loss of 84512337193.275040, Test MSE of 84512337544.548157
Epoch 21: training loss 95205252939.294
Test Loss of 79817683981.501740, Test MSE of 79817683425.728546
Epoch 22: training loss 92204611636.706
Test Loss of 76982513151.407822, Test MSE of 76982512350.876389
Epoch 23: training loss 88396246151.529
Test Loss of 74555382130.942398, Test MSE of 74555382143.393311
Epoch 24: training loss 84832789165.176
Test Loss of 73238586882.724030, Test MSE of 73238587295.764130
Epoch 25: training loss 81797027719.529
Test Loss of 68875976959.822342, Test MSE of 68875977842.054001
Epoch 26: training loss 77216098002.824
Test Loss of 65433913327.418922, Test MSE of 65433913365.013794
Epoch 27: training loss 74717533952.000
Test Loss of 63537502184.786491, Test MSE of 63537501960.790268
Epoch 28: training loss 70874937743.059
Test Loss of 60692365357.479530, Test MSE of 60692365290.916122
Epoch 29: training loss 66967520143.059
Test Loss of 57422027755.865837, Test MSE of 57422027105.672905
Epoch 30: training loss 64293501108.706
Test Loss of 56818000203.621559, Test MSE of 56818000487.420654
Epoch 31: training loss 62116026955.294
Test Loss of 52170112598.576912, Test MSE of 52170114064.258606
Epoch 32: training loss 59275977421.176
Test Loss of 49214849269.399956, Test MSE of 49214849555.990906
Epoch 33: training loss 55863919984.941
Test Loss of 44962652434.772148, Test MSE of 44962652891.658737
Epoch 34: training loss 52691325063.529
Test Loss of 45150493827.464264, Test MSE of 45150494219.899475
Epoch 35: training loss 50537674232.471
Test Loss of 43943589605.174187, Test MSE of 43943589448.125015
Epoch 36: training loss 47481512613.647
Test Loss of 39986310419.482765, Test MSE of 39986310346.216316
Epoch 37: training loss 45300065069.176
Test Loss of 39761088546.109650, Test MSE of 39761088645.208519
Epoch 38: training loss 42903430177.882
Test Loss of 37678362420.289612, Test MSE of 37678362339.336845
Epoch 39: training loss 41454733936.941
Test Loss of 37048023078.373352, Test MSE of 37048023070.995941
Epoch 40: training loss 40028214136.471
Test Loss of 35153921316.063843, Test MSE of 35153921127.764725
Epoch 41: training loss 37652711657.412
Test Loss of 33260438573.716400, Test MSE of 33260438823.883572
Epoch 42: training loss 35723459802.353
Test Loss of 30381938047.970390, Test MSE of 30381937612.190575
Epoch 43: training loss 33849226439.529
Test Loss of 29984877194.452000, Test MSE of 29984876881.191132
Epoch 44: training loss 32763386846.118
Test Loss of 31710458519.243118, Test MSE of 31710458451.190220
Epoch 45: training loss 31273474153.412
Test Loss of 29159078505.289845, Test MSE of 29159078620.588215
Epoch 46: training loss 29418365003.294
Test Loss of 25632145385.260235, Test MSE of 25632145647.501602
Epoch 47: training loss 28589361264.941
Test Loss of 26639783154.794357, Test MSE of 26639783462.419392
Epoch 48: training loss 27116485816.471
Test Loss of 26314018353.624798, Test MSE of 26314018431.637489
Epoch 49: training loss 26264324107.294
Test Loss of 25890159000.842007, Test MSE of 25890158810.821190
Epoch 50: training loss 24980663446.588
Test Loss of 25090470124.398796, Test MSE of 25090469443.098846
Epoch 51: training loss 24171300826.353
Test Loss of 25382094783.096924, Test MSE of 25382094874.140774
Epoch 52: training loss 23054433524.706
Test Loss of 22276344785.336109, Test MSE of 22276344769.374622
Epoch 53: training loss 22075627885.176
Test Loss of 23974485100.724499, Test MSE of 23974485196.411102
Epoch 54: training loss 21509194526.118
Test Loss of 21579066844.824429, Test MSE of 21579066947.294788
Epoch 55: training loss 20410882851.765
Test Loss of 22661491753.926441, Test MSE of 22661491436.193169
Epoch 56: training loss 19938952621.176
Test Loss of 23402684412.210041, Test MSE of 23402684529.898033
Epoch 57: training loss 19000304801.882
Test Loss of 19749241758.645386, Test MSE of 19749241787.069988
Epoch 58: training loss 18649288041.412
Test Loss of 20618227384.168400, Test MSE of 20618227145.177208
Epoch 59: training loss 17653557680.941
Test Loss of 20180874202.337265, Test MSE of 20180874108.309818
Epoch 60: training loss 17240361833.412
Test Loss of 19617372061.934769, Test MSE of 19617372183.567833
Epoch 61: training loss 16756696681.412
Test Loss of 17872337231.411518, Test MSE of 17872336936.922943
Epoch 62: training loss 16474585381.647
Test Loss of 19837866512.699512, Test MSE of 19837866342.982121
Epoch 63: training loss 15832353852.235
Test Loss of 20976573312.325699, Test MSE of 20976573539.997227
Epoch 64: training loss 14959855958.588
Test Loss of 18793286182.491787, Test MSE of 18793285910.966599
Epoch 65: training loss 15106597428.706
Test Loss of 18017809766.388157, Test MSE of 18017810104.021278
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18017810104.02128, 'MSE - std': 0.0, 'R2 - mean': 0.8596933578024604, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005427 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918667053.176
Test Loss of 424558822543.307861, Test MSE of 424558825591.023376
Epoch 2: training loss 427898865543.529
Test Loss of 424543625126.935913, Test MSE of 424543620317.236206
Epoch 3: training loss 427872081317.647
Test Loss of 424522468141.420288, Test MSE of 424522466924.441101
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427893415454.118
Test Loss of 424527936812.117493, Test MSE of 424527930904.447327
Epoch 2: training loss 427882066642.824
Test Loss of 424528664014.612061, Test MSE of 424528661293.264771
Epoch 3: training loss 427881386947.765
Test Loss of 424528174844.387695, Test MSE of 424528176012.137451
Epoch 4: training loss 422466419772.235
Test Loss of 407528416522.481628, Test MSE of 407528414933.892517
Epoch 5: training loss 385875189398.588
Test Loss of 353229734858.229919, Test MSE of 353229725969.483521
Epoch 6: training loss 317131350979.765
Test Loss of 275724002627.330994, Test MSE of 275724005010.552979
Epoch 7: training loss 224390089426.824
Test Loss of 180921567764.963226, Test MSE of 180921566395.311951
Epoch 8: training loss 159721335627.294
Test Loss of 142038767084.931763, Test MSE of 142038766781.422913
Epoch 9: training loss 140717124276.706
Test Loss of 131934033703.498489, Test MSE of 131934032154.356461
Epoch 10: training loss 135543634100.706
Test Loss of 127720755644.372894, Test MSE of 127720754714.911606
Epoch 11: training loss 131129994541.176
Test Loss of 125037442347.643768, Test MSE of 125037446231.132370
Epoch 12: training loss 129616485104.941
Test Loss of 121584957528.590332, Test MSE of 121584957609.185852
Epoch 13: training loss 124911167698.824
Test Loss of 119142013579.399490, Test MSE of 119142014739.250885
Epoch 14: training loss 121650599996.235
Test Loss of 116042351008.895676, Test MSE of 116042350668.824310
Epoch 15: training loss 118076309504.000
Test Loss of 112177331583.496643, Test MSE of 112177331868.670303
Epoch 16: training loss 112720433272.471
Test Loss of 109382086054.343750, Test MSE of 109382087254.378036
Epoch 17: training loss 109776447518.118
Test Loss of 104192460350.415909, Test MSE of 104192459831.583694
Epoch 18: training loss 105854732528.941
Test Loss of 101753536008.882721, Test MSE of 101753537176.382874
Epoch 19: training loss 102602284333.176
Test Loss of 97411275156.104553, Test MSE of 97411274995.536026
Epoch 20: training loss 97329235877.647
Test Loss of 94505622719.156143, Test MSE of 94505623179.020065
Epoch 21: training loss 93851713536.000
Test Loss of 91618658473.126999, Test MSE of 91618659557.700470
Epoch 22: training loss 90213373123.765
Test Loss of 88507311304.157303, Test MSE of 88507310043.505646
Epoch 23: training loss 87275949387.294
Test Loss of 85740753420.909561, Test MSE of 85740754145.063461
Epoch 24: training loss 83341413345.882
Test Loss of 81210998934.650940, Test MSE of 81210995399.401138
Epoch 25: training loss 80505587742.118
Test Loss of 77559098121.415680, Test MSE of 77559097326.270767
Epoch 26: training loss 75801320192.000
Test Loss of 73478226280.993759, Test MSE of 73478228581.250549
Epoch 27: training loss 72094663740.235
Test Loss of 69473203915.591949, Test MSE of 69473203750.379425
Epoch 28: training loss 69315154718.118
Test Loss of 66449951477.518387, Test MSE of 66449951122.399040
Epoch 29: training loss 65761675851.294
Test Loss of 66009398952.061066, Test MSE of 66009397338.377960
Epoch 30: training loss 62681412126.118
Test Loss of 61400836906.577843, Test MSE of 61400837635.689865
Epoch 31: training loss 59985786849.882
Test Loss of 58859883744.318298, Test MSE of 58859883702.831757
Epoch 32: training loss 57476091196.235
Test Loss of 57398550906.996071, Test MSE of 57398550809.176308
Epoch 33: training loss 54119976673.882
Test Loss of 55229659816.771683, Test MSE of 55229659120.425850
Epoch 34: training loss 51305825904.941
Test Loss of 51079218555.232941, Test MSE of 51079217856.432152
Epoch 35: training loss 48810837782.588
Test Loss of 51309424106.563034, Test MSE of 51309424450.230499
Epoch 36: training loss 45647763599.059
Test Loss of 47851696073.519318, Test MSE of 47851695338.548653
Epoch 37: training loss 43703308521.412
Test Loss of 46983131670.858200, Test MSE of 46983130930.501717
Epoch 38: training loss 41966392071.529
Test Loss of 43077271073.517464, Test MSE of 43077271433.380989
Epoch 39: training loss 39078327687.529
Test Loss of 43361962254.982185, Test MSE of 43361963065.914093
Epoch 40: training loss 36819547392.000
Test Loss of 39493155973.359238, Test MSE of 39493155897.407913
Epoch 41: training loss 35615803760.941
Test Loss of 38386099050.533424, Test MSE of 38386100336.597748
Epoch 42: training loss 33224487845.647
Test Loss of 38123363044.463570, Test MSE of 38123362874.057556
Epoch 43: training loss 31311827749.647
Test Loss of 36271308270.352997, Test MSE of 36271307447.999710
Epoch 44: training loss 30305055164.235
Test Loss of 35503031983.167244, Test MSE of 35503030918.898506
Epoch 45: training loss 29136478166.588
Test Loss of 31965772583.261623, Test MSE of 31965772880.405228
Epoch 46: training loss 27533513088.000
Test Loss of 34246957131.325470, Test MSE of 34246957896.740849
Epoch 47: training loss 25842010616.471
Test Loss of 34695376480.999306, Test MSE of 34695377658.827362
Epoch 48: training loss 25077187794.824
Test Loss of 29295488784.758732, Test MSE of 29295488511.787697
Epoch 49: training loss 23761714198.588
Test Loss of 32699539171.752949, Test MSE of 32699540175.630596
Epoch 50: training loss 22843187794.824
Test Loss of 34290663998.652786, Test MSE of 34290662758.910000
Epoch 51: training loss 21560713182.118
Test Loss of 29018726120.490398, Test MSE of 29018726461.867229
Epoch 52: training loss 21052717831.529
Test Loss of 29216123973.403656, Test MSE of 29216124755.213081
Epoch 53: training loss 19949435881.412
Test Loss of 29868997595.758499, Test MSE of 29868997318.224453
Epoch 54: training loss 19268972114.824
Test Loss of 26593823891.571594, Test MSE of 26593823559.758465
Epoch 55: training loss 18415311792.941
Test Loss of 28897512409.389774, Test MSE of 28897512541.875565
Epoch 56: training loss 17767074364.235
Test Loss of 26685299174.062458, Test MSE of 26685299069.006496
Epoch 57: training loss 16927951488.000
Test Loss of 28589333315.686329, Test MSE of 28589333589.925797
Epoch 58: training loss 16459186616.471
Test Loss of 25612994167.265324, Test MSE of 25612994390.939056
Epoch 59: training loss 15927184816.941
Test Loss of 27036282524.691185, Test MSE of 27036283072.165928
Epoch 60: training loss 15584150637.176
Test Loss of 28986138046.030998, Test MSE of 28986138341.181004
Epoch 61: training loss 15282972788.706
Test Loss of 26469824619.540134, Test MSE of 26469824943.535110
Epoch 62: training loss 14807504805.647
Test Loss of 24287768479.356003, Test MSE of 24287767779.229664
Epoch 63: training loss 13909940826.353
Test Loss of 26431549406.600971, Test MSE of 26431550147.071766
Epoch 64: training loss 13624992768.000
Test Loss of 25467778505.637753, Test MSE of 25467778502.826485
Epoch 65: training loss 13373823676.235
Test Loss of 28645299468.376591, Test MSE of 28645300659.586178
Epoch 66: training loss 12929371410.824
Test Loss of 25841104129.243580, Test MSE of 25841103966.474106
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21929457035.247692, 'MSE - std': 3911646931.2264137, 'R2 - mean': 0.8376025160619692, 'R2 - std': 0.02209084174049114} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005684 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927036807.529
Test Loss of 447258592755.564209, Test MSE of 447258601149.584290
Epoch 2: training loss 421906691975.529
Test Loss of 447240130174.371521, Test MSE of 447240131256.796997
Epoch 3: training loss 421879694034.824
Test Loss of 447215694493.875549, Test MSE of 447215699042.638428
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897880395.294
Test Loss of 447225175671.739075, Test MSE of 447225178574.528564
Epoch 2: training loss 421888334546.824
Test Loss of 447225721956.670837, Test MSE of 447225725280.518494
Epoch 3: training loss 421887886757.647
Test Loss of 447226223141.070557, Test MSE of 447226224509.899231
Epoch 4: training loss 416872591239.529
Test Loss of 430620533863.513306, Test MSE of 430620534323.925720
Epoch 5: training loss 381262528632.471
Test Loss of 376380844278.821167, Test MSE of 376380846748.448853
Epoch 6: training loss 314043136843.294
Test Loss of 297920618895.840881, Test MSE of 297920622057.777161
Epoch 7: training loss 222717578179.765
Test Loss of 197452883483.358765, Test MSE of 197452887162.611664
Epoch 8: training loss 155970234578.824
Test Loss of 154262164840.283142, Test MSE of 154262168167.877625
Epoch 9: training loss 137519804114.824
Test Loss of 142808779231.903778, Test MSE of 142808780112.006653
Epoch 10: training loss 131925679284.706
Test Loss of 137426688302.723114, Test MSE of 137426685182.222122
Epoch 11: training loss 129500479096.471
Test Loss of 134404066798.352997, Test MSE of 134404070352.597900
Epoch 12: training loss 124806155294.118
Test Loss of 131864560499.060837, Test MSE of 131864557717.122452
Epoch 13: training loss 121761392519.529
Test Loss of 127648921965.257462, Test MSE of 127648922498.673294
Epoch 14: training loss 116502237033.412
Test Loss of 122701372191.207962, Test MSE of 122701368358.196701
Epoch 15: training loss 114906554819.765
Test Loss of 119176587469.131622, Test MSE of 119176586385.529999
Epoch 16: training loss 112794741157.647
Test Loss of 117485874118.913712, Test MSE of 117485875986.871826
Epoch 17: training loss 105998441170.824
Test Loss of 111044731363.219986, Test MSE of 111044732020.022751
Epoch 18: training loss 102824719841.882
Test Loss of 107476568172.487625, Test MSE of 107476566915.119308
Epoch 19: training loss 98924161355.294
Test Loss of 105888395316.111954, Test MSE of 105888395630.784027
Epoch 20: training loss 94893549056.000
Test Loss of 99536414673.099243, Test MSE of 99536413593.073212
Epoch 21: training loss 92054480429.176
Test Loss of 97209522039.087677, Test MSE of 97209522022.836624
Epoch 22: training loss 88053723075.765
Test Loss of 95594468782.397415, Test MSE of 95594468151.421417
Epoch 23: training loss 84861358110.118
Test Loss of 92422562277.114960, Test MSE of 92422562493.927368
Epoch 24: training loss 81018144873.412
Test Loss of 87054878095.130234, Test MSE of 87054878193.239395
Epoch 25: training loss 77594037639.529
Test Loss of 84353152475.876938, Test MSE of 84353153200.313187
Epoch 26: training loss 73922836013.176
Test Loss of 77593988027.543839, Test MSE of 77593986528.315430
Epoch 27: training loss 70280992813.176
Test Loss of 74627596695.420776, Test MSE of 74627597371.361557
Epoch 28: training loss 66844031457.882
Test Loss of 73879410537.822815, Test MSE of 73879411049.261444
Epoch 29: training loss 64395219877.647
Test Loss of 70744158091.458710, Test MSE of 70744157159.586639
Epoch 30: training loss 60484800978.824
Test Loss of 65029177664.251678, Test MSE of 65029178269.280121
Epoch 31: training loss 57812426104.471
Test Loss of 59814815793.743233, Test MSE of 59814815522.705650
Epoch 32: training loss 54503019264.000
Test Loss of 61477463132.143417, Test MSE of 61477463294.568878
Epoch 33: training loss 52892486317.176
Test Loss of 59785296906.185516, Test MSE of 59785296749.285767
Epoch 34: training loss 49502923678.118
Test Loss of 52949005678.915565, Test MSE of 52949005156.228386
Epoch 35: training loss 46789615751.529
Test Loss of 54677231185.365715, Test MSE of 54677231476.681374
Epoch 36: training loss 44551011117.176
Test Loss of 49596987888.011101, Test MSE of 49596987332.923424
Epoch 37: training loss 42741266145.882
Test Loss of 48207788015.418922, Test MSE of 48207788146.766624
Epoch 38: training loss 40552338959.059
Test Loss of 46166036401.358315, Test MSE of 46166037827.361732
Epoch 39: training loss 38261676634.353
Test Loss of 45423053991.468887, Test MSE of 45423053193.872864
Epoch 40: training loss 36979597583.059
Test Loss of 42845799115.355080, Test MSE of 42845798892.853821
Epoch 41: training loss 34367418465.882
Test Loss of 42547193267.134857, Test MSE of 42547194390.039009
Epoch 42: training loss 32545278064.941
Test Loss of 38698659071.585472, Test MSE of 38698659748.435654
Epoch 43: training loss 31657974407.529
Test Loss of 39718376246.421463, Test MSE of 39718376342.587364
Epoch 44: training loss 29828808410.353
Test Loss of 36638367564.924355, Test MSE of 36638368264.364128
Epoch 45: training loss 28417096086.588
Test Loss of 32737307143.698357, Test MSE of 32737307534.739033
Epoch 46: training loss 27269977520.941
Test Loss of 33647493846.724960, Test MSE of 33647494419.446693
Epoch 47: training loss 25914901187.765
Test Loss of 32363101414.003239, Test MSE of 32363101263.888100
Epoch 48: training loss 25044117485.176
Test Loss of 28906826992.899376, Test MSE of 28906827578.788784
Epoch 49: training loss 23977173692.235
Test Loss of 29571222447.226463, Test MSE of 29571223141.613739
Epoch 50: training loss 22662954891.294
Test Loss of 29385337413.285217, Test MSE of 29385338123.438602
Epoch 51: training loss 22274368572.235
Test Loss of 29157350209.791348, Test MSE of 29157349928.680592
Epoch 52: training loss 21100169942.588
Test Loss of 27797983905.191765, Test MSE of 27797984211.386253
Epoch 53: training loss 20274823344.941
Test Loss of 24615038860.406200, Test MSE of 24615039249.310287
Epoch 54: training loss 19638924039.529
Test Loss of 28274658413.671986, Test MSE of 28274658844.928520
Epoch 55: training loss 19244765402.353
Test Loss of 27555552883.001617, Test MSE of 27555553003.093971
Epoch 56: training loss 18379544899.765
Test Loss of 26088615051.517929, Test MSE of 26088615335.025085
Epoch 57: training loss 17788496007.529
Test Loss of 26120038132.334026, Test MSE of 26120038533.719875
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23326317534.73842, 'MSE - std': 3755408497.9475913, 'R2 - mean': 0.8337752197917521, 'R2 - std': 0.018831708703203772} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005376 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110035245.176
Test Loss of 410764591627.135559, Test MSE of 410764590495.877625
Epoch 2: training loss 430089128056.471
Test Loss of 410745990214.604370, Test MSE of 410745988097.506165
Epoch 3: training loss 430061584624.941
Test Loss of 410722580488.529358, Test MSE of 410722582567.775635
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076865355.294
Test Loss of 410728101085.290161, Test MSE of 410728097936.018860
Epoch 2: training loss 430065963008.000
Test Loss of 410728251505.725098, Test MSE of 410728257741.227173
Epoch 3: training loss 430065497027.765
Test Loss of 410727611642.669128, Test MSE of 410727609680.149109
Epoch 4: training loss 424851237225.412
Test Loss of 394207356718.556213, Test MSE of 394207356990.934631
Epoch 5: training loss 388614223390.118
Test Loss of 339508096934.915344, Test MSE of 339508101557.404663
Epoch 6: training loss 320177551721.412
Test Loss of 262188447434.099030, Test MSE of 262188446719.073853
Epoch 7: training loss 229203668208.941
Test Loss of 164539724642.206390, Test MSE of 164539725467.772858
Epoch 8: training loss 164280370115.765
Test Loss of 125142460941.978714, Test MSE of 125142461565.581558
Epoch 9: training loss 143741126505.412
Test Loss of 113980418194.894958, Test MSE of 113980417678.930603
Epoch 10: training loss 139401970567.529
Test Loss of 109513906374.071259, Test MSE of 109513906865.187119
Epoch 11: training loss 135945886689.882
Test Loss of 106961561640.751511, Test MSE of 106961560698.780289
Epoch 12: training loss 130753938612.706
Test Loss of 104698156124.401672, Test MSE of 104698159387.122055
Epoch 13: training loss 127487658586.353
Test Loss of 100609308422.752426, Test MSE of 100609312751.433044
Epoch 14: training loss 123037041633.882
Test Loss of 97783094082.931976, Test MSE of 97783092635.345734
Epoch 15: training loss 121189170236.235
Test Loss of 95135643853.652939, Test MSE of 95135641547.663467
Epoch 16: training loss 116565820265.412
Test Loss of 92418946576.347992, Test MSE of 92418946332.374222
Epoch 17: training loss 111816340148.706
Test Loss of 87961641498.298935, Test MSE of 87961642227.185684
Epoch 18: training loss 108131912704.000
Test Loss of 86039585124.338730, Test MSE of 86039584923.935501
Epoch 19: training loss 104894243478.588
Test Loss of 82846910531.287369, Test MSE of 82846911939.559448
Epoch 20: training loss 100447807247.059
Test Loss of 80748626477.253128, Test MSE of 80748627120.305099
Epoch 21: training loss 97401190580.706
Test Loss of 77042616338.006485, Test MSE of 77042615862.756165
Epoch 22: training loss 93070363723.294
Test Loss of 72912056606.208237, Test MSE of 72912055532.747299
Epoch 23: training loss 89917238543.059
Test Loss of 69494818682.372971, Test MSE of 69494818501.070892
Epoch 24: training loss 85380216606.118
Test Loss of 68177119867.913002, Test MSE of 68177122445.293007
Epoch 25: training loss 82372006836.706
Test Loss of 64745134319.770477, Test MSE of 64745132569.049515
Epoch 26: training loss 78315273050.353
Test Loss of 62467037673.965759, Test MSE of 62467037737.167442
Epoch 27: training loss 74503308860.235
Test Loss of 58898265358.571030, Test MSE of 58898264702.272957
Epoch 28: training loss 71029733692.235
Test Loss of 57577289880.581215, Test MSE of 57577289838.029221
Epoch 29: training loss 67463795139.765
Test Loss of 53571538062.630264, Test MSE of 53571538095.072998
Epoch 30: training loss 65041963610.353
Test Loss of 50794344984.403519, Test MSE of 50794345034.931839
Epoch 31: training loss 61635828178.824
Test Loss of 50861623645.704765, Test MSE of 50861623847.298019
Epoch 32: training loss 58340112594.824
Test Loss of 46529689176.373901, Test MSE of 46529689021.260925
Epoch 33: training loss 55314925778.824
Test Loss of 46292512705.451180, Test MSE of 46292512126.913193
Epoch 34: training loss 53153011433.412
Test Loss of 44302084982.108284, Test MSE of 44302084982.753731
Epoch 35: training loss 50705987975.529
Test Loss of 40999805744.925499, Test MSE of 40999805856.197334
Epoch 36: training loss 47635370992.941
Test Loss of 36229166060.571960, Test MSE of 36229166380.612785
Epoch 37: training loss 45487342275.765
Test Loss of 39028285419.150391, Test MSE of 39028285164.603813
Epoch 38: training loss 43077398949.647
Test Loss of 35903800080.703377, Test MSE of 35903800760.131462
Epoch 39: training loss 41062198016.000
Test Loss of 34433468977.991669, Test MSE of 34433468717.811066
Epoch 40: training loss 39152040704.000
Test Loss of 32781132100.116611, Test MSE of 32781132443.679691
Epoch 41: training loss 37179218823.529
Test Loss of 31911406867.309578, Test MSE of 31911406720.437504
Epoch 42: training loss 35701595444.706
Test Loss of 30407750187.357704, Test MSE of 30407750211.104561
Epoch 43: training loss 34265775164.235
Test Loss of 30056272734.415550, Test MSE of 30056273057.089264
Epoch 44: training loss 32224905690.353
Test Loss of 29748316217.810272, Test MSE of 29748316092.362278
Epoch 45: training loss 30922347474.824
Test Loss of 27640421491.146690, Test MSE of 27640421459.296745
Epoch 46: training loss 29106034093.176
Test Loss of 26913374338.783897, Test MSE of 26913374527.151039
Epoch 47: training loss 28179429187.765
Test Loss of 27274543825.206848, Test MSE of 27274543549.514889
Epoch 48: training loss 26999723395.765
Test Loss of 25835882831.489124, Test MSE of 25835882881.842651
Epoch 49: training loss 26002683297.882
Test Loss of 24551341254.545116, Test MSE of 24551341526.417248
Epoch 50: training loss 24738589914.353
Test Loss of 25590472904.914391, Test MSE of 25590473204.247444
Epoch 51: training loss 23853644404.706
Test Loss of 23101625127.448402, Test MSE of 23101624996.297878
Epoch 52: training loss 22876234992.941
Test Loss of 22753241837.638130, Test MSE of 22753242162.385265
Epoch 53: training loss 21852313863.529
Test Loss of 23267868539.320686, Test MSE of 23267868926.304253
Epoch 54: training loss 21328486464.000
Test Loss of 19942809825.554836, Test MSE of 19942809579.466305
Epoch 55: training loss 20443632361.412
Test Loss of 21247885456.999538, Test MSE of 21247885511.441521
Epoch 56: training loss 19977144090.353
Test Loss of 20010539648.651550, Test MSE of 20010539686.293694
Epoch 57: training loss 19280034345.412
Test Loss of 20305436386.739471, Test MSE of 20305436482.082645
Epoch 58: training loss 18338406528.000
Test Loss of 19810797984.044422, Test MSE of 19810797597.315624
Epoch 59: training loss 18143658550.588
Test Loss of 21687216440.744099, Test MSE of 21687216700.787434
Epoch 60: training loss 17501766147.765
Test Loss of 19487563360.903286, Test MSE of 19487563625.069378
Epoch 61: training loss 17021790821.647
Test Loss of 20203410395.513187, Test MSE of 20203410577.500401
Epoch 62: training loss 16498131267.765
Test Loss of 19694138916.249886, Test MSE of 19694139774.787807
Epoch 63: training loss 15918061317.647
Test Loss of 20425424817.340118, Test MSE of 20425424902.808922
Epoch 64: training loss 15446847223.529
Test Loss of 20015494391.826008, Test MSE of 20015494426.907784
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22498611757.78076, 'MSE - std': 3554238357.154734, 'R2 - mean': 0.8340318381444131, 'R2 - std': 0.016314793852467393} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005285 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043829368.471
Test Loss of 431611507721.003235, Test MSE of 431611502396.934326
Epoch 2: training loss 424024835614.118
Test Loss of 431592391694.689514, Test MSE of 431592395526.396484
Epoch 3: training loss 423998638923.294
Test Loss of 431566006070.137878, Test MSE of 431566001073.247314
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009844856.471
Test Loss of 431568865013.693665, Test MSE of 431568857825.296265
Epoch 2: training loss 423999885312.000
Test Loss of 431571541860.575684, Test MSE of 431571540009.976196
Epoch 3: training loss 423999189955.765
Test Loss of 431570544203.105957, Test MSE of 431570535473.898804
Epoch 4: training loss 418774304527.059
Test Loss of 414733861697.984253, Test MSE of 414733861591.612610
Epoch 5: training loss 383017704146.824
Test Loss of 359686685725.852844, Test MSE of 359686687930.285034
Epoch 6: training loss 315433143838.118
Test Loss of 281122941924.042603, Test MSE of 281122938354.209534
Epoch 7: training loss 224431184986.353
Test Loss of 180821312716.231384, Test MSE of 180821317252.241730
Epoch 8: training loss 160100247943.529
Test Loss of 138453927802.372986, Test MSE of 138453927065.242188
Epoch 9: training loss 141075726848.000
Test Loss of 126787166022.248962, Test MSE of 126787167127.945740
Epoch 10: training loss 135512473750.588
Test Loss of 121224343478.078674, Test MSE of 121224341087.293121
Epoch 11: training loss 132497242202.353
Test Loss of 118681613472.636749, Test MSE of 118681612612.851105
Epoch 12: training loss 128884648327.529
Test Loss of 115598700640.666351, Test MSE of 115598699250.059021
Epoch 13: training loss 127097776790.588
Test Loss of 112228714703.548355, Test MSE of 112228714629.743469
Epoch 14: training loss 121565828698.353
Test Loss of 109370332434.361862, Test MSE of 109370331634.318344
Epoch 15: training loss 119007060841.412
Test Loss of 105565816296.544189, Test MSE of 105565816703.438080
Epoch 16: training loss 114274170458.353
Test Loss of 102805434589.763992, Test MSE of 102805434671.225449
Epoch 17: training loss 110219149583.059
Test Loss of 99342794395.661270, Test MSE of 99342794356.005264
Epoch 18: training loss 106129707248.941
Test Loss of 96196567112.025909, Test MSE of 96196570086.753525
Epoch 19: training loss 103026216387.765
Test Loss of 92930003582.756134, Test MSE of 92930003475.166061
Epoch 20: training loss 98659606844.235
Test Loss of 87362621313.006943, Test MSE of 87362620984.748077
Epoch 21: training loss 96130893748.706
Test Loss of 83581129092.560852, Test MSE of 83581130704.857178
Epoch 22: training loss 91933676619.294
Test Loss of 82221066201.143921, Test MSE of 82221066262.654785
Epoch 23: training loss 87512438196.706
Test Loss of 77954347836.298004, Test MSE of 77954348279.651688
Epoch 24: training loss 84063538928.941
Test Loss of 78565252138.173065, Test MSE of 78565252056.243515
Epoch 25: training loss 81056337106.824
Test Loss of 72682997603.627945, Test MSE of 72682997633.078171
Epoch 26: training loss 77366934377.412
Test Loss of 68943153418.306335, Test MSE of 68943153654.970291
Epoch 27: training loss 74044965556.706
Test Loss of 67670848183.618698, Test MSE of 67670848912.817688
Epoch 28: training loss 70739317684.706
Test Loss of 62847289843.442848, Test MSE of 62847289680.388474
Epoch 29: training loss 68016534934.588
Test Loss of 57840276348.742249, Test MSE of 57840276765.135414
Epoch 30: training loss 64569856444.235
Test Loss of 56127110277.153168, Test MSE of 56127109526.067924
Epoch 31: training loss 61991790802.824
Test Loss of 51978365762.931976, Test MSE of 51978364266.676613
Epoch 32: training loss 59106684160.000
Test Loss of 50706412215.618698, Test MSE of 50706412434.122528
Epoch 33: training loss 55249017261.176
Test Loss of 49879699031.426193, Test MSE of 49879698245.471497
Epoch 34: training loss 53276779452.235
Test Loss of 44758878533.064323, Test MSE of 44758879009.347397
Epoch 35: training loss 50414481264.941
Test Loss of 44353466506.365570, Test MSE of 44353466131.177536
Epoch 36: training loss 48338170518.588
Test Loss of 41004317141.116150, Test MSE of 41004317583.318962
Epoch 37: training loss 45796396092.235
Test Loss of 40120147181.401199, Test MSE of 40120146646.636253
Epoch 38: training loss 43865793968.941
Test Loss of 37085055934.134193, Test MSE of 37085057065.754913
Epoch 39: training loss 41418697178.353
Test Loss of 35993445601.080978, Test MSE of 35993445885.161110
Epoch 40: training loss 39597147226.353
Test Loss of 33009133254.308189, Test MSE of 33009133125.242844
Epoch 41: training loss 38031895280.941
Test Loss of 33785810191.518742, Test MSE of 33785810388.293072
Epoch 42: training loss 36168505856.000
Test Loss of 30071406257.458584, Test MSE of 30071406584.107075
Epoch 43: training loss 34494934753.882
Test Loss of 26154999383.426193, Test MSE of 26154999258.532082
Epoch 44: training loss 33065130036.706
Test Loss of 30231500419.020824, Test MSE of 30231500154.031105
Epoch 45: training loss 31554781635.765
Test Loss of 26926334159.548359, Test MSE of 26926334750.904762
Epoch 46: training loss 29785865072.941
Test Loss of 28187766267.498379, Test MSE of 28187766211.870552
Epoch 47: training loss 29141541383.529
Test Loss of 27919648573.719574, Test MSE of 27919648788.827656
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23582819163.99014, 'MSE - std': 3848130849.944303, 'R2 - mean': 0.8255244757686493, 'R2 - std': 0.02241514793904001} 
 

Saving model.....
Results After CV: {'MSE - mean': 23582819163.99014, 'MSE - std': 3848130849.944303, 'R2 - mean': 0.8255244757686493, 'R2 - std': 0.02241514793904001}
Train time: 90.98701707500076
Inference time: 0.07081865259970073
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 44 finished with value: 23582819163.99014 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005651 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525597063.529
Test Loss of 418109203328.325684, Test MSE of 418109198839.200928
Epoch 2: training loss 427504633735.529
Test Loss of 418089395553.650696, Test MSE of 418089388878.389648
Epoch 3: training loss 427477116807.529
Test Loss of 418064406046.201233, Test MSE of 418064413423.023499
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427488557537.882
Test Loss of 418066824874.903564, Test MSE of 418066821682.867920
Epoch 2: training loss 427476071243.294
Test Loss of 418067571762.453857, Test MSE of 418067575251.703674
Epoch 3: training loss 423629647992.471
Test Loss of 406042511692.095276, Test MSE of 406042516744.905334
Epoch 4: training loss 396826324751.059
Test Loss of 364158330493.897766, Test MSE of 364158331127.330078
Epoch 5: training loss 330854365424.941
Test Loss of 277971302249.822815, Test MSE of 277971305946.169495
Epoch 6: training loss 247736508596.706
Test Loss of 201092752381.394409, Test MSE of 201092750549.297852
Epoch 7: training loss 179427715312.941
Test Loss of 141268018271.933380, Test MSE of 141268017006.767303
Epoch 8: training loss 146053344707.765
Test Loss of 122485658609.077026, Test MSE of 122485658754.930466
Epoch 9: training loss 137452989078.588
Test Loss of 117211592788.800369, Test MSE of 117211593396.271362
Epoch 10: training loss 134712468178.824
Test Loss of 113971318447.640991, Test MSE of 113971316860.077728
Epoch 11: training loss 130772248801.882
Test Loss of 110924450203.684479, Test MSE of 110924448387.516357
Epoch 12: training loss 127519727299.765
Test Loss of 107929509793.250977, Test MSE of 107929512796.968430
Epoch 13: training loss 123826914755.765
Test Loss of 104727342485.288925, Test MSE of 104727341546.488022
Epoch 14: training loss 118871511250.824
Test Loss of 101532339394.946106, Test MSE of 101532337064.382919
Epoch 15: training loss 116797575062.588
Test Loss of 98554107702.658340, Test MSE of 98554104755.646866
Epoch 16: training loss 112677560545.882
Test Loss of 94734860680.260925, Test MSE of 94734861801.475632
Epoch 17: training loss 108481293598.118
Test Loss of 91292052786.749939, Test MSE of 91292053723.554871
Epoch 18: training loss 105426029040.941
Test Loss of 87666567086.989594, Test MSE of 87666569063.400467
Epoch 19: training loss 101458609784.471
Test Loss of 84018026829.042801, Test MSE of 84018027573.080536
Epoch 20: training loss 97933204796.235
Test Loss of 83308180211.149658, Test MSE of 83308181465.905670
Epoch 21: training loss 93249507832.471
Test Loss of 78991731853.886658, Test MSE of 78991731953.586838
Epoch 22: training loss 89271408707.765
Test Loss of 75364945524.896606, Test MSE of 75364947217.652237
Epoch 23: training loss 86680185705.412
Test Loss of 72001738550.658340, Test MSE of 72001739366.574677
Epoch 24: training loss 82740566136.471
Test Loss of 70118005271.805695, Test MSE of 70118005893.033997
Epoch 25: training loss 80062518776.471
Test Loss of 67809421844.963219, Test MSE of 67809421406.458824
Epoch 26: training loss 76527704395.294
Test Loss of 65541647842.983116, Test MSE of 65541648946.231148
Epoch 27: training loss 73276506194.824
Test Loss of 62690079694.967384, Test MSE of 62690079183.349396
Epoch 28: training loss 70219886373.647
Test Loss of 60576401353.519318, Test MSE of 60576401957.180565
Epoch 29: training loss 67733239168.000
Test Loss of 57706549999.359703, Test MSE of 57706549921.015961
Epoch 30: training loss 63973855043.765
Test Loss of 53342612051.497574, Test MSE of 53342611708.742310
Epoch 31: training loss 61120389534.118
Test Loss of 53548799465.141800, Test MSE of 53548800833.186157
Epoch 32: training loss 59086148645.647
Test Loss of 48784265089.983810, Test MSE of 48784264268.392281
Epoch 33: training loss 55815808956.235
Test Loss of 49590024081.617393, Test MSE of 49590023435.673019
Epoch 34: training loss 53854491459.765
Test Loss of 44843928662.932220, Test MSE of 44843927930.028625
Epoch 35: training loss 51034898341.647
Test Loss of 45047251215.929680, Test MSE of 45047251233.319908
Epoch 36: training loss 48877429202.824
Test Loss of 42236151895.642838, Test MSE of 42236151971.874657
Epoch 37: training loss 46583722710.588
Test Loss of 38673668161.850563, Test MSE of 38673668546.408844
Epoch 38: training loss 44385861857.882
Test Loss of 36329699358.793434, Test MSE of 36329699638.121658
Epoch 39: training loss 42606172438.588
Test Loss of 34748493359.729820, Test MSE of 34748493587.947357
Epoch 40: training loss 40901353231.059
Test Loss of 33580676426.910942, Test MSE of 33580676791.719509
Epoch 41: training loss 39246150031.059
Test Loss of 35554868438.843399, Test MSE of 35554868592.903717
Epoch 42: training loss 37066702388.706
Test Loss of 33312584650.229935, Test MSE of 33312584745.609009
Epoch 43: training loss 35552523331.765
Test Loss of 30745230632.327549, Test MSE of 30745231201.607780
Epoch 44: training loss 33574227124.706
Test Loss of 29897006251.258846, Test MSE of 29897006661.016010
Epoch 45: training loss 31887279582.118
Test Loss of 28184135228.520935, Test MSE of 28184135039.120930
Epoch 46: training loss 30964107873.882
Test Loss of 25690016221.535046, Test MSE of 25690016630.965664
Epoch 47: training loss 29347883783.529
Test Loss of 28596438711.694656, Test MSE of 28596438352.649147
Epoch 48: training loss 28433770462.118
Test Loss of 26527452323.678928, Test MSE of 26527452501.633747
Epoch 49: training loss 26999142121.412
Test Loss of 24834248688.366413, Test MSE of 24834249029.734486
Epoch 50: training loss 25759710174.118
Test Loss of 23062270764.472820, Test MSE of 23062270773.954655
Epoch 51: training loss 24572215744.000
Test Loss of 21851621793.369419, Test MSE of 21851621831.521969
Epoch 52: training loss 23779448591.059
Test Loss of 22992245314.442749, Test MSE of 22992245089.049984
Epoch 53: training loss 22900059760.941
Test Loss of 25198320120.538513, Test MSE of 25198320245.485153
Epoch 54: training loss 21958525586.824
Test Loss of 20543355077.077953, Test MSE of 20543354708.866859
Epoch 55: training loss 21121160628.706
Test Loss of 21100712890.122601, Test MSE of 21100713125.511028
Epoch 56: training loss 20305743947.294
Test Loss of 21345010236.757809, Test MSE of 21345010498.427471
Epoch 57: training loss 19691870817.882
Test Loss of 19894592762.848022, Test MSE of 19894592756.065762
Epoch 58: training loss 19294949989.647
Test Loss of 20704387611.121906, Test MSE of 20704387750.188614
Epoch 59: training loss 18473044837.647
Test Loss of 19597070499.442055, Test MSE of 19597070702.685913
Epoch 60: training loss 17919941526.588
Test Loss of 18012258276.049042, Test MSE of 18012258333.074898
Epoch 61: training loss 17615124400.941
Test Loss of 20489946425.382374, Test MSE of 20489946501.588795
Epoch 62: training loss 16579652879.059
Test Loss of 18551727528.238724, Test MSE of 18551727742.323486
Epoch 63: training loss 16326552474.353
Test Loss of 17883099147.369881, Test MSE of 17883098983.124485
Epoch 64: training loss 15965665061.647
Test Loss of 17338105292.717094, Test MSE of 17338105573.800545
Epoch 65: training loss 15668430592.000
Test Loss of 17498775809.243580, Test MSE of 17498776052.416862
Epoch 66: training loss 15223053084.235
Test Loss of 17431415745.702522, Test MSE of 17431415810.004902
Epoch 67: training loss 14503948845.176
Test Loss of 18035079818.452000, Test MSE of 18035079639.702194
Epoch 68: training loss 14460494279.529
Test Loss of 17704996879.633587, Test MSE of 17704996631.578972
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17704996631.57897, 'MSE - std': 0.0, 'R2 - mean': 0.8621292702523722, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003773 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918548751.059
Test Loss of 424556287461.351868, Test MSE of 424556293472.467041
Epoch 2: training loss 427898252107.294
Test Loss of 424539476469.222290, Test MSE of 424539476604.039001
Epoch 3: training loss 427869880560.941
Test Loss of 424516337182.674988, Test MSE of 424516340688.829590
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888027527.529
Test Loss of 424525840243.771484, Test MSE of 424525831025.651611
Epoch 2: training loss 427875025618.824
Test Loss of 424525949978.292847, Test MSE of 424525950279.178833
Epoch 3: training loss 423949568602.353
Test Loss of 412438747469.042786, Test MSE of 412438747258.510071
Epoch 4: training loss 396911657200.941
Test Loss of 371078676529.269470, Test MSE of 371078682239.658325
Epoch 5: training loss 330241419023.059
Test Loss of 284929387546.292847, Test MSE of 284929390675.583069
Epoch 6: training loss 246955933093.647
Test Loss of 210116067407.589172, Test MSE of 210116065235.433960
Epoch 7: training loss 176592548743.529
Test Loss of 151193397923.086731, Test MSE of 151193400656.120514
Epoch 8: training loss 143665381616.941
Test Loss of 133768545758.719406, Test MSE of 133768548642.068756
Epoch 9: training loss 136098107301.647
Test Loss of 128294498439.491089, Test MSE of 128294498265.882751
Epoch 10: training loss 131809109775.059
Test Loss of 125074042149.485077, Test MSE of 125074041929.442200
Epoch 11: training loss 130435204005.647
Test Loss of 121395285516.672684, Test MSE of 121395284741.384308
Epoch 12: training loss 125527941872.941
Test Loss of 118484896599.109879, Test MSE of 118484897390.072510
Epoch 13: training loss 121392677466.353
Test Loss of 114287039810.383530, Test MSE of 114287041831.773087
Epoch 14: training loss 117451013782.588
Test Loss of 109648914328.249832, Test MSE of 109648914395.377884
Epoch 15: training loss 114419200572.235
Test Loss of 107639615348.718948, Test MSE of 107639617303.202576
Epoch 16: training loss 109847986025.412
Test Loss of 104045279378.387238, Test MSE of 104045279498.697250
Epoch 17: training loss 105423076442.353
Test Loss of 99820122896.521866, Test MSE of 99820123843.752365
Epoch 18: training loss 102369833532.235
Test Loss of 98131703004.054596, Test MSE of 98131702677.115662
Epoch 19: training loss 98397712384.000
Test Loss of 91737975657.112198, Test MSE of 91737975858.851181
Epoch 20: training loss 94591439932.235
Test Loss of 87513757398.961838, Test MSE of 87513757509.870010
Epoch 21: training loss 91566792176.941
Test Loss of 85110936903.120987, Test MSE of 85110937192.539764
Epoch 22: training loss 87217407548.235
Test Loss of 81894456786.875778, Test MSE of 81894458187.155014
Epoch 23: training loss 83403294659.765
Test Loss of 79842135276.635666, Test MSE of 79842137672.987045
Epoch 24: training loss 80256602112.000
Test Loss of 77421490757.758957, Test MSE of 77421491474.358612
Epoch 25: training loss 76919080824.471
Test Loss of 72549206943.119125, Test MSE of 72549206522.761948
Epoch 26: training loss 72705467648.000
Test Loss of 67694327435.162621, Test MSE of 67694328041.219376
Epoch 27: training loss 71107882285.176
Test Loss of 65440470902.140182, Test MSE of 65440472513.687706
Epoch 28: training loss 67616566949.647
Test Loss of 63541100942.893364, Test MSE of 63541103154.704056
Epoch 29: training loss 63980681818.353
Test Loss of 59759126160.847557, Test MSE of 59759126215.989899
Epoch 30: training loss 61498298112.000
Test Loss of 60943974793.208420, Test MSE of 60943975590.555908
Epoch 31: training loss 58406631875.765
Test Loss of 56953726367.237564, Test MSE of 56953724596.844719
Epoch 32: training loss 55755308800.000
Test Loss of 54890258713.878326, Test MSE of 54890258375.572083
Epoch 33: training loss 53278172521.412
Test Loss of 53854644649.896828, Test MSE of 53854643940.165466
Epoch 34: training loss 50235916175.059
Test Loss of 50370887448.812401, Test MSE of 50370887941.814285
Epoch 35: training loss 47827415280.941
Test Loss of 50252183888.359009, Test MSE of 50252183663.591339
Epoch 36: training loss 46142711845.647
Test Loss of 47960252146.912788, Test MSE of 47960251858.008743
Epoch 37: training loss 44072943570.824
Test Loss of 49307491904.784637, Test MSE of 49307490738.348442
Epoch 38: training loss 41551860675.765
Test Loss of 46583940136.505203, Test MSE of 46583940022.568832
Epoch 39: training loss 39132385950.118
Test Loss of 40675425105.898682, Test MSE of 40675425106.940262
Epoch 40: training loss 37302182475.294
Test Loss of 41925925505.450844, Test MSE of 41925925858.352798
Epoch 41: training loss 35874927766.588
Test Loss of 39723900172.376587, Test MSE of 39723900233.289673
Epoch 42: training loss 33757877278.118
Test Loss of 38374251458.176270, Test MSE of 38374252407.752068
Epoch 43: training loss 32037345528.471
Test Loss of 39764965533.283371, Test MSE of 39764965060.568733
Epoch 44: training loss 30534026368.000
Test Loss of 35526558273.021515, Test MSE of 35526558777.328735
Epoch 45: training loss 28684398682.353
Test Loss of 34551250327.657646, Test MSE of 34551251935.240723
Epoch 46: training loss 27697078558.118
Test Loss of 34906901886.549156, Test MSE of 34906902384.218040
Epoch 47: training loss 26515795410.824
Test Loss of 32290677458.934998, Test MSE of 32290677390.950165
Epoch 48: training loss 25498742317.176
Test Loss of 33352668678.513996, Test MSE of 33352668959.005066
Epoch 49: training loss 24172935205.647
Test Loss of 32973827402.910942, Test MSE of 32973828684.987190
Epoch 50: training loss 22544390644.706
Test Loss of 29694747494.743465, Test MSE of 29694747320.025211
Epoch 51: training loss 21905844600.471
Test Loss of 30154443981.368496, Test MSE of 30154442650.015846
Epoch 52: training loss 20915656997.647
Test Loss of 30086961271.857506, Test MSE of 30086961627.136082
Epoch 53: training loss 19985338631.529
Test Loss of 30161321328.336803, Test MSE of 30161320875.107635
Epoch 54: training loss 19168301146.353
Test Loss of 28415867608.146194, Test MSE of 28415868128.523869
Epoch 55: training loss 18702115591.529
Test Loss of 30283550415.855656, Test MSE of 30283549639.217972
Epoch 56: training loss 17756317967.059
Test Loss of 30470332721.328709, Test MSE of 30470332559.227280
Epoch 57: training loss 17040530480.941
Test Loss of 26397511777.591488, Test MSE of 26397512772.330109
Epoch 58: training loss 16941366678.588
Test Loss of 31104047774.823040, Test MSE of 31104048117.259037
Epoch 59: training loss 16259802379.294
Test Loss of 28628158949.825584, Test MSE of 28628158188.156670
Epoch 60: training loss 15522043399.529
Test Loss of 24903296108.724499, Test MSE of 24903296727.374855
Epoch 61: training loss 14873835693.176
Test Loss of 26803759487.259773, Test MSE of 26803757940.939644
Epoch 62: training loss 14581646750.118
Test Loss of 28157295543.280128, Test MSE of 28157296477.720203
Epoch 63: training loss 13612634081.882
Test Loss of 27268102045.934769, Test MSE of 27268102849.699459
Epoch 64: training loss 13647440071.529
Test Loss of 27320971640.864216, Test MSE of 27320971232.421410
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22512983932.00019, 'MSE - std': 4807987300.421219, 'R2 - mean': 0.8335378374104261, 'R2 - std': 0.02859143284194615} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927187516.235
Test Loss of 447257486329.604431, Test MSE of 447257492442.652649
Epoch 2: training loss 421907712843.294
Test Loss of 447239089420.850342, Test MSE of 447239090827.257019
Epoch 3: training loss 421881608673.882
Test Loss of 447214722672.159119, Test MSE of 447214727044.402954
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898252649.412
Test Loss of 447223187655.446655, Test MSE of 447223187290.543396
Epoch 2: training loss 421886323410.824
Test Loss of 447223386598.062439, Test MSE of 447223385349.385132
Epoch 3: training loss 417807023405.176
Test Loss of 434313771301.248230, Test MSE of 434313775940.464722
Epoch 4: training loss 390565066631.529
Test Loss of 391283492843.628967, Test MSE of 391283495849.000061
Epoch 5: training loss 324246289347.765
Test Loss of 304029946357.222290, Test MSE of 304029945610.640747
Epoch 6: training loss 241578894275.765
Test Loss of 225759612769.769135, Test MSE of 225759617232.104889
Epoch 7: training loss 173355563008.000
Test Loss of 164488418596.063843, Test MSE of 164488416892.212891
Epoch 8: training loss 140409861812.706
Test Loss of 145165743227.410583, Test MSE of 145165743313.340424
Epoch 9: training loss 133537789108.706
Test Loss of 138648528195.331024, Test MSE of 138648529438.879700
Epoch 10: training loss 130850748355.765
Test Loss of 135197723517.009491, Test MSE of 135197725555.960526
Epoch 11: training loss 127845015732.706
Test Loss of 132636659747.057129, Test MSE of 132636660949.634720
Epoch 12: training loss 123660552643.765
Test Loss of 128281555530.496414, Test MSE of 128281555725.699432
Epoch 13: training loss 119554549940.706
Test Loss of 124517985143.324539, Test MSE of 124517986535.470367
Epoch 14: training loss 115705339843.765
Test Loss of 120604382852.530182, Test MSE of 120604381921.638306
Epoch 15: training loss 112389858635.294
Test Loss of 116669231402.459396, Test MSE of 116669230740.170212
Epoch 16: training loss 108422191043.765
Test Loss of 114979588863.940781, Test MSE of 114979588698.270264
Epoch 17: training loss 105142137012.706
Test Loss of 107411253763.908401, Test MSE of 107411256175.415222
Epoch 18: training loss 101076344199.529
Test Loss of 107610626070.266022, Test MSE of 107610625628.838623
Epoch 19: training loss 97428317936.941
Test Loss of 102168748951.065460, Test MSE of 102168748633.552628
Epoch 20: training loss 94186350712.471
Test Loss of 97912429635.034927, Test MSE of 97912427419.019958
Epoch 21: training loss 89843866985.412
Test Loss of 95389557952.577377, Test MSE of 95389557333.598038
Epoch 22: training loss 86859888097.882
Test Loss of 91631958525.275970, Test MSE of 91631957186.277344
Epoch 23: training loss 82496261104.941
Test Loss of 86751335651.634506, Test MSE of 86751335854.707886
Epoch 24: training loss 79497367446.588
Test Loss of 87034881764.226700, Test MSE of 87034881412.244080
Epoch 25: training loss 76817943386.353
Test Loss of 80945309681.313904, Test MSE of 80945310335.881119
Epoch 26: training loss 72477799604.706
Test Loss of 76528992932.507980, Test MSE of 76528992591.011444
Epoch 27: training loss 69705370202.353
Test Loss of 71888385615.470734, Test MSE of 71888384326.942062
Epoch 28: training loss 66801775058.824
Test Loss of 70899561894.817490, Test MSE of 70899561821.553986
Epoch 29: training loss 64095597643.294
Test Loss of 65679350600.897522, Test MSE of 65679349050.389168
Epoch 30: training loss 61226378443.294
Test Loss of 68996170791.083969, Test MSE of 68996170906.472733
Epoch 31: training loss 58559222384.941
Test Loss of 65507315980.139717, Test MSE of 65507316608.594551
Epoch 32: training loss 55599703401.412
Test Loss of 61219650045.275963, Test MSE of 61219649525.126068
Epoch 33: training loss 53174284438.588
Test Loss of 56572669011.142265, Test MSE of 56572669208.059258
Epoch 34: training loss 51495157797.647
Test Loss of 55082350148.100853, Test MSE of 55082351337.014191
Epoch 35: training loss 48850198204.235
Test Loss of 48708293005.945869, Test MSE of 48708293150.091171
Epoch 36: training loss 46160847329.882
Test Loss of 52159011490.139252, Test MSE of 52159011856.209885
Epoch 37: training loss 44163582471.529
Test Loss of 46552863923.312515, Test MSE of 46552864568.711502
Epoch 38: training loss 42498603399.529
Test Loss of 45800465543.727966, Test MSE of 45800465069.524330
Epoch 39: training loss 40400103092.706
Test Loss of 46659544148.563499, Test MSE of 46659544239.032913
Epoch 40: training loss 38602444800.000
Test Loss of 43623707604.652328, Test MSE of 43623707863.536896
Epoch 41: training loss 36419050240.000
Test Loss of 41654063760.610687, Test MSE of 41654064826.913483
Epoch 42: training loss 34581251139.765
Test Loss of 39130761770.755493, Test MSE of 39130762272.830193
Epoch 43: training loss 33122220830.118
Test Loss of 38318644033.317604, Test MSE of 38318644163.878632
Epoch 44: training loss 31662688809.412
Test Loss of 35889216958.030998, Test MSE of 35889216377.260643
Epoch 45: training loss 30335828939.294
Test Loss of 36230980736.148048, Test MSE of 36230980599.697739
Epoch 46: training loss 28970578831.059
Test Loss of 33463882062.464031, Test MSE of 33463881594.829617
Epoch 47: training loss 27640762176.000
Test Loss of 29826214775.324543, Test MSE of 29826214385.821579
Epoch 48: training loss 26704064301.176
Test Loss of 33269839256.605137, Test MSE of 33269838693.127247
Epoch 49: training loss 25521993080.471
Test Loss of 31530671325.002083, Test MSE of 31530670892.100002
Epoch 50: training loss 24361859862.588
Test Loss of 32895918823.779781, Test MSE of 32895919133.404846
Epoch 51: training loss 23441824843.294
Test Loss of 25766089773.242657, Test MSE of 25766089840.327332
Epoch 52: training loss 22526849234.824
Test Loss of 29233568968.631042, Test MSE of 29233568584.931007
Epoch 53: training loss 21668898292.706
Test Loss of 27318908379.403191, Test MSE of 27318907994.351868
Epoch 54: training loss 20624500288.000
Test Loss of 29197454932.208187, Test MSE of 29197454937.062458
Epoch 55: training loss 19703683154.824
Test Loss of 25385813778.653713, Test MSE of 25385813758.098606
Epoch 56: training loss 19418077191.529
Test Loss of 25543749026.080036, Test MSE of 25543749439.839611
Epoch 57: training loss 18531547907.765
Test Loss of 26829940337.580383, Test MSE of 26829940355.243744
Epoch 58: training loss 18279056911.059
Test Loss of 25052959277.124218, Test MSE of 25052959341.728848
Epoch 59: training loss 17352942806.588
Test Loss of 26064238921.252834, Test MSE of 26064238785.682137
Epoch 60: training loss 17012925522.824
Test Loss of 26431747944.875317, Test MSE of 26431748433.485039
Epoch 61: training loss 16408404227.765
Test Loss of 25206665370.914642, Test MSE of 25206666156.273815
Epoch 62: training loss 15894794601.412
Test Loss of 25066851301.707150, Test MSE of 25066851162.666950
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23364273008.88911, 'MSE - std': 4106159694.7596416, 'R2 - mean': 0.8334024336323472, 'R2 - std': 0.02334559251057711} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005427 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430111225012.706
Test Loss of 410764908859.587219, Test MSE of 410764909051.750916
Epoch 2: training loss 430091369050.353
Test Loss of 410746848096.310974, Test MSE of 410746850178.557129
Epoch 3: training loss 430064492544.000
Test Loss of 410723131615.659424, Test MSE of 410723132067.391052
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078718192.941
Test Loss of 410730026497.184631, Test MSE of 410730026148.746643
Epoch 2: training loss 430066229729.882
Test Loss of 410729679847.833435, Test MSE of 410729674871.499084
Epoch 3: training loss 426303899286.588
Test Loss of 398919044056.670044, Test MSE of 398919044593.894348
Epoch 4: training loss 399638632929.882
Test Loss of 357505365258.780212, Test MSE of 357505366591.707214
Epoch 5: training loss 333632649697.882
Test Loss of 271875909142.034241, Test MSE of 271875912633.668091
Epoch 6: training loss 250363021191.529
Test Loss of 193831213978.121246, Test MSE of 193831209158.761871
Epoch 7: training loss 181494996329.412
Test Loss of 135601219010.635818, Test MSE of 135601220911.989014
Epoch 8: training loss 149151939523.765
Test Loss of 116586376937.373444, Test MSE of 116586375176.754395
Epoch 9: training loss 139684312907.294
Test Loss of 111708848992.310974, Test MSE of 111708849506.779266
Epoch 10: training loss 136734778759.529
Test Loss of 109188703978.321152, Test MSE of 109188704627.189819
Epoch 11: training loss 133640058337.882
Test Loss of 106101366287.400284, Test MSE of 106101367076.394501
Epoch 12: training loss 130780255563.294
Test Loss of 103510687406.615463, Test MSE of 103510689124.978851
Epoch 13: training loss 125864754718.118
Test Loss of 99976832549.197601, Test MSE of 99976834200.326782
Epoch 14: training loss 122505234703.059
Test Loss of 97535111583.096710, Test MSE of 97535111353.779633
Epoch 15: training loss 119356887431.529
Test Loss of 95225776813.193893, Test MSE of 95225776971.401138
Epoch 16: training loss 115606434002.824
Test Loss of 90687870188.453491, Test MSE of 90687871294.465881
Epoch 17: training loss 110949344045.176
Test Loss of 87497425335.737152, Test MSE of 87497424108.210236
Epoch 18: training loss 108242361072.941
Test Loss of 85269073808.170288, Test MSE of 85269074025.890640
Epoch 19: training loss 103914787026.824
Test Loss of 82176335015.270706, Test MSE of 82176335336.426804
Epoch 20: training loss 99411556713.412
Test Loss of 78533897764.249878, Test MSE of 78533897771.702133
Epoch 21: training loss 96305290721.882
Test Loss of 77608511255.811203, Test MSE of 77608511885.827408
Epoch 22: training loss 93471555945.412
Test Loss of 71803000385.628876, Test MSE of 71803000928.286530
Epoch 23: training loss 89381411011.765
Test Loss of 70098008595.664963, Test MSE of 70098009353.136902
Epoch 24: training loss 85623871457.882
Test Loss of 69551985669.212402, Test MSE of 69551986523.064545
Epoch 25: training loss 82276683233.882
Test Loss of 66015781788.964371, Test MSE of 66015782782.110741
Epoch 26: training loss 78555130503.529
Test Loss of 64401873230.541412, Test MSE of 64401872741.897278
Epoch 27: training loss 75279278080.000
Test Loss of 60483540212.035172, Test MSE of 60483541382.261566
Epoch 28: training loss 71917220984.471
Test Loss of 58343022667.816750, Test MSE of 58343021986.229805
Epoch 29: training loss 68919617536.000
Test Loss of 54272560691.413231, Test MSE of 54272560619.482742
Epoch 30: training loss 66434766878.118
Test Loss of 53564799621.390099, Test MSE of 53564799534.687889
Epoch 31: training loss 63665945780.706
Test Loss of 51521406631.981491, Test MSE of 51521406199.986931
Epoch 32: training loss 60549906898.824
Test Loss of 49274932959.896347, Test MSE of 49274933157.258835
Epoch 33: training loss 57655502433.882
Test Loss of 44647134474.780197, Test MSE of 44647133767.907265
Epoch 34: training loss 55401995572.706
Test Loss of 44961550777.632576, Test MSE of 44961551642.020149
Epoch 35: training loss 53242572950.588
Test Loss of 44276454308.546043, Test MSE of 44276454330.937965
Epoch 36: training loss 51539029993.412
Test Loss of 41461971305.077278, Test MSE of 41461970725.254814
Epoch 37: training loss 48955302964.706
Test Loss of 38820537588.035172, Test MSE of 38820537547.672607
Epoch 38: training loss 46706781846.588
Test Loss of 39620224871.418785, Test MSE of 39620224733.560059
Epoch 39: training loss 44339697618.824
Test Loss of 36927521952.636742, Test MSE of 36927521991.917442
Epoch 40: training loss 42013376828.235
Test Loss of 32700380305.473392, Test MSE of 32700380618.725445
Epoch 41: training loss 40816995561.412
Test Loss of 34448151663.355850, Test MSE of 34448151414.145912
Epoch 42: training loss 38386859572.706
Test Loss of 31487973962.632114, Test MSE of 31487974259.646465
Epoch 43: training loss 36860937065.412
Test Loss of 30198783327.126331, Test MSE of 30198783337.124397
Epoch 44: training loss 34924871446.588
Test Loss of 28882345123.479870, Test MSE of 28882345185.616394
Epoch 45: training loss 33586806369.882
Test Loss of 28757448845.208698, Test MSE of 28757448813.379772
Epoch 46: training loss 32004866688.000
Test Loss of 25751648195.346600, Test MSE of 25751648346.553127
Epoch 47: training loss 30424903055.059
Test Loss of 25243379316.805183, Test MSE of 25243379095.320217
Epoch 48: training loss 29682888071.529
Test Loss of 26668800914.539566, Test MSE of 26668801355.617798
Epoch 49: training loss 28359487269.647
Test Loss of 26377437691.024525, Test MSE of 26377437921.922722
Epoch 50: training loss 27103172875.294
Test Loss of 24919704118.730217, Test MSE of 24919704683.745148
Epoch 51: training loss 25815539632.941
Test Loss of 26113003789.149467, Test MSE of 26113003945.271114
Epoch 52: training loss 24816788235.294
Test Loss of 24072598225.206848, Test MSE of 24072598600.485394
Epoch 53: training loss 23934429172.706
Test Loss of 22133497522.406292, Test MSE of 22133496985.292732
Epoch 54: training loss 22911631265.882
Test Loss of 21791500681.299400, Test MSE of 21791500205.728077
Epoch 55: training loss 22160167544.471
Test Loss of 23112304655.163349, Test MSE of 23112304609.936588
Epoch 56: training loss 21377834703.059
Test Loss of 22199763381.841740, Test MSE of 22199762878.759716
Epoch 57: training loss 20319067523.765
Test Loss of 20586262921.299400, Test MSE of 20586263419.102310
Epoch 58: training loss 19815742132.706
Test Loss of 20089622214.308189, Test MSE of 20089622007.285408
Epoch 59: training loss 19285342196.706
Test Loss of 19407930454.715408, Test MSE of 19407930323.664055
Epoch 60: training loss 18847327751.529
Test Loss of 21040745530.284126, Test MSE of 21040745538.964333
Epoch 61: training loss 18016113012.706
Test Loss of 21962762799.148540, Test MSE of 21962763386.857922
Epoch 62: training loss 17184909296.941
Test Loss of 20560633485.445625, Test MSE of 20560633744.358212
Epoch 63: training loss 17069802398.118
Test Loss of 17990130091.416935, Test MSE of 17990130645.294708
Epoch 64: training loss 16585239766.588
Test Loss of 17883363576.299862, Test MSE of 17883363351.082905
Epoch 65: training loss 15994510942.118
Test Loss of 19148548756.553448, Test MSE of 19148548940.487072
Epoch 66: training loss 15627217428.706
Test Loss of 18829011516.416473, Test MSE of 18829011423.682930
Epoch 67: training loss 15083660988.235
Test Loss of 19917167883.727905, Test MSE of 19917168042.376972
Epoch 68: training loss 14920602375.529
Test Loss of 20048188963.302174, Test MSE of 20048188861.852551
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22535251972.12997, 'MSE - std': 3835001723.24851, 'R2 - mean': 0.8336847874721947, 'R2 - std': 0.020223790157264422} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005458 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043860931.765
Test Loss of 431612824915.753845, Test MSE of 431612826938.271606
Epoch 2: training loss 424024411557.647
Test Loss of 431592761149.719604, Test MSE of 431592754922.658264
Epoch 3: training loss 423996976067.765
Test Loss of 431565508712.248047, Test MSE of 431565513490.156311
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011679141.647
Test Loss of 431569923263.911133, Test MSE of 431569927879.761230
Epoch 2: training loss 423999572329.412
Test Loss of 431570650225.251282, Test MSE of 431570649129.105591
Epoch 3: training loss 420436379045.647
Test Loss of 419736189383.848206, Test MSE of 419736192144.986145
Epoch 4: training loss 394332402507.294
Test Loss of 377550191464.840332, Test MSE of 377550187577.322083
Epoch 5: training loss 329068763316.706
Test Loss of 290313169459.887085, Test MSE of 290313170346.404541
Epoch 6: training loss 246972297095.529
Test Loss of 211777726527.496521, Test MSE of 211777728799.379608
Epoch 7: training loss 177899100732.235
Test Loss of 149152264034.680237, Test MSE of 149152264854.452271
Epoch 8: training loss 144365308295.529
Test Loss of 129104239301.834335, Test MSE of 129104238830.016052
Epoch 9: training loss 135759685180.235
Test Loss of 121686246771.028229, Test MSE of 121686248001.513824
Epoch 10: training loss 133150320368.941
Test Loss of 118705427408.140671, Test MSE of 118705428673.723114
Epoch 11: training loss 129947019354.353
Test Loss of 115708279482.461823, Test MSE of 115708278542.723526
Epoch 12: training loss 126049736704.000
Test Loss of 112329088663.870438, Test MSE of 112329088920.412140
Epoch 13: training loss 122439619087.059
Test Loss of 108445708308.849609, Test MSE of 108445709682.170609
Epoch 14: training loss 118547915354.353
Test Loss of 105457562041.632584, Test MSE of 105457563537.117859
Epoch 15: training loss 115149020280.471
Test Loss of 102309977895.922256, Test MSE of 102309977784.579681
Epoch 16: training loss 111202147659.294
Test Loss of 97355085931.091156, Test MSE of 97355086052.811981
Epoch 17: training loss 106778759273.412
Test Loss of 93094062374.737625, Test MSE of 93094062136.325745
Epoch 18: training loss 102807761061.647
Test Loss of 88720713586.791306, Test MSE of 88720715326.410645
Epoch 19: training loss 100001443388.235
Test Loss of 85792524256.725586, Test MSE of 85792522856.755402
Epoch 20: training loss 95746586624.000
Test Loss of 83696480409.528915, Test MSE of 83696480940.371521
Epoch 21: training loss 91917782663.529
Test Loss of 80134194562.191574, Test MSE of 80134193834.296585
Epoch 22: training loss 88789394868.706
Test Loss of 75947525908.020355, Test MSE of 75947524999.837204
Epoch 23: training loss 84883760414.118
Test Loss of 74114408296.366501, Test MSE of 74114409027.294708
Epoch 24: training loss 81064081897.412
Test Loss of 71187570409.373444, Test MSE of 71187569658.469543
Epoch 25: training loss 78207383205.647
Test Loss of 66405930794.765388, Test MSE of 66405930168.411430
Epoch 26: training loss 74874951318.588
Test Loss of 63407018292.953262, Test MSE of 63407017613.561760
Epoch 27: training loss 71903266740.706
Test Loss of 62574591706.210091, Test MSE of 62574589937.362740
Epoch 28: training loss 68281245816.471
Test Loss of 56181680138.424805, Test MSE of 56181681785.398376
Epoch 29: training loss 65330178627.765
Test Loss of 54681938952.055527, Test MSE of 54681939300.303230
Epoch 30: training loss 62135963708.235
Test Loss of 52564264557.223511, Test MSE of 52564264962.114273
Epoch 31: training loss 59629978450.824
Test Loss of 48538453744.007401, Test MSE of 48538454046.275261
Epoch 32: training loss 57139110091.294
Test Loss of 46599246367.985191, Test MSE of 46599246559.127586
Epoch 33: training loss 54618641084.235
Test Loss of 45307773100.483109, Test MSE of 45307773859.546547
Epoch 34: training loss 51935905648.941
Test Loss of 44408643142.367424, Test MSE of 44408642951.467720
Epoch 35: training loss 49464448790.588
Test Loss of 43073991036.979179, Test MSE of 43073991995.789825
Epoch 36: training loss 48231747738.353
Test Loss of 41486777916.890327, Test MSE of 41486778260.214340
Epoch 37: training loss 44884203339.294
Test Loss of 36998427376.481262, Test MSE of 36998427704.915443
Epoch 38: training loss 43677573940.706
Test Loss of 39377503341.460434, Test MSE of 39377503659.156334
Epoch 39: training loss 41355899083.294
Test Loss of 33230280046.289680, Test MSE of 33230279820.305759
Epoch 40: training loss 39156273837.176
Test Loss of 32265762415.118927, Test MSE of 32265762746.593895
Epoch 41: training loss 38328277104.941
Test Loss of 32638204012.038872, Test MSE of 32638204144.316391
Epoch 42: training loss 36063649626.353
Test Loss of 29792837497.425266, Test MSE of 29792837005.889080
Epoch 43: training loss 33731909406.118
Test Loss of 30525149269.293846, Test MSE of 30525149296.049759
Epoch 44: training loss 32387515188.706
Test Loss of 27572381224.040722, Test MSE of 27572380717.725166
Epoch 45: training loss 31324409524.706
Test Loss of 26649440249.366035, Test MSE of 26649440456.102684
Epoch 46: training loss 29682505976.471
Test Loss of 26449174744.077744, Test MSE of 26449175199.733906
Epoch 47: training loss 28545075983.059
Test Loss of 24240815138.591393, Test MSE of 24240815268.455120
Epoch 48: training loss 27296959992.471
Test Loss of 27182562218.232300, Test MSE of 27182562208.413769
Epoch 49: training loss 26498924592.941
Test Loss of 23177122550.167515, Test MSE of 23177122728.686661
Epoch 50: training loss 25375825904.941
Test Loss of 23333043243.120777, Test MSE of 23333043043.509956
Epoch 51: training loss 24317095198.118
Test Loss of 22101966175.126331, Test MSE of 22101966327.507107
Epoch 52: training loss 23553288862.118
Test Loss of 23943716632.758907, Test MSE of 23943716657.788960
Epoch 53: training loss 22293603949.176
Test Loss of 22755130709.649235, Test MSE of 22755130714.180244
Epoch 54: training loss 21647540506.353
Test Loss of 19886797195.194817, Test MSE of 19886797045.467716
Epoch 55: training loss 20893980886.588
Test Loss of 21690423623.907452, Test MSE of 21690423551.522991
Epoch 56: training loss 20083844032.000
Test Loss of 20282715841.569645, Test MSE of 20282715919.535526
Epoch 57: training loss 19903639792.941
Test Loss of 20613403291.187412, Test MSE of 20613403260.690487
Epoch 58: training loss 18813789865.412
Test Loss of 20316124606.844978, Test MSE of 20316124816.404022
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22091426540.98478, 'MSE - std': 3543122158.0944805, 'R2 - mean': 0.836603515095655, 'R2 - std': 0.019007294242470614} 
 

Saving model.....
Results After CV: {'MSE - mean': 22091426540.98478, 'MSE - std': 3543122158.0944805, 'R2 - mean': 0.836603515095655, 'R2 - std': 0.019007294242470614}
Train time: 98.53200686940036
Inference time: 0.07046943920067861
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 45 finished with value: 22091426540.98478 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005471 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525642842.353
Test Loss of 418111087514.855408, Test MSE of 418111084249.732361
Epoch 2: training loss 427505632677.647
Test Loss of 418093962804.941040, Test MSE of 418093961638.637085
Epoch 3: training loss 427478821104.941
Test Loss of 418070995946.444580, Test MSE of 418070994935.176086
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495174866.824
Test Loss of 418076513280.236877, Test MSE of 418076517002.236206
Epoch 2: training loss 427483250206.118
Test Loss of 418078853247.437439, Test MSE of 418078854601.017273
Epoch 3: training loss 427482758445.176
Test Loss of 418078260831.104309, Test MSE of 418078264174.879822
Epoch 4: training loss 427482397153.882
Test Loss of 418078163520.074036, Test MSE of 418078168017.884766
Epoch 5: training loss 427482197534.118
Test Loss of 418077478909.157532, Test MSE of 418077478063.994629
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418077478063.9946, 'MSE - std': 0.0, 'R2 - mean': -2.255614682745651, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005473 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917868815.059
Test Loss of 424557850668.532043, Test MSE of 424557848009.392578
Epoch 2: training loss 427896330119.529
Test Loss of 424541317563.899170, Test MSE of 424541325806.368408
Epoch 3: training loss 427868341428.706
Test Loss of 424519210124.465393, Test MSE of 424519214932.316406
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890235994.353
Test Loss of 424525732191.992615, Test MSE of 424525736389.085205
Epoch 2: training loss 427878416624.941
Test Loss of 424527690995.268127, Test MSE of 424527690177.206726
Epoch 3: training loss 427877956909.176
Test Loss of 424527232508.565369, Test MSE of 424527238066.592163
Epoch 4: training loss 427877574535.529
Test Loss of 424526537247.859375, Test MSE of 424526537493.940918
Epoch 5: training loss 427877296489.412
Test Loss of 424525620077.375916, Test MSE of 424525622827.643555
Epoch 6: training loss 427877090966.588
Test Loss of 424525322459.343994, Test MSE of 424525323238.639343
Epoch 7: training loss 427876940860.235
Test Loss of 424524979953.017822, Test MSE of 424524975758.922913
Epoch 8: training loss 417929275512.471
Test Loss of 392496224145.617371, Test MSE of 392496226360.713989
Epoch 9: training loss 351235111755.294
Test Loss of 300746036535.013672, Test MSE of 300746043992.650146
Epoch 10: training loss 251731277703.529
Test Loss of 207061576847.781647, Test MSE of 207061576734.536041
Epoch 11: training loss 177115163708.235
Test Loss of 153348514760.334961, Test MSE of 153348518511.243530
Epoch 12: training loss 144903462520.471
Test Loss of 134850354396.528336, Test MSE of 134850356428.640060
Epoch 13: training loss 135693281551.059
Test Loss of 128998352250.285446, Test MSE of 128998350908.041260
Epoch 14: training loss 133164224512.000
Test Loss of 126937566418.105942, Test MSE of 126937566727.186264
Epoch 15: training loss 129841542384.941
Test Loss of 123028224535.805695, Test MSE of 123028225117.577911
Epoch 16: training loss 124695083339.294
Test Loss of 119393279283.697433, Test MSE of 119393279714.473297
Epoch 17: training loss 122031440323.765
Test Loss of 115555427031.672455, Test MSE of 115555428156.434677
Epoch 18: training loss 115772214061.176
Test Loss of 112634480147.778854, Test MSE of 112634479835.180725
Epoch 19: training loss 114099965259.294
Test Loss of 108916602218.178116, Test MSE of 108916603211.249283
Epoch 20: training loss 109794017761.882
Test Loss of 106312916178.105942, Test MSE of 106312917366.008102
Epoch 21: training loss 106098433716.706
Test Loss of 102427132967.083969, Test MSE of 102427133648.664902
Epoch 22: training loss 102402148472.471
Test Loss of 97116034982.462173, Test MSE of 97116034745.454239
Epoch 23: training loss 96609994932.706
Test Loss of 94013758465.658112, Test MSE of 94013759433.806046
Epoch 24: training loss 92792831488.000
Test Loss of 88394319746.457550, Test MSE of 88394317223.148560
Epoch 25: training loss 89436432233.412
Test Loss of 84099805024.821655, Test MSE of 84099804921.797775
Epoch 26: training loss 85508242462.118
Test Loss of 82112345557.244507, Test MSE of 82112346308.277908
Epoch 27: training loss 81967925775.059
Test Loss of 77369945618.357620, Test MSE of 77369946292.858643
Epoch 28: training loss 77892495390.118
Test Loss of 75635342455.146896, Test MSE of 75635342692.004303
Epoch 29: training loss 75241811576.471
Test Loss of 75577870277.729355, Test MSE of 75577871017.726486
Epoch 30: training loss 70324103740.235
Test Loss of 71425572979.593796, Test MSE of 71425571710.373184
Epoch 31: training loss 66667576756.706
Test Loss of 66104141817.841316, Test MSE of 66104141718.879639
Epoch 32: training loss 63768047811.765
Test Loss of 61220538419.638214, Test MSE of 61220538119.238441
Epoch 33: training loss 60759314371.765
Test Loss of 55979407399.320839, Test MSE of 55979407889.333099
Epoch 34: training loss 57303299900.235
Test Loss of 57117276011.717789, Test MSE of 57117275821.274887
Epoch 35: training loss 54289174317.176
Test Loss of 53356497315.264397, Test MSE of 53356497177.461472
Epoch 36: training loss 51935165289.412
Test Loss of 52343003080.098083, Test MSE of 52343003613.033279
Epoch 37: training loss 48405350904.471
Test Loss of 49037016031.548462, Test MSE of 49037016789.243652
Epoch 38: training loss 46545717895.529
Test Loss of 46157215299.627113, Test MSE of 46157214762.478119
Epoch 39: training loss 43665882782.118
Test Loss of 43124911982.086517, Test MSE of 43124911831.034454
Epoch 40: training loss 41306702938.353
Test Loss of 38511903413.562805, Test MSE of 38511903515.723503
Epoch 41: training loss 39078447969.882
Test Loss of 38676383590.032845, Test MSE of 38676382764.800781
Epoch 42: training loss 36839414625.882
Test Loss of 37989969763.190376, Test MSE of 37989970179.777611
Epoch 43: training loss 34736183973.647
Test Loss of 35792136286.275269, Test MSE of 35792137702.017525
Epoch 44: training loss 32819860080.941
Test Loss of 38062313952.851257, Test MSE of 38062313312.734962
Epoch 45: training loss 30538883365.647
Test Loss of 34535499685.514687, Test MSE of 34535499420.199348
Epoch 46: training loss 29067629876.706
Test Loss of 32215837751.665047, Test MSE of 32215838618.073368
Epoch 47: training loss 27701872971.294
Test Loss of 31346024168.964146, Test MSE of 31346024209.736099
Epoch 48: training loss 25889185852.235
Test Loss of 29007369933.486931, Test MSE of 29007368934.327057
Epoch 49: training loss 24750659230.118
Test Loss of 27324832737.443443, Test MSE of 27324832843.974079
Epoch 50: training loss 23812993837.176
Test Loss of 27934416730.189220, Test MSE of 27934416600.048325
Epoch 51: training loss 22889941127.529
Test Loss of 29312760251.899143, Test MSE of 29312760139.053631
Epoch 52: training loss 21977058424.471
Test Loss of 27670020917.473976, Test MSE of 27670022021.069866
Epoch 53: training loss 20617794955.294
Test Loss of 27679903503.811241, Test MSE of 27679903306.612629
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 222878690685.30362, 'MSE - std': 195198787378.69098, 'R2 - mean': -0.7266154074408544, 'R2 - std': 1.528999275304797} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005384 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926551311.059
Test Loss of 447258352032.421936, Test MSE of 447258347236.594360
Epoch 2: training loss 421904748544.000
Test Loss of 447239605416.416382, Test MSE of 447239607762.929993
Epoch 3: training loss 421876659983.059
Test Loss of 447214530233.115906, Test MSE of 447214532124.367126
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897528621.176
Test Loss of 447221155903.008118, Test MSE of 447221151538.170715
Epoch 2: training loss 421887249106.824
Test Loss of 447222185984.000000, Test MSE of 447222184499.358765
Epoch 3: training loss 421886762887.529
Test Loss of 447221901691.469788, Test MSE of 447221903081.107483
Epoch 4: training loss 421886416173.176
Test Loss of 447222345617.143677, Test MSE of 447222340404.116821
Epoch 5: training loss 421886166919.529
Test Loss of 447222142566.210510, Test MSE of 447222145498.797852
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 297659842289.8017, 'MSE - std': 191275069816.56232, 'R2 - mean': -1.1434530411374795, 'R2 - std': 1.3806036415230356} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005246 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110407499.294
Test Loss of 410764310544.111084, Test MSE of 410764317417.100708
Epoch 2: training loss 430088965180.235
Test Loss of 410746444159.822327, Test MSE of 410746443000.477539
Epoch 3: training loss 430061216948.706
Test Loss of 410723003470.186035, Test MSE of 410723001454.395264
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077516378.353
Test Loss of 410728597487.888916, Test MSE of 410728595949.552917
Epoch 2: training loss 430066097453.176
Test Loss of 410728857979.083740, Test MSE of 410728852698.414429
Epoch 3: training loss 430065599427.765
Test Loss of 410729151602.198975, Test MSE of 410729151516.282349
Epoch 4: training loss 430065202356.706
Test Loss of 410728802769.325317, Test MSE of 410728806693.104553
Epoch 5: training loss 430064921419.294
Test Loss of 410729130995.205933, Test MSE of 410729121435.579102
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 325927162076.24603, 'MSE - std': 172733142024.7007, 'R2 - mean': -1.455080154640839, 'R2 - std': 1.3118247538643668} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 7, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005208 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042827535.059
Test Loss of 431611265744.259155, Test MSE of 431611263333.476074
Epoch 2: training loss 424022182851.765
Test Loss of 431589257142.078674, Test MSE of 431589259652.602173
Epoch 3: training loss 423994086460.235
Test Loss of 431559651837.867676, Test MSE of 431559650910.689697
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008654848.000
Test Loss of 431562673014.108276, Test MSE of 431562671119.535034
Epoch 2: training loss 423994584124.235
Test Loss of 431564773138.124939, Test MSE of 431564775142.683960
Epoch 3: training loss 423993850458.353
Test Loss of 431564273718.019409, Test MSE of 431564269679.653564
Epoch 4: training loss 423993314484.706
Test Loss of 431564599669.397522, Test MSE of 431564598690.054138
Epoch 5: training loss 423993019331.765
Test Loss of 431563104669.201294, Test MSE of 431563097060.131653
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 347054349073.02313, 'MSE - std': 160171229640.55527, 'R2 - mean': -1.6086499700308328, 'R2 - std': 1.2128652442364372} 
 

Saving model.....
Results After CV: {'MSE - mean': 347054349073.02313, 'MSE - std': 160171229640.55527, 'R2 - mean': -1.6086499700308328, 'R2 - std': 1.2128652442364372}
Train time: 25.737293824199877
Inference time: 0.06923917339881883
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 46 finished with value: 347054349073.02313 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 7, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005494 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525375638.588
Test Loss of 418110949560.286865, Test MSE of 418110951198.917480
Epoch 2: training loss 427504911902.118
Test Loss of 418093114316.124939, Test MSE of 418093109709.667542
Epoch 3: training loss 427477891433.412
Test Loss of 418069209192.223938, Test MSE of 418069206391.589966
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494707802.353
Test Loss of 418074170895.041382, Test MSE of 418074162183.502686
Epoch 2: training loss 427481823111.529
Test Loss of 418075262744.812378, Test MSE of 418075267614.896729
Epoch 3: training loss 427481356649.412
Test Loss of 418074895088.307190, Test MSE of 418074901831.892334
Epoch 4: training loss 427481009573.647
Test Loss of 418075084927.437439, Test MSE of 418075087294.270752
Epoch 5: training loss 420708824726.588
Test Loss of 396202142185.378662, Test MSE of 396202141146.971313
Epoch 6: training loss 375583502697.412
Test Loss of 330231994473.645142, Test MSE of 330231994198.742859
Epoch 7: training loss 296565906371.765
Test Loss of 244785598897.003021, Test MSE of 244785602464.146057
Epoch 8: training loss 218158075904.000
Test Loss of 172903973784.960449, Test MSE of 172903971789.610352
Epoch 9: training loss 158635825242.353
Test Loss of 125331767864.494095, Test MSE of 125331768154.452225
Epoch 10: training loss 140743512274.824
Test Loss of 118567791752.201706, Test MSE of 118567794038.418228
Epoch 11: training loss 135306333876.706
Test Loss of 115193246631.172791, Test MSE of 115193247525.659332
Epoch 12: training loss 132953049328.941
Test Loss of 112679681600.784637, Test MSE of 112679681092.411331
Epoch 13: training loss 129570114590.118
Test Loss of 108842889597.838531, Test MSE of 108842889702.696854
Epoch 14: training loss 125307292370.824
Test Loss of 105824754578.801758, Test MSE of 105824754216.519287
Epoch 15: training loss 122558342987.294
Test Loss of 101711201775.774231, Test MSE of 101711200531.996140
Epoch 16: training loss 118114220092.235
Test Loss of 99075882301.172333, Test MSE of 99075882254.004959
Epoch 17: training loss 114571057995.294
Test Loss of 96388002092.354385, Test MSE of 96388001777.531143
Epoch 18: training loss 110811923817.412
Test Loss of 91877266600.890121, Test MSE of 91877263645.254822
Epoch 19: training loss 106055513359.059
Test Loss of 88167130880.651398, Test MSE of 88167131128.211868
Epoch 20: training loss 101977915738.353
Test Loss of 86402397714.831360, Test MSE of 86402397775.338104
Epoch 21: training loss 97829943280.941
Test Loss of 82960725602.420547, Test MSE of 82960724735.154373
Epoch 22: training loss 94228810405.647
Test Loss of 78287220932.130463, Test MSE of 78287220821.869308
Epoch 23: training loss 90803623152.941
Test Loss of 76336482550.821182, Test MSE of 76336482144.684860
Epoch 24: training loss 87305730401.882
Test Loss of 74174888627.194077, Test MSE of 74174888386.371323
Epoch 25: training loss 82703838991.059
Test Loss of 69560777853.779327, Test MSE of 69560777371.595230
Epoch 26: training loss 79139038614.588
Test Loss of 67579703190.828590, Test MSE of 67579704097.334618
Epoch 27: training loss 76131387482.353
Test Loss of 63819939648.606987, Test MSE of 63819939716.408081
Epoch 28: training loss 73402030848.000
Test Loss of 60801244376.738373, Test MSE of 60801244823.626877
Epoch 29: training loss 70360841712.941
Test Loss of 59644691468.554245, Test MSE of 59644692363.356682
Epoch 30: training loss 66413033901.176
Test Loss of 52721602650.011566, Test MSE of 52721602875.544907
Epoch 31: training loss 63190314932.706
Test Loss of 52443640465.558174, Test MSE of 52443640298.783829
Epoch 32: training loss 60994297280.000
Test Loss of 50580474279.054359, Test MSE of 50580474045.277916
Epoch 33: training loss 57442137615.059
Test Loss of 47460903833.434189, Test MSE of 47460903389.556084
Epoch 34: training loss 54967426379.294
Test Loss of 45021819465.548927, Test MSE of 45021819618.800194
Epoch 35: training loss 52176794624.000
Test Loss of 46158494612.696739, Test MSE of 46158494622.049286
Epoch 36: training loss 49745687808.000
Test Loss of 42816150039.805687, Test MSE of 42816148970.037735
Epoch 37: training loss 47229505076.706
Test Loss of 43346357395.571594, Test MSE of 43346358128.052551
Epoch 38: training loss 44358038908.235
Test Loss of 35922171653.388847, Test MSE of 35922171900.016495
Epoch 39: training loss 42179707760.941
Test Loss of 36623993164.095306, Test MSE of 36623992883.945496
Epoch 40: training loss 40051143032.471
Test Loss of 35990818797.997688, Test MSE of 35990817717.077797
Epoch 41: training loss 38533170439.529
Test Loss of 30971506143.193153, Test MSE of 30971506622.463760
Epoch 42: training loss 36183366896.941
Test Loss of 31576561891.871387, Test MSE of 31576561853.957813
Epoch 43: training loss 34362916389.647
Test Loss of 31124088558.649086, Test MSE of 31124089436.585594
Epoch 44: training loss 32867175055.059
Test Loss of 26992416383.792736, Test MSE of 26992416602.015591
Epoch 45: training loss 31330003230.118
Test Loss of 28772939635.297710, Test MSE of 28772939920.647083
Epoch 46: training loss 29432940333.176
Test Loss of 26898479621.803379, Test MSE of 26898479520.990921
Epoch 47: training loss 28414445104.941
Test Loss of 25394708093.424011, Test MSE of 25394708335.962215
Epoch 48: training loss 27258662151.529
Test Loss of 24508194257.928291, Test MSE of 24508194505.737770
Epoch 49: training loss 25943667930.353
Test Loss of 24217057456.233170, Test MSE of 24217057518.340691
Epoch 50: training loss 24935701360.941
Test Loss of 22854204205.894054, Test MSE of 22854204428.567581
Epoch 51: training loss 24027757142.588
Test Loss of 24044373189.314827, Test MSE of 24044373297.195732
Epoch 52: training loss 22758858405.647
Test Loss of 21847958317.894054, Test MSE of 21847958637.751484
Epoch 53: training loss 22079472862.118
Test Loss of 19108609856.606987, Test MSE of 19108609819.343773
Epoch 54: training loss 21044459617.882
Test Loss of 20877932106.733288, Test MSE of 20877932092.247635
Epoch 55: training loss 20244921193.412
Test Loss of 19347433886.526947, Test MSE of 19347434009.514091
Epoch 56: training loss 19524532476.235
Test Loss of 20047995933.609066, Test MSE of 20047995808.368565
Epoch 57: training loss 18795392244.706
Test Loss of 19543581240.020355, Test MSE of 19543581051.379494
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19543581051.379494, 'MSE - std': 0.0, 'R2 - mean': 0.8478120138904934, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005301 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917303808.000
Test Loss of 424556249940.741150, Test MSE of 424556242896.005920
Epoch 2: training loss 427896081709.176
Test Loss of 424539871375.544739, Test MSE of 424539873264.504517
Epoch 3: training loss 427867814731.294
Test Loss of 424517261758.741638, Test MSE of 424517261750.985107
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886854625.882
Test Loss of 424524026800.647705, Test MSE of 424524037041.041321
Epoch 2: training loss 427876818703.059
Test Loss of 424523954389.659058, Test MSE of 424523958019.119690
Epoch 3: training loss 427876310317.176
Test Loss of 424523667542.695374, Test MSE of 424523671360.121155
Epoch 4: training loss 427875887947.294
Test Loss of 424523387024.966003, Test MSE of 424523393522.152039
Epoch 5: training loss 420856288918.588
Test Loss of 402819914110.312256, Test MSE of 402819920023.867249
Epoch 6: training loss 374710575826.824
Test Loss of 336983663579.284729, Test MSE of 336983662275.279053
Epoch 7: training loss 295671811734.588
Test Loss of 252450390129.698822, Test MSE of 252450395359.755219
Epoch 8: training loss 216246150806.588
Test Loss of 183683755189.681244, Test MSE of 183683756614.162201
Epoch 9: training loss 158201332374.588
Test Loss of 136752699395.316223, Test MSE of 136752697746.054413
Epoch 10: training loss 137260977724.235
Test Loss of 129223238302.823044, Test MSE of 129223236945.738846
Epoch 11: training loss 132482202834.824
Test Loss of 126936470478.967377, Test MSE of 126936472603.759018
Epoch 12: training loss 130105908163.765
Test Loss of 123801345763.752945, Test MSE of 123801343382.474579
Epoch 13: training loss 126455221820.235
Test Loss of 119787601906.735138, Test MSE of 119787604986.943069
Epoch 14: training loss 123373105513.412
Test Loss of 115539155650.353928, Test MSE of 115539158896.158829
Epoch 15: training loss 117306385468.235
Test Loss of 112527051510.465881, Test MSE of 112527052043.806671
Epoch 16: training loss 115056203896.471
Test Loss of 109510219532.968765, Test MSE of 109510217652.153015
Epoch 17: training loss 110585035083.294
Test Loss of 105493754783.592880, Test MSE of 105493753735.733658
Epoch 18: training loss 106475061549.176
Test Loss of 103986518843.632660, Test MSE of 103986519864.137482
Epoch 19: training loss 102826864128.000
Test Loss of 98119784269.398102, Test MSE of 98119785639.453629
Epoch 20: training loss 98952313133.176
Test Loss of 93693547660.465424, Test MSE of 93693549397.741089
Epoch 21: training loss 94623892299.294
Test Loss of 89550879589.322235, Test MSE of 89550879983.270935
Epoch 22: training loss 91185143868.235
Test Loss of 86236502629.499878, Test MSE of 86236502450.865662
Epoch 23: training loss 87238641965.176
Test Loss of 84029703898.751785, Test MSE of 84029703670.933258
Epoch 24: training loss 82401984798.118
Test Loss of 80515951496.379364, Test MSE of 80515951956.658112
Epoch 25: training loss 79713048967.529
Test Loss of 75878151367.209808, Test MSE of 75878150469.543259
Epoch 26: training loss 76477130932.706
Test Loss of 71354588096.281281, Test MSE of 71354587956.641434
Epoch 27: training loss 71619682213.647
Test Loss of 69047972056.027756, Test MSE of 69047969481.873108
Epoch 28: training loss 68020542283.294
Test Loss of 65296663416.745781, Test MSE of 65296665676.857460
Epoch 29: training loss 64604685462.588
Test Loss of 63131441467.040482, Test MSE of 63131442452.084122
Epoch 30: training loss 62085769607.529
Test Loss of 58995838921.045570, Test MSE of 58995838189.171715
Epoch 31: training loss 59157737411.765
Test Loss of 57060243354.144806, Test MSE of 57060242590.256676
Epoch 32: training loss 55707239514.353
Test Loss of 56249735074.672218, Test MSE of 56249736432.062401
Epoch 33: training loss 52409662599.529
Test Loss of 52914110174.068008, Test MSE of 52914109832.570938
Epoch 34: training loss 48878940672.000
Test Loss of 46787763528.068474, Test MSE of 46787764347.922737
Epoch 35: training loss 47292356276.706
Test Loss of 48065609394.483459, Test MSE of 48065608067.745972
Epoch 36: training loss 44627276634.353
Test Loss of 47753350149.448067, Test MSE of 47753349904.441406
Epoch 37: training loss 42242147004.235
Test Loss of 39657980473.915337, Test MSE of 39657980925.836563
Epoch 38: training loss 40290269048.471
Test Loss of 40521534024.601433, Test MSE of 40521534349.360153
Epoch 39: training loss 37791480440.471
Test Loss of 37244198847.096924, Test MSE of 37244198606.391869
Epoch 40: training loss 35743994563.765
Test Loss of 37673540753.676613, Test MSE of 37673541183.697243
Epoch 41: training loss 34345625389.176
Test Loss of 35171328429.213043, Test MSE of 35171328524.224007
Epoch 42: training loss 31740218563.765
Test Loss of 35945378892.983574, Test MSE of 35945379778.349144
Epoch 43: training loss 30096611546.353
Test Loss of 32458671591.246819, Test MSE of 32458672146.907967
Epoch 44: training loss 28222411143.529
Test Loss of 32384917251.493870, Test MSE of 32384916040.095203
Epoch 45: training loss 27210699550.118
Test Loss of 31806809869.442516, Test MSE of 31806808719.374985
Epoch 46: training loss 25979296662.588
Test Loss of 32424757502.874855, Test MSE of 32424757195.646519
Epoch 47: training loss 24440096414.118
Test Loss of 28541535353.989361, Test MSE of 28541535295.480873
Epoch 48: training loss 22686945400.471
Test Loss of 28034707959.591026, Test MSE of 28034707532.721554
Epoch 49: training loss 22142174742.588
Test Loss of 28943140454.684246, Test MSE of 28943140761.470943
Epoch 50: training loss 21179126177.882
Test Loss of 29095705857.480453, Test MSE of 29095705044.243507
Epoch 51: training loss 19917244536.471
Test Loss of 26476396506.811012, Test MSE of 26476396729.323898
Epoch 52: training loss 18933446185.412
Test Loss of 25949673232.284988, Test MSE of 25949673751.340961
Epoch 53: training loss 18488617562.353
Test Loss of 26521950461.690495, Test MSE of 26521950440.495979
Epoch 54: training loss 17596259471.059
Test Loss of 25645292151.975945, Test MSE of 25645292670.258682
Epoch 55: training loss 16807823085.176
Test Loss of 25436021653.170483, Test MSE of 25436021708.916176
Epoch 56: training loss 16347758896.941
Test Loss of 26130288388.441360, Test MSE of 26130288787.213276
Epoch 57: training loss 15952635956.706
Test Loss of 26156203926.828590, Test MSE of 26156202587.866501
Epoch 58: training loss 15546517831.529
Test Loss of 24519891894.332638, Test MSE of 24519891724.606228
Epoch 59: training loss 14712915625.412
Test Loss of 26957255809.095535, Test MSE of 26957256405.345963
Epoch 60: training loss 14407365650.824
Test Loss of 24794726772.837383, Test MSE of 24794726404.928688
Epoch 61: training loss 13956378398.118
Test Loss of 28009340541.897755, Test MSE of 28009341133.454945
Epoch 62: training loss 13343724043.294
Test Loss of 24554865702.136478, Test MSE of 24554866045.135014
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22049223548.257256, 'MSE - std': 2505642496.87776, 'R2 - mean': 0.8362532865002413, 'R2 - std': 0.0115587273902521} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003563 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926973319.529
Test Loss of 447256984190.134644, Test MSE of 447256987375.048218
Epoch 2: training loss 421905554130.824
Test Loss of 447237922961.913513, Test MSE of 447237916561.451050
Epoch 3: training loss 421878239352.471
Test Loss of 447213164619.325439, Test MSE of 447213161959.667358
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897348517.647
Test Loss of 447221580742.440002, Test MSE of 447221570255.973938
Epoch 2: training loss 421885918750.118
Test Loss of 447223386673.861694, Test MSE of 447223393400.383667
Epoch 3: training loss 421885439638.588
Test Loss of 447223351821.383301, Test MSE of 447223351476.232849
Epoch 4: training loss 421885104489.412
Test Loss of 447223426695.846375, Test MSE of 447223428511.247375
Epoch 5: training loss 415105079055.059
Test Loss of 425631157897.741394, Test MSE of 425631156885.696960
Epoch 6: training loss 369779789583.059
Test Loss of 358000551500.628296, Test MSE of 358000546295.613037
Epoch 7: training loss 290883158256.941
Test Loss of 271475492479.792725, Test MSE of 271475489030.814362
Epoch 8: training loss 212251466752.000
Test Loss of 199746867026.609314, Test MSE of 199746870292.704803
Epoch 9: training loss 154424569735.529
Test Loss of 148905641967.418915, Test MSE of 148905643899.452179
Epoch 10: training loss 133028827617.882
Test Loss of 140162825542.647247, Test MSE of 140162825595.294464
Epoch 11: training loss 131075694260.706
Test Loss of 137066269686.288223, Test MSE of 137066268020.500778
Epoch 12: training loss 127405454546.824
Test Loss of 133139156348.654175, Test MSE of 133139155122.548782
Epoch 13: training loss 124426653635.765
Test Loss of 129790437823.689102, Test MSE of 129790438105.555664
Epoch 14: training loss 119769583826.824
Test Loss of 126367462322.542679, Test MSE of 126367463177.501556
Epoch 15: training loss 116182961121.882
Test Loss of 123449543973.485077, Test MSE of 123449543967.271225
Epoch 16: training loss 112661322631.529
Test Loss of 118934700376.886429, Test MSE of 118934701783.848541
Epoch 17: training loss 108766686087.529
Test Loss of 113536246068.881790, Test MSE of 113536244873.663467
Epoch 18: training loss 103621789786.353
Test Loss of 111093250390.517700, Test MSE of 111093252253.380676
Epoch 19: training loss 101995051881.412
Test Loss of 107551582928.566269, Test MSE of 107551581215.842957
Epoch 20: training loss 97207940698.353
Test Loss of 102148912819.904694, Test MSE of 102148915592.518051
Epoch 21: training loss 94411602718.118
Test Loss of 96089854549.866302, Test MSE of 96089854817.474274
Epoch 22: training loss 89783957564.235
Test Loss of 94349455679.304184, Test MSE of 94349456964.077774
Epoch 23: training loss 85287702091.294
Test Loss of 92712319706.988663, Test MSE of 92712319130.628616
Epoch 24: training loss 81870319164.235
Test Loss of 86766139173.603516, Test MSE of 86766138817.605103
Epoch 25: training loss 78521730078.118
Test Loss of 83678671978.355774, Test MSE of 83678670569.931396
Epoch 26: training loss 74486481438.118
Test Loss of 77906189627.277359, Test MSE of 77906190060.673172
Epoch 27: training loss 71308836758.588
Test Loss of 74046778007.716858, Test MSE of 74046778973.179901
Epoch 28: training loss 67929014287.059
Test Loss of 70479507727.929672, Test MSE of 70479506113.566818
Epoch 29: training loss 64977736372.706
Test Loss of 68348930779.936157, Test MSE of 68348930254.723213
Epoch 30: training loss 61413658503.529
Test Loss of 63390697120.007401, Test MSE of 63390698909.460579
Epoch 31: training loss 58502724713.412
Test Loss of 65112901099.984268, Test MSE of 65112900418.673302
Epoch 32: training loss 55171970439.529
Test Loss of 60150587435.584549, Test MSE of 60150586659.221420
Epoch 33: training loss 52930966452.706
Test Loss of 57818868478.045799, Test MSE of 57818868273.769211
Epoch 34: training loss 50252804201.412
Test Loss of 52934997498.196625, Test MSE of 52934997126.943130
Epoch 35: training loss 48072715429.647
Test Loss of 50594618298.596344, Test MSE of 50594618317.737930
Epoch 36: training loss 44914654863.059
Test Loss of 49902287970.302109, Test MSE of 49902288087.831223
Epoch 37: training loss 42882805970.824
Test Loss of 44768126752.866066, Test MSE of 44768126402.248901
Epoch 38: training loss 40723279292.235
Test Loss of 45936861058.931297, Test MSE of 45936860425.128876
Epoch 39: training loss 38330754928.941
Test Loss of 41037746857.245430, Test MSE of 41037748140.018494
Epoch 40: training loss 36049953972.706
Test Loss of 41835343570.698128, Test MSE of 41835343807.124847
Epoch 41: training loss 34363905091.765
Test Loss of 39363217465.323158, Test MSE of 39363217527.322273
Epoch 42: training loss 32393331855.059
Test Loss of 37643353469.601669, Test MSE of 37643353017.566635
Epoch 43: training loss 30521089626.353
Test Loss of 36820192327.772377, Test MSE of 36820191952.548996
Epoch 44: training loss 29181769547.294
Test Loss of 31484541117.971779, Test MSE of 31484541355.130768
Epoch 45: training loss 27810768045.176
Test Loss of 33987738877.690495, Test MSE of 33987739237.616238
Epoch 46: training loss 26589863529.412
Test Loss of 29046676763.536434, Test MSE of 29046676353.588573
Epoch 47: training loss 25532445880.471
Test Loss of 29739164060.158222, Test MSE of 29739163782.871025
Epoch 48: training loss 24551729547.294
Test Loss of 32003884212.260006, Test MSE of 32003883765.504345
Epoch 49: training loss 23073724525.176
Test Loss of 29289046367.992599, Test MSE of 29289046292.068485
Epoch 50: training loss 22438306563.765
Test Loss of 27119125650.624104, Test MSE of 27119125646.863796
Epoch 51: training loss 21453691836.235
Test Loss of 28649312238.945175, Test MSE of 28649312410.357754
Epoch 52: training loss 20406132717.176
Test Loss of 25009627345.395329, Test MSE of 25009628049.166927
Epoch 53: training loss 19719322541.176
Test Loss of 27479936050.453850, Test MSE of 27479936521.195896
Epoch 54: training loss 19097448816.941
Test Loss of 25435248094.719406, Test MSE of 25435248370.104660
Epoch 55: training loss 18537966110.118
Test Loss of 24161860933.226002, Test MSE of 24161860907.451061
Epoch 56: training loss 17442216670.118
Test Loss of 26787224706.990517, Test MSE of 26787225002.599323
Epoch 57: training loss 17162741824.000
Test Loss of 24310300186.885033, Test MSE of 24310300127.060005
Epoch 58: training loss 16534284306.824
Test Loss of 24573893593.389774, Test MSE of 24573893719.978683
Epoch 59: training loss 15853959988.706
Test Loss of 23731103617.983807, Test MSE of 23731103461.137676
Epoch 60: training loss 15889351785.412
Test Loss of 24509815290.433495, Test MSE of 24509815214.996681
Epoch 61: training loss 15158528576.000
Test Loss of 22788870228.326626, Test MSE of 22788870612.821274
Epoch 62: training loss 14620714096.941
Test Loss of 21493677782.014343, Test MSE of 21493677299.419155
Epoch 63: training loss 14312075497.412
Test Loss of 25810847873.806152, Test MSE of 25810847363.747643
Epoch 64: training loss 13964729987.765
Test Loss of 26415525372.565350, Test MSE of 26415525590.889668
Epoch 65: training loss 13422304301.176
Test Loss of 23342438586.655563, Test MSE of 23342438752.102886
Epoch 66: training loss 13066311514.353
Test Loss of 23818683397.448067, Test MSE of 23818683210.065235
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22639043435.52658, 'MSE - std': 2209359910.383551, 'R2 - mean': 0.8379823902551108, 'R2 - std': 0.00974931034372774} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005313 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110628442.353
Test Loss of 410764870102.063843, Test MSE of 410764868595.744690
Epoch 2: training loss 430089869914.353
Test Loss of 410746708942.245239, Test MSE of 410746706068.791504
Epoch 3: training loss 430063024971.294
Test Loss of 410723254605.593689, Test MSE of 410723248235.718872
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077566976.000
Test Loss of 410725094208.088867, Test MSE of 410725096485.244568
Epoch 2: training loss 430066439830.588
Test Loss of 410726335540.597900, Test MSE of 410726333444.468872
Epoch 3: training loss 430066017219.765
Test Loss of 410725918321.014343, Test MSE of 410725916794.409912
Epoch 4: training loss 430065703996.235
Test Loss of 410725709597.497437, Test MSE of 410725706043.064758
Epoch 5: training loss 423129783838.118
Test Loss of 389030099741.023621, Test MSE of 389030108277.344604
Epoch 6: training loss 377649020807.529
Test Loss of 321901493832.262817, Test MSE of 321901491149.245605
Epoch 7: training loss 298864435200.000
Test Loss of 237682359106.931976, Test MSE of 237682354370.343903
Epoch 8: training loss 220450417121.882
Test Loss of 167416592392.529388, Test MSE of 167416592965.571472
Epoch 9: training loss 161206597842.824
Test Loss of 119524094825.788055, Test MSE of 119524094250.164703
Epoch 10: training loss 141531006825.412
Test Loss of 111973241853.630722, Test MSE of 111973241333.883133
Epoch 11: training loss 137513751431.529
Test Loss of 109189299242.173065, Test MSE of 109189300082.912048
Epoch 12: training loss 134194970804.706
Test Loss of 106781646988.260986, Test MSE of 106781645741.430191
Epoch 13: training loss 131264489441.882
Test Loss of 103936061449.477097, Test MSE of 103936061592.400299
Epoch 14: training loss 128532152922.353
Test Loss of 100636410432.207306, Test MSE of 100636411606.755630
Epoch 15: training loss 123161911265.882
Test Loss of 97784744306.554367, Test MSE of 97784743943.999542
Epoch 16: training loss 120700022693.647
Test Loss of 95335019441.813980, Test MSE of 95335019957.652649
Epoch 17: training loss 115523577856.000
Test Loss of 91193944775.729752, Test MSE of 91193946968.113068
Epoch 18: training loss 112120714992.941
Test Loss of 87931759716.931046, Test MSE of 87931760218.888351
Epoch 19: training loss 107624440892.235
Test Loss of 85141139173.582596, Test MSE of 85141138358.710632
Epoch 20: training loss 104077171260.235
Test Loss of 82134347876.457199, Test MSE of 82134345896.201904
Epoch 21: training loss 100214621214.118
Test Loss of 79687236616.055527, Test MSE of 79687236267.447876
Epoch 22: training loss 96253772438.588
Test Loss of 75221591122.924576, Test MSE of 75221591198.655197
Epoch 23: training loss 91631780683.294
Test Loss of 72710127051.165207, Test MSE of 72710127659.432587
Epoch 24: training loss 88353152210.824
Test Loss of 71519092061.230911, Test MSE of 71519091263.408752
Epoch 25: training loss 84816782938.353
Test Loss of 65350752499.561317, Test MSE of 65350753520.064293
Epoch 26: training loss 80952890684.235
Test Loss of 63630298061.297546, Test MSE of 63630297372.022041
Epoch 27: training loss 77239030422.588
Test Loss of 60378901157.612213, Test MSE of 60378902144.860123
Epoch 28: training loss 74121446565.647
Test Loss of 58046350081.540031, Test MSE of 58046351333.141960
Epoch 29: training loss 71258961317.647
Test Loss of 58039507024.555298, Test MSE of 58039505494.993591
Epoch 30: training loss 67721192764.235
Test Loss of 54928888471.396576, Test MSE of 54928888877.828598
Epoch 31: training loss 64610724540.235
Test Loss of 52079176357.612213, Test MSE of 52079176029.866486
Epoch 32: training loss 61520552071.529
Test Loss of 48555189476.871819, Test MSE of 48555189757.264359
Epoch 33: training loss 58797899730.824
Test Loss of 48019941515.313278, Test MSE of 48019942992.248695
Epoch 34: training loss 54943920820.706
Test Loss of 44134478697.788063, Test MSE of 44134478246.995972
Epoch 35: training loss 52243647947.294
Test Loss of 41666854320.629341, Test MSE of 41666854939.310448
Epoch 36: training loss 50814031480.471
Test Loss of 42415461993.432671, Test MSE of 42415462831.267212
Epoch 37: training loss 47449966637.176
Test Loss of 40214642887.018974, Test MSE of 40214642884.934326
Epoch 38: training loss 45546769310.118
Test Loss of 37936824177.843590, Test MSE of 37936824329.940742
Epoch 39: training loss 42578020555.294
Test Loss of 34044003905.628876, Test MSE of 34044003967.062672
Epoch 40: training loss 40916606885.647
Test Loss of 33918699022.452568, Test MSE of 33918699025.609165
Epoch 41: training loss 38499752967.529
Test Loss of 29859162383.992596, Test MSE of 29859162088.699707
Epoch 42: training loss 37109372867.765
Test Loss of 31760444024.122166, Test MSE of 31760444218.013111
Epoch 43: training loss 34899622686.118
Test Loss of 33324673457.103191, Test MSE of 33324672918.420044
Epoch 44: training loss 33013649807.059
Test Loss of 30425144153.677002, Test MSE of 30425143578.325012
Epoch 45: training loss 31673280775.529
Test Loss of 28489181215.274410, Test MSE of 28489180924.622646
Epoch 46: training loss 29922767796.706
Test Loss of 28060725090.680241, Test MSE of 28060724858.965660
Epoch 47: training loss 28376858804.706
Test Loss of 25526423282.850533, Test MSE of 25526423303.786015
Epoch 48: training loss 27277400402.824
Test Loss of 25200562235.705692, Test MSE of 25200562615.074192
Epoch 49: training loss 25870035885.176
Test Loss of 25249346628.708931, Test MSE of 25249346348.572994
Epoch 50: training loss 25145601739.294
Test Loss of 24232367197.349377, Test MSE of 24232366783.226341
Epoch 51: training loss 23726797327.059
Test Loss of 24277700660.124016, Test MSE of 24277700309.312668
Epoch 52: training loss 23322726339.765
Test Loss of 25623515823.089310, Test MSE of 25623515372.893188
Epoch 53: training loss 22215318599.529
Test Loss of 21029179060.301712, Test MSE of 21029179017.172237
Epoch 54: training loss 21196005861.647
Test Loss of 20778395958.848682, Test MSE of 20778396218.647312
Epoch 55: training loss 20724313110.588
Test Loss of 22434500447.837112, Test MSE of 22434500502.428310
Epoch 56: training loss 19888222972.235
Test Loss of 22896219723.105968, Test MSE of 22896219788.954967
Epoch 57: training loss 19271442247.529
Test Loss of 23163007071.718647, Test MSE of 23163007702.378704
Epoch 58: training loss 18766464527.059
Test Loss of 21757248362.735771, Test MSE of 21757248236.298401
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22418594635.719536, 'MSE - std': 1951088524.4438834, 'R2 - mean': 0.8335933155112276, 'R2 - std': 0.011361281637910293} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005325 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043446753.882
Test Loss of 431612906858.498840, Test MSE of 431612908164.329102
Epoch 2: training loss 424023886426.353
Test Loss of 431592389238.226746, Test MSE of 431592390543.084412
Epoch 3: training loss 423996742475.294
Test Loss of 431565034387.487305, Test MSE of 431565033789.026611
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011069680.941
Test Loss of 431567991437.919495, Test MSE of 431567998594.729065
Epoch 2: training loss 423999252359.529
Test Loss of 431569909222.648804, Test MSE of 431569918056.277161
Epoch 3: training loss 423998663258.353
Test Loss of 431569801077.634399, Test MSE of 431569811009.635376
Epoch 4: training loss 423998264500.706
Test Loss of 431568208895.526123, Test MSE of 431568205436.855530
Epoch 5: training loss 417389252848.941
Test Loss of 409985949373.778809, Test MSE of 409985945692.889465
Epoch 6: training loss 372755479732.706
Test Loss of 343004009227.017151, Test MSE of 343004011352.130859
Epoch 7: training loss 294496698247.529
Test Loss of 255681084295.167053, Test MSE of 255681086530.498383
Epoch 8: training loss 215462344764.235
Test Loss of 183887792240.777405, Test MSE of 183887794257.910583
Epoch 9: training loss 156921883105.882
Test Loss of 131637903052.942154, Test MSE of 131637905156.984909
Epoch 10: training loss 137792372705.882
Test Loss of 124307701214.119385, Test MSE of 124307699269.192307
Epoch 11: training loss 136445378409.412
Test Loss of 121012447212.098099, Test MSE of 121012446511.467377
Epoch 12: training loss 131062830531.765
Test Loss of 117399918293.471542, Test MSE of 117399919369.614212
Epoch 13: training loss 128768229978.353
Test Loss of 114141576848.288757, Test MSE of 114141578224.514252
Epoch 14: training loss 125860108619.294
Test Loss of 109890657386.617310, Test MSE of 109890656480.619843
Epoch 15: training loss 119814141259.294
Test Loss of 106408066958.274872, Test MSE of 106408069151.521378
Epoch 16: training loss 116159079815.529
Test Loss of 102835005238.137894, Test MSE of 102835004528.034210
Epoch 17: training loss 112649859990.588
Test Loss of 98402383861.575195, Test MSE of 98402382894.901993
Epoch 18: training loss 109445135224.471
Test Loss of 97432730336.370193, Test MSE of 97432728825.027405
Epoch 19: training loss 105090861854.118
Test Loss of 93044160903.403976, Test MSE of 93044160209.331879
Epoch 20: training loss 100851323271.529
Test Loss of 87758564111.755676, Test MSE of 87758565158.073410
Epoch 21: training loss 96857085018.353
Test Loss of 83046119822.985657, Test MSE of 83046119510.785919
Epoch 22: training loss 93053540487.529
Test Loss of 81717025420.971771, Test MSE of 81717026318.912506
Epoch 23: training loss 89514794767.059
Test Loss of 75356642025.847290, Test MSE of 75356642692.128067
Epoch 24: training loss 85806618834.824
Test Loss of 75307582717.512268, Test MSE of 75307582797.047699
Epoch 25: training loss 81700844694.588
Test Loss of 69962786017.080978, Test MSE of 69962787548.724808
Epoch 26: training loss 79218918053.647
Test Loss of 71486196836.931046, Test MSE of 71486197155.091797
Epoch 27: training loss 75279664986.353
Test Loss of 66027789909.530769, Test MSE of 66027789538.893074
Epoch 28: training loss 71679939132.235
Test Loss of 63334560994.976402, Test MSE of 63334559998.129219
Epoch 29: training loss 69166525680.941
Test Loss of 60612759074.354469, Test MSE of 60612757178.774796
Epoch 30: training loss 65473362710.588
Test Loss of 56692724432.259140, Test MSE of 56692724242.710365
Epoch 31: training loss 62453124969.412
Test Loss of 55239107384.507172, Test MSE of 55239107832.666115
Epoch 32: training loss 59668250639.059
Test Loss of 49803441462.374825, Test MSE of 49803440576.073387
Epoch 33: training loss 57068994379.294
Test Loss of 49541835784.055527, Test MSE of 49541835824.085815
Epoch 34: training loss 53416306733.176
Test Loss of 44618525179.024529, Test MSE of 44618523747.946869
Epoch 35: training loss 50925597906.824
Test Loss of 45272460335.859322, Test MSE of 45272460695.406845
Epoch 36: training loss 48602018409.412
Test Loss of 41536057165.830635, Test MSE of 41536057818.052551
Epoch 37: training loss 46062470821.647
Test Loss of 36187109245.216103, Test MSE of 36187109182.409836
Epoch 38: training loss 44178456741.647
Test Loss of 36383448195.257751, Test MSE of 36383448842.361481
Epoch 39: training loss 41982399638.588
Test Loss of 38434097151.052292, Test MSE of 38434097396.159515
Epoch 40: training loss 40118569208.471
Test Loss of 34563124397.904671, Test MSE of 34563124015.724564
Epoch 41: training loss 37911953874.824
Test Loss of 35965685324.053680, Test MSE of 35965685620.051918
Epoch 42: training loss 36342796190.118
Test Loss of 31696466895.192966, Test MSE of 31696467339.510330
Epoch 43: training loss 33523135382.588
Test Loss of 30650515118.615456, Test MSE of 30650515483.278286
Epoch 44: training loss 32502451892.706
Test Loss of 25705408790.626560, Test MSE of 25705408634.854404
Epoch 45: training loss 31296557372.235
Test Loss of 26419582073.780659, Test MSE of 26419581623.356789
Epoch 46: training loss 29467567013.647
Test Loss of 25935027665.799168, Test MSE of 25935027479.072544
Epoch 47: training loss 28075035090.824
Test Loss of 27593971536.673763, Test MSE of 27593971136.638954
Epoch 48: training loss 27375336681.412
Test Loss of 24970714326.656178, Test MSE of 24970713769.804951
Epoch 49: training loss 25690099610.353
Test Loss of 21790053094.056454, Test MSE of 21790053649.416393
Epoch 50: training loss 24678894516.706
Test Loss of 22180192641.717724, Test MSE of 22180192916.490093
Epoch 51: training loss 23943521193.412
Test Loss of 23465294879.748264, Test MSE of 23465294759.468655
Epoch 52: training loss 23091389368.471
Test Loss of 21367162431.733456, Test MSE of 21367162593.009518
Epoch 53: training loss 21591129155.765
Test Loss of 22187701576.381306, Test MSE of 22187701444.480476
Epoch 54: training loss 21364413349.647
Test Loss of 21699612663.470615, Test MSE of 21699612687.146763
Epoch 55: training loss 20714158012.235
Test Loss of 23663356524.749653, Test MSE of 23663356769.988064
Epoch 56: training loss 20068440225.882
Test Loss of 22028461412.812588, Test MSE of 22028461690.204845
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22340568046.6166, 'MSE - std': 1752070129.4758275, 'R2 - mean': 0.8339727784573048, 'R2 - std': 0.01019013959053146} 
 

Saving model.....
Results After CV: {'MSE - mean': 22340568046.6166, 'MSE - std': 1752070129.4758275, 'R2 - mean': 0.8339727784573048, 'R2 - std': 0.01019013959053146}
Train time: 91.15885063120004
Inference time: 0.07075891559943556
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 47 finished with value: 22340568046.6166 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005539 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427523759887.059
Test Loss of 418110923576.316467, Test MSE of 418110923815.818054
Epoch 2: training loss 427501595587.765
Test Loss of 418091740303.781616, Test MSE of 418091752910.350586
Epoch 3: training loss 427473560154.353
Test Loss of 418067694041.745056, Test MSE of 418067691104.969482
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494230497.882
Test Loss of 418069439925.029846, Test MSE of 418069445571.368530
Epoch 2: training loss 427480128451.765
Test Loss of 418071333366.643555, Test MSE of 418071326524.387756
Epoch 3: training loss 427479724995.765
Test Loss of 418070398512.203552, Test MSE of 418070404240.376099
Epoch 4: training loss 427479420446.118
Test Loss of 418069814001.728455, Test MSE of 418069819704.560120
Epoch 5: training loss 427479193600.000
Test Loss of 418068187684.123047, Test MSE of 418068189526.743103
Epoch 6: training loss 419174113400.471
Test Loss of 391580776585.859802, Test MSE of 391580776973.355774
Epoch 7: training loss 365448959879.529
Test Loss of 313694710226.638916, Test MSE of 313694706883.762573
Epoch 8: training loss 278783035934.118
Test Loss of 224806478834.024506, Test MSE of 224806479305.273071
Epoch 9: training loss 199608305423.059
Test Loss of 159485520361.378662, Test MSE of 159485522507.938416
Epoch 10: training loss 156704656730.353
Test Loss of 129745871846.654633, Test MSE of 129745871291.908218
Epoch 11: training loss 140428827738.353
Test Loss of 117541074735.078415, Test MSE of 117541074485.660004
Epoch 12: training loss 134503586032.941
Test Loss of 114556531960.479294, Test MSE of 114556533488.193375
Epoch 13: training loss 130923919947.294
Test Loss of 111129683773.290771, Test MSE of 111129684702.878250
Epoch 14: training loss 127741721961.412
Test Loss of 108466742597.225998, Test MSE of 108466741600.768677
Epoch 15: training loss 124689416222.118
Test Loss of 104734150205.942169, Test MSE of 104734151357.800247
Epoch 16: training loss 120774202172.235
Test Loss of 101426628589.523941, Test MSE of 101426629974.169571
Epoch 17: training loss 116802354868.706
Test Loss of 97298140269.671982, Test MSE of 97298138725.370636
Epoch 18: training loss 111849927198.118
Test Loss of 93008296439.591019, Test MSE of 93008297052.722366
Epoch 19: training loss 108715710177.882
Test Loss of 90709835122.231781, Test MSE of 90709836083.890472
Epoch 20: training loss 105225534132.706
Test Loss of 88672109065.119598, Test MSE of 88672107621.337830
Epoch 21: training loss 100743319055.059
Test Loss of 84503930507.162613, Test MSE of 84503930030.816025
Epoch 22: training loss 96100608662.588
Test Loss of 79726462246.906311, Test MSE of 79726462821.799301
Epoch 23: training loss 91840100773.647
Test Loss of 77501942076.935455, Test MSE of 77501942013.317566
Epoch 24: training loss 88885635102.118
Test Loss of 75579301095.898224, Test MSE of 75579302793.232437
Epoch 25: training loss 84607219907.765
Test Loss of 71159245410.657410, Test MSE of 71159246746.955338
Epoch 26: training loss 81622964645.647
Test Loss of 69046050901.274109, Test MSE of 69046051474.595642
Epoch 27: training loss 77800159789.176
Test Loss of 62730299894.643532, Test MSE of 62730299938.436256
Epoch 28: training loss 73956819870.118
Test Loss of 64637793663.022903, Test MSE of 64637793330.701309
Epoch 29: training loss 71091419075.765
Test Loss of 56259367693.679390, Test MSE of 56259367298.541985
Epoch 30: training loss 67647536075.294
Test Loss of 57164922618.492714, Test MSE of 57164923061.366455
Epoch 31: training loss 64499946006.588
Test Loss of 54396344648.305344, Test MSE of 54396343938.130577
Epoch 32: training loss 61457964423.529
Test Loss of 51696302134.480682, Test MSE of 51696302351.391289
Epoch 33: training loss 57624130698.353
Test Loss of 48328745005.479530, Test MSE of 48328745815.823784
Epoch 34: training loss 55224627501.176
Test Loss of 46395688051.830673, Test MSE of 46395688737.596245
Epoch 35: training loss 52510121615.059
Test Loss of 43498985291.503120, Test MSE of 43498985625.740227
Epoch 36: training loss 49564466631.529
Test Loss of 40473045306.092995, Test MSE of 40473045840.569397
Epoch 37: training loss 47302622878.118
Test Loss of 38109301884.831833, Test MSE of 38109301612.861595
Epoch 38: training loss 44785181959.529
Test Loss of 37214870727.683556, Test MSE of 37214871565.597061
Epoch 39: training loss 42329056587.294
Test Loss of 35085690659.708534, Test MSE of 35085690559.374901
Epoch 40: training loss 39913161626.353
Test Loss of 32484953425.069626, Test MSE of 32484953239.246120
Epoch 41: training loss 38803986115.765
Test Loss of 33073297853.794125, Test MSE of 33073297728.786095
Epoch 42: training loss 36036527250.824
Test Loss of 29529517516.243351, Test MSE of 29529517990.470390
Epoch 43: training loss 34197419911.529
Test Loss of 29243105836.176727, Test MSE of 29243105697.792484
Epoch 44: training loss 32699046132.706
Test Loss of 27787027610.677769, Test MSE of 27787028039.151436
Epoch 45: training loss 31280729174.588
Test Loss of 27826265901.657181, Test MSE of 27826266182.601364
Epoch 46: training loss 29838709330.824
Test Loss of 25985626536.475597, Test MSE of 25985626072.230091
Epoch 47: training loss 28354905889.882
Test Loss of 23325723963.751099, Test MSE of 23325724157.061432
Epoch 48: training loss 27034149511.529
Test Loss of 24352406287.100624, Test MSE of 24352406400.479950
Epoch 49: training loss 25581565315.765
Test Loss of 24646779110.240112, Test MSE of 24646778748.975597
Epoch 50: training loss 24288361483.294
Test Loss of 23890539292.839233, Test MSE of 23890539663.386169
Epoch 51: training loss 23679128353.882
Test Loss of 20920138783.504047, Test MSE of 20920138736.717754
Epoch 52: training loss 22593600512.000
Test Loss of 20147787217.454544, Test MSE of 20147786881.267254
Epoch 53: training loss 21564174464.000
Test Loss of 21305799206.018044, Test MSE of 21305799303.255451
Epoch 54: training loss 20687705001.412
Test Loss of 21977111327.681702, Test MSE of 21977111445.497513
Epoch 55: training loss 19877747779.765
Test Loss of 20371906617.323154, Test MSE of 20371906652.731354
Epoch 56: training loss 19299948653.176
Test Loss of 20316647344.884571, Test MSE of 20316647788.554722
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20316647788.55472, 'MSE - std': 0.0, 'R2 - mean': 0.8417920593310069, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005348 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427917082142.118
Test Loss of 424556187732.326599, Test MSE of 424556192472.103149
Epoch 2: training loss 427895547301.647
Test Loss of 424539043307.036804, Test MSE of 424539045855.258240
Epoch 3: training loss 427867774012.235
Test Loss of 424516187539.867676, Test MSE of 424516183224.133606
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885999284.706
Test Loss of 424518416461.220459, Test MSE of 424518418576.359924
Epoch 2: training loss 427875579783.529
Test Loss of 424519712468.593079, Test MSE of 424519708801.158325
Epoch 3: training loss 427875015740.235
Test Loss of 424519208434.142944, Test MSE of 424519205273.800171
Epoch 4: training loss 427874627102.118
Test Loss of 424518905070.530640, Test MSE of 424518896492.942261
Epoch 5: training loss 427874323034.353
Test Loss of 424518397261.279663, Test MSE of 424518395315.257568
Epoch 6: training loss 419471042439.529
Test Loss of 398075909279.415222, Test MSE of 398075901833.550537
Epoch 7: training loss 365573717413.647
Test Loss of 321644017259.895447, Test MSE of 321644021019.873108
Epoch 8: training loss 277521377882.353
Test Loss of 232813726888.416382, Test MSE of 232813725583.382843
Epoch 9: training loss 198804787380.706
Test Loss of 169544927924.141571, Test MSE of 169544929751.873932
Epoch 10: training loss 156126317688.471
Test Loss of 141429778764.095306, Test MSE of 141429781017.685638
Epoch 11: training loss 137939964235.294
Test Loss of 129948047747.997223, Test MSE of 129948047106.577133
Epoch 12: training loss 132522866808.471
Test Loss of 126781262807.968536, Test MSE of 126781263902.671829
Epoch 13: training loss 129691475937.882
Test Loss of 123313204252.187836, Test MSE of 123313203949.243896
Epoch 14: training loss 126748135664.941
Test Loss of 121181161843.889893, Test MSE of 121181162842.332291
Epoch 15: training loss 123168212389.647
Test Loss of 117256527337.615540, Test MSE of 117256530354.070206
Epoch 16: training loss 119219433712.941
Test Loss of 114085640754.335419, Test MSE of 114085639269.851761
Epoch 17: training loss 115264144624.941
Test Loss of 110787495485.468430, Test MSE of 110787493882.368668
Epoch 18: training loss 110812983536.941
Test Loss of 107156064328.483002, Test MSE of 107156064934.124832
Epoch 19: training loss 107789453402.353
Test Loss of 103068811407.071014, Test MSE of 103068812416.735825
Epoch 20: training loss 102763171011.765
Test Loss of 98883530198.665741, Test MSE of 98883532258.734329
Epoch 21: training loss 99560732732.235
Test Loss of 95900894872.427475, Test MSE of 95900894313.053391
Epoch 22: training loss 96023472489.412
Test Loss of 93445292456.475601, Test MSE of 93445293773.227188
Epoch 23: training loss 90716491474.824
Test Loss of 88256468235.429108, Test MSE of 88256469003.759140
Epoch 24: training loss 88283898819.765
Test Loss of 83705136279.835297, Test MSE of 83705135677.599014
Epoch 25: training loss 84964993626.353
Test Loss of 82853201632.436737, Test MSE of 82853202063.585205
Epoch 26: training loss 79468531772.235
Test Loss of 78742956530.853577, Test MSE of 78742956328.612656
Epoch 27: training loss 75828068653.176
Test Loss of 76143499851.917648, Test MSE of 76143498903.734604
Epoch 28: training loss 73322770191.059
Test Loss of 69352696483.086746, Test MSE of 69352696883.441162
Epoch 29: training loss 69131455638.588
Test Loss of 67214072560.780937, Test MSE of 67214073670.403435
Epoch 30: training loss 67776946251.294
Test Loss of 64709389661.387001, Test MSE of 64709390594.406647
Epoch 31: training loss 63078613775.059
Test Loss of 62837759814.291924, Test MSE of 62837760158.435081
Epoch 32: training loss 60063451843.765
Test Loss of 60383784660.593109, Test MSE of 60383786151.288460
Epoch 33: training loss 56266839928.471
Test Loss of 57140513743.914871, Test MSE of 57140513728.642555
Epoch 34: training loss 53571821620.706
Test Loss of 53643251788.272957, Test MSE of 53643252671.506828
Epoch 35: training loss 50956119973.647
Test Loss of 53039648219.403191, Test MSE of 53039649319.970375
Epoch 36: training loss 48552143984.941
Test Loss of 49978832716.450615, Test MSE of 49978834242.212151
Epoch 37: training loss 44985729852.235
Test Loss of 46090513447.083969, Test MSE of 46090512896.223579
Epoch 38: training loss 42363092818.824
Test Loss of 46480099020.065697, Test MSE of 46480098422.098816
Epoch 39: training loss 40990327544.471
Test Loss of 41551874821.625725, Test MSE of 41551874069.209770
Epoch 40: training loss 39507222663.529
Test Loss of 39159148052.726349, Test MSE of 39159148339.492409
Epoch 41: training loss 36921646509.176
Test Loss of 38892166729.785797, Test MSE of 38892167192.870247
Epoch 42: training loss 34490668220.235
Test Loss of 39373679208.816101, Test MSE of 39373679132.073082
Epoch 43: training loss 32807174031.059
Test Loss of 37643853538.331711, Test MSE of 37643851997.886169
Epoch 44: training loss 30680671382.588
Test Loss of 33479868488.009254, Test MSE of 33479868436.499596
Epoch 45: training loss 29328337641.412
Test Loss of 37077340992.370110, Test MSE of 37077341608.200920
Epoch 46: training loss 27635994096.941
Test Loss of 32158211443.653019, Test MSE of 32158211535.645287
Epoch 47: training loss 26247935721.412
Test Loss of 31398939518.430721, Test MSE of 31398940055.785675
Epoch 48: training loss 25252148190.118
Test Loss of 31626189747.963913, Test MSE of 31626188649.366837
Epoch 49: training loss 23680317854.118
Test Loss of 31973737148.195236, Test MSE of 31973737791.787926
Epoch 50: training loss 22815002300.235
Test Loss of 32073479713.043720, Test MSE of 32073478883.291531
Epoch 51: training loss 21515701391.059
Test Loss of 29821263800.938236, Test MSE of 29821263715.206978
Epoch 52: training loss 21035827606.588
Test Loss of 30514196399.937080, Test MSE of 30514196756.180576
Epoch 53: training loss 19501741891.765
Test Loss of 29499292240.181355, Test MSE of 29499292276.415272
Epoch 54: training loss 18837208594.824
Test Loss of 28742827808.866066, Test MSE of 28742828587.446911
Epoch 55: training loss 18071692916.706
Test Loss of 27675843944.756882, Test MSE of 27675843120.181965
Epoch 56: training loss 17644649054.118
Test Loss of 30385764822.902615, Test MSE of 30385765276.602779
Epoch 57: training loss 16740788871.529
Test Loss of 26899864746.074486, Test MSE of 26899864172.977879
Epoch 58: training loss 16075204901.647
Test Loss of 28118028484.604210, Test MSE of 28118029378.961426
Epoch 59: training loss 15717543198.118
Test Loss of 26855295991.709461, Test MSE of 26855295939.400173
Epoch 60: training loss 15069564574.118
Test Loss of 28675203682.657413, Test MSE of 28675202229.094002
Epoch 61: training loss 14273658752.000
Test Loss of 25433242891.902843, Test MSE of 25433242736.758335
Epoch 62: training loss 14105532137.412
Test Loss of 25496186977.354614, Test MSE of 25496186997.522064
Epoch 63: training loss 13665326957.176
Test Loss of 28375062411.221836, Test MSE of 28375062345.058704
Epoch 64: training loss 13447187162.353
Test Loss of 27227776559.729816, Test MSE of 27227776726.499149
Epoch 65: training loss 13104756148.706
Test Loss of 27207283406.908165, Test MSE of 27207283122.263382
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23761965455.40905, 'MSE - std': 3445317666.85433, 'R2 - mean': 0.8237750607626265, 'R2 - std': 0.01801699856838035} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005298 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926576730.353
Test Loss of 447256773377.362000, Test MSE of 447256767167.361755
Epoch 2: training loss 421905530759.529
Test Loss of 447238172841.600769, Test MSE of 447238168011.911316
Epoch 3: training loss 421878668348.235
Test Loss of 447213680705.850586, Test MSE of 447213673460.179016
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897681980.235
Test Loss of 447220304814.278992, Test MSE of 447220305947.642456
Epoch 2: training loss 421886315459.765
Test Loss of 447221745241.893127, Test MSE of 447221757693.068359
Epoch 3: training loss 421885878994.824
Test Loss of 447222228962.154053, Test MSE of 447222222219.696655
Epoch 4: training loss 421885555772.235
Test Loss of 447222535297.095520, Test MSE of 447222537534.323669
Epoch 5: training loss 421885363621.647
Test Loss of 447222531416.175781, Test MSE of 447222524848.301880
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 164915485253.04, 'MSE - std': 199641042319.46915, 'R2 - mean': -0.10986023743669689, 'R2 - std': 1.3204416489363529} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005318 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106513889.882
Test Loss of 410761814004.627502, Test MSE of 410761808691.030762
Epoch 2: training loss 430084242733.176
Test Loss of 410742924518.767212, Test MSE of 410742924548.228943
Epoch 3: training loss 430057969543.529
Test Loss of 410720074729.728821, Test MSE of 410720077915.234741
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430072395414.588
Test Loss of 410721158272.414612, Test MSE of 410721151309.153931
Epoch 2: training loss 430062404547.765
Test Loss of 410722058702.482178, Test MSE of 410722056590.075562
Epoch 3: training loss 430062112768.000
Test Loss of 410722347442.998596, Test MSE of 410722355544.596802
Epoch 4: training loss 430061925556.706
Test Loss of 410722523929.232788, Test MSE of 410722519360.549255
Epoch 5: training loss 430061769426.824
Test Loss of 410722456315.853760, Test MSE of 410722462383.062622
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 226367229535.54565, 'MSE - std': 203030440968.56622, 'R2 - mean': -0.679871811707538, 'R2 - std': 1.510766095592724} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005330 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043282311.529
Test Loss of 431612750327.233704, Test MSE of 431612747610.378784
Epoch 2: training loss 424024642258.824
Test Loss of 431592662936.699646, Test MSE of 431592668479.907532
Epoch 3: training loss 423998401114.353
Test Loss of 431565421583.637207, Test MSE of 431565419795.733582
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424014182038.588
Test Loss of 431566263104.088867, Test MSE of 431566264944.715454
Epoch 2: training loss 423999791826.824
Test Loss of 431569778514.569153, Test MSE of 431569782470.900269
Epoch 3: training loss 423999143454.118
Test Loss of 431569815816.410950, Test MSE of 431569819592.243530
Epoch 4: training loss 423998733854.118
Test Loss of 431569384798.178650, Test MSE of 431569383667.452209
Epoch 5: training loss 423998464361.412
Test Loss of 431568825892.249878, Test MSE of 431568829695.121460
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 267407549567.46085, 'MSE - std': 199284518819.70267, 'R2 - mean': -0.9884918579903801, 'R2 - std': 1.4855694828396555} 
 

Saving model.....
Results After CV: {'MSE - mean': 267407549567.46085, 'MSE - std': 199284518819.70267, 'R2 - mean': -0.9884918579903801, 'R2 - std': 1.4855694828396555}
Train time: 45.10738119039888
Inference time: 0.06974003860013908
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 48 finished with value: 267407549567.46085 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005320 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525582968.471
Test Loss of 418110999390.689819, Test MSE of 418110997177.907471
Epoch 2: training loss 427505096463.059
Test Loss of 418092139621.618347, Test MSE of 418092139750.467285
Epoch 3: training loss 427477665189.647
Test Loss of 418067732873.682190, Test MSE of 418067728988.742310
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427488310693.647
Test Loss of 418070577572.685608, Test MSE of 418070581667.342712
Epoch 2: training loss 427479621150.118
Test Loss of 418072158562.361328, Test MSE of 418072160958.563293
Epoch 3: training loss 427479391774.118
Test Loss of 418071625087.496643, Test MSE of 418071619421.732849
Epoch 4: training loss 427479169746.824
Test Loss of 418071562303.008118, Test MSE of 418071558817.268555
Epoch 5: training loss 427478971331.765
Test Loss of 418071471654.728638, Test MSE of 418071472478.417358
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418071472478.41736, 'MSE - std': 0.0, 'R2 - mean': -2.25556791659916, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005475 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917844239.059
Test Loss of 424556954251.873230, Test MSE of 424556949972.625366
Epoch 2: training loss 427897217746.824
Test Loss of 424540467058.823975, Test MSE of 424540476279.547119
Epoch 3: training loss 427870203181.176
Test Loss of 424518669653.333313, Test MSE of 424518674246.233643
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889221391.059
Test Loss of 424527314295.916748, Test MSE of 424527308512.253601
Epoch 2: training loss 427877289743.059
Test Loss of 424527444094.489929, Test MSE of 424527445074.759338
Epoch 3: training loss 427876922307.765
Test Loss of 424527307648.325684, Test MSE of 424527310103.546143
Epoch 4: training loss 427876664982.588
Test Loss of 424527428987.706665, Test MSE of 424527430417.433960
Epoch 5: training loss 427876464760.471
Test Loss of 424526928098.923889, Test MSE of 424526927173.338867
Epoch 6: training loss 427876293451.294
Test Loss of 424527239527.572510, Test MSE of 424527245159.677795
Epoch 7: training loss 427876153946.353
Test Loss of 424527060459.510498, Test MSE of 424527057696.683899
Epoch 8: training loss 427876029379.765
Test Loss of 424527150198.199402, Test MSE of 424527146347.440186
Epoch 9: training loss 427875927702.588
Test Loss of 424526663218.572266, Test MSE of 424526663657.022217
Epoch 10: training loss 415483977728.000
Test Loss of 385544508893.771912, Test MSE of 385544513163.516907
Epoch 11: training loss 337661965974.588
Test Loss of 282221779379.371704, Test MSE of 282221777349.942200
Epoch 12: training loss 232017545517.176
Test Loss of 188815352150.991455, Test MSE of 188815355072.145111
Epoch 13: training loss 164582723584.000
Test Loss of 144153521284.411743, Test MSE of 144153523766.151154
Epoch 14: training loss 140546116276.706
Test Loss of 131794601948.469116, Test MSE of 131794601865.713943
Epoch 15: training loss 135580127111.529
Test Loss of 128129075463.876007, Test MSE of 128129076925.622940
Epoch 16: training loss 131869519088.941
Test Loss of 125318971992.708771, Test MSE of 125318971908.156662
Epoch 17: training loss 129011517861.647
Test Loss of 122840033712.529266, Test MSE of 122840032318.024719
Epoch 18: training loss 126070112768.000
Test Loss of 120011642231.442978, Test MSE of 120011641436.053497
Epoch 19: training loss 122127772762.353
Test Loss of 116252871984.854965, Test MSE of 116252870826.742889
Epoch 20: training loss 117051618334.118
Test Loss of 110912150890.651855, Test MSE of 110912150633.868469
Epoch 21: training loss 112492345675.294
Test Loss of 106812762307.656723, Test MSE of 106812762996.448410
Epoch 22: training loss 108631225916.235
Test Loss of 103664812620.391388, Test MSE of 103664811777.283981
Epoch 23: training loss 104674804705.882
Test Loss of 98319437863.794586, Test MSE of 98319435648.192780
Epoch 24: training loss 100550412739.765
Test Loss of 96524206688.288681, Test MSE of 96524206419.872620
Epoch 25: training loss 97130533165.176
Test Loss of 91075284328.756882, Test MSE of 91075283700.895996
Epoch 26: training loss 93199654490.353
Test Loss of 87930885755.055283, Test MSE of 87930884987.914001
Epoch 27: training loss 89851613334.588
Test Loss of 85344568126.001389, Test MSE of 85344569010.420349
Epoch 28: training loss 85206734260.706
Test Loss of 83155746606.604675, Test MSE of 83155744988.400208
Epoch 29: training loss 81509812690.824
Test Loss of 76939263590.684250, Test MSE of 76939265240.912964
Epoch 30: training loss 78046897618.824
Test Loss of 75225824194.176270, Test MSE of 75225826708.222153
Epoch 31: training loss 73786397093.647
Test Loss of 69722442224.011108, Test MSE of 69722443502.515701
Epoch 32: training loss 70042208647.529
Test Loss of 66521192553.645157, Test MSE of 66521194574.744720
Epoch 33: training loss 66609123102.118
Test Loss of 63583880834.635208, Test MSE of 63583880619.916893
Epoch 34: training loss 63586123444.706
Test Loss of 58968363288.457092, Test MSE of 58968363241.908707
Epoch 35: training loss 60890237289.412
Test Loss of 57050941016.471893, Test MSE of 57050940400.834251
Epoch 36: training loss 57125370955.294
Test Loss of 52591630875.595650, Test MSE of 52591629397.468132
Epoch 37: training loss 54608529904.941
Test Loss of 50917187400.660652, Test MSE of 50917188105.375793
Epoch 38: training loss 52008583695.059
Test Loss of 48798902505.556328, Test MSE of 48798902604.338219
Epoch 39: training loss 48202344410.353
Test Loss of 48326940740.219292, Test MSE of 48326939926.557495
Epoch 40: training loss 45265500016.941
Test Loss of 44033155634.809158, Test MSE of 44033155554.817688
Epoch 41: training loss 43484017686.588
Test Loss of 44171686394.433495, Test MSE of 44171687502.817421
Epoch 42: training loss 40150797793.882
Test Loss of 37413693707.665970, Test MSE of 37413694498.323158
Epoch 43: training loss 38522948871.529
Test Loss of 39493444779.732590, Test MSE of 39493444479.460579
Epoch 44: training loss 36425117703.529
Test Loss of 35451019828.230392, Test MSE of 35451020473.782532
Epoch 45: training loss 34595578857.412
Test Loss of 32430881155.997223, Test MSE of 32430880374.689400
Epoch 46: training loss 32609962684.235
Test Loss of 33144070401.717327, Test MSE of 33144071308.324131
Epoch 47: training loss 31074266586.353
Test Loss of 33287815164.683784, Test MSE of 33287815434.421120
Epoch 48: training loss 28624769340.235
Test Loss of 28823863592.090679, Test MSE of 28823863604.248829
Epoch 49: training loss 27263264353.882
Test Loss of 30894980039.387463, Test MSE of 30894980063.001339
Epoch 50: training loss 26083103977.412
Test Loss of 28301436514.420540, Test MSE of 28301436654.153374
Epoch 51: training loss 24793872090.353
Test Loss of 30042216295.454082, Test MSE of 30042215412.852570
Epoch 52: training loss 23241962733.176
Test Loss of 26436185952.821651, Test MSE of 26436186005.405296
Epoch 53: training loss 22661469696.000
Test Loss of 26585123854.212353, Test MSE of 26585122949.243206
Epoch 54: training loss 21449509677.176
Test Loss of 26556650677.918114, Test MSE of 26556651313.000866
Epoch 55: training loss 20450817645.176
Test Loss of 27371392647.846401, Test MSE of 27371393147.960430
Epoch 56: training loss 19401472496.941
Test Loss of 25757757676.872543, Test MSE of 25757756774.895447
Epoch 57: training loss 18797010793.412
Test Loss of 27442927925.355541, Test MSE of 27442928961.071701
Epoch 58: training loss 17881463130.353
Test Loss of 27228331986.994217, Test MSE of 27228332549.999817
Epoch 59: training loss 17073999649.882
Test Loss of 27610852735.970390, Test MSE of 27610851393.922550
Epoch 60: training loss 16506974475.294
Test Loss of 23803088591.855656, Test MSE of 23803088446.357140
Epoch 61: training loss 15836419565.176
Test Loss of 26068169311.814945, Test MSE of 26068169823.720680
Epoch 62: training loss 15519003663.059
Test Loss of 24065228448.007401, Test MSE of 24065228791.892414
Epoch 63: training loss 15009056587.294
Test Loss of 22971567648.333103, Test MSE of 22971566794.235268
Epoch 64: training loss 14338038599.529
Test Loss of 24534070830.545456, Test MSE of 24534070565.554649
Epoch 65: training loss 13881361231.059
Test Loss of 24982041836.161926, Test MSE of 24982043283.077328
Epoch 66: training loss 13570064161.882
Test Loss of 23979746363.455009, Test MSE of 23979746438.839935
Epoch 67: training loss 13266189263.059
Test Loss of 24182437794.198475, Test MSE of 24182438005.127663
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221126955241.77252, 'MSE - std': 196944517236.64484, 'R2 - mean': -0.7141072342745303, 'R2 - std': 1.5414606823246295} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005569 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927769148.235
Test Loss of 447258271730.735107, Test MSE of 447258284677.991089
Epoch 2: training loss 421907819821.176
Test Loss of 447239721161.815430, Test MSE of 447239719048.122192
Epoch 3: training loss 421880939700.706
Test Loss of 447215002644.134155, Test MSE of 447214998594.118042
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895328346.353
Test Loss of 447221909043.993530, Test MSE of 447221916759.571594
Epoch 2: training loss 421886537487.059
Test Loss of 447222771290.366882, Test MSE of 447222774125.674622
Epoch 3: training loss 421886180894.118
Test Loss of 447222823061.229675, Test MSE of 447222830598.538574
Epoch 4: training loss 421885955614.118
Test Loss of 447222624770.487183, Test MSE of 447222627782.143860
Epoch 5: training loss 421885782136.471
Test Loss of 447222351681.080750, Test MSE of 447222353829.414490
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 296492088104.3198, 'MSE - std': 192919417760.79, 'R2 - mean': -1.1351147213076522, 'R2 - std': 1.3923225075550434} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003749 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430111234409.412
Test Loss of 410765059446.819092, Test MSE of 410765064670.333618
Epoch 2: training loss 430092080067.765
Test Loss of 410746709275.838989, Test MSE of 410746706876.985413
Epoch 3: training loss 430065772664.471
Test Loss of 410722979345.295715, Test MSE of 410722980019.673523
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079495228.235
Test Loss of 410728377285.715881, Test MSE of 410728378400.468445
Epoch 2: training loss 430066163712.000
Test Loss of 410728381334.330383, Test MSE of 410728386246.200378
Epoch 3: training loss 430065873618.824
Test Loss of 410728213491.205933, Test MSE of 410728214367.914246
Epoch 4: training loss 430065672673.882
Test Loss of 410728308701.882446, Test MSE of 410728301021.751038
Epoch 5: training loss 430065500160.000
Test Loss of 410728363714.517334, Test MSE of 410728361453.190796
Epoch 6: training loss 430065344030.118
Test Loss of 410728545128.840332, Test MSE of 410728552625.301086
Epoch 7: training loss 430065238256.941
Test Loss of 410728327276.986572, Test MSE of 410728329603.749207
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 325051148479.1772, 'MSE - std': 174242033104.40308, 'R2 - mean': -1.4488247809182775, 'R2 - std': 1.3225594422505096} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 9, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005366 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424043671311.059
Test Loss of 431612564470.049072, Test MSE of 431612565211.745483
Epoch 2: training loss 424024515282.824
Test Loss of 431592622526.371155, Test MSE of 431592626679.012939
Epoch 3: training loss 423997676604.235
Test Loss of 431566172487.907471, Test MSE of 431566182242.014526
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424013918328.471
Test Loss of 431567351286.759827, Test MSE of 431567348901.801453
Epoch 2: training loss 423999896515.765
Test Loss of 431568818780.638611, Test MSE of 431568816379.953918
Epoch 3: training loss 423999452943.059
Test Loss of 431568395844.471985, Test MSE of 431568396636.169312
Epoch 4: training loss 423999079363.765
Test Loss of 431567936728.551575, Test MSE of 431567930693.389954
Epoch 5: training loss 423998832158.118
Test Loss of 431568558850.487732, Test MSE of 431568561551.460327
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 346354631093.6338, 'MSE - std': 161566030978.98563, 'R2 - mean': -1.6036538328576087, 'R2 - std': 1.2227914474375878} 
 

Saving model.....
Results After CV: {'MSE - mean': 346354631093.6338, 'MSE - std': 161566030978.98563, 'R2 - mean': -1.6036538328576087, 'R2 - std': 1.2227914474375878}
Train time: 31.417771386400272
Inference time: 0.06944560600022669
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 49 finished with value: 346354631093.6338 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 9, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005400 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526093281.882
Test Loss of 418111570098.128174, Test MSE of 418111572316.876343
Epoch 2: training loss 427506237801.412
Test Loss of 418093424524.406189, Test MSE of 418093425595.606567
Epoch 3: training loss 427478937720.471
Test Loss of 418069512480.036987, Test MSE of 418069511849.332397
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493645010.824
Test Loss of 418074050798.767517, Test MSE of 418074054889.313232
Epoch 2: training loss 427481851301.647
Test Loss of 418075228468.408020, Test MSE of 418075227632.901245
Epoch 3: training loss 427481377249.882
Test Loss of 418074623234.427917, Test MSE of 418074636450.015503
Epoch 4: training loss 422354161784.471
Test Loss of 401849846213.374023, Test MSE of 401849842588.541992
Epoch 5: training loss 386542913776.941
Test Loss of 347499712982.902588, Test MSE of 347499710139.080444
Epoch 6: training loss 318570394804.706
Test Loss of 270139723576.079559, Test MSE of 270139721274.074493
Epoch 7: training loss 226332759040.000
Test Loss of 171563078716.876251, Test MSE of 171563080641.041107
Epoch 8: training loss 161893335070.118
Test Loss of 131592712869.455475, Test MSE of 131592710885.730591
Epoch 9: training loss 141594847774.118
Test Loss of 119851026339.619705, Test MSE of 119851026341.512085
Epoch 10: training loss 136489085575.529
Test Loss of 116508831547.869537, Test MSE of 116508829215.801788
Epoch 11: training loss 133647237722.353
Test Loss of 113290900093.660889, Test MSE of 113290899528.445007
Epoch 12: training loss 130409568617.412
Test Loss of 109830284304.344208, Test MSE of 109830285159.673492
Epoch 13: training loss 126922268611.765
Test Loss of 107241994000.758728, Test MSE of 107241995236.436646
Epoch 14: training loss 121786657882.353
Test Loss of 103886010020.744858, Test MSE of 103886008775.864487
Epoch 15: training loss 119878584967.529
Test Loss of 101627709501.113113, Test MSE of 101627709341.050064
Epoch 16: training loss 115948357571.765
Test Loss of 97304258901.570206, Test MSE of 97304258589.453979
Epoch 17: training loss 112061990189.176
Test Loss of 94539859363.975021, Test MSE of 94539859861.791229
Epoch 18: training loss 107656979847.529
Test Loss of 92278661176.849411, Test MSE of 92278662143.905334
Epoch 19: training loss 104422890714.353
Test Loss of 88695933157.766373, Test MSE of 88695935143.850281
Epoch 20: training loss 100664117368.471
Test Loss of 84348459051.110809, Test MSE of 84348459861.094986
Epoch 21: training loss 95668861906.824
Test Loss of 82071134998.206802, Test MSE of 82071135999.745575
Epoch 22: training loss 93909561313.882
Test Loss of 79920497591.280136, Test MSE of 79920497830.347351
Epoch 23: training loss 89925865637.647
Test Loss of 77286242313.001160, Test MSE of 77286242635.955002
Epoch 24: training loss 87244197511.529
Test Loss of 72209112259.893585, Test MSE of 72209110804.617859
Epoch 25: training loss 82610178469.647
Test Loss of 69959513222.780472, Test MSE of 69959513431.376389
Epoch 26: training loss 78581831307.294
Test Loss of 65871317826.265091, Test MSE of 65871318733.293198
Epoch 27: training loss 75135490424.471
Test Loss of 64180474768.433029, Test MSE of 64180476150.449272
Epoch 28: training loss 72808617735.529
Test Loss of 60088298368.799446, Test MSE of 60088299156.878555
Epoch 29: training loss 69121136384.000
Test Loss of 59131461112.775391, Test MSE of 59131461404.423172
Epoch 30: training loss 66453490337.882
Test Loss of 52832202476.280357, Test MSE of 52832203522.572495
Epoch 31: training loss 63406203188.706
Test Loss of 53177778122.940552, Test MSE of 53177778145.778366
Epoch 32: training loss 60352400030.118
Test Loss of 51423503339.392090, Test MSE of 51423503511.422913
Epoch 33: training loss 57365913276.235
Test Loss of 46452963876.123062, Test MSE of 46452963993.691643
Epoch 34: training loss 54883410544.941
Test Loss of 46265793875.912102, Test MSE of 46265793868.451599
Epoch 35: training loss 51484405933.176
Test Loss of 41816541048.982651, Test MSE of 41816541895.308266
Epoch 36: training loss 49657236020.706
Test Loss of 44083347036.261856, Test MSE of 44083346971.315681
Epoch 37: training loss 46657217392.941
Test Loss of 37286650308.900299, Test MSE of 37286650483.698227
Epoch 38: training loss 44885790042.353
Test Loss of 38408465962.992363, Test MSE of 38408466433.754021
Epoch 39: training loss 42692887495.529
Test Loss of 35946087100.195236, Test MSE of 35946086904.261040
Epoch 40: training loss 41003514270.118
Test Loss of 35769323043.412445, Test MSE of 35769322811.180862
Epoch 41: training loss 38663835881.412
Test Loss of 31727406551.613232, Test MSE of 31727407095.187809
Epoch 42: training loss 36917141033.412
Test Loss of 29904493096.860512, Test MSE of 29904493293.140404
Epoch 43: training loss 35290999860.706
Test Loss of 29879748325.174183, Test MSE of 29879747946.751778
Epoch 44: training loss 33240382064.941
Test Loss of 30997808142.449226, Test MSE of 30997808568.002270
Epoch 45: training loss 32038033701.647
Test Loss of 27523307866.781403, Test MSE of 27523307539.682556
Epoch 46: training loss 30546768222.118
Test Loss of 29144686238.586166, Test MSE of 29144686169.542595
Epoch 47: training loss 29632034808.471
Test Loss of 26911572408.582928, Test MSE of 26911573005.134514
Epoch 48: training loss 28077195568.941
Test Loss of 21960259580.683784, Test MSE of 21960259925.480831
Epoch 49: training loss 26658658416.941
Test Loss of 27651137652.541290, Test MSE of 27651137861.733242
Epoch 50: training loss 25533908992.000
Test Loss of 22563514119.757576, Test MSE of 22563513999.905315
Epoch 51: training loss 25026955290.353
Test Loss of 23241288789.037243, Test MSE of 23241288649.426434
Epoch 52: training loss 23738959235.765
Test Loss of 23672717372.165627, Test MSE of 23672717974.550724
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23672717974.550724, 'MSE - std': 0.0, 'R2 - mean': 0.8156579766618107, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005429 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917571252.706
Test Loss of 424556592265.149231, Test MSE of 424556590487.423462
Epoch 2: training loss 427896408666.353
Test Loss of 424539619115.762207, Test MSE of 424539624631.660950
Epoch 3: training loss 427868144338.824
Test Loss of 424516771042.213257, Test MSE of 424516768487.871155
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886087951.059
Test Loss of 424523234873.204712, Test MSE of 424523236760.657593
Epoch 2: training loss 427876321400.471
Test Loss of 424524881595.958374, Test MSE of 424524887103.551208
Epoch 3: training loss 427875666522.353
Test Loss of 424524387597.324097, Test MSE of 424524384588.271484
Epoch 4: training loss 422483480816.941
Test Loss of 407716043627.007141, Test MSE of 407716038290.126587
Epoch 5: training loss 385659556080.941
Test Loss of 352640014703.152466, Test MSE of 352640017774.251282
Epoch 6: training loss 316841635478.588
Test Loss of 277179225990.247498, Test MSE of 277179227646.722290
Epoch 7: training loss 224430723493.647
Test Loss of 180940001231.678009, Test MSE of 180940006496.617523
Epoch 8: training loss 159867969716.706
Test Loss of 141800444992.666199, Test MSE of 141800447731.218323
Epoch 9: training loss 140230673679.059
Test Loss of 131827926100.563492, Test MSE of 131827928161.577438
Epoch 10: training loss 133938749168.941
Test Loss of 127471607881.904236, Test MSE of 127471610420.120819
Epoch 11: training loss 131788913995.294
Test Loss of 124919774654.030991, Test MSE of 124919775800.119446
Epoch 12: training loss 128086342686.118
Test Loss of 122280918249.793198, Test MSE of 122280917553.490952
Epoch 13: training loss 124223635395.765
Test Loss of 117669622537.889435, Test MSE of 117669623190.400253
Epoch 14: training loss 120225415288.471
Test Loss of 115576816209.128845, Test MSE of 115576813585.047363
Epoch 15: training loss 116508723471.059
Test Loss of 111950596098.368729, Test MSE of 111950599642.638184
Epoch 16: training loss 113327552542.118
Test Loss of 107069998659.153366, Test MSE of 107069999985.178177
Epoch 17: training loss 108428246226.824
Test Loss of 104763915139.641922, Test MSE of 104763919414.547958
Epoch 18: training loss 103917133974.588
Test Loss of 100973951106.279892, Test MSE of 100973950912.802841
Epoch 19: training loss 100273264850.824
Test Loss of 96475244957.579453, Test MSE of 96475244921.974350
Epoch 20: training loss 97668233788.235
Test Loss of 93691325964.672684, Test MSE of 93691323703.877182
Epoch 21: training loss 93339137536.000
Test Loss of 88418677053.409210, Test MSE of 88418678072.522400
Epoch 22: training loss 90327597206.588
Test Loss of 86287858090.844315, Test MSE of 86287855939.709091
Epoch 23: training loss 85321453086.118
Test Loss of 80512229315.360626, Test MSE of 80512229381.978195
Epoch 24: training loss 80801481035.294
Test Loss of 80302757064.157303, Test MSE of 80302758574.583160
Epoch 25: training loss 77951580807.529
Test Loss of 76757885906.757339, Test MSE of 76757886633.387253
Epoch 26: training loss 74684432609.882
Test Loss of 71981904560.825348, Test MSE of 71981903225.909805
Epoch 27: training loss 71394681660.235
Test Loss of 69340890686.415909, Test MSE of 69340888644.176346
Epoch 28: training loss 67956037014.588
Test Loss of 64419050166.510292, Test MSE of 64419050927.205040
Epoch 29: training loss 65026666465.882
Test Loss of 60001899402.748093, Test MSE of 60001899563.199097
Epoch 30: training loss 61785969392.941
Test Loss of 60649860413.172333, Test MSE of 60649861228.727631
Epoch 31: training loss 59226998512.941
Test Loss of 57607106948.944717, Test MSE of 57607104845.337379
Epoch 32: training loss 55410424485.647
Test Loss of 54050110551.879715, Test MSE of 54050108878.911003
Epoch 33: training loss 52658180397.176
Test Loss of 52791973203.438354, Test MSE of 52791974157.390228
Epoch 34: training loss 50573764050.824
Test Loss of 47431255147.777008, Test MSE of 47431254273.892204
Epoch 35: training loss 47423879408.941
Test Loss of 48121068620.983574, Test MSE of 48121068737.639664
Epoch 36: training loss 44716002288.941
Test Loss of 42493677324.021278, Test MSE of 42493677651.751007
Epoch 37: training loss 42300800496.941
Test Loss of 43145165650.372429, Test MSE of 43145164939.711189
Epoch 38: training loss 40178724480.000
Test Loss of 36634295759.085823, Test MSE of 36634296878.793617
Epoch 39: training loss 37742281140.706
Test Loss of 40365304072.349754, Test MSE of 40365303927.614510
Epoch 40: training loss 36253933221.647
Test Loss of 35735603310.856346, Test MSE of 35735603826.145836
Epoch 41: training loss 34294842164.706
Test Loss of 38002613375.674301, Test MSE of 38002613072.353973
Epoch 42: training loss 32556904960.000
Test Loss of 34899245480.949341, Test MSE of 34899245713.050323
Epoch 43: training loss 31176517496.471
Test Loss of 33075904498.261391, Test MSE of 33075904135.862965
Epoch 44: training loss 29169888263.529
Test Loss of 33325909530.885033, Test MSE of 33325908981.509274
Epoch 45: training loss 28041197387.294
Test Loss of 30558620271.448532, Test MSE of 30558618902.441845
Epoch 46: training loss 26579521114.353
Test Loss of 29733002091.007172, Test MSE of 29733001425.881104
Epoch 47: training loss 25180028807.529
Test Loss of 31894389116.417301, Test MSE of 31894390048.076298
Epoch 48: training loss 24185680218.353
Test Loss of 30633052128.732826, Test MSE of 30633052131.290024
Epoch 49: training loss 23178921170.824
Test Loss of 29397874223.256073, Test MSE of 29397873328.517921
Epoch 50: training loss 22112642416.941
Test Loss of 25688692021.355541, Test MSE of 25688692833.842682
Epoch 51: training loss 21163990682.353
Test Loss of 26409180092.017582, Test MSE of 26409180399.533321
Epoch 52: training loss 20240505957.647
Test Loss of 26585358054.121674, Test MSE of 26585358217.779072
Epoch 53: training loss 19436365402.353
Test Loss of 26763411185.491558, Test MSE of 26763410654.599815
Epoch 54: training loss 18897574294.588
Test Loss of 25832600401.424938, Test MSE of 25832599943.782135
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24752658959.166428, 'MSE - std': 1079940984.6157055, 'R2 - mean': 0.8156151820296238, 'R2 - std': 4.2794632186982096e-05} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005488 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926331090.824
Test Loss of 447257236343.798279, Test MSE of 447257236684.695862
Epoch 2: training loss 421904587474.824
Test Loss of 447237701279.533630, Test MSE of 447237707730.239746
Epoch 3: training loss 421876900321.882
Test Loss of 447212151744.755005, Test MSE of 447212157628.048828
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896599070.118
Test Loss of 447219742778.270630, Test MSE of 447219742926.968628
Epoch 2: training loss 421885171832.471
Test Loss of 447220403034.899841, Test MSE of 447220403950.273438
Epoch 3: training loss 421884681878.588
Test Loss of 447219219763.697449, Test MSE of 447219216337.236572
Epoch 4: training loss 416329520790.588
Test Loss of 429795779039.430054, Test MSE of 429795770819.637268
Epoch 5: training loss 379576018823.529
Test Loss of 374097297495.169067, Test MSE of 374097304934.333679
Epoch 6: training loss 311604099192.471
Test Loss of 295419591290.344666, Test MSE of 295419594060.562561
Epoch 7: training loss 219966016210.824
Test Loss of 195035511975.468903, Test MSE of 195035516141.801514
Epoch 8: training loss 154507663360.000
Test Loss of 153853258998.110565, Test MSE of 153853260955.487396
Epoch 9: training loss 137290762601.412
Test Loss of 142737133284.700439, Test MSE of 142737133717.222595
Epoch 10: training loss 131951655092.706
Test Loss of 137457417974.702759, Test MSE of 137457418756.925507
Epoch 11: training loss 129405692054.588
Test Loss of 134657275750.032852, Test MSE of 134657278055.319885
Epoch 12: training loss 125849267862.588
Test Loss of 130472540115.941711, Test MSE of 130472540677.091949
Epoch 13: training loss 120894007928.471
Test Loss of 128283301265.972702, Test MSE of 128283303202.318909
Epoch 14: training loss 118973150208.000
Test Loss of 122462334296.649551, Test MSE of 122462336572.254745
Epoch 15: training loss 114595169069.176
Test Loss of 120287114435.419846, Test MSE of 120287116187.444824
Epoch 16: training loss 110392545671.529
Test Loss of 116212960295.557709, Test MSE of 116212960306.507980
Epoch 17: training loss 106966350938.353
Test Loss of 111789017094.158691, Test MSE of 111789015925.518448
Epoch 18: training loss 102487650785.882
Test Loss of 111349003311.611374, Test MSE of 111349003862.908020
Epoch 19: training loss 98716152320.000
Test Loss of 105643222911.615082, Test MSE of 105643223840.525970
Epoch 20: training loss 95199099512.471
Test Loss of 100847730426.492706, Test MSE of 100847733322.174225
Epoch 21: training loss 91360347151.059
Test Loss of 97740663836.187836, Test MSE of 97740664822.542938
Epoch 22: training loss 87297546661.647
Test Loss of 93603447331.649323, Test MSE of 93603447897.535339
Epoch 23: training loss 84041949590.588
Test Loss of 88377078644.008331, Test MSE of 88377079514.037521
Epoch 24: training loss 79531373628.235
Test Loss of 85991037052.594955, Test MSE of 85991037147.218246
Epoch 25: training loss 77203149056.000
Test Loss of 83413368227.501266, Test MSE of 83413366671.303894
Epoch 26: training loss 73181182328.471
Test Loss of 75410120527.766830, Test MSE of 75410120248.958862
Epoch 27: training loss 69659102238.118
Test Loss of 73516060030.549149, Test MSE of 73516060477.906754
Epoch 28: training loss 66401989767.529
Test Loss of 71060446362.204025, Test MSE of 71060446162.037537
Epoch 29: training loss 64123735777.882
Test Loss of 70003824948.408051, Test MSE of 70003824681.485855
Epoch 30: training loss 60210817867.294
Test Loss of 63027083674.736992, Test MSE of 63027084172.012962
Epoch 31: training loss 58127976734.118
Test Loss of 64556355827.504974, Test MSE of 64556356699.752487
Epoch 32: training loss 55083003354.353
Test Loss of 59040727837.786720, Test MSE of 59040728645.389481
Epoch 33: training loss 52304623329.882
Test Loss of 56256530044.002777, Test MSE of 56256530443.529045
Epoch 34: training loss 49013482413.176
Test Loss of 54894918071.161690, Test MSE of 54894918345.755814
Epoch 35: training loss 46584956709.647
Test Loss of 46479767982.634285, Test MSE of 46479768167.831329
Epoch 36: training loss 44543703529.412
Test Loss of 49771160111.966690, Test MSE of 49771161828.674484
Epoch 37: training loss 42145698304.000
Test Loss of 46103794286.501038, Test MSE of 46103794844.454460
Epoch 38: training loss 40332121931.294
Test Loss of 45344162918.565811, Test MSE of 45344163554.239098
Epoch 39: training loss 38059542174.118
Test Loss of 43781211280.255379, Test MSE of 43781210802.036682
Epoch 40: training loss 36924511096.471
Test Loss of 41820708543.748322, Test MSE of 41820710119.162926
Epoch 41: training loss 34705236660.706
Test Loss of 37579768203.103401, Test MSE of 37579768645.731804
Epoch 42: training loss 32970366305.882
Test Loss of 37129157581.546150, Test MSE of 37129157659.585594
Epoch 43: training loss 30903157496.471
Test Loss of 36025724209.802452, Test MSE of 36025724108.062195
Epoch 44: training loss 29348296636.235
Test Loss of 33513437598.526947, Test MSE of 33513437641.027084
Epoch 45: training loss 28309491576.471
Test Loss of 32028765891.538284, Test MSE of 32028766261.697468
Epoch 46: training loss 26719057148.235
Test Loss of 34059907439.626186, Test MSE of 34059908471.007172
Epoch 47: training loss 25805329562.353
Test Loss of 30743532002.272495, Test MSE of 30743531987.319035
Epoch 48: training loss 24432393927.529
Test Loss of 31594602481.787647, Test MSE of 31594603085.437782
Epoch 49: training loss 23698298439.529
Test Loss of 28704528658.298405, Test MSE of 28704528850.498447
Epoch 50: training loss 23328217008.941
Test Loss of 31020593741.338886, Test MSE of 31020593354.008991
Epoch 51: training loss 21830306537.412
Test Loss of 27833826627.331020, Test MSE of 27833827244.333141
Epoch 52: training loss 20890379915.294
Test Loss of 28655496820.896599, Test MSE of 28655496572.800102
Epoch 53: training loss 20281383439.059
Test Loss of 27048197109.340736, Test MSE of 27048196824.342976
Epoch 54: training loss 19347923538.824
Test Loss of 26287645826.516769, Test MSE of 26287646018.922207
Epoch 55: training loss 19003019986.824
Test Loss of 29189525031.676151, Test MSE of 29189525163.609985
Epoch 56: training loss 18301737185.882
Test Loss of 24279288274.165165, Test MSE of 24279288787.986870
Epoch 57: training loss 17537154646.588
Test Loss of 24844463789.746010, Test MSE of 24844464022.030342
Epoch 58: training loss 17072676491.294
Test Loss of 25493037081.582233, Test MSE of 25493037481.544029
Epoch 59: training loss 16669155565.176
Test Loss of 23426115363.234791, Test MSE of 23426115501.398094
Epoch 60: training loss 16042818616.471
Test Loss of 24441896419.456860, Test MSE of 24441896817.362976
Epoch 61: training loss 15572890319.059
Test Loss of 23588447612.180431, Test MSE of 23588447658.924034
Epoch 62: training loss 15105942471.529
Test Loss of 22381895319.006245, Test MSE of 22381895450.756889
Epoch 63: training loss 15022109903.059
Test Loss of 24043367268.848484, Test MSE of 24043366958.690739
Epoch 64: training loss 14386566464.000
Test Loss of 22793570211.145962, Test MSE of 22793570503.393089
Epoch 65: training loss 13938829161.412
Test Loss of 23601237447.979645, Test MSE of 23601237974.542583
Epoch 66: training loss 13457368726.588
Test Loss of 23776689012.008327, Test MSE of 23776689089.601036
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24427335669.3113, 'MSE - std': 994578054.2586007, 'R2 - mean': 0.8243168379708868, 'R2 - std': 0.012306049453849824} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005624 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110441472.000
Test Loss of 410763677125.478943, Test MSE of 410763676098.618469
Epoch 2: training loss 430089410680.471
Test Loss of 410745339570.406311, Test MSE of 410745326206.044434
Epoch 3: training loss 430061755452.235
Test Loss of 410721765215.837097, Test MSE of 410721766633.366150
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075972186.353
Test Loss of 410726517591.781555, Test MSE of 410726527230.031128
Epoch 2: training loss 430065059960.471
Test Loss of 410726374480.081421, Test MSE of 410726371791.750916
Epoch 3: training loss 430064617833.412
Test Loss of 410726933901.564087, Test MSE of 410726940343.406006
Epoch 4: training loss 424879042319.059
Test Loss of 394355970849.762146, Test MSE of 394355976170.323364
Epoch 5: training loss 389092536681.412
Test Loss of 340215617798.515503, Test MSE of 340215618640.353760
Epoch 6: training loss 320663673916.235
Test Loss of 262554555078.782043, Test MSE of 262554554660.331207
Epoch 7: training loss 229245746838.588
Test Loss of 165377698825.950958, Test MSE of 165377699522.738098
Epoch 8: training loss 164078165052.235
Test Loss of 125208725305.928741, Test MSE of 125208727636.419922
Epoch 9: training loss 143791585611.294
Test Loss of 114534473693.882462, Test MSE of 114534473572.847870
Epoch 10: training loss 137651739798.588
Test Loss of 110705296015.814896, Test MSE of 110705296881.099457
Epoch 11: training loss 134106439077.647
Test Loss of 107478002974.682098, Test MSE of 107478003242.490784
Epoch 12: training loss 131154094290.824
Test Loss of 104479020059.957428, Test MSE of 104479020780.620850
Epoch 13: training loss 129265837086.118
Test Loss of 101495758116.368347, Test MSE of 101495759116.776352
Epoch 14: training loss 124583585008.941
Test Loss of 98062334438.174911, Test MSE of 98062335835.442413
Epoch 15: training loss 121023881306.353
Test Loss of 94135441283.850067, Test MSE of 94135441812.546951
Epoch 16: training loss 116004998204.235
Test Loss of 91291096084.375748, Test MSE of 91291096303.748260
Epoch 17: training loss 113059962548.706
Test Loss of 89587151399.566864, Test MSE of 89587151973.333420
Epoch 18: training loss 109103231307.294
Test Loss of 85597023268.486816, Test MSE of 85597022640.117264
Epoch 19: training loss 105495571666.824
Test Loss of 82766569396.183243, Test MSE of 82766569547.300842
Epoch 20: training loss 101795925985.882
Test Loss of 78373390206.163818, Test MSE of 78373388867.253204
Epoch 21: training loss 97441653007.059
Test Loss of 77000838995.516891, Test MSE of 77000838494.865585
Epoch 22: training loss 93269379147.294
Test Loss of 73850603314.347061, Test MSE of 73850603905.230789
Epoch 23: training loss 90084424463.059
Test Loss of 69346510926.659882, Test MSE of 69346511579.949265
Epoch 24: training loss 85782884276.706
Test Loss of 68447800720.407219, Test MSE of 68447799056.417267
Epoch 25: training loss 81973100800.000
Test Loss of 62853757345.939842, Test MSE of 62853756466.486328
Epoch 26: training loss 77848540762.353
Test Loss of 61778600311.292923, Test MSE of 61778601744.450966
Epoch 27: training loss 75392424146.824
Test Loss of 61733228932.560852, Test MSE of 61733228970.937050
Epoch 28: training loss 72326172310.588
Test Loss of 58448848648.647850, Test MSE of 58448848705.224556
Epoch 29: training loss 68266352263.529
Test Loss of 54296624587.165199, Test MSE of 54296623545.845947
Epoch 30: training loss 65021251591.529
Test Loss of 50416694908.386856, Test MSE of 50416695178.072823
Epoch 31: training loss 62035613816.471
Test Loss of 49526141422.704300, Test MSE of 49526141894.293671
Epoch 32: training loss 59726838784.000
Test Loss of 47024824494.378525, Test MSE of 47024824331.360435
Epoch 33: training loss 56209635177.412
Test Loss of 44731183109.686256, Test MSE of 44731183016.245132
Epoch 34: training loss 53007373786.353
Test Loss of 41821768094.149002, Test MSE of 41821768695.280495
Epoch 35: training loss 50622863081.412
Test Loss of 40237654632.011108, Test MSE of 40237654455.109253
Epoch 36: training loss 48336154240.000
Test Loss of 37523938137.677002, Test MSE of 37523938289.345009
Epoch 37: training loss 45957245440.000
Test Loss of 35072941283.924110, Test MSE of 35072941106.559769
Epoch 38: training loss 43407514804.706
Test Loss of 35788626986.173065, Test MSE of 35788626209.393562
Epoch 39: training loss 41930826413.176
Test Loss of 32579249162.898659, Test MSE of 32579248687.871101
Epoch 40: training loss 39399950930.824
Test Loss of 31885338909.260529, Test MSE of 31885338965.617958
Epoch 41: training loss 38106817377.882
Test Loss of 31364798273.510410, Test MSE of 31364798258.269547
Epoch 42: training loss 35992210823.529
Test Loss of 30974237296.540489, Test MSE of 30974236868.126102
Epoch 43: training loss 34418078501.647
Test Loss of 28243347118.615456, Test MSE of 28243347500.872635
Epoch 44: training loss 32626817046.588
Test Loss of 27327514226.909763, Test MSE of 27327513812.470810
Epoch 45: training loss 31330938390.588
Test Loss of 25978925732.664509, Test MSE of 25978925768.193291
Epoch 46: training loss 29905680180.706
Test Loss of 25787122236.416473, Test MSE of 25787122732.161736
Epoch 47: training loss 28173274127.059
Test Loss of 24818959895.929661, Test MSE of 24818960055.539165
Epoch 48: training loss 27010285906.824
Test Loss of 24275609128.040722, Test MSE of 24275609239.534538
Epoch 49: training loss 25936392786.824
Test Loss of 23031336326.930126, Test MSE of 23031336376.444012
Epoch 50: training loss 24808117327.059
Test Loss of 22097677349.434521, Test MSE of 22097677934.185440
Epoch 51: training loss 24014068257.882
Test Loss of 22190026074.387783, Test MSE of 22190026002.776188
Epoch 52: training loss 23125606080.000
Test Loss of 22517028261.256824, Test MSE of 22517028384.316364
Epoch 53: training loss 22301060980.706
Test Loss of 21973070959.355854, Test MSE of 21973070938.449730
Epoch 54: training loss 21507452856.471
Test Loss of 21914632062.163815, Test MSE of 21914632108.525784
Epoch 55: training loss 20683721946.353
Test Loss of 21842303882.957890, Test MSE of 21842304047.361404
Epoch 56: training loss 19984068762.353
Test Loss of 19672168195.909302, Test MSE of 19672168712.073845
Epoch 57: training loss 19266288914.824
Test Loss of 19233645975.041183, Test MSE of 19233645808.521214
Epoch 58: training loss 18729828239.059
Test Loss of 19441377551.044888, Test MSE of 19441377442.218147
Epoch 59: training loss 18434763467.294
Test Loss of 20252170804.834797, Test MSE of 20252170671.446899
Epoch 60: training loss 17638324137.412
Test Loss of 20079421917.645535, Test MSE of 20079421734.805218
Epoch 61: training loss 17011778558.118
Test Loss of 18981135833.380840, Test MSE of 18981135469.704769
Epoch 62: training loss 16824772577.882
Test Loss of 19040252279.292919, Test MSE of 19040251958.989574
Epoch 63: training loss 16083957888.000
Test Loss of 18829399302.989357, Test MSE of 18829399595.939678
Epoch 64: training loss 15760366671.059
Test Loss of 19411061252.501621, Test MSE of 19411061412.773586
Epoch 65: training loss 15124744451.765
Test Loss of 19797460049.029152, Test MSE of 19797459926.700897
Epoch 66: training loss 14870879258.353
Test Loss of 19128342174.504395, Test MSE of 19128342498.123653
Epoch 67: training loss 14690958010.353
Test Loss of 18963372064.222118, Test MSE of 18963372467.005619
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23061344868.73488, 'MSE - std': 2517872461.2831054, 'R2 - mean': 0.8291089794937292, 'R2 - std': 0.013508256771149238} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005342 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043399047.529
Test Loss of 431611648558.200806, Test MSE of 431611647097.095886
Epoch 2: training loss 424023520677.647
Test Loss of 431590852250.713562, Test MSE of 431590852039.570312
Epoch 3: training loss 423996142772.706
Test Loss of 431563210949.123535, Test MSE of 431563209414.447449
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424007753728.000
Test Loss of 431564687131.602051, Test MSE of 431564689284.803223
Epoch 2: training loss 423997556133.647
Test Loss of 431566617350.278564, Test MSE of 431566611291.983215
Epoch 3: training loss 423996904990.118
Test Loss of 431566562762.217468, Test MSE of 431566565834.294067
Epoch 4: training loss 418434111969.882
Test Loss of 413870035397.005066, Test MSE of 413870042366.787231
Epoch 5: training loss 381595228762.353
Test Loss of 357743928505.277161, Test MSE of 357743930835.247375
Epoch 6: training loss 313517860261.647
Test Loss of 278794500536.684875, Test MSE of 278794501286.202209
Epoch 7: training loss 222898182957.176
Test Loss of 178681329755.927826, Test MSE of 178681327774.423370
Epoch 8: training loss 159999783062.588
Test Loss of 137542279808.651550, Test MSE of 137542280613.754730
Epoch 9: training loss 140748544391.529
Test Loss of 126203631730.672836, Test MSE of 126203632176.298843
Epoch 10: training loss 135919956510.118
Test Loss of 120921514385.828781, Test MSE of 120921515127.777039
Epoch 11: training loss 131984817091.765
Test Loss of 117812782534.426651, Test MSE of 117812781580.449249
Epoch 12: training loss 128634373089.882
Test Loss of 113821109758.341507, Test MSE of 113821111451.657166
Epoch 13: training loss 125855233084.235
Test Loss of 111973550420.227676, Test MSE of 111973548065.325272
Epoch 14: training loss 121912265607.529
Test Loss of 107688979867.779724, Test MSE of 107688979664.602646
Epoch 15: training loss 118155942701.176
Test Loss of 103959191817.358627, Test MSE of 103959192022.191254
Epoch 16: training loss 113096829650.824
Test Loss of 99741528725.501160, Test MSE of 99741530304.358521
Epoch 17: training loss 110057735107.765
Test Loss of 96537511700.020355, Test MSE of 96537510679.755066
Epoch 18: training loss 107338812175.059
Test Loss of 93405699682.798706, Test MSE of 93405700653.414124
Epoch 19: training loss 101949542445.176
Test Loss of 91349559314.006485, Test MSE of 91349561556.250641
Epoch 20: training loss 98730190908.235
Test Loss of 87490955820.305420, Test MSE of 87490954596.773636
Epoch 21: training loss 95323446031.059
Test Loss of 84602738556.268387, Test MSE of 84602739466.304611
Epoch 22: training loss 91809617423.059
Test Loss of 79543961642.646927, Test MSE of 79543960506.098450
Epoch 23: training loss 87820946537.412
Test Loss of 76162184216.166595, Test MSE of 76162183807.923523
Epoch 24: training loss 84315155764.706
Test Loss of 73377965988.072189, Test MSE of 73377965868.777359
Epoch 25: training loss 80883047122.824
Test Loss of 70026770450.006485, Test MSE of 70026771845.970413
Epoch 26: training loss 76849576161.882
Test Loss of 65680023428.797775, Test MSE of 65680021545.968994
Epoch 27: training loss 73726105931.294
Test Loss of 63898667829.664047, Test MSE of 63898666297.862579
Epoch 28: training loss 69997013135.059
Test Loss of 58093523857.591858, Test MSE of 58093522362.658203
Epoch 29: training loss 67041941338.353
Test Loss of 58005929951.777878, Test MSE of 58005931534.993088
Epoch 30: training loss 64215200421.647
Test Loss of 55405784807.478020, Test MSE of 55405785684.816437
Epoch 31: training loss 61121367205.647
Test Loss of 50239640215.396576, Test MSE of 50239640021.343033
Epoch 32: training loss 57636072719.059
Test Loss of 48026528626.317444, Test MSE of 48026528814.534912
Epoch 33: training loss 54670024719.059
Test Loss of 46715661740.838501, Test MSE of 46715661237.822319
Epoch 34: training loss 52488351706.353
Test Loss of 39899174634.321144, Test MSE of 39899174980.631454
Epoch 35: training loss 49644190531.765
Test Loss of 43397236892.372047, Test MSE of 43397237191.819618
Epoch 36: training loss 48236584372.706
Test Loss of 38421294832.955116, Test MSE of 38421295125.751373
Epoch 37: training loss 45584616711.529
Test Loss of 37390382073.839890, Test MSE of 37390382734.989960
Epoch 38: training loss 43320349790.118
Test Loss of 34323983152.925499, Test MSE of 34323983114.127098
Epoch 39: training loss 41061289795.765
Test Loss of 33474665378.650623, Test MSE of 33474665883.465794
Epoch 40: training loss 38736146078.118
Test Loss of 32851904161.347523, Test MSE of 32851904477.883957
Epoch 41: training loss 37284104508.235
Test Loss of 27112173314.013882, Test MSE of 27112173574.501751
Epoch 42: training loss 35720459602.824
Test Loss of 28081935302.663582, Test MSE of 28081935567.491741
Epoch 43: training loss 33660549376.000
Test Loss of 28592182683.779732, Test MSE of 28592182069.188854
Epoch 44: training loss 32492401054.118
Test Loss of 29064987131.972237, Test MSE of 29064987604.407696
Epoch 45: training loss 31020567183.059
Test Loss of 25680562405.345673, Test MSE of 25680562028.686363
Epoch 46: training loss 29630785776.941
Test Loss of 26325380756.079594, Test MSE of 26325380394.427246
Epoch 47: training loss 28124865889.882
Test Loss of 27233926182.382229, Test MSE of 27233925678.973225
Epoch 48: training loss 27413874970.353
Test Loss of 22509422881.051365, Test MSE of 22509422736.602020
Epoch 49: training loss 25838805669.647
Test Loss of 23096704406.093475, Test MSE of 23096704406.543034
Epoch 50: training loss 25118165718.588
Test Loss of 22647293515.105968, Test MSE of 22647293321.171337
Epoch 51: training loss 24257116449.882
Test Loss of 21819891528.144379, Test MSE of 21819891561.191692
Epoch 52: training loss 23528648297.412
Test Loss of 24006202038.670986, Test MSE of 24006201961.813847
Epoch 53: training loss 22611122093.176
Test Loss of 21421792812.779270, Test MSE of 21421792829.316948
Epoch 54: training loss 21714370703.059
Test Loss of 19798382102.034245, Test MSE of 19798382544.157768
Epoch 55: training loss 20752500736.000
Test Loss of 21611470306.857937, Test MSE of 21611470222.801689
Epoch 56: training loss 20212790038.588
Test Loss of 19824086179.479870, Test MSE of 19824086829.006905
Epoch 57: training loss 19749308348.235
Test Loss of 20548339657.032856, Test MSE of 20548339492.574604
Epoch 58: training loss 19096130300.235
Test Loss of 20378774168.344284, Test MSE of 20378774086.920456
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22524830712.371994, 'MSE - std': 2494621242.782436, 'R2 - mean': 0.8328492952972406, 'R2 - std': 0.014210497881363112} 
 

Saving model.....
Results After CV: {'MSE - mean': 22524830712.371994, 'MSE - std': 2494621242.782436, 'R2 - mean': 0.8328492952972406, 'R2 - std': 0.014210497881363112}
Train time: 90.70778340760008
Inference time: 0.07028293440089328
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 50 finished with value: 22524830712.371994 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004046 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525726810.353
Test Loss of 418112654011.721497, Test MSE of 418112656006.059937
Epoch 2: training loss 427506102753.882
Test Loss of 418095142672.758728, Test MSE of 418095142255.932922
Epoch 3: training loss 427479545615.059
Test Loss of 418072015119.929688, Test MSE of 418072025794.201111
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495529532.235
Test Loss of 418076639728.484863, Test MSE of 418076637752.211670
Epoch 2: training loss 427484733801.412
Test Loss of 418078052413.586853, Test MSE of 418078060756.301025
Epoch 3: training loss 427484273242.353
Test Loss of 418077845633.332397, Test MSE of 418077846590.051147
Epoch 4: training loss 427483908577.882
Test Loss of 418076763084.124939, Test MSE of 418076764336.534607
Epoch 5: training loss 420557445360.941
Test Loss of 396162102304.451538, Test MSE of 396162097032.052429
Epoch 6: training loss 374859868039.529
Test Loss of 329698948427.858459, Test MSE of 329698948880.403931
Epoch 7: training loss 296234001167.059
Test Loss of 244612364622.937775, Test MSE of 244612361027.184113
Epoch 8: training loss 217455588954.353
Test Loss of 173509712277.052032, Test MSE of 173509711189.905090
Epoch 9: training loss 160060903514.353
Test Loss of 126316447969.976410, Test MSE of 126316447047.104248
Epoch 10: training loss 140049736929.882
Test Loss of 118940620445.164932, Test MSE of 118940624177.131729
Epoch 11: training loss 135981440120.471
Test Loss of 115467980420.056442, Test MSE of 115467980428.671112
Epoch 12: training loss 132626475248.941
Test Loss of 112505031285.607224, Test MSE of 112505029102.020035
Epoch 13: training loss 129378944572.235
Test Loss of 109458363744.703217, Test MSE of 109458362526.943420
Epoch 14: training loss 125822134768.941
Test Loss of 106327768629.414764, Test MSE of 106327770181.541031
Epoch 15: training loss 121901237549.176
Test Loss of 101900814205.720108, Test MSE of 101900815191.155243
Epoch 16: training loss 117815136752.941
Test Loss of 100147859474.002319, Test MSE of 100147860116.490387
Epoch 17: training loss 113128533202.824
Test Loss of 96099752848.906784, Test MSE of 96099754151.919937
Epoch 18: training loss 110008377720.471
Test Loss of 92927092630.354843, Test MSE of 92927090625.551376
Epoch 19: training loss 106641172894.118
Test Loss of 89669629610.429794, Test MSE of 89669630014.438461
Epoch 20: training loss 102790632884.706
Test Loss of 86143022696.105484, Test MSE of 86143022123.423584
Epoch 21: training loss 98735319823.059
Test Loss of 84261840989.327774, Test MSE of 84261840046.179276
Epoch 22: training loss 95562675998.118
Test Loss of 81033870715.706680, Test MSE of 81033872092.626846
Epoch 23: training loss 90532999032.471
Test Loss of 76877749093.322235, Test MSE of 76877748657.716049
Epoch 24: training loss 87345633792.000
Test Loss of 73936785954.701828, Test MSE of 73936783855.650558
Epoch 25: training loss 83720591314.824
Test Loss of 71006405428.526489, Test MSE of 71006405803.881210
Epoch 26: training loss 79805416026.353
Test Loss of 68276887580.424706, Test MSE of 68276888097.804276
Epoch 27: training loss 75800240233.412
Test Loss of 63661098745.071480, Test MSE of 63661099192.357025
Epoch 28: training loss 72637987433.412
Test Loss of 62238200115.934303, Test MSE of 62238199367.367218
Epoch 29: training loss 70602544173.176
Test Loss of 57859833499.743698, Test MSE of 57859834843.896034
Epoch 30: training loss 67365854825.412
Test Loss of 54433670029.590561, Test MSE of 54433670917.040947
Epoch 31: training loss 63799604792.471
Test Loss of 55312608915.216286, Test MSE of 55312609128.161270
Epoch 32: training loss 60465965116.235
Test Loss of 49744552231.616936, Test MSE of 49744552191.374382
Epoch 33: training loss 57998927646.118
Test Loss of 47829696955.425400, Test MSE of 47829697587.833130
Epoch 34: training loss 54762703077.647
Test Loss of 46424365104.795746, Test MSE of 46424366184.665573
Epoch 35: training loss 52791569042.824
Test Loss of 40548088668.794815, Test MSE of 40548088337.584381
Epoch 36: training loss 49327001788.235
Test Loss of 43319031038.164238, Test MSE of 43319031894.111633
Epoch 37: training loss 47262158938.353
Test Loss of 37591675939.294006, Test MSE of 37591674818.340637
Epoch 38: training loss 44250442503.529
Test Loss of 36719382827.406891, Test MSE of 36719383033.001427
Epoch 39: training loss 42331166238.118
Test Loss of 35150187707.366180, Test MSE of 35150187524.326920
Epoch 40: training loss 40041392030.118
Test Loss of 32360177170.357620, Test MSE of 32360176747.773163
Epoch 41: training loss 38649954443.294
Test Loss of 31682807648.347908, Test MSE of 31682807011.474285
Epoch 42: training loss 36666914270.118
Test Loss of 29542096236.546841, Test MSE of 29542096188.058403
Epoch 43: training loss 34838114108.235
Test Loss of 27797228177.084431, Test MSE of 27797228748.635410
Epoch 44: training loss 32539156254.118
Test Loss of 25529955505.891277, Test MSE of 25529955822.226486
Epoch 45: training loss 31188016278.588
Test Loss of 25384991983.478142, Test MSE of 25384992085.705082
Epoch 46: training loss 30061492961.882
Test Loss of 24062546051.701134, Test MSE of 24062545739.577660
Epoch 47: training loss 28562211124.706
Test Loss of 26242908899.042332, Test MSE of 26242908857.467133
Epoch 48: training loss 27213661270.588
Test Loss of 24890053190.706455, Test MSE of 24890053391.337540
Epoch 49: training loss 26012611245.176
Test Loss of 26811253188.900299, Test MSE of 26811253384.972679
Epoch 50: training loss 24777634168.471
Test Loss of 22825911931.529030, Test MSE of 22825911523.988316
Epoch 51: training loss 23477880323.765
Test Loss of 22742993292.524635, Test MSE of 22742993922.849987
Epoch 52: training loss 22622442499.765
Test Loss of 19309072894.697201, Test MSE of 19309073253.789902
Epoch 53: training loss 21559889822.118
Test Loss of 21238479666.631504, Test MSE of 21238479640.237408
Epoch 54: training loss 21066141963.294
Test Loss of 19491066609.254684, Test MSE of 19491066607.457569
Epoch 55: training loss 20446200086.588
Test Loss of 20532224357.203793, Test MSE of 20532224022.655975
Epoch 56: training loss 19510104161.882
Test Loss of 18864628266.281750, Test MSE of 18864628381.822662
Epoch 57: training loss 19238217475.765
Test Loss of 20783518701.287067, Test MSE of 20783518810.099689
Epoch 58: training loss 18280330676.706
Test Loss of 21707835803.684479, Test MSE of 21707835786.413395
Epoch 59: training loss 17646406565.647
Test Loss of 18325295004.987278, Test MSE of 18325295114.441368
Epoch 60: training loss 17294970164.706
Test Loss of 18562744365.005783, Test MSE of 18562744338.438435
Epoch 61: training loss 16619820649.412
Test Loss of 19290757082.100391, Test MSE of 19290757118.787842
Epoch 62: training loss 16173917579.294
Test Loss of 18561854718.164238, Test MSE of 18561854910.319489
Epoch 63: training loss 15718163493.647
Test Loss of 20094778826.822113, Test MSE of 20094778926.213226
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20094778926.213226, 'MSE - std': 0.0, 'R2 - mean': 0.8435197762346486, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005417 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917892186.353
Test Loss of 424555438237.283386, Test MSE of 424555435830.889526
Epoch 2: training loss 427896851757.176
Test Loss of 424538587738.603760, Test MSE of 424538585602.135315
Epoch 3: training loss 427869316035.765
Test Loss of 424516289656.568115, Test MSE of 424516297684.494080
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888795888.941
Test Loss of 424523782333.024292, Test MSE of 424523784067.188782
Epoch 2: training loss 427876048896.000
Test Loss of 424524507776.977112, Test MSE of 424524507620.750916
Epoch 3: training loss 427875433050.353
Test Loss of 424524751623.046936, Test MSE of 424524759136.286926
Epoch 4: training loss 427874988272.941
Test Loss of 424524333112.849426, Test MSE of 424524333011.950867
Epoch 5: training loss 420984808869.647
Test Loss of 402784019946.089294, Test MSE of 402784017588.062561
Epoch 6: training loss 375338390226.824
Test Loss of 337128083449.367554, Test MSE of 337128082850.808289
Epoch 7: training loss 296336266541.176
Test Loss of 252406146250.052277, Test MSE of 252406149122.498779
Epoch 8: training loss 217423160741.647
Test Loss of 184314111833.715485, Test MSE of 184314113132.954926
Epoch 9: training loss 156525427952.941
Test Loss of 137639601831.587311, Test MSE of 137639606523.325592
Epoch 10: training loss 138533835023.059
Test Loss of 130209495210.074478, Test MSE of 130209496170.350266
Epoch 11: training loss 133060286403.765
Test Loss of 126818089346.102249, Test MSE of 126818089950.609589
Epoch 12: training loss 130065083783.529
Test Loss of 124073717042.749939, Test MSE of 124073716164.632706
Epoch 13: training loss 126668212133.647
Test Loss of 119795250070.117981, Test MSE of 119795249480.700851
Epoch 14: training loss 123798536944.941
Test Loss of 117169114847.015503, Test MSE of 117169119136.341705
Epoch 15: training loss 120084718652.235
Test Loss of 113555087128.812393, Test MSE of 113555089366.595337
Epoch 16: training loss 115894766019.765
Test Loss of 109746085510.898911, Test MSE of 109746085774.376999
Epoch 17: training loss 112019096485.647
Test Loss of 105784167448.397873, Test MSE of 105784168151.731949
Epoch 18: training loss 107166907904.000
Test Loss of 102833015438.241959, Test MSE of 102833012264.495789
Epoch 19: training loss 103980784248.471
Test Loss of 97438488619.110809, Test MSE of 97438490585.829086
Epoch 20: training loss 100782109846.588
Test Loss of 95172641796.026840, Test MSE of 95172641686.199265
Epoch 21: training loss 96199967924.706
Test Loss of 91087144287.755722, Test MSE of 91087143916.925369
Epoch 22: training loss 91087198629.647
Test Loss of 88186611961.663666, Test MSE of 88186612313.420425
Epoch 23: training loss 88256507000.471
Test Loss of 83819412899.501266, Test MSE of 83819410868.689423
Epoch 24: training loss 83708508491.294
Test Loss of 79885303013.055756, Test MSE of 79885303713.155945
Epoch 25: training loss 80474458895.059
Test Loss of 79336459866.366882, Test MSE of 79336458219.220016
Epoch 26: training loss 76476056997.647
Test Loss of 73472086154.333557, Test MSE of 73472085462.148666
Epoch 27: training loss 73043596001.882
Test Loss of 68968015349.459167, Test MSE of 68968014300.479782
Epoch 28: training loss 70286473185.882
Test Loss of 67787270734.760117, Test MSE of 67787270768.443993
Epoch 29: training loss 66083252540.235
Test Loss of 60908428711.291229, Test MSE of 60908428813.900398
Epoch 30: training loss 62953969859.765
Test Loss of 62377800238.071709, Test MSE of 62377799927.488785
Epoch 31: training loss 59745264835.765
Test Loss of 60231921369.330559, Test MSE of 60231921418.447617
Epoch 32: training loss 56367749601.882
Test Loss of 55715879539.712234, Test MSE of 55715878886.335907
Epoch 33: training loss 54237747200.000
Test Loss of 53497818595.456856, Test MSE of 53497817760.095131
Epoch 34: training loss 51958379008.000
Test Loss of 49450751925.622025, Test MSE of 49450750785.780434
Epoch 35: training loss 48622985803.294
Test Loss of 46956448558.367798, Test MSE of 46956447505.784599
Epoch 36: training loss 46060189214.118
Test Loss of 47272202492.979874, Test MSE of 47272202930.219917
Epoch 37: training loss 43840197165.176
Test Loss of 43343951360.829056, Test MSE of 43343950344.376472
Epoch 38: training loss 40978861191.529
Test Loss of 43079765609.052971, Test MSE of 43079765846.490013
Epoch 39: training loss 38994316777.412
Test Loss of 41536174296.027756, Test MSE of 41536173948.205254
Epoch 40: training loss 36732854467.765
Test Loss of 38632664954.877632, Test MSE of 38632665501.480392
Epoch 41: training loss 34950536771.765
Test Loss of 34532518637.938469, Test MSE of 34532518631.709839
Epoch 42: training loss 32322342098.824
Test Loss of 33067524925.053898, Test MSE of 33067525304.004837
Epoch 43: training loss 30949345987.765
Test Loss of 31025191560.557022, Test MSE of 31025191639.529488
Epoch 44: training loss 29214162243.765
Test Loss of 32874960606.541752, Test MSE of 32874961414.520580
Epoch 45: training loss 27889415905.882
Test Loss of 28919204545.880177, Test MSE of 28919204878.384327
Epoch 46: training loss 26623072248.471
Test Loss of 30147023755.695583, Test MSE of 30147024385.226089
Epoch 47: training loss 25629502049.882
Test Loss of 28569715739.714088, Test MSE of 28569716002.425179
Epoch 48: training loss 23636371591.529
Test Loss of 31420410551.931530, Test MSE of 31420411337.301640
Epoch 49: training loss 22555768041.412
Test Loss of 29934313327.034004, Test MSE of 29934314122.607182
Epoch 50: training loss 21905383755.294
Test Loss of 28110073625.286144, Test MSE of 28110073769.635307
Epoch 51: training loss 20710786567.529
Test Loss of 28061215380.400646, Test MSE of 28061216400.615864
Epoch 52: training loss 19935150317.176
Test Loss of 30085667247.108028, Test MSE of 30085668031.313480
Epoch 53: training loss 18891627245.176
Test Loss of 28380627537.602592, Test MSE of 28380627737.803162
Epoch 54: training loss 18216047823.059
Test Loss of 25411910348.776314, Test MSE of 25411910335.136707
Epoch 55: training loss 17660023454.118
Test Loss of 24561724611.419846, Test MSE of 24561724761.753052
Epoch 56: training loss 16992391239.529
Test Loss of 25687234992.766136, Test MSE of 25687235413.303284
Epoch 57: training loss 16429410849.882
Test Loss of 25132736057.915337, Test MSE of 25132736139.242779
Epoch 58: training loss 15736136256.000
Test Loss of 25188357443.567894, Test MSE of 25188356877.476158
Epoch 59: training loss 15363338213.647
Test Loss of 26053315910.173492, Test MSE of 26053315442.744503
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23074047184.478867, 'MSE - std': 2979268258.2656384, 'R2 - mean': 0.8287582007543881, 'R2 - std': 0.014761575480260525} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005401 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927124871.529
Test Loss of 447258800718.286377, Test MSE of 447258802691.441345
Epoch 2: training loss 421905657615.059
Test Loss of 447239834830.315979, Test MSE of 447239841696.969910
Epoch 3: training loss 421878445839.059
Test Loss of 447214949736.283142, Test MSE of 447214953019.153748
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900756630.588
Test Loss of 447221679933.290771, Test MSE of 447221680562.302734
Epoch 2: training loss 421887637624.471
Test Loss of 447223632536.190613, Test MSE of 447223634655.213623
Epoch 3: training loss 421887174053.647
Test Loss of 447223686687.148743, Test MSE of 447223683857.382812
Epoch 4: training loss 421886838543.059
Test Loss of 447223691265.421265, Test MSE of 447223692686.618408
Epoch 5: training loss 415303631088.941
Test Loss of 425909285351.957458, Test MSE of 425909287714.889282
Epoch 6: training loss 370342251580.235
Test Loss of 358627387786.629639, Test MSE of 358627382343.480103
Epoch 7: training loss 291415068912.941
Test Loss of 271059826117.137177, Test MSE of 271059828313.138916
Epoch 8: training loss 212800702584.471
Test Loss of 199787285812.644928, Test MSE of 199787283538.715363
Epoch 9: training loss 153052045884.235
Test Loss of 148658597663.444824, Test MSE of 148658596903.256927
Epoch 10: training loss 134762644871.529
Test Loss of 140243204711.394867, Test MSE of 140243206276.915070
Epoch 11: training loss 129429827794.824
Test Loss of 136692680967.876007, Test MSE of 136692680672.153488
Epoch 12: training loss 127721046829.176
Test Loss of 133262870594.798050, Test MSE of 133262870478.425278
Epoch 13: training loss 124519707015.529
Test Loss of 129030695325.579453, Test MSE of 129030693927.054001
Epoch 14: training loss 119990740600.471
Test Loss of 124578098967.864914, Test MSE of 124578099890.533401
Epoch 15: training loss 116399810770.824
Test Loss of 121262929004.250748, Test MSE of 121262930063.579376
Epoch 16: training loss 112912344696.471
Test Loss of 117153326016.755035, Test MSE of 117153327559.712219
Epoch 17: training loss 108404329622.588
Test Loss of 115041654548.548691, Test MSE of 115041654631.545456
Epoch 18: training loss 104894591668.706
Test Loss of 110996918686.053207, Test MSE of 110996919733.902908
Epoch 19: training loss 101597640914.824
Test Loss of 106535073247.666901, Test MSE of 106535074328.172409
Epoch 20: training loss 97307612506.353
Test Loss of 103486959315.645615, Test MSE of 103486960951.233398
Epoch 21: training loss 92215275791.059
Test Loss of 98408510972.091599, Test MSE of 98408511653.497421
Epoch 22: training loss 88546983815.529
Test Loss of 95324658823.254227, Test MSE of 95324658760.181610
Epoch 23: training loss 85560503160.471
Test Loss of 93933777978.981262, Test MSE of 93933779132.349655
Epoch 24: training loss 82867359804.235
Test Loss of 86092904807.572525, Test MSE of 86092905765.922501
Epoch 25: training loss 78649014934.588
Test Loss of 84497934715.469818, Test MSE of 84497933403.006104
Epoch 26: training loss 74702346179.765
Test Loss of 79444963916.391388, Test MSE of 79444964593.734604
Epoch 27: training loss 71082385844.706
Test Loss of 75252195796.060150, Test MSE of 75252197476.023254
Epoch 28: training loss 67995208176.941
Test Loss of 71056273035.636368, Test MSE of 71056274712.467712
Epoch 29: training loss 64011583066.353
Test Loss of 69053176249.056671, Test MSE of 69053176966.775909
Epoch 30: training loss 61336272128.000
Test Loss of 64514593056.984505, Test MSE of 64514592241.469391
Epoch 31: training loss 59363568158.118
Test Loss of 61744525309.157532, Test MSE of 61744525596.620102
Epoch 32: training loss 56210220122.353
Test Loss of 58878165187.182976, Test MSE of 58878165672.554115
Epoch 33: training loss 53282805729.882
Test Loss of 55876785629.771919, Test MSE of 55876784590.449493
Epoch 34: training loss 50407769773.176
Test Loss of 51492257980.550545, Test MSE of 51492257604.937698
Epoch 35: training loss 47890610966.588
Test Loss of 50358879380.045341, Test MSE of 50358880085.240158
Epoch 36: training loss 44813495574.588
Test Loss of 49926028981.325928, Test MSE of 49926028148.947311
Epoch 37: training loss 42305048862.118
Test Loss of 46340089839.418922, Test MSE of 46340089614.127686
Epoch 38: training loss 40493858063.059
Test Loss of 43007789174.910019, Test MSE of 43007789400.814278
Epoch 39: training loss 38258283655.529
Test Loss of 43044503821.087212, Test MSE of 43044503587.972328
Epoch 40: training loss 36699602710.588
Test Loss of 39338901213.120522, Test MSE of 39338900870.437912
Epoch 41: training loss 34772640331.294
Test Loss of 37760033819.714088, Test MSE of 37760033712.692841
Epoch 42: training loss 32270082936.471
Test Loss of 36680486982.114273, Test MSE of 36680487042.324547
Epoch 43: training loss 31035624786.824
Test Loss of 32439215687.890816, Test MSE of 32439216513.977634
Epoch 44: training loss 29153842917.647
Test Loss of 33360453542.935925, Test MSE of 33360453904.532516
Epoch 45: training loss 28329393709.176
Test Loss of 33943832702.489937, Test MSE of 33943832770.042332
Epoch 46: training loss 26697886753.882
Test Loss of 30533232840.394169, Test MSE of 30533233746.223904
Epoch 47: training loss 25310009765.647
Test Loss of 34176805500.239647, Test MSE of 34176805197.507713
Epoch 48: training loss 24114225825.882
Test Loss of 29497981823.851955, Test MSE of 29497982578.778461
Epoch 49: training loss 23455273577.412
Test Loss of 26921272556.161926, Test MSE of 26921272369.557869
Epoch 50: training loss 22232903815.529
Test Loss of 29245085862.284523, Test MSE of 29245086029.545807
Epoch 51: training loss 21447763501.176
Test Loss of 32814062463.851955, Test MSE of 32814061753.275200
Epoch 52: training loss 20041171177.412
Test Loss of 30173402679.783485, Test MSE of 30173402225.519928
Epoch 53: training loss 19766728583.529
Test Loss of 23638141329.262085, Test MSE of 23638141714.990940
Epoch 54: training loss 19305263954.824
Test Loss of 26789000405.422161, Test MSE of 26789000422.595329
Epoch 55: training loss 18197684269.176
Test Loss of 26659302198.421467, Test MSE of 26659301985.894989
Epoch 56: training loss 17772812047.059
Test Loss of 23729319614.327087, Test MSE of 23729319413.820549
Epoch 57: training loss 16968039216.941
Test Loss of 25664644167.772381, Test MSE of 25664643963.876129
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23937579444.277954, 'MSE - std': 2721899244.504387, 'R2 - mean': 0.8288895201193855, 'R2 - std': 0.012054206595937058} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005512 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109899595.294
Test Loss of 410765264879.888916, Test MSE of 410765260810.274292
Epoch 2: training loss 430088789293.176
Test Loss of 410746814282.039795, Test MSE of 410746814634.766052
Epoch 3: training loss 430060819877.647
Test Loss of 410722677867.565002, Test MSE of 410722672744.258301
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077824301.176
Test Loss of 410727241217.184631, Test MSE of 410727248948.563599
Epoch 2: training loss 430065476306.824
Test Loss of 410727404647.774170, Test MSE of 410727399170.391357
Epoch 3: training loss 430065023819.294
Test Loss of 410726848804.842224, Test MSE of 410726848213.244690
Epoch 4: training loss 430064719028.706
Test Loss of 410726148424.855164, Test MSE of 410726148422.271973
Epoch 5: training loss 423206004856.471
Test Loss of 388796061025.969482, Test MSE of 388796059178.910339
Epoch 6: training loss 377672148751.059
Test Loss of 322608855253.234619, Test MSE of 322608850083.910706
Epoch 7: training loss 298653726960.941
Test Loss of 236718558456.773712, Test MSE of 236718558858.763153
Epoch 8: training loss 219889016410.353
Test Loss of 167386553560.551605, Test MSE of 167386554202.674072
Epoch 9: training loss 161679226458.353
Test Loss of 119970220887.781586, Test MSE of 119970220568.506226
Epoch 10: training loss 141170077605.647
Test Loss of 112197176477.793610, Test MSE of 112197176689.649857
Epoch 11: training loss 138408212781.176
Test Loss of 109215682865.162430, Test MSE of 109215683015.495926
Epoch 12: training loss 133348498432.000
Test Loss of 107224209966.200836, Test MSE of 107224208577.738312
Epoch 13: training loss 131156913483.294
Test Loss of 104163243289.943542, Test MSE of 104163244539.272659
Epoch 14: training loss 126962587256.471
Test Loss of 100222346971.157791, Test MSE of 100222346736.812088
Epoch 15: training loss 123092532043.294
Test Loss of 97794046358.567337, Test MSE of 97794048960.804138
Epoch 16: training loss 119981397624.471
Test Loss of 95114977014.641373, Test MSE of 95114976540.640884
Epoch 17: training loss 116213344677.647
Test Loss of 92473832809.551132, Test MSE of 92473833520.337463
Epoch 18: training loss 111890887589.647
Test Loss of 88573605752.477554, Test MSE of 88573605609.279205
Epoch 19: training loss 107408518776.471
Test Loss of 85814551938.191574, Test MSE of 85814551759.892334
Epoch 20: training loss 103837000432.941
Test Loss of 83392467143.018967, Test MSE of 83392467009.248215
Epoch 21: training loss 101101673020.235
Test Loss of 79049930313.210556, Test MSE of 79049932067.112305
Epoch 22: training loss 95895878053.647
Test Loss of 77475799021.045807, Test MSE of 77475799905.230499
Epoch 23: training loss 91802388299.294
Test Loss of 73656833726.726517, Test MSE of 73656834072.449066
Epoch 24: training loss 87198601652.706
Test Loss of 71144168402.983810, Test MSE of 71144167529.337692
Epoch 25: training loss 84911118441.412
Test Loss of 68922700930.310043, Test MSE of 68922701139.181458
Epoch 26: training loss 80841254475.294
Test Loss of 64970407663.059692, Test MSE of 64970407892.855461
Epoch 27: training loss 77016400353.882
Test Loss of 63319450907.838966, Test MSE of 63319451077.385979
Epoch 28: training loss 74389102185.412
Test Loss of 58553591151.237389, Test MSE of 58553591306.147850
Epoch 29: training loss 70377850096.941
Test Loss of 57115996017.843590, Test MSE of 57115995614.703598
Epoch 30: training loss 67097603222.588
Test Loss of 55568004440.492363, Test MSE of 55568004318.073494
Epoch 31: training loss 64080988250.353
Test Loss of 52249972849.725128, Test MSE of 52249972089.153114
Epoch 32: training loss 60807420190.118
Test Loss of 50955527863.144844, Test MSE of 50955528018.116882
Epoch 33: training loss 58117384116.706
Test Loss of 48278648609.288292, Test MSE of 48278647644.122681
Epoch 34: training loss 54573801803.294
Test Loss of 46467755124.094398, Test MSE of 46467755387.231819
Epoch 35: training loss 52435175943.529
Test Loss of 44455032342.034241, Test MSE of 44455031477.075989
Epoch 36: training loss 49356397635.765
Test Loss of 41532852742.397041, Test MSE of 41532852039.908188
Epoch 37: training loss 46864887845.647
Test Loss of 42161869501.304955, Test MSE of 42161869647.968170
Epoch 38: training loss 44951398949.647
Test Loss of 36015474869.012497, Test MSE of 36015475045.248817
Epoch 39: training loss 42377937453.176
Test Loss of 36758864696.507172, Test MSE of 36758864436.494522
Epoch 40: training loss 40270458797.176
Test Loss of 33681443264.266544, Test MSE of 33681443402.702873
Epoch 41: training loss 38445514014.118
Test Loss of 33256051135.318832, Test MSE of 33256051128.687733
Epoch 42: training loss 36404114996.706
Test Loss of 34337361906.732067, Test MSE of 34337362448.680363
Epoch 43: training loss 34228615642.353
Test Loss of 31917362879.674225, Test MSE of 31917362682.585140
Epoch 44: training loss 32636484540.235
Test Loss of 29732734542.896809, Test MSE of 29732734631.509640
Epoch 45: training loss 31292503356.235
Test Loss of 28272648321.836185, Test MSE of 28272647919.855194
Epoch 46: training loss 29783186876.235
Test Loss of 27479204148.005554, Test MSE of 27479204352.616489
Epoch 47: training loss 28591110738.824
Test Loss of 27887497641.521519, Test MSE of 27887497844.499393
Epoch 48: training loss 27269662407.529
Test Loss of 27755367870.844978, Test MSE of 27755367592.834110
Epoch 49: training loss 25637777344.000
Test Loss of 23252739679.955578, Test MSE of 23252739792.219471
Epoch 50: training loss 24954168865.882
Test Loss of 24484557662.415550, Test MSE of 24484557255.347298
Epoch 51: training loss 23609452442.353
Test Loss of 23069414921.714020, Test MSE of 23069415002.022964
Epoch 52: training loss 22733621579.294
Test Loss of 22985818586.802406, Test MSE of 22985818510.025188
Epoch 53: training loss 21782447947.294
Test Loss of 22372401206.967144, Test MSE of 22372401034.928478
Epoch 54: training loss 20941470928.941
Test Loss of 21532685342.800556, Test MSE of 21532685752.050404
Epoch 55: training loss 20331819301.647
Test Loss of 21298416325.834335, Test MSE of 21298416147.046494
Epoch 56: training loss 19107603271.529
Test Loss of 19196599848.040722, Test MSE of 19196599780.474525
Epoch 57: training loss 18907524848.941
Test Loss of 20803630311.714947, Test MSE of 20803629950.903522
Epoch 58: training loss 18086859578.353
Test Loss of 20437166460.031467, Test MSE of 20437166418.977119
Epoch 59: training loss 17554302829.176
Test Loss of 19664637358.733921, Test MSE of 19664637296.959858
Epoch 60: training loss 17161667670.588
Test Loss of 19985406587.913002, Test MSE of 19985406799.673515
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22949536283.126846, 'MSE - std': 2912943440.545593, 'R2 - mean': 0.8304296456071527, 'R2 - std': 0.010774687097422337} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042783683.765
Test Loss of 431612297018.876465, Test MSE of 431612301186.845581
Epoch 2: training loss 424022891821.176
Test Loss of 431592515791.548340, Test MSE of 431592510709.449036
Epoch 3: training loss 423995317067.294
Test Loss of 431565194375.996277, Test MSE of 431565194152.937256
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010187474.824
Test Loss of 431567399293.926880, Test MSE of 431567392842.901550
Epoch 2: training loss 423998977927.529
Test Loss of 431568760947.620544, Test MSE of 431568758422.498230
Epoch 3: training loss 423998488576.000
Test Loss of 431569028777.876892, Test MSE of 431569035081.823669
Epoch 4: training loss 423998120417.882
Test Loss of 431568868455.774170, Test MSE of 431568869964.619873
Epoch 5: training loss 417591911122.824
Test Loss of 409945873002.854248, Test MSE of 409945873751.983582
Epoch 6: training loss 373170359235.765
Test Loss of 343696038165.678833, Test MSE of 343696043582.097656
Epoch 7: training loss 295366608655.059
Test Loss of 256574011559.744568, Test MSE of 256574008535.805237
Epoch 8: training loss 216511308016.941
Test Loss of 184720359962.298950, Test MSE of 184720360333.148895
Epoch 9: training loss 158381193456.941
Test Loss of 133051186013.467834, Test MSE of 133051187162.876556
Epoch 10: training loss 137629427260.235
Test Loss of 123852877329.295700, Test MSE of 123852877724.989136
Epoch 11: training loss 133737124984.471
Test Loss of 120146218187.283661, Test MSE of 120146218020.499542
Epoch 12: training loss 131369269699.765
Test Loss of 118064715833.810272, Test MSE of 118064714799.266251
Epoch 13: training loss 127017165462.588
Test Loss of 113803635839.466919, Test MSE of 113803637720.574905
Epoch 14: training loss 123709874025.412
Test Loss of 111618669871.740860, Test MSE of 111618671026.066376
Epoch 15: training loss 120484486249.412
Test Loss of 106890982587.172607, Test MSE of 106890979803.458771
Epoch 16: training loss 116572138526.118
Test Loss of 103904486001.488205, Test MSE of 103904485540.454971
Epoch 17: training loss 112945784350.118
Test Loss of 99772546059.846375, Test MSE of 99772546290.454376
Epoch 18: training loss 108908019952.941
Test Loss of 95407460289.451187, Test MSE of 95407460559.870453
Epoch 19: training loss 104102398614.588
Test Loss of 94086249688.551590, Test MSE of 94086250502.988144
Epoch 20: training loss 101062595388.235
Test Loss of 88243242309.538177, Test MSE of 88243241125.677780
Epoch 21: training loss 96673428766.118
Test Loss of 86266923465.743637, Test MSE of 86266923175.032745
Epoch 22: training loss 93345455766.588
Test Loss of 80461674790.737625, Test MSE of 80461674645.253082
Epoch 23: training loss 89952384632.471
Test Loss of 77820259519.911148, Test MSE of 77820258968.923798
Epoch 24: training loss 85652042224.941
Test Loss of 74245146892.201752, Test MSE of 74245147938.911179
Epoch 25: training loss 82165613793.882
Test Loss of 72230822399.763077, Test MSE of 72230823913.156525
Epoch 26: training loss 78191338616.471
Test Loss of 65770821883.142990, Test MSE of 65770820864.805748
Epoch 27: training loss 74915020664.471
Test Loss of 67244159908.546043, Test MSE of 67244159397.501793
Epoch 28: training loss 71542855062.588
Test Loss of 61477431939.494675, Test MSE of 61477432331.669006
Epoch 29: training loss 68236588483.765
Test Loss of 63849914493.097641, Test MSE of 63849914293.522209
Epoch 30: training loss 64829232609.882
Test Loss of 57684020362.839424, Test MSE of 57684020164.683434
Epoch 31: training loss 62418214806.588
Test Loss of 55153048766.015732, Test MSE of 55153049315.129311
Epoch 32: training loss 59202882243.765
Test Loss of 51722831088.244331, Test MSE of 51722831274.297180
Epoch 33: training loss 56416680161.882
Test Loss of 45281822459.379913, Test MSE of 45281822743.186340
Epoch 34: training loss 52626225392.941
Test Loss of 47129421436.386856, Test MSE of 47129420579.021111
Epoch 35: training loss 51227471570.824
Test Loss of 42189964818.243408, Test MSE of 42189964037.470169
Epoch 36: training loss 48767095469.176
Test Loss of 43776135075.124481, Test MSE of 43776134719.214920
Epoch 37: training loss 46231437786.353
Test Loss of 40953080375.677925, Test MSE of 40953079971.214767
Epoch 38: training loss 43326972495.059
Test Loss of 40583499895.411385, Test MSE of 40583500362.926704
Epoch 39: training loss 41336492732.235
Test Loss of 37529274008.818138, Test MSE of 37529274150.146805
Epoch 40: training loss 38834515335.529
Test Loss of 34943365492.449791, Test MSE of 34943365896.565742
Epoch 41: training loss 37895775472.941
Test Loss of 30825032676.042572, Test MSE of 30825032851.444920
Epoch 42: training loss 35762077515.294
Test Loss of 33911772470.374828, Test MSE of 33911772021.067547
Epoch 43: training loss 33792063887.059
Test Loss of 30311705584.362797, Test MSE of 30311705161.141918
Epoch 44: training loss 31825275813.647
Test Loss of 32109266436.975475, Test MSE of 32109266667.145020
Epoch 45: training loss 30646527433.412
Test Loss of 30368175227.676075, Test MSE of 30368175902.154972
Epoch 46: training loss 29374784451.765
Test Loss of 25191906005.945396, Test MSE of 25191905688.440926
Epoch 47: training loss 27898038057.412
Test Loss of 27374456180.923645, Test MSE of 27374455898.668434
Epoch 48: training loss 26619548114.824
Test Loss of 24839161599.170753, Test MSE of 24839161782.358585
Epoch 49: training loss 25359495506.824
Test Loss of 23694679415.766773, Test MSE of 23694678881.054886
Epoch 50: training loss 24211829424.941
Test Loss of 24737189822.608051, Test MSE of 24737189937.197357
Epoch 51: training loss 23751502957.176
Test Loss of 23606340751.577972, Test MSE of 23606339989.654594
Epoch 52: training loss 22209316246.588
Test Loss of 26170626211.953724, Test MSE of 26170626747.493366
Epoch 53: training loss 21454462125.176
Test Loss of 24065520069.952801, Test MSE of 24065519900.926502
Epoch 54: training loss 20873195896.471
Test Loss of 23340409950.297085, Test MSE of 23340409852.975719
Epoch 55: training loss 20055583032.471
Test Loss of 21481408583.078205, Test MSE of 21481408982.757664
Epoch 56: training loss 19470316393.412
Test Loss of 21526280302.882000, Test MSE of 21526280262.845161
Epoch 57: training loss 18805861248.000
Test Loss of 19720415084.157333, Test MSE of 19720415212.818203
Epoch 58: training loss 18450997394.824
Test Loss of 20708297682.036095, Test MSE of 20708297859.563374
Epoch 59: training loss 18107238832.941
Test Loss of 20657155896.033318, Test MSE of 20657155729.085339
Epoch 60: training loss 17590013093.647
Test Loss of 21851803592.085144, Test MSE of 21851803246.653481
Epoch 61: training loss 17008486667.294
Test Loss of 21214071262.593243, Test MSE of 21214071060.924522
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22602443238.686382, 'MSE - std': 2696309684.782124, 'R2 - mean': 0.832658222425153, 'R2 - std': 0.010617971754191102} 
 

Saving model.....
Results After CV: {'MSE - mean': 22602443238.686382, 'MSE - std': 2696309684.782124, 'R2 - mean': 0.832658222425153, 'R2 - std': 0.010617971754191102}
Train time: 90.26955646619972
Inference time: 0.07048386059977929
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 51 finished with value: 22602443238.686382 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003918 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525629470.118
Test Loss of 418111916455.054382, Test MSE of 418111915591.761536
Epoch 2: training loss 427505524254.118
Test Loss of 418094633506.701843, Test MSE of 418094631708.993225
Epoch 3: training loss 427478754484.706
Test Loss of 418071300159.008118, Test MSE of 418071306089.432617
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492920259.765
Test Loss of 418076782670.641663, Test MSE of 418076779697.076111
Epoch 2: training loss 427483820754.824
Test Loss of 418077887300.160095, Test MSE of 418077880696.892517
Epoch 3: training loss 427483415371.294
Test Loss of 418076982431.888977, Test MSE of 418076986544.958130
Epoch 4: training loss 427483079619.765
Test Loss of 418076675990.828613, Test MSE of 418076682178.980774
Epoch 5: training loss 421113253165.176
Test Loss of 397523671304.823486, Test MSE of 397523667362.974182
Epoch 6: training loss 376723623454.118
Test Loss of 331329025750.014343, Test MSE of 331329024653.196960
Epoch 7: training loss 298170936139.294
Test Loss of 246312768311.368958, Test MSE of 246312765064.679688
Epoch 8: training loss 218725846497.882
Test Loss of 175557754989.908875, Test MSE of 175557753775.063782
Epoch 9: training loss 160447459297.882
Test Loss of 125996080210.194778, Test MSE of 125996078367.247253
Epoch 10: training loss 139529791186.824
Test Loss of 118324197070.434418, Test MSE of 118324199583.875107
Epoch 11: training loss 135995772310.588
Test Loss of 115585150164.000931, Test MSE of 115585150213.318848
Epoch 12: training loss 133655167096.471
Test Loss of 112890524242.076340, Test MSE of 112890522793.694824
Epoch 13: training loss 129122431744.000
Test Loss of 109412342297.700668, Test MSE of 109412342101.770157
Epoch 14: training loss 126445939139.765
Test Loss of 106271580919.887115, Test MSE of 106271580431.505264
Epoch 15: training loss 122298796619.294
Test Loss of 102647362384.003708, Test MSE of 102647364679.131210
Epoch 16: training loss 118250950053.647
Test Loss of 100068779939.619705, Test MSE of 100068778461.939743
Epoch 17: training loss 114182695589.647
Test Loss of 96338833865.637756, Test MSE of 96338832547.954285
Epoch 18: training loss 110040182121.412
Test Loss of 92664190858.037476, Test MSE of 92664191701.979279
Epoch 19: training loss 107045428675.765
Test Loss of 89452228095.407822, Test MSE of 89452229870.629425
Epoch 20: training loss 102681042898.824
Test Loss of 85061336576.355316, Test MSE of 85061336033.122025
Epoch 21: training loss 98016420668.235
Test Loss of 83394664993.280594, Test MSE of 83394665078.766418
Epoch 22: training loss 95238319375.059
Test Loss of 81789010196.667130, Test MSE of 81789010090.681580
Epoch 23: training loss 90874258160.941
Test Loss of 79034232677.559097, Test MSE of 79034233033.984589
Epoch 24: training loss 87116029259.294
Test Loss of 75027523933.150131, Test MSE of 75027524520.934479
Epoch 25: training loss 83146372577.882
Test Loss of 69980914655.074722, Test MSE of 69980913765.632477
Epoch 26: training loss 80422242145.882
Test Loss of 66873358534.499191, Test MSE of 66873358616.936058
Epoch 27: training loss 76642505984.000
Test Loss of 65351732941.960678, Test MSE of 65351731917.669647
Epoch 28: training loss 73105658315.294
Test Loss of 62810113021.394402, Test MSE of 62810113593.705055
Epoch 29: training loss 69408860596.706
Test Loss of 59025248956.195236, Test MSE of 59025250911.254791
Epoch 30: training loss 67401233897.412
Test Loss of 57841444218.285446, Test MSE of 57841444469.454720
Epoch 31: training loss 63318989778.824
Test Loss of 53051650505.637749, Test MSE of 53051650903.921150
Epoch 32: training loss 60284058917.647
Test Loss of 53037075821.494331, Test MSE of 53037075674.564987
Epoch 33: training loss 57986843843.765
Test Loss of 52136525238.451073, Test MSE of 52136525602.215057
Epoch 34: training loss 54711499727.059
Test Loss of 50866873296.862366, Test MSE of 50866874191.414513
Epoch 35: training loss 51979677221.647
Test Loss of 46265972902.284523, Test MSE of 46265972412.061142
Epoch 36: training loss 49596793449.412
Test Loss of 44478385477.699745, Test MSE of 44478386108.473915
Epoch 37: training loss 47130291817.412
Test Loss of 37003983488.977097, Test MSE of 37003983374.781525
Epoch 38: training loss 44937867768.471
Test Loss of 42213471952.329399, Test MSE of 42213471721.863480
Epoch 39: training loss 43091634725.647
Test Loss of 39116766195.445755, Test MSE of 39116765910.401001
Epoch 40: training loss 40634199913.412
Test Loss of 36413050641.232475, Test MSE of 36413050948.772713
Epoch 41: training loss 38349645664.000
Test Loss of 33742677186.472359, Test MSE of 33742677360.068298
Epoch 42: training loss 36625853266.824
Test Loss of 32701692482.442749, Test MSE of 32701692769.746387
Epoch 43: training loss 34872713404.235
Test Loss of 31290496741.174183, Test MSE of 31290496584.971905
Epoch 44: training loss 33187079318.588
Test Loss of 30302955907.049732, Test MSE of 30302956214.403324
Epoch 45: training loss 31740238994.824
Test Loss of 28996934755.249596, Test MSE of 28996934626.723183
Epoch 46: training loss 29779836683.294
Test Loss of 28129044092.476521, Test MSE of 28129044098.614292
Epoch 47: training loss 28361458115.765
Test Loss of 29425133502.149433, Test MSE of 29425133056.962807
Epoch 48: training loss 27041888613.647
Test Loss of 26789346642.017117, Test MSE of 26789346585.394814
Epoch 49: training loss 25924921434.353
Test Loss of 24929891233.014111, Test MSE of 24929891442.030724
Epoch 50: training loss 24958034108.235
Test Loss of 24020161758.897060, Test MSE of 24020162197.439686
Epoch 51: training loss 23788302275.765
Test Loss of 23062529859.923203, Test MSE of 23062529919.266216
Epoch 52: training loss 23096582848.000
Test Loss of 23591687429.270412, Test MSE of 23591687546.562313
Epoch 53: training loss 21899600670.118
Test Loss of 24622004968.253529, Test MSE of 24622005408.704712
Epoch 54: training loss 21332732344.471
Test Loss of 22663045138.002312, Test MSE of 22663044685.002514
Epoch 55: training loss 20766549007.059
Test Loss of 21553955441.817257, Test MSE of 21553955463.149967
Epoch 56: training loss 19611159766.588
Test Loss of 21247445447.742771, Test MSE of 21247445674.111614
Epoch 57: training loss 18784575762.824
Test Loss of 20227309612.058292, Test MSE of 20227309085.008316
Epoch 58: training loss 18558987734.588
Test Loss of 19758367637.170483, Test MSE of 19758367847.780491
Epoch 59: training loss 17749431209.412
Test Loss of 21475470318.708305, Test MSE of 21475470329.041870
Epoch 60: training loss 17484775111.529
Test Loss of 19915331645.586861, Test MSE of 19915331583.298569
Epoch 61: training loss 16694366200.471
Test Loss of 20000054574.012493, Test MSE of 20000054780.213459
Epoch 62: training loss 16333298496.000
Test Loss of 18346750703.833450, Test MSE of 18346750382.279118
Epoch 63: training loss 15861022595.765
Test Loss of 20138972254.512146, Test MSE of 20138972168.735203
Epoch 64: training loss 15126419271.529
Test Loss of 20147116705.665508, Test MSE of 20147116685.242153
Epoch 65: training loss 14758888376.471
Test Loss of 18405263001.374969, Test MSE of 18405263436.439747
Epoch 66: training loss 14540381246.118
Test Loss of 18777461545.630348, Test MSE of 18777461306.817337
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18777461306.817337, 'MSE - std': 0.0, 'R2 - mean': 0.8537778714647589, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005511 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918001091.765
Test Loss of 424557585742.700928, Test MSE of 424557592840.293640
Epoch 2: training loss 427897623250.824
Test Loss of 424541618115.360657, Test MSE of 424541621728.192139
Epoch 3: training loss 427870232094.118
Test Loss of 424520280083.423523, Test MSE of 424520286791.651978
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889274157.176
Test Loss of 424528977921.894958, Test MSE of 424528980578.658447
Epoch 2: training loss 427879872873.412
Test Loss of 424530337418.688904, Test MSE of 424530334124.101440
Epoch 3: training loss 427879286181.647
Test Loss of 424530491639.768677, Test MSE of 424530486555.707092
Epoch 4: training loss 427878861643.294
Test Loss of 424530026596.433960, Test MSE of 424530024542.303101
Epoch 5: training loss 421192516547.765
Test Loss of 403191315824.336792, Test MSE of 403191326068.194214
Epoch 6: training loss 375965993682.824
Test Loss of 338451858181.151978, Test MSE of 338451864578.567017
Epoch 7: training loss 296893287122.824
Test Loss of 253915296152.368256, Test MSE of 253915299033.277863
Epoch 8: training loss 216878491768.471
Test Loss of 184364988049.795044, Test MSE of 184364988182.367676
Epoch 9: training loss 156986604664.471
Test Loss of 136329354231.235718, Test MSE of 136329353043.262054
Epoch 10: training loss 138358586849.882
Test Loss of 130408347801.019669, Test MSE of 130408347842.858749
Epoch 11: training loss 134016391830.588
Test Loss of 126684035547.166321, Test MSE of 126684033652.347229
Epoch 12: training loss 129722067456.000
Test Loss of 124303538952.468201, Test MSE of 124303540567.554749
Epoch 13: training loss 127382948442.353
Test Loss of 120725659989.096466, Test MSE of 120725661215.417908
Epoch 14: training loss 123053646576.941
Test Loss of 117334995788.213745, Test MSE of 117334992652.173599
Epoch 15: training loss 119252351879.529
Test Loss of 113853242190.582458, Test MSE of 113853245953.519119
Epoch 16: training loss 115812690432.000
Test Loss of 110386093670.684250, Test MSE of 110386095618.677963
Epoch 17: training loss 111541972239.059
Test Loss of 106410087598.575058, Test MSE of 106410085891.884903
Epoch 18: training loss 107274138744.471
Test Loss of 104593151282.276199, Test MSE of 104593149695.781601
Epoch 19: training loss 103606359943.529
Test Loss of 99825454311.898224, Test MSE of 99825453199.246552
Epoch 20: training loss 99515496658.824
Test Loss of 95517836078.841553, Test MSE of 95517839197.516891
Epoch 21: training loss 96278323440.941
Test Loss of 92381712194.501968, Test MSE of 92381709905.941742
Epoch 22: training loss 91237118479.059
Test Loss of 86743528756.171173, Test MSE of 86743527156.762619
Epoch 23: training loss 88355265340.235
Test Loss of 86074923170.494568, Test MSE of 86074924946.877762
Epoch 24: training loss 83923341613.176
Test Loss of 81354093950.075409, Test MSE of 81354093000.090042
Epoch 25: training loss 79942277797.647
Test Loss of 74069180633.922745, Test MSE of 74069179699.819244
Epoch 26: training loss 76851837364.706
Test Loss of 74403291514.048584, Test MSE of 74403290691.249313
Epoch 27: training loss 74579381037.176
Test Loss of 67509152734.600975, Test MSE of 67509152931.600899
Epoch 28: training loss 69879071623.529
Test Loss of 66583282863.522552, Test MSE of 66583282424.668633
Epoch 29: training loss 66628811369.412
Test Loss of 64455008543.089523, Test MSE of 64455007724.058853
Epoch 30: training loss 64109353758.118
Test Loss of 62383471560.808701, Test MSE of 62383472645.203873
Epoch 31: training loss 60368845824.000
Test Loss of 55420744923.580849, Test MSE of 55420745738.828072
Epoch 32: training loss 57617217942.588
Test Loss of 53991160380.284065, Test MSE of 53991159095.780281
Epoch 33: training loss 54775962112.000
Test Loss of 51314437088.022209, Test MSE of 51314439617.371490
Epoch 34: training loss 51846574727.529
Test Loss of 49620499153.987511, Test MSE of 49620498405.801224
Epoch 35: training loss 48815735085.176
Test Loss of 45091804497.069626, Test MSE of 45091805019.645569
Epoch 36: training loss 46116567559.529
Test Loss of 43530210721.132545, Test MSE of 43530209550.927635
Epoch 37: training loss 43977044111.059
Test Loss of 44285380262.166092, Test MSE of 44285381292.578041
Epoch 38: training loss 41541040624.941
Test Loss of 41456419996.099007, Test MSE of 41456419315.818451
Epoch 39: training loss 39300466108.235
Test Loss of 39852793336.301643, Test MSE of 39852794481.675446
Epoch 40: training loss 37095349225.412
Test Loss of 37629672976.936386, Test MSE of 37629673049.831947
Epoch 41: training loss 35069874281.412
Test Loss of 33123564428.169327, Test MSE of 33123564854.965115
Epoch 42: training loss 33395018910.118
Test Loss of 34124566883.782558, Test MSE of 34124567405.468281
Epoch 43: training loss 31608692336.941
Test Loss of 32170721255.365257, Test MSE of 32170722648.791561
Epoch 44: training loss 29911825536.000
Test Loss of 32578079000.457088, Test MSE of 32578079155.417858
Epoch 45: training loss 28111288666.353
Test Loss of 31170443801.226925, Test MSE of 31170444599.758553
Epoch 46: training loss 26829768892.235
Test Loss of 29423209571.249596, Test MSE of 29423210243.981899
Epoch 47: training loss 25363632662.588
Test Loss of 28531746529.857967, Test MSE of 28531746725.584961
Epoch 48: training loss 24341310945.882
Test Loss of 26905203815.513302, Test MSE of 26905203738.134426
Epoch 49: training loss 23135541330.824
Test Loss of 27582424212.045338, Test MSE of 27582424097.870296
Epoch 50: training loss 22144644009.412
Test Loss of 26495667479.272728, Test MSE of 26495668016.972500
Epoch 51: training loss 21040837161.412
Test Loss of 26602954913.310200, Test MSE of 26602955120.102322
Epoch 52: training loss 19654841886.118
Test Loss of 23698913177.907936, Test MSE of 23698913883.597271
Epoch 53: training loss 19493278320.941
Test Loss of 26373564128.673607, Test MSE of 26373565983.195473
Epoch 54: training loss 18505325259.294
Test Loss of 26429934180.552395, Test MSE of 26429934042.037724
Epoch 55: training loss 17663527751.529
Test Loss of 24646630593.998611, Test MSE of 24646630257.871181
Epoch 56: training loss 17217225227.294
Test Loss of 24829643787.606754, Test MSE of 24829643414.596775
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21803552360.707054, 'MSE - std': 3026091053.889719, 'R2 - mean': 0.8382553512946415, 'R2 - std': 0.01552252017011746} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005529 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926240256.000
Test Loss of 447258798413.990295, Test MSE of 447258800556.226318
Epoch 2: training loss 421904653010.824
Test Loss of 447239419132.269287, Test MSE of 447239417404.595703
Epoch 3: training loss 421877134516.706
Test Loss of 447214106621.631287, Test MSE of 447214106633.213318
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421894216523.294
Test Loss of 447220190251.347656, Test MSE of 447220188621.202942
Epoch 2: training loss 421886404126.118
Test Loss of 447222113338.033752, Test MSE of 447222116894.390381
Epoch 3: training loss 421886049942.588
Test Loss of 447222921994.363159, Test MSE of 447222933501.316956
Epoch 4: training loss 421885690699.294
Test Loss of 447222664868.271118, Test MSE of 447222669314.226257
Epoch 5: training loss 414869569536.000
Test Loss of 424703577589.932922, Test MSE of 424703578106.684082
Epoch 6: training loss 368805323836.235
Test Loss of 356865887703.613220, Test MSE of 356865885716.006592
Epoch 7: training loss 290274177144.471
Test Loss of 269793038492.335876, Test MSE of 269793044751.829407
Epoch 8: training loss 211908975736.471
Test Loss of 198718716000.170258, Test MSE of 198718713256.659485
Epoch 9: training loss 153646863811.765
Test Loss of 148445525915.092285, Test MSE of 148445526135.678528
Epoch 10: training loss 134885876675.765
Test Loss of 140036543507.186676, Test MSE of 140036543820.780640
Epoch 11: training loss 131707612099.765
Test Loss of 135987470110.497345, Test MSE of 135987472267.194824
Epoch 12: training loss 127305607228.235
Test Loss of 134170072587.251450, Test MSE of 134170076980.042847
Epoch 13: training loss 124816530642.824
Test Loss of 129464261433.737686, Test MSE of 129464259884.853836
Epoch 14: training loss 119745871811.765
Test Loss of 125981873265.935699, Test MSE of 125981874477.955841
Epoch 15: training loss 117286630430.118
Test Loss of 123559841626.189224, Test MSE of 123559841005.016312
Epoch 16: training loss 113207094151.529
Test Loss of 119962468094.282669, Test MSE of 119962468124.635269
Epoch 17: training loss 109354810066.824
Test Loss of 116471676810.511215, Test MSE of 116471676113.740509
Epoch 18: training loss 104493907907.765
Test Loss of 112112965991.572525, Test MSE of 112112964905.094467
Epoch 19: training loss 100619769675.294
Test Loss of 107526913475.715942, Test MSE of 107526915809.842712
Epoch 20: training loss 96611574241.882
Test Loss of 104306372823.080261, Test MSE of 104306370456.034286
Epoch 21: training loss 93543633468.235
Test Loss of 97792626995.934311, Test MSE of 97792626726.426712
Epoch 22: training loss 88451177336.471
Test Loss of 96931168299.584549, Test MSE of 96931170342.923065
Epoch 23: training loss 84938535062.588
Test Loss of 93148200829.246353, Test MSE of 93148199984.172134
Epoch 24: training loss 81891150260.706
Test Loss of 90278567314.446442, Test MSE of 90278568322.325241
Epoch 25: training loss 78551040858.353
Test Loss of 85578223209.526718, Test MSE of 85578221782.610825
Epoch 26: training loss 74517541270.588
Test Loss of 78383793676.198929, Test MSE of 78383793365.856674
Epoch 27: training loss 71240581315.765
Test Loss of 78980803000.109177, Test MSE of 78980803559.288635
Epoch 28: training loss 67185455796.706
Test Loss of 74553901497.530426, Test MSE of 74553902071.009216
Epoch 29: training loss 64533033818.353
Test Loss of 72947187298.894287, Test MSE of 72947187208.358841
Epoch 30: training loss 60522665682.824
Test Loss of 64303109607.246819, Test MSE of 64303109214.512047
Epoch 31: training loss 57696319322.353
Test Loss of 62669948604.432106, Test MSE of 62669948048.950943
Epoch 32: training loss 55437761807.059
Test Loss of 59744371807.933380, Test MSE of 59744371510.496910
Epoch 33: training loss 52628535777.882
Test Loss of 53753927455.918571, Test MSE of 53753927033.189194
Epoch 34: training loss 49141081449.412
Test Loss of 56002376592.433029, Test MSE of 56002377357.013306
Epoch 35: training loss 46869490304.000
Test Loss of 52044645047.457787, Test MSE of 52044646324.369057
Epoch 36: training loss 44529968120.471
Test Loss of 48092626724.182281, Test MSE of 48092626771.102356
Epoch 37: training loss 42464243041.882
Test Loss of 46590402694.543602, Test MSE of 46590403244.986351
Epoch 38: training loss 40008424327.529
Test Loss of 45981497205.192688, Test MSE of 45981497986.122208
Epoch 39: training loss 37542156672.000
Test Loss of 45767986286.619476, Test MSE of 45767986098.647705
Epoch 40: training loss 35716711582.118
Test Loss of 40753846183.409668, Test MSE of 40753846278.109879
Epoch 41: training loss 34411058048.000
Test Loss of 41545871820.480225, Test MSE of 41545871936.810043
Epoch 42: training loss 32358945031.529
Test Loss of 42238415745.273193, Test MSE of 42238415287.287971
Epoch 43: training loss 30656139941.647
Test Loss of 35960035342.686096, Test MSE of 35960034749.346130
Epoch 44: training loss 28703196619.294
Test Loss of 35100055703.124680, Test MSE of 35100056186.566574
Epoch 45: training loss 27681510889.412
Test Loss of 34099525851.343975, Test MSE of 34099525889.026226
Epoch 46: training loss 26563770880.000
Test Loss of 33812115489.162155, Test MSE of 33812115688.170692
Epoch 47: training loss 25214179772.235
Test Loss of 31030512662.502892, Test MSE of 31030512789.916935
Epoch 48: training loss 24005559702.588
Test Loss of 31608460609.436039, Test MSE of 31608460706.641552
Epoch 49: training loss 23175867828.706
Test Loss of 31097816811.806614, Test MSE of 31097816394.158237
Epoch 50: training loss 21795719951.059
Test Loss of 26162295632.003700, Test MSE of 26162295913.491924
Epoch 51: training loss 21305681562.353
Test Loss of 26673791275.880638, Test MSE of 26673791377.434299
Epoch 52: training loss 20213285266.824
Test Loss of 28910870918.365948, Test MSE of 28910870527.913010
Epoch 53: training loss 19249864143.059
Test Loss of 25958713290.466805, Test MSE of 25958713283.444588
Epoch 54: training loss 18656742362.353
Test Loss of 27981255379.408745, Test MSE of 27981255795.022106
Epoch 55: training loss 17996337347.765
Test Loss of 26299376598.073559, Test MSE of 26299376923.261700
Epoch 56: training loss 17438084355.765
Test Loss of 22112427720.275734, Test MSE of 22112427872.166805
Epoch 57: training loss 17293999439.059
Test Loss of 26023496809.882027, Test MSE of 26023497305.584259
Epoch 58: training loss 16647395136.000
Test Loss of 22082842409.393478, Test MSE of 22082842463.318596
Epoch 59: training loss 16247186846.118
Test Loss of 25710135747.479065, Test MSE of 25710135931.984581
Epoch 60: training loss 15505788709.647
Test Loss of 26676418654.275272, Test MSE of 26676419140.547047
Epoch 61: training loss 15291759913.412
Test Loss of 24452514345.808002, Test MSE of 24452514835.600372
Epoch 62: training loss 14400937016.471
Test Loss of 21249020398.116123, Test MSE of 21249020894.342564
Epoch 63: training loss 14104089148.235
Test Loss of 23007198703.774231, Test MSE of 23007198644.171444
Epoch 64: training loss 13784802085.647
Test Loss of 21578017481.933842, Test MSE of 21578017911.873287
Epoch 65: training loss 13427171376.941
Test Loss of 21670975107.582699, Test MSE of 21670975611.554859
Epoch 66: training loss 13051428192.000
Test Loss of 20989182249.985657, Test MSE of 20989182446.840343
Epoch 67: training loss 12946465566.118
Test Loss of 24637989782.117973, Test MSE of 24637989404.063129
Epoch 68: training loss 12385538906.353
Test Loss of 22629461029.188988, Test MSE of 22629460910.372135
Epoch 69: training loss 12198670217.412
Test Loss of 20527117711.130234, Test MSE of 20527117691.226589
Epoch 70: training loss 11937765805.176
Test Loss of 21878624077.398102, Test MSE of 21878624218.954056
Epoch 71: training loss 11588045278.118
Test Loss of 23413877541.366642, Test MSE of 23413877521.565552
Epoch 72: training loss 11439663902.118
Test Loss of 21951561722.315060, Test MSE of 21951561499.254532
Epoch 73: training loss 10857136009.412
Test Loss of 21396973916.676384, Test MSE of 21396974211.202976
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21668026310.87236, 'MSE - std': 2478215621.09821, 'R2 - mean': 0.844690819113627, 'R2 - std': 0.015603298168000577} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005484 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110208722.824
Test Loss of 410765837554.139771, Test MSE of 410765837462.226135
Epoch 2: training loss 430089204916.706
Test Loss of 410748028836.072205, Test MSE of 410748030242.225159
Epoch 3: training loss 430062040244.706
Test Loss of 410724733077.738098, Test MSE of 410724735535.168274
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077914895.059
Test Loss of 410730607935.851929, Test MSE of 410730610456.717346
Epoch 2: training loss 430067737780.706
Test Loss of 410731717316.886597, Test MSE of 410731720533.797180
Epoch 3: training loss 430067283365.647
Test Loss of 410731171436.275818, Test MSE of 410731166661.207153
Epoch 4: training loss 430066969660.235
Test Loss of 410730869988.871826, Test MSE of 410730867538.244934
Epoch 5: training loss 423245456805.647
Test Loss of 389164891128.892151, Test MSE of 389164896901.771057
Epoch 6: training loss 377827010319.059
Test Loss of 323115436372.227661, Test MSE of 323115433481.541443
Epoch 7: training loss 299392755109.647
Test Loss of 237355156858.609894, Test MSE of 237355156248.268341
Epoch 8: training loss 220821786985.412
Test Loss of 167567900929.776947, Test MSE of 167567905585.625031
Epoch 9: training loss 160906516058.353
Test Loss of 120224025765.375290, Test MSE of 120224028118.365738
Epoch 10: training loss 141492677692.235
Test Loss of 112766594891.935211, Test MSE of 112766595058.106293
Epoch 11: training loss 137697208169.412
Test Loss of 109555915947.061539, Test MSE of 109555913454.985794
Epoch 12: training loss 134430228299.294
Test Loss of 107441288690.021286, Test MSE of 107441287031.705811
Epoch 13: training loss 131135577810.824
Test Loss of 104569118973.986115, Test MSE of 104569118314.578766
Epoch 14: training loss 128291999472.941
Test Loss of 100827324791.292923, Test MSE of 100827325204.002579
Epoch 15: training loss 122868513460.706
Test Loss of 98524658229.782501, Test MSE of 98524659474.404938
Epoch 16: training loss 118742420630.588
Test Loss of 95006562922.854233, Test MSE of 95006563300.057465
Epoch 17: training loss 116770035892.706
Test Loss of 91832566673.591858, Test MSE of 91832567422.425201
Epoch 18: training loss 110614449724.235
Test Loss of 89170721265.547424, Test MSE of 89170719548.215149
Epoch 19: training loss 107422624617.412
Test Loss of 85678786255.785278, Test MSE of 85678787162.831741
Epoch 20: training loss 103598449362.824
Test Loss of 82556235269.923187, Test MSE of 82556235698.999191
Epoch 21: training loss 100555505061.647
Test Loss of 79402442649.647385, Test MSE of 79402442706.393997
Epoch 22: training loss 95839145999.059
Test Loss of 76435973168.807037, Test MSE of 76435972867.033966
Epoch 23: training loss 92586898703.059
Test Loss of 71230869562.757980, Test MSE of 71230868676.325058
Epoch 24: training loss 87828912820.706
Test Loss of 69612489900.956970, Test MSE of 69612490108.612564
Epoch 25: training loss 84811720282.353
Test Loss of 68482873890.828323, Test MSE of 68482875276.490776
Epoch 26: training loss 80666952192.000
Test Loss of 62943906674.317444, Test MSE of 62943907484.009155
Epoch 27: training loss 76943973586.824
Test Loss of 61917320837.863953, Test MSE of 61917321170.300110
Epoch 28: training loss 73230643049.412
Test Loss of 59601530753.006943, Test MSE of 59601530308.867317
Epoch 29: training loss 70718663664.941
Test Loss of 57935783434.661728, Test MSE of 57935784066.731216
Epoch 30: training loss 67691067407.059
Test Loss of 52655611777.006943, Test MSE of 52655611708.719460
Epoch 31: training loss 64260643945.412
Test Loss of 52184617420.586769, Test MSE of 52184616449.848022
Epoch 32: training loss 61463988781.176
Test Loss of 48908345408.444237, Test MSE of 48908345883.540161
Epoch 33: training loss 57441378891.294
Test Loss of 46823857567.570572, Test MSE of 46823857285.004051
Epoch 34: training loss 54917690232.471
Test Loss of 44052669693.512260, Test MSE of 44052669057.269608
Epoch 35: training loss 52265220751.059
Test Loss of 41790480812.364647, Test MSE of 41790480823.150902
Epoch 36: training loss 49707220028.235
Test Loss of 41728353304.166588, Test MSE of 41728353116.315605
Epoch 37: training loss 47362793283.765
Test Loss of 40429541375.052292, Test MSE of 40429541966.360001
Epoch 38: training loss 44667925586.824
Test Loss of 36813310544.318375, Test MSE of 36813310322.759583
Epoch 39: training loss 42920129965.176
Test Loss of 37616756505.706619, Test MSE of 37616756314.574524
Epoch 40: training loss 40312443753.412
Test Loss of 32009329319.507637, Test MSE of 32009330114.591747
Epoch 41: training loss 38371805778.824
Test Loss of 30575137213.423416, Test MSE of 30575136591.784203
Epoch 42: training loss 36149014204.235
Test Loss of 31173124181.293846, Test MSE of 31173124326.563293
Epoch 43: training loss 33959794575.059
Test Loss of 29253317314.991207, Test MSE of 29253317418.220169
Epoch 44: training loss 32645470479.059
Test Loss of 28365260048.466450, Test MSE of 28365259685.204781
Epoch 45: training loss 31573832331.294
Test Loss of 28209160633.632576, Test MSE of 28209160552.020195
Epoch 46: training loss 29817769496.471
Test Loss of 27425732835.450256, Test MSE of 27425732584.858414
Epoch 47: training loss 28630046870.588
Test Loss of 25906862870.389633, Test MSE of 25906862398.875759
Epoch 48: training loss 27597077240.471
Test Loss of 25348466013.230911, Test MSE of 25348465954.200874
Epoch 49: training loss 25674487951.059
Test Loss of 23463959645.349377, Test MSE of 23463959919.906086
Epoch 50: training loss 25092390674.824
Test Loss of 21920825701.286442, Test MSE of 21920825361.144051
Epoch 51: training loss 23865049347.765
Test Loss of 21409647857.192039, Test MSE of 21409647650.767918
Epoch 52: training loss 22978442017.882
Test Loss of 22500669539.509487, Test MSE of 22500669496.669613
Epoch 53: training loss 22166702234.353
Test Loss of 21826078247.093014, Test MSE of 21826078474.482220
Epoch 54: training loss 21371369204.706
Test Loss of 20843429594.210087, Test MSE of 20843429565.157753
Epoch 55: training loss 20600954533.647
Test Loss of 22510495772.431282, Test MSE of 22510495805.529972
Epoch 56: training loss 19702486921.412
Test Loss of 21519918996.908840, Test MSE of 21519918961.863907
Epoch 57: training loss 19025896079.059
Test Loss of 19841080657.858398, Test MSE of 19841080349.254322
Epoch 58: training loss 18535103634.824
Test Loss of 20768589824.947708, Test MSE of 20768589738.613918
Epoch 59: training loss 18099877961.412
Test Loss of 18915786487.589081, Test MSE of 18915786444.325924
Epoch 60: training loss 17520167634.824
Test Loss of 22163150909.601112, Test MSE of 22163150438.610016
Epoch 61: training loss 17099543100.235
Test Loss of 19628610657.140213, Test MSE of 19628610501.648136
Epoch 62: training loss 16324152820.706
Test Loss of 20262340191.955578, Test MSE of 20262339965.227310
Epoch 63: training loss 15689688135.529
Test Loss of 20336122073.973160, Test MSE of 20336122436.034966
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21335050342.16301, 'MSE - std': 2222337392.410337, 'R2 - mean': 0.8415569601212951, 'R2 - std': 0.014562293838578618} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005414 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043163407.059
Test Loss of 431612659301.641846, Test MSE of 431612652574.264099
Epoch 2: training loss 424023405869.176
Test Loss of 431591825540.679321, Test MSE of 431591833154.676331
Epoch 3: training loss 423996175179.294
Test Loss of 431563512320.710754, Test MSE of 431563513744.349426
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011220992.000
Test Loss of 431567343432.144348, Test MSE of 431567349526.558289
Epoch 2: training loss 423998017897.412
Test Loss of 431569742956.512695, Test MSE of 431569754107.539612
Epoch 3: training loss 423997360850.824
Test Loss of 431567972438.241577, Test MSE of 431567968832.425354
Epoch 4: training loss 423996922940.235
Test Loss of 431568218933.664062, Test MSE of 431568219531.095764
Epoch 5: training loss 417508069135.059
Test Loss of 409798768216.373901, Test MSE of 409798767866.283203
Epoch 6: training loss 373201736402.824
Test Loss of 342522980309.353088, Test MSE of 342522982759.901062
Epoch 7: training loss 294764591104.000
Test Loss of 254124919700.434998, Test MSE of 254124916565.328522
Epoch 8: training loss 216782311363.765
Test Loss of 182569096788.583069, Test MSE of 182569097166.218658
Epoch 9: training loss 156622048105.412
Test Loss of 132059894902.937531, Test MSE of 132059894769.126877
Epoch 10: training loss 138829725726.118
Test Loss of 123433036756.879227, Test MSE of 123433038628.873276
Epoch 11: training loss 134029898270.118
Test Loss of 119681816322.961594, Test MSE of 119681815859.350525
Epoch 12: training loss 131190142584.471
Test Loss of 117765407637.382690, Test MSE of 117765405641.657928
Epoch 13: training loss 127717465630.118
Test Loss of 114544668040.825546, Test MSE of 114544667765.406006
Epoch 14: training loss 124563374140.235
Test Loss of 109912432598.300781, Test MSE of 109912430589.097275
Epoch 15: training loss 120903907388.235
Test Loss of 106380825210.965286, Test MSE of 106380826489.833893
Epoch 16: training loss 116005270437.647
Test Loss of 102979654782.519211, Test MSE of 102979655109.152420
Epoch 17: training loss 112646496542.118
Test Loss of 96961513990.397034, Test MSE of 96961514508.229370
Epoch 18: training loss 107841787783.529
Test Loss of 93232945289.417862, Test MSE of 93232943934.144608
Epoch 19: training loss 104085813187.765
Test Loss of 91614230570.646927, Test MSE of 91614230460.231552
Epoch 20: training loss 100684211591.529
Test Loss of 89914142346.602493, Test MSE of 89914142864.769379
Epoch 21: training loss 97093664692.706
Test Loss of 84374614332.534943, Test MSE of 84374616083.966263
Epoch 22: training loss 92711905792.000
Test Loss of 79292344156.046280, Test MSE of 79292343390.389145
Epoch 23: training loss 88884930906.353
Test Loss of 76793414780.623779, Test MSE of 76793414083.949326
Epoch 24: training loss 85557984933.647
Test Loss of 74212747700.894028, Test MSE of 74212747727.995193
Epoch 25: training loss 83488840899.765
Test Loss of 71454662701.490051, Test MSE of 71454662321.027161
Epoch 26: training loss 79917207838.118
Test Loss of 67975311380.849609, Test MSE of 67975312956.998581
Epoch 27: training loss 75192082552.471
Test Loss of 63870353341.660339, Test MSE of 63870355868.877464
Epoch 28: training loss 71788812009.412
Test Loss of 64023909726.178619, Test MSE of 64023909377.616394
Epoch 29: training loss 69238867230.118
Test Loss of 56457447042.073112, Test MSE of 56457448041.957863
Epoch 30: training loss 65373885475.765
Test Loss of 56895547482.032394, Test MSE of 56895546473.007675
Epoch 31: training loss 62663192922.353
Test Loss of 51225488585.388245, Test MSE of 51225487190.730522
Epoch 32: training loss 59079000350.118
Test Loss of 52723515301.019897, Test MSE of 52723515775.048561
Epoch 33: training loss 56984301266.824
Test Loss of 49808082821.745491, Test MSE of 49808082972.641518
Epoch 34: training loss 53523601761.882
Test Loss of 43968200351.452103, Test MSE of 43968200132.975571
Epoch 35: training loss 51445099068.235
Test Loss of 41721720464.288757, Test MSE of 41721720399.112785
Epoch 36: training loss 48334305935.059
Test Loss of 41066519755.283669, Test MSE of 41066520879.955780
Epoch 37: training loss 46222644359.529
Test Loss of 38297833384.810738, Test MSE of 38297833519.319153
Epoch 38: training loss 43820466763.294
Test Loss of 39554972865.806572, Test MSE of 39554973525.630844
Epoch 39: training loss 41996179614.118
Test Loss of 33763654272.651550, Test MSE of 33763654412.283970
Epoch 40: training loss 39135204592.941
Test Loss of 36948889926.012032, Test MSE of 36948889641.243439
Epoch 41: training loss 37576466371.765
Test Loss of 33333483330.458122, Test MSE of 33333483014.998989
Epoch 42: training loss 35996787915.294
Test Loss of 31600339550.534012, Test MSE of 31600339330.449306
Epoch 43: training loss 33686119506.824
Test Loss of 29384517047.263306, Test MSE of 29384516892.595448
Epoch 44: training loss 32573481728.000
Test Loss of 28914094345.358631, Test MSE of 28914094433.122143
Epoch 45: training loss 30809256553.412
Test Loss of 26686085771.076355, Test MSE of 26686085265.414494
Epoch 46: training loss 29305438471.529
Test Loss of 27035519936.977325, Test MSE of 27035519729.688412
Epoch 47: training loss 27950688124.235
Test Loss of 24844744269.949097, Test MSE of 24844744507.748703
Epoch 48: training loss 26699145042.824
Test Loss of 25907051459.820454, Test MSE of 25907051495.534981
Epoch 49: training loss 25613717240.471
Test Loss of 20891031676.149929, Test MSE of 20891031498.868774
Epoch 50: training loss 24720082153.412
Test Loss of 22469443787.757519, Test MSE of 22469443932.479485
Epoch 51: training loss 23731127981.176
Test Loss of 25641573220.101803, Test MSE of 25641573046.338219
Epoch 52: training loss 22476515015.529
Test Loss of 24057765702.248959, Test MSE of 24057765905.324638
Epoch 53: training loss 22088424489.412
Test Loss of 22309614093.031006, Test MSE of 22309613705.657349
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21529963014.861877, 'MSE - std': 2025584011.7059093, 'R2 - mean': 0.8399237634073995, 'R2 - std': 0.01342824067024426} 
 

Saving model.....
Results After CV: {'MSE - mean': 21529963014.861877, 'MSE - std': 2025584011.7059093, 'R2 - mean': 0.8399237634073995, 'R2 - std': 0.01342824067024426}
Train time: 94.5890729138002
Inference time: 0.07038941879873165
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 52 finished with value: 21529963014.861877 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005455 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524488975.059
Test Loss of 418111181649.898682, Test MSE of 418111184928.820007
Epoch 2: training loss 427502904018.824
Test Loss of 418092991415.280151, Test MSE of 418092991233.925476
Epoch 3: training loss 427474791243.294
Test Loss of 418068677149.964355, Test MSE of 418068673465.830200
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493815356.235
Test Loss of 418074445924.907715, Test MSE of 418074445080.227600
Epoch 2: training loss 427481347011.765
Test Loss of 418076201737.889404, Test MSE of 418076206321.320007
Epoch 3: training loss 427480914160.941
Test Loss of 418076466815.318970, Test MSE of 418076465522.443604
Epoch 4: training loss 422301219779.765
Test Loss of 401468664066.191101, Test MSE of 401468665044.203308
Epoch 5: training loss 386370024267.294
Test Loss of 347342139667.719666, Test MSE of 347342136571.695557
Epoch 6: training loss 318419706337.882
Test Loss of 269703847195.062683, Test MSE of 269703846393.927887
Epoch 7: training loss 226283319356.235
Test Loss of 171603562390.828583, Test MSE of 171603565891.863861
Epoch 8: training loss 160997304591.059
Test Loss of 130729212754.372421, Test MSE of 130729212480.349930
Epoch 9: training loss 141591746228.706
Test Loss of 119848060740.870697, Test MSE of 119848060544.213455
Epoch 10: training loss 135481100107.294
Test Loss of 115264093728.569977, Test MSE of 115264094946.150299
Epoch 11: training loss 132432088726.588
Test Loss of 112058400404.163773, Test MSE of 112058399856.872345
Epoch 12: training loss 129285733616.941
Test Loss of 110005777671.639145, Test MSE of 110005778517.226852
Epoch 13: training loss 125909548769.882
Test Loss of 105988320329.667358, Test MSE of 105988321224.650940
Epoch 14: training loss 122506081039.059
Test Loss of 102490213661.905151, Test MSE of 102490215704.005951
Epoch 15: training loss 118699309146.353
Test Loss of 98282558032.181351, Test MSE of 98282557905.952042
Epoch 16: training loss 115225227113.412
Test Loss of 94679278329.782089, Test MSE of 94679282084.305206
Epoch 17: training loss 110311887600.941
Test Loss of 93037085755.928757, Test MSE of 93037086462.011276
Epoch 18: training loss 107130779346.824
Test Loss of 90007265268.630112, Test MSE of 90007267485.940109
Epoch 19: training loss 102894556958.118
Test Loss of 85669773950.608368, Test MSE of 85669774372.123306
Epoch 20: training loss 98496424071.529
Test Loss of 83359428951.938934, Test MSE of 83359428222.102554
Epoch 21: training loss 93829243979.294
Test Loss of 79322302964.748550, Test MSE of 79322303449.939972
Epoch 22: training loss 90459036566.588
Test Loss of 76553197976.842010, Test MSE of 76553198359.171509
Epoch 23: training loss 86886693662.118
Test Loss of 72219046425.226929, Test MSE of 72219043931.618256
Epoch 24: training loss 83757511619.765
Test Loss of 69065786522.677765, Test MSE of 69065786458.223404
Epoch 25: training loss 79764341443.765
Test Loss of 66011232733.061302, Test MSE of 66011232021.994247
Epoch 26: training loss 75381470554.353
Test Loss of 62991182849.658112, Test MSE of 62991183561.498634
Epoch 27: training loss 72480231920.941
Test Loss of 60704009947.225540, Test MSE of 60704009582.611977
Epoch 28: training loss 68791964431.059
Test Loss of 57726077838.064308, Test MSE of 57726078506.693634
Epoch 29: training loss 66183940826.353
Test Loss of 54545639195.181122, Test MSE of 54545638218.963547
Epoch 30: training loss 62969998592.000
Test Loss of 51572898812.920654, Test MSE of 51572899034.018806
Epoch 31: training loss 60076367277.176
Test Loss of 50097675522.427940, Test MSE of 50097675303.735657
Epoch 32: training loss 57136409396.706
Test Loss of 48998317128.719872, Test MSE of 48998317312.212318
Epoch 33: training loss 54208529532.235
Test Loss of 44840111357.690491, Test MSE of 44840112336.480667
Epoch 34: training loss 51077668284.235
Test Loss of 44276767920.943787, Test MSE of 44276768153.571739
Epoch 35: training loss 48766241366.588
Test Loss of 38938439141.825584, Test MSE of 38938439219.905434
Epoch 36: training loss 46396893123.765
Test Loss of 39717473567.800140, Test MSE of 39717473996.233154
Epoch 37: training loss 44459340724.706
Test Loss of 37946849275.736298, Test MSE of 37946849129.933990
Epoch 38: training loss 41660112414.118
Test Loss of 36234137461.666435, Test MSE of 36234137575.033760
Epoch 39: training loss 39721428397.176
Test Loss of 31827321558.724960, Test MSE of 31827321109.446278
Epoch 40: training loss 38134255113.412
Test Loss of 32426229075.912098, Test MSE of 32426228781.287331
Epoch 41: training loss 35794796487.529
Test Loss of 29263502349.975479, Test MSE of 29263502505.658424
Epoch 42: training loss 34409827809.882
Test Loss of 32623956348.891048, Test MSE of 32623956273.095867
Epoch 43: training loss 32550689325.176
Test Loss of 28752863996.624565, Test MSE of 28752864099.090221
Epoch 44: training loss 31399566266.353
Test Loss of 25697632726.428867, Test MSE of 25697632769.050247
Epoch 45: training loss 29491997688.471
Test Loss of 23705302894.560261, Test MSE of 23705302436.178963
Epoch 46: training loss 28485781650.824
Test Loss of 24662564527.640991, Test MSE of 24662565090.947968
Epoch 47: training loss 26990472583.529
Test Loss of 24835316382.349293, Test MSE of 24835316651.027332
Epoch 48: training loss 25829609185.882
Test Loss of 24225125693.646080, Test MSE of 24225125601.093418
Epoch 49: training loss 24864382174.118
Test Loss of 21540348037.122368, Test MSE of 21540348131.595428
Epoch 50: training loss 23839484600.471
Test Loss of 22194034660.522785, Test MSE of 22194034295.021168
Epoch 51: training loss 23278894576.941
Test Loss of 23104180040.660652, Test MSE of 23104180009.642231
Epoch 52: training loss 21880920237.176
Test Loss of 19862835131.780708, Test MSE of 19862835060.877052
Epoch 53: training loss 21318184222.118
Test Loss of 20550287607.294933, Test MSE of 20550287922.305256
Epoch 54: training loss 19989184591.059
Test Loss of 20353884783.922276, Test MSE of 20353885105.090332
Epoch 55: training loss 19728850989.176
Test Loss of 19751841175.657646, Test MSE of 19751841145.264942
Epoch 56: training loss 19115386104.471
Test Loss of 18635313891.516075, Test MSE of 18635314202.911003
Epoch 57: training loss 18389020890.353
Test Loss of 20800205413.499886, Test MSE of 20800205527.748379
Epoch 58: training loss 17898126817.882
Test Loss of 20296498083.382835, Test MSE of 20296498222.557743
Epoch 59: training loss 17223293970.824
Test Loss of 19477049878.858200, Test MSE of 19477050107.158447
Epoch 60: training loss 16942684777.412
Test Loss of 18024548980.659725, Test MSE of 18024549076.637390
Epoch 61: training loss 16245888914.824
Test Loss of 20581216102.032848, Test MSE of 20581216514.178352
Epoch 62: training loss 15995998083.765
Test Loss of 18304205133.516541, Test MSE of 18304205378.933784
Epoch 63: training loss 15163652431.059
Test Loss of 18754163630.752720, Test MSE of 18754163804.324257
Epoch 64: training loss 15102929511.529
Test Loss of 18411235606.088364, Test MSE of 18411235796.827854
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18411235796.827854, 'MSE - std': 0.0, 'R2 - mean': 0.8566297092462126, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005535 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918668016.941
Test Loss of 424557336151.050659, Test MSE of 424557340459.599854
Epoch 2: training loss 427898534490.353
Test Loss of 424541679361.125122, Test MSE of 424541682791.454895
Epoch 3: training loss 427871000696.471
Test Loss of 424519993986.872070, Test MSE of 424519990388.277832
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889128628.706
Test Loss of 424525716888.131409, Test MSE of 424525719343.181641
Epoch 2: training loss 427879302806.588
Test Loss of 424526787733.940308, Test MSE of 424526789348.131958
Epoch 3: training loss 427878832971.294
Test Loss of 424525697172.755981, Test MSE of 424525704869.928040
Epoch 4: training loss 422773982147.765
Test Loss of 408412836750.538025, Test MSE of 408412833832.238464
Epoch 5: training loss 386799398309.647
Test Loss of 354918931413.126099, Test MSE of 354918933158.019775
Epoch 6: training loss 318551314432.000
Test Loss of 278403750228.622742, Test MSE of 278403748486.548584
Epoch 7: training loss 226212981398.588
Test Loss of 181964262223.529968, Test MSE of 181964265039.413849
Epoch 8: training loss 160114780762.353
Test Loss of 142085592177.935699, Test MSE of 142085590961.016083
Epoch 9: training loss 141074879186.824
Test Loss of 131637843611.269958, Test MSE of 131637844200.441879
Epoch 10: training loss 134980530085.647
Test Loss of 127626593442.257690, Test MSE of 127626590133.225159
Epoch 11: training loss 130458300114.824
Test Loss of 124504122837.244507, Test MSE of 124504123381.753296
Epoch 12: training loss 129048669936.941
Test Loss of 121339108032.458939, Test MSE of 121339110745.078293
Epoch 13: training loss 125439136677.647
Test Loss of 118310318274.235489, Test MSE of 118310317546.127075
Epoch 14: training loss 121224490556.235
Test Loss of 115827373204.045334, Test MSE of 115827374421.978714
Epoch 15: training loss 117725658383.059
Test Loss of 111470471092.674530, Test MSE of 111470472206.027649
Epoch 16: training loss 115269455751.529
Test Loss of 108110498497.406433, Test MSE of 108110501003.077209
Epoch 17: training loss 109359823781.647
Test Loss of 104687922131.231094, Test MSE of 104687923419.041260
Epoch 18: training loss 105725662268.235
Test Loss of 100672661069.575760, Test MSE of 100672662683.597565
Epoch 19: training loss 100833014000.941
Test Loss of 97368232023.169098, Test MSE of 97368231037.263382
Epoch 20: training loss 97284781116.235
Test Loss of 93219828603.588242, Test MSE of 93219832513.604156
Epoch 21: training loss 93949453131.294
Test Loss of 91568039054.360397, Test MSE of 91568040062.049088
Epoch 22: training loss 90888762488.471
Test Loss of 84998656698.300247, Test MSE of 84998658622.350311
Epoch 23: training loss 85735165861.647
Test Loss of 83817048905.845016, Test MSE of 83817048350.332428
Epoch 24: training loss 82761269187.765
Test Loss of 81238304419.797363, Test MSE of 81238304194.210129
Epoch 25: training loss 80157112892.235
Test Loss of 77151468677.359238, Test MSE of 77151469827.886978
Epoch 26: training loss 75581868077.176
Test Loss of 73330276794.951660, Test MSE of 73330278165.796097
Epoch 27: training loss 72137982208.000
Test Loss of 68891108343.709457, Test MSE of 68891109033.608093
Epoch 28: training loss 69699785517.176
Test Loss of 64869767913.201019, Test MSE of 64869765791.568176
Epoch 29: training loss 65486264847.059
Test Loss of 62445469892.130463, Test MSE of 62445471640.635323
Epoch 30: training loss 62319356506.353
Test Loss of 59576448976.151749, Test MSE of 59576449916.648857
Epoch 31: training loss 59725014106.353
Test Loss of 58676642653.268562, Test MSE of 58676640034.428757
Epoch 32: training loss 56917959966.118
Test Loss of 55460657407.822342, Test MSE of 55460660085.367638
Epoch 33: training loss 54408064752.941
Test Loss of 52672480663.183899, Test MSE of 52672480608.366882
Epoch 34: training loss 50609533470.118
Test Loss of 49895175825.795052, Test MSE of 49895176111.374359
Epoch 35: training loss 48107749293.176
Test Loss of 49172115136.222069, Test MSE of 49172115858.343185
Epoch 36: training loss 45780724743.529
Test Loss of 47890417041.972702, Test MSE of 47890415740.294975
Epoch 37: training loss 44384029040.941
Test Loss of 41022093295.418922, Test MSE of 41022093157.570343
Epoch 38: training loss 40988161671.529
Test Loss of 40251510213.137177, Test MSE of 40251510932.557053
Epoch 39: training loss 39236413583.059
Test Loss of 40568763181.420311, Test MSE of 40568764501.812202
Epoch 40: training loss 37254766569.412
Test Loss of 39123599974.921120, Test MSE of 39123600309.028694
Epoch 41: training loss 35125123471.059
Test Loss of 37997961713.432335, Test MSE of 37997961773.352348
Epoch 42: training loss 33467187847.529
Test Loss of 37840284156.328476, Test MSE of 37840284453.308510
Epoch 43: training loss 31881293304.471
Test Loss of 38978779783.135788, Test MSE of 38978781714.633186
Epoch 44: training loss 29653171998.118
Test Loss of 31792655373.501736, Test MSE of 31792655747.662273
Epoch 45: training loss 28643597790.118
Test Loss of 33420938846.630581, Test MSE of 33420938005.886379
Epoch 46: training loss 27594441027.765
Test Loss of 34053637738.000462, Test MSE of 34053638423.283833
Epoch 47: training loss 25784418944.000
Test Loss of 27196830890.548229, Test MSE of 27196830685.321255
Epoch 48: training loss 25194333229.176
Test Loss of 28081492052.563499, Test MSE of 28081491907.925045
Epoch 49: training loss 23939024203.294
Test Loss of 31937378407.276428, Test MSE of 31937378819.721859
Epoch 50: training loss 22647467384.471
Test Loss of 28733750948.271107, Test MSE of 28733750488.421207
Epoch 51: training loss 21088782132.706
Test Loss of 29003395240.653252, Test MSE of 29003395721.394756
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23707315759.111305, 'MSE - std': 5296079962.283451, 'R2 - mean': 0.8247823599863879, 'R2 - std': 0.03184734925982474} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005434 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926939708.235
Test Loss of 447257290706.994202, Test MSE of 447257289877.827026
Epoch 2: training loss 421906184553.412
Test Loss of 447239267033.567444, Test MSE of 447239274613.442993
Epoch 3: training loss 421879509955.765
Test Loss of 447215006176.377502, Test MSE of 447215009946.872375
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897213108.706
Test Loss of 447223024960.014832, Test MSE of 447223026773.973389
Epoch 2: training loss 421887057197.176
Test Loss of 447224705201.180664, Test MSE of 447224703955.341064
Epoch 3: training loss 421886559292.235
Test Loss of 447225616959.600281, Test MSE of 447225618653.202148
Epoch 4: training loss 416645828487.529
Test Loss of 430481336377.323181, Test MSE of 430481340854.418762
Epoch 5: training loss 380436895503.059
Test Loss of 374813869559.354126, Test MSE of 374813871509.376953
Epoch 6: training loss 312506447269.647
Test Loss of 295797904801.132568, Test MSE of 295797900806.489075
Epoch 7: training loss 220499130247.529
Test Loss of 195058328569.604431, Test MSE of 195058332003.911072
Epoch 8: training loss 157568509831.529
Test Loss of 154440131304.727264, Test MSE of 154440132573.367920
Epoch 9: training loss 137101229266.824
Test Loss of 142818061190.484375, Test MSE of 142818062481.668457
Epoch 10: training loss 131446970548.706
Test Loss of 137544481732.071259, Test MSE of 137544481917.565704
Epoch 11: training loss 129613593449.412
Test Loss of 133340318577.639603, Test MSE of 133340318220.391113
Epoch 12: training loss 125100582731.294
Test Loss of 131240529166.745316, Test MSE of 131240529679.733521
Epoch 13: training loss 121049792481.882
Test Loss of 127653065135.108032, Test MSE of 127653065355.302719
Epoch 14: training loss 118251863943.529
Test Loss of 123180079823.618790, Test MSE of 123180077806.293289
Epoch 15: training loss 114907598968.471
Test Loss of 121008726787.257004, Test MSE of 121008726720.927002
Epoch 16: training loss 110883708175.059
Test Loss of 116047690199.376358, Test MSE of 116047690456.614014
Epoch 17: training loss 106823635666.824
Test Loss of 110732907611.195923, Test MSE of 110732905826.559341
Epoch 18: training loss 102435625592.471
Test Loss of 107051948941.116821, Test MSE of 107051949768.077774
Epoch 19: training loss 99604138300.235
Test Loss of 104119601347.656723, Test MSE of 104119602513.992386
Epoch 20: training loss 95120656956.235
Test Loss of 99683082279.794586, Test MSE of 99683083810.909286
Epoch 21: training loss 91675207604.706
Test Loss of 95303550536.838303, Test MSE of 95303550305.885406
Epoch 22: training loss 87491493300.706
Test Loss of 91222101577.312057, Test MSE of 91222104155.774872
Epoch 23: training loss 84258396611.765
Test Loss of 86467971267.656723, Test MSE of 86467974431.688614
Epoch 24: training loss 80500435983.059
Test Loss of 84882127927.665054, Test MSE of 84882127624.126862
Epoch 25: training loss 77015830573.176
Test Loss of 82377071313.513763, Test MSE of 82377072232.603638
Epoch 26: training loss 73286558584.471
Test Loss of 76382539466.170715, Test MSE of 76382542143.939651
Epoch 27: training loss 69791972276.706
Test Loss of 71371654482.490860, Test MSE of 71371654524.091385
Epoch 28: training loss 66700871288.471
Test Loss of 69354619950.663895, Test MSE of 69354619513.986923
Epoch 29: training loss 63429289336.471
Test Loss of 69071391992.005554, Test MSE of 69071392891.810486
Epoch 30: training loss 60754643139.765
Test Loss of 64740610681.870926, Test MSE of 64740610180.832832
Epoch 31: training loss 57360666142.118
Test Loss of 61697025694.112419, Test MSE of 61697025234.110741
Epoch 32: training loss 54683822057.412
Test Loss of 57855972717.968079, Test MSE of 57855973478.990341
Epoch 33: training loss 52274833129.412
Test Loss of 53589550661.048347, Test MSE of 53589550496.966965
Epoch 34: training loss 48873704794.353
Test Loss of 54363625259.288460, Test MSE of 54363625480.416985
Epoch 35: training loss 47412803335.529
Test Loss of 50967067970.857277, Test MSE of 50967067762.607330
Epoch 36: training loss 43954949360.941
Test Loss of 46822669596.957664, Test MSE of 46822670235.258331
Epoch 37: training loss 42099198320.941
Test Loss of 45485382616.916031, Test MSE of 45485382912.965721
Epoch 38: training loss 40390926667.294
Test Loss of 42277081827.042336, Test MSE of 42277082153.088326
Epoch 39: training loss 38438873509.647
Test Loss of 43400508108.302567, Test MSE of 43400508059.118912
Epoch 40: training loss 36135084581.647
Test Loss of 37103398056.416374, Test MSE of 37103398962.639732
Epoch 41: training loss 33596500088.471
Test Loss of 38092620930.990517, Test MSE of 38092621018.063194
Epoch 42: training loss 32838399126.588
Test Loss of 37422878419.645615, Test MSE of 37422879161.207123
Epoch 43: training loss 31585158949.647
Test Loss of 35986271209.023361, Test MSE of 35986271481.684341
Epoch 44: training loss 30138135506.824
Test Loss of 35594373281.547073, Test MSE of 35594373204.631599
Epoch 45: training loss 27914062712.471
Test Loss of 30522980469.015038, Test MSE of 30522979954.964523
Epoch 46: training loss 27010416120.471
Test Loss of 30198716048.847561, Test MSE of 30198716002.598297
Epoch 47: training loss 25975086245.647
Test Loss of 31523081354.096691, Test MSE of 31523081538.090546
Epoch 48: training loss 24535117281.882
Test Loss of 29754609942.562111, Test MSE of 29754609770.916397
Epoch 49: training loss 24003097667.765
Test Loss of 30796960080.359009, Test MSE of 30796959666.583443
Epoch 50: training loss 22773301443.765
Test Loss of 28121505191.291233, Test MSE of 28121504802.616550
Epoch 51: training loss 21721308980.706
Test Loss of 27116863868.417301, Test MSE of 27116864122.755913
Epoch 52: training loss 21117161603.765
Test Loss of 25023230252.828129, Test MSE of 25023230689.333553
Epoch 53: training loss 20403468352.000
Test Loss of 28202499517.083508, Test MSE of 28202499498.330978
Epoch 54: training loss 19592579026.824
Test Loss of 25807787288.693962, Test MSE of 25807787694.583733
Epoch 55: training loss 18991174945.882
Test Loss of 26332093588.045338, Test MSE of 26332093746.563164
Epoch 56: training loss 18143186612.706
Test Loss of 25846928601.685867, Test MSE of 25846928526.537704
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24420520014.920105, 'MSE - std': 4440303585.597996, 'R2 - mean': 0.8258344741727456, 'R2 - std': 0.02604578644951571} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005442 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110760237.176
Test Loss of 410765841360.140686, Test MSE of 410765838434.827942
Epoch 2: training loss 430090057366.588
Test Loss of 410747510204.001831, Test MSE of 410747504266.936035
Epoch 3: training loss 430062864022.588
Test Loss of 410723619147.698303, Test MSE of 410723615153.105530
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430081240124.235
Test Loss of 410729689688.847778, Test MSE of 410729684315.982056
Epoch 2: training loss 430066844129.882
Test Loss of 410729826962.658020, Test MSE of 410729830987.790161
Epoch 3: training loss 430066361524.706
Test Loss of 410729371379.798218, Test MSE of 410729371594.832336
Epoch 4: training loss 424589721359.059
Test Loss of 393569301770.780212, Test MSE of 393569307489.338318
Epoch 5: training loss 387904370206.118
Test Loss of 338732691874.887573, Test MSE of 338732690693.806824
Epoch 6: training loss 320043924660.706
Test Loss of 261353447056.288757, Test MSE of 261353445790.909546
Epoch 7: training loss 228322171241.412
Test Loss of 164755635685.701050, Test MSE of 164755633410.313568
Epoch 8: training loss 164114963516.235
Test Loss of 124389617294.393341, Test MSE of 124389619018.107895
Epoch 9: training loss 144043399529.412
Test Loss of 114473717416.929199, Test MSE of 114473715159.602310
Epoch 10: training loss 138374433792.000
Test Loss of 110369486189.341965, Test MSE of 110369486954.402756
Epoch 11: training loss 135431125082.353
Test Loss of 107712615417.839890, Test MSE of 107712613498.539413
Epoch 12: training loss 132604260894.118
Test Loss of 104997220096.118469, Test MSE of 104997217503.656601
Epoch 13: training loss 128485150268.235
Test Loss of 101213581916.638596, Test MSE of 101213581398.819550
Epoch 14: training loss 124147886832.941
Test Loss of 98769718206.134201, Test MSE of 98769718577.988525
Epoch 15: training loss 120422804570.353
Test Loss of 96341127802.965286, Test MSE of 96341129212.460220
Epoch 16: training loss 116871677048.471
Test Loss of 92921627473.147614, Test MSE of 92921626020.858566
Epoch 17: training loss 113529195730.824
Test Loss of 90012681094.219345, Test MSE of 90012679807.440262
Epoch 18: training loss 108943159024.941
Test Loss of 86433088500.153625, Test MSE of 86433088287.614243
Epoch 19: training loss 106059515211.294
Test Loss of 84015509369.425262, Test MSE of 84015510988.281586
Epoch 20: training loss 101866448775.529
Test Loss of 81915992644.945862, Test MSE of 81915990893.234375
Epoch 21: training loss 97095726501.647
Test Loss of 78061467493.523361, Test MSE of 78061466303.737076
Epoch 22: training loss 93784316656.941
Test Loss of 74757611030.981949, Test MSE of 74757612033.874603
Epoch 23: training loss 90139848794.353
Test Loss of 71571871362.073120, Test MSE of 71571870231.437286
Epoch 24: training loss 85987071322.353
Test Loss of 68318522901.086533, Test MSE of 68318523958.998207
Epoch 25: training loss 82595741214.118
Test Loss of 65961528564.982880, Test MSE of 65961529126.058075
Epoch 26: training loss 78636816338.824
Test Loss of 63177072640.000000, Test MSE of 63177073064.444458
Epoch 27: training loss 75895560688.941
Test Loss of 60816654100.968071, Test MSE of 60816652841.798157
Epoch 28: training loss 72473719341.176
Test Loss of 57168187192.981026, Test MSE of 57168187263.056557
Epoch 29: training loss 69343477187.765
Test Loss of 56389557551.740860, Test MSE of 56389557875.915810
Epoch 30: training loss 65682880647.529
Test Loss of 52201654475.757523, Test MSE of 52201654377.797676
Epoch 31: training loss 62414373376.000
Test Loss of 51248686917.301247, Test MSE of 51248687899.513191
Epoch 32: training loss 59053134561.882
Test Loss of 47035974911.407684, Test MSE of 47035974015.860146
Epoch 33: training loss 57077967736.471
Test Loss of 46619753334.108284, Test MSE of 46619753385.265846
Epoch 34: training loss 53868291824.941
Test Loss of 41754965872.895882, Test MSE of 41754963709.806702
Epoch 35: training loss 51470402725.647
Test Loss of 43445232575.081909, Test MSE of 43445231959.734047
Epoch 36: training loss 48835353178.353
Test Loss of 39329409559.455811, Test MSE of 39329409305.987289
Epoch 37: training loss 46912048173.176
Test Loss of 39389709195.905602, Test MSE of 39389709717.991440
Epoch 38: training loss 44716017889.882
Test Loss of 33717662401.095791, Test MSE of 33717662214.129223
Epoch 39: training loss 41955381353.412
Test Loss of 33118401222.782043, Test MSE of 33118401834.202835
Epoch 40: training loss 39902995621.647
Test Loss of 33275541144.818138, Test MSE of 33275541182.401543
Epoch 41: training loss 38181069266.824
Test Loss of 32534096346.802406, Test MSE of 32534095690.042030
Epoch 42: training loss 36293795968.000
Test Loss of 31279568430.674686, Test MSE of 31279568399.707970
Epoch 43: training loss 34507406524.235
Test Loss of 26679257052.934753, Test MSE of 26679257387.612896
Epoch 44: training loss 33281840918.588
Test Loss of 26668758672.288754, Test MSE of 26668758563.263950
Epoch 45: training loss 31218333251.765
Test Loss of 26181570263.366959, Test MSE of 26181570082.122917
Epoch 46: training loss 30401106375.529
Test Loss of 26566595283.102268, Test MSE of 26566595540.900887
Epoch 47: training loss 28901711431.529
Test Loss of 25642726128.481258, Test MSE of 25642725799.097771
Epoch 48: training loss 27852822407.529
Test Loss of 25485254904.299862, Test MSE of 25485254919.113018
Epoch 49: training loss 26690930432.000
Test Loss of 22828627448.655251, Test MSE of 22828627391.063919
Epoch 50: training loss 25469948611.765
Test Loss of 23880786087.744564, Test MSE of 23880785784.513599
Epoch 51: training loss 24604348905.412
Test Loss of 23603848446.459972, Test MSE of 23603848007.741184
Epoch 52: training loss 23636795015.529
Test Loss of 22029034257.651089, Test MSE of 22029034011.434780
Epoch 53: training loss 22361127171.765
Test Loss of 20663582727.581676, Test MSE of 20663582755.766075
Epoch 54: training loss 21609208376.471
Test Loss of 20564511734.049053, Test MSE of 20564511656.956646
Epoch 55: training loss 20674329513.412
Test Loss of 22415037551.355854, Test MSE of 22415037406.572552
Epoch 56: training loss 20103837707.294
Test Loss of 21030154893.919483, Test MSE of 21030154693.107414
Epoch 57: training loss 19672727536.941
Test Loss of 20911050949.123554, Test MSE of 20911051005.440216
Epoch 58: training loss 18847335879.529
Test Loss of 19313597800.603424, Test MSE of 19313597967.475353
Epoch 59: training loss 18469663939.765
Test Loss of 22059242508.794075, Test MSE of 22059242539.951878
Epoch 60: training loss 18101645432.471
Test Loss of 19635019010.250809, Test MSE of 19635019028.660069
Epoch 61: training loss 17178397082.353
Test Loss of 20224486264.951412, Test MSE of 20224486022.292751
Epoch 62: training loss 16929001005.176
Test Loss of 20475911978.765385, Test MSE of 20475911998.981613
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23434368010.935482, 'MSE - std': 4207696433.215504, 'R2 - mean': 0.8271262623866722, 'R2 - std': 0.022667011143934504} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005470 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043119555.765
Test Loss of 431612657269.752869, Test MSE of 431612657167.903503
Epoch 2: training loss 424022960971.294
Test Loss of 431591848952.892151, Test MSE of 431591850563.158630
Epoch 3: training loss 423995832681.412
Test Loss of 431563820485.478943, Test MSE of 431563825603.786926
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008871815.529
Test Loss of 431564942588.564575, Test MSE of 431564945802.875305
Epoch 2: training loss 423998087890.824
Test Loss of 431568046617.351196, Test MSE of 431568039032.854919
Epoch 3: training loss 423997387474.824
Test Loss of 431568791001.380859, Test MSE of 431568787261.682861
Epoch 4: training loss 418940753920.000
Test Loss of 415089858390.360046, Test MSE of 415089856927.700195
Epoch 5: training loss 383552747760.941
Test Loss of 360046517487.296631, Test MSE of 360046526109.462646
Epoch 6: training loss 315970065829.647
Test Loss of 280960766086.574707, Test MSE of 280960768607.145813
Epoch 7: training loss 224952237327.059
Test Loss of 180752197991.181854, Test MSE of 180752199456.347412
Epoch 8: training loss 159342743476.706
Test Loss of 138124328565.752899, Test MSE of 138124326855.685608
Epoch 9: training loss 142187041249.882
Test Loss of 126207285965.416016, Test MSE of 126207287975.119507
Epoch 10: training loss 135931483467.294
Test Loss of 121508870743.900040, Test MSE of 121508870337.016510
Epoch 11: training loss 129711577374.118
Test Loss of 117957989210.624710, Test MSE of 117957992543.220123
Epoch 12: training loss 128941740815.059
Test Loss of 114365305474.546967, Test MSE of 114365305231.365173
Epoch 13: training loss 124874898371.765
Test Loss of 111747318112.547897, Test MSE of 111747318865.861450
Epoch 14: training loss 121898636589.176
Test Loss of 107969293727.570572, Test MSE of 107969293719.587891
Epoch 15: training loss 118173519736.471
Test Loss of 103597325452.734848, Test MSE of 103597323788.532745
Epoch 16: training loss 112761771188.706
Test Loss of 100538158912.562698, Test MSE of 100538160109.002213
Epoch 17: training loss 109346873660.235
Test Loss of 98211147046.263763, Test MSE of 98211147111.668411
Epoch 18: training loss 106380144489.412
Test Loss of 93319476202.676544, Test MSE of 93319473555.146439
Epoch 19: training loss 104403577404.235
Test Loss of 90870066758.367416, Test MSE of 90870065847.497421
Epoch 20: training loss 98746143563.294
Test Loss of 86646418491.231842, Test MSE of 86646418584.192627
Epoch 21: training loss 94991050285.176
Test Loss of 83602587204.472000, Test MSE of 83602587213.288300
Epoch 22: training loss 91575622791.529
Test Loss of 78149788938.780197, Test MSE of 78149789102.592789
Epoch 23: training loss 87207804717.176
Test Loss of 78425047707.187408, Test MSE of 78425047536.407318
Epoch 24: training loss 83985767740.235
Test Loss of 73613473537.066177, Test MSE of 73613472327.026642
Epoch 25: training loss 80702909093.647
Test Loss of 71920923804.845901, Test MSE of 71920922926.130325
Epoch 26: training loss 77350331000.471
Test Loss of 67929517613.726974, Test MSE of 67929518926.148064
Epoch 27: training loss 73771029398.588
Test Loss of 60982235757.697365, Test MSE of 60982235890.937096
Epoch 28: training loss 70706960677.647
Test Loss of 60516381494.611755, Test MSE of 60516381518.175896
Epoch 29: training loss 67014899049.412
Test Loss of 57413366838.493294, Test MSE of 57413367225.617226
Epoch 30: training loss 64212269312.000
Test Loss of 56023852048.584915, Test MSE of 56023851843.567825
Epoch 31: training loss 61330476333.176
Test Loss of 49789465582.467377, Test MSE of 49789466742.392746
Epoch 32: training loss 58998429184.000
Test Loss of 51396907864.729294, Test MSE of 51396907892.274750
Epoch 33: training loss 55085053040.941
Test Loss of 46304817271.411385, Test MSE of 46304817683.310295
Epoch 34: training loss 52668593558.588
Test Loss of 44342649529.514114, Test MSE of 44342649881.569687
Epoch 35: training loss 49751100400.941
Test Loss of 40311990729.743637, Test MSE of 40311991213.380234
Epoch 36: training loss 47497870320.941
Test Loss of 38166864431.148544, Test MSE of 38166865294.882416
Epoch 37: training loss 45640429289.412
Test Loss of 37259086431.481720, Test MSE of 37259086269.888641
Epoch 38: training loss 43124758836.706
Test Loss of 33178240120.832947, Test MSE of 33178240228.676090
Epoch 39: training loss 40732736768.000
Test Loss of 32897232302.733921, Test MSE of 32897232444.120155
Epoch 40: training loss 38928691486.118
Test Loss of 32436303886.215641, Test MSE of 32436303730.576283
Epoch 41: training loss 36529290398.118
Test Loss of 29634812208.214714, Test MSE of 29634811814.909630
Epoch 42: training loss 35840933857.882
Test Loss of 25793562248.707081, Test MSE of 25793561984.565311
Epoch 43: training loss 33896596397.176
Test Loss of 27591608237.549282, Test MSE of 27591607971.670719
Epoch 44: training loss 32395210142.118
Test Loss of 25624633082.432209, Test MSE of 25624633434.248974
Epoch 45: training loss 30621355527.529
Test Loss of 24851722061.830635, Test MSE of 24851722190.473362
Epoch 46: training loss 29320473088.000
Test Loss of 25436113849.395649, Test MSE of 25436114470.614292
Epoch 47: training loss 28223123011.765
Test Loss of 26463612221.008793, Test MSE of 26463612349.760078
Epoch 48: training loss 27269679947.294
Test Loss of 23334668227.346600, Test MSE of 23334668550.394756
Epoch 49: training loss 25905659271.529
Test Loss of 26820938139.305878, Test MSE of 26820938443.818687
Epoch 50: training loss 24888584888.471
Test Loss of 22782571274.069412, Test MSE of 22782571246.628471
Epoch 51: training loss 23473796065.882
Test Loss of 24579667627.772327, Test MSE of 24579667103.208244
Epoch 52: training loss 23007463427.765
Test Loss of 22434793573.404903, Test MSE of 22434793944.457264
Epoch 53: training loss 22323508483.765
Test Loss of 23002965265.414162, Test MSE of 23002965323.099369
Epoch 54: training loss 21264600794.353
Test Loss of 22857942565.197594, Test MSE of 22857942927.546730
Epoch 55: training loss 21039733541.647
Test Loss of 21202648928.784821, Test MSE of 21202648643.059288
Epoch 56: training loss 20148019358.118
Test Loss of 21254679505.088387, Test MSE of 21254679520.804031
Epoch 57: training loss 19315422460.235
Test Loss of 19929469748.716335, Test MSE of 19929470069.193394
Epoch 58: training loss 18780558878.118
Test Loss of 22215572972.335030, Test MSE of 22215573192.156822
Epoch 59: training loss 18485627019.294
Test Loss of 20948770054.041649, Test MSE of 20948770522.678776
Epoch 60: training loss 17641687928.471
Test Loss of 21395359064.492363, Test MSE of 21395359080.523537
Epoch 61: training loss 17361362522.353
Test Loss of 17924450860.305412, Test MSE of 17924450652.708298
Epoch 62: training loss 16880625795.765
Test Loss of 22397538269.882462, Test MSE of 22397538883.884228
Epoch 63: training loss 16307404999.529
Test Loss of 20665788955.246647, Test MSE of 20665788558.386890
Epoch 64: training loss 15706819030.588
Test Loss of 19268519545.543728, Test MSE of 19268519635.038078
Epoch 65: training loss 15566153455.059
Test Loss of 19048714327.189262, Test MSE of 19048714187.198330
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22557237246.188053, 'MSE - std': 4152252513.1717644, 'R2 - mean': 0.83324970895411, 'R2 - std': 0.023685884125707903} 
 

Saving model.....
Results After CV: {'MSE - mean': 22557237246.188053, 'MSE - std': 4152252513.1717644, 'R2 - mean': 0.83324970895411, 'R2 - std': 0.023685884125707903}
Train time: 90.68826226120072
Inference time: 0.07074989840039052
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 53 finished with value: 22557237246.188053 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525072052.706
Test Loss of 418112448588.272949, Test MSE of 418112446176.438904
Epoch 2: training loss 427503979941.647
Test Loss of 418093877803.703003, Test MSE of 418093878240.213745
Epoch 3: training loss 427475647909.647
Test Loss of 418069878590.238281, Test MSE of 418069874632.844910
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496604250.353
Test Loss of 418075760055.872314, Test MSE of 418075759400.290588
Epoch 2: training loss 427482331858.824
Test Loss of 418077987256.582947, Test MSE of 418077985256.278748
Epoch 3: training loss 427481818292.706
Test Loss of 418077671666.557495, Test MSE of 418077671178.493408
Epoch 4: training loss 427481478083.765
Test Loss of 418076882346.607422, Test MSE of 418076880911.958679
Epoch 5: training loss 427481223047.529
Test Loss of 418076527356.150818, Test MSE of 418076518556.769714
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 418076518556.7697, 'MSE - std': 0.0, 'R2 - mean': -2.255607210958785, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005445 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917496320.000
Test Loss of 424556825082.433472, Test MSE of 424556815888.148132
Epoch 2: training loss 427896478780.235
Test Loss of 424540501509.566528, Test MSE of 424540502580.178162
Epoch 3: training loss 427868428167.529
Test Loss of 424518743390.808228, Test MSE of 424518738806.693054
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888052826.353
Test Loss of 424526975662.930359, Test MSE of 424526976324.012146
Epoch 2: training loss 427877948717.176
Test Loss of 424527416852.252625, Test MSE of 424527414775.995728
Epoch 3: training loss 427877416719.059
Test Loss of 424526999956.578308, Test MSE of 424527001075.669128
Epoch 4: training loss 427877000372.706
Test Loss of 424526322561.746948, Test MSE of 424526325149.102966
Epoch 5: training loss 427876731000.471
Test Loss of 424525502156.539429, Test MSE of 424525500009.486450
Epoch 6: training loss 419593414535.529
Test Loss of 398892149039.670593, Test MSE of 398892149195.367493
Epoch 7: training loss 365875008692.706
Test Loss of 322947514762.155884, Test MSE of 322947511733.585754
Epoch 8: training loss 279203044653.176
Test Loss of 234116982678.354858, Test MSE of 234116979788.318634
Epoch 9: training loss 199582377261.176
Test Loss of 170428704393.267639, Test MSE of 170428702453.642273
Epoch 10: training loss 154865822629.647
Test Loss of 140896333862.847107, Test MSE of 140896336428.579010
Epoch 11: training loss 138024279973.647
Test Loss of 129675349804.709686, Test MSE of 129675351452.821304
Epoch 12: training loss 132500170179.765
Test Loss of 126087234254.197556, Test MSE of 126087234184.928528
Epoch 13: training loss 129437009648.941
Test Loss of 123476860533.844086, Test MSE of 123476858363.118469
Epoch 14: training loss 126149001155.765
Test Loss of 120428465013.429565, Test MSE of 120428464787.122986
Epoch 15: training loss 121988124160.000
Test Loss of 117084830428.883652, Test MSE of 117084831329.341461
Epoch 16: training loss 118775531369.412
Test Loss of 113594881132.250748, Test MSE of 113594879428.927170
Epoch 17: training loss 114905683847.529
Test Loss of 109899876913.387924, Test MSE of 109899875245.747681
Epoch 18: training loss 109771127145.412
Test Loss of 106242443648.207260, Test MSE of 106242441816.454498
Epoch 19: training loss 105724072869.647
Test Loss of 101364205484.147125, Test MSE of 101364205978.199173
Epoch 20: training loss 102205293417.412
Test Loss of 99310622613.407349, Test MSE of 99310623218.677795
Epoch 21: training loss 98869342750.118
Test Loss of 94924062325.844086, Test MSE of 94924063839.256454
Epoch 22: training loss 93741590046.118
Test Loss of 91195652027.306961, Test MSE of 91195651361.512527
Epoch 23: training loss 89802634631.529
Test Loss of 84053264550.284531, Test MSE of 84053263436.822693
Epoch 24: training loss 86468918061.176
Test Loss of 83134676184.738373, Test MSE of 83134675392.327957
Epoch 25: training loss 81892052992.000
Test Loss of 79998665492.311813, Test MSE of 79998663433.871979
Epoch 26: training loss 79561487917.176
Test Loss of 76609528299.747391, Test MSE of 76609527984.951996
Epoch 27: training loss 74900316611.765
Test Loss of 72027457102.523254, Test MSE of 72027457359.986053
Epoch 28: training loss 71652840719.059
Test Loss of 69682048157.046494, Test MSE of 69682049843.468079
Epoch 29: training loss 68487165515.294
Test Loss of 65268941584.521858, Test MSE of 65268941086.657372
Epoch 30: training loss 65390092257.882
Test Loss of 61864658574.952576, Test MSE of 61864658283.740295
Epoch 31: training loss 61239557978.353
Test Loss of 62267999893.585007, Test MSE of 62268001191.419525
Epoch 32: training loss 59012132352.000
Test Loss of 52530921927.269028, Test MSE of 52530922274.438049
Epoch 33: training loss 55051865810.824
Test Loss of 52794832620.280357, Test MSE of 52794833012.219177
Epoch 34: training loss 52073502253.176
Test Loss of 49772207129.345360, Test MSE of 49772206414.541214
Epoch 35: training loss 50137687988.706
Test Loss of 47164115540.918808, Test MSE of 47164114578.459236
Epoch 36: training loss 46752781658.353
Test Loss of 44715505005.020584, Test MSE of 44715504754.329491
Epoch 37: training loss 44194665253.647
Test Loss of 44801827931.195930, Test MSE of 44801826859.111076
Epoch 38: training loss 41496891678.118
Test Loss of 41994722154.059677, Test MSE of 41994723138.847809
Epoch 39: training loss 39211601980.235
Test Loss of 38040572472.494102, Test MSE of 38040572101.922188
Epoch 40: training loss 37329021071.059
Test Loss of 36427433408.399719, Test MSE of 36427433713.199181
Epoch 41: training loss 35216356118.588
Test Loss of 36366012593.180664, Test MSE of 36366012019.134491
Epoch 42: training loss 32882280704.000
Test Loss of 33130464260.026833, Test MSE of 33130464175.972942
Epoch 43: training loss 31712369408.000
Test Loss of 32119946498.427944, Test MSE of 32119946576.289124
Epoch 44: training loss 29959840090.353
Test Loss of 33016634241.746937, Test MSE of 33016634282.581078
Epoch 45: training loss 28048617946.353
Test Loss of 30272343581.727505, Test MSE of 30272344370.757534
Epoch 46: training loss 26750029989.647
Test Loss of 29941578323.971317, Test MSE of 29941578023.906612
Epoch 47: training loss 25384495450.353
Test Loss of 29796841694.186443, Test MSE of 29796841311.441879
Epoch 48: training loss 23975553264.941
Test Loss of 27350733564.150822, Test MSE of 27350732691.563042
Epoch 49: training loss 23372541206.588
Test Loss of 27408064854.517696, Test MSE of 27408065210.903461
Epoch 50: training loss 21954401611.294
Test Loss of 26960744323.405045, Test MSE of 26960744072.871204
Epoch 51: training loss 20702642499.765
Test Loss of 26521410414.560261, Test MSE of 26521410516.403835
Epoch 52: training loss 19543080154.353
Test Loss of 26918678275.020126, Test MSE of 26918678134.220093
Epoch 53: training loss 19278368967.529
Test Loss of 25207167692.539440, Test MSE of 25207168587.590256
Epoch 54: training loss 18497735932.235
Test Loss of 25952209491.023827, Test MSE of 25952209929.249168
Epoch 55: training loss 17505746330.353
Test Loss of 26503797437.142723, Test MSE of 26503797970.769444
Epoch 56: training loss 17055101116.235
Test Loss of 27936885853.801525, Test MSE of 27936885962.075943
Epoch 57: training loss 16206844291.765
Test Loss of 24744832792.812397, Test MSE of 24744833479.492012
Epoch 58: training loss 15676493191.529
Test Loss of 26962430592.029610, Test MSE of 26962430412.662537
Epoch 59: training loss 15354437108.706
Test Loss of 25433054985.652557, Test MSE of 25433055842.878887
Epoch 60: training loss 14438802292.706
Test Loss of 26617095067.329170, Test MSE of 26617094285.292000
Epoch 61: training loss 13874062840.471
Test Loss of 25096606109.579460, Test MSE of 25096606131.061062
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221586562343.91537, 'MSE - std': 196489956212.85434, 'R2 - mean': -0.717390158192847, 'R2 - std': 1.538217052765938} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005359 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926053285.647
Test Loss of 447257438485.140869, Test MSE of 447257448142.864502
Epoch 2: training loss 421904027527.529
Test Loss of 447237919126.473267, Test MSE of 447237921507.328125
Epoch 3: training loss 421876099674.353
Test Loss of 447212455851.199646, Test MSE of 447212446592.793335
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898350712.471
Test Loss of 447219362084.300720, Test MSE of 447219361318.696167
Epoch 2: training loss 421885431446.588
Test Loss of 447221159556.530212, Test MSE of 447221158468.378845
Epoch 3: training loss 421884925831.529
Test Loss of 447221508399.670593, Test MSE of 447221501027.071167
Epoch 4: training loss 421884541168.941
Test Loss of 447221340352.814270, Test MSE of 447221345423.684326
Epoch 5: training loss 421884290469.647
Test Loss of 447220878417.247253, Test MSE of 447220874358.950623
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 296797999682.26044, 'MSE - std': 192489971707.66904, 'R2 - mean': -1.137300054340783, 'R2 - std': 1.3892647129732187} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005500 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110333530.353
Test Loss of 410763105361.029175, Test MSE of 410763101978.655396
Epoch 2: training loss 430088620152.471
Test Loss of 410744101695.141113, Test MSE of 410744108166.827393
Epoch 3: training loss 430060574599.529
Test Loss of 410719623665.547424, Test MSE of 410719617347.981079
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074869157.647
Test Loss of 410724404002.709839, Test MSE of 410724404323.909790
Epoch 2: training loss 430062711747.765
Test Loss of 410725012462.467346, Test MSE of 410725010287.246094
Epoch 3: training loss 430062276367.059
Test Loss of 410724467021.593689, Test MSE of 410724466487.565002
Epoch 4: training loss 430061927966.118
Test Loss of 410723752008.973633, Test MSE of 410723753309.249207
Epoch 5: training loss 430061623175.529
Test Loss of 410723403418.713562, Test MSE of 410723406782.822266
Epoch 6: training loss 421601221933.176
Test Loss of 384306718669.771423, Test MSE of 384306719077.793884
Epoch 7: training loss 367546371252.706
Test Loss of 307310415119.518738, Test MSE of 307310421461.501038
Epoch 8: training loss 280941617633.882
Test Loss of 217548500031.022675, Test MSE of 217548501504.786346
Epoch 9: training loss 202459890145.882
Test Loss of 152539968430.023132, Test MSE of 152539967601.952393
Epoch 10: training loss 158457129773.176
Test Loss of 123406886179.420639, Test MSE of 123406885264.891754
Epoch 11: training loss 142418907738.353
Test Loss of 112076482532.516434, Test MSE of 112076482561.978958
Epoch 12: training loss 136968194770.824
Test Loss of 108969632532.494217, Test MSE of 108969632659.782272
Epoch 13: training loss 134278189899.294
Test Loss of 106486804114.184174, Test MSE of 106486803558.556442
Epoch 14: training loss 129956693955.765
Test Loss of 103794693299.590927, Test MSE of 103794692691.186249
Epoch 15: training loss 126977167992.471
Test Loss of 100924555658.247101, Test MSE of 100924555373.608521
Epoch 16: training loss 122057792481.882
Test Loss of 97627214367.985199, Test MSE of 97627213157.820007
Epoch 17: training loss 118028280862.118
Test Loss of 94482645681.932434, Test MSE of 94482644993.225433
Epoch 18: training loss 114598984884.706
Test Loss of 90809603721.654785, Test MSE of 90809603880.801575
Epoch 19: training loss 112131428171.294
Test Loss of 88624043737.262375, Test MSE of 88624041870.066238
Epoch 20: training loss 107428404766.118
Test Loss of 85171039825.739929, Test MSE of 85171041061.156464
Epoch 21: training loss 103453124547.765
Test Loss of 82590192087.959274, Test MSE of 82590191879.399841
Epoch 22: training loss 98083016402.824
Test Loss of 78110654918.900513, Test MSE of 78110653653.278503
Epoch 23: training loss 95488997616.941
Test Loss of 75152550410.187881, Test MSE of 75152550125.435425
Epoch 24: training loss 91800955271.529
Test Loss of 71103394309.449326, Test MSE of 71103394258.245026
Epoch 25: training loss 88096167830.588
Test Loss of 70145653419.298477, Test MSE of 70145654265.342880
Epoch 26: training loss 84038628050.824
Test Loss of 66674892292.027763, Test MSE of 66674893338.017815
Epoch 27: training loss 80446068570.353
Test Loss of 63302965832.736694, Test MSE of 63302964956.030014
Epoch 28: training loss 75981040911.059
Test Loss of 60818149290.706154, Test MSE of 60818149638.147141
Epoch 29: training loss 73487800154.353
Test Loss of 57376715402.128647, Test MSE of 57376714327.902496
Epoch 30: training loss 70547045692.235
Test Loss of 54282185294.422951, Test MSE of 54282185778.051079
Epoch 31: training loss 67375500559.059
Test Loss of 54928976799.807495, Test MSE of 54928976589.146751
Epoch 32: training loss 63392846757.647
Test Loss of 48683154093.193893, Test MSE of 48683154483.242783
Epoch 33: training loss 60797467979.294
Test Loss of 48440175263.925957, Test MSE of 48440174474.788795
Epoch 34: training loss 57387297054.118
Test Loss of 45648056354.117538, Test MSE of 45648056030.607880
Epoch 35: training loss 55502640685.176
Test Loss of 42741242582.893105, Test MSE of 42741242683.891411
Epoch 36: training loss 52300676284.235
Test Loss of 39967440242.080521, Test MSE of 39967441222.325287
Epoch 37: training loss 49480162334.118
Test Loss of 37745377537.776955, Test MSE of 37745376580.658897
Epoch 38: training loss 46241991077.647
Test Loss of 37944362427.054138, Test MSE of 37944362057.672791
Epoch 39: training loss 44749373846.588
Test Loss of 35938094383.740860, Test MSE of 35938094336.765862
Epoch 40: training loss 42218939723.294
Test Loss of 34932658646.063858, Test MSE of 34932658203.329285
Epoch 41: training loss 40623700276.706
Test Loss of 33888656183.559464, Test MSE of 33888656398.014164
Epoch 42: training loss 38142362036.706
Test Loss of 32106736322.043499, Test MSE of 32106736486.002480
Epoch 43: training loss 36096910968.471
Test Loss of 29355061333.293846, Test MSE of 29355061555.276413
Epoch 44: training loss 34398074390.588
Test Loss of 27487044850.139751, Test MSE of 27487044240.473717
Epoch 45: training loss 33073478460.235
Test Loss of 28187052466.050903, Test MSE of 28187052988.022781
Epoch 46: training loss 31208602443.294
Test Loss of 25930147823.888939, Test MSE of 25930148284.845554
Epoch 47: training loss 29747850059.294
Test Loss of 25265677649.858398, Test MSE of 25265677732.971889
Epoch 48: training loss 27890297133.176
Test Loss of 24450751732.982880, Test MSE of 24450752083.186470
Epoch 49: training loss 27056121272.471
Test Loss of 21269171945.373440, Test MSE of 21269171631.048916
Epoch 50: training loss 25599259986.824
Test Loss of 21585384011.579823, Test MSE of 21585383978.304955
Epoch 51: training loss 24601970979.765
Test Loss of 21427720227.065247, Test MSE of 21427720547.192158
Epoch 52: training loss 23420034049.882
Test Loss of 20057708664.359093, Test MSE of 20057708468.697567
Epoch 53: training loss 22580726113.882
Test Loss of 19689116842.113834, Test MSE of 19689116946.695301
Epoch 54: training loss 21671243112.471
Test Loss of 21123489202.524757, Test MSE of 21123489003.217682
Epoch 55: training loss 20975854482.824
Test Loss of 20868512135.403980, Test MSE of 20868511971.738373
Epoch 56: training loss 20172698138.353
Test Loss of 20989815920.303562, Test MSE of 20989816008.141544
Epoch 57: training loss 19682151397.647
Test Loss of 18719690020.368347, Test MSE of 18719690219.385029
Epoch 58: training loss 18688167296.000
Test Loss of 19752044749.652939, Test MSE of 19752044774.040440
Epoch 59: training loss 18280931200.000
Test Loss of 19062752215.722351, Test MSE of 19062752613.396851
Epoch 60: training loss 17777805266.824
Test Loss of 16695243097.440075, Test MSE of 16695242690.134766
Epoch 61: training loss 17204901315.765
Test Loss of 18582737339.527996, Test MSE of 18582737415.897167
Epoch 62: training loss 16521241325.176
Test Loss of 17970425896.751503, Test MSE of 17970425701.343880
Epoch 63: training loss 15991578951.529
Test Loss of 19021293596.431282, Test MSE of 19021293607.729893
Epoch 64: training loss 15712803749.647
Test Loss of 19817388608.207310, Test MSE of 19817388543.724705
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 227552846897.62653, 'MSE - std': 205363008982.91376, 'R2 - mean': -0.6438658496802525, 'R2 - std': 1.4757961495482128} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005384 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043045466.353
Test Loss of 431612787416.788513, Test MSE of 431612792248.848755
Epoch 2: training loss 424023915459.765
Test Loss of 431593405819.557617, Test MSE of 431593403028.463745
Epoch 3: training loss 423997632030.118
Test Loss of 431566571147.550232, Test MSE of 431566565240.040588
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010045199.059
Test Loss of 431570221147.927795, Test MSE of 431570222346.264771
Epoch 2: training loss 424000824621.176
Test Loss of 431570718596.797791, Test MSE of 431570717618.849731
Epoch 3: training loss 424000293466.353
Test Loss of 431570736155.957397, Test MSE of 431570729214.143433
Epoch 4: training loss 423999873505.882
Test Loss of 431569676601.691833, Test MSE of 431569679958.764343
Epoch 5: training loss 423999598351.059
Test Loss of 431569872527.341064, Test MSE of 431569877460.036682
Epoch 6: training loss 415934574351.059
Test Loss of 405236500259.183716, Test MSE of 405236497165.065796
Epoch 7: training loss 362726215680.000
Test Loss of 327102904980.553467, Test MSE of 327102900316.490601
Epoch 8: training loss 276927268924.235
Test Loss of 235740944517.627014, Test MSE of 235740944320.266846
Epoch 9: training loss 199474909063.529
Test Loss of 168264190178.502533, Test MSE of 168264189402.863953
Epoch 10: training loss 155888689874.824
Test Loss of 137539192307.916718, Test MSE of 137539193344.921844
Epoch 11: training loss 139366856794.353
Test Loss of 123819903763.546509, Test MSE of 123819903951.075775
Epoch 12: training loss 132887515376.941
Test Loss of 119689345422.985657, Test MSE of 119689344151.847610
Epoch 13: training loss 131182356208.941
Test Loss of 116848950958.615463, Test MSE of 116848952760.894791
Epoch 14: training loss 127417994541.176
Test Loss of 113317754581.471542, Test MSE of 113317755594.994324
Epoch 15: training loss 123414231280.941
Test Loss of 110905892257.465988, Test MSE of 110905891642.559128
Epoch 16: training loss 120153337012.706
Test Loss of 106623389349.138367, Test MSE of 106623388105.371445
Epoch 17: training loss 116624936116.706
Test Loss of 103253896054.108276, Test MSE of 103253896972.676041
Epoch 18: training loss 110674259516.235
Test Loss of 97927719108.175842, Test MSE of 97927719701.413834
Epoch 19: training loss 108436795211.294
Test Loss of 97382912278.626556, Test MSE of 97382911979.521225
Epoch 20: training loss 104373238452.706
Test Loss of 92931209959.951874, Test MSE of 92931209100.642212
Epoch 21: training loss 100758314224.941
Test Loss of 88508608684.483109, Test MSE of 88508608055.107788
Epoch 22: training loss 96639421289.412
Test Loss of 83767055054.363724, Test MSE of 83767054843.382141
Epoch 23: training loss 93402781394.824
Test Loss of 80713438743.929657, Test MSE of 80713438696.768387
Epoch 24: training loss 88877787376.941
Test Loss of 76285118790.959747, Test MSE of 76285118731.800766
Epoch 25: training loss 84843478678.588
Test Loss of 73590079575.189270, Test MSE of 73590079924.743790
Epoch 26: training loss 81500837827.765
Test Loss of 72243160012.349838, Test MSE of 72243160115.791306
Epoch 27: training loss 78363049472.000
Test Loss of 65950704692.597870, Test MSE of 65950704647.319382
Epoch 28: training loss 75134387486.118
Test Loss of 64833856566.493294, Test MSE of 64833857028.040901
Epoch 29: training loss 70783481005.176
Test Loss of 59655497810.450714, Test MSE of 59655497120.890083
Epoch 30: training loss 67941117168.941
Test Loss of 61022721667.968536, Test MSE of 61022721287.616295
Epoch 31: training loss 65586376820.706
Test Loss of 58132520554.380379, Test MSE of 58132520887.931862
Epoch 32: training loss 61582061899.294
Test Loss of 53616650822.841278, Test MSE of 53616649986.456253
Epoch 33: training loss 59182484653.176
Test Loss of 51517976704.888481, Test MSE of 51517976809.333771
Epoch 34: training loss 56247885417.412
Test Loss of 48382204219.587227, Test MSE of 48382204485.271431
Epoch 35: training loss 53583582023.529
Test Loss of 44761814021.212402, Test MSE of 44761815050.340797
Epoch 36: training loss 50767768440.471
Test Loss of 42937185241.617767, Test MSE of 42937184356.077538
Epoch 37: training loss 48228955120.941
Test Loss of 40696892494.186028, Test MSE of 40696892320.665131
Epoch 38: training loss 45240865264.941
Test Loss of 39427619266.161964, Test MSE of 39427619420.570740
Epoch 39: training loss 43416373729.882
Test Loss of 36618647592.751503, Test MSE of 36618647639.305717
Epoch 40: training loss 41095465057.882
Test Loss of 36583507816.366493, Test MSE of 36583507944.666252
Epoch 41: training loss 39335071766.588
Test Loss of 33434688351.363258, Test MSE of 33434688754.150726
Epoch 42: training loss 37035115346.824
Test Loss of 31973152114.080517, Test MSE of 31973151627.491341
Epoch 43: training loss 35247119924.706
Test Loss of 32063507274.513653, Test MSE of 32063507072.708706
Epoch 44: training loss 33846264500.706
Test Loss of 29406990047.896343, Test MSE of 29406990441.893322
Epoch 45: training loss 32008070799.059
Test Loss of 27184783821.534473, Test MSE of 27184784100.287865
Epoch 46: training loss 30674157206.588
Test Loss of 26770295680.533085, Test MSE of 26770295760.553555
Epoch 47: training loss 29523889389.176
Test Loss of 27394575506.421101, Test MSE of 27394575686.731075
Epoch 48: training loss 27786449460.706
Test Loss of 28907982936.136974, Test MSE of 28907982868.780605
Epoch 49: training loss 26500380122.353
Test Loss of 25577029522.065712, Test MSE of 25577029957.468307
Epoch 50: training loss 24923348163.765
Test Loss of 23066402672.895882, Test MSE of 23066402712.016361
Epoch 51: training loss 24680527348.706
Test Loss of 27115962829.534473, Test MSE of 27115962205.438293
Epoch 52: training loss 23318100352.000
Test Loss of 23463746605.963905, Test MSE of 23463746471.375759
Epoch 53: training loss 22450216926.118
Test Loss of 25409208542.711708, Test MSE of 25409208270.070362
Epoch 54: training loss 21415594921.412
Test Loss of 23188406027.490978, Test MSE of 23188405982.635525
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 186679958714.62833, 'MSE - std': 201051098779.62006, 'R2 - mean': -0.34972705510415925, 'R2 - std': 1.4451470312448396} 
 

Saving model.....
Results After CV: {'MSE - mean': 186679958714.62833, 'MSE - std': 201051098779.62006, 'R2 - mean': -0.34972705510415925, 'R2 - std': 1.4451470312448396}
Train time: 58.93421719420148
Inference time: 0.07023373219926725
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 54 finished with value: 186679958714.62833 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005929 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525533936.941
Test Loss of 418111484467.756653, Test MSE of 418111491023.462769
Epoch 2: training loss 427504803117.176
Test Loss of 418093667127.368958, Test MSE of 418093667373.227783
Epoch 3: training loss 427476712749.176
Test Loss of 418069037484.265564, Test MSE of 418069034229.665039
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492043474.824
Test Loss of 418073528057.071472, Test MSE of 418073530750.857788
Epoch 2: training loss 427481162330.353
Test Loss of 418074622506.755493, Test MSE of 418074615532.157593
Epoch 3: training loss 423709569144.471
Test Loss of 406293507909.344421, Test MSE of 406293514440.242188
Epoch 4: training loss 397292935168.000
Test Loss of 364632883840.503357, Test MSE of 364632881978.728455
Epoch 5: training loss 331545100047.059
Test Loss of 278637554596.804077, Test MSE of 278637557314.689026
Epoch 6: training loss 248774604197.647
Test Loss of 201349553685.200104, Test MSE of 201349556681.582397
Epoch 7: training loss 180235472052.706
Test Loss of 140894478349.264862, Test MSE of 140894478964.832397
Epoch 8: training loss 146484356126.118
Test Loss of 123203011408.951187, Test MSE of 123203011158.774246
Epoch 9: training loss 137948349545.412
Test Loss of 117427917727.592880, Test MSE of 117427918451.331604
Epoch 10: training loss 134709882789.647
Test Loss of 113630054536.912323, Test MSE of 113630054702.180542
Epoch 11: training loss 131315685210.353
Test Loss of 111327385150.415909, Test MSE of 111327385286.856613
Epoch 12: training loss 127048753498.353
Test Loss of 108012087116.450607, Test MSE of 108012087705.097763
Epoch 13: training loss 123416965330.824
Test Loss of 104770447066.751785, Test MSE of 104770447937.520569
Epoch 14: training loss 119382566400.000
Test Loss of 101830423550.341888, Test MSE of 101830422484.719467
Epoch 15: training loss 116624197601.882
Test Loss of 98579600177.684021, Test MSE of 98579599603.570328
Epoch 16: training loss 113454996690.824
Test Loss of 95756173577.297241, Test MSE of 95756174937.028732
Epoch 17: training loss 108837549056.000
Test Loss of 91774152625.358322, Test MSE of 91774153501.522751
Epoch 18: training loss 105679775984.941
Test Loss of 89470937158.588013, Test MSE of 89470938743.727280
Epoch 19: training loss 100976416843.294
Test Loss of 85955959190.947021, Test MSE of 85955958684.247116
Epoch 20: training loss 98911418428.235
Test Loss of 81288929386.118896, Test MSE of 81288930831.449982
Epoch 21: training loss 95087290947.765
Test Loss of 79105163809.754333, Test MSE of 79105163403.444870
Epoch 22: training loss 90243917628.235
Test Loss of 75587706868.630112, Test MSE of 75587706963.586044
Epoch 23: training loss 86444866793.412
Test Loss of 71076403578.522324, Test MSE of 71076403863.095566
Epoch 24: training loss 83258192587.294
Test Loss of 70199473038.538055, Test MSE of 70199473241.392410
Epoch 25: training loss 79378137690.353
Test Loss of 68451584288.510757, Test MSE of 68451582910.332047
Epoch 26: training loss 75731022448.941
Test Loss of 64364707650.501968, Test MSE of 64364706764.103539
Epoch 27: training loss 73914034868.706
Test Loss of 61666058552.671753, Test MSE of 61666059803.741264
Epoch 28: training loss 70345098224.941
Test Loss of 57586200813.820030, Test MSE of 57586201406.290512
Epoch 29: training loss 67040165692.235
Test Loss of 54813158390.288223, Test MSE of 54813157253.231346
Epoch 30: training loss 64541612062.118
Test Loss of 55052091690.459404, Test MSE of 55052091681.802238
Epoch 31: training loss 62011114601.412
Test Loss of 51667797107.830673, Test MSE of 51667797144.250969
Epoch 32: training loss 58509767416.471
Test Loss of 49667133339.566040, Test MSE of 49667134349.027115
Epoch 33: training loss 55500477793.882
Test Loss of 48056871162.611153, Test MSE of 48056871462.417702
Epoch 34: training loss 53573527868.235
Test Loss of 44217135465.941246, Test MSE of 44217135245.641022
Epoch 35: training loss 51859752726.588
Test Loss of 43173664007.639137, Test MSE of 43173663662.609451
Epoch 36: training loss 49217842827.294
Test Loss of 40961404431.515152, Test MSE of 40961403395.000763
Epoch 37: training loss 46555210025.412
Test Loss of 42056097897.171410, Test MSE of 42056098515.449173
Epoch 38: training loss 44649070046.118
Test Loss of 38804613956.160072, Test MSE of 38804614935.765327
Epoch 39: training loss 42671084619.294
Test Loss of 37069109789.490631, Test MSE of 37069109957.174332
Epoch 40: training loss 41469176011.294
Test Loss of 33809271009.028915, Test MSE of 33809269739.626503
Epoch 41: training loss 39082627749.647
Test Loss of 33876938471.542912, Test MSE of 33876937865.142666
Epoch 42: training loss 37190798652.235
Test Loss of 33815384169.171410, Test MSE of 33815384867.369743
Epoch 43: training loss 35567896478.118
Test Loss of 31008510817.058525, Test MSE of 31008510981.235065
Epoch 44: training loss 33808121355.294
Test Loss of 31632169066.592644, Test MSE of 31632169420.898205
Epoch 45: training loss 32260420939.294
Test Loss of 30349611508.274811, Test MSE of 30349611635.453846
Epoch 46: training loss 31315301760.000
Test Loss of 28119506514.786953, Test MSE of 28119506797.095802
Epoch 47: training loss 29795194164.706
Test Loss of 26351673037.960674, Test MSE of 26351673173.047462
Epoch 48: training loss 28336039085.176
Test Loss of 25854303539.697433, Test MSE of 25854302987.062775
Epoch 49: training loss 27621105377.882
Test Loss of 25646874123.725189, Test MSE of 25646874194.448551
Epoch 50: training loss 26684108107.294
Test Loss of 25963870353.913486, Test MSE of 25963870594.346859
Epoch 51: training loss 24979503420.235
Test Loss of 24670884389.544296, Test MSE of 24670884115.478718
Epoch 52: training loss 24447136033.882
Test Loss of 25511966526.712006, Test MSE of 25511966826.355816
Epoch 53: training loss 23437385965.176
Test Loss of 24626506201.271339, Test MSE of 24626506020.576496
Epoch 54: training loss 22188887563.294
Test Loss of 23728620076.176727, Test MSE of 23728620073.054684
Epoch 55: training loss 21417825313.882
Test Loss of 21374127656.149895, Test MSE of 21374127983.827923
Epoch 56: training loss 20954581534.118
Test Loss of 22629084121.389774, Test MSE of 22629084157.094872
Epoch 57: training loss 19852449882.353
Test Loss of 21602183022.086514, Test MSE of 21602183351.236813
Epoch 58: training loss 19415944636.235
Test Loss of 21842695458.168865, Test MSE of 21842695253.113049
Epoch 59: training loss 18578116024.471
Test Loss of 22171365150.497341, Test MSE of 22171365569.975479
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22171365569.97548, 'MSE - std': 0.0, 'R2 - mean': 0.8273491707317349, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918328048.941
Test Loss of 424557755472.299805, Test MSE of 424557753503.473328
Epoch 2: training loss 427897228589.176
Test Loss of 424540330157.864441, Test MSE of 424540336624.038269
Epoch 3: training loss 427869765872.941
Test Loss of 424517806944.584778, Test MSE of 424517807850.500977
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887838388.706
Test Loss of 424524084893.164917, Test MSE of 424524083919.423584
Epoch 2: training loss 427876890744.471
Test Loss of 424525000979.719666, Test MSE of 424525002705.796814
Epoch 3: training loss 424242215755.294
Test Loss of 413128425873.735840, Test MSE of 413128426652.519104
Epoch 4: training loss 397886091866.353
Test Loss of 372554123038.971069, Test MSE of 372554126141.236511
Epoch 5: training loss 331664254373.647
Test Loss of 287577023807.541077, Test MSE of 287577027665.670349
Epoch 6: training loss 248082866296.471
Test Loss of 211565057255.424469, Test MSE of 211565057735.418213
Epoch 7: training loss 178686444122.353
Test Loss of 152727379909.255615, Test MSE of 152727384481.956329
Epoch 8: training loss 144677743465.412
Test Loss of 134499429498.936844, Test MSE of 134499425885.006470
Epoch 9: training loss 135052536621.176
Test Loss of 128878879112.260925, Test MSE of 128878879541.271652
Epoch 10: training loss 132220238637.176
Test Loss of 125587585747.408737, Test MSE of 125587587287.481262
Epoch 11: training loss 127664347437.176
Test Loss of 123065327843.397644, Test MSE of 123065327718.104050
Epoch 12: training loss 125136287954.824
Test Loss of 118863302876.765213, Test MSE of 118863303765.659836
Epoch 13: training loss 122120077974.588
Test Loss of 117152064037.544296, Test MSE of 117152064720.353256
Epoch 14: training loss 118443147053.176
Test Loss of 112504285042.350220, Test MSE of 112504285708.862915
Epoch 15: training loss 114280517993.412
Test Loss of 110469759987.919495, Test MSE of 110469759882.258499
Epoch 16: training loss 110461242398.118
Test Loss of 105009084566.177185, Test MSE of 105009081609.597809
Epoch 17: training loss 107113012766.118
Test Loss of 101926703966.216049, Test MSE of 101926704205.774109
Epoch 18: training loss 102552160858.353
Test Loss of 96592815792.114731, Test MSE of 96592816509.465942
Epoch 19: training loss 98581940946.824
Test Loss of 94946723854.686096, Test MSE of 94946722637.833878
Epoch 20: training loss 94776109176.471
Test Loss of 89403657751.095078, Test MSE of 89403655486.748596
Epoch 21: training loss 91813972088.471
Test Loss of 85853196243.467957, Test MSE of 85853195175.171310
Epoch 22: training loss 87206324796.235
Test Loss of 81820365907.616013, Test MSE of 81820367077.551620
Epoch 23: training loss 83902587602.824
Test Loss of 79832211562.118896, Test MSE of 79832211668.376099
Epoch 24: training loss 79978317974.588
Test Loss of 76534644644.093460, Test MSE of 76534643333.033615
Epoch 25: training loss 77675284871.529
Test Loss of 74485906925.405502, Test MSE of 74485906055.212616
Epoch 26: training loss 73447282160.941
Test Loss of 69830228320.940094, Test MSE of 69830228761.398972
Epoch 27: training loss 70582141816.471
Test Loss of 68168230715.395790, Test MSE of 68168229193.586052
Epoch 28: training loss 67156750953.412
Test Loss of 64740304608.436737, Test MSE of 64740306527.331001
Epoch 29: training loss 64597794364.235
Test Loss of 60292549327.618782, Test MSE of 60292549966.974411
Epoch 30: training loss 61394651407.059
Test Loss of 57597815614.475136, Test MSE of 57597817553.101593
Epoch 31: training loss 58178561520.941
Test Loss of 58299831013.884804, Test MSE of 58299830139.826492
Epoch 32: training loss 55877783446.588
Test Loss of 52814370346.518623, Test MSE of 52814370909.995125
Epoch 33: training loss 53220198610.824
Test Loss of 53597202224.025909, Test MSE of 53597201585.232071
Epoch 34: training loss 50770390753.882
Test Loss of 50947370243.849182, Test MSE of 50947369000.889542
Epoch 35: training loss 48596843414.588
Test Loss of 47475950574.708305, Test MSE of 47475950583.361374
Epoch 36: training loss 45976804547.765
Test Loss of 47997354561.969002, Test MSE of 47997355150.971695
Epoch 37: training loss 43758519326.118
Test Loss of 45286807281.017815, Test MSE of 45286806534.403656
Epoch 38: training loss 41690928015.059
Test Loss of 43183425865.726578, Test MSE of 43183424664.127632
Epoch 39: training loss 39618482800.941
Test Loss of 40719529020.876244, Test MSE of 40719528919.690445
Epoch 40: training loss 37680499666.824
Test Loss of 40584518596.071243, Test MSE of 40584518973.436340
Epoch 41: training loss 35420171045.647
Test Loss of 39717026342.965530, Test MSE of 39717025892.851921
Epoch 42: training loss 34396361005.176
Test Loss of 35881446708.171181, Test MSE of 35881445500.934738
Epoch 43: training loss 32296415239.529
Test Loss of 34113724921.486004, Test MSE of 34113726348.944496
Epoch 44: training loss 30866212928.000
Test Loss of 35345629413.766365, Test MSE of 35345629488.069611
Epoch 45: training loss 29166672338.824
Test Loss of 34668451768.464493, Test MSE of 34668451507.637001
Epoch 46: training loss 27890827595.294
Test Loss of 35436469582.700905, Test MSE of 35436469802.105721
Epoch 47: training loss 26638667188.706
Test Loss of 31523392425.541523, Test MSE of 31523392088.832001
Epoch 48: training loss 25514395678.118
Test Loss of 29523353826.923897, Test MSE of 29523353781.779152
Epoch 49: training loss 24382886979.765
Test Loss of 31184515506.424244, Test MSE of 31184515236.103302
Epoch 50: training loss 23114208775.529
Test Loss of 28094641609.637753, Test MSE of 28094641303.087166
Epoch 51: training loss 22137091279.059
Test Loss of 33275535486.963684, Test MSE of 33275535733.996487
Epoch 52: training loss 21502513016.471
Test Loss of 30053759123.808468, Test MSE of 30053759542.979240
Epoch 53: training loss 20388996385.882
Test Loss of 29149739647.082119, Test MSE of 29149739788.305084
Epoch 54: training loss 19603072188.235
Test Loss of 27605404771.249596, Test MSE of 27605405226.750656
Epoch 55: training loss 18913676999.529
Test Loss of 25811110555.980568, Test MSE of 25811110989.758423
Epoch 56: training loss 18027887363.765
Test Loss of 26123019501.109413, Test MSE of 26123018897.823967
Epoch 57: training loss 17586178902.588
Test Loss of 28015279653.781170, Test MSE of 28015278495.616222
Epoch 58: training loss 16622147346.824
Test Loss of 27191674549.799675, Test MSE of 27191674958.272041
Epoch 59: training loss 16106596630.588
Test Loss of 25680910581.873699, Test MSE of 25680911148.896442
Epoch 60: training loss 15778244306.824
Test Loss of 24408884825.893131, Test MSE of 24408884234.232399
Epoch 61: training loss 15273097630.118
Test Loss of 24007089679.752026, Test MSE of 24007090099.483723
Epoch 62: training loss 14756858808.471
Test Loss of 26378131973.803379, Test MSE of 26378131772.456425
Epoch 63: training loss 14347067271.529
Test Loss of 23811165952.414528, Test MSE of 23811165772.923946
Epoch 64: training loss 13843163640.471
Test Loss of 22829058940.061993, Test MSE of 22829059053.251148
Epoch 65: training loss 13632899941.647
Test Loss of 24442443368.579227, Test MSE of 24442443721.258629
Epoch 66: training loss 13155765707.294
Test Loss of 23821998023.624336, Test MSE of 23821998681.258018
Epoch 67: training loss 12768703823.059
Test Loss of 24387769289.519314, Test MSE of 24387769778.072872
Epoch 68: training loss 12330742678.588
Test Loss of 22536234626.161461, Test MSE of 22536234342.302227
Epoch 69: training loss 12214370142.118
Test Loss of 23257338494.134628, Test MSE of 23257338608.470413
Epoch 70: training loss 11861811907.765
Test Loss of 25200728943.744621, Test MSE of 25200729225.169476
Epoch 71: training loss 11322753001.412
Test Loss of 23832142101.614620, Test MSE of 23832141908.984367
Epoch 72: training loss 11101359920.941
Test Loss of 24591887232.325699, Test MSE of 24591887552.003906
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23381626560.989693, 'MSE - std': 1210260991.0142136, 'R2 - mean': 0.8258897104311544, 'R2 - std': 0.001459460300580362} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004018 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927267629.176
Test Loss of 447258046273.554504, Test MSE of 447258037781.182983
Epoch 2: training loss 421906222140.235
Test Loss of 447238896951.487366, Test MSE of 447238893321.324890
Epoch 3: training loss 421878117195.294
Test Loss of 447213513962.740662, Test MSE of 447213510257.881592
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896060687.059
Test Loss of 447222043526.958130, Test MSE of 447222049986.424377
Epoch 2: training loss 421886157643.294
Test Loss of 447222509222.166077, Test MSE of 447222508166.331909
Epoch 3: training loss 418059894784.000
Test Loss of 435125817629.194519, Test MSE of 435125818860.099182
Epoch 4: training loss 391496778450.824
Test Loss of 393248158202.907227, Test MSE of 393248161144.759338
Epoch 5: training loss 325352314518.588
Test Loss of 305373438435.219971, Test MSE of 305373441980.218994
Epoch 6: training loss 242738468562.824
Test Loss of 226426589757.231567, Test MSE of 226426591389.217285
Epoch 7: training loss 174454216884.706
Test Loss of 165660674471.054352, Test MSE of 165660676676.496887
Epoch 8: training loss 141588406000.941
Test Loss of 145188157850.737000, Test MSE of 145188158731.173004
Epoch 9: training loss 132513590512.941
Test Loss of 138583843621.366638, Test MSE of 138583842376.549744
Epoch 10: training loss 130176479262.118
Test Loss of 135566726765.790421, Test MSE of 135566726467.003586
Epoch 11: training loss 126222994432.000
Test Loss of 131451391974.417770, Test MSE of 131451395447.457993
Epoch 12: training loss 122821695277.176
Test Loss of 129286874946.975708, Test MSE of 129286874012.252487
Epoch 13: training loss 119119773123.765
Test Loss of 124836431165.646072, Test MSE of 124836431433.888992
Epoch 14: training loss 116308947847.529
Test Loss of 122983913433.626648, Test MSE of 122983915916.019409
Epoch 15: training loss 111572286494.118
Test Loss of 118703184951.665054, Test MSE of 118703188011.910400
Epoch 16: training loss 108515932702.118
Test Loss of 114073428648.297943, Test MSE of 114073429102.161209
Epoch 17: training loss 104244848489.412
Test Loss of 112648184162.361328, Test MSE of 112648183776.793228
Epoch 18: training loss 100657073844.706
Test Loss of 107980648113.535965, Test MSE of 107980647615.574661
Epoch 19: training loss 97167079695.059
Test Loss of 103913077249.065933, Test MSE of 103913076583.071503
Epoch 20: training loss 93579488587.294
Test Loss of 100297171482.174423, Test MSE of 100297171739.389175
Epoch 21: training loss 89269048756.706
Test Loss of 96963993065.615540, Test MSE of 96963991930.901260
Epoch 22: training loss 85076363128.471
Test Loss of 91707765532.839233, Test MSE of 91707764876.769684
Epoch 23: training loss 82484684980.706
Test Loss of 88861303441.558182, Test MSE of 88861302290.775391
Epoch 24: training loss 79505949229.176
Test Loss of 82664016088.501511, Test MSE of 82664015023.232513
Epoch 25: training loss 75735243836.235
Test Loss of 81664319552.429337, Test MSE of 81664320674.057373
Epoch 26: training loss 72612786311.529
Test Loss of 77237104100.878098, Test MSE of 77237105013.855484
Epoch 27: training loss 68865182900.706
Test Loss of 73033534693.055756, Test MSE of 73033534847.673309
Epoch 28: training loss 66543020528.941
Test Loss of 70327201503.015503, Test MSE of 70327200917.245697
Epoch 29: training loss 63425536617.412
Test Loss of 67140092402.379829, Test MSE of 67140092489.141281
Epoch 30: training loss 60455256478.118
Test Loss of 66066349259.473511, Test MSE of 66066349802.660133
Epoch 31: training loss 57439903683.765
Test Loss of 60072976912.699516, Test MSE of 60072977833.330750
Epoch 32: training loss 55336776734.118
Test Loss of 60491821447.313438, Test MSE of 60491821935.785118
Epoch 33: training loss 52522869353.412
Test Loss of 57437085866.548233, Test MSE of 57437086051.814438
Epoch 34: training loss 49958308946.824
Test Loss of 50856601111.568817, Test MSE of 50856600213.318443
Epoch 35: training loss 48182715331.765
Test Loss of 48588051070.608376, Test MSE of 48588051322.669250
Epoch 36: training loss 45948806430.118
Test Loss of 48669615230.253067, Test MSE of 48669615752.530548
Epoch 37: training loss 43617528154.353
Test Loss of 45387120911.929680, Test MSE of 45387121065.921883
Epoch 38: training loss 41645587742.118
Test Loss of 46399078498.538979, Test MSE of 46399078603.492630
Epoch 39: training loss 39366350599.529
Test Loss of 43009141164.739304, Test MSE of 43009141628.071548
Epoch 40: training loss 37165007879.529
Test Loss of 44096238828.161926, Test MSE of 44096238515.052246
Epoch 41: training loss 35771116137.412
Test Loss of 40927263302.232712, Test MSE of 40927263489.004143
Epoch 42: training loss 34180695559.529
Test Loss of 39857044367.011795, Test MSE of 39857043826.302147
Epoch 43: training loss 32725844894.118
Test Loss of 37530286969.219521, Test MSE of 37530286926.451218
Epoch 44: training loss 31098788058.353
Test Loss of 35874063011.560493, Test MSE of 35874062818.098297
Epoch 45: training loss 29429325854.118
Test Loss of 32541252610.605598, Test MSE of 32541253093.078362
Epoch 46: training loss 28267946432.000
Test Loss of 35274714200.827202, Test MSE of 35274714753.881424
Epoch 47: training loss 26956909869.176
Test Loss of 32498599059.097847, Test MSE of 32498599176.963715
Epoch 48: training loss 25998848534.588
Test Loss of 28662836390.521397, Test MSE of 28662835994.360832
Epoch 49: training loss 24783714985.412
Test Loss of 29640868655.315289, Test MSE of 29640869184.842869
Epoch 50: training loss 23327417445.647
Test Loss of 26427208876.206337, Test MSE of 26427208953.250309
Epoch 51: training loss 22750479043.765
Test Loss of 26648759530.740688, Test MSE of 26648759835.727543
Epoch 52: training loss 21919421146.353
Test Loss of 27596861205.496181, Test MSE of 27596861308.296341
Epoch 53: training loss 21025113912.471
Test Loss of 25154079398.876705, Test MSE of 25154079717.648102
Epoch 54: training loss 19910012615.529
Test Loss of 27419066432.429333, Test MSE of 27419066179.153225
Epoch 55: training loss 19508248342.588
Test Loss of 27610317626.211426, Test MSE of 27610317476.295044
Epoch 56: training loss 19035201641.412
Test Loss of 23981801098.688873, Test MSE of 23981801149.648884
Epoch 57: training loss 18395653993.412
Test Loss of 27498158578.379829, Test MSE of 27498158763.648239
Epoch 58: training loss 17416975435.294
Test Loss of 23488111827.053436, Test MSE of 23488111789.895969
Epoch 59: training loss 17152767962.353
Test Loss of 22903022554.811012, Test MSE of 22903022743.781078
Epoch 60: training loss 16540079378.824
Test Loss of 23144250970.366875, Test MSE of 23144251294.857639
Epoch 61: training loss 16050848289.882
Test Loss of 23373088210.402035, Test MSE of 23373088386.874912
Epoch 62: training loss 15825908408.471
Test Loss of 23171147163.921352, Test MSE of 23171146978.542839
Epoch 63: training loss 15005702520.471
Test Loss of 22905760303.019199, Test MSE of 22905760136.589752
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23223004419.523045, 'MSE - std': 1013316211.8052788, 'R2 - mean': 0.8330990957598945, 'R2 - std': 0.01026501290220733} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005498 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110834688.000
Test Loss of 410765103799.618713, Test MSE of 410765095993.926331
Epoch 2: training loss 430089555004.235
Test Loss of 410746973087.807495, Test MSE of 410746972468.967896
Epoch 3: training loss 430061276461.176
Test Loss of 410722913369.558533, Test MSE of 410722913738.294739
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080423333.647
Test Loss of 410729315760.629333, Test MSE of 410729321635.976196
Epoch 2: training loss 430066444890.353
Test Loss of 410729225387.061523, Test MSE of 410729225595.719788
Epoch 3: training loss 426407905641.412
Test Loss of 399143936056.862549, Test MSE of 399143936257.277405
Epoch 4: training loss 399847028254.118
Test Loss of 357682793227.964844, Test MSE of 357682788583.806458
Epoch 5: training loss 333950855529.412
Test Loss of 272223098096.244324, Test MSE of 272223100588.539398
Epoch 6: training loss 251190500653.176
Test Loss of 195412859574.670990, Test MSE of 195412860976.684174
Epoch 7: training loss 183250486272.000
Test Loss of 135680579027.694580, Test MSE of 135680578867.472061
Epoch 8: training loss 148658871627.294
Test Loss of 116416239984.658951, Test MSE of 116416240832.871323
Epoch 9: training loss 139691961856.000
Test Loss of 111091597380.235077, Test MSE of 111091599584.178436
Epoch 10: training loss 135600459956.706
Test Loss of 108644988144.244324, Test MSE of 108644986811.864471
Epoch 11: training loss 132997888361.412
Test Loss of 105469015014.885696, Test MSE of 105469013359.466202
Epoch 12: training loss 128666906172.235
Test Loss of 102665124735.585373, Test MSE of 102665121195.844574
Epoch 13: training loss 126624706921.412
Test Loss of 100127338403.124481, Test MSE of 100127338025.459717
Epoch 14: training loss 123118434454.588
Test Loss of 96733178666.291534, Test MSE of 96733178393.484131
Epoch 15: training loss 118801760105.412
Test Loss of 93889614034.391479, Test MSE of 93889612898.178528
Epoch 16: training loss 114949368862.118
Test Loss of 91298255839.304031, Test MSE of 91298257134.813171
Epoch 17: training loss 112586319661.176
Test Loss of 87602574617.943542, Test MSE of 87602575001.769119
Epoch 18: training loss 107784814381.176
Test Loss of 84547126226.036087, Test MSE of 84547126616.223999
Epoch 19: training loss 103727896244.706
Test Loss of 82178897466.047195, Test MSE of 82178897944.881271
Epoch 20: training loss 98128154563.765
Test Loss of 79949562765.801025, Test MSE of 79949564048.239517
Epoch 21: training loss 95667000079.059
Test Loss of 77611176870.441467, Test MSE of 77611175541.328583
Epoch 22: training loss 91761902049.882
Test Loss of 73480369331.590927, Test MSE of 73480369386.349182
Epoch 23: training loss 87996879947.294
Test Loss of 70108103967.629807, Test MSE of 70108104426.890869
Epoch 24: training loss 85839367785.412
Test Loss of 68394833648.007401, Test MSE of 68394833762.545540
Epoch 25: training loss 81865572924.235
Test Loss of 65805264280.936600, Test MSE of 65805264856.811546
Epoch 26: training loss 78161415273.412
Test Loss of 64069122374.959740, Test MSE of 64069123740.995934
Epoch 27: training loss 75110734622.118
Test Loss of 60698202800.037018, Test MSE of 60698203284.586548
Epoch 28: training loss 71128351111.529
Test Loss of 57426943287.322533, Test MSE of 57426943554.310646
Epoch 29: training loss 69113826575.059
Test Loss of 57133158182.026840, Test MSE of 57133158135.540619
Epoch 30: training loss 65700684619.294
Test Loss of 53141117078.685791, Test MSE of 53141116803.782509
Epoch 31: training loss 63306098281.412
Test Loss of 50394313153.214256, Test MSE of 50394312808.340408
Epoch 32: training loss 60841533078.588
Test Loss of 48420881526.937531, Test MSE of 48420882777.722336
Epoch 33: training loss 56845835610.353
Test Loss of 46652366794.928276, Test MSE of 46652366843.414207
Epoch 34: training loss 54918239872.000
Test Loss of 45191558867.576118, Test MSE of 45191558771.953392
Epoch 35: training loss 52255989044.706
Test Loss of 46082278046.978249, Test MSE of 46082277999.601151
Epoch 36: training loss 50259451587.765
Test Loss of 42890560898.191582, Test MSE of 42890560735.885597
Epoch 37: training loss 47161962925.176
Test Loss of 40977450360.714485, Test MSE of 40977450569.405441
Epoch 38: training loss 45328344003.765
Test Loss of 41044678376.425728, Test MSE of 41044677938.969398
Epoch 39: training loss 43472024553.412
Test Loss of 36675698989.371590, Test MSE of 36675699010.685211
Epoch 40: training loss 41874318177.882
Test Loss of 37146289600.740395, Test MSE of 37146290456.756233
Epoch 41: training loss 39635156811.294
Test Loss of 35398398306.443314, Test MSE of 35398398445.478439
Epoch 42: training loss 37803066880.000
Test Loss of 34780172159.111519, Test MSE of 34780171891.373390
Epoch 43: training loss 35833086938.353
Test Loss of 34651176546.798706, Test MSE of 34651176251.424583
Epoch 44: training loss 34561344587.294
Test Loss of 31764842697.862103, Test MSE of 31764842530.142281
Epoch 45: training loss 33185972276.706
Test Loss of 32319867093.234615, Test MSE of 32319867594.403202
Epoch 46: training loss 30951755659.294
Test Loss of 28787741401.736233, Test MSE of 28787741989.268082
Epoch 47: training loss 30256151838.118
Test Loss of 26978348530.968994, Test MSE of 26978348433.260685
Epoch 48: training loss 28677590479.059
Test Loss of 27759946191.429893, Test MSE of 27759947022.669483
Epoch 49: training loss 27339427787.294
Test Loss of 26086961349.123554, Test MSE of 26086961009.696293
Epoch 50: training loss 26638648478.118
Test Loss of 26027021537.080982, Test MSE of 26027022037.334591
Epoch 51: training loss 25543235727.059
Test Loss of 26812058934.374828, Test MSE of 26812059434.143597
Epoch 52: training loss 24172686102.588
Test Loss of 24046049703.626099, Test MSE of 24046049957.951157
Epoch 53: training loss 23301115501.176
Test Loss of 23852008626.643219, Test MSE of 23852008540.386307
Epoch 54: training loss 22817197212.235
Test Loss of 23355595613.467838, Test MSE of 23355595020.667328
Epoch 55: training loss 21515109308.235
Test Loss of 21877656204.024063, Test MSE of 21877656005.056545
Epoch 56: training loss 21082117195.294
Test Loss of 22360001923.139286, Test MSE of 22360001720.300026
Epoch 57: training loss 20229432256.000
Test Loss of 23013182794.276722, Test MSE of 23013182643.653660
Epoch 58: training loss 19379711190.588
Test Loss of 23083958507.979641, Test MSE of 23083958326.258198
Epoch 59: training loss 19119215676.235
Test Loss of 22341800869.493752, Test MSE of 22341800720.321705
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23002703494.72271, 'MSE - std': 956924658.0761725, 'R2 - mean': 0.8287246905656851, 'R2 - std': 0.011680502121965444} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005515 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043229665.882
Test Loss of 431612009794.695068, Test MSE of 431612010221.520691
Epoch 2: training loss 424022897844.706
Test Loss of 431591290168.270264, Test MSE of 431591295850.188599
Epoch 3: training loss 423994884698.353
Test Loss of 431563683181.341980, Test MSE of 431563691380.150513
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424007723610.353
Test Loss of 431564010300.298035, Test MSE of 431564014932.040405
Epoch 2: training loss 423998181014.588
Test Loss of 431567882747.024536, Test MSE of 431567876977.626343
Epoch 3: training loss 420309301488.941
Test Loss of 419634559136.162903, Test MSE of 419634559986.403198
Epoch 4: training loss 393895448214.588
Test Loss of 377211727623.700134, Test MSE of 377211716582.890869
Epoch 5: training loss 328736782095.059
Test Loss of 289820865211.409546, Test MSE of 289820868843.503113
Epoch 6: training loss 246232962710.588
Test Loss of 211811314879.911163, Test MSE of 211811316544.826050
Epoch 7: training loss 177749259173.647
Test Loss of 149078091619.627960, Test MSE of 149078090048.686554
Epoch 8: training loss 144396693383.529
Test Loss of 128959053683.739014, Test MSE of 128959053651.093536
Epoch 9: training loss 136862953321.412
Test Loss of 122818458932.479401, Test MSE of 122818460922.235748
Epoch 10: training loss 132930999747.765
Test Loss of 120147445061.538177, Test MSE of 120147447496.553207
Epoch 11: training loss 129471004928.000
Test Loss of 116340007548.860718, Test MSE of 116340005219.072525
Epoch 12: training loss 126345504993.882
Test Loss of 113641683490.828323, Test MSE of 113641683961.443283
Epoch 13: training loss 122755311013.647
Test Loss of 109460951849.343826, Test MSE of 109460953375.712296
Epoch 14: training loss 118218716340.706
Test Loss of 104975773826.783890, Test MSE of 104975772624.596375
Epoch 15: training loss 114617228137.412
Test Loss of 102294816228.753357, Test MSE of 102294818684.115509
Epoch 16: training loss 111834985020.235
Test Loss of 98150476442.239700, Test MSE of 98150475964.097198
Epoch 17: training loss 107278096534.588
Test Loss of 96080857890.709854, Test MSE of 96080857710.256729
Epoch 18: training loss 103953855442.824
Test Loss of 93296284495.252197, Test MSE of 93296284786.872803
Epoch 19: training loss 101594513468.235
Test Loss of 88489056730.802399, Test MSE of 88489055861.776642
Epoch 20: training loss 95841211783.529
Test Loss of 81556316956.075897, Test MSE of 81556318690.990677
Epoch 21: training loss 92594525500.235
Test Loss of 79395824476.520126, Test MSE of 79395825488.353043
Epoch 22: training loss 88427704410.353
Test Loss of 79873225166.482178, Test MSE of 79873224136.832230
Epoch 23: training loss 85995324702.118
Test Loss of 74397288191.170761, Test MSE of 74397288528.205002
Epoch 24: training loss 82536156016.941
Test Loss of 71845633157.153168, Test MSE of 71845630496.530014
Epoch 25: training loss 78500930778.353
Test Loss of 67745077247.052292, Test MSE of 67745076785.772354
Epoch 26: training loss 75203033268.706
Test Loss of 63305070943.600182, Test MSE of 63305071749.185837
Epoch 27: training loss 71502743386.353
Test Loss of 59921549415.300323, Test MSE of 59921550166.380905
Epoch 28: training loss 69166295747.765
Test Loss of 60137898615.174454, Test MSE of 60137898114.795685
Epoch 29: training loss 66028949752.471
Test Loss of 57722589803.801941, Test MSE of 57722590395.761894
Epoch 30: training loss 62902216259.765
Test Loss of 53828694499.331787, Test MSE of 53828693311.074524
Epoch 31: training loss 60005527913.412
Test Loss of 52037811505.636276, Test MSE of 52037811419.950523
Epoch 32: training loss 57159230667.294
Test Loss of 48425015389.823227, Test MSE of 48425014980.757950
Epoch 33: training loss 55124656128.000
Test Loss of 47516195940.931053, Test MSE of 47516195520.374382
Epoch 34: training loss 52424921298.824
Test Loss of 46247860380.372047, Test MSE of 46247859938.880638
Epoch 35: training loss 50384491023.059
Test Loss of 44624043256.773712, Test MSE of 44624043157.269829
Epoch 36: training loss 47334600033.882
Test Loss of 36893795924.583061, Test MSE of 36893795863.151230
Epoch 37: training loss 45348804517.647
Test Loss of 39584516930.931976, Test MSE of 39584517145.925697
Epoch 38: training loss 43741835520.000
Test Loss of 35742976851.990746, Test MSE of 35742977412.087067
Epoch 39: training loss 41536451900.235
Test Loss of 35472247402.854233, Test MSE of 35472247771.241226
Epoch 40: training loss 39655353246.118
Test Loss of 32949432401.503006, Test MSE of 32949432886.627575
Epoch 41: training loss 37221409656.471
Test Loss of 32058342934.034245, Test MSE of 32058342190.726292
Epoch 42: training loss 36767058861.176
Test Loss of 30051819035.720501, Test MSE of 30051818771.873997
Epoch 43: training loss 34757995602.824
Test Loss of 27555023523.716797, Test MSE of 27555022817.500656
Epoch 44: training loss 32907797601.882
Test Loss of 29184199562.010181, Test MSE of 29184199157.833344
Epoch 45: training loss 31567997752.471
Test Loss of 26991368173.045811, Test MSE of 26991368216.514435
Epoch 46: training loss 30470561325.176
Test Loss of 24164793491.368813, Test MSE of 24164793536.063072
Epoch 47: training loss 29154942983.529
Test Loss of 25462607268.309116, Test MSE of 25462607258.430103
Epoch 48: training loss 27964505871.059
Test Loss of 23802373791.452106, Test MSE of 23802373677.347618
Epoch 49: training loss 26876725225.412
Test Loss of 25855334749.704765, Test MSE of 25855334897.724735
Epoch 50: training loss 25606920896.000
Test Loss of 24632269814.049053, Test MSE of 24632269371.205879
Epoch 51: training loss 24819405643.294
Test Loss of 25013224755.531700, Test MSE of 25013224562.913021
Epoch 52: training loss 24032111420.235
Test Loss of 25747516038.337807, Test MSE of 25747516075.108486
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23551666010.799866, 'MSE - std': 1392121839.9329863, 'R2 - mean': 0.8245230712559355, 'R2 - std': 0.013407524870505973} 
 

Saving model.....
Results After CV: {'MSE - mean': 23551666010.799866, 'MSE - std': 1392121839.9329863, 'R2 - mean': 0.8245230712559355, 'R2 - std': 0.013407524870505973}
Train time: 92.66228991699828
Inference time: 0.07098279439960606
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 55 finished with value: 23551666010.799866 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003864 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525465750.588
Test Loss of 418113802589.623901, Test MSE of 418113799802.404663
Epoch 2: training loss 427504787335.529
Test Loss of 418095322385.114014, Test MSE of 418095323156.873108
Epoch 3: training loss 427476685402.353
Test Loss of 418070913332.881775, Test MSE of 418070916238.275513
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493512011.294
Test Loss of 418078288801.014099, Test MSE of 418078291578.990051
Epoch 2: training loss 427483766904.471
Test Loss of 418078804008.268311, Test MSE of 418078810166.498596
Epoch 3: training loss 427483277432.471
Test Loss of 418078126651.336548, Test MSE of 418078127294.762024
Epoch 4: training loss 427482897227.294
Test Loss of 418076763834.537109, Test MSE of 418076767064.458740
Epoch 5: training loss 420427938032.941
Test Loss of 395803734287.929688, Test MSE of 395803731010.624634
Epoch 6: training loss 374581447619.765
Test Loss of 329020412825.907959, Test MSE of 329020413841.053528
Epoch 7: training loss 296197757650.824
Test Loss of 244278167209.482300, Test MSE of 244278166902.184540
Epoch 8: training loss 218400240549.647
Test Loss of 174449877479.009949, Test MSE of 174449874477.049500
Epoch 9: training loss 159996255472.941
Test Loss of 128085806718.371506, Test MSE of 128085809146.793823
Epoch 10: training loss 140530467388.235
Test Loss of 118909769750.266022, Test MSE of 118909769190.043930
Epoch 11: training loss 134696210688.000
Test Loss of 116314364779.007172, Test MSE of 116314365677.521851
Epoch 12: training loss 133440918317.176
Test Loss of 113371504124.565353, Test MSE of 113371501899.607162
Epoch 13: training loss 131090854671.059
Test Loss of 110457347303.898224, Test MSE of 110457347087.313828
Epoch 14: training loss 125425516513.882
Test Loss of 107553158955.288452, Test MSE of 107553160730.285797
Epoch 15: training loss 124087750866.824
Test Loss of 104745516205.153824, Test MSE of 104745516804.063904
Epoch 16: training loss 119575296903.529
Test Loss of 100822736878.945175, Test MSE of 100822735136.403625
Epoch 17: training loss 116502730435.765
Test Loss of 96867356420.915100, Test MSE of 96867356185.843201
Epoch 18: training loss 112800219075.765
Test Loss of 94863573581.338882, Test MSE of 94863574287.409500
Epoch 19: training loss 110044997903.059
Test Loss of 93179085254.795288, Test MSE of 93179083499.044235
Epoch 20: training loss 105866267331.765
Test Loss of 88961254530.043030, Test MSE of 88961255824.450821
Epoch 21: training loss 101930361810.824
Test Loss of 86988308396.620865, Test MSE of 86988308988.063293
Epoch 22: training loss 99927023254.588
Test Loss of 83572362903.006241, Test MSE of 83572363151.371582
Epoch 23: training loss 96135237722.353
Test Loss of 81366390186.607452, Test MSE of 81366390378.026245
Epoch 24: training loss 92956440621.176
Test Loss of 75830061136.062912, Test MSE of 75830060730.645477
Epoch 25: training loss 89310231552.000
Test Loss of 74641348825.448990, Test MSE of 74641349516.858231
Epoch 26: training loss 86431211655.529
Test Loss of 73752866771.467957, Test MSE of 73752865626.872223
Epoch 27: training loss 82457011576.471
Test Loss of 68235993315.397644, Test MSE of 68235994707.632896
Epoch 28: training loss 80162012363.294
Test Loss of 67589481893.869995, Test MSE of 67589481795.396599
Epoch 29: training loss 76775230720.000
Test Loss of 67563352639.126534, Test MSE of 67563353496.812309
Epoch 30: training loss 74756116103.529
Test Loss of 64441424100.818878, Test MSE of 64441425622.352364
Epoch 31: training loss 71646910644.706
Test Loss of 62528490596.197083, Test MSE of 62528489933.998016
Epoch 32: training loss 69780719977.412
Test Loss of 56959382334.948875, Test MSE of 56959381620.611465
Epoch 33: training loss 65868058443.294
Test Loss of 57867357155.338425, Test MSE of 57867357028.845612
Epoch 34: training loss 64004150076.235
Test Loss of 53499307608.945641, Test MSE of 53499308233.467491
Epoch 35: training loss 60361000538.353
Test Loss of 52157779732.074951, Test MSE of 52157778524.310806
Epoch 36: training loss 58476841577.412
Test Loss of 51181565224.801292, Test MSE of 51181564864.139816
Epoch 37: training loss 55368889328.941
Test Loss of 47903618774.961830, Test MSE of 47903618960.536575
Epoch 38: training loss 52827196724.706
Test Loss of 44528734292.326622, Test MSE of 44528734732.492142
Epoch 39: training loss 51103237692.235
Test Loss of 43094596244.874390, Test MSE of 43094597091.565636
Epoch 40: training loss 48758373259.294
Test Loss of 41764337663.526253, Test MSE of 41764337723.223328
Epoch 41: training loss 46698707561.412
Test Loss of 39016040185.071480, Test MSE of 39016039864.966148
Epoch 42: training loss 44820134921.412
Test Loss of 39309148916.097153, Test MSE of 39309149207.310966
Epoch 43: training loss 42526510494.118
Test Loss of 34137996974.693501, Test MSE of 34137996248.398163
Epoch 44: training loss 40637138597.647
Test Loss of 36834933207.376358, Test MSE of 36834933759.866257
Epoch 45: training loss 39511274298.353
Test Loss of 35926335254.917419, Test MSE of 35926335288.917854
Epoch 46: training loss 37457550569.412
Test Loss of 33552603585.584084, Test MSE of 33552603656.818573
Epoch 47: training loss 35531015533.176
Test Loss of 31269372558.715706, Test MSE of 31269372577.657547
Epoch 48: training loss 34135047356.235
Test Loss of 30796285604.744854, Test MSE of 30796285977.995190
Epoch 49: training loss 32282178981.647
Test Loss of 27345247268.715244, Test MSE of 27345247825.186321
Epoch 50: training loss 30510822482.824
Test Loss of 27628836013.627575, Test MSE of 27628835653.324741
Epoch 51: training loss 29516845285.647
Test Loss of 24682562034.379829, Test MSE of 24682561893.611389
Epoch 52: training loss 28752323911.529
Test Loss of 26313901620.941013, Test MSE of 26313902202.572388
Epoch 53: training loss 27031825129.412
Test Loss of 24086208992.851261, Test MSE of 24086208966.962662
Epoch 54: training loss 25937141741.176
Test Loss of 25421625024.222069, Test MSE of 25421625396.858994
Epoch 55: training loss 24705143006.118
Test Loss of 22942157722.381680, Test MSE of 22942158017.368900
Epoch 56: training loss 24136308126.118
Test Loss of 22956052785.091835, Test MSE of 22956052600.186363
Epoch 57: training loss 23087639544.471
Test Loss of 22782612075.184826, Test MSE of 22782611549.339108
Epoch 58: training loss 22451631589.647
Test Loss of 21122362644.430256, Test MSE of 21122362484.386452
Epoch 59: training loss 21655673509.647
Test Loss of 21629176436.185982, Test MSE of 21629176514.204002
Epoch 60: training loss 20579506906.353
Test Loss of 21071217262.027294, Test MSE of 21071217202.243732
Epoch 61: training loss 20058717001.412
Test Loss of 22887884887.642841, Test MSE of 22887885014.111546
Epoch 62: training loss 19496352666.353
Test Loss of 20270841075.741844, Test MSE of 20270841067.296734
Epoch 63: training loss 18849725116.235
Test Loss of 22282362685.764515, Test MSE of 22282363117.246964
Epoch 64: training loss 17970115802.353
Test Loss of 20197324677.773769, Test MSE of 20197324809.212894
Epoch 65: training loss 17252051621.647
Test Loss of 20045886873.552624, Test MSE of 20045886443.602158
Epoch 66: training loss 17204914036.706
Test Loss of 21238775970.494564, Test MSE of 21238775947.277550
Epoch 67: training loss 16706550699.294
Test Loss of 20738503005.150127, Test MSE of 20738503248.412689
Epoch 68: training loss 16097462516.706
Test Loss of 19658867324.239647, Test MSE of 19658867638.760258
Epoch 69: training loss 15590565417.412
Test Loss of 18983078050.968307, Test MSE of 18983077975.109985
Epoch 70: training loss 15161405191.529
Test Loss of 20686930842.855423, Test MSE of 20686931024.514950
Epoch 71: training loss 14840414652.235
Test Loss of 17163246812.054592, Test MSE of 17163247192.104116
Epoch 72: training loss 14231020378.353
Test Loss of 19027817932.243351, Test MSE of 19027817964.211933
Epoch 73: training loss 14122778936.471
Test Loss of 20469510806.295628, Test MSE of 20469510439.035381
Epoch 74: training loss 13843849829.647
Test Loss of 19329446562.612999, Test MSE of 19329446229.119564
Epoch 75: training loss 13381621255.529
Test Loss of 18636883538.786953, Test MSE of 18636883973.119461
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18636883973.11946, 'MSE - std': 0.0, 'R2 - mean': 0.8548725624147889, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005470 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918022294.588
Test Loss of 424556096932.448792, Test MSE of 424556104639.218933
Epoch 2: training loss 427897321351.529
Test Loss of 424540116745.178833, Test MSE of 424540119225.907837
Epoch 3: training loss 427869948024.471
Test Loss of 424517991364.071228, Test MSE of 424517994821.110901
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886584289.882
Test Loss of 424524147291.077515, Test MSE of 424524139671.486511
Epoch 2: training loss 427877363471.059
Test Loss of 424525328303.463318, Test MSE of 424525328894.693726
Epoch 3: training loss 427876879661.176
Test Loss of 424525511411.623413, Test MSE of 424525512336.047424
Epoch 4: training loss 427876448858.353
Test Loss of 424525127208.149902, Test MSE of 424525128864.088806
Epoch 5: training loss 420993219162.353
Test Loss of 403193395769.915344, Test MSE of 403193405240.563477
Epoch 6: training loss 375046509628.235
Test Loss of 337459469962.688904, Test MSE of 337459471317.421082
Epoch 7: training loss 295964971248.941
Test Loss of 253585814291.364319, Test MSE of 253585818926.402802
Epoch 8: training loss 217738595388.235
Test Loss of 184586089648.470032, Test MSE of 184586089386.357056
Epoch 9: training loss 159292470181.647
Test Loss of 138870942786.087433, Test MSE of 138870945590.433258
Epoch 10: training loss 138148615559.529
Test Loss of 130213772353.139954, Test MSE of 130213771528.894119
Epoch 11: training loss 134205986454.588
Test Loss of 127662976289.695114, Test MSE of 127662978675.450317
Epoch 12: training loss 130942900284.235
Test Loss of 124676545752.738373, Test MSE of 124676545403.204407
Epoch 13: training loss 127579654625.882
Test Loss of 122011404459.021973, Test MSE of 122011405732.795944
Epoch 14: training loss 123844694768.941
Test Loss of 119515035675.950958, Test MSE of 119515034282.093750
Epoch 15: training loss 120326977566.118
Test Loss of 114890269807.093216, Test MSE of 114890271029.437668
Epoch 16: training loss 115498331196.235
Test Loss of 111758630064.943787, Test MSE of 111758631577.804672
Epoch 17: training loss 112630875196.235
Test Loss of 108178828099.212585, Test MSE of 108178826510.123337
Epoch 18: training loss 109132761931.294
Test Loss of 103977883170.938705, Test MSE of 103977884332.003418
Epoch 19: training loss 106506579817.412
Test Loss of 101381413960.719864, Test MSE of 101381410283.447525
Epoch 20: training loss 102645746176.000
Test Loss of 98946367253.022446, Test MSE of 98946368131.151535
Epoch 21: training loss 99325341967.059
Test Loss of 95621084155.499420, Test MSE of 95621084107.177078
Epoch 22: training loss 95766341120.000
Test Loss of 91744464756.718948, Test MSE of 91744464017.670944
Epoch 23: training loss 92302149857.882
Test Loss of 87563724541.808929, Test MSE of 87563726775.823669
Epoch 24: training loss 89421492946.824
Test Loss of 84793397265.054825, Test MSE of 84793398103.045654
Epoch 25: training loss 86612380340.706
Test Loss of 83184409890.642609, Test MSE of 83184409870.283905
Epoch 26: training loss 82748055597.176
Test Loss of 80144531618.020813, Test MSE of 80144529073.341995
Epoch 27: training loss 80099472956.235
Test Loss of 77768219267.819565, Test MSE of 77768218010.534897
Epoch 28: training loss 76289983879.529
Test Loss of 74706415761.913483, Test MSE of 74706416434.933624
Epoch 29: training loss 73676874300.235
Test Loss of 70774615216.943787, Test MSE of 70774617492.182663
Epoch 30: training loss 70449361317.647
Test Loss of 66803159771.462410, Test MSE of 66803160569.594955
Epoch 31: training loss 67994477101.176
Test Loss of 65585401494.058754, Test MSE of 65585400523.588989
Epoch 32: training loss 65568467968.000
Test Loss of 62014485450.229935, Test MSE of 62014486218.965836
Epoch 33: training loss 62189475011.765
Test Loss of 59541581151.281982, Test MSE of 59541581776.022079
Epoch 34: training loss 59659717541.647
Test Loss of 61760181286.136482, Test MSE of 61760180690.929886
Epoch 35: training loss 56332040176.941
Test Loss of 55780738174.726807, Test MSE of 55780737823.231750
Epoch 36: training loss 54429608402.824
Test Loss of 56317584547.205185, Test MSE of 56317584360.491585
Epoch 37: training loss 51403401185.882
Test Loss of 50579446698.015266, Test MSE of 50579446351.839005
Epoch 38: training loss 50133653217.882
Test Loss of 48604156123.343971, Test MSE of 48604158046.011780
Epoch 39: training loss 46960575254.588
Test Loss of 49552468538.389084, Test MSE of 49552471166.694740
Epoch 40: training loss 45168865776.941
Test Loss of 47483923950.352997, Test MSE of 47483924678.881126
Epoch 41: training loss 42838408237.176
Test Loss of 44132826914.761047, Test MSE of 44132826025.288139
Epoch 42: training loss 40458765251.765
Test Loss of 44971480231.942635, Test MSE of 44971480093.391014
Epoch 43: training loss 38637856278.588
Test Loss of 39206965895.135788, Test MSE of 39206965144.581703
Epoch 44: training loss 36820706364.235
Test Loss of 39460120331.073792, Test MSE of 39460120513.677238
Epoch 45: training loss 35439007585.882
Test Loss of 38842349757.024292, Test MSE of 38842348108.853325
Epoch 46: training loss 33326946025.412
Test Loss of 37816573877.148277, Test MSE of 37816575121.824913
Epoch 47: training loss 31535979700.706
Test Loss of 34543676910.352997, Test MSE of 34543676792.472466
Epoch 48: training loss 30251401012.706
Test Loss of 32924606862.419617, Test MSE of 32924606487.369251
Epoch 49: training loss 28863947053.176
Test Loss of 35465001001.689568, Test MSE of 35465000441.254021
Epoch 50: training loss 27647949248.000
Test Loss of 32767921671.935230, Test MSE of 32767922133.419334
Epoch 51: training loss 26107422983.529
Test Loss of 30993350364.646774, Test MSE of 30993349861.421814
Epoch 52: training loss 25207322812.235
Test Loss of 30100554121.208420, Test MSE of 30100554865.610390
Epoch 53: training loss 23766647314.824
Test Loss of 28470729933.842239, Test MSE of 28470729452.447506
Epoch 54: training loss 22710220536.471
Test Loss of 29595719019.599354, Test MSE of 29595719371.494095
Epoch 55: training loss 21854585204.706
Test Loss of 29893820332.147121, Test MSE of 29893820250.473976
Epoch 56: training loss 20816317801.412
Test Loss of 28236195841.421234, Test MSE of 28236196136.417591
Epoch 57: training loss 20121922409.412
Test Loss of 26540662316.887348, Test MSE of 26540662156.842804
Epoch 58: training loss 19221899023.059
Test Loss of 26589413614.767521, Test MSE of 26589413492.445053
Epoch 59: training loss 18960126768.941
Test Loss of 27792670169.981956, Test MSE of 27792671045.632050
Epoch 60: training loss 17862772781.176
Test Loss of 27453954126.641685, Test MSE of 27453954253.639748
Epoch 61: training loss 17233192997.647
Test Loss of 26888780671.851955, Test MSE of 26888781440.853325
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22762832706.986393, 'MSE - std': 4125948733.866932, 'R2 - mean': 0.8314522575759999, 'R2 - std': 0.02342030483878893} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005472 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926983077.647
Test Loss of 447257259629.316650, Test MSE of 447257252280.311951
Epoch 2: training loss 421905787241.412
Test Loss of 447237944125.053894, Test MSE of 447237949282.920166
Epoch 3: training loss 421878462584.471
Test Loss of 447213558274.960876, Test MSE of 447213561185.908142
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899731666.824
Test Loss of 447220642666.533447, Test MSE of 447220635012.194092
Epoch 2: training loss 421887277417.412
Test Loss of 447221348948.445068, Test MSE of 447221349908.035950
Epoch 3: training loss 421886507248.941
Test Loss of 447219741853.520264, Test MSE of 447219740341.109741
Epoch 4: training loss 421886059580.235
Test Loss of 447219318287.515137, Test MSE of 447219312774.795044
Epoch 5: training loss 415313268856.471
Test Loss of 425599899575.517029, Test MSE of 425599902294.799500
Epoch 6: training loss 370692848218.353
Test Loss of 359045553840.588501, Test MSE of 359045551626.118652
Epoch 7: training loss 292573984165.647
Test Loss of 272241334078.238251, Test MSE of 272241334339.726105
Epoch 8: training loss 213086067169.882
Test Loss of 198787647103.792725, Test MSE of 198787644229.627930
Epoch 9: training loss 154161605300.706
Test Loss of 150366900414.445526, Test MSE of 150366902085.243317
Epoch 10: training loss 134629065728.000
Test Loss of 140435064534.961823, Test MSE of 140435060678.591675
Epoch 11: training loss 131683687122.824
Test Loss of 137498910593.036316, Test MSE of 137498910781.021545
Epoch 12: training loss 128514132871.529
Test Loss of 134665041864.571823, Test MSE of 134665041747.765091
Epoch 13: training loss 125450631017.412
Test Loss of 130427801551.677994, Test MSE of 130427801470.786316
Epoch 14: training loss 120964901074.824
Test Loss of 128143709453.324081, Test MSE of 128143712948.739243
Epoch 15: training loss 117341346243.765
Test Loss of 124752665121.280594, Test MSE of 124752665369.568939
Epoch 16: training loss 114475817110.588
Test Loss of 122374525933.287064, Test MSE of 122374524390.458878
Epoch 17: training loss 111595588216.471
Test Loss of 116662175352.212814, Test MSE of 116662175285.330597
Epoch 18: training loss 108366312448.000
Test Loss of 113931400118.332642, Test MSE of 113931403493.748444
Epoch 19: training loss 104177408060.235
Test Loss of 111435288520.098083, Test MSE of 111435289451.949371
Epoch 20: training loss 99783783875.765
Test Loss of 108544202615.087677, Test MSE of 108544203714.336426
Epoch 21: training loss 97888029364.706
Test Loss of 105373260567.628036, Test MSE of 105373262226.261261
Epoch 22: training loss 93754465852.235
Test Loss of 101362376753.269485, Test MSE of 101362377377.814911
Epoch 23: training loss 90693529901.176
Test Loss of 99062590693.055756, Test MSE of 99062589359.611191
Epoch 24: training loss 87857574083.765
Test Loss of 95298634031.196854, Test MSE of 95298633216.986755
Epoch 25: training loss 83440955196.235
Test Loss of 93913487554.709229, Test MSE of 93913488531.881744
Epoch 26: training loss 81189124291.765
Test Loss of 86598115623.616928, Test MSE of 86598114440.143829
Epoch 27: training loss 78354489404.235
Test Loss of 84361904673.754333, Test MSE of 84361905406.142166
Epoch 28: training loss 74549327194.353
Test Loss of 82278325201.336105, Test MSE of 82278326730.706482
Epoch 29: training loss 71013468069.647
Test Loss of 78787210434.946106, Test MSE of 78787211494.113968
Epoch 30: training loss 69702652190.118
Test Loss of 75670613133.886658, Test MSE of 75670612504.102295
Epoch 31: training loss 66786120960.000
Test Loss of 71675746716.631973, Test MSE of 71675748098.921524
Epoch 32: training loss 64375190558.118
Test Loss of 71159196902.950729, Test MSE of 71159196254.467499
Epoch 33: training loss 61409906386.824
Test Loss of 65857994255.515152, Test MSE of 65857995079.778511
Epoch 34: training loss 57968430546.824
Test Loss of 63525783116.628265, Test MSE of 63525784135.699867
Epoch 35: training loss 55278210123.294
Test Loss of 60979007826.253990, Test MSE of 60979009151.530563
Epoch 36: training loss 53186481159.529
Test Loss of 58594273172.459869, Test MSE of 58594274341.824509
Epoch 37: training loss 50865447695.059
Test Loss of 57614831904.984505, Test MSE of 57614831985.181297
Epoch 38: training loss 48566606313.412
Test Loss of 53827454214.691650, Test MSE of 53827454670.291336
Epoch 39: training loss 46613211045.647
Test Loss of 50170881539.671524, Test MSE of 50170881726.219772
Epoch 40: training loss 44442136560.941
Test Loss of 46734039158.910019, Test MSE of 46734040050.895119
Epoch 41: training loss 41412051072.000
Test Loss of 48345226875.529030, Test MSE of 48345227261.512932
Epoch 42: training loss 40001675527.529
Test Loss of 45601944478.645386, Test MSE of 45601944677.371498
Epoch 43: training loss 38391457054.118
Test Loss of 42495323091.231087, Test MSE of 42495323102.753922
Epoch 44: training loss 36424480082.824
Test Loss of 40719839636.578300, Test MSE of 40719840115.533943
Epoch 45: training loss 35319122612.706
Test Loss of 39171726345.238029, Test MSE of 39171726399.052483
Epoch 46: training loss 33323606912.000
Test Loss of 34196480864.111034, Test MSE of 34196480436.265728
Epoch 47: training loss 31521718806.588
Test Loss of 38829947319.161690, Test MSE of 38829947180.499504
Epoch 48: training loss 30154105472.000
Test Loss of 39642969408.488548, Test MSE of 39642969557.309456
Epoch 49: training loss 28732105584.941
Test Loss of 32662752576.962296, Test MSE of 32662753121.970852
Epoch 50: training loss 27823265310.118
Test Loss of 31733502196.452465, Test MSE of 31733502303.182991
Epoch 51: training loss 26326632726.588
Test Loss of 30585567075.190376, Test MSE of 30585567296.457539
Epoch 52: training loss 25553670859.294
Test Loss of 32816371783.535507, Test MSE of 32816372183.395203
Epoch 53: training loss 24241358987.294
Test Loss of 33819778440.971546, Test MSE of 33819778574.540562
Epoch 54: training loss 23184506812.235
Test Loss of 28840609895.750172, Test MSE of 28840609694.354591
Epoch 55: training loss 22226397052.235
Test Loss of 27899851421.401806, Test MSE of 27899851843.747139
Epoch 56: training loss 21625175469.176
Test Loss of 27916345921.969002, Test MSE of 27916345808.293346
Epoch 57: training loss 20546037221.647
Test Loss of 26944169164.894749, Test MSE of 26944168837.782867
Epoch 58: training loss 20113935085.176
Test Loss of 23585852687.692806, Test MSE of 23585852827.631092
Epoch 59: training loss 19493972393.412
Test Loss of 24888677236.008327, Test MSE of 24888677231.673149
Epoch 60: training loss 18616126162.824
Test Loss of 26044509373.734905, Test MSE of 26044509585.379612
Epoch 61: training loss 17734846332.235
Test Loss of 25564313266.483459, Test MSE of 25564312992.262115
Epoch 62: training loss 17406333616.941
Test Loss of 24709855907.086746, Test MSE of 24709855926.991695
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23411840446.98816, 'MSE - std': 3491617208.328613, 'R2 - mean': 0.8328042116508249, 'R2 - std': 0.019217943320468465} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005643 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110071386.353
Test Loss of 410764360643.820435, Test MSE of 410764357370.408020
Epoch 2: training loss 430089446580.706
Test Loss of 410747150059.268860, Test MSE of 410747151567.271057
Epoch 3: training loss 430062392018.824
Test Loss of 410724258555.853760, Test MSE of 410724260493.560242
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077876585.412
Test Loss of 410730110198.878296, Test MSE of 410730114312.828613
Epoch 2: training loss 430067554183.529
Test Loss of 410730333570.191589, Test MSE of 410730328476.024963
Epoch 3: training loss 430067062663.529
Test Loss of 410729756619.875977, Test MSE of 410729756516.863037
Epoch 4: training loss 430066733296.941
Test Loss of 410729968830.963440, Test MSE of 410729971731.837524
Epoch 5: training loss 423310056267.294
Test Loss of 389423312723.516907, Test MSE of 389423308049.392822
Epoch 6: training loss 377733372988.235
Test Loss of 322768013644.645996, Test MSE of 322768017148.305359
Epoch 7: training loss 299321077398.588
Test Loss of 237695555892.005554, Test MSE of 237695557889.954834
Epoch 8: training loss 220068337061.647
Test Loss of 168315023013.138367, Test MSE of 168315026325.280182
Epoch 9: training loss 162483550328.471
Test Loss of 121255620761.528915, Test MSE of 121255620303.745300
Epoch 10: training loss 142738710347.294
Test Loss of 113069829167.859329, Test MSE of 113069828938.596329
Epoch 11: training loss 138285352869.647
Test Loss of 110170644191.896347, Test MSE of 110170644657.502472
Epoch 12: training loss 134284952967.529
Test Loss of 107829636606.341507, Test MSE of 107829637336.769501
Epoch 13: training loss 130968070686.118
Test Loss of 104768401061.138367, Test MSE of 104768403386.239868
Epoch 14: training loss 127789985340.235
Test Loss of 101927492551.611298, Test MSE of 101927493458.112045
Epoch 15: training loss 124412498070.588
Test Loss of 97585612056.048126, Test MSE of 97585612217.274734
Epoch 16: training loss 121255260461.176
Test Loss of 95561800799.244797, Test MSE of 95561801849.105652
Epoch 17: training loss 117487539290.353
Test Loss of 93658603053.726974, Test MSE of 93658604406.668442
Epoch 18: training loss 113315248850.824
Test Loss of 90199197141.590012, Test MSE of 90199197229.369629
Epoch 19: training loss 111259973782.588
Test Loss of 88203698191.163345, Test MSE of 88203697549.460983
Epoch 20: training loss 107006062923.294
Test Loss of 84691557683.531693, Test MSE of 84691557121.554581
Epoch 21: training loss 103637804062.118
Test Loss of 80621131231.067093, Test MSE of 80621132557.256912
Epoch 22: training loss 100075393189.647
Test Loss of 79144619419.779724, Test MSE of 79144620164.521149
Epoch 23: training loss 97467893428.706
Test Loss of 77541356075.357712, Test MSE of 77541356427.509567
Epoch 24: training loss 93855010635.294
Test Loss of 74301506324.494217, Test MSE of 74301506505.845795
Epoch 25: training loss 90446327687.529
Test Loss of 71037281870.896805, Test MSE of 71037282073.251526
Epoch 26: training loss 86556390189.176
Test Loss of 69924711828.198059, Test MSE of 69924709644.832260
Epoch 27: training loss 83642840350.118
Test Loss of 66965295136.222122, Test MSE of 66965294922.121429
Epoch 28: training loss 81034258160.941
Test Loss of 64052703237.686256, Test MSE of 64052702588.201180
Epoch 29: training loss 77638023017.412
Test Loss of 60928810378.720963, Test MSE of 60928809218.594124
Epoch 30: training loss 74722090887.529
Test Loss of 62168480173.312355, Test MSE of 62168480272.434227
Epoch 31: training loss 72022212728.471
Test Loss of 56382303996.327629, Test MSE of 56382303821.425133
Epoch 32: training loss 69017200203.294
Test Loss of 54210169589.219803, Test MSE of 54210169140.009468
Epoch 33: training loss 66764393818.353
Test Loss of 52232025754.713562, Test MSE of 52232025724.821693
Epoch 34: training loss 63306646814.118
Test Loss of 51968241768.248032, Test MSE of 51968241898.263908
Epoch 35: training loss 61186248101.647
Test Loss of 49955650861.845444, Test MSE of 49955651391.468407
Epoch 36: training loss 58353055638.588
Test Loss of 48126360352.814438, Test MSE of 48126359962.902489
Epoch 37: training loss 55530456109.176
Test Loss of 45686815094.345207, Test MSE of 45686814826.311111
Epoch 38: training loss 53278121110.588
Test Loss of 44538672313.277184, Test MSE of 44538672935.335899
Epoch 39: training loss 51421403768.471
Test Loss of 41618490587.394722, Test MSE of 41618490296.430702
Epoch 40: training loss 49340522315.294
Test Loss of 39626232613.552986, Test MSE of 39626233208.219849
Epoch 41: training loss 46867507734.588
Test Loss of 38310687652.546043, Test MSE of 38310687190.772415
Epoch 42: training loss 44349879280.941
Test Loss of 38657702950.382233, Test MSE of 38657702670.466103
Epoch 43: training loss 43149381699.765
Test Loss of 35873719411.146690, Test MSE of 35873719365.484612
Epoch 44: training loss 40568191834.353
Test Loss of 35986935349.308655, Test MSE of 35986935269.288162
Epoch 45: training loss 38717398392.471
Test Loss of 31596894216.529385, Test MSE of 31596894223.093384
Epoch 46: training loss 37320348272.941
Test Loss of 29571849688.433132, Test MSE of 29571850175.849068
Epoch 47: training loss 35228993152.000
Test Loss of 30216592930.354465, Test MSE of 30216593430.114567
Epoch 48: training loss 34185652480.000
Test Loss of 30430320468.464600, Test MSE of 30430320661.056736
Epoch 49: training loss 32275985332.706
Test Loss of 29371381442.517353, Test MSE of 29371381150.891106
Epoch 50: training loss 30925820333.176
Test Loss of 25131397894.278576, Test MSE of 25131397768.562260
Epoch 51: training loss 29752523233.882
Test Loss of 23109236860.149929, Test MSE of 23109237011.399086
Epoch 52: training loss 28670570947.765
Test Loss of 23803499919.459511, Test MSE of 23803499939.233555
Epoch 53: training loss 27103171915.294
Test Loss of 24491478983.137436, Test MSE of 24491479387.889725
Epoch 54: training loss 25791829959.529
Test Loss of 22932840459.372513, Test MSE of 22932840453.089775
Epoch 55: training loss 24680101534.118
Test Loss of 21024355202.428505, Test MSE of 21024355409.658394
Epoch 56: training loss 24315319363.765
Test Loss of 21283630018.872746, Test MSE of 21283629818.950768
Epoch 57: training loss 23455650593.882
Test Loss of 19858461950.459972, Test MSE of 19858462078.667763
Epoch 58: training loss 22455273686.588
Test Loss of 22227214847.763073, Test MSE of 22227214629.059738
Epoch 59: training loss 21418557857.882
Test Loss of 19991422989.267933, Test MSE of 19991423162.926800
Epoch 60: training loss 21065442974.118
Test Loss of 19008070916.146229, Test MSE of 19008071035.339130
Epoch 61: training loss 20282565820.235
Test Loss of 19117986249.743637, Test MSE of 19117986205.337807
Epoch 62: training loss 19606092905.412
Test Loss of 18946487893.530773, Test MSE of 18946488341.430340
Epoch 63: training loss 18744182560.000
Test Loss of 20203753483.372513, Test MSE of 20203753366.709175
Epoch 64: training loss 18275311969.882
Test Loss of 18649771467.165203, Test MSE of 18649771480.635529
Epoch 65: training loss 17800835433.412
Test Loss of 18750712797.408607, Test MSE of 18750712644.243637
Epoch 66: training loss 17129917048.471
Test Loss of 18034656873.432671, Test MSE of 18034657021.009300
Epoch 67: training loss 16597569746.824
Test Loss of 16794427526.100880, Test MSE of 16794427949.375788
Epoch 68: training loss 16390273754.353
Test Loss of 17931049801.092087, Test MSE of 17931049701.720421
Epoch 69: training loss 15815981571.765
Test Loss of 18215831472.392410, Test MSE of 18215831725.649742
Epoch 70: training loss 15211411824.941
Test Loss of 17624071855.563164, Test MSE of 17624071743.590946
Epoch 71: training loss 15059480431.059
Test Loss of 18018013782.004627, Test MSE of 18018013943.069077
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22063383821.00839, 'MSE - std': 3820804740.3091164, 'R2 - mean': 0.8374251438991607, 'R2 - std': 0.018467702987450357} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005444 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424044017664.000
Test Loss of 431612893029.523376, Test MSE of 431612894086.204346
Epoch 2: training loss 424023664037.647
Test Loss of 431591878293.975037, Test MSE of 431591873191.776794
Epoch 3: training loss 423995305261.176
Test Loss of 431563094782.696899, Test MSE of 431563096102.304749
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424006679250.824
Test Loss of 431565045471.896362, Test MSE of 431565046840.947144
Epoch 2: training loss 423997264233.412
Test Loss of 431568825786.106445, Test MSE of 431568824819.499634
Epoch 3: training loss 423996733078.588
Test Loss of 431568949200.614502, Test MSE of 431568948339.547119
Epoch 4: training loss 423996285891.765
Test Loss of 431568917236.272095, Test MSE of 431568912334.869751
Epoch 5: training loss 417469576372.706
Test Loss of 410223162884.975464, Test MSE of 410223161060.934998
Epoch 6: training loss 372988632003.765
Test Loss of 341615029235.205933, Test MSE of 341615030456.946411
Epoch 7: training loss 295052863186.824
Test Loss of 255580823358.667297, Test MSE of 255580820157.178986
Epoch 8: training loss 216572605982.118
Test Loss of 183911001454.289673, Test MSE of 183911000738.427246
Epoch 9: training loss 158638599047.529
Test Loss of 134951284669.660339, Test MSE of 134951286676.542999
Epoch 10: training loss 138106001408.000
Test Loss of 124468487686.870895, Test MSE of 124468487533.146744
Epoch 11: training loss 134706126938.353
Test Loss of 121045312333.356781, Test MSE of 121045315873.067596
Epoch 12: training loss 130833957315.765
Test Loss of 117092302707.739014, Test MSE of 117092302591.469757
Epoch 13: training loss 129110889984.000
Test Loss of 113913050598.648773, Test MSE of 113913051811.028168
Epoch 14: training loss 124091732224.000
Test Loss of 110729655604.479401, Test MSE of 110729656315.495438
Epoch 15: training loss 120333878618.353
Test Loss of 107921309400.314667, Test MSE of 107921311690.186646
Epoch 16: training loss 118966371599.059
Test Loss of 103711209340.268387, Test MSE of 103711208234.030365
Epoch 17: training loss 115422507640.471
Test Loss of 101001392558.260071, Test MSE of 101001393949.577255
Epoch 18: training loss 110485139937.882
Test Loss of 97976877635.998154, Test MSE of 97976878462.703735
Epoch 19: training loss 107497185400.471
Test Loss of 93714963255.559464, Test MSE of 93714962745.249008
Epoch 20: training loss 104424610168.471
Test Loss of 91244475833.158722, Test MSE of 91244476384.045120
Epoch 21: training loss 99645103826.824
Test Loss of 86773564620.231369, Test MSE of 86773563297.150162
Epoch 22: training loss 97192187949.176
Test Loss of 84480415760.111053, Test MSE of 84480414910.852554
Epoch 23: training loss 94316021805.176
Test Loss of 83078582986.572876, Test MSE of 83078581458.211151
Epoch 24: training loss 90618142072.471
Test Loss of 79475217849.158722, Test MSE of 79475217535.359100
Epoch 25: training loss 88351536459.294
Test Loss of 73516621679.948166, Test MSE of 73516622599.644196
Epoch 26: training loss 85310423341.176
Test Loss of 72583647199.777878, Test MSE of 72583647121.770599
Epoch 27: training loss 82110293217.882
Test Loss of 69556367003.661270, Test MSE of 69556367278.383972
Epoch 28: training loss 78956781733.647
Test Loss of 65893680227.983337, Test MSE of 65893681285.279732
Epoch 29: training loss 76360692751.059
Test Loss of 68609644391.418785, Test MSE of 68609644550.999855
Epoch 30: training loss 73413770872.471
Test Loss of 65758985260.068489, Test MSE of 65758984342.058998
Epoch 31: training loss 70313482646.588
Test Loss of 60907460627.901894, Test MSE of 60907460688.951218
Epoch 32: training loss 67414783224.471
Test Loss of 58012326338.161964, Test MSE of 58012326455.664375
Epoch 33: training loss 64420098409.412
Test Loss of 55275106569.358627, Test MSE of 55275106129.859100
Epoch 34: training loss 62249302768.941
Test Loss of 54408557779.813049, Test MSE of 54408557117.247231
Epoch 35: training loss 60076796054.588
Test Loss of 50502252893.704765, Test MSE of 50502252935.466194
Epoch 36: training loss 56956865671.529
Test Loss of 46808941338.180473, Test MSE of 46808940253.493729
Epoch 37: training loss 54321902019.765
Test Loss of 45110874604.808884, Test MSE of 45110873661.579529
Epoch 38: training loss 51800660163.765
Test Loss of 43269800707.435448, Test MSE of 43269800309.646477
Epoch 39: training loss 49969342908.235
Test Loss of 43313922332.312820, Test MSE of 43313921697.815109
Epoch 40: training loss 47437158068.706
Test Loss of 42468978267.217026, Test MSE of 42468978824.849106
Epoch 41: training loss 45818681630.118
Test Loss of 38879064202.365570, Test MSE of 38879064386.727005
Epoch 42: training loss 43956672481.882
Test Loss of 36519378455.929665, Test MSE of 36519378756.545624
Epoch 43: training loss 41803191936.000
Test Loss of 32806653842.065712, Test MSE of 32806653911.763073
Epoch 44: training loss 40131748961.882
Test Loss of 37323356053.856544, Test MSE of 37323355801.367981
Epoch 45: training loss 38075686761.412
Test Loss of 35155266746.224899, Test MSE of 35155265779.797531
Epoch 46: training loss 36536008048.941
Test Loss of 30377236919.737160, Test MSE of 30377237316.269745
Epoch 47: training loss 34821447469.176
Test Loss of 30659599610.669136, Test MSE of 30659599576.495754
Epoch 48: training loss 33347097675.294
Test Loss of 31391180818.954189, Test MSE of 31391179788.685539
Epoch 49: training loss 31878396400.941
Test Loss of 28845340237.949097, Test MSE of 28845339911.168064
Epoch 50: training loss 30672893545.412
Test Loss of 27571262276.827396, Test MSE of 27571262502.971100
Epoch 51: training loss 29373117364.706
Test Loss of 29066504682.913467, Test MSE of 29066504635.202766
Epoch 52: training loss 28385217950.118
Test Loss of 24557876604.505322, Test MSE of 24557877185.069778
Epoch 53: training loss 26815911811.765
Test Loss of 27186817955.598335, Test MSE of 27186818009.376831
Epoch 54: training loss 25963299301.647
Test Loss of 24610933145.410458, Test MSE of 24610933374.938766
Epoch 55: training loss 25081143661.176
Test Loss of 25835711719.714947, Test MSE of 25835711710.127060
Epoch 56: training loss 23885047668.706
Test Loss of 24149244336.155483, Test MSE of 24149243996.863308
Epoch 57: training loss 23092815284.706
Test Loss of 22053402157.726978, Test MSE of 22053402579.837959
Epoch 58: training loss 22287923339.294
Test Loss of 23525626133.678852, Test MSE of 23525626913.977196
Epoch 59: training loss 21708117289.412
Test Loss of 23286951447.929661, Test MSE of 23286951445.458351
Epoch 60: training loss 20779875730.824
Test Loss of 22411524892.075890, Test MSE of 22411525230.275928
Epoch 61: training loss 20349851896.471
Test Loss of 24014420913.813976, Test MSE of 24014420999.052738
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22453591256.617256, 'MSE - std': 3505408173.3904676, 'R2 - mean': 0.8340719975973194, 'R2 - std': 0.017827484494239633} 
 

Saving model.....
Results After CV: {'MSE - mean': 22453591256.617256, 'MSE - std': 3505408173.3904676, 'R2 - mean': 0.8340719975973194, 'R2 - std': 0.017827484494239633}
Train time: 99.80973124660014
Inference time: 0.06971161259934888
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 56 finished with value: 22453591256.617256 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005439 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524560655.059
Test Loss of 418112553448.904907, Test MSE of 418112557549.928589
Epoch 2: training loss 427504205101.176
Test Loss of 418094259710.460327, Test MSE of 418094267215.917297
Epoch 3: training loss 427477703981.176
Test Loss of 418071065931.858459, Test MSE of 418071062394.414734
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494192911.059
Test Loss of 418077811334.188293, Test MSE of 418077815523.285339
Epoch 2: training loss 427483817261.176
Test Loss of 418078138680.671753, Test MSE of 418078136385.858337
Epoch 3: training loss 427483329475.765
Test Loss of 418077554170.196594, Test MSE of 418077559449.840576
Epoch 4: training loss 427482961076.706
Test Loss of 418076376796.173035, Test MSE of 418076375691.734558
Epoch 5: training loss 427482685680.941
Test Loss of 418075923357.697876, Test MSE of 418075925649.280701
Epoch 6: training loss 427482494494.118
Test Loss of 418074734045.298157, Test MSE of 418074729466.623474
Epoch 7: training loss 427482359326.118
Test Loss of 418073860588.221130, Test MSE of 418073854173.596924
Epoch 8: training loss 427482249456.941
Test Loss of 418073390670.997009, Test MSE of 418073394327.056763
Epoch 9: training loss 416241404144.941
Test Loss of 382287285746.142944, Test MSE of 382287286138.474426
Epoch 10: training loss 344191480048.941
Test Loss of 282704692158.860046, Test MSE of 282704693069.478699
Epoch 11: training loss 242677080847.059
Test Loss of 187360003325.216736, Test MSE of 187360000958.433960
Epoch 12: training loss 170855203599.059
Test Loss of 137155632206.404816, Test MSE of 137155631902.694397
Epoch 13: training loss 144979765082.353
Test Loss of 122007392798.911865, Test MSE of 122007393414.766312
Epoch 14: training loss 136890047457.882
Test Loss of 117675352516.900299, Test MSE of 117675353681.843689
Epoch 15: training loss 134832722160.941
Test Loss of 114404714560.903076, Test MSE of 114404713865.458771
Epoch 16: training loss 132034640685.176
Test Loss of 111880000807.853806, Test MSE of 111880001292.734344
Epoch 17: training loss 128743025392.941
Test Loss of 107520447361.273193, Test MSE of 107520450543.814560
Epoch 18: training loss 122525347840.000
Test Loss of 104492119184.255371, Test MSE of 104492120121.344849
Epoch 19: training loss 119820286644.706
Test Loss of 101144077631.541061, Test MSE of 101144074186.233627
Epoch 20: training loss 114955762334.118
Test Loss of 97480278878.216049, Test MSE of 97480279565.295807
Epoch 21: training loss 111404541665.882
Test Loss of 93811757557.222305, Test MSE of 93811758692.136765
Epoch 22: training loss 107343581455.059
Test Loss of 90702803301.203796, Test MSE of 90702803550.912125
Epoch 23: training loss 103741057927.529
Test Loss of 87108905767.735367, Test MSE of 87108905927.865005
Epoch 24: training loss 100733858966.588
Test Loss of 86020194557.927368, Test MSE of 86020195753.838791
Epoch 25: training loss 96796782802.824
Test Loss of 81493384308.304413, Test MSE of 81493383762.281403
Epoch 26: training loss 92307367002.353
Test Loss of 77180870419.127457, Test MSE of 77180870876.816956
Epoch 27: training loss 88085151081.412
Test Loss of 74804095808.370117, Test MSE of 74804094981.579285
Epoch 28: training loss 84763098292.706
Test Loss of 73398676098.872086, Test MSE of 73398675913.492203
Epoch 29: training loss 81356970300.235
Test Loss of 69389972243.364334, Test MSE of 69389973461.092834
Epoch 30: training loss 77744718637.176
Test Loss of 65233274509.057602, Test MSE of 65233273818.462784
Epoch 31: training loss 73678701970.824
Test Loss of 63260910735.307892, Test MSE of 63260910343.968338
Epoch 32: training loss 70950879683.765
Test Loss of 60017258184.275734, Test MSE of 60017257879.284050
Epoch 33: training loss 67402185246.118
Test Loss of 55093026259.823273, Test MSE of 55093026741.877808
Epoch 34: training loss 63260238275.765
Test Loss of 50552307535.766830, Test MSE of 50552307542.950905
Epoch 35: training loss 61406256982.588
Test Loss of 50821670258.705528, Test MSE of 50821670300.082771
Epoch 36: training loss 57346354296.471
Test Loss of 47207842062.034698, Test MSE of 47207841777.046143
Epoch 37: training loss 54678610880.000
Test Loss of 46769294349.975479, Test MSE of 46769294424.480522
Epoch 38: training loss 52354155550.118
Test Loss of 41266834082.139252, Test MSE of 41266834774.157661
Epoch 39: training loss 49278269726.118
Test Loss of 41966341491.179276, Test MSE of 41966341351.028839
Epoch 40: training loss 47209014298.353
Test Loss of 39436799491.197777, Test MSE of 39436800775.883560
Epoch 41: training loss 45129256783.059
Test Loss of 35379851457.287994, Test MSE of 35379851402.930359
Epoch 42: training loss 42392577217.882
Test Loss of 36691911890.579689, Test MSE of 36691911892.688271
Epoch 43: training loss 40183552880.941
Test Loss of 31535306976.081425, Test MSE of 31535306292.892738
Epoch 44: training loss 37917493963.294
Test Loss of 31647352157.860744, Test MSE of 31647352266.788010
Epoch 45: training loss 35486251998.118
Test Loss of 31676673950.408512, Test MSE of 31676674235.477283
Epoch 46: training loss 34102846008.471
Test Loss of 25749212394.740688, Test MSE of 25749212158.797615
Epoch 47: training loss 32279134441.412
Test Loss of 28406183742.238262, Test MSE of 28406184024.922749
Epoch 48: training loss 31027836438.588
Test Loss of 29760094093.353691, Test MSE of 29760094343.867378
Epoch 49: training loss 29667324032.000
Test Loss of 25939081136.884571, Test MSE of 25939081438.643223
Epoch 50: training loss 28243844065.882
Test Loss of 25543000698.818413, Test MSE of 25543000673.745602
Epoch 51: training loss 27146642699.294
Test Loss of 23376294399.644691, Test MSE of 23376293811.250774
Epoch 52: training loss 26096381293.176
Test Loss of 22512496813.864445, Test MSE of 22512496327.498390
Epoch 53: training loss 24527830260.706
Test Loss of 21204977921.243580, Test MSE of 21204978098.410450
Epoch 54: training loss 23302580397.176
Test Loss of 21702977162.688873, Test MSE of 21702977437.670368
Epoch 55: training loss 22356915614.118
Test Loss of 22232431007.474438, Test MSE of 22232430799.882835
Epoch 56: training loss 21496085955.765
Test Loss of 19212677405.194542, Test MSE of 19212677443.324268
Epoch 57: training loss 20979901168.941
Test Loss of 20161278043.906548, Test MSE of 20161278356.833176
Epoch 58: training loss 20245930221.176
Test Loss of 19019720204.198936, Test MSE of 19019720543.280380
Epoch 59: training loss 19392231789.176
Test Loss of 19564601541.077953, Test MSE of 19564601721.730186
Epoch 60: training loss 18520534859.294
Test Loss of 19439826047.674301, Test MSE of 19439826180.084671
Epoch 61: training loss 18050253808.941
Test Loss of 19037064033.058525, Test MSE of 19037063813.568981
Epoch 62: training loss 17466368929.882
Test Loss of 20289615345.195465, Test MSE of 20289615855.175705
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20289615855.175705, 'MSE - std': 0.0, 'R2 - mean': 0.8420025599291718, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005341 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917608839.529
Test Loss of 424556170594.124451, Test MSE of 424556173900.539124
Epoch 2: training loss 427897091734.588
Test Loss of 424540520322.931274, Test MSE of 424540511250.714661
Epoch 3: training loss 427869475056.941
Test Loss of 424518743527.246826, Test MSE of 424518745946.273132
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887518418.824
Test Loss of 424524868876.850342, Test MSE of 424524872773.385376
Epoch 2: training loss 427878038588.235
Test Loss of 424525721913.619263, Test MSE of 424525720301.491760
Epoch 3: training loss 427877539840.000
Test Loss of 424525925889.302795, Test MSE of 424525926643.209473
Epoch 4: training loss 427877146383.059
Test Loss of 424525783182.123535, Test MSE of 424525779992.600159
Epoch 5: training loss 427876890021.647
Test Loss of 424525724938.007874, Test MSE of 424525729539.469482
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 222407672697.3226, 'MSE - std': 202118056842.1469, 'R2 - mean': -0.5944146471529459, 'R2 - std': 1.4364172070821177} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005392 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927222573.176
Test Loss of 447259275797.436951, Test MSE of 447259269969.064575
Epoch 2: training loss 421905881449.412
Test Loss of 447240672078.108704, Test MSE of 447240683084.980103
Epoch 3: training loss 421878211764.706
Test Loss of 447215781602.331726, Test MSE of 447215783287.515137
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899669744.941
Test Loss of 447226440715.132996, Test MSE of 447226440467.142944
Epoch 2: training loss 421888431043.765
Test Loss of 447227081567.163574, Test MSE of 447227093671.315979
Epoch 3: training loss 421887886396.235
Test Loss of 447226947053.879272, Test MSE of 447226949124.598267
Epoch 4: training loss 421887494384.941
Test Loss of 447226192988.143433, Test MSE of 447226190196.399170
Epoch 5: training loss 421887295488.000
Test Loss of 447225628465.920898, Test MSE of 447225628044.152771
Epoch 6: training loss 421887150441.412
Test Loss of 447225298223.907471, Test MSE of 447225302700.468445
Epoch 7: training loss 421887043704.471
Test Loss of 447225042780.557922, Test MSE of 447225050485.916443
Epoch 8: training loss 421886946243.765
Test Loss of 447224202455.317139, Test MSE of 447224197521.501831
Epoch 9: training loss 410388865385.412
Test Loss of 410213813564.224854, Test MSE of 410213813907.413269
Epoch 10: training loss 337777231992.471
Test Loss of 308989128456.705078, Test MSE of 308989129582.832764
Epoch 11: training loss 236001824948.706
Test Loss of 211945866121.800598, Test MSE of 211945864101.085175
Epoch 12: training loss 165579224907.294
Test Loss of 160752747545.819122, Test MSE of 160752750505.990112
Epoch 13: training loss 139781199088.941
Test Loss of 144256950008.597748, Test MSE of 144256951309.415039
Epoch 14: training loss 133990535830.588
Test Loss of 139263477269.673828, Test MSE of 139263476754.138794
Epoch 15: training loss 131303884559.059
Test Loss of 136802288010.155914, Test MSE of 136802285499.169495
Epoch 16: training loss 127406139452.235
Test Loss of 133629956180.326630, Test MSE of 133629957192.280426
Epoch 17: training loss 123210239156.706
Test Loss of 129033387785.652557, Test MSE of 129033388050.778656
Epoch 18: training loss 119934847578.353
Test Loss of 125044375154.054123, Test MSE of 125044373449.890549
Epoch 19: training loss 115089424805.647
Test Loss of 120937830930.120743, Test MSE of 120937832349.724915
Epoch 20: training loss 111298940355.765
Test Loss of 116852150272.947495, Test MSE of 116852152343.986588
Epoch 21: training loss 107531684201.412
Test Loss of 113327768908.805923, Test MSE of 113327768167.132858
Epoch 22: training loss 103575133756.235
Test Loss of 110601263865.308350, Test MSE of 110601264793.171753
Epoch 23: training loss 100445274172.235
Test Loss of 106538785669.536896, Test MSE of 106538782912.122833
Epoch 24: training loss 97096875776.000
Test Loss of 102874176783.692810, Test MSE of 102874180162.643448
Epoch 25: training loss 93113003369.412
Test Loss of 98789068260.878098, Test MSE of 98789067892.818939
Epoch 26: training loss 88631725869.176
Test Loss of 94653868379.255142, Test MSE of 94653868251.871658
Epoch 27: training loss 84438523286.588
Test Loss of 91739586927.626190, Test MSE of 91739587973.611267
Epoch 28: training loss 81148388201.412
Test Loss of 84018076253.919968, Test MSE of 84018077496.422302
Epoch 29: training loss 77258082936.471
Test Loss of 85905481815.169098, Test MSE of 85905482533.628265
Epoch 30: training loss 73137017133.176
Test Loss of 78005732696.175797, Test MSE of 78005733636.121002
Epoch 31: training loss 69562903461.647
Test Loss of 74071806517.651627, Test MSE of 74071807131.591064
Epoch 32: training loss 67941380999.529
Test Loss of 72644642590.971085, Test MSE of 72644642245.483185
Epoch 33: training loss 63901256643.765
Test Loss of 67320103806.430717, Test MSE of 67320104703.392159
Epoch 34: training loss 60436527013.647
Test Loss of 63702174689.443443, Test MSE of 63702177307.245667
Epoch 35: training loss 57586086264.471
Test Loss of 59545896841.089981, Test MSE of 59545897301.166893
Epoch 36: training loss 54571803045.647
Test Loss of 55098584420.019432, Test MSE of 55098584283.170746
Epoch 37: training loss 51853697475.765
Test Loss of 54984424488.505203, Test MSE of 54984424621.972939
Epoch 38: training loss 49002282059.294
Test Loss of 53852222439.602127, Test MSE of 53852222306.312035
Epoch 39: training loss 46413951555.765
Test Loss of 52732868372.548691, Test MSE of 52732867584.771538
Epoch 40: training loss 43827855344.941
Test Loss of 45005673580.250755, Test MSE of 45005673969.969864
Epoch 41: training loss 41973501974.588
Test Loss of 46598405312.103630, Test MSE of 46598405903.551521
Epoch 42: training loss 39395256417.882
Test Loss of 40694771721.474899, Test MSE of 40694772070.513283
Epoch 43: training loss 37809211000.471
Test Loss of 41689926246.447372, Test MSE of 41689926557.816025
Epoch 44: training loss 35208121359.059
Test Loss of 37975112071.787186, Test MSE of 37975111844.526123
Epoch 45: training loss 33675940841.412
Test Loss of 39561740864.074020, Test MSE of 39561741097.027977
Epoch 46: training loss 31519469040.941
Test Loss of 35866626345.038170, Test MSE of 35866626190.530914
Epoch 47: training loss 30220760350.118
Test Loss of 33445839560.512608, Test MSE of 33445839942.636059
Epoch 48: training loss 28711325552.941
Test Loss of 34485185628.617165, Test MSE of 34485184828.126450
Epoch 49: training loss 27468603753.412
Test Loss of 32301060682.496414, Test MSE of 32301060921.375729
Epoch 50: training loss 25962416316.235
Test Loss of 31359697942.976635, Test MSE of 31359697808.403084
Epoch 51: training loss 24786566923.294
Test Loss of 29480735339.658569, Test MSE of 29480735450.812008
Epoch 52: training loss 23627216237.176
Test Loss of 32063096337.883877, Test MSE of 32063097354.764145
Epoch 53: training loss 22205693632.000
Test Loss of 26441266825.741383, Test MSE of 26441267006.502041
Epoch 54: training loss 21592099136.000
Test Loss of 26315469589.733055, Test MSE of 26315469072.527058
Epoch 55: training loss 20917486776.471
Test Loss of 30769938181.862595, Test MSE of 30769938557.789932
Epoch 56: training loss 20074496146.824
Test Loss of 27864456225.872772, Test MSE of 27864455966.524055
Epoch 57: training loss 19629226695.529
Test Loss of 26088481847.191303, Test MSE of 26088481254.481655
Epoch 58: training loss 18625903495.529
Test Loss of 26223325937.254684, Test MSE of 26223326313.325748
Epoch 59: training loss 17803653601.882
Test Loss of 26629932596.467270, Test MSE of 26629932763.776314
Epoch 60: training loss 17295317662.118
Test Loss of 25046292414.149433, Test MSE of 25046292647.583145
Epoch 61: training loss 16809219685.647
Test Loss of 22698139444.763359, Test MSE of 22698139460.264904
Epoch 62: training loss 16181720783.059
Test Loss of 25266388434.875782, Test MSE of 25266388677.426868
Epoch 63: training loss 15453073682.824
Test Loss of 24435699999.563267, Test MSE of 24435699930.383591
Epoch 64: training loss 15220320387.765
Test Loss of 24278317058.368725, Test MSE of 24278317250.861908
Epoch 65: training loss 14991849626.353
Test Loss of 21965353892.804070, Test MSE of 21965353664.895527
Epoch 66: training loss 14529466288.941
Test Loss of 23315594562.857277, Test MSE of 23315594811.900646
Epoch 67: training loss 14272817125.647
Test Loss of 24765433787.780708, Test MSE of 24765433455.528084
Epoch 68: training loss 13549409893.647
Test Loss of 23055986279.631737, Test MSE of 23055986614.696442
Epoch 69: training loss 13584255145.412
Test Loss of 24055303240.719872, Test MSE of 24055303132.889538
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 156290216175.8449, 'MSE - std': 189677380609.38068, 'R2 - mean': -0.11632128658391594, 'R2 - std': 1.353763686188043} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005353 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110045123.765
Test Loss of 410763398423.100403, Test MSE of 410763398386.662964
Epoch 2: training loss 430089891117.176
Test Loss of 410746060026.669128, Test MSE of 410746056540.396179
Epoch 3: training loss 430063600097.882
Test Loss of 410723221670.796875, Test MSE of 410723223179.831421
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076829936.941
Test Loss of 410727656511.022705, Test MSE of 410727662256.682495
Epoch 2: training loss 430066417904.941
Test Loss of 410728649983.407654, Test MSE of 410728651426.857849
Epoch 3: training loss 430065869763.765
Test Loss of 410728680567.885254, Test MSE of 410728674005.480896
Epoch 4: training loss 430065544975.059
Test Loss of 410728154551.263306, Test MSE of 410728150497.343933
Epoch 5: training loss 430065353185.882
Test Loss of 410728047452.520142, Test MSE of 410728050696.540161
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 219899674806.01874, 'MSE - std': 197791863376.1214, 'R2 - mean': -0.6847291293838373, 'R2 - std': 1.5309374630599577} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 8, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005228 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043646735.059
Test Loss of 431613402032.866272, Test MSE of 431613403670.946655
Epoch 2: training loss 424024310121.412
Test Loss of 431593733165.963928, Test MSE of 431593738608.231506
Epoch 3: training loss 423996939324.235
Test Loss of 431565973802.528442, Test MSE of 431565975068.180176
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012482198.588
Test Loss of 431568798431.422485, Test MSE of 431568796203.895386
Epoch 2: training loss 423999927235.765
Test Loss of 431571582907.764954, Test MSE of 431571581185.982788
Epoch 3: training loss 423999265852.235
Test Loss of 431571461980.046265, Test MSE of 431571466479.155701
Epoch 4: training loss 423998847216.941
Test Loss of 431571364798.134216, Test MSE of 431571366335.596497
Epoch 5: training loss 423998615552.000
Test Loss of 431571477022.089783, Test MSE of 431571483987.175781
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 262234036642.25015, 'MSE - std': 196127738330.30984, 'R2 - mean': -0.9923816766017414, 'R2 - std': 1.5012048400798026} 
 

Saving model.....
Results After CV: {'MSE - mean': 262234036642.25015, 'MSE - std': 196127738330.30984, 'R2 - mean': -0.9923816766017414, 'R2 - std': 1.5012048400798026}
Train time: 46.937127947000405
Inference time: 0.07000085759864305
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 57 finished with value: 262234036642.25015 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 8, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525097833.412
Test Loss of 418114250760.053650, Test MSE of 418114244909.072571
Epoch 2: training loss 427504453511.529
Test Loss of 418095308210.661133, Test MSE of 418095309644.580078
Epoch 3: training loss 427476661187.765
Test Loss of 418069870017.347229, Test MSE of 418069868692.111267
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493793671.529
Test Loss of 418076611690.355774, Test MSE of 418076613862.201904
Epoch 2: training loss 427482744711.529
Test Loss of 418077568875.244019, Test MSE of 418077574730.046509
Epoch 3: training loss 427482190305.882
Test Loss of 418076545199.285706, Test MSE of 418076547925.509766
Epoch 4: training loss 421991460743.529
Test Loss of 400861593653.059448, Test MSE of 400861595071.675781
Epoch 5: training loss 385377301925.647
Test Loss of 346131940446.038391, Test MSE of 346131941028.427307
Epoch 6: training loss 317299537076.706
Test Loss of 268504100639.444824, Test MSE of 268504101717.913147
Epoch 7: training loss 226470917180.235
Test Loss of 171033729554.120758, Test MSE of 171033727929.749237
Epoch 8: training loss 162112378398.118
Test Loss of 130922384544.836456, Test MSE of 130922383581.125259
Epoch 9: training loss 141935128756.706
Test Loss of 120713693065.089981, Test MSE of 120713693222.779877
Epoch 10: training loss 136123296677.647
Test Loss of 116159438533.433258, Test MSE of 116159437789.998901
Epoch 11: training loss 132456276419.765
Test Loss of 113057822838.910019, Test MSE of 113057827040.112167
Epoch 12: training loss 129543633528.471
Test Loss of 109861511584.895676, Test MSE of 109861513117.542267
Epoch 13: training loss 126454340773.647
Test Loss of 106572626300.417297, Test MSE of 106572626216.017548
Epoch 14: training loss 122485127860.706
Test Loss of 103461070242.080032, Test MSE of 103461071356.115692
Epoch 15: training loss 119758189206.588
Test Loss of 100871959139.131165, Test MSE of 100871961392.145477
Epoch 16: training loss 115369063905.882
Test Loss of 97455857470.712006, Test MSE of 97455859235.251343
Epoch 17: training loss 111217579926.588
Test Loss of 94580206978.339111, Test MSE of 94580206614.223251
Epoch 18: training loss 107054507248.941
Test Loss of 90761647020.857742, Test MSE of 90761647041.081284
Epoch 19: training loss 103841457935.059
Test Loss of 87172093220.063843, Test MSE of 87172092548.108017
Epoch 20: training loss 98891092886.588
Test Loss of 84287630123.525330, Test MSE of 84287631402.128708
Epoch 21: training loss 96159939418.353
Test Loss of 81430150023.431870, Test MSE of 81430151155.387634
Epoch 22: training loss 92298159585.882
Test Loss of 78018419818.829514, Test MSE of 78018419133.831223
Epoch 23: training loss 88541591311.059
Test Loss of 74127795932.646774, Test MSE of 74127797315.767349
Epoch 24: training loss 85068149519.059
Test Loss of 72519533161.763596, Test MSE of 72519533075.246185
Epoch 25: training loss 82029648459.294
Test Loss of 71145370828.894745, Test MSE of 71145369899.958176
Epoch 26: training loss 78446138352.941
Test Loss of 67236025361.291695, Test MSE of 67236025386.088585
Epoch 27: training loss 74233489374.118
Test Loss of 64074356197.351837, Test MSE of 64074357015.409813
Epoch 28: training loss 70915415047.529
Test Loss of 59826993064.357162, Test MSE of 59826991311.641014
Epoch 29: training loss 68324910945.882
Test Loss of 55058521808.803146, Test MSE of 55058519975.359009
Epoch 30: training loss 64346982569.412
Test Loss of 56506421084.084198, Test MSE of 56506420688.085129
Epoch 31: training loss 61948705837.176
Test Loss of 51287255321.878326, Test MSE of 51287254828.614853
Epoch 32: training loss 58464216406.588
Test Loss of 50427247735.146889, Test MSE of 50427248299.074387
Epoch 33: training loss 56085844954.353
Test Loss of 48113933107.105247, Test MSE of 48113933520.353424
Epoch 34: training loss 53221687296.000
Test Loss of 46395469758.623177, Test MSE of 46395469751.698502
Epoch 35: training loss 50827634145.882
Test Loss of 43844811785.238029, Test MSE of 43844811637.724113
Epoch 36: training loss 48473107561.412
Test Loss of 40283046441.334259, Test MSE of 40283046925.562149
Epoch 37: training loss 46428532901.647
Test Loss of 37046575349.399956, Test MSE of 37046575136.440666
Epoch 38: training loss 43929881088.000
Test Loss of 37710977619.497574, Test MSE of 37710977609.740654
Epoch 39: training loss 42046409359.059
Test Loss of 33846764552.764282, Test MSE of 33846764509.252979
Epoch 40: training loss 39978346624.000
Test Loss of 31598226399.548462, Test MSE of 31598226213.868958
Epoch 41: training loss 37713187408.941
Test Loss of 30905834997.932919, Test MSE of 30905835154.786419
Epoch 42: training loss 36301875614.118
Test Loss of 29563772296.024055, Test MSE of 29563771674.496979
Epoch 43: training loss 34500906051.765
Test Loss of 26752344130.324310, Test MSE of 26752343967.049526
Epoch 44: training loss 32971549338.353
Test Loss of 30368956603.603054, Test MSE of 30368956697.854950
Epoch 45: training loss 31341624865.882
Test Loss of 27200619636.778164, Test MSE of 27200620017.574554
Epoch 46: training loss 29878365421.176
Test Loss of 27164787700.866989, Test MSE of 27164787777.470734
Epoch 47: training loss 28559540803.765
Test Loss of 22896548136.090679, Test MSE of 22896548350.649471
Epoch 48: training loss 26975487909.647
Test Loss of 25429589611.421700, Test MSE of 25429589135.652115
Epoch 49: training loss 26224564065.882
Test Loss of 24612624881.906082, Test MSE of 24612625333.730576
Epoch 50: training loss 25345578206.118
Test Loss of 24509482191.263474, Test MSE of 24509482766.032944
Epoch 51: training loss 24510607352.471
Test Loss of 23902017016.301643, Test MSE of 23902016839.260323
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23902016839.260323, 'MSE - std': 0.0, 'R2 - mean': 0.8138724015235794, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005289 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918040124.235
Test Loss of 424557138557.660889, Test MSE of 424557134442.968811
Epoch 2: training loss 427898047548.235
Test Loss of 424541021840.847534, Test MSE of 424541018842.726074
Epoch 3: training loss 427871219712.000
Test Loss of 424519421589.111267, Test MSE of 424519414696.437805
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889832658.824
Test Loss of 424522699412.400635, Test MSE of 424522703933.519226
Epoch 2: training loss 427878709127.529
Test Loss of 424524585986.605591, Test MSE of 424524578482.220459
Epoch 3: training loss 427878108943.059
Test Loss of 424525365801.334229, Test MSE of 424525365448.127441
Epoch 4: training loss 422570618639.059
Test Loss of 407786898806.495483, Test MSE of 407786898977.815735
Epoch 5: training loss 385987082721.882
Test Loss of 354130862559.430054, Test MSE of 354130860943.015747
Epoch 6: training loss 317504291418.353
Test Loss of 277626072564.037964, Test MSE of 277626076458.000977
Epoch 7: training loss 225791611000.471
Test Loss of 180956130186.274353, Test MSE of 180956132254.625549
Epoch 8: training loss 160824716047.059
Test Loss of 141581870067.919495, Test MSE of 141581867611.800476
Epoch 9: training loss 140817162059.294
Test Loss of 132196989348.211884, Test MSE of 132196989371.339081
Epoch 10: training loss 135679327322.353
Test Loss of 127905908736.473740, Test MSE of 127905909968.014069
Epoch 11: training loss 131577426070.588
Test Loss of 125122435416.412674, Test MSE of 125122436524.070236
Epoch 12: training loss 127380484939.294
Test Loss of 122755752161.502655, Test MSE of 122755751563.884750
Epoch 13: training loss 125124801295.059
Test Loss of 119109796569.567429, Test MSE of 119109796726.252182
Epoch 14: training loss 120550314074.353
Test Loss of 116163408426.281754, Test MSE of 116163408892.735992
Epoch 15: training loss 117324664500.706
Test Loss of 111401260115.616013, Test MSE of 111401259680.071487
Epoch 16: training loss 113470712440.471
Test Loss of 106730070592.074020, Test MSE of 106730070674.601059
Epoch 17: training loss 109410826119.529
Test Loss of 104180668267.717789, Test MSE of 104180666083.786224
Epoch 18: training loss 106728202541.176
Test Loss of 101375296691.312515, Test MSE of 101375300290.884232
Epoch 19: training loss 101117504692.706
Test Loss of 98704031590.506592, Test MSE of 98704032759.823593
Epoch 20: training loss 96250339448.471
Test Loss of 91047882165.503586, Test MSE of 91047882167.847992
Epoch 21: training loss 94005588088.471
Test Loss of 88487904177.595184, Test MSE of 88487903640.748291
Epoch 22: training loss 90402147840.000
Test Loss of 85645006345.356461, Test MSE of 85645007232.621582
Epoch 23: training loss 86172954759.529
Test Loss of 84119177426.816559, Test MSE of 84119176923.660507
Epoch 24: training loss 81879319145.412
Test Loss of 77366548043.680771, Test MSE of 77366546546.307510
Epoch 25: training loss 79779874876.235
Test Loss of 76063740621.250061, Test MSE of 76063739044.289581
Epoch 26: training loss 75404712192.000
Test Loss of 69019194913.991211, Test MSE of 69019192753.923721
Epoch 27: training loss 71752960376.471
Test Loss of 68138716929.125145, Test MSE of 68138717112.391930
Epoch 28: training loss 68838732754.824
Test Loss of 65455395112.564423, Test MSE of 65455394885.808609
Epoch 29: training loss 65657517643.294
Test Loss of 61482278628.700439, Test MSE of 61482279071.577423
Epoch 30: training loss 63013848244.706
Test Loss of 58544032744.075874, Test MSE of 58544032910.643112
Epoch 31: training loss 59412885775.059
Test Loss of 55638531679.104324, Test MSE of 55638530536.854179
Epoch 32: training loss 57385453176.471
Test Loss of 51591732436.948418, Test MSE of 51591731071.778900
Epoch 33: training loss 53794163832.471
Test Loss of 48408884382.467728, Test MSE of 48408884968.164062
Epoch 34: training loss 51169103088.941
Test Loss of 46106020017.180664, Test MSE of 46106019720.939499
Epoch 35: training loss 47736840297.412
Test Loss of 45571495539.949112, Test MSE of 45571495636.554214
Epoch 36: training loss 46100286735.059
Test Loss of 47240301209.611847, Test MSE of 47240301281.571510
Epoch 37: training loss 43532146447.059
Test Loss of 41704357304.819801, Test MSE of 41704357415.475388
Epoch 38: training loss 41470490729.412
Test Loss of 42886349266.165161, Test MSE of 42886349905.424080
Epoch 39: training loss 38678787192.471
Test Loss of 36249869271.021049, Test MSE of 36249869462.450966
Epoch 40: training loss 36747441927.529
Test Loss of 36244076227.064537, Test MSE of 36244075787.555054
Epoch 41: training loss 34825342200.471
Test Loss of 34983003582.030998, Test MSE of 34983004169.657921
Epoch 42: training loss 33805681679.059
Test Loss of 35053776138.007866, Test MSE of 35053776740.845963
Epoch 43: training loss 31376096828.235
Test Loss of 32509671893.955124, Test MSE of 32509672101.769100
Epoch 44: training loss 29935155787.294
Test Loss of 32914511669.710850, Test MSE of 32914510770.174706
Epoch 45: training loss 28710717884.235
Test Loss of 31207127365.936619, Test MSE of 31207127567.983055
Epoch 46: training loss 26564357680.941
Test Loss of 29606964808.364563, Test MSE of 29606965288.033691
Epoch 47: training loss 26124780544.000
Test Loss of 29923741118.030998, Test MSE of 29923741673.900059
Epoch 48: training loss 25048320783.059
Test Loss of 30507262384.529263, Test MSE of 30507263213.370857
Epoch 49: training loss 23588648041.412
Test Loss of 29094072886.125378, Test MSE of 29094072705.301765
Epoch 50: training loss 22306332837.647
Test Loss of 29669681031.668747, Test MSE of 29669680634.123329
Epoch 51: training loss 21394869918.118
Test Loss of 28092184096.569973, Test MSE of 28092183821.831818
Epoch 52: training loss 20874710433.882
Test Loss of 27355486758.491787, Test MSE of 27355487078.942833
Epoch 53: training loss 19686523162.353
Test Loss of 27900226234.300255, Test MSE of 27900225724.974400
Epoch 54: training loss 19223006008.471
Test Loss of 25804812768.851261, Test MSE of 25804813243.642067
Epoch 55: training loss 18639631947.294
Test Loss of 26519570665.556328, Test MSE of 26519570367.152748
Epoch 56: training loss 17764410368.000
Test Loss of 27563470226.209576, Test MSE of 27563470724.556747
Epoch 57: training loss 16863539794.824
Test Loss of 27415506588.217442, Test MSE of 27415506940.434467
Epoch 58: training loss 16557079676.235
Test Loss of 22682185394.246590, Test MSE of 22682184985.796791
Epoch 59: training loss 16108506959.059
Test Loss of 25980310030.093918, Test MSE of 25980309655.964848
Epoch 60: training loss 15351096854.588
Test Loss of 24443120721.484154, Test MSE of 24443120994.731876
Epoch 61: training loss 15055284024.471
Test Loss of 25103340706.968307, Test MSE of 25103340130.547276
Epoch 62: training loss 14347844047.059
Test Loss of 25738168022.014343, Test MSE of 25738168086.431862
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24820092462.846092, 'MSE - std': 918075623.5857697, 'R2 - mean': 0.8150594848432539, 'R2 - std': 0.0011870833196744313} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005375 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421928047555.765
Test Loss of 447259768886.480713, Test MSE of 447259765177.063721
Epoch 2: training loss 421908315678.118
Test Loss of 447242191882.659241, Test MSE of 447242190770.792297
Epoch 3: training loss 421881391465.412
Test Loss of 447218088020.800354, Test MSE of 447218093701.653931
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899470607.059
Test Loss of 447225395641.056702, Test MSE of 447225396705.422852
Epoch 2: training loss 421890461093.647
Test Loss of 447226804581.677551, Test MSE of 447226807046.808899
Epoch 3: training loss 421890020653.176
Test Loss of 447226835810.953491, Test MSE of 447226837185.279480
Epoch 4: training loss 416683192801.882
Test Loss of 430551922856.179504, Test MSE of 430551919264.420593
Epoch 5: training loss 380705165673.412
Test Loss of 374921932943.071045, Test MSE of 374921934470.326599
Epoch 6: training loss 312902483124.706
Test Loss of 296681934170.544556, Test MSE of 296681932191.023193
Epoch 7: training loss 220917018925.176
Test Loss of 196254757105.609985, Test MSE of 196254757138.132935
Epoch 8: training loss 156468506142.118
Test Loss of 154315494468.456177, Test MSE of 154315495584.822021
Epoch 9: training loss 137384348160.000
Test Loss of 143524958663.742767, Test MSE of 143524956713.610413
Epoch 10: training loss 133418974539.294
Test Loss of 138576264124.491333, Test MSE of 138576265558.633148
Epoch 11: training loss 129167643376.941
Test Loss of 134931480310.702759, Test MSE of 134931481480.259689
Epoch 12: training loss 125311220404.706
Test Loss of 132525942034.061539, Test MSE of 132525940910.717621
Epoch 13: training loss 123358569231.059
Test Loss of 129229757635.893585, Test MSE of 129229755019.832870
Epoch 14: training loss 120399591303.529
Test Loss of 125775918279.209808, Test MSE of 125775917177.044617
Epoch 15: training loss 115355085010.824
Test Loss of 122010898288.928986, Test MSE of 122010897323.141418
Epoch 16: training loss 112184714209.882
Test Loss of 118028520328.379364, Test MSE of 118028521265.445724
Epoch 17: training loss 109011640922.353
Test Loss of 114919394915.841782, Test MSE of 114919397725.876099
Epoch 18: training loss 104172460754.824
Test Loss of 110150790321.891281, Test MSE of 110150790343.529755
Epoch 19: training loss 100425017133.176
Test Loss of 109052196304.507050, Test MSE of 109052194140.971909
Epoch 20: training loss 97514093839.059
Test Loss of 104971833246.408508, Test MSE of 104971833882.744629
Epoch 21: training loss 93083839924.706
Test Loss of 101454885137.824661, Test MSE of 101454884641.394974
Epoch 22: training loss 88958753852.235
Test Loss of 97791104743.069168, Test MSE of 97791105555.686417
Epoch 23: training loss 86857814497.882
Test Loss of 92575932376.205414, Test MSE of 92575931574.695145
Epoch 24: training loss 83050436653.176
Test Loss of 90261909700.130463, Test MSE of 90261910218.978348
Epoch 25: training loss 78883060088.471
Test Loss of 86222037668.507980, Test MSE of 86222037269.852142
Epoch 26: training loss 75966663318.588
Test Loss of 81830147074.131851, Test MSE of 81830146996.984863
Epoch 27: training loss 73419363026.824
Test Loss of 76554728980.726349, Test MSE of 76554728496.514496
Epoch 28: training loss 68770682383.059
Test Loss of 76880253825.273193, Test MSE of 76880255720.358215
Epoch 29: training loss 66346612013.176
Test Loss of 69729212758.517700, Test MSE of 69729212639.733337
Epoch 30: training loss 63408479608.471
Test Loss of 65373206207.274574, Test MSE of 65373205244.116455
Epoch 31: training loss 59824030486.588
Test Loss of 67154187823.256073, Test MSE of 67154187369.912376
Epoch 32: training loss 57804566806.588
Test Loss of 63536569320.075874, Test MSE of 63536568680.007347
Epoch 33: training loss 55603765925.647
Test Loss of 57189655395.900993, Test MSE of 57189655963.089462
Epoch 34: training loss 52393575326.118
Test Loss of 55179380711.838997, Test MSE of 55179380471.161392
Epoch 35: training loss 49669595429.647
Test Loss of 53542217825.117744, Test MSE of 53542218189.535820
Epoch 36: training loss 46601228423.529
Test Loss of 48856411203.034927, Test MSE of 48856412448.988708
Epoch 37: training loss 45275390080.000
Test Loss of 47836085837.102013, Test MSE of 47836085491.621353
Epoch 38: training loss 42170330925.176
Test Loss of 47301963129.811707, Test MSE of 47301962791.944946
Epoch 39: training loss 40353807119.059
Test Loss of 41739470260.082352, Test MSE of 41739469993.332565
Epoch 40: training loss 38943856286.118
Test Loss of 41802698042.329865, Test MSE of 41802698612.661507
Epoch 41: training loss 36706940604.235
Test Loss of 40032792682.118896, Test MSE of 40032792497.137314
Epoch 42: training loss 35322722492.235
Test Loss of 38743260245.984734, Test MSE of 38743260700.133804
Epoch 43: training loss 33279444909.176
Test Loss of 35777159975.261627, Test MSE of 35777160101.876892
Epoch 44: training loss 31859809317.647
Test Loss of 34477529431.465187, Test MSE of 34477529519.358192
Epoch 45: training loss 30086371644.235
Test Loss of 33495328701.201942, Test MSE of 33495328603.899143
Epoch 46: training loss 29057489287.529
Test Loss of 32684214247.839001, Test MSE of 32684214780.291477
Epoch 47: training loss 28157804487.529
Test Loss of 29941111933.779320, Test MSE of 29941111522.266510
Epoch 48: training loss 26213329825.882
Test Loss of 28319342793.341660, Test MSE of 28319343498.308952
Epoch 49: training loss 25356946176.000
Test Loss of 28556278901.251907, Test MSE of 28556278908.301590
Epoch 50: training loss 24579165014.588
Test Loss of 30833107056.751331, Test MSE of 30833107347.027905
Epoch 51: training loss 23583083681.882
Test Loss of 29027895574.562111, Test MSE of 29027895805.428867
Epoch 52: training loss 22313806113.882
Test Loss of 27869477829.966228, Test MSE of 27869477934.358643
Epoch 53: training loss 21525021428.706
Test Loss of 27083178252.139717, Test MSE of 27083178202.630196
Epoch 54: training loss 20723015111.529
Test Loss of 27855475667.231091, Test MSE of 27855475543.227497
Epoch 55: training loss 19997267414.588
Test Loss of 24894277554.779552, Test MSE of 24894277571.585617
Epoch 56: training loss 19143758351.059
Test Loss of 27267799368.779087, Test MSE of 27267799650.868916
Epoch 57: training loss 18545143405.176
Test Loss of 24033844256.451538, Test MSE of 24033844761.046482
Epoch 58: training loss 18134864922.353
Test Loss of 24890270347.873238, Test MSE of 24890270568.083076
Epoch 59: training loss 17502432504.471
Test Loss of 23671982959.270878, Test MSE of 23671982631.827240
Epoch 60: training loss 16971445443.765
Test Loss of 24193689825.028915, Test MSE of 24193689569.698624
Epoch 61: training loss 16411856120.471
Test Loss of 24705726076.002777, Test MSE of 24705726242.684502
Epoch 62: training loss 16136838000.941
Test Loss of 23820229541.751560, Test MSE of 23820229201.313816
Epoch 63: training loss 15555496681.412
Test Loss of 26116126167.139488, Test MSE of 26116126352.270752
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25252103759.32098, 'MSE - std': 967045028.7579294, 'R2 - mean': 0.8187552133571837, 'R2 - std': 0.005315662050574077} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005379 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110500743.529
Test Loss of 410765305305.380859, Test MSE of 410765311951.640320
Epoch 2: training loss 430089977133.176
Test Loss of 410747218825.062500, Test MSE of 410747210634.060730
Epoch 3: training loss 430063208568.471
Test Loss of 410723228676.264709, Test MSE of 410723227256.024414
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077765752.471
Test Loss of 410728837660.194336, Test MSE of 410728837229.534363
Epoch 2: training loss 430066199130.353
Test Loss of 410728888032.844055, Test MSE of 410728894940.744385
Epoch 3: training loss 430065702791.529
Test Loss of 410728403897.395630, Test MSE of 410728405898.175720
Epoch 4: training loss 424650154706.824
Test Loss of 393907396793.751038, Test MSE of 393907398995.567078
Epoch 5: training loss 388023703552.000
Test Loss of 339267596115.516907, Test MSE of 339267599891.197632
Epoch 6: training loss 319636231710.118
Test Loss of 262191823781.493744, Test MSE of 262191822986.088745
Epoch 7: training loss 227942785144.471
Test Loss of 165140080612.516418, Test MSE of 165140078334.568787
Epoch 8: training loss 163952481340.235
Test Loss of 125423708693.560394, Test MSE of 125423707037.892242
Epoch 9: training loss 144963888218.353
Test Loss of 114617912864.459045, Test MSE of 114617913140.828491
Epoch 10: training loss 139185881298.824
Test Loss of 110311901838.867188, Test MSE of 110311900783.842194
Epoch 11: training loss 135316872734.118
Test Loss of 107492734613.027298, Test MSE of 107492733672.988235
Epoch 12: training loss 133126766561.882
Test Loss of 105311290242.428513, Test MSE of 105311291217.837967
Epoch 13: training loss 129277162827.294
Test Loss of 102408500718.230453, Test MSE of 102408500262.115021
Epoch 14: training loss 125893057385.412
Test Loss of 99293419388.268387, Test MSE of 99293419328.055069
Epoch 15: training loss 121686579862.588
Test Loss of 96271534279.966675, Test MSE of 96271535947.568832
Epoch 16: training loss 117117063619.765
Test Loss of 93091499283.309586, Test MSE of 93091498430.784485
Epoch 17: training loss 113652635166.118
Test Loss of 90357372489.210556, Test MSE of 90357371300.181595
Epoch 18: training loss 110946448896.000
Test Loss of 86531741007.015274, Test MSE of 86531741544.905426
Epoch 19: training loss 106097452995.765
Test Loss of 83446351620.857010, Test MSE of 83446352200.013245
Epoch 20: training loss 102584286750.118
Test Loss of 80249549882.284134, Test MSE of 80249549328.794205
Epoch 21: training loss 97727551277.176
Test Loss of 78450381340.194351, Test MSE of 78450383346.500580
Epoch 22: training loss 95283959717.647
Test Loss of 73994350032.851456, Test MSE of 73994350358.361343
Epoch 23: training loss 89178281441.882
Test Loss of 72210608421.789917, Test MSE of 72210607285.953827
Epoch 24: training loss 86531010951.529
Test Loss of 68931938344.751511, Test MSE of 68931939365.265823
Epoch 25: training loss 83411439314.824
Test Loss of 66443331230.504395, Test MSE of 66443332183.654282
Epoch 26: training loss 79350522322.824
Test Loss of 64966699837.245720, Test MSE of 64966699779.587746
Epoch 27: training loss 75866750870.588
Test Loss of 60327610450.450714, Test MSE of 60327609860.920586
Epoch 28: training loss 72916861485.176
Test Loss of 57138529609.329018, Test MSE of 57138528994.484169
Epoch 29: training loss 69764037541.647
Test Loss of 56034394305.332718, Test MSE of 56034395260.603409
Epoch 30: training loss 66905705607.529
Test Loss of 53505197080.166588, Test MSE of 53505196442.539742
Epoch 31: training loss 63042725556.706
Test Loss of 50166986536.869965, Test MSE of 50166986817.984535
Epoch 32: training loss 59274902701.176
Test Loss of 48131650798.348915, Test MSE of 48131651161.695419
Epoch 33: training loss 57232140943.059
Test Loss of 46568103020.512726, Test MSE of 46568101938.432655
Epoch 34: training loss 54312434176.000
Test Loss of 44704853352.603424, Test MSE of 44704852707.154587
Epoch 35: training loss 52007626232.471
Test Loss of 40447014524.386856, Test MSE of 40447014014.183731
Epoch 36: training loss 49216513272.471
Test Loss of 40868551344.984726, Test MSE of 40868552194.780441
Epoch 37: training loss 46750022896.941
Test Loss of 39185502198.522903, Test MSE of 39185502594.112709
Epoch 38: training loss 45031390976.000
Test Loss of 36878689129.788063, Test MSE of 36878689050.316841
Epoch 39: training loss 42717281310.118
Test Loss of 34310276442.387783, Test MSE of 34310276271.828682
Epoch 40: training loss 40062405089.882
Test Loss of 33207271151.059696, Test MSE of 33207270819.448952
Epoch 41: training loss 38923379990.588
Test Loss of 29589654917.034706, Test MSE of 29589654621.683178
Epoch 42: training loss 36483094949.647
Test Loss of 28473280575.496529, Test MSE of 28473280310.546951
Epoch 43: training loss 34881296353.882
Test Loss of 28706709216.844055, Test MSE of 28706709397.583443
Epoch 44: training loss 33626850883.765
Test Loss of 27693800367.918556, Test MSE of 27693800939.966141
Epoch 45: training loss 31761136286.118
Test Loss of 28869942276.264690, Test MSE of 28869942187.972668
Epoch 46: training loss 30266849600.000
Test Loss of 27380316050.065712, Test MSE of 27380315960.954693
Epoch 47: training loss 28926269688.471
Test Loss of 25015400230.974548, Test MSE of 25015400434.373825
Epoch 48: training loss 27733542735.059
Test Loss of 22730933794.354465, Test MSE of 22730933791.553520
Epoch 49: training loss 26578917861.647
Test Loss of 24752248640.562702, Test MSE of 24752248482.160362
Epoch 50: training loss 25616243527.529
Test Loss of 26433784779.402130, Test MSE of 26433785055.306408
Epoch 51: training loss 24565797989.647
Test Loss of 24053359353.958351, Test MSE of 24053359683.933002
Epoch 52: training loss 23749845552.941
Test Loss of 21521864133.478947, Test MSE of 21521863778.008587
Epoch 53: training loss 22341801074.824
Test Loss of 23304848833.688107, Test MSE of 23304849180.506130
Epoch 54: training loss 21805624952.471
Test Loss of 23076599022.348911, Test MSE of 23076599274.877079
Epoch 55: training loss 20811042304.000
Test Loss of 21494124230.308189, Test MSE of 21494124072.091801
Epoch 56: training loss 20081413778.824
Test Loss of 19863970615.559464, Test MSE of 19863970955.066689
Epoch 57: training loss 19826149605.647
Test Loss of 19615485611.772327, Test MSE of 19615485779.626011
Epoch 58: training loss 19000121901.176
Test Loss of 21680671094.345211, Test MSE of 21680670957.107677
Epoch 59: training loss 18545665600.000
Test Loss of 21163379539.990746, Test MSE of 21163379025.785923
Epoch 60: training loss 17746484837.647
Test Loss of 19239803730.095325, Test MSE of 19239803946.532692
Epoch 61: training loss 17474663439.059
Test Loss of 19402319678.667282, Test MSE of 19402319462.824196
Epoch 62: training loss 16940606219.294
Test Loss of 21320594481.754742, Test MSE of 21320594361.962433
Epoch 63: training loss 16398452382.118
Test Loss of 18880242755.761223, Test MSE of 18880242693.917580
Epoch 64: training loss 16073880018.824
Test Loss of 20001224343.870430, Test MSE of 20001224073.287373
Epoch 65: training loss 15850872459.294
Test Loss of 19883332307.102268, Test MSE of 19883332274.762245
Epoch 66: training loss 15059865859.765
Test Loss of 19890264811.742710, Test MSE of 19890264789.034870
Epoch 67: training loss 14817914356.706
Test Loss of 20652327044.205460, Test MSE of 20652327066.336346
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24102159586.07482, 'MSE - std': 2160670467.813739, 'R2 - mean': 0.821452805402124, 'R2 - std': 0.006559207711279585} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005374 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042884638.118
Test Loss of 431612019605.382690, Test MSE of 431612024042.205566
Epoch 2: training loss 424022713524.706
Test Loss of 431591925194.217468, Test MSE of 431591921815.498169
Epoch 3: training loss 423994870844.235
Test Loss of 431564460712.455322, Test MSE of 431564467254.039917
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011080161.882
Test Loss of 431567735980.956970, Test MSE of 431567738914.371460
Epoch 2: training loss 423998574471.529
Test Loss of 431569273453.697388, Test MSE of 431569271452.332581
Epoch 3: training loss 423997881163.294
Test Loss of 431569328708.945862, Test MSE of 431569328771.180908
Epoch 4: training loss 418798298413.176
Test Loss of 414754812474.521057, Test MSE of 414754820504.186584
Epoch 5: training loss 382953497298.824
Test Loss of 358681111440.644165, Test MSE of 358681103000.107483
Epoch 6: training loss 315259266891.294
Test Loss of 280794121185.199463, Test MSE of 280794124440.591370
Epoch 7: training loss 224399886938.353
Test Loss of 180438482496.681152, Test MSE of 180438485300.958649
Epoch 8: training loss 159635817140.706
Test Loss of 138953702082.043488, Test MSE of 138953704437.167175
Epoch 9: training loss 140685512944.941
Test Loss of 126656428556.083298, Test MSE of 126656431944.755966
Epoch 10: training loss 135777737938.824
Test Loss of 121669850158.911621, Test MSE of 121669849436.748413
Epoch 11: training loss 131150725601.882
Test Loss of 118658641485.949097, Test MSE of 118658644717.225967
Epoch 12: training loss 128973821108.706
Test Loss of 114617120772.738541, Test MSE of 114617119528.740738
Epoch 13: training loss 125544497061.647
Test Loss of 111513186029.164276, Test MSE of 111513185992.293213
Epoch 14: training loss 120627532107.294
Test Loss of 107078099929.617767, Test MSE of 107078100587.297638
Epoch 15: training loss 117415173315.765
Test Loss of 103793698716.490509, Test MSE of 103793697355.281754
Epoch 16: training loss 114260760425.412
Test Loss of 99373869146.032394, Test MSE of 99373868579.580093
Epoch 17: training loss 109401703905.882
Test Loss of 96821022710.522903, Test MSE of 96821024661.672226
Epoch 18: training loss 105124895382.588
Test Loss of 92640814534.426651, Test MSE of 92640813485.605423
Epoch 19: training loss 101956654908.235
Test Loss of 88430080278.626556, Test MSE of 88430077958.975800
Epoch 20: training loss 98375537272.471
Test Loss of 86849534134.907913, Test MSE of 86849534398.460037
Epoch 21: training loss 94155679292.235
Test Loss of 82574714264.936600, Test MSE of 82574713448.929306
Epoch 22: training loss 90666421744.941
Test Loss of 78351705341.512268, Test MSE of 78351703292.678360
Epoch 23: training loss 86804201020.235
Test Loss of 75513048856.758911, Test MSE of 75513047382.554977
Epoch 24: training loss 83405736975.059
Test Loss of 70707536757.160568, Test MSE of 70707536857.533752
Epoch 25: training loss 79031863296.000
Test Loss of 66947345213.245720, Test MSE of 66947345640.294205
Epoch 26: training loss 76362977445.647
Test Loss of 63219898188.882927, Test MSE of 63219898272.495552
Epoch 27: training loss 72384495450.353
Test Loss of 61690447006.741325, Test MSE of 61690447645.049423
Epoch 28: training loss 69957615375.059
Test Loss of 57525600847.370659, Test MSE of 57525600750.524132
Epoch 29: training loss 66480724118.588
Test Loss of 52157947777.006943, Test MSE of 52157949133.358688
Epoch 30: training loss 63858922345.412
Test Loss of 52280739230.622856, Test MSE of 52280739422.637131
Epoch 31: training loss 60218312892.235
Test Loss of 50607420777.551132, Test MSE of 50607421040.074242
Epoch 32: training loss 57750313253.647
Test Loss of 49171025929.477097, Test MSE of 49171025405.595894
Epoch 33: training loss 54662459663.059
Test Loss of 44364245696.148079, Test MSE of 44364245889.207787
Epoch 34: training loss 52091088474.353
Test Loss of 43856266893.445625, Test MSE of 43856267601.456085
Epoch 35: training loss 49276685176.471
Test Loss of 42609965528.906990, Test MSE of 42609965428.307350
Epoch 36: training loss 47530771158.588
Test Loss of 37559334027.787132, Test MSE of 37559334199.503960
Epoch 37: training loss 44478301168.941
Test Loss of 38121565464.995834, Test MSE of 38121564945.534576
Epoch 38: training loss 42703705592.471
Test Loss of 37107695059.694588, Test MSE of 37107696029.480675
Epoch 39: training loss 40313576741.647
Test Loss of 32132775156.982880, Test MSE of 32132775107.023941
Epoch 40: training loss 39023236404.706
Test Loss of 32606332473.099491, Test MSE of 32606332434.812267
Epoch 41: training loss 36055915565.176
Test Loss of 30187100656.599724, Test MSE of 30187100440.288055
Epoch 42: training loss 35286274537.412
Test Loss of 30205092835.094864, Test MSE of 30205093109.914536
Epoch 43: training loss 33336467712.000
Test Loss of 29702512239.118927, Test MSE of 29702512202.946892
Epoch 44: training loss 32020973703.529
Test Loss of 27729228422.337807, Test MSE of 27729228333.816353
Epoch 45: training loss 30792394533.647
Test Loss of 27853323606.596947, Test MSE of 27853323616.645744
Epoch 46: training loss 29184343958.588
Test Loss of 27948733547.565018, Test MSE of 27948733080.785950
Epoch 47: training loss 27955248071.529
Test Loss of 26551676550.811661, Test MSE of 26551675568.167484
Epoch 48: training loss 26695103823.059
Test Loss of 26280529229.119850, Test MSE of 26280529198.080208
Epoch 49: training loss 25887876796.235
Test Loss of 23641850490.965294, Test MSE of 23641850847.285187
Epoch 50: training loss 24767550366.118
Test Loss of 23856202569.565941, Test MSE of 23856202630.285145
Epoch 51: training loss 23818015706.353
Test Loss of 24110982605.060619, Test MSE of 24110982781.973030
Epoch 52: training loss 22817542772.706
Test Loss of 24229816756.894032, Test MSE of 24229816648.612556
Epoch 53: training loss 22111744210.824
Test Loss of 22421546154.113834, Test MSE of 22421546316.927376
Epoch 54: training loss 21178688470.588
Test Loss of 21535544691.975937, Test MSE of 21535544353.705700
Epoch 55: training loss 20096734023.529
Test Loss of 19603202181.627026, Test MSE of 19603201598.847446
Epoch 56: training loss 20100873494.588
Test Loss of 22666594402.561775, Test MSE of 22666594674.583904
Epoch 57: training loss 19102271695.059
Test Loss of 20538585323.979641, Test MSE of 20538585655.010368
Epoch 58: training loss 18553586413.176
Test Loss of 20149181158.056454, Test MSE of 20149181413.082901
Epoch 59: training loss 18014590230.588
Test Loss of 19603985305.173531, Test MSE of 19603985049.756256
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23202524678.811108, 'MSE - std': 2640486576.72322, 'R2 - mean': 0.82788158668302, 'R2 - std': 0.014132780323424417} 
 

Saving model.....
Results After CV: {'MSE - mean': 23202524678.811108, 'MSE - std': 2640486576.72322, 'R2 - mean': 0.82788158668302, 'R2 - std': 0.014132780323424417}
Train time: 91.95290946400056
Inference time: 0.07073236639917013
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 58 finished with value: 23202524678.811108 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003802 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427526784060.235
Test Loss of 418112371833.989380, Test MSE of 418112375891.668518
Epoch 2: training loss 427507721035.294
Test Loss of 418093587333.536865, Test MSE of 418093585553.528320
Epoch 3: training loss 427481609758.118
Test Loss of 418068944463.470764, Test MSE of 418068944044.593445
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490687578.353
Test Loss of 418070776545.621094, Test MSE of 418070776035.038574
Epoch 2: training loss 427480912112.941
Test Loss of 418072932275.253296, Test MSE of 418072935096.218506
Epoch 3: training loss 423565833999.059
Test Loss of 405748217592.123962, Test MSE of 405748214490.735840
Epoch 4: training loss 396784417129.412
Test Loss of 364356565705.933838, Test MSE of 364356569048.388550
Epoch 5: training loss 332115550087.529
Test Loss of 280007690674.424255, Test MSE of 280007692453.516235
Epoch 6: training loss 250437911672.471
Test Loss of 204113054755.767761, Test MSE of 204113058064.318939
Epoch 7: training loss 183619245417.412
Test Loss of 145738192118.584320, Test MSE of 145738191605.092224
Epoch 8: training loss 149232188747.294
Test Loss of 124233035559.024750, Test MSE of 124233035392.909729
Epoch 9: training loss 139781653112.471
Test Loss of 118871766495.666901, Test MSE of 118871763760.599930
Epoch 10: training loss 136827479130.353
Test Loss of 115323899141.270416, Test MSE of 115323899222.595306
Epoch 11: training loss 134739894452.706
Test Loss of 113577294687.637283, Test MSE of 113577294963.749939
Epoch 12: training loss 129874259079.529
Test Loss of 110981922459.743698, Test MSE of 110981919109.362762
Epoch 13: training loss 127877772890.353
Test Loss of 108533122286.056900, Test MSE of 108533120796.724030
Epoch 14: training loss 125490016948.706
Test Loss of 106365286439.320847, Test MSE of 106365286802.327896
Epoch 15: training loss 122740448286.118
Test Loss of 103489594134.206802, Test MSE of 103489595909.970123
Epoch 16: training loss 118380605078.588
Test Loss of 101224546749.320374, Test MSE of 101224547801.191025
Epoch 17: training loss 116967160922.353
Test Loss of 98723861113.870926, Test MSE of 98723859312.635223
Epoch 18: training loss 113043623830.588
Test Loss of 95618599549.187134, Test MSE of 95618600464.017807
Epoch 19: training loss 111523521024.000
Test Loss of 94468770055.876007, Test MSE of 94468770542.567810
Epoch 20: training loss 107617683425.882
Test Loss of 90370594161.758041, Test MSE of 90370595128.379822
Epoch 21: training loss 104355248233.412
Test Loss of 87688842033.210266, Test MSE of 87688842920.807114
Epoch 22: training loss 101409919111.529
Test Loss of 84812754840.723572, Test MSE of 84812753773.510117
Epoch 23: training loss 97326415104.000
Test Loss of 82120585108.933609, Test MSE of 82120584212.253525
Epoch 24: training loss 94730383073.882
Test Loss of 81340815295.096924, Test MSE of 81340815221.788834
Epoch 25: training loss 91821065750.588
Test Loss of 77408051383.813095, Test MSE of 77408051932.913422
Epoch 26: training loss 88199820604.235
Test Loss of 75377069502.978485, Test MSE of 75377069902.631760
Epoch 27: training loss 85247980724.706
Test Loss of 73377474475.199631, Test MSE of 73377473819.671265
Epoch 28: training loss 82499006945.882
Test Loss of 71647255323.417999, Test MSE of 71647254646.962173
Epoch 29: training loss 79801942110.118
Test Loss of 66454469151.385612, Test MSE of 66454468823.232330
Epoch 30: training loss 77385073377.882
Test Loss of 64567729001.112190, Test MSE of 64567728229.568214
Epoch 31: training loss 74607843215.059
Test Loss of 63858311213.479530, Test MSE of 63858311195.996590
Epoch 32: training loss 71051113426.824
Test Loss of 61843781256.320145, Test MSE of 61843780597.859627
Epoch 33: training loss 68298742723.765
Test Loss of 60156437321.371269, Test MSE of 60156436438.207886
Epoch 34: training loss 65845054087.529
Test Loss of 55252513469.142723, Test MSE of 55252513579.073380
Epoch 35: training loss 62839468784.941
Test Loss of 54065442058.007866, Test MSE of 54065441879.452972
Epoch 36: training loss 60548226232.471
Test Loss of 51477435645.216751, Test MSE of 51477435744.487511
Epoch 37: training loss 58541621760.000
Test Loss of 48425454495.119133, Test MSE of 48425454296.024086
Epoch 38: training loss 56182731452.235
Test Loss of 47310888917.362946, Test MSE of 47310889368.814941
Epoch 39: training loss 53437948634.353
Test Loss of 45675476154.892433, Test MSE of 45675476282.978729
Epoch 40: training loss 51314380378.353
Test Loss of 44647365479.217209, Test MSE of 44647365401.545815
Epoch 41: training loss 49339023894.588
Test Loss of 42843840469.362946, Test MSE of 42843839646.093353
Epoch 42: training loss 46311001961.412
Test Loss of 43057076503.035858, Test MSE of 43057076256.968582
Epoch 43: training loss 44582325458.824
Test Loss of 38457072806.521400, Test MSE of 38457072513.160881
Epoch 44: training loss 42937890138.353
Test Loss of 37078920712.882721, Test MSE of 37078919801.437386
Epoch 45: training loss 41202550949.647
Test Loss of 34169413322.170715, Test MSE of 34169413338.003700
Epoch 46: training loss 39086493010.824
Test Loss of 35576845588.903999, Test MSE of 35576845352.304100
Epoch 47: training loss 37864366151.529
Test Loss of 36030564138.340965, Test MSE of 36030563480.779579
Epoch 48: training loss 35472748288.000
Test Loss of 33246574135.783485, Test MSE of 33246573910.128208
Epoch 49: training loss 34109590979.765
Test Loss of 30175971104.866066, Test MSE of 30175971276.495453
Epoch 50: training loss 32819341078.588
Test Loss of 30746521682.431644, Test MSE of 30746521961.180599
Epoch 51: training loss 31383731267.765
Test Loss of 28185274848.851261, Test MSE of 28185275126.857765
Epoch 52: training loss 29809875584.000
Test Loss of 25681820391.779781, Test MSE of 25681820643.455967
Epoch 53: training loss 28824742776.471
Test Loss of 24749797386.422390, Test MSE of 24749797755.921829
Epoch 54: training loss 27506116946.824
Test Loss of 25470576045.923664, Test MSE of 25470576272.340359
Epoch 55: training loss 26379320323.765
Test Loss of 23110131153.928291, Test MSE of 23110131382.081734
Epoch 56: training loss 25350688259.765
Test Loss of 23475744684.620865, Test MSE of 23475745077.755886
Epoch 57: training loss 24624878027.294
Test Loss of 24404067653.699745, Test MSE of 24404067500.828423
Epoch 58: training loss 23297948239.059
Test Loss of 23356187999.755726, Test MSE of 23356187576.196518
Epoch 59: training loss 22576112918.588
Test Loss of 22964889689.537823, Test MSE of 22964890361.847061
Epoch 60: training loss 21709156306.824
Test Loss of 21894497402.463104, Test MSE of 21894497458.189117
Epoch 61: training loss 20911924103.529
Test Loss of 21666106201.004856, Test MSE of 21666105966.791283
Epoch 62: training loss 19912432365.176
Test Loss of 21754573636.160072, Test MSE of 21754573500.930794
Epoch 63: training loss 19524420604.235
Test Loss of 18564244351.851955, Test MSE of 18564244391.199081
Epoch 64: training loss 18654824527.059
Test Loss of 22078026462.304882, Test MSE of 22078026977.317337
Epoch 65: training loss 18314989784.471
Test Loss of 19601416665.745083, Test MSE of 19601416929.620949
Epoch 66: training loss 17674076333.176
Test Loss of 19503840042.577839, Test MSE of 19503840063.018604
Epoch 67: training loss 17295537268.706
Test Loss of 19119817470.519547, Test MSE of 19119817605.469002
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19119817605.469, 'MSE - std': 0.0, 'R2 - mean': 0.8511119058217825, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003776 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918266127.059
Test Loss of 424556471623.120972, Test MSE of 424556477010.882812
Epoch 2: training loss 427897492178.824
Test Loss of 424539927535.182068, Test MSE of 424539918374.357300
Epoch 3: training loss 427870102949.647
Test Loss of 424517673719.887085, Test MSE of 424517673290.728455
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888447488.000
Test Loss of 424525931490.864685, Test MSE of 424525928218.878296
Epoch 2: training loss 427876503552.000
Test Loss of 424525114163.105225, Test MSE of 424525105241.569824
Epoch 3: training loss 424086950610.824
Test Loss of 412769062151.165405, Test MSE of 412769060684.082336
Epoch 4: training loss 397455766467.765
Test Loss of 371508436914.305786, Test MSE of 371508439762.474792
Epoch 5: training loss 333111714514.824
Test Loss of 288622847034.033752, Test MSE of 288622851512.276917
Epoch 6: training loss 250181064583.529
Test Loss of 212948761244.454315, Test MSE of 212948758899.759399
Epoch 7: training loss 182689245184.000
Test Loss of 155361485262.375214, Test MSE of 155361487718.928528
Epoch 8: training loss 146720993129.412
Test Loss of 134881566117.396255, Test MSE of 134881568812.555115
Epoch 9: training loss 136623641057.882
Test Loss of 129422539759.655792, Test MSE of 129422543651.423889
Epoch 10: training loss 132626349568.000
Test Loss of 126228946703.337494, Test MSE of 126228946598.714844
Epoch 11: training loss 130742425298.824
Test Loss of 123824054258.735138, Test MSE of 123824053416.067871
Epoch 12: training loss 128083905626.353
Test Loss of 120806715798.473282, Test MSE of 120806715681.269394
Epoch 13: training loss 124392423062.588
Test Loss of 118190653151.962982, Test MSE of 118190651321.592575
Epoch 14: training loss 121227521505.882
Test Loss of 116112049681.883881, Test MSE of 116112048824.258026
Epoch 15: training loss 118611451422.118
Test Loss of 111795784273.602585, Test MSE of 111795784496.645554
Epoch 16: training loss 114883433743.059
Test Loss of 109563267239.232010, Test MSE of 109563269504.052795
Epoch 17: training loss 112897773929.412
Test Loss of 106270378132.282211, Test MSE of 106270380096.446625
Epoch 18: training loss 108172631431.529
Test Loss of 103842232268.361786, Test MSE of 103842231429.390656
Epoch 19: training loss 105692755606.588
Test Loss of 100761636199.809387, Test MSE of 100761636921.123840
Epoch 20: training loss 101918781560.471
Test Loss of 97634036019.460556, Test MSE of 97634032601.942856
Epoch 21: training loss 98478959284.706
Test Loss of 92090599111.801987, Test MSE of 92090598142.906876
Epoch 22: training loss 96987893880.471
Test Loss of 90160529135.122833, Test MSE of 90160526983.724335
Epoch 23: training loss 93755149643.294
Test Loss of 87748669379.360626, Test MSE of 87748669785.622620
Epoch 24: training loss 90289571824.941
Test Loss of 84307665800.853104, Test MSE of 84307664495.118469
Epoch 25: training loss 87327012577.882
Test Loss of 81263931782.365952, Test MSE of 81263931883.867981
Epoch 26: training loss 83787958497.882
Test Loss of 77205847138.302109, Test MSE of 77205847103.977707
Epoch 27: training loss 79949165959.529
Test Loss of 79990918843.010880, Test MSE of 79990918491.918488
Epoch 28: training loss 77098308562.824
Test Loss of 77022061993.659958, Test MSE of 77022063542.394806
Epoch 29: training loss 75353842823.529
Test Loss of 71233437408.910477, Test MSE of 71233438591.559647
Epoch 30: training loss 71079415401.412
Test Loss of 68301903183.648392, Test MSE of 68301901872.626823
Epoch 31: training loss 68072941974.588
Test Loss of 67253171821.790421, Test MSE of 67253173473.431297
Epoch 32: training loss 65175501025.882
Test Loss of 63468796865.228775, Test MSE of 63468795787.961777
Epoch 33: training loss 62227426575.059
Test Loss of 59874797850.825813, Test MSE of 59874797552.414597
Epoch 34: training loss 59554701703.529
Test Loss of 55230226540.487625, Test MSE of 55230226783.214149
Epoch 35: training loss 57302285206.588
Test Loss of 56865508659.697433, Test MSE of 56865507775.799217
Epoch 36: training loss 54467600986.353
Test Loss of 54243981311.052513, Test MSE of 54243981379.689308
Epoch 37: training loss 52415155471.059
Test Loss of 52734175916.087898, Test MSE of 52734176705.753975
Epoch 38: training loss 49100785950.118
Test Loss of 51090700562.298401, Test MSE of 51090699796.585449
Epoch 39: training loss 47569942437.647
Test Loss of 45510881032.705063, Test MSE of 45510881269.967789
Epoch 40: training loss 45155167902.118
Test Loss of 48266968159.933380, Test MSE of 48266969103.092354
Epoch 41: training loss 42696165978.353
Test Loss of 44861764226.872078, Test MSE of 44861765505.027153
Epoch 42: training loss 41241626955.294
Test Loss of 42003674583.139488, Test MSE of 42003674112.284203
Epoch 43: training loss 38944565820.235
Test Loss of 43067959801.012260, Test MSE of 43067959260.750763
Epoch 44: training loss 36652341955.765
Test Loss of 38711108527.700211, Test MSE of 38711109004.742661
Epoch 45: training loss 34453903427.765
Test Loss of 38803419406.508446, Test MSE of 38803420308.930061
Epoch 46: training loss 33414883004.235
Test Loss of 38036091085.605370, Test MSE of 38036091808.896919
Epoch 47: training loss 31387692920.471
Test Loss of 35454697491.897293, Test MSE of 35454698625.141808
Epoch 48: training loss 30408143412.706
Test Loss of 36144943743.555862, Test MSE of 36144944217.674042
Epoch 49: training loss 29341560801.882
Test Loss of 32289761267.682629, Test MSE of 32289761486.947056
Epoch 50: training loss 27307071947.294
Test Loss of 32503244258.509369, Test MSE of 32503243856.262798
Epoch 51: training loss 26184258620.235
Test Loss of 33456768581.522091, Test MSE of 33456767872.671524
Epoch 52: training loss 24427824711.529
Test Loss of 30147345653.163082, Test MSE of 30147346814.164127
Epoch 53: training loss 23783347651.765
Test Loss of 29764883222.680546, Test MSE of 29764883264.599274
Epoch 54: training loss 22581983977.412
Test Loss of 30481416452.559795, Test MSE of 30481415647.262310
Epoch 55: training loss 21751903367.529
Test Loss of 26897130960.507057, Test MSE of 26897129920.016911
Epoch 56: training loss 20772389165.176
Test Loss of 30061391538.483459, Test MSE of 30061392468.787552
Epoch 57: training loss 19847705396.706
Test Loss of 30511565166.678696, Test MSE of 30511565091.535419
Epoch 58: training loss 19268920688.941
Test Loss of 28901946429.349987, Test MSE of 28901946227.434685
Epoch 59: training loss 18278650379.294
Test Loss of 26708366267.070091, Test MSE of 26708366198.673290
Epoch 60: training loss 17720240018.824
Test Loss of 27531252406.984039, Test MSE of 27531252219.059551
Epoch 61: training loss 16740743868.235
Test Loss of 26135600863.962990, Test MSE of 26135601884.330593
Epoch 62: training loss 16414665577.412
Test Loss of 30113530330.218830, Test MSE of 30113529759.973042
Epoch 63: training loss 15841066770.824
Test Loss of 25746246431.681702, Test MSE of 25746246346.952629
Epoch 64: training loss 15626696929.882
Test Loss of 28016457220.145271, Test MSE of 28016458224.349018
Epoch 65: training loss 14838723241.412
Test Loss of 27817743451.432800, Test MSE of 27817743092.938614
Epoch 66: training loss 14287368466.824
Test Loss of 25334683890.083736, Test MSE of 25334683996.060219
Epoch 67: training loss 13991775235.765
Test Loss of 27836974423.702057, Test MSE of 27836974325.148026
Epoch 68: training loss 13531141985.882
Test Loss of 24212386815.289383, Test MSE of 24212386966.579372
Epoch 69: training loss 13123796909.176
Test Loss of 22769611733.126068, Test MSE of 22769611362.894901
Epoch 70: training loss 12664438106.353
Test Loss of 24656492257.621098, Test MSE of 24656491924.106483
Epoch 71: training loss 12464708171.294
Test Loss of 24859500078.308582, Test MSE of 24859500295.519112
Epoch 72: training loss 12235298823.529
Test Loss of 25591892149.681240, Test MSE of 25591891799.551395
Epoch 73: training loss 11762950358.588
Test Loss of 24169219573.932919, Test MSE of 24169219348.513336
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21644518476.99117, 'MSE - std': 2524700871.522167, 'R2 - mean': 0.8392798631519334, 'R2 - std': 0.011832042669849108} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005447 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927849261.176
Test Loss of 447259174302.290100, Test MSE of 447259179777.520447
Epoch 2: training loss 421908100035.765
Test Loss of 447240867973.596130, Test MSE of 447240870513.952026
Epoch 3: training loss 421881833110.588
Test Loss of 447216737779.090454, Test MSE of 447216744434.281982
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900190177.882
Test Loss of 447222356486.750854, Test MSE of 447222354364.796082
Epoch 2: training loss 421888198053.647
Test Loss of 447223912538.485291, Test MSE of 447223911280.429321
Epoch 3: training loss 418289342704.941
Test Loss of 435652074271.207947, Test MSE of 435652074501.956299
Epoch 4: training loss 392178286953.412
Test Loss of 393822935755.828796, Test MSE of 393822937779.700684
Epoch 5: training loss 328526630671.059
Test Loss of 309431472617.852417, Test MSE of 309431471817.566589
Epoch 6: training loss 246775648617.412
Test Loss of 231155879800.508911, Test MSE of 231155882172.472412
Epoch 7: training loss 178906071702.588
Test Loss of 170345517614.071716, Test MSE of 170345517420.140228
Epoch 8: training loss 145122479224.471
Test Loss of 147233265189.781158, Test MSE of 147233267681.077850
Epoch 9: training loss 134203858823.529
Test Loss of 140415836033.036316, Test MSE of 140415836755.483643
Epoch 10: training loss 132099384079.059
Test Loss of 136868855523.752945, Test MSE of 136868854300.458969
Epoch 11: training loss 128905481788.235
Test Loss of 133931898784.777237, Test MSE of 133931899999.469955
Epoch 12: training loss 126342495021.176
Test Loss of 131591377620.829987, Test MSE of 131591375939.991852
Epoch 13: training loss 123538452630.588
Test Loss of 128348678572.739304, Test MSE of 128348679454.289093
Epoch 14: training loss 119848170224.941
Test Loss of 125004142246.639832, Test MSE of 125004143365.963928
Epoch 15: training loss 116800890910.118
Test Loss of 123720991205.114960, Test MSE of 123720989225.582825
Epoch 16: training loss 114470847397.647
Test Loss of 120330901139.690033, Test MSE of 120330900906.307602
Epoch 17: training loss 111537531934.118
Test Loss of 117251966205.216751, Test MSE of 117251964998.495483
Epoch 18: training loss 107663459659.294
Test Loss of 114053084726.835999, Test MSE of 114053081993.414032
Epoch 19: training loss 104830153667.765
Test Loss of 112263573165.746002, Test MSE of 112263573284.884048
Epoch 20: training loss 102587618484.706
Test Loss of 107825637035.377289, Test MSE of 107825634107.730896
Epoch 21: training loss 99084785543.529
Test Loss of 102815045252.530182, Test MSE of 102815045924.867599
Epoch 22: training loss 95545054418.824
Test Loss of 101203124326.328934, Test MSE of 101203123523.955551
Epoch 23: training loss 93994915900.235
Test Loss of 97255381353.467499, Test MSE of 97255380170.097702
Epoch 24: training loss 89142318260.706
Test Loss of 95234843651.079346, Test MSE of 95234845265.117737
Epoch 25: training loss 87538811843.765
Test Loss of 92497909687.516998, Test MSE of 92497911163.688934
Epoch 26: training loss 84258479646.118
Test Loss of 88517346303.289383, Test MSE of 88517346114.930832
Epoch 27: training loss 81502726219.294
Test Loss of 83802760195.079346, Test MSE of 83802761042.700775
Epoch 28: training loss 78080335119.059
Test Loss of 83586568227.057129, Test MSE of 83586568843.197525
Epoch 29: training loss 75081418767.059
Test Loss of 77787587825.846863, Test MSE of 77787589190.769455
Epoch 30: training loss 72032463540.706
Test Loss of 75099968275.364334, Test MSE of 75099968503.257629
Epoch 31: training loss 68566204762.353
Test Loss of 73272033964.087906, Test MSE of 73272034108.076370
Epoch 32: training loss 66206015713.882
Test Loss of 72858318773.622025, Test MSE of 72858317957.422256
Epoch 33: training loss 64144320090.353
Test Loss of 70497126517.962524, Test MSE of 70497127347.494461
Epoch 34: training loss 61114556912.941
Test Loss of 63990593707.969467, Test MSE of 63990594276.654594
Epoch 35: training loss 58713338051.765
Test Loss of 63552662264.360863, Test MSE of 63552662157.494469
Epoch 36: training loss 56372373248.000
Test Loss of 57663086358.917419, Test MSE of 57663086411.329262
Epoch 37: training loss 53413311683.765
Test Loss of 59808393983.940781, Test MSE of 59808395503.470276
Epoch 38: training loss 50767388114.824
Test Loss of 55646660678.588020, Test MSE of 55646661091.796127
Epoch 39: training loss 48866083425.882
Test Loss of 53067977621.170486, Test MSE of 53067978572.532234
Epoch 40: training loss 46758277511.529
Test Loss of 53451436864.843857, Test MSE of 53451436546.456879
Epoch 41: training loss 44168313795.765
Test Loss of 50415575897.478600, Test MSE of 50415577114.951164
Epoch 42: training loss 42504289204.706
Test Loss of 46378040689.284294, Test MSE of 46378040293.146469
Epoch 43: training loss 40459625840.941
Test Loss of 43152122662.314133, Test MSE of 43152123440.257332
Epoch 44: training loss 38859756762.353
Test Loss of 38961845477.529495, Test MSE of 38961845139.047318
Epoch 45: training loss 36926750479.059
Test Loss of 41410365687.768677, Test MSE of 41410364845.811462
Epoch 46: training loss 35206611538.824
Test Loss of 38794658945.095535, Test MSE of 38794659922.408440
Epoch 47: training loss 33770517880.471
Test Loss of 37335836549.300026, Test MSE of 37335836049.537239
Epoch 48: training loss 32414165458.824
Test Loss of 38116073005.834839, Test MSE of 38116073029.437950
Epoch 49: training loss 30534177528.471
Test Loss of 36098309648.462639, Test MSE of 36098309320.201324
Epoch 50: training loss 29297487412.706
Test Loss of 31650096102.654636, Test MSE of 31650095624.516033
Epoch 51: training loss 28683491926.588
Test Loss of 33237296088.442284, Test MSE of 33237295834.803608
Epoch 52: training loss 27161233505.882
Test Loss of 34728088787.527184, Test MSE of 34728088378.154343
Epoch 53: training loss 26223078848.000
Test Loss of 33577697868.391396, Test MSE of 33577698089.781258
Epoch 54: training loss 24746538447.059
Test Loss of 32799637450.940552, Test MSE of 32799637139.519279
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25362891364.500538, 'MSE - std': 5648185861.103787, 'R2 - mean': 0.8200715688169836, 'R2 - std': 0.028831382756934948} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005515 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430110584350.118
Test Loss of 410764473125.552979, Test MSE of 410764469033.655884
Epoch 2: training loss 430089492841.412
Test Loss of 410744823849.699219, Test MSE of 410744823444.661804
Epoch 3: training loss 430062303593.412
Test Loss of 410719680998.174927, Test MSE of 410719674562.868835
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430072591781.647
Test Loss of 410724962984.455322, Test MSE of 410724967390.236267
Epoch 2: training loss 430063084001.882
Test Loss of 410725098923.890808, Test MSE of 410725095281.551392
Epoch 3: training loss 426257897833.412
Test Loss of 398912240094.593262, Test MSE of 398912242456.182434
Epoch 4: training loss 399590328681.412
Test Loss of 357352724575.718628, Test MSE of 357352715944.023804
Epoch 5: training loss 335225434955.294
Test Loss of 274482605179.676086, Test MSE of 274482605040.784241
Epoch 6: training loss 253514131456.000
Test Loss of 197201178216.484955, Test MSE of 197201176089.110504
Epoch 7: training loss 186451216143.059
Test Loss of 138971652372.731140, Test MSE of 138971653085.134827
Epoch 8: training loss 151337784018.824
Test Loss of 118721414446.793152, Test MSE of 118721412411.634109
Epoch 9: training loss 141546523497.412
Test Loss of 111864322735.089310, Test MSE of 111864323245.567352
Epoch 10: training loss 137831338586.353
Test Loss of 109384198905.010651, Test MSE of 109384198748.944366
Epoch 11: training loss 135536784293.647
Test Loss of 107591814161.532623, Test MSE of 107591814736.163666
Epoch 12: training loss 132184475678.118
Test Loss of 105288541369.277191, Test MSE of 105288541053.450302
Epoch 13: training loss 128011945984.000
Test Loss of 102324154555.646454, Test MSE of 102324153869.635742
Epoch 14: training loss 125947059712.000
Test Loss of 100021595051.180008, Test MSE of 100021593801.321762
Epoch 15: training loss 123064579734.588
Test Loss of 97355073425.117996, Test MSE of 97355073540.265564
Epoch 16: training loss 119652211200.000
Test Loss of 95586186286.911621, Test MSE of 95586185138.720993
Epoch 17: training loss 117311313136.941
Test Loss of 93033372164.501617, Test MSE of 93033372212.150452
Epoch 18: training loss 113494633110.588
Test Loss of 90009482999.589081, Test MSE of 90009483622.232712
Epoch 19: training loss 111963523222.588
Test Loss of 88155992642.102737, Test MSE of 88155994064.995316
Epoch 20: training loss 106581270046.118
Test Loss of 85595124707.094864, Test MSE of 85595123132.425522
Epoch 21: training loss 103707841445.647
Test Loss of 83185794461.675156, Test MSE of 83185792953.571487
Epoch 22: training loss 100115154281.412
Test Loss of 80244841070.645065, Test MSE of 80244840236.083115
Epoch 23: training loss 97698597285.647
Test Loss of 78315656951.589081, Test MSE of 78315656581.979004
Epoch 24: training loss 95393793626.353
Test Loss of 75278476272.362793, Test MSE of 75278476413.312515
Epoch 25: training loss 92086246415.059
Test Loss of 71967523460.442383, Test MSE of 71967522529.240082
Epoch 26: training loss 88734437180.235
Test Loss of 70800401987.524292, Test MSE of 70800402769.514359
Epoch 27: training loss 84866688406.588
Test Loss of 68589522221.845444, Test MSE of 68589522300.215813
Epoch 28: training loss 81982137359.059
Test Loss of 67246360836.620087, Test MSE of 67246360759.751633
Epoch 29: training loss 78565667553.882
Test Loss of 63126543929.099487, Test MSE of 63126544980.731987
Epoch 30: training loss 76389009513.412
Test Loss of 60320415054.067558, Test MSE of 60320415823.052307
Epoch 31: training loss 73123577569.882
Test Loss of 59194963116.009254, Test MSE of 59194962556.944031
Epoch 32: training loss 70554869850.353
Test Loss of 57075121141.101341, Test MSE of 57075121424.225113
Epoch 33: training loss 67643020769.882
Test Loss of 53399601261.934288, Test MSE of 53399600873.152039
Epoch 34: training loss 65264297020.235
Test Loss of 51863721313.969460, Test MSE of 51863720926.372910
Epoch 35: training loss 61646865859.765
Test Loss of 51481311720.070335, Test MSE of 51481312138.381836
Epoch 36: training loss 59740992557.176
Test Loss of 49310432463.074501, Test MSE of 49310432480.628212
Epoch 37: training loss 57161728376.471
Test Loss of 46536321966.970848, Test MSE of 46536322328.008934
Epoch 38: training loss 54473308423.529
Test Loss of 44959566255.207771, Test MSE of 44959567276.076309
Epoch 39: training loss 51666639676.235
Test Loss of 42616024451.139290, Test MSE of 42616024734.208046
Epoch 40: training loss 50015747922.824
Test Loss of 40985969961.106895, Test MSE of 40985970263.534622
Epoch 41: training loss 47330940581.647
Test Loss of 38719472175.148544, Test MSE of 38719471998.185280
Epoch 42: training loss 45009084611.765
Test Loss of 38739653320.677467, Test MSE of 38739652675.551254
Epoch 43: training loss 43302713728.000
Test Loss of 37892654007.500229, Test MSE of 37892654177.228371
Epoch 44: training loss 41381906913.882
Test Loss of 35357251255.144844, Test MSE of 35357250871.103409
Epoch 45: training loss 39411218552.471
Test Loss of 36284252347.646461, Test MSE of 36284251966.452194
Epoch 46: training loss 37712952711.529
Test Loss of 30617001254.263767, Test MSE of 30617000974.478100
Epoch 47: training loss 36111230720.000
Test Loss of 31090012033.006943, Test MSE of 31090011552.510941
Epoch 48: training loss 34618381281.882
Test Loss of 32867163835.883389, Test MSE of 32867162849.263882
Epoch 49: training loss 32612522277.647
Test Loss of 28922642176.118465, Test MSE of 28922641750.890163
Epoch 50: training loss 31459065419.294
Test Loss of 28631955530.869041, Test MSE of 28631955381.378067
Epoch 51: training loss 30326326505.412
Test Loss of 27176404985.366035, Test MSE of 27176404478.339737
Epoch 52: training loss 28784066032.941
Test Loss of 26241990166.034245, Test MSE of 26241989891.270164
Epoch 53: training loss 27370486384.941
Test Loss of 27211437391.015270, Test MSE of 27211436740.508278
Epoch 54: training loss 26231352410.353
Test Loss of 24671617687.396576, Test MSE of 24671617419.879272
Epoch 55: training loss 25096125244.235
Test Loss of 22269682786.087921, Test MSE of 22269683110.492760
Epoch 56: training loss 24478735785.412
Test Loss of 24475335816.944008, Test MSE of 24475335593.058556
Epoch 57: training loss 23342883900.235
Test Loss of 24936104174.822765, Test MSE of 24936104160.623417
Epoch 58: training loss 22166255156.706
Test Loss of 23759648791.692734, Test MSE of 23759648759.841915
Epoch 59: training loss 21592977031.529
Test Loss of 21917183871.111523, Test MSE of 21917184273.603424
Epoch 60: training loss 20967484928.000
Test Loss of 21461370664.869968, Test MSE of 21461370536.176006
Epoch 61: training loss 19776399130.353
Test Loss of 22388506237.334568, Test MSE of 22388506482.206581
Epoch 62: training loss 19501312538.353
Test Loss of 19813679379.783432, Test MSE of 19813679495.663387
Epoch 63: training loss 19035748096.000
Test Loss of 19418971109.937992, Test MSE of 19418970990.354485
Epoch 64: training loss 18601264244.706
Test Loss of 21454323345.236465, Test MSE of 21454322537.215496
Epoch 65: training loss 17882261417.412
Test Loss of 20887229314.428505, Test MSE of 20887229397.081692
Epoch 66: training loss 17236315559.529
Test Loss of 19928328554.972698, Test MSE of 19928328440.041946
Epoch 67: training loss 16615704617.412
Test Loss of 20101650861.312355, Test MSE of 20101650370.874249
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24047581116.093967, 'MSE - std': 5395982374.866474, 'R2 - mean': 0.8235763274366714, 'R2 - std': 0.025696040019903683} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005312 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424044816865.882
Test Loss of 431614393200.421997, Test MSE of 431614394270.431946
Epoch 2: training loss 424026922887.529
Test Loss of 431595409171.072632, Test MSE of 431595408460.720703
Epoch 3: training loss 424000773421.176
Test Loss of 431569070340.620056, Test MSE of 431569073840.120239
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424015988615.529
Test Loss of 431571489926.100891, Test MSE of 431571491094.898804
Epoch 2: training loss 424002465671.529
Test Loss of 431572476483.998169, Test MSE of 431572477671.581970
Epoch 3: training loss 420400540732.235
Test Loss of 419531525490.080505, Test MSE of 419531522643.332214
Epoch 4: training loss 394589857912.471
Test Loss of 378290141745.991699, Test MSE of 378290141409.923645
Epoch 5: training loss 331517683471.059
Test Loss of 294241814337.510437, Test MSE of 294241812642.262512
Epoch 6: training loss 249721965869.176
Test Loss of 215666618036.301727, Test MSE of 215666620491.920074
Epoch 7: training loss 182832204528.941
Test Loss of 153598215796.805176, Test MSE of 153598217146.613251
Epoch 8: training loss 148869007119.059
Test Loss of 131311941147.720505, Test MSE of 131311941405.254013
Epoch 9: training loss 138092632425.412
Test Loss of 124377497876.731140, Test MSE of 124377499927.467453
Epoch 10: training loss 134353395862.588
Test Loss of 121370262816.577515, Test MSE of 121370261882.379944
Epoch 11: training loss 132416717281.882
Test Loss of 117864904481.288300, Test MSE of 117864905902.623047
Epoch 12: training loss 129993138085.647
Test Loss of 116330782784.444244, Test MSE of 116330784319.879639
Epoch 13: training loss 127767013014.588
Test Loss of 113298684212.005554, Test MSE of 113298685278.650467
Epoch 14: training loss 125006350697.412
Test Loss of 110900410634.306335, Test MSE of 110900410284.904739
Epoch 15: training loss 120877716374.588
Test Loss of 107833955094.389633, Test MSE of 107833955651.264847
Epoch 16: training loss 117288791702.588
Test Loss of 105501111270.885696, Test MSE of 105501111445.715958
Epoch 17: training loss 115440425411.765
Test Loss of 102374886493.349380, Test MSE of 102374886727.414001
Epoch 18: training loss 112954714458.353
Test Loss of 99467566194.198975, Test MSE of 99467564695.096970
Epoch 19: training loss 110891393295.059
Test Loss of 95690487471.563171, Test MSE of 95690485829.938065
Epoch 20: training loss 107542474691.765
Test Loss of 92917970626.517349, Test MSE of 92917968418.052536
Epoch 21: training loss 103677285752.471
Test Loss of 91692985460.094406, Test MSE of 91692985789.620163
Epoch 22: training loss 100107260400.941
Test Loss of 87533158873.854691, Test MSE of 87533157899.073746
Epoch 23: training loss 97699421033.412
Test Loss of 85659115887.711243, Test MSE of 85659116062.419922
Epoch 24: training loss 94323875840.000
Test Loss of 79402718520.270248, Test MSE of 79402717985.624161
Epoch 25: training loss 91509658955.294
Test Loss of 79168007655.596481, Test MSE of 79168007377.249100
Epoch 26: training loss 87923498646.588
Test Loss of 76328131321.010651, Test MSE of 76328130098.313232
Epoch 27: training loss 85097105935.059
Test Loss of 71840422311.626099, Test MSE of 71840422527.211594
Epoch 28: training loss 83484616207.059
Test Loss of 68604148823.663116, Test MSE of 68604150497.415001
Epoch 29: training loss 80040339230.118
Test Loss of 66670115055.770477, Test MSE of 66670113329.586533
Epoch 30: training loss 76123343796.706
Test Loss of 68533048694.819069, Test MSE of 68533050238.882576
Epoch 31: training loss 73854909952.000
Test Loss of 66700059334.782043, Test MSE of 66700060409.022919
Epoch 32: training loss 71062384700.235
Test Loss of 62508402690.843124, Test MSE of 62508402918.992226
Epoch 33: training loss 69675358810.353
Test Loss of 60296921994.010178, Test MSE of 60296922631.553352
Epoch 34: training loss 65869892818.824
Test Loss of 55442164150.789452, Test MSE of 55442164201.767357
Epoch 35: training loss 63980872553.412
Test Loss of 50543055382.508095, Test MSE of 50543054168.123840
Epoch 36: training loss 60918117940.706
Test Loss of 53390312129.095787, Test MSE of 53390312275.816162
Epoch 37: training loss 58872203226.353
Test Loss of 51355487871.229988, Test MSE of 51355488978.519157
Epoch 38: training loss 56565975536.941
Test Loss of 48011797029.671448, Test MSE of 48011797622.692764
Epoch 39: training loss 53573567341.176
Test Loss of 45889713515.446556, Test MSE of 45889713663.214211
Epoch 40: training loss 51046697615.059
Test Loss of 44219596947.842667, Test MSE of 44219597212.612503
Epoch 41: training loss 48933952150.588
Test Loss of 41418455103.496529, Test MSE of 41418454310.899406
Epoch 42: training loss 47095894671.059
Test Loss of 39420524916.449791, Test MSE of 39420524786.653000
Epoch 43: training loss 45542218985.412
Test Loss of 38037418135.633507, Test MSE of 38037418367.751289
Epoch 44: training loss 43148125733.647
Test Loss of 35237383331.479874, Test MSE of 35237384029.098877
Epoch 45: training loss 40930763565.176
Test Loss of 36232050234.047203, Test MSE of 36232050521.242645
Epoch 46: training loss 39020079593.412
Test Loss of 33545714533.049515, Test MSE of 33545713972.641487
Epoch 47: training loss 37774417114.353
Test Loss of 31276652778.084221, Test MSE of 31276653041.769344
Epoch 48: training loss 36658868577.882
Test Loss of 32410028885.886166, Test MSE of 32410028784.852978
Epoch 49: training loss 34322958930.824
Test Loss of 33626680616.159187, Test MSE of 33626680286.896561
Epoch 50: training loss 33080166994.824
Test Loss of 27891815683.198521, Test MSE of 27891814686.029083
Epoch 51: training loss 31715623770.353
Test Loss of 29900513180.490513, Test MSE of 29900513240.074772
Epoch 52: training loss 30430186179.765
Test Loss of 28812802267.868580, Test MSE of 28812802722.467030
Epoch 53: training loss 28909171960.471
Test Loss of 26240075703.026375, Test MSE of 26240075764.415207
Epoch 54: training loss 27563939109.647
Test Loss of 25681851718.485886, Test MSE of 25681851581.155937
Epoch 55: training loss 26799325116.235
Test Loss of 26428606718.933826, Test MSE of 26428606762.115562
Epoch 56: training loss 25544532250.353
Test Loss of 24263831645.823231, Test MSE of 24263831405.911896
Epoch 57: training loss 24567026145.882
Test Loss of 23699116890.624710, Test MSE of 23699116962.699257
Epoch 58: training loss 23846832734.118
Test Loss of 24123243345.147617, Test MSE of 24123242916.421780
Epoch 59: training loss 23007863533.176
Test Loss of 24252443580.238777, Test MSE of 24252442994.662640
Epoch 60: training loss 22397686351.059
Test Loss of 22498453127.759369, Test MSE of 22498453373.519398
Epoch 61: training loss 21280293409.882
Test Loss of 21745183042.221195, Test MSE of 21745182831.346142
Epoch 62: training loss 20736636111.059
Test Loss of 20186748359.374363, Test MSE of 20186748323.537544
Epoch 63: training loss 20183289765.647
Test Loss of 23662444816.940304, Test MSE of 23662444417.902454
Epoch 64: training loss 19322449867.294
Test Loss of 21217052345.987968, Test MSE of 21217051946.261612
Epoch 65: training loss 18961083463.529
Test Loss of 22633039829.353077, Test MSE of 22633039945.734066
Epoch 66: training loss 18029025163.294
Test Loss of 21120606468.620083, Test MSE of 21120606216.234718
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23462186136.12212, 'MSE - std': 4966291348.904198, 'R2 - mean': 0.8273151676748149, 'R2 - std': 0.024169089421594632} 
 

Saving model.....
Results After CV: {'MSE - mean': 23462186136.12212, 'MSE - std': 4966291348.904198, 'R2 - mean': 0.8273151676748149, 'R2 - std': 0.024169089421594632}
Train time: 100.04171050099976
Inference time: 0.07095883699948899
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 59 finished with value: 23462186136.12212 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 2, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005497 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524361396.706
Test Loss of 418111410040.508911, Test MSE of 418111410813.441040
Epoch 2: training loss 427502018560.000
Test Loss of 418092274142.482544, Test MSE of 418092274185.973267
Epoch 3: training loss 427474547651.765
Test Loss of 418068593414.573242, Test MSE of 418068597809.599670
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492211290.353
Test Loss of 418070940302.241943, Test MSE of 418070939778.239746
Epoch 2: training loss 427480858142.118
Test Loss of 418072369238.695374, Test MSE of 418072372677.381409
Epoch 3: training loss 427480453240.471
Test Loss of 418071616090.130005, Test MSE of 418071617726.369324
Epoch 4: training loss 427480137005.176
Test Loss of 418071145149.616455, Test MSE of 418071142528.782593
Epoch 5: training loss 420871878535.529
Test Loss of 396994387469.620178, Test MSE of 396994380522.712952
Epoch 6: training loss 376350457253.647
Test Loss of 331088706951.076538, Test MSE of 331088710775.667480
Epoch 7: training loss 297595509097.412
Test Loss of 245199913979.499420, Test MSE of 245199915099.640900
Epoch 8: training loss 218195807744.000
Test Loss of 173343271795.534576, Test MSE of 173343272175.497284
Epoch 9: training loss 160053389583.059
Test Loss of 126107964594.365021, Test MSE of 126107966601.356750
Epoch 10: training loss 139465601536.000
Test Loss of 118290544630.051346, Test MSE of 118290543507.098328
Epoch 11: training loss 135354058420.706
Test Loss of 115288885854.867447, Test MSE of 115288886914.384323
Epoch 12: training loss 132424564269.176
Test Loss of 112718775725.449921, Test MSE of 112718776859.871216
Epoch 13: training loss 128805607785.412
Test Loss of 109160718021.433258, Test MSE of 109160716281.044296
Epoch 14: training loss 125161017991.529
Test Loss of 105688090762.807312, Test MSE of 105688093520.050140
Epoch 15: training loss 121174549865.412
Test Loss of 102045592014.848953, Test MSE of 102045592717.585770
Epoch 16: training loss 116774821285.647
Test Loss of 99348885285.129776, Test MSE of 99348886059.269287
Epoch 17: training loss 113529990716.235
Test Loss of 95233118366.230865, Test MSE of 95233118673.612183
Epoch 18: training loss 110267111514.353
Test Loss of 93236705368.590332, Test MSE of 93236705349.299957
Epoch 19: training loss 105235836762.353
Test Loss of 88283428101.744156, Test MSE of 88283427550.635620
Epoch 20: training loss 101328121057.882
Test Loss of 84717552819.312515, Test MSE of 84717553536.421753
Epoch 21: training loss 98106699745.882
Test Loss of 81197039199.104324, Test MSE of 81197039016.631149
Epoch 22: training loss 93543055435.294
Test Loss of 79157775346.498260, Test MSE of 79157775791.917023
Epoch 23: training loss 89444233366.588
Test Loss of 74901738189.250061, Test MSE of 74901739608.833008
Epoch 24: training loss 86503172600.471
Test Loss of 72516740930.975708, Test MSE of 72516742526.142410
Epoch 25: training loss 82565467233.882
Test Loss of 70422018993.121445, Test MSE of 70422020557.509903
Epoch 26: training loss 78126526049.882
Test Loss of 64452801756.054588, Test MSE of 64452803639.377159
Epoch 27: training loss 74942986119.529
Test Loss of 62267152582.499191, Test MSE of 62267152769.118507
Epoch 28: training loss 71524447691.294
Test Loss of 56490580856.272034, Test MSE of 56490581410.542839
Epoch 29: training loss 68273528244.706
Test Loss of 56176501069.516541, Test MSE of 56176501973.049393
Epoch 30: training loss 65593528779.294
Test Loss of 53680555004.446915, Test MSE of 53680555187.326683
Epoch 31: training loss 62112840489.412
Test Loss of 50048596608.029610, Test MSE of 50048597064.399872
Epoch 32: training loss 59315398520.471
Test Loss of 46698567217.151054, Test MSE of 46698567285.799423
Epoch 33: training loss 56660027881.412
Test Loss of 43451239353.648857, Test MSE of 43451239749.506752
Epoch 34: training loss 53235309327.059
Test Loss of 41586408317.956978, Test MSE of 41586407834.694641
Epoch 35: training loss 50471322383.059
Test Loss of 40455408682.400185, Test MSE of 40455409899.475853
Epoch 36: training loss 48036461191.529
Test Loss of 40424814059.036781, Test MSE of 40424813898.091049
Epoch 37: training loss 45484394518.588
Test Loss of 39148089252.093452, Test MSE of 39148089592.894012
Epoch 38: training loss 43168465404.235
Test Loss of 33375904486.832291, Test MSE of 33375904409.916473
Epoch 39: training loss 41601602966.588
Test Loss of 35711814398.045799, Test MSE of 35711815241.335602
Epoch 40: training loss 39188001648.941
Test Loss of 32637490989.183437, Test MSE of 32637491707.351147
Epoch 41: training loss 37330929792.000
Test Loss of 32467049559.879715, Test MSE of 32467049438.267288
Epoch 42: training loss 35202316340.706
Test Loss of 30109809776.514458, Test MSE of 30109809306.685345
Epoch 43: training loss 33340391047.529
Test Loss of 24818608221.327782, Test MSE of 24818608900.239758
Epoch 44: training loss 31536411459.765
Test Loss of 25013359831.790886, Test MSE of 25013359794.713604
Epoch 45: training loss 30054720903.529
Test Loss of 24584410399.326393, Test MSE of 24584410215.368732
Epoch 46: training loss 28645622272.000
Test Loss of 26157120471.968540, Test MSE of 26157120603.991051
Epoch 47: training loss 27779510068.706
Test Loss of 25112054290.120750, Test MSE of 25112054272.122444
Epoch 48: training loss 26562913509.647
Test Loss of 22248203021.205643, Test MSE of 22248203286.073357
Epoch 49: training loss 25508928000.000
Test Loss of 23578568109.449921, Test MSE of 23578567732.174683
Epoch 50: training loss 23808278319.059
Test Loss of 22288887419.055286, Test MSE of 22288887891.129208
Epoch 51: training loss 23100143717.647
Test Loss of 21749470749.253757, Test MSE of 21749471394.166565
Epoch 52: training loss 22218572178.824
Test Loss of 22821198766.278973, Test MSE of 22821198900.000259
Epoch 53: training loss 21154187294.118
Test Loss of 20918522908.661579, Test MSE of 20918522294.016613
Epoch 54: training loss 20425400534.588
Test Loss of 21720280614.254917, Test MSE of 21720280663.112450
Epoch 55: training loss 20030963115.294
Test Loss of 23450539030.502892, Test MSE of 23450539295.527020
Epoch 56: training loss 19003005059.765
Test Loss of 19563742601.919037, Test MSE of 19563742721.726486
Epoch 57: training loss 18563708660.706
Test Loss of 22053290763.310665, Test MSE of 22053290689.037224
Epoch 58: training loss 17887684837.647
Test Loss of 20193946165.651630, Test MSE of 20193946251.144779
Epoch 59: training loss 17408335777.882
Test Loss of 19824454294.769375, Test MSE of 19824454583.785145
Epoch 60: training loss 16894305528.471
Test Loss of 20674372585.497108, Test MSE of 20674372542.222172
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20674372542.22217, 'MSE - std': 0.0, 'R2 - mean': 0.839006417861358, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005431 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916074526.118
Test Loss of 424556465369.685852, Test MSE of 424556473531.382751
Epoch 2: training loss 427893563392.000
Test Loss of 424538726943.859375, Test MSE of 424538720320.011108
Epoch 3: training loss 427865775646.118
Test Loss of 424517297338.892456, Test MSE of 424517292899.610779
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888832512.000
Test Loss of 424521358986.688904, Test MSE of 424521360603.404236
Epoch 2: training loss 427876205748.706
Test Loss of 424521974923.517944, Test MSE of 424521982314.819214
Epoch 3: training loss 427875712301.176
Test Loss of 424522249559.228333, Test MSE of 424522250783.547607
Epoch 4: training loss 427875337637.647
Test Loss of 424522536095.415222, Test MSE of 424522544779.145813
Epoch 5: training loss 420657342584.471
Test Loss of 402212238394.981262, Test MSE of 402212241996.161316
Epoch 6: training loss 374312407883.294
Test Loss of 336390001319.350464, Test MSE of 336390002158.882507
Epoch 7: training loss 295088867809.882
Test Loss of 251584636257.650696, Test MSE of 251584632820.468903
Epoch 8: training loss 215948949202.824
Test Loss of 183725097172.237793, Test MSE of 183725100293.017914
Epoch 9: training loss 156442443836.235
Test Loss of 136296670125.805222, Test MSE of 136296671014.449051
Epoch 10: training loss 136491977095.529
Test Loss of 129595814861.072403, Test MSE of 129595816613.121704
Epoch 11: training loss 132749315252.706
Test Loss of 126776673893.263016, Test MSE of 126776671694.675369
Epoch 12: training loss 130754127329.882
Test Loss of 124094485068.391388, Test MSE of 124094484096.443604
Epoch 13: training loss 126819769615.059
Test Loss of 120775971330.487167, Test MSE of 120775968297.696182
Epoch 14: training loss 122649365775.059
Test Loss of 116200862064.573669, Test MSE of 116200861685.487686
Epoch 15: training loss 118495366896.941
Test Loss of 115172353301.851486, Test MSE of 115172350089.049820
Epoch 16: training loss 114939465095.529
Test Loss of 111278866407.365250, Test MSE of 111278864455.578995
Epoch 17: training loss 109616973944.471
Test Loss of 106797080303.122833, Test MSE of 106797078407.154587
Epoch 18: training loss 107829332690.824
Test Loss of 102441522103.280136, Test MSE of 102441524036.016342
Epoch 19: training loss 102380619776.000
Test Loss of 99109999146.281754, Test MSE of 99109999536.282272
Epoch 20: training loss 98963889182.118
Test Loss of 95826147942.447372, Test MSE of 95826149812.780365
Epoch 21: training loss 94669207491.765
Test Loss of 91594933325.931061, Test MSE of 91594934115.065521
Epoch 22: training loss 90934437827.765
Test Loss of 89463776071.476288, Test MSE of 89463774924.439011
Epoch 23: training loss 86840887175.529
Test Loss of 83379846937.759888, Test MSE of 83379844988.021347
Epoch 24: training loss 83374687638.588
Test Loss of 81013976701.424011, Test MSE of 81013975055.503357
Epoch 25: training loss 79131520828.235
Test Loss of 77008071618.176270, Test MSE of 77008072075.798264
Epoch 26: training loss 76653044525.176
Test Loss of 73708450976.362717, Test MSE of 73708451272.012115
Epoch 27: training loss 71615393671.529
Test Loss of 67905683647.393013, Test MSE of 67905685173.520271
Epoch 28: training loss 69559275730.824
Test Loss of 67546585363.245895, Test MSE of 67546587346.092705
Epoch 29: training loss 65113745483.294
Test Loss of 62768766337.391624, Test MSE of 62768765904.143562
Epoch 30: training loss 61985328399.059
Test Loss of 60363464451.967613, Test MSE of 60363465903.853287
Epoch 31: training loss 59066511721.412
Test Loss of 57860958976.888275, Test MSE of 57860958547.385193
Epoch 32: training loss 56370223405.176
Test Loss of 54830428535.679855, Test MSE of 54830428596.631233
Epoch 33: training loss 52361179256.471
Test Loss of 52731127755.414299, Test MSE of 52731128899.011063
Epoch 34: training loss 50320686614.588
Test Loss of 52365183201.502663, Test MSE of 52365184184.087662
Epoch 35: training loss 47651200941.176
Test Loss of 45109509043.963913, Test MSE of 45109509647.527367
Epoch 36: training loss 44662988732.235
Test Loss of 44720876591.611382, Test MSE of 44720875952.862053
Epoch 37: training loss 42638190102.588
Test Loss of 46157744745.289848, Test MSE of 46157746206.298546
Epoch 38: training loss 40342636897.882
Test Loss of 44958964464.307198, Test MSE of 44958962823.010338
Epoch 39: training loss 37909557940.706
Test Loss of 39356767253.081657, Test MSE of 39356767740.228706
Epoch 40: training loss 36066710746.353
Test Loss of 38619031741.024292, Test MSE of 38619030293.367775
Epoch 41: training loss 33651260009.412
Test Loss of 36918235294.230858, Test MSE of 36918235061.749092
Epoch 42: training loss 31620198784.000
Test Loss of 37652785400.242424, Test MSE of 37652785791.862228
Epoch 43: training loss 30715334512.941
Test Loss of 36215757182.786026, Test MSE of 36215757121.790596
Epoch 44: training loss 28303472256.000
Test Loss of 33066392034.272495, Test MSE of 33066391557.575020
Epoch 45: training loss 27578201645.176
Test Loss of 33112887484.076797, Test MSE of 33112887273.928753
Epoch 46: training loss 25574918957.176
Test Loss of 33494516888.309044, Test MSE of 33494517722.234848
Epoch 47: training loss 24326123211.294
Test Loss of 30389619776.192459, Test MSE of 30389618877.716595
Epoch 48: training loss 23247099651.765
Test Loss of 28554771459.553089, Test MSE of 28554771696.464314
Epoch 49: training loss 22473004058.353
Test Loss of 30291213271.021049, Test MSE of 30291213163.710026
Epoch 50: training loss 21148081200.941
Test Loss of 31785102989.057598, Test MSE of 31785103345.256924
Epoch 51: training loss 20045545652.706
Test Loss of 30098039201.843166, Test MSE of 30098040340.137081
Epoch 52: training loss 19188189214.118
Test Loss of 26563746099.934303, Test MSE of 26563745491.574520
Epoch 53: training loss 18102262083.765
Test Loss of 27527000458.155910, Test MSE of 27527000293.025761
Epoch 54: training loss 17581751431.529
Test Loss of 30273880531.586399, Test MSE of 30273881192.580708
Epoch 55: training loss 16991410477.176
Test Loss of 25108362008.575527, Test MSE of 25108362922.290092
Epoch 56: training loss 16516956329.412
Test Loss of 24077414183.024750, Test MSE of 24077414283.549805
Epoch 57: training loss 15798216737.882
Test Loss of 30871502387.046032, Test MSE of 30871501159.664318
Epoch 58: training loss 15471752380.235
Test Loss of 27060003003.839928, Test MSE of 27060002450.664211
Epoch 59: training loss 14831173492.706
Test Loss of 27232035645.053898, Test MSE of 27232035774.796993
Epoch 60: training loss 14484933150.118
Test Loss of 27184889457.106640, Test MSE of 27184890244.441628
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23929631393.3319, 'MSE - std': 3255258851.109728, 'R2 - mean': 0.8224621751679924, 'R2 - std': 0.016544242693365685} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005692 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421925752229.647
Test Loss of 447256732855.102478, Test MSE of 447256741034.405029
Epoch 2: training loss 421903260611.765
Test Loss of 447237036111.115417, Test MSE of 447237031397.136292
Epoch 3: training loss 421875066880.000
Test Loss of 447212092803.286621, Test MSE of 447212091601.477234
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895796856.471
Test Loss of 447219510362.722168, Test MSE of 447219521344.745422
Epoch 2: training loss 421885173519.059
Test Loss of 447221190164.252625, Test MSE of 447221192252.847473
Epoch 3: training loss 421884664169.412
Test Loss of 447220837697.909790, Test MSE of 447220839792.044189
Epoch 4: training loss 421884293481.412
Test Loss of 447220747451.366211, Test MSE of 447220743761.858459
Epoch 5: training loss 414802153110.588
Test Loss of 424720715928.545898, Test MSE of 424720711426.069092
Epoch 6: training loss 368820555776.000
Test Loss of 356263520267.843628, Test MSE of 356263527102.227661
Epoch 7: training loss 289365800719.059
Test Loss of 269908887016.194305, Test MSE of 269908891926.854431
Epoch 8: training loss 211798264169.412
Test Loss of 197853820220.698578, Test MSE of 197853821245.736938
Epoch 9: training loss 153933328504.471
Test Loss of 148587086331.854736, Test MSE of 148587087133.124817
Epoch 10: training loss 133639168150.588
Test Loss of 139921812362.274353, Test MSE of 139921812380.325867
Epoch 11: training loss 130276144037.647
Test Loss of 136246359493.137177, Test MSE of 136246358225.862106
Epoch 12: training loss 128674000143.059
Test Loss of 133553289600.207260, Test MSE of 133553289175.450211
Epoch 13: training loss 123697504677.647
Test Loss of 128615781842.402039, Test MSE of 128615784148.570755
Epoch 14: training loss 119450315264.000
Test Loss of 126275028376.605133, Test MSE of 126275025324.893753
Epoch 15: training loss 116604882281.412
Test Loss of 121582616810.977554, Test MSE of 121582615568.008408
Epoch 16: training loss 113216594672.941
Test Loss of 119228934136.893829, Test MSE of 119228934228.875137
Epoch 17: training loss 108742282330.353
Test Loss of 113603008431.937088, Test MSE of 113603010335.825150
Epoch 18: training loss 105578880903.529
Test Loss of 110932037632.947495, Test MSE of 110932037490.257492
Epoch 19: training loss 100421216707.765
Test Loss of 108761329776.751328, Test MSE of 108761329941.481262
Epoch 20: training loss 96395380585.412
Test Loss of 102680350315.421692, Test MSE of 102680349244.745331
Epoch 21: training loss 93285903028.706
Test Loss of 100642030094.804535, Test MSE of 100642031029.668549
Epoch 22: training loss 89325885093.647
Test Loss of 97905417216.710617, Test MSE of 97905416339.658249
Epoch 23: training loss 86147752207.059
Test Loss of 92838533254.069855, Test MSE of 92838534078.202881
Epoch 24: training loss 82025557714.824
Test Loss of 87415361953.843170, Test MSE of 87415361477.624451
Epoch 25: training loss 78244084495.059
Test Loss of 84913254266.403885, Test MSE of 84913255136.090424
Epoch 26: training loss 74778245074.824
Test Loss of 80499078168.871613, Test MSE of 80499080099.848251
Epoch 27: training loss 72013680775.529
Test Loss of 73814710858.970154, Test MSE of 73814711328.139267
Epoch 28: training loss 68280565368.471
Test Loss of 77544334035.882492, Test MSE of 77544332661.506561
Epoch 29: training loss 64173476713.412
Test Loss of 72369117031.927826, Test MSE of 72369116786.069733
Epoch 30: training loss 61524546017.882
Test Loss of 68919649080.316452, Test MSE of 68919649472.733093
Epoch 31: training loss 58000742279.529
Test Loss of 63119645717.555405, Test MSE of 63119646740.004761
Epoch 32: training loss 55372711152.941
Test Loss of 60775407534.515846, Test MSE of 60775407267.650887
Epoch 33: training loss 51461385630.118
Test Loss of 59604285636.604210, Test MSE of 59604286424.994301
Epoch 34: training loss 50129290872.471
Test Loss of 52570492747.029381, Test MSE of 52570492893.147453
Epoch 35: training loss 46820396491.294
Test Loss of 52835152915.186676, Test MSE of 52835153719.589958
Epoch 36: training loss 44797813842.824
Test Loss of 53560954909.372192, Test MSE of 53560954183.936310
Epoch 37: training loss 42571651215.059
Test Loss of 45986855917.760811, Test MSE of 45986855861.551582
Epoch 38: training loss 40096226612.706
Test Loss of 48284814101.733055, Test MSE of 48284814362.366135
Epoch 39: training loss 38101486599.529
Test Loss of 43103636718.530647, Test MSE of 43103636639.218048
Epoch 40: training loss 36013389650.824
Test Loss of 42232108905.822807, Test MSE of 42232108916.491585
Epoch 41: training loss 34228998422.588
Test Loss of 38442287195.906548, Test MSE of 38442287062.642685
Epoch 42: training loss 32423459937.882
Test Loss of 41345920238.530647, Test MSE of 41345920921.467438
Epoch 43: training loss 30798013108.706
Test Loss of 38167194070.428871, Test MSE of 38167194278.931725
Epoch 44: training loss 29205479883.294
Test Loss of 36064456130.768448, Test MSE of 36064455576.827362
Epoch 45: training loss 28446241716.706
Test Loss of 33787962181.581310, Test MSE of 33787962621.379692
Epoch 46: training loss 26242441505.882
Test Loss of 31323345803.695583, Test MSE of 31323346312.593842
Epoch 47: training loss 25446665592.471
Test Loss of 32884031067.314365, Test MSE of 32884030610.796055
Epoch 48: training loss 23954344176.941
Test Loss of 29964929333.355541, Test MSE of 29964929851.341003
Epoch 49: training loss 22836594586.353
Test Loss of 25199696879.182049, Test MSE of 25199696489.900677
Epoch 50: training loss 22224540069.647
Test Loss of 28350527220.334026, Test MSE of 28350526779.315739
Epoch 51: training loss 21359352722.824
Test Loss of 30970521482.984962, Test MSE of 30970522425.595924
Epoch 52: training loss 20327180461.176
Test Loss of 28602486032.640297, Test MSE of 28602485934.709625
Epoch 53: training loss 19261384568.471
Test Loss of 32074565516.879944, Test MSE of 32074565054.831184
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26644609280.498333, 'MSE - std': 4669762661.430541, 'R2 - mean': 0.8104686958334354, 'R2 - std': 0.02168321326037449} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005592 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430107462716.235
Test Loss of 410762548047.252197, Test MSE of 410762537250.491211
Epoch 2: training loss 430084714736.941
Test Loss of 410743613344.281372, Test MSE of 410743620604.683350
Epoch 3: training loss 430058299873.882
Test Loss of 410719874391.544678, Test MSE of 410719873427.847717
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076586586.353
Test Loss of 410722873505.110596, Test MSE of 410722873503.652100
Epoch 2: training loss 430062256128.000
Test Loss of 410724074002.717285, Test MSE of 410724077216.113281
Epoch 3: training loss 430061850864.941
Test Loss of 410724465626.565491, Test MSE of 410724461614.479248
Epoch 4: training loss 430061623657.412
Test Loss of 410724908957.438232, Test MSE of 410724910659.148621
Epoch 5: training loss 422749873573.647
Test Loss of 388093987702.108276, Test MSE of 388093986143.103455
Epoch 6: training loss 376220479006.118
Test Loss of 320864092508.757080, Test MSE of 320864095421.166382
Epoch 7: training loss 296970528527.059
Test Loss of 234875147200.977325, Test MSE of 234875145083.103241
Epoch 8: training loss 219589719762.824
Test Loss of 166995878824.336884, Test MSE of 166995878759.902374
Epoch 9: training loss 160841304847.059
Test Loss of 120364658909.763992, Test MSE of 120364659049.286804
Epoch 10: training loss 141682947644.235
Test Loss of 112783613303.766769, Test MSE of 112783614891.780197
Epoch 11: training loss 138587872677.647
Test Loss of 109772340277.545578, Test MSE of 109772340000.947815
Epoch 12: training loss 135324096752.941
Test Loss of 107315364093.986115, Test MSE of 107315364141.681335
Epoch 13: training loss 132427386398.118
Test Loss of 104526248289.495605, Test MSE of 104526247212.476028
Epoch 14: training loss 127451482413.176
Test Loss of 101334551884.172150, Test MSE of 101334553696.671509
Epoch 15: training loss 124365231706.353
Test Loss of 98214334833.606659, Test MSE of 98214335092.991837
Epoch 16: training loss 120745935299.765
Test Loss of 95638005580.882919, Test MSE of 95638004843.097321
Epoch 17: training loss 117185923312.941
Test Loss of 92711030600.144379, Test MSE of 92711032143.114243
Epoch 18: training loss 112658765793.882
Test Loss of 88402797863.211472, Test MSE of 88402797771.607666
Epoch 19: training loss 109775203749.647
Test Loss of 85576242526.178619, Test MSE of 85576244176.229645
Epoch 20: training loss 104120045447.529
Test Loss of 85147331135.259598, Test MSE of 85147329912.282822
Epoch 21: training loss 100595297566.118
Test Loss of 80465022045.349380, Test MSE of 80465023727.005157
Epoch 22: training loss 98056866816.000
Test Loss of 76952467544.136978, Test MSE of 76952468851.251190
Epoch 23: training loss 93717845549.176
Test Loss of 74749579463.018967, Test MSE of 74749579394.843048
Epoch 24: training loss 90231576681.412
Test Loss of 72143366927.755676, Test MSE of 72143367335.299973
Epoch 25: training loss 85631468077.176
Test Loss of 68844740350.223038, Test MSE of 68844740489.058105
Epoch 26: training loss 82986535845.647
Test Loss of 65508521613.445625, Test MSE of 65508520850.970802
Epoch 27: training loss 78414180442.353
Test Loss of 60821255230.074966, Test MSE of 60821253896.996269
Epoch 28: training loss 75389918840.471
Test Loss of 60651734951.863029, Test MSE of 60651734631.523819
Epoch 29: training loss 72111702648.471
Test Loss of 58437605873.547432, Test MSE of 58437605404.200020
Epoch 30: training loss 69020067297.882
Test Loss of 53114338664.603424, Test MSE of 53114338446.176720
Epoch 31: training loss 65418319360.000
Test Loss of 53105615369.240166, Test MSE of 53105615650.365387
Epoch 32: training loss 63607480696.471
Test Loss of 51900595397.597412, Test MSE of 51900595196.630135
Epoch 33: training loss 59901212137.412
Test Loss of 47928496129.895416, Test MSE of 47928497151.206100
Epoch 34: training loss 56479486456.471
Test Loss of 46610601890.176773, Test MSE of 46610601401.023727
Epoch 35: training loss 53938149910.588
Test Loss of 45298173456.347984, Test MSE of 45298172577.175903
Epoch 36: training loss 51436454309.647
Test Loss of 42640109681.725128, Test MSE of 42640109634.782478
Epoch 37: training loss 48503056120.471
Test Loss of 38837378437.034706, Test MSE of 38837378188.922493
Epoch 38: training loss 46790047239.529
Test Loss of 38767129711.355850, Test MSE of 38767130366.897728
Epoch 39: training loss 44102375107.765
Test Loss of 36332517615.770477, Test MSE of 36332516847.027473
Epoch 40: training loss 41751282266.353
Test Loss of 33394988645.641834, Test MSE of 33394988854.791893
Epoch 41: training loss 39650637206.588
Test Loss of 32810360790.300785, Test MSE of 32810360817.923084
Epoch 42: training loss 37894037805.176
Test Loss of 30425242361.010643, Test MSE of 30425243211.312813
Epoch 43: training loss 35825897042.824
Test Loss of 29547445030.500694, Test MSE of 29547444929.178288
Epoch 44: training loss 34574481321.412
Test Loss of 28306794659.953724, Test MSE of 28306794277.009293
Epoch 45: training loss 32335831461.647
Test Loss of 26771553174.330402, Test MSE of 26771553924.391853
Epoch 46: training loss 31104812431.059
Test Loss of 26788037928.159187, Test MSE of 26788037710.340416
Epoch 47: training loss 29424377046.588
Test Loss of 24507301253.982414, Test MSE of 24507301388.337711
Epoch 48: training loss 27908765515.294
Test Loss of 27299800018.983803, Test MSE of 27299800577.824150
Epoch 49: training loss 26981009317.647
Test Loss of 24008294846.371124, Test MSE of 24008295363.753189
Epoch 50: training loss 25612913690.353
Test Loss of 21391407029.130959, Test MSE of 21391407160.478710
Epoch 51: training loss 24603386895.059
Test Loss of 23015191056.821842, Test MSE of 23015190771.646942
Epoch 52: training loss 23758135751.529
Test Loss of 24442726601.862103, Test MSE of 24442726395.375431
Epoch 53: training loss 22793909711.059
Test Loss of 23084993486.245258, Test MSE of 23084993425.357155
Epoch 54: training loss 22021673788.235
Test Loss of 22680035550.711708, Test MSE of 22680035894.457466
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25653465933.988113, 'MSE - std': 4393416425.239308, 'R2 - mean': 0.8110539828290715, 'R2 - std': 0.018805557305566413} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005465 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043497472.000
Test Loss of 431612901278.385925, Test MSE of 431612903086.931946
Epoch 2: training loss 424024272173.176
Test Loss of 431592923928.285034, Test MSE of 431592918122.394165
Epoch 3: training loss 423997910919.529
Test Loss of 431566204679.700134, Test MSE of 431566202867.316040
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424013495958.588
Test Loss of 431566576060.475708, Test MSE of 431566576623.249756
Epoch 2: training loss 424000368880.941
Test Loss of 431569348057.380859, Test MSE of 431569349682.863464
Epoch 3: training loss 423999740626.824
Test Loss of 431569311604.686707, Test MSE of 431569313705.201965
Epoch 4: training loss 423999339941.647
Test Loss of 431568956524.512695, Test MSE of 431568956547.901733
Epoch 5: training loss 417409020144.941
Test Loss of 409901795827.442871, Test MSE of 409901793603.477173
Epoch 6: training loss 372675803015.529
Test Loss of 342187342052.397949, Test MSE of 342187342068.259155
Epoch 7: training loss 294310742136.471
Test Loss of 255936330183.848206, Test MSE of 255936328479.162659
Epoch 8: training loss 216989728768.000
Test Loss of 184145018463.007874, Test MSE of 184145021189.778259
Epoch 9: training loss 157626502776.471
Test Loss of 132572489215.289215, Test MSE of 132572491298.484497
Epoch 10: training loss 138277613507.765
Test Loss of 123031439305.980560, Test MSE of 123031442428.632553
Epoch 11: training loss 134669909142.588
Test Loss of 121048765668.871826, Test MSE of 121048762537.285522
Epoch 12: training loss 131982943201.882
Test Loss of 118336415098.609909, Test MSE of 118336412787.785858
Epoch 13: training loss 127150363919.059
Test Loss of 113917691884.098099, Test MSE of 113917691563.541992
Epoch 14: training loss 123008912399.059
Test Loss of 111525735297.480789, Test MSE of 111525735593.488815
Epoch 15: training loss 118893464455.529
Test Loss of 106382355022.422958, Test MSE of 106382357339.177032
Epoch 16: training loss 116624996186.353
Test Loss of 102940259177.788055, Test MSE of 102940259635.344315
Epoch 17: training loss 112846015638.588
Test Loss of 99736461406.770935, Test MSE of 99736461858.563507
Epoch 18: training loss 108737772227.765
Test Loss of 94563042549.930588, Test MSE of 94563043232.406845
Epoch 19: training loss 104701108555.294
Test Loss of 93256106226.613602, Test MSE of 93256106907.026733
Epoch 20: training loss 101095084197.647
Test Loss of 87540220673.540024, Test MSE of 87540220812.657715
Epoch 21: training loss 98071504775.529
Test Loss of 86627604560.555298, Test MSE of 86627606347.712036
Epoch 22: training loss 93526809419.294
Test Loss of 83812314463.126328, Test MSE of 83812315781.146439
Epoch 23: training loss 90032517639.529
Test Loss of 76824168397.771408, Test MSE of 76824168247.477554
Epoch 24: training loss 86867754706.824
Test Loss of 76857872251.320679, Test MSE of 76857871498.047699
Epoch 25: training loss 83322327717.647
Test Loss of 69046001980.534943, Test MSE of 69046000746.570663
Epoch 26: training loss 80068750607.059
Test Loss of 68919346397.290146, Test MSE of 68919346146.433884
Epoch 27: training loss 75242673106.824
Test Loss of 64334447861.930588, Test MSE of 64334448424.045898
Epoch 28: training loss 72348505840.941
Test Loss of 60915690193.206848, Test MSE of 60915689155.755249
Epoch 29: training loss 68873193667.765
Test Loss of 61987210821.419716, Test MSE of 61987211089.149269
Epoch 30: training loss 65895055917.176
Test Loss of 56391418824.558998, Test MSE of 56391419893.952759
Epoch 31: training loss 62636959593.412
Test Loss of 52190955270.752426, Test MSE of 52190955111.172592
Epoch 32: training loss 60262698586.353
Test Loss of 52806489684.583061, Test MSE of 52806490137.453232
Epoch 33: training loss 57352331760.941
Test Loss of 50928689000.366493, Test MSE of 50928688418.541298
Epoch 34: training loss 54618019696.941
Test Loss of 41601541384.884773, Test MSE of 41601540621.170578
Epoch 35: training loss 51775503691.294
Test Loss of 44644641441.821381, Test MSE of 44644641830.971550
Epoch 36: training loss 49169750881.882
Test Loss of 41736747149.208702, Test MSE of 41736748413.789360
Epoch 37: training loss 46848584199.529
Test Loss of 40038761769.106895, Test MSE of 40038762475.516098
Epoch 38: training loss 44667494821.647
Test Loss of 38150744384.325775, Test MSE of 38150744676.276955
Epoch 39: training loss 41720371117.176
Test Loss of 37457461309.601112, Test MSE of 37457462232.060028
Epoch 40: training loss 40047730108.235
Test Loss of 34503280493.105042, Test MSE of 34503280521.113045
Epoch 41: training loss 38439299516.235
Test Loss of 32865022781.245720, Test MSE of 32865023211.409431
Epoch 42: training loss 35912179727.059
Test Loss of 31941724127.777882, Test MSE of 31941723822.974091
Epoch 43: training loss 34870341097.412
Test Loss of 29388149956.649700, Test MSE of 29388150234.997463
Epoch 44: training loss 33435201419.294
Test Loss of 27408963543.248497, Test MSE of 27408964221.397564
Epoch 45: training loss 31430753754.353
Test Loss of 27218241394.791302, Test MSE of 27218241574.469154
Epoch 46: training loss 29827054486.588
Test Loss of 28486426657.169830, Test MSE of 28486425772.569355
Epoch 47: training loss 28400737377.882
Test Loss of 27329580538.550671, Test MSE of 27329580276.078228
Epoch 48: training loss 27057151224.471
Test Loss of 23975429287.270706, Test MSE of 23975429452.073524
Epoch 49: training loss 26407645899.294
Test Loss of 27820086683.779732, Test MSE of 27820086815.056194
Epoch 50: training loss 25098897136.941
Test Loss of 24727487416.447941, Test MSE of 24727487321.560707
Epoch 51: training loss 24172670614.588
Test Loss of 22995156937.980564, Test MSE of 22995157324.559296
Epoch 52: training loss 22893004502.588
Test Loss of 22524474376.055531, Test MSE of 22524474556.392677
Epoch 53: training loss 22218635339.294
Test Loss of 22198139063.855621, Test MSE of 22198139304.792213
Epoch 54: training loss 21153910671.059
Test Loss of 22214618787.242943, Test MSE of 22214618774.011169
Epoch 55: training loss 20581651508.706
Test Loss of 23124124506.624710, Test MSE of 23124124585.100887
Epoch 56: training loss 20084330669.176
Test Loss of 22778218408.810738, Test MSE of 22778218973.167274
Epoch 57: training loss 19147823977.412
Test Loss of 22398294699.298473, Test MSE of 22398294404.104313
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25002431628.011353, 'MSE - std': 4139694309.811366, 'R2 - mean': 0.8153889274239263, 'R2 - std': 0.01892316482568913} 
 

Saving model.....
Results After CV: {'MSE - mean': 25002431628.011353, 'MSE - std': 4139694309.811366, 'R2 - mean': 0.8153889274239263, 'R2 - std': 0.01892316482568913}
Train time: 87.576858162399
Inference time: 0.07052613759951783
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 60 finished with value: 25002431628.011353 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005643 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525146262.588
Test Loss of 418111403665.795044, Test MSE of 418111399408.094604
Epoch 2: training loss 427504969125.647
Test Loss of 418093639559.195007, Test MSE of 418093643361.759094
Epoch 3: training loss 427477968655.059
Test Loss of 418070257260.369202, Test MSE of 418070255081.890442
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493798731.294
Test Loss of 418077983875.937988, Test MSE of 418077985212.954285
Epoch 2: training loss 427483046369.882
Test Loss of 418078576413.549866, Test MSE of 418078577217.226746
Epoch 3: training loss 427482559668.706
Test Loss of 418077618826.925720, Test MSE of 418077620202.637390
Epoch 4: training loss 427482187655.529
Test Loss of 418076756209.136230, Test MSE of 418076761068.703491
Epoch 5: training loss 420887414543.059
Test Loss of 396916190150.913696, Test MSE of 396916190213.156494
Epoch 6: training loss 376151511280.941
Test Loss of 330291011536.625488, Test MSE of 330291015604.791016
Epoch 7: training loss 297936087642.353
Test Loss of 244901265889.325012, Test MSE of 244901270518.360413
Epoch 8: training loss 218951341658.353
Test Loss of 175316634864.425629, Test MSE of 175316637484.560303
Epoch 9: training loss 158316332769.882
Test Loss of 126132919135.637283, Test MSE of 126132918027.667816
Epoch 10: training loss 139990230528.000
Test Loss of 118116386560.651398, Test MSE of 118116387001.200195
Epoch 11: training loss 134745089264.941
Test Loss of 115143340355.094147, Test MSE of 115143342824.997330
Epoch 12: training loss 132528644246.588
Test Loss of 112647570687.111725, Test MSE of 112647570710.996170
Epoch 13: training loss 129169306503.529
Test Loss of 109258898857.186218, Test MSE of 109258900333.435226
Epoch 14: training loss 125378832308.706
Test Loss of 106162527045.107559, Test MSE of 106162528237.006638
Epoch 15: training loss 121144258349.176
Test Loss of 102178022812.395096, Test MSE of 102178022823.951416
Epoch 16: training loss 117524948028.235
Test Loss of 98470323659.769608, Test MSE of 98470323093.316299
Epoch 17: training loss 113825341786.353
Test Loss of 96295112898.235489, Test MSE of 96295112347.409180
Epoch 18: training loss 109208082085.647
Test Loss of 92368784518.306732, Test MSE of 92368783010.992874
Epoch 19: training loss 105565349345.882
Test Loss of 89147054799.618790, Test MSE of 89147054590.284271
Epoch 20: training loss 101618207924.706
Test Loss of 86940632381.646072, Test MSE of 86940633066.061996
Epoch 21: training loss 98504514770.824
Test Loss of 83817502838.910019, Test MSE of 83817501649.486862
Epoch 22: training loss 94305273193.412
Test Loss of 76791522221.805222, Test MSE of 76791521590.867981
Epoch 23: training loss 91035336154.353
Test Loss of 74586158500.922501, Test MSE of 74586159167.721222
Epoch 24: training loss 86521818081.882
Test Loss of 72912702696.845703, Test MSE of 72912703358.040466
Epoch 25: training loss 83348373082.353
Test Loss of 68608700151.650246, Test MSE of 68608700627.021858
Epoch 26: training loss 79616059617.882
Test Loss of 63776437769.356468, Test MSE of 63776437667.859505
Epoch 27: training loss 76620784158.118
Test Loss of 64806889984.118439, Test MSE of 64806889903.742310
Epoch 28: training loss 73087032357.647
Test Loss of 60875146872.212814, Test MSE of 60875147338.734367
Epoch 29: training loss 68835916047.059
Test Loss of 56755663377.647003, Test MSE of 56755662834.494202
Epoch 30: training loss 65691763094.588
Test Loss of 55505720169.585938, Test MSE of 55505720851.881981
Epoch 31: training loss 62437811862.588
Test Loss of 51309891480.723572, Test MSE of 51309891368.778473
Epoch 32: training loss 58974922352.941
Test Loss of 48282473079.975945, Test MSE of 48282472605.537117
Epoch 33: training loss 56414854686.118
Test Loss of 46618590478.745316, Test MSE of 46618590406.279045
Epoch 34: training loss 53896195873.882
Test Loss of 44579864424.401573, Test MSE of 44579865318.587471
Epoch 35: training loss 51482820615.529
Test Loss of 41170604628.918808, Test MSE of 41170604513.412865
Epoch 36: training loss 48729289908.706
Test Loss of 41234409986.960907, Test MSE of 41234410026.438904
Epoch 37: training loss 46679540638.118
Test Loss of 37684820083.120056, Test MSE of 37684820496.052773
Epoch 38: training loss 44228378477.176
Test Loss of 35870611897.056671, Test MSE of 35870611977.895088
Epoch 39: training loss 41448654913.882
Test Loss of 33609764051.053436, Test MSE of 33609764632.870922
Epoch 40: training loss 39738246475.294
Test Loss of 33987610804.496876, Test MSE of 33987611323.476261
Epoch 41: training loss 37826117150.118
Test Loss of 31072104737.931992, Test MSE of 31072104059.791550
Epoch 42: training loss 35802936312.471
Test Loss of 30263236382.023594, Test MSE of 30263237165.954727
Epoch 43: training loss 33816516193.882
Test Loss of 28438622687.430027, Test MSE of 28438622541.455826
Epoch 44: training loss 32573243335.529
Test Loss of 29905790698.859127, Test MSE of 29905790845.410362
Epoch 45: training loss 30862350516.706
Test Loss of 24678195723.962063, Test MSE of 24678196086.640732
Epoch 46: training loss 29309204958.118
Test Loss of 26016601831.069164, Test MSE of 26016601809.455269
Epoch 47: training loss 28116201027.765
Test Loss of 24086358710.984039, Test MSE of 24086358936.123646
Epoch 48: training loss 26972834992.941
Test Loss of 22204024934.328938, Test MSE of 22204025071.248425
Epoch 49: training loss 25472023066.353
Test Loss of 25288795995.610455, Test MSE of 25288796378.197945
Epoch 50: training loss 24212316077.176
Test Loss of 22744383966.482536, Test MSE of 22744384151.851398
Epoch 51: training loss 23565468397.176
Test Loss of 20991034001.084431, Test MSE of 20991034389.455708
Epoch 52: training loss 22434989910.588
Test Loss of 22791069022.334492, Test MSE of 22791068973.493610
Epoch 53: training loss 21548888030.118
Test Loss of 21097744874.799908, Test MSE of 21097744654.142269
Epoch 54: training loss 21240764355.765
Test Loss of 21165897534.712006, Test MSE of 21165897774.769230
Epoch 55: training loss 19977823491.765
Test Loss of 20916571978.555634, Test MSE of 20916572049.972927
Epoch 56: training loss 19387476980.706
Test Loss of 20282296288.259079, Test MSE of 20282296194.838726
Epoch 57: training loss 18897462524.235
Test Loss of 19449155962.048576, Test MSE of 19449156087.144787
Epoch 58: training loss 18105045978.353
Test Loss of 20014517912.901226, Test MSE of 20014517946.585011
Epoch 59: training loss 17588653726.118
Test Loss of 19115632380.387695, Test MSE of 19115632622.499485
Epoch 60: training loss 17032462166.588
Test Loss of 19890754255.145039, Test MSE of 19890754337.266014
Epoch 61: training loss 16319371410.824
Test Loss of 20947760523.814018, Test MSE of 20947760337.287487
Epoch 62: training loss 15977283279.059
Test Loss of 18071472446.356697, Test MSE of 18071472291.936447
Epoch 63: training loss 15431477842.824
Test Loss of 18105704949.696045, Test MSE of 18105705229.134274
Epoch 64: training loss 14933984365.176
Test Loss of 18136684287.230164, Test MSE of 18136683958.982391
Epoch 65: training loss 14417875418.353
Test Loss of 16815726643.164469, Test MSE of 16815727344.932661
Epoch 66: training loss 14412665159.529
Test Loss of 20361543719.794586, Test MSE of 20361543592.076973
Epoch 67: training loss 14088075392.000
Test Loss of 18112297660.195236, Test MSE of 18112297742.031532
Epoch 68: training loss 13701101831.529
Test Loss of 19509282533.174183, Test MSE of 19509282716.307793
Epoch 69: training loss 13105513833.412
Test Loss of 19116350101.348137, Test MSE of 19116350418.619213
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19116350418.619213, 'MSE - std': 0.0, 'R2 - mean': 0.8511389051819688, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918409487.059
Test Loss of 424556564909.213074, Test MSE of 424556561119.368896
Epoch 2: training loss 427897866842.353
Test Loss of 424540198206.593567, Test MSE of 424540201941.663513
Epoch 3: training loss 427869826108.235
Test Loss of 424518164466.735107, Test MSE of 424518165613.059692
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885251644.235
Test Loss of 424527644386.331726, Test MSE of 424527639528.053223
Epoch 2: training loss 427877472135.529
Test Loss of 424528214229.659058, Test MSE of 424528216717.206238
Epoch 3: training loss 427876950738.824
Test Loss of 424527688228.596802, Test MSE of 424527682237.921448
Epoch 4: training loss 427876550776.471
Test Loss of 424527269316.663452, Test MSE of 424527274597.132996
Epoch 5: training loss 420897227715.765
Test Loss of 402811781294.338196, Test MSE of 402811784047.427673
Epoch 6: training loss 374776074480.941
Test Loss of 336381069579.665955, Test MSE of 336381074816.249023
Epoch 7: training loss 294585317978.353
Test Loss of 252226014838.791565, Test MSE of 252226020699.610352
Epoch 8: training loss 216147470275.765
Test Loss of 183897438626.316895, Test MSE of 183897436450.169464
Epoch 9: training loss 157505871751.529
Test Loss of 137439479787.628967, Test MSE of 137439478579.315826
Epoch 10: training loss 136926477884.235
Test Loss of 130122184595.512375, Test MSE of 130122187269.634811
Epoch 11: training loss 134068377389.176
Test Loss of 127380047529.482300, Test MSE of 127380049443.106491
Epoch 12: training loss 131229501891.765
Test Loss of 124351856289.428635, Test MSE of 124351854598.848282
Epoch 13: training loss 127939130247.529
Test Loss of 120753263244.820724, Test MSE of 120753261441.576477
Epoch 14: training loss 122510295582.118
Test Loss of 118342042664.978943, Test MSE of 118342043102.850159
Epoch 15: training loss 119703092193.882
Test Loss of 113967043142.706451, Test MSE of 113967044040.277145
Epoch 16: training loss 115996957304.471
Test Loss of 110301819398.277115, Test MSE of 110301820904.093582
Epoch 17: training loss 111929595873.882
Test Loss of 107555705077.399948, Test MSE of 107555705856.547699
Epoch 18: training loss 106778819674.353
Test Loss of 103549342251.229233, Test MSE of 103549341823.573792
Epoch 19: training loss 103454980035.765
Test Loss of 99246411513.545227, Test MSE of 99246409790.093781
Epoch 20: training loss 99762581865.412
Test Loss of 96834966927.130234, Test MSE of 96834966514.507294
Epoch 21: training loss 96251557647.059
Test Loss of 93523407861.814484, Test MSE of 93523407010.438965
Epoch 22: training loss 92307498044.235
Test Loss of 89542893768.867920, Test MSE of 89542891544.086334
Epoch 23: training loss 87053565349.647
Test Loss of 84591772650.444595, Test MSE of 84591771901.160568
Epoch 24: training loss 84214003260.235
Test Loss of 81513523524.515381, Test MSE of 81513522252.514511
Epoch 25: training loss 81161087382.588
Test Loss of 80261678388.171173, Test MSE of 80261678105.856979
Epoch 26: training loss 77588232342.588
Test Loss of 75537823895.124680, Test MSE of 75537826235.011414
Epoch 27: training loss 74025157601.882
Test Loss of 71826508740.071243, Test MSE of 71826507772.202469
Epoch 28: training loss 71199421876.706
Test Loss of 68204894810.130005, Test MSE of 68204895277.164154
Epoch 29: training loss 67468495872.000
Test Loss of 67394950057.067780, Test MSE of 67394949777.989082
Epoch 30: training loss 63977551962.353
Test Loss of 61967779575.176498, Test MSE of 61967778426.071663
Epoch 31: training loss 59488336278.588
Test Loss of 59117741392.595886, Test MSE of 59117742514.201927
Epoch 32: training loss 57177661168.941
Test Loss of 57055037831.787186, Test MSE of 57055035846.580643
Epoch 33: training loss 54564667888.941
Test Loss of 55977799423.230164, Test MSE of 55977799241.816368
Epoch 34: training loss 51490447616.000
Test Loss of 51525945104.284988, Test MSE of 51525943894.461197
Epoch 35: training loss 49130061108.706
Test Loss of 48238796041.534119, Test MSE of 48238795705.391891
Epoch 36: training loss 47041046279.529
Test Loss of 47763247244.702293, Test MSE of 47763247670.281761
Epoch 37: training loss 43743050872.471
Test Loss of 46784394355.356926, Test MSE of 46784395176.183670
Epoch 38: training loss 41424280621.176
Test Loss of 39803104674.080040, Test MSE of 39803105678.775581
Epoch 39: training loss 39058531606.588
Test Loss of 41991477959.565117, Test MSE of 41991477271.901497
Epoch 40: training loss 37415484822.588
Test Loss of 39227063826.357620, Test MSE of 39227063515.928001
Epoch 41: training loss 35331113374.118
Test Loss of 39034421022.497337, Test MSE of 39034420480.972229
Epoch 42: training loss 33222158305.882
Test Loss of 35766429217.991211, Test MSE of 35766430158.082031
Epoch 43: training loss 31022300611.765
Test Loss of 33164560725.570206, Test MSE of 33164561190.895741
Epoch 44: training loss 29720195764.706
Test Loss of 37020626625.643303, Test MSE of 37020628530.564636
Epoch 45: training loss 28065674262.588
Test Loss of 33301797474.183670, Test MSE of 33301797488.514580
Epoch 46: training loss 26732252438.588
Test Loss of 29163444191.785336, Test MSE of 29163444234.164791
Epoch 47: training loss 25499699282.824
Test Loss of 29846819809.917187, Test MSE of 29846819895.792149
Epoch 48: training loss 24020755870.118
Test Loss of 30229547048.031460, Test MSE of 30229548070.675072
Epoch 49: training loss 22735816075.294
Test Loss of 27968735899.506824, Test MSE of 27968736091.671448
Epoch 50: training loss 21773352832.000
Test Loss of 29620500677.551701, Test MSE of 29620500069.332752
Epoch 51: training loss 21325468408.471
Test Loss of 28109869644.154522, Test MSE of 28109868857.660774
Epoch 52: training loss 20121608722.824
Test Loss of 27260992122.581539, Test MSE of 27260991872.723267
Epoch 53: training loss 19180540355.765
Test Loss of 26769695736.420078, Test MSE of 26769695896.971592
Epoch 54: training loss 18462206934.588
Test Loss of 25700357312.814251, Test MSE of 25700356912.269821
Epoch 55: training loss 17884405790.118
Test Loss of 29182903874.205875, Test MSE of 29182904054.088242
Epoch 56: training loss 17378502358.588
Test Loss of 27444167638.310432, Test MSE of 27444167940.750793
Epoch 57: training loss 16532359032.471
Test Loss of 25001798150.040249, Test MSE of 25001798458.189957
Epoch 58: training loss 15755881946.353
Test Loss of 24710873874.416840, Test MSE of 24710873837.740288
Epoch 59: training loss 15756456282.353
Test Loss of 24756393377.843166, Test MSE of 24756393588.688526
Epoch 60: training loss 15094400937.412
Test Loss of 25103217181.016888, Test MSE of 25103217038.356647
Epoch 61: training loss 14415339346.824
Test Loss of 25282828457.363869, Test MSE of 25282828538.617512
Epoch 62: training loss 13793424854.588
Test Loss of 25684629340.084202, Test MSE of 25684628788.924068
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22400489603.77164, 'MSE - std': 3284139185.1524277, 'R2 - mean': 0.833883854191422, 'R2 - std': 0.01725505099054675} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005459 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927362198.588
Test Loss of 447257940942.967407, Test MSE of 447257941106.460449
Epoch 2: training loss 421907444314.353
Test Loss of 447239977787.632690, Test MSE of 447239983566.248474
Epoch 3: training loss 421880994273.882
Test Loss of 447215862464.932678, Test MSE of 447215865684.714355
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900806264.471
Test Loss of 447225539265.406433, Test MSE of 447225539171.527710
Epoch 2: training loss 421888571392.000
Test Loss of 447226025911.517029, Test MSE of 447226019190.339111
Epoch 3: training loss 421888146492.235
Test Loss of 447225401416.956726, Test MSE of 447225404713.833130
Epoch 4: training loss 421887777972.706
Test Loss of 447225160875.732605, Test MSE of 447225158086.317688
Epoch 5: training loss 414617421101.176
Test Loss of 424280644985.811707, Test MSE of 424280651868.937561
Epoch 6: training loss 368192721136.941
Test Loss of 356261587645.616455, Test MSE of 356261583322.933411
Epoch 7: training loss 288782695363.765
Test Loss of 269273588612.115662, Test MSE of 269273588355.864838
Epoch 8: training loss 210468965496.471
Test Loss of 197802033682.594482, Test MSE of 197802037160.293549
Epoch 9: training loss 153108084224.000
Test Loss of 148134153509.485077, Test MSE of 148134153568.417114
Epoch 10: training loss 135290679205.647
Test Loss of 140296564780.532043, Test MSE of 140296565371.815125
Epoch 11: training loss 130220910411.294
Test Loss of 136583548872.098083, Test MSE of 136583553891.281891
Epoch 12: training loss 128487352380.235
Test Loss of 134682172486.824890, Test MSE of 134682172430.431015
Epoch 13: training loss 125095539862.588
Test Loss of 128742539775.644684, Test MSE of 128742542296.695923
Epoch 14: training loss 120449899218.824
Test Loss of 126739921746.372421, Test MSE of 126739919211.910568
Epoch 15: training loss 116579217829.647
Test Loss of 123141873483.976868, Test MSE of 123141870088.351837
Epoch 16: training loss 112487442823.529
Test Loss of 118809882403.234787, Test MSE of 118809882277.088135
Epoch 17: training loss 109684207947.294
Test Loss of 114130077433.071472, Test MSE of 114130078276.776169
Epoch 18: training loss 106008629413.647
Test Loss of 112862290945.184357, Test MSE of 112862292327.069016
Epoch 19: training loss 102027129404.235
Test Loss of 107312554728.727280, Test MSE of 107312554342.123581
Epoch 20: training loss 97842191299.765
Test Loss of 104104083308.191528, Test MSE of 104104082987.051132
Epoch 21: training loss 95025632798.118
Test Loss of 101514986639.307892, Test MSE of 101514985160.380554
Epoch 22: training loss 90512042405.647
Test Loss of 95912923212.746704, Test MSE of 95912923308.833588
Epoch 23: training loss 86444961008.941
Test Loss of 90894672628.097153, Test MSE of 90894671053.408844
Epoch 24: training loss 83550498665.412
Test Loss of 90576643145.193619, Test MSE of 90576643861.934326
Epoch 25: training loss 79185008640.000
Test Loss of 84286696502.717560, Test MSE of 84286695715.264053
Epoch 26: training loss 76346464496.941
Test Loss of 83970601952.022202, Test MSE of 83970602305.555176
Epoch 27: training loss 72926251279.059
Test Loss of 76574990667.621552, Test MSE of 76574991737.939224
Epoch 28: training loss 70045009408.000
Test Loss of 71928812042.067078, Test MSE of 71928811507.930634
Epoch 29: training loss 66970062230.588
Test Loss of 74700801561.463791, Test MSE of 74700800739.352631
Epoch 30: training loss 63376566678.588
Test Loss of 67218229483.451309, Test MSE of 67218230555.020691
Epoch 31: training loss 60381778040.471
Test Loss of 59865393910.465881, Test MSE of 59865394132.176926
Epoch 32: training loss 56937493760.000
Test Loss of 61899986263.228317, Test MSE of 61899985324.237328
Epoch 33: training loss 53932418032.941
Test Loss of 58885512247.665047, Test MSE of 58885511859.077286
Epoch 34: training loss 51055174565.647
Test Loss of 55239784676.818878, Test MSE of 55239785680.495399
Epoch 35: training loss 48643557760.000
Test Loss of 53413523415.257919, Test MSE of 53413523495.051239
Epoch 36: training loss 46629894829.176
Test Loss of 48656920435.771454, Test MSE of 48656919440.552658
Epoch 37: training loss 44138958479.059
Test Loss of 48111926812.780014, Test MSE of 48111927050.132515
Epoch 38: training loss 42100306334.118
Test Loss of 46921358404.929909, Test MSE of 46921357396.223961
Epoch 39: training loss 39444566038.588
Test Loss of 42733507335.046959, Test MSE of 42733508311.226189
Epoch 40: training loss 37772376809.412
Test Loss of 39970355097.197319, Test MSE of 39970355123.815155
Epoch 41: training loss 36394933097.412
Test Loss of 38700174740.341431, Test MSE of 38700174364.113235
Epoch 42: training loss 33945829933.176
Test Loss of 38108101685.770065, Test MSE of 38108101806.779808
Epoch 43: training loss 32045683478.588
Test Loss of 35405622008.360863, Test MSE of 35405621863.764374
Epoch 44: training loss 30517053921.882
Test Loss of 33274282229.399952, Test MSE of 33274281531.715794
Epoch 45: training loss 29196339350.588
Test Loss of 33309554518.873005, Test MSE of 33309553978.530476
Epoch 46: training loss 27892582272.000
Test Loss of 33377832406.665741, Test MSE of 33377833083.375404
Epoch 47: training loss 26519412973.176
Test Loss of 31037498138.944252, Test MSE of 31037497921.147923
Epoch 48: training loss 24996319269.647
Test Loss of 29566598355.053436, Test MSE of 29566598537.800865
Epoch 49: training loss 24252354831.059
Test Loss of 30438694235.492020, Test MSE of 30438694848.885975
Epoch 50: training loss 22984632549.647
Test Loss of 29813077731.752949, Test MSE of 29813078032.103626
Epoch 51: training loss 22267999529.412
Test Loss of 26421532661.577610, Test MSE of 26421532799.139957
Epoch 52: training loss 21269690454.588
Test Loss of 27349569878.991440, Test MSE of 27349569818.199780
Epoch 53: training loss 20499933507.765
Test Loss of 25618088159.607681, Test MSE of 25618088263.776371
Epoch 54: training loss 19288894810.353
Test Loss of 27604810429.616470, Test MSE of 27604810142.435291
Epoch 55: training loss 18955345716.706
Test Loss of 26799875527.032154, Test MSE of 26799875907.509007
Epoch 56: training loss 18312105712.941
Test Loss of 23491031032.893826, Test MSE of 23491031315.111420
Epoch 57: training loss 17762922194.824
Test Loss of 21684117310.238262, Test MSE of 21684117386.933697
Epoch 58: training loss 17091313573.647
Test Loss of 23309247855.389313, Test MSE of 23309247971.936966
Epoch 59: training loss 16972222904.471
Test Loss of 24642381522.698128, Test MSE of 24642381350.165802
Epoch 60: training loss 16250043956.706
Test Loss of 23305152874.888733, Test MSE of 23305152901.933941
Epoch 61: training loss 15585958561.882
Test Loss of 23065703715.590099, Test MSE of 23065703991.916836
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22622227733.15337, 'MSE - std': 2699762160.0823507, 'R2 - mean': 0.8380736130368893, 'R2 - std': 0.015283957215239024} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004090 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110712530.824
Test Loss of 410764273803.313293, Test MSE of 410764275878.640808
Epoch 2: training loss 430090429138.824
Test Loss of 410745839339.268860, Test MSE of 410745843046.600647
Epoch 3: training loss 430063507335.529
Test Loss of 410722404062.948608, Test MSE of 410722410086.687012
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078188845.176
Test Loss of 410725542042.476624, Test MSE of 410725545402.156006
Epoch 2: training loss 430066083478.588
Test Loss of 410725537948.372070, Test MSE of 410725536702.319153
Epoch 3: training loss 430065603764.706
Test Loss of 410725366071.796387, Test MSE of 410725363587.926086
Epoch 4: training loss 430065263073.882
Test Loss of 410725335214.378540, Test MSE of 410725335903.597778
Epoch 5: training loss 423757280436.706
Test Loss of 390298215518.297058, Test MSE of 390298218199.360840
Epoch 6: training loss 379598060845.176
Test Loss of 324952252434.954163, Test MSE of 324952252056.785400
Epoch 7: training loss 300862195591.529
Test Loss of 239274495952.614532, Test MSE of 239274498843.526978
Epoch 8: training loss 221991874198.588
Test Loss of 169456467437.282745, Test MSE of 169456465188.779968
Epoch 9: training loss 161843645861.647
Test Loss of 120481997789.408600, Test MSE of 120481996741.847260
Epoch 10: training loss 142229452920.471
Test Loss of 112768982846.667282, Test MSE of 112768982663.475906
Epoch 11: training loss 137369675294.118
Test Loss of 110372668624.022217, Test MSE of 110372668256.283615
Epoch 12: training loss 134445408557.176
Test Loss of 107402028202.113831, Test MSE of 107402029682.423508
Epoch 13: training loss 131236965044.706
Test Loss of 104323494943.748260, Test MSE of 104323493213.944229
Epoch 14: training loss 127885887367.529
Test Loss of 101634297645.608521, Test MSE of 101634296915.031860
Epoch 15: training loss 125162373662.118
Test Loss of 98084260748.853302, Test MSE of 98084261160.259964
Epoch 16: training loss 120785897381.647
Test Loss of 94730090925.312363, Test MSE of 94730089659.244705
Epoch 17: training loss 116252255804.235
Test Loss of 91736721001.432663, Test MSE of 91736718879.072861
Epoch 18: training loss 113251659625.412
Test Loss of 88707651224.818146, Test MSE of 88707649025.572067
Epoch 19: training loss 109124482409.412
Test Loss of 86071920422.974548, Test MSE of 86071919125.014328
Epoch 20: training loss 104346353664.000
Test Loss of 82314281430.063858, Test MSE of 82314280912.907196
Epoch 21: training loss 102147380254.118
Test Loss of 78432520266.869049, Test MSE of 78432520335.358582
Epoch 22: training loss 96575973406.118
Test Loss of 76709632190.489594, Test MSE of 76709632357.363266
Epoch 23: training loss 94022336286.118
Test Loss of 73258634689.688110, Test MSE of 73258634092.365692
Epoch 24: training loss 89379911077.647
Test Loss of 70756221379.109665, Test MSE of 70756219918.010376
Epoch 25: training loss 85262408011.294
Test Loss of 66230219189.841743, Test MSE of 66230218960.276894
Epoch 26: training loss 82160999348.706
Test Loss of 64881847063.337341, Test MSE of 64881849229.337723
Epoch 27: training loss 78006644992.000
Test Loss of 60828528198.367424, Test MSE of 60828527843.614952
Epoch 28: training loss 75670605974.588
Test Loss of 60267919815.848221, Test MSE of 60267920280.599510
Epoch 29: training loss 71335122793.412
Test Loss of 55918806641.014343, Test MSE of 55918807549.594948
Epoch 30: training loss 68020380581.647
Test Loss of 50431469051.498383, Test MSE of 50431469318.316101
Epoch 31: training loss 65967144628.706
Test Loss of 49122119276.275795, Test MSE of 49122118867.783783
Epoch 32: training loss 62528660028.235
Test Loss of 48407927480.566406, Test MSE of 48407926823.856453
Epoch 33: training loss 59346231250.824
Test Loss of 44757531733.767700, Test MSE of 44757532342.851051
Epoch 34: training loss 56340385302.588
Test Loss of 43306785674.957893, Test MSE of 43306786498.846336
Epoch 35: training loss 53262362307.765
Test Loss of 40649082192.910690, Test MSE of 40649081683.801193
Epoch 36: training loss 51286377803.294
Test Loss of 40441177927.196671, Test MSE of 40441177789.662781
Epoch 37: training loss 48796441743.059
Test Loss of 39244417125.404907, Test MSE of 39244417033.107689
Epoch 38: training loss 46378610951.529
Test Loss of 39542055084.483109, Test MSE of 39542055114.070656
Epoch 39: training loss 43347127152.941
Test Loss of 32616088289.317909, Test MSE of 32616088456.363777
Epoch 40: training loss 41904944338.824
Test Loss of 30907236073.373440, Test MSE of 30907236095.242466
Epoch 41: training loss 39646141899.294
Test Loss of 29350052135.685329, Test MSE of 29350052634.163200
Epoch 42: training loss 37852473479.529
Test Loss of 29038928436.834797, Test MSE of 29038927753.520599
Epoch 43: training loss 35081836965.647
Test Loss of 28412894210.369274, Test MSE of 28412894055.978043
Epoch 44: training loss 33685097660.235
Test Loss of 26326473133.312355, Test MSE of 26326473278.746738
Epoch 45: training loss 31947556961.882
Test Loss of 27074690726.086071, Test MSE of 27074691154.293209
Epoch 46: training loss 30511912116.706
Test Loss of 23648581762.783897, Test MSE of 23648581561.855217
Epoch 47: training loss 29161961291.294
Test Loss of 25450677303.440998, Test MSE of 25450677085.281639
Epoch 48: training loss 28173988084.706
Test Loss of 22323397382.278576, Test MSE of 22323396802.077850
Epoch 49: training loss 26356244656.941
Test Loss of 23147347326.400742, Test MSE of 23147347216.480492
Epoch 50: training loss 25801049182.118
Test Loss of 21538605827.435448, Test MSE of 21538605637.807545
Epoch 51: training loss 24707546014.118
Test Loss of 22046216038.471077, Test MSE of 22046215725.141167
Epoch 52: training loss 23338375920.941
Test Loss of 20567260172.794075, Test MSE of 20567260260.662212
Epoch 53: training loss 22579719574.588
Test Loss of 20425175737.514114, Test MSE of 20425176029.304230
Epoch 54: training loss 21963681957.647
Test Loss of 21189526555.009716, Test MSE of 21189526964.889854
Epoch 55: training loss 20754688764.235
Test Loss of 20052713548.764462, Test MSE of 20052713900.119274
Epoch 56: training loss 20175642089.412
Test Loss of 21602272649.773254, Test MSE of 21602272641.105331
Epoch 57: training loss 19676665814.588
Test Loss of 18307811658.750580, Test MSE of 18307811630.923683
Epoch 58: training loss 18860154688.000
Test Loss of 19208456989.023602, Test MSE of 19208456954.878109
Epoch 59: training loss 18562189967.059
Test Loss of 19270364851.354004, Test MSE of 19270365052.541889
Epoch 60: training loss 17782875358.118
Test Loss of 19156887912.129570, Test MSE of 19156888252.374134
Epoch 61: training loss 17086936525.176
Test Loss of 18838431309.001389, Test MSE of 18838431678.784462
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21676278719.561142, 'MSE - std': 2854994816.0561895, 'R2 - mean': 0.8396843611530906, 'R2 - std': 0.013527122369501968} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005422 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043555056.941
Test Loss of 431611044162.221191, Test MSE of 431611047477.434631
Epoch 2: training loss 424024709120.000
Test Loss of 431591336628.775574, Test MSE of 431591334364.913879
Epoch 3: training loss 423998792041.412
Test Loss of 431564775882.691345, Test MSE of 431564778798.368835
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009813895.529
Test Loss of 431565558963.590942, Test MSE of 431565554418.677368
Epoch 2: training loss 423998602420.706
Test Loss of 431568508053.264221, Test MSE of 431568510356.963257
Epoch 3: training loss 423998040064.000
Test Loss of 431569128446.578430, Test MSE of 431569131609.923706
Epoch 4: training loss 423997662750.118
Test Loss of 431569020316.727417, Test MSE of 431569020520.666687
Epoch 5: training loss 417351043072.000
Test Loss of 409846579579.083740, Test MSE of 409846582007.714355
Epoch 6: training loss 372397186831.059
Test Loss of 341624258923.920410, Test MSE of 341624260647.194580
Epoch 7: training loss 294273406253.176
Test Loss of 255785509102.822754, Test MSE of 255785507045.753906
Epoch 8: training loss 216335426620.235
Test Loss of 183104313940.109222, Test MSE of 183104316207.144318
Epoch 9: training loss 157427819791.059
Test Loss of 132478945540.620087, Test MSE of 132478944297.147644
Epoch 10: training loss 138223394334.118
Test Loss of 124361202479.977783, Test MSE of 124361200716.320374
Epoch 11: training loss 135479951239.529
Test Loss of 120827796715.505783, Test MSE of 120827798120.062271
Epoch 12: training loss 131423725025.882
Test Loss of 118034283205.834335, Test MSE of 118034282056.899567
Epoch 13: training loss 127535535796.706
Test Loss of 113210695949.149475, Test MSE of 113210697675.999054
Epoch 14: training loss 124042031028.706
Test Loss of 110606146207.925964, Test MSE of 110606145308.754135
Epoch 15: training loss 120889974272.000
Test Loss of 106070196606.874588, Test MSE of 106070192327.994431
Epoch 16: training loss 116815620638.118
Test Loss of 103899067950.674683, Test MSE of 103899067757.133438
Epoch 17: training loss 112888236965.647
Test Loss of 99991081644.246185, Test MSE of 99991081422.517792
Epoch 18: training loss 109254633261.176
Test Loss of 95550487973.256821, Test MSE of 95550488085.103806
Epoch 19: training loss 104150687247.059
Test Loss of 92105996884.109207, Test MSE of 92105998735.237091
Epoch 20: training loss 100715215781.647
Test Loss of 88266318548.049973, Test MSE of 88266319791.584564
Epoch 21: training loss 97835891531.294
Test Loss of 84991164245.412308, Test MSE of 84991164092.158890
Epoch 22: training loss 94095854802.824
Test Loss of 80000572103.729752, Test MSE of 80000572558.123993
Epoch 23: training loss 89998774091.294
Test Loss of 78081382397.156876, Test MSE of 78081382071.634995
Epoch 24: training loss 86570348333.176
Test Loss of 74592389977.677002, Test MSE of 74592389441.597885
Epoch 25: training loss 82790184086.588
Test Loss of 72838937719.885239, Test MSE of 72838936083.954041
Epoch 26: training loss 79603074349.176
Test Loss of 69980941767.374359, Test MSE of 69980942177.833771
Epoch 27: training loss 75911611467.294
Test Loss of 67168428795.853775, Test MSE of 67168431086.630440
Epoch 28: training loss 72765316833.882
Test Loss of 61148905447.359558, Test MSE of 61148904911.078773
Epoch 29: training loss 69686572679.529
Test Loss of 61624988356.886627, Test MSE of 61624989402.516838
Epoch 30: training loss 65504687623.529
Test Loss of 56388828706.354469, Test MSE of 56388827762.615387
Epoch 31: training loss 62932403335.529
Test Loss of 53483932200.514580, Test MSE of 53483931886.721474
Epoch 32: training loss 60461708976.941
Test Loss of 50303373090.236000, Test MSE of 50303372420.535187
Epoch 33: training loss 56705576146.824
Test Loss of 50549985173.856544, Test MSE of 50549985157.443413
Epoch 34: training loss 54177718324.706
Test Loss of 46529827344.821838, Test MSE of 46529827458.779602
Epoch 35: training loss 51154431344.941
Test Loss of 42244774285.090233, Test MSE of 42244774504.424950
Epoch 36: training loss 49523945118.118
Test Loss of 41509179083.994446, Test MSE of 41509178713.097412
Epoch 37: training loss 46878719879.529
Test Loss of 41862753587.531700, Test MSE of 41862754382.937294
Epoch 38: training loss 44701267139.765
Test Loss of 37196686011.409531, Test MSE of 37196686132.298866
Epoch 39: training loss 42177154650.353
Test Loss of 33269252843.742710, Test MSE of 33269252624.407623
Epoch 40: training loss 39916614031.059
Test Loss of 35587862778.195282, Test MSE of 35587863629.029701
Epoch 41: training loss 38320587045.647
Test Loss of 33491022043.868580, Test MSE of 33491022448.892319
Epoch 42: training loss 36848025261.176
Test Loss of 30916024124.771866, Test MSE of 30916024283.523724
Epoch 43: training loss 34533593155.765
Test Loss of 30393016505.751041, Test MSE of 30393016386.809853
Epoch 44: training loss 32906284039.529
Test Loss of 27344897528.655251, Test MSE of 27344897172.882549
Epoch 45: training loss 31233512184.471
Test Loss of 24764341640.825542, Test MSE of 24764341565.878490
Epoch 46: training loss 29750694896.941
Test Loss of 23724986990.645073, Test MSE of 23724987311.638897
Epoch 47: training loss 28345415070.118
Test Loss of 24559445825.510410, Test MSE of 24559445344.079601
Epoch 48: training loss 27053663232.000
Test Loss of 22647189186.517353, Test MSE of 22647188925.099010
Epoch 49: training loss 25644269872.941
Test Loss of 24376323392.799629, Test MSE of 24376323065.064007
Epoch 50: training loss 24840145370.353
Test Loss of 23178744036.397964, Test MSE of 23178744022.296772
Epoch 51: training loss 24087775480.471
Test Loss of 21587307593.447479, Test MSE of 21587307665.637478
Epoch 52: training loss 22794935442.824
Test Loss of 21241103463.774178, Test MSE of 21241103737.099903
Epoch 53: training loss 21997551446.588
Test Loss of 22211311545.869503, Test MSE of 22211311682.755474
Epoch 54: training loss 21158395380.706
Test Loss of 20299124919.855621, Test MSE of 20299124815.983299
Epoch 55: training loss 20596467297.882
Test Loss of 21273954032.007404, Test MSE of 21273953874.478981
Epoch 56: training loss 20015267166.118
Test Loss of 22229093885.393799, Test MSE of 22229093808.820568
Epoch 57: training loss 19255913133.176
Test Loss of 21453643993.973160, Test MSE of 21453644221.164116
Epoch 58: training loss 18766899828.706
Test Loss of 21162599327.807495, Test MSE of 21162599253.568367
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21573542826.362587, 'MSE - std': 2561838202.301502, 'R2 - mean': 0.8401388735355717, 'R2 - std': 0.012133126460352198} 
 

Saving model.....
Results After CV: {'MSE - mean': 21573542826.362587, 'MSE - std': 2561838202.301502, 'R2 - mean': 0.8401388735355717, 'R2 - std': 0.012133126460352198}
Train time: 94.7831007625995
Inference time: 0.07091932419934892
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 61 finished with value: 21573542826.362587 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005530 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525678622.118
Test Loss of 418112322784.318298, Test MSE of 418112320578.799805
Epoch 2: training loss 427504699873.882
Test Loss of 418094295730.246582, Test MSE of 418094293865.322021
Epoch 3: training loss 427477004890.353
Test Loss of 418070540097.554504, Test MSE of 418070546380.078186
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491665679.059
Test Loss of 418078915994.026367, Test MSE of 418078911198.144348
Epoch 2: training loss 427482611230.118
Test Loss of 418078568651.710388, Test MSE of 418078568040.071777
Epoch 3: training loss 427482153803.294
Test Loss of 418077551236.767090, Test MSE of 418077553045.325562
Epoch 4: training loss 427481813955.765
Test Loss of 418076010996.748535, Test MSE of 418076006702.492371
Epoch 5: training loss 427481581688.471
Test Loss of 418076394146.612976, Test MSE of 418076395684.258850
Epoch 6: training loss 418797797737.412
Test Loss of 390940736789.377747, Test MSE of 390940741272.799316
Epoch 7: training loss 364253780208.941
Test Loss of 312752373737.260254, Test MSE of 312752372529.000671
Epoch 8: training loss 276237469816.471
Test Loss of 223485071671.724274, Test MSE of 223485066769.555634
Epoch 9: training loss 200063538145.882
Test Loss of 158729019227.136719, Test MSE of 158729019499.011139
Epoch 10: training loss 156200899463.529
Test Loss of 129825311950.315979, Test MSE of 129825311487.390030
Epoch 11: training loss 139451826582.588
Test Loss of 118154686895.108032, Test MSE of 118154686440.625381
Epoch 12: training loss 134297212220.235
Test Loss of 114048198349.486923, Test MSE of 114048200731.004807
Epoch 13: training loss 132043313423.059
Test Loss of 111362814180.345123, Test MSE of 111362816777.933640
Epoch 14: training loss 127872839258.353
Test Loss of 108392457452.161926, Test MSE of 108392456692.319397
Epoch 15: training loss 124736058247.529
Test Loss of 104987303443.305115, Test MSE of 104987303933.105087
Epoch 16: training loss 120583674563.765
Test Loss of 101671952755.416153, Test MSE of 101671955906.695694
Epoch 17: training loss 116788958569.412
Test Loss of 98816763508.422852, Test MSE of 98816764360.106262
Epoch 18: training loss 113062945656.471
Test Loss of 95761381464.590332, Test MSE of 95761381331.916519
Epoch 19: training loss 109218405044.706
Test Loss of 92020139122.883179, Test MSE of 92020140833.083054
Epoch 20: training loss 104953210179.765
Test Loss of 89122404421.640533, Test MSE of 89122404686.594116
Epoch 21: training loss 101147449268.706
Test Loss of 84277084036.826279, Test MSE of 84277083762.610367
Epoch 22: training loss 96717293959.529
Test Loss of 81225937675.784409, Test MSE of 81225939324.836075
Epoch 23: training loss 92668610108.235
Test Loss of 78355376562.424240, Test MSE of 78355377204.683640
Epoch 24: training loss 88081033291.294
Test Loss of 74714675085.827438, Test MSE of 74714674861.671555
Epoch 25: training loss 84886720353.882
Test Loss of 71390322272.999313, Test MSE of 71390322534.113693
Epoch 26: training loss 81994398494.118
Test Loss of 65698662635.451309, Test MSE of 65698662344.406746
Epoch 27: training loss 77768361140.706
Test Loss of 65608528526.715706, Test MSE of 65608529413.668694
Epoch 28: training loss 73457234522.353
Test Loss of 61138420117.525795, Test MSE of 61138419939.888893
Epoch 29: training loss 71541345995.294
Test Loss of 61319320921.123291, Test MSE of 61319322413.398300
Epoch 30: training loss 67693113562.353
Test Loss of 57232946162.972008, Test MSE of 57232947326.291779
Epoch 31: training loss 64518393878.588
Test Loss of 53457616501.844093, Test MSE of 53457616402.912674
Epoch 32: training loss 60939267237.647
Test Loss of 51176049291.399490, Test MSE of 51176049993.574699
Epoch 33: training loss 58090298578.824
Test Loss of 48510155470.671295, Test MSE of 48510155831.818962
Epoch 34: training loss 54993451369.412
Test Loss of 45069865277.409210, Test MSE of 45069865536.685158
Epoch 35: training loss 52585109210.353
Test Loss of 42640188916.511681, Test MSE of 42640188813.346748
Epoch 36: training loss 49562047661.176
Test Loss of 41277187245.627571, Test MSE of 41277187748.349388
Epoch 37: training loss 47460147719.529
Test Loss of 39303045690.389084, Test MSE of 39303046316.106941
Epoch 38: training loss 44531515241.412
Test Loss of 39780215309.383301, Test MSE of 39780215856.436279
Epoch 39: training loss 42636881430.588
Test Loss of 38307774981.329636, Test MSE of 38307775177.238838
Epoch 40: training loss 40549582433.882
Test Loss of 33604413214.023594, Test MSE of 33604413667.409943
Epoch 41: training loss 38894398795.294
Test Loss of 34918323165.179733, Test MSE of 34918323710.085403
Epoch 42: training loss 36673773921.882
Test Loss of 32022065381.292622, Test MSE of 32022065405.970230
Epoch 43: training loss 34121203787.294
Test Loss of 29872897988.308121, Test MSE of 29872898555.286373
Epoch 44: training loss 32611416719.059
Test Loss of 28365803515.262550, Test MSE of 28365803401.623863
Epoch 45: training loss 30979254234.353
Test Loss of 28154311444.311821, Test MSE of 28154311148.893063
Epoch 46: training loss 29980298217.412
Test Loss of 26127651131.514225, Test MSE of 26127651357.591682
Epoch 47: training loss 28116134381.176
Test Loss of 25806212709.736755, Test MSE of 25806212986.343678
Epoch 48: training loss 27018983476.706
Test Loss of 25598207401.659958, Test MSE of 25598207798.739521
Epoch 49: training loss 25845218074.353
Test Loss of 25774903310.686096, Test MSE of 25774903350.435593
Epoch 50: training loss 24823130910.118
Test Loss of 26004413128.986351, Test MSE of 26004413087.044609
Epoch 51: training loss 23690859610.353
Test Loss of 23010078099.393940, Test MSE of 23010077720.244041
Epoch 52: training loss 22479749820.235
Test Loss of 25885698257.869072, Test MSE of 25885698970.638309
Epoch 53: training loss 21710489991.529
Test Loss of 22605174715.780708, Test MSE of 22605174690.930553
Epoch 54: training loss 20685967800.471
Test Loss of 22846751189.955124, Test MSE of 22846751517.124149
Epoch 55: training loss 19764941556.706
Test Loss of 19761394933.636826, Test MSE of 19761394945.693718
Epoch 56: training loss 19310755693.176
Test Loss of 19611152198.291927, Test MSE of 19611152205.629864
Epoch 57: training loss 18717474424.471
Test Loss of 19027407377.173260, Test MSE of 19027407211.391113
Epoch 58: training loss 17694908451.765
Test Loss of 19502160731.847328, Test MSE of 19502160930.506001
Epoch 59: training loss 17280294885.647
Test Loss of 18491415817.534119, Test MSE of 18491415993.042274
Epoch 60: training loss 16817010337.882
Test Loss of 20871837116.609760, Test MSE of 20871837426.366203
Epoch 61: training loss 16255868336.941
Test Loss of 17414488217.493408, Test MSE of 17414488604.879200
Epoch 62: training loss 15769759657.412
Test Loss of 17610089593.515614, Test MSE of 17610089987.633293
Epoch 63: training loss 15190780961.882
Test Loss of 19073869414.684246, Test MSE of 19073869629.569668
Epoch 64: training loss 14862604762.353
Test Loss of 17381393166.153133, Test MSE of 17381393019.867561
Epoch 65: training loss 14484559619.765
Test Loss of 17282058355.593800, Test MSE of 17282058365.134346
Epoch 66: training loss 14055782991.059
Test Loss of 18750155485.831135, Test MSE of 18750155482.550068
Epoch 67: training loss 13801894027.294
Test Loss of 17625662449.077030, Test MSE of 17625662885.723442
Epoch 68: training loss 13275757781.176
Test Loss of 17910632142.908165, Test MSE of 17910632256.844402
Epoch 69: training loss 13113014964.706
Test Loss of 17689898365.601665, Test MSE of 17689898437.906296
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17689898437.906296, 'MSE - std': 0.0, 'R2 - mean': 0.8622468415246429, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005519 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918246369.882
Test Loss of 424556818343.883423, Test MSE of 424556819086.344604
Epoch 2: training loss 427897702520.471
Test Loss of 424540100743.964844, Test MSE of 424540102467.387939
Epoch 3: training loss 427870270162.824
Test Loss of 424517180797.601685, Test MSE of 424517178749.666138
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887078942.118
Test Loss of 424523728523.162598, Test MSE of 424523725277.382263
Epoch 2: training loss 427876566196.706
Test Loss of 424525133234.187378, Test MSE of 424525132717.886536
Epoch 3: training loss 427876033716.706
Test Loss of 424525040213.392578, Test MSE of 424525042683.900330
Epoch 4: training loss 427875594480.941
Test Loss of 424524247671.975952, Test MSE of 424524249459.249268
Epoch 5: training loss 427875295472.941
Test Loss of 424523774351.367126, Test MSE of 424523784751.243896
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221106841594.5751, 'MSE - std': 203416943156.6688, 'R2 - mean': -0.5842855641068451, 'R2 - std': 1.4465324056314879} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005518 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926557575.529
Test Loss of 447257758357.821899, Test MSE of 447257751047.249146
Epoch 2: training loss 421905384990.118
Test Loss of 447238991745.983826, Test MSE of 447238988566.057983
Epoch 3: training loss 421878061176.471
Test Loss of 447214474748.091614, Test MSE of 447214472808.008118
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900647122.824
Test Loss of 447222819635.105225, Test MSE of 447222828619.641235
Epoch 2: training loss 421887215375.059
Test Loss of 447222964532.881775, Test MSE of 447222961017.420654
Epoch 3: training loss 421886673257.412
Test Loss of 447223319197.401794, Test MSE of 447223320744.362549
Epoch 4: training loss 421886260404.706
Test Loss of 447223808011.369873, Test MSE of 447223808201.785034
Epoch 5: training loss 421886031390.118
Test Loss of 447223742990.804504, Test MSE of 447223743015.646790
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 296479142068.2657, 'MSE - std': 197351469548.26947, 'R2 - mean': -1.0485700237695523, 'R2 - std': 1.3513292663139496} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005447 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109959107.765
Test Loss of 410765458606.852356, Test MSE of 410765456245.605225
Epoch 2: training loss 430088198264.471
Test Loss of 410746461446.041626, Test MSE of 410746464661.718201
Epoch 3: training loss 430060822528.000
Test Loss of 410722597425.991699, Test MSE of 410722600114.936401
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075775578.353
Test Loss of 410727279459.154114, Test MSE of 410727282522.173950
Epoch 2: training loss 430065552926.118
Test Loss of 410728490131.368835, Test MSE of 410728495314.600342
Epoch 3: training loss 430065125014.588
Test Loss of 410728153065.255005, Test MSE of 410728156494.038147
Epoch 4: training loss 430064828416.000
Test Loss of 410728068878.334106, Test MSE of 410728066926.633301
Epoch 5: training loss 430064567476.706
Test Loss of 410727602574.985657, Test MSE of 410727603535.995239
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 325041257435.19806, 'MSE - std': 177927190733.59198, 'R2 - mean': -1.3839147596108097, 'R2 - std': 1.3064977478230781} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005389 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042589123.765
Test Loss of 431612005897.714050, Test MSE of 431612006405.323730
Epoch 2: training loss 424022853391.059
Test Loss of 431592139285.560364, Test MSE of 431592131768.117920
Epoch 3: training loss 423995813044.706
Test Loss of 431564877143.544678, Test MSE of 431564880213.711060
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011278336.000
Test Loss of 431569137635.568726, Test MSE of 431569132426.924561
Epoch 2: training loss 423999149116.235
Test Loss of 431571196075.535400, Test MSE of 431571192263.884338
Epoch 3: training loss 423998602420.706
Test Loss of 431571506590.622864, Test MSE of 431571507816.165405
Epoch 4: training loss 423998179809.882
Test Loss of 431571343872.710754, Test MSE of 431571353224.157227
Epoch 5: training loss 423997927544.471
Test Loss of 431570890018.472900, Test MSE of 431570892720.101013
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 346347184492.17865, 'MSE - std': 164749015999.81693, 'R2 - mean': -1.5517292976624257, 'R2 - std': 1.2158108277110096} 
 

Saving model.....
Results After CV: {'MSE - mean': 346347184492.17865, 'MSE - std': 164749015999.81693, 'R2 - mean': -1.5517292976624257, 'R2 - std': 1.2158108277110096}
Train time: 30.622791272999894
Inference time: 0.06958917480005766
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 62 finished with value: 346347184492.17865 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005549 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526175322.353
Test Loss of 418111533964.643066, Test MSE of 418111535548.975586
Epoch 2: training loss 427505544372.706
Test Loss of 418093228689.558167, Test MSE of 418093231683.611267
Epoch 3: training loss 427478487401.412
Test Loss of 418069272272.329407, Test MSE of 418069277320.687256
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492473554.824
Test Loss of 418075338430.327087, Test MSE of 418075341002.073059
Epoch 2: training loss 427481828532.706
Test Loss of 418076965293.686768, Test MSE of 418076963152.630127
Epoch 3: training loss 427481368214.588
Test Loss of 418075749732.019409, Test MSE of 418075747347.895691
Epoch 4: training loss 427481008730.353
Test Loss of 418075202893.753418, Test MSE of 418075201057.254089
Epoch 5: training loss 421169204043.294
Test Loss of 397741201934.330811, Test MSE of 397741206124.408875
Epoch 6: training loss 376856157244.235
Test Loss of 331971697439.207947, Test MSE of 331971699028.231628
Epoch 7: training loss 298580437293.176
Test Loss of 246570648954.285461, Test MSE of 246570648882.829315
Epoch 8: training loss 219303490439.529
Test Loss of 174700076966.935913, Test MSE of 174700077271.988708
Epoch 9: training loss 160025379779.765
Test Loss of 126680872707.493866, Test MSE of 126680871806.553619
Epoch 10: training loss 140304418153.412
Test Loss of 118359617597.586868, Test MSE of 118359617672.026337
Epoch 11: training loss 136479621300.706
Test Loss of 114986581828.396942, Test MSE of 114986579821.624313
Epoch 12: training loss 132743910671.059
Test Loss of 112229656396.450607, Test MSE of 112229657099.954590
Epoch 13: training loss 129264502573.176
Test Loss of 109195932410.729584, Test MSE of 109195934476.934570
Epoch 14: training loss 125557472496.941
Test Loss of 104922415382.562103, Test MSE of 104922415154.508514
Epoch 15: training loss 122162257091.765
Test Loss of 101745124715.125610, Test MSE of 101745124218.944443
Epoch 16: training loss 118284789293.176
Test Loss of 98330376934.358551, Test MSE of 98330379373.757050
Epoch 17: training loss 114945212355.765
Test Loss of 95331848890.300247, Test MSE of 95331848239.126831
Epoch 18: training loss 110672451041.882
Test Loss of 92432396281.367569, Test MSE of 92432397686.332748
Epoch 19: training loss 105864128888.471
Test Loss of 88439359236.441360, Test MSE of 88439358701.030426
Epoch 20: training loss 102142003425.882
Test Loss of 84148586523.003464, Test MSE of 84148586464.900421
Epoch 21: training loss 97651903216.941
Test Loss of 82132470635.954666, Test MSE of 82132470886.657898
Epoch 22: training loss 94820714209.882
Test Loss of 79578141300.185989, Test MSE of 79578140955.641800
Epoch 23: training loss 90791666733.176
Test Loss of 76650390843.751099, Test MSE of 76650391533.836594
Epoch 24: training loss 87654345411.765
Test Loss of 72949087998.282669, Test MSE of 72949089917.760971
Epoch 25: training loss 84331206934.588
Test Loss of 68902875601.217667, Test MSE of 68902876420.077866
Epoch 26: training loss 80074863450.353
Test Loss of 67507152460.628265, Test MSE of 67507152489.104004
Epoch 27: training loss 76165392888.471
Test Loss of 63635727857.669212, Test MSE of 63635728214.291420
Epoch 28: training loss 72716674138.353
Test Loss of 61402975661.923668, Test MSE of 61402974462.167664
Epoch 29: training loss 69703768862.118
Test Loss of 55206100214.110573, Test MSE of 55206101118.746223
Epoch 30: training loss 65734902919.529
Test Loss of 54416261566.267868, Test MSE of 54416263021.614204
Epoch 31: training loss 63209624297.412
Test Loss of 50917561842.616699, Test MSE of 50917561535.288834
Epoch 32: training loss 60032150053.647
Test Loss of 49842926752.362709, Test MSE of 49842926606.474808
Epoch 33: training loss 57438829387.294
Test Loss of 47733497675.029381, Test MSE of 47733496097.918343
Epoch 34: training loss 54281305935.059
Test Loss of 45535181742.278976, Test MSE of 45535180854.695885
Epoch 35: training loss 51706872960.000
Test Loss of 41370973738.518623, Test MSE of 41370973975.001587
Epoch 36: training loss 49381043964.235
Test Loss of 37841543936.414528, Test MSE of 37841544021.047371
Epoch 37: training loss 46502021955.765
Test Loss of 36293729989.433266, Test MSE of 36293730196.366112
Epoch 38: training loss 44324836043.294
Test Loss of 34656527495.727966, Test MSE of 34656527649.722702
Epoch 39: training loss 42487499316.706
Test Loss of 35147770363.380989, Test MSE of 35147770823.198357
Epoch 40: training loss 40417786420.706
Test Loss of 33000536344.930836, Test MSE of 33000536152.749985
Epoch 41: training loss 37773855745.882
Test Loss of 32620509417.556328, Test MSE of 32620509675.930241
Epoch 42: training loss 36383269556.706
Test Loss of 29504983921.165855, Test MSE of 29504984023.761864
Epoch 43: training loss 34194887973.647
Test Loss of 29430931597.176037, Test MSE of 29430931960.621349
Epoch 44: training loss 33260783683.765
Test Loss of 30005136461.457321, Test MSE of 30005136617.689056
Epoch 45: training loss 31230418710.588
Test Loss of 27987598894.782326, Test MSE of 27987598935.696030
Epoch 46: training loss 29876230196.706
Test Loss of 27541868399.981495, Test MSE of 27541868605.507076
Epoch 47: training loss 28439791567.059
Test Loss of 24419954593.250984, Test MSE of 24419954651.579048
Epoch 48: training loss 27261595233.882
Test Loss of 24986892464.706917, Test MSE of 24986892734.253315
Epoch 49: training loss 26146376150.588
Test Loss of 23798069065.134396, Test MSE of 23798069382.139259
Epoch 50: training loss 24832558584.471
Test Loss of 24719513212.950268, Test MSE of 24719513222.163082
Epoch 51: training loss 23393968862.118
Test Loss of 21481167699.793663, Test MSE of 21481167958.671860
Epoch 52: training loss 22817990464.000
Test Loss of 20901674216.845711, Test MSE of 20901674162.123341
Epoch 53: training loss 21648331945.412
Test Loss of 22517476761.078880, Test MSE of 22517476330.002193
Epoch 54: training loss 21512313129.412
Test Loss of 21359472446.238262, Test MSE of 21359472877.282486
Epoch 55: training loss 20305849114.353
Test Loss of 21402907326.090214, Test MSE of 21402907378.306522
Epoch 56: training loss 19766613180.235
Test Loss of 20148717743.759426, Test MSE of 20148717468.031307
Epoch 57: training loss 19104263503.059
Test Loss of 20358010301.083508, Test MSE of 20358010036.530781
Epoch 58: training loss 18433623348.706
Test Loss of 20307172116.785564, Test MSE of 20307172166.937008
Epoch 59: training loss 17654892807.529
Test Loss of 20283992018.046726, Test MSE of 20283991527.085831
Epoch 60: training loss 16970105611.294
Test Loss of 21414310873.863522, Test MSE of 21414311407.960510
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21414311407.96051, 'MSE - std': 0.0, 'R2 - mean': 0.8332444336310972, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005205 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917772679.529
Test Loss of 424557412821.955139, Test MSE of 424557414606.728210
Epoch 2: training loss 427896300724.706
Test Loss of 424540703832.827209, Test MSE of 424540706992.807678
Epoch 3: training loss 427868042179.765
Test Loss of 424518577390.530640, Test MSE of 424518572442.726868
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889566659.765
Test Loss of 424528236923.943542, Test MSE of 424528234834.174072
Epoch 2: training loss 427877940284.235
Test Loss of 424528326374.595398, Test MSE of 424528325359.928162
Epoch 3: training loss 427877355760.941
Test Loss of 424527743152.706909, Test MSE of 424527746139.150330
Epoch 4: training loss 427876947124.706
Test Loss of 424527397978.248413, Test MSE of 424527398837.396790
Epoch 5: training loss 420880166430.118
Test Loss of 402976266254.686096, Test MSE of 402976264659.204834
Epoch 6: training loss 374886841886.118
Test Loss of 337218686067.356934, Test MSE of 337218692943.171265
Epoch 7: training loss 295997810447.059
Test Loss of 252642217943.257935, Test MSE of 252642218337.757721
Epoch 8: training loss 216290338394.353
Test Loss of 184154933276.898438, Test MSE of 184154932649.035675
Epoch 9: training loss 156561247352.471
Test Loss of 137098613656.012955, Test MSE of 137098613989.963074
Epoch 10: training loss 136060429824.000
Test Loss of 130122512084.119354, Test MSE of 130122509427.672470
Epoch 11: training loss 132945218951.529
Test Loss of 126693283216.077728, Test MSE of 126693283031.046021
Epoch 12: training loss 130197928237.176
Test Loss of 124262921430.369644, Test MSE of 124262920265.295227
Epoch 13: training loss 126346171181.176
Test Loss of 120505590303.859360, Test MSE of 120505593398.303726
Epoch 14: training loss 122604471446.588
Test Loss of 116728532566.576920, Test MSE of 116728527805.527222
Epoch 15: training loss 118411863401.412
Test Loss of 113171662397.468430, Test MSE of 113171663572.419815
Epoch 16: training loss 114913683636.706
Test Loss of 109932559033.826508, Test MSE of 109932558970.356354
Epoch 17: training loss 111027776421.647
Test Loss of 106677388371.616013, Test MSE of 106677386317.615784
Epoch 18: training loss 106827139252.706
Test Loss of 102969845895.017349, Test MSE of 102969846476.682846
Epoch 19: training loss 102446434635.294
Test Loss of 97593081092.322922, Test MSE of 97593082033.359177
Epoch 20: training loss 98360077492.706
Test Loss of 95209548742.913712, Test MSE of 95209547683.563583
Epoch 21: training loss 94829025761.882
Test Loss of 91460185522.661118, Test MSE of 91460183711.903809
Epoch 22: training loss 90797128192.000
Test Loss of 85916666335.903778, Test MSE of 85916664919.226181
Epoch 23: training loss 87552845763.765
Test Loss of 84812541274.544525, Test MSE of 84812540705.255264
Epoch 24: training loss 82614904877.176
Test Loss of 79140772589.938461, Test MSE of 79140772428.904007
Epoch 25: training loss 79621786277.647
Test Loss of 78521373439.467041, Test MSE of 78521372302.697372
Epoch 26: training loss 76088842917.647
Test Loss of 75322323711.467041, Test MSE of 75322323598.147888
Epoch 27: training loss 71936088199.529
Test Loss of 70378988585.926437, Test MSE of 70378989156.073502
Epoch 28: training loss 69167645093.647
Test Loss of 65693598207.644691, Test MSE of 65693597715.968544
Epoch 29: training loss 64699580265.412
Test Loss of 66272558730.688873, Test MSE of 66272560245.285057
Epoch 30: training loss 61940973974.588
Test Loss of 61620587928.131393, Test MSE of 61620585598.087898
Epoch 31: training loss 58649868227.765
Test Loss of 57634473789.290771, Test MSE of 57634473379.605400
Epoch 32: training loss 55381519405.176
Test Loss of 56259152826.596344, Test MSE of 56259153367.267708
Epoch 33: training loss 52894110659.765
Test Loss of 54511252287.896370, Test MSE of 54511252304.304916
Epoch 34: training loss 49754594153.412
Test Loss of 47449345697.428635, Test MSE of 47449344976.803062
Epoch 35: training loss 46747241728.000
Test Loss of 46691151993.278740, Test MSE of 46691153620.001167
Epoch 36: training loss 44588885488.941
Test Loss of 44757119728.307198, Test MSE of 44757120097.666611
Epoch 37: training loss 42016221447.529
Test Loss of 43210514083.797363, Test MSE of 43210514211.999916
Epoch 38: training loss 39506813891.765
Test Loss of 40192377734.484383, Test MSE of 40192377643.657791
Epoch 39: training loss 37896234345.412
Test Loss of 39069366641.521164, Test MSE of 39069365417.341202
Epoch 40: training loss 35961043952.941
Test Loss of 38252882077.757111, Test MSE of 38252881180.865982
Epoch 41: training loss 33899369577.412
Test Loss of 35555468355.508675, Test MSE of 35555468390.416580
Epoch 42: training loss 31752118211.765
Test Loss of 37268836544.340508, Test MSE of 37268835765.674004
Epoch 43: training loss 30169327608.471
Test Loss of 32846497224.453388, Test MSE of 32846497498.018829
Epoch 44: training loss 28841610232.471
Test Loss of 30986681650.276196, Test MSE of 30986681031.295422
Epoch 45: training loss 27302804495.059
Test Loss of 32220291192.804996, Test MSE of 32220292113.065773
Epoch 46: training loss 25418067516.235
Test Loss of 30130308569.981956, Test MSE of 30130307933.939098
Epoch 47: training loss 24606509639.529
Test Loss of 30099259571.075642, Test MSE of 30099259936.373947
Epoch 48: training loss 23026869639.529
Test Loss of 29868846734.952579, Test MSE of 29868845848.422268
Epoch 49: training loss 22211537242.353
Test Loss of 28863561382.166088, Test MSE of 28863561583.610374
Epoch 50: training loss 20798083324.235
Test Loss of 29522787048.964146, Test MSE of 29522786744.820740
Epoch 51: training loss 20539880926.118
Test Loss of 29858937979.647469, Test MSE of 29858939132.889462
Epoch 52: training loss 19214045330.824
Test Loss of 24631945182.600971, Test MSE of 24631945253.322964
Epoch 53: training loss 18204066688.000
Test Loss of 27377860661.533195, Test MSE of 27377860609.394764
Epoch 54: training loss 17961630061.176
Test Loss of 28129191047.254223, Test MSE of 28129190675.412975
Epoch 55: training loss 16984490767.059
Test Loss of 24753048763.839928, Test MSE of 24753048011.318417
Epoch 56: training loss 16515988589.176
Test Loss of 26171222587.336571, Test MSE of 26171222484.912537
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23792766946.436523, 'MSE - std': 2378455538.476013, 'R2 - mean': 0.8231996404529878, 'R2 - std': 0.010044793178109412} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003575 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926280011.294
Test Loss of 447258990701.435120, Test MSE of 447258987124.255859
Epoch 2: training loss 421905493293.176
Test Loss of 447240294203.869507, Test MSE of 447240298889.983459
Epoch 3: training loss 421877907937.882
Test Loss of 447215704605.490662, Test MSE of 447215712115.785767
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900561709.176
Test Loss of 447223231497.711792, Test MSE of 447223233663.550537
Epoch 2: training loss 421888314789.647
Test Loss of 447223821897.785767, Test MSE of 447223823149.380188
Epoch 3: training loss 421887865675.294
Test Loss of 447223483196.580139, Test MSE of 447223491193.173584
Epoch 4: training loss 421887536429.176
Test Loss of 447222908714.340942, Test MSE of 447222901164.083984
Epoch 5: training loss 414623731712.000
Test Loss of 424430314989.879272, Test MSE of 424430311044.519287
Epoch 6: training loss 368181472075.294
Test Loss of 355626534194.276184, Test MSE of 355626541410.309326
Epoch 7: training loss 289055870012.235
Test Loss of 268876544403.157074, Test MSE of 268876547087.835938
Epoch 8: training loss 210728193987.765
Test Loss of 198779213122.383545, Test MSE of 198779211770.415405
Epoch 9: training loss 152877856225.882
Test Loss of 148720514614.835999, Test MSE of 148720516745.262482
Epoch 10: training loss 134060493372.235
Test Loss of 140000656396.080505, Test MSE of 140000654735.898407
Epoch 11: training loss 131151028796.235
Test Loss of 136648646654.105026, Test MSE of 136648646722.216141
Epoch 12: training loss 127871910701.176
Test Loss of 133706024560.632889, Test MSE of 133706025697.362427
Epoch 13: training loss 124205424549.647
Test Loss of 128915427162.189224, Test MSE of 128915428872.500504
Epoch 14: training loss 119546259395.765
Test Loss of 124985547115.362473, Test MSE of 124985546753.835114
Epoch 15: training loss 116621736568.471
Test Loss of 122226633386.666672, Test MSE of 122226634361.781662
Epoch 16: training loss 111583586665.412
Test Loss of 118521672751.611374, Test MSE of 118521671690.539734
Epoch 17: training loss 109546192896.000
Test Loss of 115633543580.631973, Test MSE of 115633544872.441650
Epoch 18: training loss 105833559491.765
Test Loss of 110196571364.108261, Test MSE of 110196573320.893311
Epoch 19: training loss 101670172129.882
Test Loss of 108065188992.621796, Test MSE of 108065190147.234421
Epoch 20: training loss 97175738368.000
Test Loss of 103008635318.214203, Test MSE of 103008633460.280945
Epoch 21: training loss 94122043346.824
Test Loss of 100453527014.773071, Test MSE of 100453527312.473679
Epoch 22: training loss 89203116724.706
Test Loss of 95086830847.111725, Test MSE of 95086832015.686874
Epoch 23: training loss 85978139783.529
Test Loss of 89351810762.170715, Test MSE of 89351812790.240173
Epoch 24: training loss 83131863853.176
Test Loss of 87459793866.703674, Test MSE of 87459793633.552109
Epoch 25: training loss 79279237089.882
Test Loss of 84301004587.762207, Test MSE of 84301003188.979019
Epoch 26: training loss 74992212178.824
Test Loss of 78951062848.962296, Test MSE of 78951061944.427902
Epoch 27: training loss 72152981654.588
Test Loss of 77978523476.978027, Test MSE of 77978522365.300140
Epoch 28: training loss 68796198896.941
Test Loss of 72128425861.773773, Test MSE of 72128424811.594269
Epoch 29: training loss 65387421515.294
Test Loss of 71461613313.125137, Test MSE of 71461613418.408859
Epoch 30: training loss 61890397831.529
Test Loss of 66855692203.199631, Test MSE of 66855693278.376877
Epoch 31: training loss 58879609148.235
Test Loss of 68901098354.113342, Test MSE of 68901096768.040329
Epoch 32: training loss 56361104150.588
Test Loss of 60097311450.514923, Test MSE of 60097311600.548302
Epoch 33: training loss 53338601035.294
Test Loss of 58232627251.164467, Test MSE of 58232628618.570160
Epoch 34: training loss 50664744154.353
Test Loss of 58471234062.330788, Test MSE of 58471234084.449844
Epoch 35: training loss 47995363207.529
Test Loss of 52692012636.261856, Test MSE of 52692013102.380737
Epoch 36: training loss 45592130273.882
Test Loss of 50342465634.538979, Test MSE of 50342464911.332214
Epoch 37: training loss 43742431623.529
Test Loss of 45997602226.187370, Test MSE of 45997603607.862907
Epoch 38: training loss 40938367066.353
Test Loss of 48542162797.139023, Test MSE of 48542162154.818947
Epoch 39: training loss 38422097814.588
Test Loss of 42229354033.624794, Test MSE of 42229353770.003548
Epoch 40: training loss 36488192933.647
Test Loss of 39725750073.500809, Test MSE of 39725750546.867363
Epoch 41: training loss 34863471164.235
Test Loss of 39959050508.613464, Test MSE of 39959051166.794609
Epoch 42: training loss 32990755719.529
Test Loss of 36363680193.584084, Test MSE of 36363680559.987038
Epoch 43: training loss 31337602477.176
Test Loss of 35304454504.520012, Test MSE of 35304454438.754349
Epoch 44: training loss 30178277210.353
Test Loss of 32027076868.086052, Test MSE of 32027077000.197998
Epoch 45: training loss 28574659124.706
Test Loss of 35252048205.516541, Test MSE of 35252047257.661392
Epoch 46: training loss 27107968850.824
Test Loss of 32437926041.967152, Test MSE of 32437926303.690174
Epoch 47: training loss 25365225690.353
Test Loss of 29603200843.029377, Test MSE of 29603201340.932217
Epoch 48: training loss 24613827937.882
Test Loss of 29869662241.162155, Test MSE of 29869662304.552711
Epoch 49: training loss 23514503273.412
Test Loss of 27676025536.932686, Test MSE of 27676025691.706627
Epoch 50: training loss 22366513152.000
Test Loss of 28513429951.215359, Test MSE of 28513430371.359100
Epoch 51: training loss 21489976749.176
Test Loss of 26198785406.075409, Test MSE of 26198785533.446796
Epoch 52: training loss 20571030701.176
Test Loss of 26268001844.941013, Test MSE of 26268002104.936901
Epoch 53: training loss 19734845059.765
Test Loss of 25270693357.405506, Test MSE of 25270693650.448200
Epoch 54: training loss 18845410085.647
Test Loss of 26135278010.004166, Test MSE of 26135278013.379135
Epoch 55: training loss 18390441792.000
Test Loss of 24702198947.678928, Test MSE of 24702198894.685471
Epoch 56: training loss 18078774192.941
Test Loss of 24882388351.970390, Test MSE of 24882388124.468750
Epoch 57: training loss 17141702215.529
Test Loss of 24031352937.645153, Test MSE of 24031352678.370087
Epoch 58: training loss 16729737671.529
Test Loss of 25724432857.745083, Test MSE of 25724433426.782391
Epoch 59: training loss 16288059015.529
Test Loss of 21685817872.225769, Test MSE of 21685818028.945496
Epoch 60: training loss 15709810240.000
Test Loss of 22354131954.024521, Test MSE of 22354132137.270458
Epoch 61: training loss 15293842074.353
Test Loss of 22374164702.186443, Test MSE of 22374164738.906101
Epoch 62: training loss 14811427083.294
Test Loss of 22745073794.753643, Test MSE of 22745073739.372471
Epoch 63: training loss 14424737163.294
Test Loss of 23416360860.039787, Test MSE of 23416360794.499008
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23667298229.12402, 'MSE - std': 1950090245.0193353, 'R2 - mean': 0.8301727041467718, 'R2 - std': 0.012826241897601912} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005335 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110584350.118
Test Loss of 410766138395.009705, Test MSE of 410766138844.772705
Epoch 2: training loss 430089381044.706
Test Loss of 410747593359.814880, Test MSE of 410747586479.916565
Epoch 3: training loss 430061903149.176
Test Loss of 410723297305.588135, Test MSE of 410723292337.797913
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078161859.765
Test Loss of 410728463792.629333, Test MSE of 410728472073.575500
Epoch 2: training loss 430066953035.294
Test Loss of 410729348119.218872, Test MSE of 410729344730.093445
Epoch 3: training loss 430066483200.000
Test Loss of 410728847106.961609, Test MSE of 410728847080.855713
Epoch 4: training loss 430066122752.000
Test Loss of 410727920110.704285, Test MSE of 410727918643.831970
Epoch 5: training loss 423530167838.118
Test Loss of 389808018657.080994, Test MSE of 389808015123.266174
Epoch 6: training loss 378637703047.529
Test Loss of 324037914618.313721, Test MSE of 324037913678.569519
Epoch 7: training loss 300463445775.059
Test Loss of 239047408526.748718, Test MSE of 239047407251.370850
Epoch 8: training loss 221539110369.882
Test Loss of 169136628572.520142, Test MSE of 169136628237.072815
Epoch 9: training loss 162359469296.941
Test Loss of 120629773137.147614, Test MSE of 120629772351.247940
Epoch 10: training loss 141778461214.118
Test Loss of 112930475863.307724, Test MSE of 112930477334.127136
Epoch 11: training loss 137200245669.647
Test Loss of 109955954465.288300, Test MSE of 109955955366.309113
Epoch 12: training loss 134969166817.882
Test Loss of 107656714286.437759, Test MSE of 107656714126.279724
Epoch 13: training loss 130635039111.529
Test Loss of 104810103545.958359, Test MSE of 104810103296.480286
Epoch 14: training loss 128365785359.059
Test Loss of 101559878799.577972, Test MSE of 101559879761.032196
Epoch 15: training loss 122936790678.588
Test Loss of 98155194234.372971, Test MSE of 98155192935.413925
Epoch 16: training loss 119304075806.118
Test Loss of 95271048663.485428, Test MSE of 95271047296.015121
Epoch 17: training loss 116406169690.353
Test Loss of 92009096536.492371, Test MSE of 92009096021.191513
Epoch 18: training loss 113010493168.941
Test Loss of 89285444266.350769, Test MSE of 89285444896.112122
Epoch 19: training loss 107499013782.588
Test Loss of 85411082154.706161, Test MSE of 85411080231.998932
Epoch 20: training loss 105423557903.059
Test Loss of 82663447093.782501, Test MSE of 82663445828.310944
Epoch 21: training loss 101034697065.412
Test Loss of 80441700532.064789, Test MSE of 80441700229.063278
Epoch 22: training loss 97897653519.059
Test Loss of 76459188030.667282, Test MSE of 76459187158.829407
Epoch 23: training loss 93102676540.235
Test Loss of 72749236603.083755, Test MSE of 72749237200.112885
Epoch 24: training loss 89160149052.235
Test Loss of 69991784019.635361, Test MSE of 69991785174.900742
Epoch 25: training loss 85893189526.588
Test Loss of 67220330345.314209, Test MSE of 67220329757.600998
Epoch 26: training loss 81097538740.706
Test Loss of 64628156286.163811, Test MSE of 64628155136.698784
Epoch 27: training loss 78935080734.118
Test Loss of 62015854627.065247, Test MSE of 62015855646.115997
Epoch 28: training loss 73864260276.706
Test Loss of 58541810202.298935, Test MSE of 58541810472.840714
Epoch 29: training loss 71410417754.353
Test Loss of 56495054082.250809, Test MSE of 56495053250.059334
Epoch 30: training loss 68984538134.588
Test Loss of 52621657097.003242, Test MSE of 52621656658.908806
Epoch 31: training loss 65128318042.353
Test Loss of 50397694972.683014, Test MSE of 50397694506.366974
Epoch 32: training loss 61650931094.588
Test Loss of 49407830521.129105, Test MSE of 49407830458.176483
Epoch 33: training loss 58934706206.118
Test Loss of 49044392939.624245, Test MSE of 49044392541.189537
Epoch 34: training loss 55692351036.235
Test Loss of 45223099159.337341, Test MSE of 45223098964.927551
Epoch 35: training loss 53398271962.353
Test Loss of 44264480440.092552, Test MSE of 44264479685.050972
Epoch 36: training loss 50225701654.588
Test Loss of 40094745705.669594, Test MSE of 40094745439.103912
Epoch 37: training loss 48217950471.529
Test Loss of 38082036710.885704, Test MSE of 38082036636.027573
Epoch 38: training loss 45861724144.941
Test Loss of 37728711283.383621, Test MSE of 37728711681.387756
Epoch 39: training loss 43561133176.471
Test Loss of 33552643031.722351, Test MSE of 33552643645.885643
Epoch 40: training loss 40648224722.824
Test Loss of 34347456338.569180, Test MSE of 34347455821.872456
Epoch 41: training loss 38474697095.529
Test Loss of 29339781428.005554, Test MSE of 29339782146.692741
Epoch 42: training loss 37365941677.176
Test Loss of 30877583332.516426, Test MSE of 30877583268.691433
Epoch 43: training loss 35105403648.000
Test Loss of 29727997091.479870, Test MSE of 29727997426.593613
Epoch 44: training loss 33746791183.059
Test Loss of 25676914523.098564, Test MSE of 25676914620.471077
Epoch 45: training loss 31943060043.294
Test Loss of 25516486154.661732, Test MSE of 25516485849.019993
Epoch 46: training loss 30367869116.235
Test Loss of 23982297863.226284, Test MSE of 23982297972.224968
Epoch 47: training loss 28722746119.529
Test Loss of 25883035249.962055, Test MSE of 25883035550.926441
Epoch 48: training loss 27618074386.824
Test Loss of 21850559376.644146, Test MSE of 21850559308.268906
Epoch 49: training loss 26109426179.765
Test Loss of 23270881121.732533, Test MSE of 23270881099.199150
Epoch 50: training loss 25365536278.588
Test Loss of 21614006476.705231, Test MSE of 21614006801.693287
Epoch 51: training loss 24041949146.353
Test Loss of 21822699359.837112, Test MSE of 21822699401.622017
Epoch 52: training loss 23294227640.471
Test Loss of 21328178759.315132, Test MSE of 21328178488.981674
Epoch 53: training loss 22338292171.294
Test Loss of 19185229174.819065, Test MSE of 19185229168.924217
Epoch 54: training loss 21480196039.529
Test Loss of 20451128041.847294, Test MSE of 20451128619.223217
Epoch 55: training loss 20831512941.176
Test Loss of 19616250954.395187, Test MSE of 19616251093.666893
Epoch 56: training loss 20105104256.000
Test Loss of 20283586917.286442, Test MSE of 20283587020.977310
Epoch 57: training loss 19393965248.000
Test Loss of 18799051988.760757, Test MSE of 18799052203.911392
Epoch 58: training loss 18498436348.235
Test Loss of 18626275646.430355, Test MSE of 18626275690.262699
Epoch 59: training loss 17969878878.118
Test Loss of 17213355811.183712, Test MSE of 17213355868.734779
Epoch 60: training loss 17436045507.765
Test Loss of 18237255245.001389, Test MSE of 18237255215.730038
Epoch 61: training loss 16904097611.294
Test Loss of 19309995228.342434, Test MSE of 19309995336.854763
Epoch 62: training loss 16487152530.824
Test Loss of 18562348104.499767, Test MSE of 18562347842.884575
Epoch 63: training loss 16061704589.176
Test Loss of 19022613374.637669, Test MSE of 19022613342.660694
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22506127007.508186, 'MSE - std': 2626232051.916982, 'R2 - mean': 0.8333786426704411, 'R2 - std': 0.01241847359237401} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005340 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042974629.647
Test Loss of 431614072252.949585, Test MSE of 431614069111.668213
Epoch 2: training loss 424022629436.235
Test Loss of 431593578469.464111, Test MSE of 431593576416.089966
Epoch 3: training loss 423995082992.941
Test Loss of 431565936333.889893, Test MSE of 431565940628.221680
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012893967.059
Test Loss of 431568841798.604370, Test MSE of 431568842710.598999
Epoch 2: training loss 424000596208.941
Test Loss of 431569962521.825073, Test MSE of 431569968508.775269
Epoch 3: training loss 423999910128.941
Test Loss of 431570217902.970825, Test MSE of 431570222824.476196
Epoch 4: training loss 423999383672.471
Test Loss of 431568833307.128174, Test MSE of 431568835819.427734
Epoch 5: training loss 417238445236.706
Test Loss of 409483843899.587219, Test MSE of 409483847201.560974
Epoch 6: training loss 371935941812.706
Test Loss of 341663903594.261902, Test MSE of 341663899771.266785
Epoch 7: training loss 293399208658.824
Test Loss of 254632934840.210999, Test MSE of 254632935811.127960
Epoch 8: training loss 215074348152.471
Test Loss of 183227683905.865814, Test MSE of 183227685862.217102
Epoch 9: training loss 156749581643.294
Test Loss of 132095392374.226746, Test MSE of 132095392091.449585
Epoch 10: training loss 138093276491.294
Test Loss of 123899799660.986588, Test MSE of 123899798815.565399
Epoch 11: training loss 133487992832.000
Test Loss of 119853391640.758911, Test MSE of 119853396240.167450
Epoch 12: training loss 130758899742.118
Test Loss of 117461019058.998611, Test MSE of 117461020618.911209
Epoch 13: training loss 126951779267.765
Test Loss of 113090330385.177231, Test MSE of 113090329333.524933
Epoch 14: training loss 124060505509.647
Test Loss of 109784642311.226288, Test MSE of 109784645218.877350
Epoch 15: training loss 120155762657.882
Test Loss of 107657498925.371582, Test MSE of 107657498156.338013
Epoch 16: training loss 116216341594.353
Test Loss of 102870246012.860718, Test MSE of 102870246006.147873
Epoch 17: training loss 112645212220.235
Test Loss of 99705740887.426193, Test MSE of 99705741463.095871
Epoch 18: training loss 108894218691.765
Test Loss of 96980942321.073578, Test MSE of 96980942171.046982
Epoch 19: training loss 104782953622.588
Test Loss of 91205935759.814896, Test MSE of 91205936686.940704
Epoch 20: training loss 101114218315.294
Test Loss of 88141904809.284592, Test MSE of 88141903699.164337
Epoch 21: training loss 96994174192.941
Test Loss of 85895358319.000458, Test MSE of 85895356944.586868
Epoch 22: training loss 92750292148.706
Test Loss of 81583620888.285049, Test MSE of 81583620569.380692
Epoch 23: training loss 88925859162.353
Test Loss of 78389166627.776031, Test MSE of 78389166565.795547
Epoch 24: training loss 86886525168.941
Test Loss of 76890640061.778809, Test MSE of 76890641334.825409
Epoch 25: training loss 83236609227.294
Test Loss of 73479557421.371582, Test MSE of 73479559407.458359
Epoch 26: training loss 78800153509.647
Test Loss of 70473213926.411850, Test MSE of 70473213916.052383
Epoch 27: training loss 74688882944.000
Test Loss of 66421333115.676079, Test MSE of 66421332422.997047
Epoch 28: training loss 71448007951.059
Test Loss of 61406995148.468300, Test MSE of 61406993230.506012
Epoch 29: training loss 68620848286.118
Test Loss of 58442758508.868118, Test MSE of 58442759540.912613
Epoch 30: training loss 65160807936.000
Test Loss of 59488509392.851456, Test MSE of 59488510063.229385
Epoch 31: training loss 62149945012.706
Test Loss of 55275004802.428505, Test MSE of 55275005314.629250
Epoch 32: training loss 59155971576.471
Test Loss of 48922472190.223045, Test MSE of 48922472167.036362
Epoch 33: training loss 56340975872.000
Test Loss of 50723235054.348915, Test MSE of 50723234007.360947
Epoch 34: training loss 53725411124.706
Test Loss of 45673228973.193893, Test MSE of 45673228285.001160
Epoch 35: training loss 51276448880.941
Test Loss of 48530959216.422028, Test MSE of 48530959968.522202
Epoch 36: training loss 48844705468.235
Test Loss of 42186917538.295235, Test MSE of 42186918020.876038
Epoch 37: training loss 46100344538.353
Test Loss of 38592784671.155945, Test MSE of 38592784533.333549
Epoch 38: training loss 44077063085.176
Test Loss of 39032865857.865807, Test MSE of 39032865531.547577
Epoch 39: training loss 41696121697.882
Test Loss of 35047002080.725594, Test MSE of 35047001893.678047
Epoch 40: training loss 39302364732.235
Test Loss of 33341334625.140213, Test MSE of 33341334892.682388
Epoch 41: training loss 37880332555.294
Test Loss of 32125248850.806107, Test MSE of 32125248530.238468
Epoch 42: training loss 35839872045.176
Test Loss of 30439922751.970383, Test MSE of 30439923054.832783
Epoch 43: training loss 33792276096.000
Test Loss of 24823690045.245720, Test MSE of 24823690104.520294
Epoch 44: training loss 32146273987.765
Test Loss of 28823325718.271172, Test MSE of 28823325739.063080
Epoch 45: training loss 30598708818.824
Test Loss of 28136341692.594170, Test MSE of 28136342463.316509
Epoch 46: training loss 29542307644.235
Test Loss of 25892068663.322536, Test MSE of 25892068660.660782
Epoch 47: training loss 28340362469.647
Test Loss of 24879140689.621471, Test MSE of 24879140915.678661
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22980729789.14228, 'MSE - std': 2533508838.3015656, 'R2 - mean': 0.8295432445259389, 'R2 - std': 0.013498737155711947} 
 

Saving model.....
Results After CV: {'MSE - mean': 22980729789.14228, 'MSE - std': 2533508838.3015656, 'R2 - mean': 0.8295432445259389, 'R2 - std': 0.013498737155711947}
Train time: 88.33138497259934
Inference time: 0.0707319187989924
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 63 finished with value: 22980729789.14228 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005377 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526013168.941
Test Loss of 418113313949.993958, Test MSE of 418113311516.862915
Epoch 2: training loss 427506508860.235
Test Loss of 418096161452.087891, Test MSE of 418096168149.751404
Epoch 3: training loss 427480090864.941
Test Loss of 418073101671.335632, Test MSE of 418073102283.868713
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496867117.176
Test Loss of 418077455562.999756, Test MSE of 418077463817.686768
Epoch 2: training loss 427485746597.647
Test Loss of 418078980499.157043, Test MSE of 418078979945.910950
Epoch 3: training loss 427485233754.353
Test Loss of 418078590512.203552, Test MSE of 418078592167.604675
Epoch 4: training loss 422229677598.118
Test Loss of 401388961450.903564, Test MSE of 401388962824.972839
Epoch 5: training loss 386147346070.588
Test Loss of 346303190880.347900, Test MSE of 346303189183.877991
Epoch 6: training loss 318528937261.176
Test Loss of 269894638598.395569, Test MSE of 269894640141.624115
Epoch 7: training loss 226930986586.353
Test Loss of 171747145892.389557, Test MSE of 171747146336.090546
Epoch 8: training loss 161754939738.353
Test Loss of 131587831225.767288, Test MSE of 131587830476.813492
Epoch 9: training loss 141467522710.588
Test Loss of 120896420819.231094, Test MSE of 120896422568.014023
Epoch 10: training loss 137301715275.294
Test Loss of 116039810742.273422, Test MSE of 116039809052.651428
Epoch 11: training loss 133151750701.176
Test Loss of 113539729734.410370, Test MSE of 113539729593.093048
Epoch 12: training loss 131355253338.353
Test Loss of 110851475645.971771, Test MSE of 110851475681.441940
Epoch 13: training loss 127215115836.235
Test Loss of 107301770681.767288, Test MSE of 107301770971.089493
Epoch 14: training loss 122665487691.294
Test Loss of 104547158340.515381, Test MSE of 104547159132.915359
Epoch 15: training loss 119369412171.294
Test Loss of 101002811188.763351, Test MSE of 101002811531.079254
Epoch 16: training loss 116047239559.529
Test Loss of 97866225600.991898, Test MSE of 97866225949.645325
Epoch 17: training loss 112791562089.412
Test Loss of 94190730932.378448, Test MSE of 94190731228.606033
Epoch 18: training loss 108815157594.353
Test Loss of 91519633774.204956, Test MSE of 91519633541.555283
Epoch 19: training loss 105626310113.882
Test Loss of 86895964160.947495, Test MSE of 86895964979.477432
Epoch 20: training loss 101203090221.176
Test Loss of 85105573325.190842, Test MSE of 85105573794.732452
Epoch 21: training loss 97235065856.000
Test Loss of 82573752340.371033, Test MSE of 82573753026.650787
Epoch 22: training loss 93306268032.000
Test Loss of 80253708608.725418, Test MSE of 80253709746.954803
Epoch 23: training loss 89987227648.000
Test Loss of 75398266889.474899, Test MSE of 75398268374.661423
Epoch 24: training loss 86631706624.000
Test Loss of 74773981970.890579, Test MSE of 74773982988.148285
Epoch 25: training loss 82819660792.471
Test Loss of 69738248697.486008, Test MSE of 69738249799.585846
Epoch 26: training loss 79574570526.118
Test Loss of 67681814431.829750, Test MSE of 67681816401.245865
Epoch 27: training loss 76383043840.000
Test Loss of 66410387237.603516, Test MSE of 66410387268.430283
Epoch 28: training loss 73041893888.000
Test Loss of 62579058686.578766, Test MSE of 62579059042.115494
Epoch 29: training loss 70093448037.647
Test Loss of 59039399475.519775, Test MSE of 59039399672.160942
Epoch 30: training loss 66569819256.471
Test Loss of 55449283311.833450, Test MSE of 55449282459.956863
Epoch 31: training loss 62828659817.412
Test Loss of 54907943422.223457, Test MSE of 54907942903.810638
Epoch 32: training loss 60349088960.000
Test Loss of 50790838608.359009, Test MSE of 50790837695.206566
Epoch 33: training loss 57413430704.941
Test Loss of 49059680190.623177, Test MSE of 49059680254.938751
Epoch 34: training loss 54720335947.294
Test Loss of 47545016378.744392, Test MSE of 47545016498.842278
Epoch 35: training loss 52174753686.588
Test Loss of 43829980087.753876, Test MSE of 43829979770.232574
Epoch 36: training loss 50349739030.588
Test Loss of 41294678369.887581, Test MSE of 41294677522.106552
Epoch 37: training loss 47031700540.235
Test Loss of 41249650931.031227, Test MSE of 41249651011.365639
Epoch 38: training loss 45187231209.412
Test Loss of 40694155743.903770, Test MSE of 40694155053.098297
Epoch 39: training loss 43103928342.588
Test Loss of 34311551694.671291, Test MSE of 34311552086.389942
Epoch 40: training loss 40667278787.765
Test Loss of 36367751174.158684, Test MSE of 36367751453.612686
Epoch 41: training loss 39074627343.059
Test Loss of 32330209069.183437, Test MSE of 32330209452.105171
Epoch 42: training loss 37416266281.412
Test Loss of 28876073719.413372, Test MSE of 28876073997.814476
Epoch 43: training loss 35577901948.235
Test Loss of 32500720951.013649, Test MSE of 32500720765.851494
Epoch 44: training loss 34012972747.294
Test Loss of 31166788030.267868, Test MSE of 31166788769.230114
Epoch 45: training loss 32510780724.706
Test Loss of 27464468940.953968, Test MSE of 27464469229.744350
Epoch 46: training loss 30879239416.471
Test Loss of 29242500487.550312, Test MSE of 29242500148.729477
Epoch 47: training loss 29220847555.765
Test Loss of 25637552732.972473, Test MSE of 25637552651.178226
Epoch 48: training loss 28614646102.588
Test Loss of 24176256624.632893, Test MSE of 24176256737.410618
Epoch 49: training loss 27298456643.765
Test Loss of 24484805185.495258, Test MSE of 24484805077.655045
Epoch 50: training loss 26274603226.353
Test Loss of 24203293266.076336, Test MSE of 24203292782.407276
Epoch 51: training loss 25464625475.765
Test Loss of 21413776256.325699, Test MSE of 21413776358.957474
Epoch 52: training loss 23951461696.000
Test Loss of 23031768264.631042, Test MSE of 23031768403.444679
Epoch 53: training loss 23075725624.471
Test Loss of 22817793085.349987, Test MSE of 22817792891.835690
Epoch 54: training loss 21892892393.412
Test Loss of 22479614886.699051, Test MSE of 22479614901.250870
Epoch 55: training loss 21709907429.647
Test Loss of 20307706433.021511, Test MSE of 20307706440.338451
Epoch 56: training loss 20283101741.176
Test Loss of 20873585162.067081, Test MSE of 20873585379.204922
Epoch 57: training loss 20037726309.647
Test Loss of 19274677242.078186, Test MSE of 19274677206.385738
Epoch 58: training loss 19155038422.588
Test Loss of 21442160261.003933, Test MSE of 21442160619.400223
Epoch 59: training loss 18883396672.000
Test Loss of 20712657077.444366, Test MSE of 20712657570.772755
Epoch 60: training loss 17972108231.529
Test Loss of 19869049522.246590, Test MSE of 19869049101.937153
Epoch 61: training loss 17440906232.471
Test Loss of 18368741424.795742, Test MSE of 18368741256.477276
Epoch 62: training loss 16858685376.000
Test Loss of 17953936057.826508, Test MSE of 17953936347.483379
Epoch 63: training loss 16489749214.118
Test Loss of 19093211265.806152, Test MSE of 19093211133.380211
Epoch 64: training loss 16293862919.529
Test Loss of 20280224343.761276, Test MSE of 20280224489.046421
Epoch 65: training loss 15683327111.529
Test Loss of 19268765276.261856, Test MSE of 19268765637.055885
Epoch 66: training loss 15232007676.235
Test Loss of 18653676001.798752, Test MSE of 18653676479.497486
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18653676479.497486, 'MSE - std': 0.0, 'R2 - mean': 0.8547417973456501, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005433 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918094817.882
Test Loss of 424557221663.918579, Test MSE of 424557222969.585876
Epoch 2: training loss 427898423175.529
Test Loss of 424541435007.200562, Test MSE of 424541431519.269653
Epoch 3: training loss 427871886396.235
Test Loss of 424519650631.594727, Test MSE of 424519651952.795532
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887973315.765
Test Loss of 424526712639.659485, Test MSE of 424526711289.957581
Epoch 2: training loss 427879280880.941
Test Loss of 424527862346.970154, Test MSE of 424527860545.919067
Epoch 3: training loss 427878789360.941
Test Loss of 424527398660.441345, Test MSE of 424527402891.021484
Epoch 4: training loss 422627411365.647
Test Loss of 408265910920.557007, Test MSE of 408265921118.744873
Epoch 5: training loss 386441775465.412
Test Loss of 354405701055.452209, Test MSE of 354405696817.768433
Epoch 6: training loss 318418519702.588
Test Loss of 277982141419.865845, Test MSE of 277982142728.358337
Epoch 7: training loss 225477458221.176
Test Loss of 180726899459.256989, Test MSE of 180726899499.341705
Epoch 8: training loss 158785993968.941
Test Loss of 142485612144.632904, Test MSE of 142485614306.278778
Epoch 9: training loss 141024927051.294
Test Loss of 132080043562.518616, Test MSE of 132080043182.105865
Epoch 10: training loss 134952902987.294
Test Loss of 127725333640.201706, Test MSE of 127725334662.130356
Epoch 11: training loss 130543883474.824
Test Loss of 124982585314.154053, Test MSE of 124982584631.565094
Epoch 12: training loss 128652043655.529
Test Loss of 122283623690.481613, Test MSE of 122283623796.764252
Epoch 13: training loss 124812576256.000
Test Loss of 118751765751.531799, Test MSE of 118751763549.776947
Epoch 14: training loss 120243703055.059
Test Loss of 114946764639.874161, Test MSE of 114946763883.521713
Epoch 15: training loss 117047398972.235
Test Loss of 111888589491.667816, Test MSE of 111888592027.367279
Epoch 16: training loss 112953164559.059
Test Loss of 108278639517.224152, Test MSE of 108278639152.704620
Epoch 17: training loss 110328729389.176
Test Loss of 103628037332.948410, Test MSE of 103628037994.312775
Epoch 18: training loss 106259375917.176
Test Loss of 99675552225.798752, Test MSE of 99675552678.608185
Epoch 19: training loss 101491993569.882
Test Loss of 96339966594.872086, Test MSE of 96339965986.406799
Epoch 20: training loss 97062032745.412
Test Loss of 94411385905.032623, Test MSE of 94411384059.097702
Epoch 21: training loss 94113337976.471
Test Loss of 89654335519.740921, Test MSE of 89654336313.560944
Epoch 22: training loss 89905619456.000
Test Loss of 85970448031.533661, Test MSE of 85970447833.194672
Epoch 23: training loss 86410908129.882
Test Loss of 80879787551.622482, Test MSE of 80879787840.361435
Epoch 24: training loss 82324157018.353
Test Loss of 78946458267.506821, Test MSE of 78946457471.479767
Epoch 25: training loss 78759724679.529
Test Loss of 75043950325.755264, Test MSE of 75043951681.824371
Epoch 26: training loss 75113191604.706
Test Loss of 72083770415.137634, Test MSE of 72083768520.798523
Epoch 27: training loss 72694911774.118
Test Loss of 67459876315.876938, Test MSE of 67459878059.035477
Epoch 28: training loss 68306156348.235
Test Loss of 64222177541.981033, Test MSE of 64222176007.572205
Epoch 29: training loss 65401291730.824
Test Loss of 62934985362.979416, Test MSE of 62934985717.135040
Epoch 30: training loss 62541944545.882
Test Loss of 58748342578.039322, Test MSE of 58748343045.178307
Epoch 31: training loss 59594995712.000
Test Loss of 55687627780.974327, Test MSE of 55687627729.357948
Epoch 32: training loss 56016110742.588
Test Loss of 52872376645.462875, Test MSE of 52872377643.430717
Epoch 33: training loss 53851299764.706
Test Loss of 51686469419.288460, Test MSE of 51686469249.296005
Epoch 34: training loss 50500202066.824
Test Loss of 49590445703.846405, Test MSE of 49590443490.072403
Epoch 35: training loss 47697223446.588
Test Loss of 48941729663.615082, Test MSE of 48941729142.394157
Epoch 36: training loss 45647682522.353
Test Loss of 44955505719.191299, Test MSE of 44955504485.583771
Epoch 37: training loss 43397226142.118
Test Loss of 41505382706.513069, Test MSE of 41505383689.308754
Epoch 38: training loss 40389323700.706
Test Loss of 39647862902.436272, Test MSE of 39647862715.390457
Epoch 39: training loss 38883967488.000
Test Loss of 38882869170.305809, Test MSE of 38882869696.894806
Epoch 40: training loss 36774392274.824
Test Loss of 37027168028.128616, Test MSE of 37027167894.710678
Epoch 41: training loss 34853218876.235
Test Loss of 34527378653.949570, Test MSE of 34527378938.642700
Epoch 42: training loss 32993953325.176
Test Loss of 34701306494.134628, Test MSE of 34701306689.553238
Epoch 43: training loss 31497896839.529
Test Loss of 34486072224.066620, Test MSE of 34486072335.121460
Epoch 44: training loss 30221478814.118
Test Loss of 32567638234.396484, Test MSE of 32567638338.488064
Epoch 45: training loss 28359419203.765
Test Loss of 34778311442.890587, Test MSE of 34778311117.297569
Epoch 46: training loss 26888622238.118
Test Loss of 31766356310.517696, Test MSE of 31766356980.211624
Epoch 47: training loss 26152173349.647
Test Loss of 29527362460.750404, Test MSE of 29527361743.813309
Epoch 48: training loss 24567484129.882
Test Loss of 28521931637.903309, Test MSE of 28521931724.221062
Epoch 49: training loss 23714443700.706
Test Loss of 30954013842.150360, Test MSE of 30954013319.890854
Epoch 50: training loss 22041787888.941
Test Loss of 28858525826.279900, Test MSE of 28858525862.349461
Epoch 51: training loss 21389400726.588
Test Loss of 28232248499.786259, Test MSE of 28232247640.153820
Epoch 52: training loss 20703122488.471
Test Loss of 28955386886.395557, Test MSE of 28955386420.819702
Epoch 53: training loss 19869866635.294
Test Loss of 25902285261.427711, Test MSE of 25902285126.565678
Epoch 54: training loss 18931379102.118
Test Loss of 28240348760.945641, Test MSE of 28240348549.613491
Epoch 55: training loss 18067279695.059
Test Loss of 27021914438.410362, Test MSE of 27021914594.440987
Epoch 56: training loss 17656456670.118
Test Loss of 26849021354.607449, Test MSE of 26849021664.074238
Epoch 57: training loss 17103304809.412
Test Loss of 26370092407.442978, Test MSE of 26370092254.400120
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22511884366.948803, 'MSE - std': 3858207887.451317, 'R2 - mean': 0.8332384232508347, 'R2 - std': 0.021503374094815342} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927036205.176
Test Loss of 447259114527.030273, Test MSE of 447259119066.900879
Epoch 2: training loss 421905556178.824
Test Loss of 447240424881.713623, Test MSE of 447240427684.369934
Epoch 3: training loss 421877825656.471
Test Loss of 447215958381.257446, Test MSE of 447215960118.180664
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899810454.588
Test Loss of 447224736157.579468, Test MSE of 447224737854.058655
Epoch 2: training loss 421888985088.000
Test Loss of 447225285004.524658, Test MSE of 447225276229.280151
Epoch 3: training loss 421888391770.353
Test Loss of 447224761595.795532, Test MSE of 447224767702.635437
Epoch 4: training loss 416446448218.353
Test Loss of 429892612655.966675, Test MSE of 429892614380.421082
Epoch 5: training loss 380039671085.176
Test Loss of 374609893686.303040, Test MSE of 374609893123.778564
Epoch 6: training loss 312115703567.059
Test Loss of 295487825962.163330, Test MSE of 295487828502.923889
Epoch 7: training loss 221463254738.824
Test Loss of 195739453180.861450, Test MSE of 195739449305.876068
Epoch 8: training loss 156034802537.412
Test Loss of 154216700312.842010, Test MSE of 154216702133.029663
Epoch 9: training loss 137638629074.824
Test Loss of 142855238267.765900, Test MSE of 142855236336.296539
Epoch 10: training loss 132029839179.294
Test Loss of 137891823957.096466, Test MSE of 137891822742.263733
Epoch 11: training loss 129607085899.294
Test Loss of 134036970180.248901, Test MSE of 134036970464.832184
Epoch 12: training loss 125139656583.529
Test Loss of 130293260570.352066, Test MSE of 130293259878.782990
Epoch 13: training loss 122223369306.353
Test Loss of 127991686088.334946, Test MSE of 127991683720.060394
Epoch 14: training loss 118630072350.118
Test Loss of 124832257293.324081, Test MSE of 124832256591.796936
Epoch 15: training loss 114852455273.412
Test Loss of 121162693905.587784, Test MSE of 121162697057.104401
Epoch 16: training loss 110537931504.941
Test Loss of 118289474856.801300, Test MSE of 118289475518.705093
Epoch 17: training loss 107131089920.000
Test Loss of 113924341377.213974, Test MSE of 113924341891.294464
Epoch 18: training loss 102861621368.471
Test Loss of 107931708116.356232, Test MSE of 107931708563.504593
Epoch 19: training loss 99412234812.235
Test Loss of 105504479352.804993, Test MSE of 105504479726.417847
Epoch 20: training loss 95428710791.529
Test Loss of 102064489710.056900, Test MSE of 102064488171.926895
Epoch 21: training loss 91764926735.059
Test Loss of 97498005166.693497, Test MSE of 97498005613.738678
Epoch 22: training loss 88418282752.000
Test Loss of 94606764424.734680, Test MSE of 94606764741.430374
Epoch 23: training loss 84335309643.294
Test Loss of 88929766460.402496, Test MSE of 88929765708.045670
Epoch 24: training loss 81455152911.059
Test Loss of 83400500706.509369, Test MSE of 83400500982.412766
Epoch 25: training loss 78274623156.706
Test Loss of 85900761182.275269, Test MSE of 85900760678.286041
Epoch 26: training loss 73726927962.353
Test Loss of 80709830537.089981, Test MSE of 80709831277.692368
Epoch 27: training loss 70790044144.941
Test Loss of 77222647032.479294, Test MSE of 77222647342.296112
Epoch 28: training loss 66459117372.235
Test Loss of 71632221108.437653, Test MSE of 71632221570.685440
Epoch 29: training loss 64138992715.294
Test Loss of 69336219354.278046, Test MSE of 69336219496.400986
Epoch 30: training loss 61557228784.941
Test Loss of 65044313406.593567, Test MSE of 65044312212.974411
Epoch 31: training loss 58599923215.059
Test Loss of 60983338313.015961, Test MSE of 60983338068.359840
Epoch 32: training loss 54935893760.000
Test Loss of 59941386620.417305, Test MSE of 59941386709.629250
Epoch 33: training loss 52738639488.000
Test Loss of 59086907019.162621, Test MSE of 59086906702.294167
Epoch 34: training loss 50981151013.647
Test Loss of 53105292117.688644, Test MSE of 53105292257.144341
Epoch 35: training loss 48102155602.824
Test Loss of 54551176661.244507, Test MSE of 54551176624.348038
Epoch 36: training loss 45285234469.647
Test Loss of 49573632055.428177, Test MSE of 49573631949.372147
Epoch 37: training loss 43089961103.059
Test Loss of 45711551551.244972, Test MSE of 45711552066.149864
Epoch 38: training loss 40200113272.471
Test Loss of 46033535582.393707, Test MSE of 46033535073.922791
Epoch 39: training loss 39064644525.176
Test Loss of 44585152477.890350, Test MSE of 44585152977.971199
Epoch 40: training loss 37096676336.941
Test Loss of 41857797858.805458, Test MSE of 41857797794.041565
Epoch 41: training loss 34813377822.118
Test Loss of 41760050295.146889, Test MSE of 41760051791.821220
Epoch 42: training loss 33399034240.000
Test Loss of 39213018988.191536, Test MSE of 39213018800.257057
Epoch 43: training loss 31816471254.588
Test Loss of 39662765029.707146, Test MSE of 39662764887.219826
Epoch 44: training loss 29744591600.941
Test Loss of 36202236229.462875, Test MSE of 36202236364.860367
Epoch 45: training loss 29011740363.294
Test Loss of 32592059180.472820, Test MSE of 32592059538.787880
Epoch 46: training loss 27934719736.471
Test Loss of 29981474059.902843, Test MSE of 29981473851.195381
Epoch 47: training loss 26413143608.471
Test Loss of 31419565090.346519, Test MSE of 31419564897.577785
Epoch 48: training loss 25101355192.471
Test Loss of 29816033040.284988, Test MSE of 29816033248.685055
Epoch 49: training loss 24296793099.294
Test Loss of 31194668658.527874, Test MSE of 31194668926.785866
Epoch 50: training loss 23537096975.059
Test Loss of 29167735656.638447, Test MSE of 29167735964.094131
Epoch 51: training loss 22274751559.529
Test Loss of 27683170465.547073, Test MSE of 27683170568.727535
Epoch 52: training loss 21245382524.235
Test Loss of 26588178950.277122, Test MSE of 26588179019.626553
Epoch 53: training loss 20585570834.824
Test Loss of 24714739438.649086, Test MSE of 24714739420.476933
Epoch 54: training loss 19918882503.529
Test Loss of 26148730562.590794, Test MSE of 26148730544.662117
Epoch 55: training loss 19472087405.176
Test Loss of 22904546566.217903, Test MSE of 22904546452.959324
Epoch 56: training loss 18472439247.059
Test Loss of 27636599833.819107, Test MSE of 27636600026.046841
Epoch 57: training loss 17968137735.529
Test Loss of 25957509497.101086, Test MSE of 25957510010.780880
Epoch 58: training loss 17106302580.706
Test Loss of 23685894120.312744, Test MSE of 23685894090.087822
Epoch 59: training loss 16645776549.647
Test Loss of 22592044532.985428, Test MSE of 22592044100.832443
Epoch 60: training loss 16278613266.824
Test Loss of 23017228072.919731, Test MSE of 23017227994.184475
Epoch 61: training loss 16160597601.882
Test Loss of 20007668909.627575, Test MSE of 20007668699.592960
Epoch 62: training loss 15433666093.176
Test Loss of 22550991744.799446, Test MSE of 22550991499.907970
Epoch 63: training loss 14904514962.824
Test Loss of 22253181333.052048, Test MSE of 22253181002.697716
Epoch 64: training loss 14303790832.941
Test Loss of 19995208378.300255, Test MSE of 19995208963.960087
Epoch 65: training loss 14102897532.235
Test Loss of 21076826405.721951, Test MSE of 21076825781.118668
Epoch 66: training loss 13692094753.882
Test Loss of 21276193138.942402, Test MSE of 21276193524.464977
Epoch 67: training loss 13393913223.529
Test Loss of 21598081002.207726, Test MSE of 21598081315.684505
Epoch 68: training loss 13058530936.471
Test Loss of 21305927176.882721, Test MSE of 21305927598.475014
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22109898777.45754, 'MSE - std': 3201098284.9919357, 'R2 - mean': 0.8415482308485505, 'R2 - std': 0.021127451406400783} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005430 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109517944.471
Test Loss of 410763241042.687622, Test MSE of 410763239615.702209
Epoch 2: training loss 430087790351.059
Test Loss of 410745022201.484497, Test MSE of 410745021773.903870
Epoch 3: training loss 430059875147.294
Test Loss of 410721497036.823669, Test MSE of 410721493061.334595
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076786808.471
Test Loss of 410724760417.258667, Test MSE of 410724755749.595093
Epoch 2: training loss 430065230546.824
Test Loss of 410725384980.494202, Test MSE of 410725381396.757080
Epoch 3: training loss 430064765048.471
Test Loss of 410725411546.683960, Test MSE of 410725412057.902649
Epoch 4: training loss 424488986744.471
Test Loss of 393283396634.062012, Test MSE of 393283396057.588928
Epoch 5: training loss 387447676928.000
Test Loss of 337241073606.189697, Test MSE of 337241070213.217163
Epoch 6: training loss 319558570947.765
Test Loss of 261529803567.977783, Test MSE of 261529805306.748077
Epoch 7: training loss 227652459339.294
Test Loss of 164165599626.247101, Test MSE of 164165599751.314392
Epoch 8: training loss 163177181093.647
Test Loss of 124544384099.035629, Test MSE of 124544383530.649811
Epoch 9: training loss 144753496997.647
Test Loss of 114848611760.629333, Test MSE of 114848613205.353729
Epoch 10: training loss 138106048271.059
Test Loss of 110341872049.103195, Test MSE of 110341870817.554718
Epoch 11: training loss 134880748815.059
Test Loss of 107338007335.448410, Test MSE of 107338005545.459091
Epoch 12: training loss 131175390599.529
Test Loss of 104602228494.334106, Test MSE of 104602229214.133881
Epoch 13: training loss 127685529901.176
Test Loss of 101535139285.590012, Test MSE of 101535138788.521225
Epoch 14: training loss 125382891520.000
Test Loss of 97587734595.287369, Test MSE of 97587735119.552612
Epoch 15: training loss 119508710219.294
Test Loss of 95514459576.211014, Test MSE of 95514458445.995438
Epoch 16: training loss 116642980803.765
Test Loss of 92250807971.242950, Test MSE of 92250807153.130814
Epoch 17: training loss 112335557451.294
Test Loss of 89018206309.404907, Test MSE of 89018207762.343155
Epoch 18: training loss 108725609923.765
Test Loss of 86265762711.278107, Test MSE of 86265763011.456436
Epoch 19: training loss 104141234085.647
Test Loss of 81665147149.149475, Test MSE of 81665147116.346024
Epoch 20: training loss 101639475139.765
Test Loss of 78929996821.323456, Test MSE of 78929996384.342987
Epoch 21: training loss 96468716062.118
Test Loss of 76067519469.519669, Test MSE of 76067519404.905289
Epoch 22: training loss 91682629827.765
Test Loss of 71172295011.391022, Test MSE of 71172294668.034241
Epoch 23: training loss 88202262121.412
Test Loss of 68109929550.186028, Test MSE of 68109930551.244446
Epoch 24: training loss 84887306300.235
Test Loss of 66138202775.396576, Test MSE of 66138203158.107880
Epoch 25: training loss 80910442691.765
Test Loss of 63625697155.850067, Test MSE of 63625697911.976562
Epoch 26: training loss 76953175311.059
Test Loss of 58914605415.181862, Test MSE of 58914604687.432625
Epoch 27: training loss 73350639691.294
Test Loss of 58126622375.033783, Test MSE of 58126622219.594322
Epoch 28: training loss 70636819832.471
Test Loss of 56285739384.240631, Test MSE of 56285739160.227982
Epoch 29: training loss 67531091433.412
Test Loss of 51398358570.883850, Test MSE of 51398359290.661438
Epoch 30: training loss 64360842721.882
Test Loss of 50952342306.709854, Test MSE of 50952342493.757561
Epoch 31: training loss 61245181153.882
Test Loss of 49925381103.415085, Test MSE of 49925382134.319534
Epoch 32: training loss 58565926000.941
Test Loss of 45242797028.042572, Test MSE of 45242797153.978653
Epoch 33: training loss 55230036397.176
Test Loss of 45401989231.829712, Test MSE of 45401989967.118126
Epoch 34: training loss 52027346191.059
Test Loss of 42305156643.302177, Test MSE of 42305155846.118416
Epoch 35: training loss 49954650383.059
Test Loss of 38497618783.363258, Test MSE of 38497619321.883507
Epoch 36: training loss 47130037496.471
Test Loss of 38785094665.003242, Test MSE of 38785093983.917122
Epoch 37: training loss 44939532513.882
Test Loss of 36349142548.138824, Test MSE of 36349142446.502426
Epoch 38: training loss 42485075267.765
Test Loss of 37528853951.792686, Test MSE of 37528853085.272522
Epoch 39: training loss 40826170714.353
Test Loss of 32694204504.136974, Test MSE of 32694204486.034237
Epoch 40: training loss 38142824839.529
Test Loss of 32000041316.338734, Test MSE of 32000041955.665565
Epoch 41: training loss 36069983608.471
Test Loss of 29160788119.633503, Test MSE of 29160788212.899689
Epoch 42: training loss 34744983190.588
Test Loss of 27993921193.876907, Test MSE of 27993921635.609673
Epoch 43: training loss 32580822151.529
Test Loss of 26863891256.507172, Test MSE of 26863890791.198486
Epoch 44: training loss 31532593295.059
Test Loss of 26089430725.360481, Test MSE of 26089431240.729424
Epoch 45: training loss 30437339158.588
Test Loss of 25841199256.581211, Test MSE of 25841199775.365341
Epoch 46: training loss 29044841148.235
Test Loss of 23647390703.415085, Test MSE of 23647391137.092522
Epoch 47: training loss 27692750930.824
Test Loss of 23727793069.075428, Test MSE of 23727793259.680054
Epoch 48: training loss 26532466665.412
Test Loss of 23991625072.185101, Test MSE of 23991625395.212078
Epoch 49: training loss 25606845018.353
Test Loss of 21946864138.187878, Test MSE of 21946864127.739040
Epoch 50: training loss 24137258337.882
Test Loss of 21032504559.296623, Test MSE of 21032504441.896027
Epoch 51: training loss 23226235843.765
Test Loss of 22653230425.440075, Test MSE of 22653230344.107857
Epoch 52: training loss 22123764434.824
Test Loss of 21953364924.238777, Test MSE of 21953365118.387272
Epoch 53: training loss 21593385517.176
Test Loss of 19689759290.521053, Test MSE of 19689759265.047829
Epoch 54: training loss 20628614674.824
Test Loss of 19873971137.925034, Test MSE of 19873971319.097172
Epoch 55: training loss 20040737505.882
Test Loss of 18872086908.031467, Test MSE of 18872086971.169518
Epoch 56: training loss 19568099011.765
Test Loss of 19079890790.471077, Test MSE of 19079890639.062523
Epoch 57: training loss 19066772399.059
Test Loss of 20583209209.247570, Test MSE of 20583209153.457336
Epoch 58: training loss 18438787478.588
Test Loss of 19635843767.618694, Test MSE of 19635843780.187405
Epoch 59: training loss 17965569867.294
Test Loss of 19596149556.716335, Test MSE of 19596149417.478649
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21481461437.46282, 'MSE - std': 2978266801.939118, 'R2 - mean': 0.8407268646642682, 'R2 - std': 0.018352134192605975} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005304 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043283636.706
Test Loss of 431612819032.373901, Test MSE of 431612821253.510437
Epoch 2: training loss 424023278893.176
Test Loss of 431592693915.424316, Test MSE of 431592689607.696472
Epoch 3: training loss 423995950863.059
Test Loss of 431565062166.744995, Test MSE of 431565071591.459045
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010079533.176
Test Loss of 431570477332.731140, Test MSE of 431570477165.068054
Epoch 2: training loss 423999476675.765
Test Loss of 431571492473.543701, Test MSE of 431571487835.874023
Epoch 3: training loss 423998906368.000
Test Loss of 431571077649.769531, Test MSE of 431571082944.881104
Epoch 4: training loss 418755522800.941
Test Loss of 414472350510.082397, Test MSE of 414472352499.266174
Epoch 5: training loss 382557963324.235
Test Loss of 359207108289.569641, Test MSE of 359207112500.980103
Epoch 6: training loss 315087799175.529
Test Loss of 280684434452.375732, Test MSE of 280684433020.401001
Epoch 7: training loss 223826806272.000
Test Loss of 180173598509.608521, Test MSE of 180173601004.680786
Epoch 8: training loss 160055619192.471
Test Loss of 138445324765.645538, Test MSE of 138445325384.224823
Epoch 9: training loss 141126722665.412
Test Loss of 126123246299.631653, Test MSE of 126123249257.771805
Epoch 10: training loss 136408683791.059
Test Loss of 121933668496.051834, Test MSE of 121933670559.648544
Epoch 11: training loss 132674705227.294
Test Loss of 118148009944.196213, Test MSE of 118148008480.343475
Epoch 12: training loss 129472286810.353
Test Loss of 115620180303.489120, Test MSE of 115620178440.958282
Epoch 13: training loss 124543843915.294
Test Loss of 110963367943.581680, Test MSE of 110963368404.218414
Epoch 14: training loss 121248845447.529
Test Loss of 107600184880.570099, Test MSE of 107600185400.927551
Epoch 15: training loss 118442237048.471
Test Loss of 104073635513.040253, Test MSE of 104073638174.492950
Epoch 16: training loss 114740885338.353
Test Loss of 101139395892.479401, Test MSE of 101139398425.001938
Epoch 17: training loss 110466404080.941
Test Loss of 96492337789.808426, Test MSE of 96492335154.638107
Epoch 18: training loss 106265481818.353
Test Loss of 93089082419.650162, Test MSE of 93089080415.728104
Epoch 19: training loss 101732206682.353
Test Loss of 89951147873.732529, Test MSE of 89951146615.905014
Epoch 20: training loss 98382134151.529
Test Loss of 85888621041.547424, Test MSE of 85888620415.329193
Epoch 21: training loss 95061340792.471
Test Loss of 83721097722.076813, Test MSE of 83721097432.560471
Epoch 22: training loss 91374641347.765
Test Loss of 80600348464.451645, Test MSE of 80600345425.150940
Epoch 23: training loss 87383377242.353
Test Loss of 75820945553.947250, Test MSE of 75820945112.663803
Epoch 24: training loss 84746896338.824
Test Loss of 71621109923.953720, Test MSE of 71621108541.576981
Epoch 25: training loss 79896612758.588
Test Loss of 69048126496.695969, Test MSE of 69048125958.100327
Epoch 26: training loss 76266087476.706
Test Loss of 67833762303.763069, Test MSE of 67833761942.110001
Epoch 27: training loss 73488771568.941
Test Loss of 62669930586.980103, Test MSE of 62669929893.365280
Epoch 28: training loss 69689076796.235
Test Loss of 58783015095.855621, Test MSE of 58783014047.895073
Epoch 29: training loss 66960359002.353
Test Loss of 56919799363.524292, Test MSE of 56919800163.491737
Epoch 30: training loss 63473087518.118
Test Loss of 52182200695.292923, Test MSE of 52182201366.410500
Epoch 31: training loss 60447158256.941
Test Loss of 50302333424.599724, Test MSE of 50302333008.140976
Epoch 32: training loss 57998437059.765
Test Loss of 47571060947.339195, Test MSE of 47571061590.661736
Epoch 33: training loss 55168520779.294
Test Loss of 43959451185.517815, Test MSE of 43959451846.731926
Epoch 34: training loss 52505384835.765
Test Loss of 45282256682.765388, Test MSE of 45282256088.876404
Epoch 35: training loss 49594908513.882
Test Loss of 40220960883.620544, Test MSE of 40220960319.056541
Epoch 36: training loss 48123032794.353
Test Loss of 37153642136.818138, Test MSE of 37153642448.320862
Epoch 37: training loss 45421640553.412
Test Loss of 36209035303.803795, Test MSE of 36209035374.364685
Epoch 38: training loss 43189850518.588
Test Loss of 35593641308.283203, Test MSE of 35593641567.853600
Epoch 39: training loss 41042957921.882
Test Loss of 30881903004.253586, Test MSE of 30881903883.009876
Epoch 40: training loss 38617706375.529
Test Loss of 34352039849.758446, Test MSE of 34352040463.471352
Epoch 41: training loss 36873415378.824
Test Loss of 34656060427.846367, Test MSE of 34656060341.204575
Epoch 42: training loss 35460823627.294
Test Loss of 26465361461.308655, Test MSE of 26465361445.700394
Epoch 43: training loss 33235541827.765
Test Loss of 29330046895.444702, Test MSE of 29330047032.866318
Epoch 44: training loss 32373170755.765
Test Loss of 27719954465.169830, Test MSE of 27719953918.134438
Epoch 45: training loss 31011860525.176
Test Loss of 29595350340.590466, Test MSE of 29595350574.281898
Epoch 46: training loss 29520306187.294
Test Loss of 25553566412.468300, Test MSE of 25553567090.067497
Epoch 47: training loss 28427092367.059
Test Loss of 26453906302.637669, Test MSE of 26453906110.129879
Epoch 48: training loss 27139787203.765
Test Loss of 25434934969.040260, Test MSE of 25434935113.274441
Epoch 49: training loss 25649726467.765
Test Loss of 23685431237.242016, Test MSE of 23685431554.853855
Epoch 50: training loss 25142976399.059
Test Loss of 24154102629.049515, Test MSE of 24154102453.013496
Epoch 51: training loss 24182976843.294
Test Loss of 21240457956.161037, Test MSE of 21240457669.365593
Epoch 52: training loss 22851593946.353
Test Loss of 22743208388.057381, Test MSE of 22743208529.462605
Epoch 53: training loss 22080522296.471
Test Loss of 20463003682.117538, Test MSE of 20463003642.074821
Epoch 54: training loss 21657571312.941
Test Loss of 21041646573.519669, Test MSE of 21041646476.237774
Epoch 55: training loss 20753242326.588
Test Loss of 21079568137.595558, Test MSE of 21079567670.685413
Epoch 56: training loss 19937227666.824
Test Loss of 20053709773.297546, Test MSE of 20053709996.996414
Epoch 57: training loss 19420222923.294
Test Loss of 19928013343.037483, Test MSE of 19928013398.474411
Epoch 58: training loss 18711966817.882
Test Loss of 20756480073.921333, Test MSE of 20756479938.180550
Epoch 59: training loss 18051122017.882
Test Loss of 21223390906.935677, Test MSE of 21223390699.262829
Epoch 60: training loss 17586644182.588
Test Loss of 18673415056.644146, Test MSE of 18673414743.009155
Epoch 61: training loss 17186532976.941
Test Loss of 21415794914.502544, Test MSE of 21415794651.181934
Epoch 62: training loss 16603531094.588
Test Loss of 20083423172.294308, Test MSE of 20083423348.265293
Epoch 63: training loss 16207632135.529
Test Loss of 18524808913.680702, Test MSE of 18524808791.816498
Epoch 64: training loss 16172752225.882
Test Loss of 18732618060.645996, Test MSE of 18732618370.061150
Epoch 65: training loss 15207573338.353
Test Loss of 18957740206.852383, Test MSE of 18957739857.408302
Epoch 66: training loss 15088850627.765
Test Loss of 19528689307.187412, Test MSE of 19528689305.947727
Epoch 67: training loss 14948348713.412
Test Loss of 19894498240.977325, Test MSE of 19894498054.141560
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21164068760.798565, 'MSE - std': 2738432208.3313513, 'R2 - mean': 0.8428669217121966, 'R2 - std': 0.01696349139217175} 
 

Saving model.....
Results After CV: {'MSE - mean': 21164068760.798565, 'MSE - std': 2738432208.3313513, 'R2 - mean': 0.8428669217121966, 'R2 - std': 0.01696349139217175}
Train time: 95.4756177342002
Inference time: 0.07063442059952649
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 64 finished with value: 21164068760.798565 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005495 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526055092.706
Test Loss of 418112109182.134644, Test MSE of 418112109288.456421
Epoch 2: training loss 427504549285.647
Test Loss of 418093193071.507751, Test MSE of 418093197249.385498
Epoch 3: training loss 427475715132.235
Test Loss of 418067806285.220459, Test MSE of 418067813723.672424
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491405101.176
Test Loss of 418073464605.549866, Test MSE of 418073463995.160645
Epoch 2: training loss 427480733334.588
Test Loss of 418074294531.138550, Test MSE of 418074294707.038635
Epoch 3: training loss 427480270607.059
Test Loss of 418074079807.126526, Test MSE of 418074081512.838196
Epoch 4: training loss 427479873295.059
Test Loss of 418073338778.855408, Test MSE of 418073338852.883179
Epoch 5: training loss 420609165071.059
Test Loss of 396280840961.362000, Test MSE of 396280844353.188171
Epoch 6: training loss 374987192560.941
Test Loss of 329395399380.356262, Test MSE of 329395395817.994080
Epoch 7: training loss 296393053424.941
Test Loss of 243760846445.790436, Test MSE of 243760852479.153656
Epoch 8: training loss 217497124562.824
Test Loss of 173302340216.449677, Test MSE of 173302340866.828796
Epoch 9: training loss 159984612894.118
Test Loss of 126392268812.791122, Test MSE of 126392268976.490494
Epoch 10: training loss 139441353758.118
Test Loss of 118457112831.348602, Test MSE of 118457113572.654755
Epoch 11: training loss 136222876792.471
Test Loss of 115826548813.694199, Test MSE of 115826547592.639130
Epoch 12: training loss 131961362130.824
Test Loss of 112820855970.257690, Test MSE of 112820853726.941452
Epoch 13: training loss 128521671424.000
Test Loss of 109612467794.313202, Test MSE of 109612466180.687881
Epoch 14: training loss 126945819527.529
Test Loss of 106433726208.651398, Test MSE of 106433728980.677933
Epoch 15: training loss 121782529897.412
Test Loss of 102501614689.828354, Test MSE of 102501615341.767731
Epoch 16: training loss 117236236438.588
Test Loss of 99921268151.398560, Test MSE of 99921267741.934387
Epoch 17: training loss 113374851493.647
Test Loss of 95582368197.847794, Test MSE of 95582370447.254303
Epoch 18: training loss 111375954793.412
Test Loss of 92063776208.980804, Test MSE of 92063777832.580322
Epoch 19: training loss 105573592289.882
Test Loss of 89763427642.803604, Test MSE of 89763427746.475449
Epoch 20: training loss 101616412762.353
Test Loss of 87200653677.731201, Test MSE of 87200653570.253479
Epoch 21: training loss 100108900291.765
Test Loss of 84126697976.538513, Test MSE of 84126699824.367935
Epoch 22: training loss 94817057581.176
Test Loss of 80776761534.445526, Test MSE of 80776762643.454498
Epoch 23: training loss 91356670885.647
Test Loss of 75499516625.040024, Test MSE of 75499518249.410797
Epoch 24: training loss 87394104259.765
Test Loss of 74009738041.737686, Test MSE of 74009739549.333664
Epoch 25: training loss 84557223589.647
Test Loss of 72073192140.302567, Test MSE of 72073193336.601868
Epoch 26: training loss 80205233716.706
Test Loss of 68087933504.547768, Test MSE of 68087933961.272118
Epoch 27: training loss 77300530262.588
Test Loss of 64142726936.338654, Test MSE of 64142727577.693230
Epoch 28: training loss 72508912775.529
Test Loss of 62635524431.411522, Test MSE of 62635524027.294891
Epoch 29: training loss 70520236717.176
Test Loss of 58684498973.609070, Test MSE of 58684499502.145645
Epoch 30: training loss 66955565063.529
Test Loss of 55515267098.529724, Test MSE of 55515267334.943115
Epoch 31: training loss 63640083712.000
Test Loss of 52134676037.048347, Test MSE of 52134676154.330971
Epoch 32: training loss 60877482443.294
Test Loss of 49021122256.329399, Test MSE of 49021121562.947823
Epoch 33: training loss 57984528892.235
Test Loss of 49137239960.249825, Test MSE of 49137240042.368958
Epoch 34: training loss 54979536850.824
Test Loss of 44898823884.065697, Test MSE of 44898824730.307777
Epoch 35: training loss 52082396385.882
Test Loss of 42394135043.908394, Test MSE of 42394135277.373734
Epoch 36: training loss 48992947290.353
Test Loss of 40535660634.485313, Test MSE of 40535660653.703949
Epoch 37: training loss 47256117330.824
Test Loss of 38758431787.347672, Test MSE of 38758431476.149971
Epoch 38: training loss 45013669278.118
Test Loss of 37777422284.361786, Test MSE of 37777422415.399536
Epoch 39: training loss 42569175457.882
Test Loss of 35399765789.549850, Test MSE of 35399765303.416679
Epoch 40: training loss 40040962657.882
Test Loss of 33522933531.654869, Test MSE of 33522933819.246529
Epoch 41: training loss 38624377031.529
Test Loss of 31639723088.299793, Test MSE of 31639723037.677830
Epoch 42: training loss 36823598230.588
Test Loss of 28957731249.476753, Test MSE of 28957731260.362560
Epoch 43: training loss 34664527898.353
Test Loss of 29956253757.113117, Test MSE of 29956254045.650543
Epoch 44: training loss 33096219791.059
Test Loss of 29003689141.681240, Test MSE of 29003689331.064220
Epoch 45: training loss 31202333078.588
Test Loss of 29504542399.274578, Test MSE of 29504542857.935196
Epoch 46: training loss 30322017076.706
Test Loss of 23502736954.625954, Test MSE of 23502736997.263397
Epoch 47: training loss 28549299508.706
Test Loss of 24907877190.055054, Test MSE of 24907876720.224483
Epoch 48: training loss 27713790373.647
Test Loss of 24449367413.074253, Test MSE of 24449367812.489620
Epoch 49: training loss 26135338014.118
Test Loss of 23309304620.946564, Test MSE of 23309304956.640800
Epoch 50: training loss 24904608184.471
Test Loss of 20448571468.746704, Test MSE of 20448571581.175014
Epoch 51: training loss 24012758765.176
Test Loss of 22521108736.769836, Test MSE of 22521108502.101448
Epoch 52: training loss 22659942524.235
Test Loss of 19595196936.172104, Test MSE of 19595197258.931297
Epoch 53: training loss 21964003361.882
Test Loss of 20546680236.028683, Test MSE of 20546680838.512650
Epoch 54: training loss 21261492069.647
Test Loss of 18898972476.343281, Test MSE of 18898972233.590046
Epoch 55: training loss 20457935009.882
Test Loss of 19424575645.757114, Test MSE of 19424575578.388008
Epoch 56: training loss 19730934720.000
Test Loss of 21716527210.118900, Test MSE of 21716527327.786747
Epoch 57: training loss 19051767740.235
Test Loss of 18626030440.638447, Test MSE of 18626030678.745842
Epoch 58: training loss 18621389379.765
Test Loss of 18399016824.508907, Test MSE of 18399016813.156223
Epoch 59: training loss 17909632444.235
Test Loss of 20914457409.791348, Test MSE of 20914457374.454884
Epoch 60: training loss 17031375864.471
Test Loss of 19474077057.865372, Test MSE of 19474076922.456570
Epoch 61: training loss 16829126400.000
Test Loss of 17854915435.007172, Test MSE of 17854915256.051109
Epoch 62: training loss 16555960237.176
Test Loss of 18329457628.705990, Test MSE of 18329457387.441181
Epoch 63: training loss 15811931561.412
Test Loss of 17605405153.798752, Test MSE of 17605405072.105316
Epoch 64: training loss 15329588965.647
Test Loss of 18627038951.542912, Test MSE of 18627038579.286331
Epoch 65: training loss 14795966789.647
Test Loss of 19218166004.926208, Test MSE of 19218166062.988564
Epoch 66: training loss 14440379068.235
Test Loss of 20421002024.209114, Test MSE of 20421001854.152977
Epoch 67: training loss 14092201185.882
Test Loss of 18781267967.526257, Test MSE of 18781267762.113560
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18781267762.11356, 'MSE - std': 0.0, 'R2 - mean': 0.85374823018437, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005872 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917565470.118
Test Loss of 424557097588.185974, Test MSE of 424557099596.142151
Epoch 2: training loss 427896455649.882
Test Loss of 424539847938.427917, Test MSE of 424539848041.254578
Epoch 3: training loss 427868554902.588
Test Loss of 424517197094.432556, Test MSE of 424517200861.117737
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887871638.588
Test Loss of 424521349329.869080, Test MSE of 424521351756.764648
Epoch 2: training loss 427876505238.588
Test Loss of 424522691582.341919, Test MSE of 424522692520.309814
Epoch 3: training loss 427875952519.529
Test Loss of 424522552248.227600, Test MSE of 424522556629.837830
Epoch 4: training loss 427875514970.353
Test Loss of 424522785292.909546, Test MSE of 424522792771.284424
Epoch 5: training loss 420866070889.412
Test Loss of 402641992579.641907, Test MSE of 402642003074.562439
Epoch 6: training loss 374804078351.059
Test Loss of 336983428055.968567, Test MSE of 336983429405.907837
Epoch 7: training loss 296618639721.412
Test Loss of 252506240661.585022, Test MSE of 252506238946.281708
Epoch 8: training loss 216638401897.412
Test Loss of 183919736007.209808, Test MSE of 183919736402.819061
Epoch 9: training loss 156098247198.118
Test Loss of 137242405842.283600, Test MSE of 137242405975.940521
Epoch 10: training loss 136771781541.647
Test Loss of 129757583314.046722, Test MSE of 129757582849.702133
Epoch 11: training loss 133440139294.118
Test Loss of 127599756673.865372, Test MSE of 127599757035.607773
Epoch 12: training loss 130347012306.824
Test Loss of 123039167960.086975, Test MSE of 123039165998.693130
Epoch 13: training loss 126960916811.294
Test Loss of 120625705838.323380, Test MSE of 120625704648.347412
Epoch 14: training loss 121859750610.824
Test Loss of 116220439506.283600, Test MSE of 116220441585.679932
Epoch 15: training loss 118291241140.706
Test Loss of 113358144888.390472, Test MSE of 113358146148.337402
Epoch 16: training loss 113422744214.588
Test Loss of 110003805860.271103, Test MSE of 110003807395.347565
Epoch 17: training loss 110979957278.118
Test Loss of 105658491533.768219, Test MSE of 105658491935.185471
Epoch 18: training loss 106642804555.294
Test Loss of 103044854137.101089, Test MSE of 103044853532.209534
Epoch 19: training loss 102687543446.588
Test Loss of 97668391330.790649, Test MSE of 97668391271.667999
Epoch 20: training loss 99086017807.059
Test Loss of 94897123010.590790, Test MSE of 94897124032.575516
Epoch 21: training loss 94335066398.118
Test Loss of 89771302355.823273, Test MSE of 89771303202.794098
Epoch 22: training loss 91122982189.176
Test Loss of 86418865375.370804, Test MSE of 86418866859.341400
Epoch 23: training loss 86027413504.000
Test Loss of 81479397557.918106, Test MSE of 81479397561.377289
Epoch 24: training loss 82644204619.294
Test Loss of 78970859260.387695, Test MSE of 78970858274.721222
Epoch 25: training loss 78451938514.824
Test Loss of 76050322595.205185, Test MSE of 76050322777.683823
Epoch 26: training loss 75129135721.412
Test Loss of 73132952062.223450, Test MSE of 73132952168.622681
Epoch 27: training loss 71445246298.353
Test Loss of 66495527572.874390, Test MSE of 66495527602.274551
Epoch 28: training loss 68229566388.706
Test Loss of 65464025465.101089, Test MSE of 65464025141.005943
Epoch 29: training loss 65315752207.059
Test Loss of 60560778429.734909, Test MSE of 60560777980.396835
Epoch 30: training loss 60974665246.118
Test Loss of 58693466560.636597, Test MSE of 58693466707.259018
Epoch 31: training loss 58272171113.412
Test Loss of 57072135337.837616, Test MSE of 57072137110.071709
Epoch 32: training loss 55109981680.941
Test Loss of 52782313676.184128, Test MSE of 52782314637.766502
Epoch 33: training loss 52582962838.588
Test Loss of 54184853794.879478, Test MSE of 54184853789.332527
Epoch 34: training loss 49641233965.176
Test Loss of 52520569773.331482, Test MSE of 52520569766.496689
Epoch 35: training loss 46088706386.824
Test Loss of 47211732229.744156, Test MSE of 47211731931.084015
Epoch 36: training loss 44960775017.412
Test Loss of 41712595210.718483, Test MSE of 41712595012.105682
Epoch 37: training loss 41844349703.529
Test Loss of 42481499358.423317, Test MSE of 42481499489.034958
Epoch 38: training loss 39215385923.765
Test Loss of 38216118556.247047, Test MSE of 38216118747.521477
Epoch 39: training loss 37206097611.294
Test Loss of 38856520697.604439, Test MSE of 38856521809.781586
Epoch 40: training loss 35161171230.118
Test Loss of 36353600621.198242, Test MSE of 36353601896.957550
Epoch 41: training loss 33222187768.471
Test Loss of 36041225367.361557, Test MSE of 36041224179.698807
Epoch 42: training loss 31974781440.000
Test Loss of 30601137143.709461, Test MSE of 30601136994.402267
Epoch 43: training loss 29970935612.235
Test Loss of 28632111713.709923, Test MSE of 28632111575.403709
Epoch 44: training loss 28284064978.824
Test Loss of 30658710496.969696, Test MSE of 30658710336.852238
Epoch 45: training loss 26930553931.294
Test Loss of 29370829630.001389, Test MSE of 29370829731.828911
Epoch 46: training loss 25220943454.118
Test Loss of 28705634994.246590, Test MSE of 28705634824.743263
Epoch 47: training loss 23826129136.941
Test Loss of 30013029260.169327, Test MSE of 30013029555.701004
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24397148658.90728, 'MSE - std': 5615880896.793722, 'R2 - mean': 0.8197375628292676, 'R2 - std': 0.034010667355102386} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926814900.706
Test Loss of 447257755613.890381, Test MSE of 447257746404.100647
Epoch 2: training loss 421905542565.647
Test Loss of 447238368024.575500, Test MSE of 447238370355.784668
Epoch 3: training loss 421877890108.235
Test Loss of 447212872383.037720, Test MSE of 447212876762.082031
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895479175.529
Test Loss of 447223522415.093201, Test MSE of 447223521987.846741
Epoch 2: training loss 421885824903.529
Test Loss of 447224371775.600281, Test MSE of 447224370337.129944
Epoch 3: training loss 421885343864.471
Test Loss of 447224996012.443237, Test MSE of 447224996827.825928
Epoch 4: training loss 421884967514.353
Test Loss of 447224476848.470032, Test MSE of 447224472727.710205
Epoch 5: training loss 415121522928.941
Test Loss of 425574009209.574829, Test MSE of 425574015552.540405
Epoch 6: training loss 369969237895.529
Test Loss of 358084889078.643555, Test MSE of 358084900282.553406
Epoch 7: training loss 291584450560.000
Test Loss of 271384828115.764038, Test MSE of 271384828216.498322
Epoch 8: training loss 212781680941.176
Test Loss of 200154740559.293091, Test MSE of 200154742568.497345
Epoch 9: training loss 154519499866.353
Test Loss of 148704217734.662048, Test MSE of 148704220845.736084
Epoch 10: training loss 135358079307.294
Test Loss of 140328915464.172089, Test MSE of 140328918784.205414
Epoch 11: training loss 132287090567.529
Test Loss of 136996998062.278976, Test MSE of 136996999050.333511
Epoch 12: training loss 127747674112.000
Test Loss of 133844635323.010880, Test MSE of 133844636046.296906
Epoch 13: training loss 126300117112.471
Test Loss of 128984359008.643997, Test MSE of 128984358336.187851
Epoch 14: training loss 120981153822.118
Test Loss of 126241542978.738846, Test MSE of 126241542701.091827
Epoch 15: training loss 117329535216.941
Test Loss of 123948972116.800369, Test MSE of 123948974633.828506
Epoch 16: training loss 114146556385.882
Test Loss of 119643540394.489014, Test MSE of 119643542023.976730
Epoch 17: training loss 109405383107.765
Test Loss of 115091050515.897293, Test MSE of 115091048334.699524
Epoch 18: training loss 105965041332.706
Test Loss of 109717014204.432114, Test MSE of 109717014912.675629
Epoch 19: training loss 101260246256.941
Test Loss of 107398291340.406204, Test MSE of 107398292340.360596
Epoch 20: training loss 97185086192.941
Test Loss of 102773261610.696274, Test MSE of 102773263171.417587
Epoch 21: training loss 95651626435.765
Test Loss of 97904970399.296783, Test MSE of 97904968542.631287
Epoch 22: training loss 89701159559.529
Test Loss of 94008726026.067078, Test MSE of 94008725441.431320
Epoch 23: training loss 87521171094.588
Test Loss of 92593700657.447144, Test MSE of 92593699063.472931
Epoch 24: training loss 83570448188.235
Test Loss of 87062693263.130234, Test MSE of 87062692428.227829
Epoch 25: training loss 79686860694.588
Test Loss of 86202023043.464264, Test MSE of 86202021307.587189
Epoch 26: training loss 76189719371.294
Test Loss of 79621321889.310196, Test MSE of 79621325930.769363
Epoch 27: training loss 72894165172.706
Test Loss of 72990382797.723801, Test MSE of 72990381993.989594
Epoch 28: training loss 69703726516.706
Test Loss of 71824018549.251907, Test MSE of 71824017455.381790
Epoch 29: training loss 66022149345.882
Test Loss of 72490398024.779083, Test MSE of 72490397109.251007
Epoch 30: training loss 62798969554.824
Test Loss of 66685729841.980110, Test MSE of 66685729750.916176
Epoch 31: training loss 59431250191.059
Test Loss of 62542566529.095535, Test MSE of 62542567535.859184
Epoch 32: training loss 57474226808.471
Test Loss of 60985528074.600044, Test MSE of 60985527498.086311
Epoch 33: training loss 54206031277.176
Test Loss of 57547012116.371040, Test MSE of 57547012908.287590
Epoch 34: training loss 51505373455.059
Test Loss of 54480768545.517464, Test MSE of 54480768197.334534
Epoch 35: training loss 48839234266.353
Test Loss of 54888674118.528801, Test MSE of 54888674487.747368
Epoch 36: training loss 46332280485.647
Test Loss of 49541787331.538284, Test MSE of 49541787403.525826
Epoch 37: training loss 44020032429.176
Test Loss of 48107364639.089523, Test MSE of 48107364398.149025
Epoch 38: training loss 42188819154.824
Test Loss of 45242258496.666206, Test MSE of 45242258679.133629
Epoch 39: training loss 39994769370.353
Test Loss of 43982057435.047882, Test MSE of 43982057925.457703
Epoch 40: training loss 37504453647.059
Test Loss of 39179223067.477219, Test MSE of 39179222500.459641
Epoch 41: training loss 35759042288.941
Test Loss of 37099583847.572517, Test MSE of 37099584028.387840
Epoch 42: training loss 33927430068.706
Test Loss of 38862365110.687950, Test MSE of 38862365305.882225
Epoch 43: training loss 32357997492.706
Test Loss of 36671296340.741150, Test MSE of 36671296427.415451
Epoch 44: training loss 30127937611.294
Test Loss of 36432884485.151978, Test MSE of 36432884968.688217
Epoch 45: training loss 28890947166.118
Test Loss of 32637686001.609993, Test MSE of 32637685398.389668
Epoch 46: training loss 27580206275.765
Test Loss of 27331026283.599354, Test MSE of 27331026519.407566
Epoch 47: training loss 26332118633.412
Test Loss of 29986083138.146656, Test MSE of 29986082976.130806
Epoch 48: training loss 25152219241.412
Test Loss of 30948006524.239647, Test MSE of 30948006803.468449
Epoch 49: training loss 23879103488.000
Test Loss of 27752166891.036781, Test MSE of 27752167234.101551
Epoch 50: training loss 23102652705.882
Test Loss of 26074507475.764053, Test MSE of 26074507866.622013
Epoch 51: training loss 21953132675.765
Test Loss of 25134413637.818180, Test MSE of 25134414045.151985
Epoch 52: training loss 21589442729.412
Test Loss of 23613636042.348370, Test MSE of 23613635863.681335
Epoch 53: training loss 20599918912.000
Test Loss of 23354909147.166321, Test MSE of 23354909227.190948
Epoch 54: training loss 19712057682.824
Test Loss of 25579905812.311821, Test MSE of 25579905465.663639
Epoch 55: training loss 19018423559.529
Test Loss of 22551255972.330326, Test MSE of 22551255866.037880
Epoch 56: training loss 18197273069.176
Test Loss of 23589872076.953968, Test MSE of 23589872144.039101
Epoch 57: training loss 17951686317.176
Test Loss of 24699457014.880405, Test MSE of 24699457227.453598
Epoch 58: training loss 16856797835.294
Test Loss of 24001902680.116585, Test MSE of 24001902571.476562
Epoch 59: training loss 16599608986.353
Test Loss of 21461792535.391163, Test MSE of 21461793021.271545
Epoch 60: training loss 15975835053.176
Test Loss of 21848528806.462177, Test MSE of 21848528791.399307
Epoch 61: training loss 15402327875.765
Test Loss of 21034278221.753410, Test MSE of 21034278223.171791
Epoch 62: training loss 15140981052.235
Test Loss of 21478555972.989128, Test MSE of 21478555744.102196
Epoch 63: training loss 14565335582.118
Test Loss of 21558098429.275967, Test MSE of 21558098883.012436
Epoch 64: training loss 14355407875.765
Test Loss of 22052585707.451305, Test MSE of 22052585942.314220
Epoch 65: training loss 13883529340.235
Test Loss of 21615410662.299328, Test MSE of 21615410789.682709
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23469902702.49909, 'MSE - std': 4769169979.356903, 'R2 - mean': 0.8318609208409115, 'R2 - std': 0.03263589968500837} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005451 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109894535.529
Test Loss of 410765856614.471069, Test MSE of 410765855578.456665
Epoch 2: training loss 430088903740.235
Test Loss of 410747145586.080505, Test MSE of 410747143606.447266
Epoch 3: training loss 430061449216.000
Test Loss of 410723418460.757080, Test MSE of 410723424099.641907
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430074255239.529
Test Loss of 410730636776.544189, Test MSE of 410730637273.123779
Epoch 2: training loss 430066750403.765
Test Loss of 410730613864.721863, Test MSE of 410730614079.375549
Epoch 3: training loss 430066305144.471
Test Loss of 410730243681.851013, Test MSE of 410730244128.593262
Epoch 4: training loss 430065963248.941
Test Loss of 410730072608.932922, Test MSE of 410730064573.407654
Epoch 5: training loss 423394689264.941
Test Loss of 389455371033.232788, Test MSE of 389455367943.042175
Epoch 6: training loss 378318380333.176
Test Loss of 323271398526.519226, Test MSE of 323271399944.249268
Epoch 7: training loss 299185687371.294
Test Loss of 238253337594.787598, Test MSE of 238253340240.242920
Epoch 8: training loss 220178883041.882
Test Loss of 167485668473.780670, Test MSE of 167485669123.337158
Epoch 9: training loss 161205943687.529
Test Loss of 120669575112.085144, Test MSE of 120669575771.995438
Epoch 10: training loss 141292002424.471
Test Loss of 112291655575.751968, Test MSE of 112291657448.751480
Epoch 11: training loss 137623170108.235
Test Loss of 109436236441.291992, Test MSE of 109436236669.829193
Epoch 12: training loss 135408412822.588
Test Loss of 106754426708.464600, Test MSE of 106754427643.825516
Epoch 13: training loss 131360941116.235
Test Loss of 103437777530.017578, Test MSE of 103437776298.963913
Epoch 14: training loss 126677896101.647
Test Loss of 100431867011.257751, Test MSE of 100431864952.761826
Epoch 15: training loss 123133184391.529
Test Loss of 97712942279.492828, Test MSE of 97712942979.491592
Epoch 16: training loss 118914735043.765
Test Loss of 95397801151.911148, Test MSE of 95397802249.311493
Epoch 17: training loss 116922590358.588
Test Loss of 90471383426.665436, Test MSE of 90471382794.266113
Epoch 18: training loss 111339433170.824
Test Loss of 88286177696.044418, Test MSE of 88286179037.887360
Epoch 19: training loss 108065050006.588
Test Loss of 85058962677.930588, Test MSE of 85058961513.349609
Epoch 20: training loss 103349087623.529
Test Loss of 81109077199.074509, Test MSE of 81109075978.684326
Epoch 21: training loss 100204474684.235
Test Loss of 79591471461.286438, Test MSE of 79591471868.540497
Epoch 22: training loss 95808583258.353
Test Loss of 76426442824.973618, Test MSE of 76426439935.085861
Epoch 23: training loss 91765066420.706
Test Loss of 72074158652.416473, Test MSE of 72074158378.734985
Epoch 24: training loss 88360453662.118
Test Loss of 69214569532.179550, Test MSE of 69214568056.583984
Epoch 25: training loss 84431394680.471
Test Loss of 66569459428.634888, Test MSE of 66569459216.502007
Epoch 26: training loss 80592814968.471
Test Loss of 63676322320.821838, Test MSE of 63676321562.135948
Epoch 27: training loss 77008932954.353
Test Loss of 60357333589.056915, Test MSE of 60357333085.678955
Epoch 28: training loss 74625730409.412
Test Loss of 59282077242.047203, Test MSE of 59282076960.016037
Epoch 29: training loss 70763177231.059
Test Loss of 54261043779.998146, Test MSE of 54261043373.759789
Epoch 30: training loss 66613120948.706
Test Loss of 53487292988.416473, Test MSE of 53487293920.112083
Epoch 31: training loss 64542033106.824
Test Loss of 50969285766.100876, Test MSE of 50969286649.652451
Epoch 32: training loss 61950582384.941
Test Loss of 49330592326.841278, Test MSE of 49330592054.691422
Epoch 33: training loss 58095470930.824
Test Loss of 45612507268.679314, Test MSE of 45612506761.076149
Epoch 34: training loss 55461425347.765
Test Loss of 43770142756.012955, Test MSE of 43770143375.113129
Epoch 35: training loss 52332795301.647
Test Loss of 38663768290.502548, Test MSE of 38663768946.887512
Epoch 36: training loss 49240559262.118
Test Loss of 38625463053.860252, Test MSE of 38625462274.231232
Epoch 37: training loss 47064050484.706
Test Loss of 40789139474.954185, Test MSE of 40789139177.032272
Epoch 38: training loss 44862674221.176
Test Loss of 36445701549.786209, Test MSE of 36445701818.130112
Epoch 39: training loss 42456147448.471
Test Loss of 34174559747.080055, Test MSE of 34174559985.034561
Epoch 40: training loss 40415730153.412
Test Loss of 33392727691.550209, Test MSE of 33392727604.983082
Epoch 41: training loss 38299038539.294
Test Loss of 29756515263.081905, Test MSE of 29756514848.259956
Epoch 42: training loss 36192164675.765
Test Loss of 28662945466.935677, Test MSE of 28662946054.096615
Epoch 43: training loss 34327715267.765
Test Loss of 28088554202.683941, Test MSE of 28088553907.273033
Epoch 44: training loss 32985263864.471
Test Loss of 27547862860.882923, Test MSE of 27547862301.034077
Epoch 45: training loss 31763257140.706
Test Loss of 26255353230.511799, Test MSE of 26255352909.841648
Epoch 46: training loss 29702888229.647
Test Loss of 23800564428.468300, Test MSE of 23800565070.084167
Epoch 47: training loss 28449153377.882
Test Loss of 24497059211.668671, Test MSE of 24497059189.032127
Epoch 48: training loss 26773475527.529
Test Loss of 23184996340.627487, Test MSE of 23184996542.869316
Epoch 49: training loss 26260134960.941
Test Loss of 22613974172.845905, Test MSE of 22613973964.968437
Epoch 50: training loss 24933994228.706
Test Loss of 21855740060.372051, Test MSE of 21855739796.835976
Epoch 51: training loss 24032017159.529
Test Loss of 21956816451.524296, Test MSE of 21956816048.667999
Epoch 52: training loss 22539575917.176
Test Loss of 22258874417.280888, Test MSE of 22258874387.348419
Epoch 53: training loss 22416100009.412
Test Loss of 20053733841.325314, Test MSE of 20053733711.180183
Epoch 54: training loss 21279931753.412
Test Loss of 20831184112.244331, Test MSE of 20831183830.506535
Epoch 55: training loss 20570119747.765
Test Loss of 20249854469.449329, Test MSE of 20249854560.422665
Epoch 56: training loss 20086186910.118
Test Loss of 20360050892.705231, Test MSE of 20360050772.672390
Epoch 57: training loss 19106437300.706
Test Loss of 21324396764.816288, Test MSE of 21324396558.001770
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22933526166.37476, 'MSE - std': 4233418959.247188, 'R2 - mean': 0.8298953510721621, 'R2 - std': 0.028467821390737145} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005276 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042525153.882
Test Loss of 431609989960.618225, Test MSE of 431609995769.005493
Epoch 2: training loss 424021674947.765
Test Loss of 431588813310.815369, Test MSE of 431588811239.907593
Epoch 3: training loss 423993665897.412
Test Loss of 431560356569.736206, Test MSE of 431560364791.229492
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424007896726.588
Test Loss of 431560780082.110107, Test MSE of 431560782495.790405
Epoch 2: training loss 423994839160.471
Test Loss of 431564913080.684875, Test MSE of 431564898595.174072
Epoch 3: training loss 423994283068.235
Test Loss of 431564580245.145752, Test MSE of 431564577499.659302
Epoch 4: training loss 423993849253.647
Test Loss of 431564952550.885681, Test MSE of 431564949156.149231
Epoch 5: training loss 417187356190.118
Test Loss of 409370675070.163818, Test MSE of 409370673522.928772
Epoch 6: training loss 371739933756.235
Test Loss of 341541156834.621033, Test MSE of 341541155877.466675
Epoch 7: training loss 292942016210.824
Test Loss of 253563623436.320221, Test MSE of 253563621557.840851
Epoch 8: training loss 214826247890.824
Test Loss of 182321009252.220276, Test MSE of 182321007235.180817
Epoch 9: training loss 155749018970.353
Test Loss of 131853467231.007874, Test MSE of 131853467832.110474
Epoch 10: training loss 138014552937.412
Test Loss of 123805277040.422028, Test MSE of 123805276837.719482
Epoch 11: training loss 134918291636.706
Test Loss of 120497980972.779266, Test MSE of 120497983545.095673
Epoch 12: training loss 131525164649.412
Test Loss of 117481742688.547897, Test MSE of 117481741862.216095
Epoch 13: training loss 127683592463.059
Test Loss of 114269684428.468307, Test MSE of 114269685310.017273
Epoch 14: training loss 124224401739.294
Test Loss of 109701944973.445633, Test MSE of 109701945918.354691
Epoch 15: training loss 119992197586.824
Test Loss of 106712897282.961594, Test MSE of 106712899309.125778
Epoch 16: training loss 116277891538.824
Test Loss of 103300519626.099030, Test MSE of 103300518853.573807
Epoch 17: training loss 111757844118.588
Test Loss of 99292956909.875061, Test MSE of 99292957616.400726
Epoch 18: training loss 106787971975.529
Test Loss of 95965174792.055527, Test MSE of 95965175036.430710
Epoch 19: training loss 103793164182.588
Test Loss of 90557234692.027771, Test MSE of 90557234594.041534
Epoch 20: training loss 100192253078.588
Test Loss of 90084541613.904678, Test MSE of 90084542958.631500
Epoch 21: training loss 95490257031.529
Test Loss of 84063771022.985657, Test MSE of 84063772978.208908
Epoch 22: training loss 91764295612.235
Test Loss of 81712812071.803787, Test MSE of 81712811942.903076
Epoch 23: training loss 88552718607.059
Test Loss of 75465302326.848679, Test MSE of 75465300613.170990
Epoch 24: training loss 84307509609.412
Test Loss of 75298261199.548355, Test MSE of 75298261483.617416
Epoch 25: training loss 81220897656.471
Test Loss of 70752858667.357712, Test MSE of 70752857371.513031
Epoch 26: training loss 77884164412.235
Test Loss of 68646001636.042572, Test MSE of 68646000209.569077
Epoch 27: training loss 73661767936.000
Test Loss of 63718122760.884773, Test MSE of 63718122902.758560
Epoch 28: training loss 70396418665.412
Test Loss of 59705868731.528000, Test MSE of 59705868160.584656
Epoch 29: training loss 66871007872.000
Test Loss of 58198976253.275337, Test MSE of 58198975711.210800
Epoch 30: training loss 63018137600.000
Test Loss of 55576160952.566406, Test MSE of 55576161832.525177
Epoch 31: training loss 60957573895.529
Test Loss of 52572396508.460899, Test MSE of 52572395135.919777
Epoch 32: training loss 58302408975.059
Test Loss of 47841464321.895416, Test MSE of 47841463763.331215
Epoch 33: training loss 55310086942.118
Test Loss of 46708709475.509483, Test MSE of 46708709379.730682
Epoch 34: training loss 52130763361.882
Test Loss of 41677941631.585381, Test MSE of 41677942272.947792
Epoch 35: training loss 49689691256.471
Test Loss of 42638732320.695976, Test MSE of 42638731359.948280
Epoch 36: training loss 47375011930.353
Test Loss of 42149737990.870895, Test MSE of 42149736919.614212
Epoch 37: training loss 45162923070.118
Test Loss of 38035215706.387787, Test MSE of 38035216380.621071
Epoch 38: training loss 42935045485.176
Test Loss of 37863763365.256828, Test MSE of 37863763620.844460
Epoch 39: training loss 40215743510.588
Test Loss of 30729777766.115688, Test MSE of 30729778347.063251
Epoch 40: training loss 38170459587.765
Test Loss of 35944992380.386856, Test MSE of 35944992245.342537
Epoch 41: training loss 36789782422.588
Test Loss of 31968259702.700600, Test MSE of 31968259931.569698
Epoch 42: training loss 35021986816.000
Test Loss of 34252910376.869968, Test MSE of 34252910997.667385
Epoch 43: training loss 32922606682.353
Test Loss of 29916572200.514576, Test MSE of 29916572040.837749
Epoch 44: training loss 31519369664.000
Test Loss of 26163371001.839890, Test MSE of 26163370808.468781
Epoch 45: training loss 29872892717.176
Test Loss of 27502842636.912540, Test MSE of 27502841951.217522
Epoch 46: training loss 28555355806.118
Test Loss of 27616755303.537251, Test MSE of 27616755472.995502
Epoch 47: training loss 27057131248.941
Test Loss of 23889368472.936604, Test MSE of 23889368308.231159
Epoch 48: training loss 26256903973.647
Test Loss of 25463585402.017586, Test MSE of 25463585929.558098
Epoch 49: training loss 24872249750.588
Test Loss of 23377740600.507172, Test MSE of 23377739904.725334
Epoch 50: training loss 23655701473.882
Test Loss of 22128267647.822304, Test MSE of 22128267452.268375
Epoch 51: training loss 22921614053.647
Test Loss of 26336043189.012493, Test MSE of 26336043530.589424
Epoch 52: training loss 22087725184.000
Test Loss of 24923436776.425728, Test MSE of 24923436961.920059
Epoch 53: training loss 21007778394.353
Test Loss of 20845440193.806572, Test MSE of 20845440060.007198
Epoch 54: training loss 20268902023.529
Test Loss of 20973327096.062935, Test MSE of 20973327130.395576
Epoch 55: training loss 19853894987.294
Test Loss of 21951961504.044422, Test MSE of 21951961414.582520
Epoch 56: training loss 19049042522.353
Test Loss of 20249327943.907452, Test MSE of 20249327888.721230
Epoch 57: training loss 18474910810.353
Test Loss of 20967048912.732994, Test MSE of 20967049038.639923
Epoch 58: training loss 17673053398.588
Test Loss of 21474407045.390099, Test MSE of 21474407191.964188
Epoch 59: training loss 17492012656.941
Test Loss of 19974563578.432209, Test MSE of 19974563351.774178
Epoch 60: training loss 16995480926.118
Test Loss of 20785893814.789448, Test MSE of 20785894257.707397
Epoch 61: training loss 16416686976.000
Test Loss of 19864635608.077744, Test MSE of 19864635600.395374
Epoch 62: training loss 16271775981.176
Test Loss of 20353362346.469227, Test MSE of 20353362203.011093
Epoch 63: training loss 15809832880.941
Test Loss of 19451826332.372051, Test MSE of 19451826275.706539
Epoch 64: training loss 15345143141.647
Test Loss of 19480026177.865803, Test MSE of 19480026192.846973
Epoch 65: training loss 14580315489.882
Test Loss of 19255358852.086998, Test MSE of 19255358694.428829
Epoch 66: training loss 14558145072.941
Test Loss of 21275760902.515503, Test MSE of 21275761336.704681
Epoch 67: training loss 13930857114.353
Test Loss of 19258911426.991207, Test MSE of 19258911596.905910
Epoch 68: training loss 13935529125.647
Test Loss of 20261022446.585838, Test MSE of 20261022708.191628
Epoch 69: training loss 13492025340.235
Test Loss of 20350953005.726978, Test MSE of 20350953356.433868
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22417011604.38658, 'MSE - std': 3924871722.5654902, 'R2 - mean': 0.8335199458096229, 'R2 - std': 0.026474218247782982} 
 

Saving model.....
Results After CV: {'MSE - mean': 22417011604.38658, 'MSE - std': 3924871722.5654902, 'R2 - mean': 0.8335199458096229, 'R2 - std': 0.026474218247782982}
Train time: 92.65784283559988
Inference time: 0.07043020159981098
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 65 finished with value: 22417011604.38658 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005391 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525241554.824
Test Loss of 418111260549.300049, Test MSE of 418111261839.757202
Epoch 2: training loss 427504350388.706
Test Loss of 418092841446.536194, Test MSE of 418092839338.243774
Epoch 3: training loss 427476599747.765
Test Loss of 418069153070.486206, Test MSE of 418069160369.026428
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494220498.824
Test Loss of 418074986176.222046, Test MSE of 418074983440.598328
Epoch 2: training loss 427481934426.353
Test Loss of 418077420968.238708, Test MSE of 418077429214.224304
Epoch 3: training loss 427481556028.235
Test Loss of 418076722008.531128, Test MSE of 418076725532.259827
Epoch 4: training loss 422008595275.294
Test Loss of 401140585574.565796, Test MSE of 401140585151.199768
Epoch 5: training loss 385723548009.412
Test Loss of 345148766599.787170, Test MSE of 345148768938.404236
Epoch 6: training loss 317287493391.059
Test Loss of 268412298347.303253, Test MSE of 268412295624.541748
Epoch 7: training loss 225992047134.118
Test Loss of 170847923908.485779, Test MSE of 170847923691.614624
Epoch 8: training loss 160746892739.765
Test Loss of 130317607311.840851, Test MSE of 130317608568.468719
Epoch 9: training loss 141617498895.059
Test Loss of 119855243800.279434, Test MSE of 119855244986.513275
Epoch 10: training loss 136373720033.882
Test Loss of 115672587952.588486, Test MSE of 115672588628.280777
Epoch 11: training loss 133198396355.765
Test Loss of 113269871837.949570, Test MSE of 113269870285.714310
Epoch 12: training loss 129514478471.529
Test Loss of 109611561933.072403, Test MSE of 109611562673.226715
Epoch 13: training loss 126181375427.765
Test Loss of 106881793432.131393, Test MSE of 106881793059.194443
Epoch 14: training loss 121670879382.588
Test Loss of 103931463178.303955, Test MSE of 103931461514.515244
Epoch 15: training loss 118973676754.824
Test Loss of 100879134277.522095, Test MSE of 100879134169.198792
Epoch 16: training loss 113870511631.059
Test Loss of 97259771708.343277, Test MSE of 97259772463.997452
Epoch 17: training loss 110252669590.588
Test Loss of 93998752918.887802, Test MSE of 93998752885.530151
Epoch 18: training loss 106536148028.235
Test Loss of 90746802825.030762, Test MSE of 90746803334.416016
Epoch 19: training loss 102154155670.588
Test Loss of 87045356673.569275, Test MSE of 87045355793.796295
Epoch 20: training loss 99828579809.882
Test Loss of 82785166731.340271, Test MSE of 82785167623.345627
Epoch 21: training loss 94889418450.824
Test Loss of 79960841830.684250, Test MSE of 79960840758.502075
Epoch 22: training loss 90625446038.588
Test Loss of 78071315765.592407, Test MSE of 78071317353.875854
Epoch 23: training loss 87171118125.176
Test Loss of 72455873063.439285, Test MSE of 72455873137.000641
Epoch 24: training loss 83592002288.941
Test Loss of 70760374309.899612, Test MSE of 70760373607.684998
Epoch 25: training loss 80799510437.647
Test Loss of 68861421956.944717, Test MSE of 68861422442.405151
Epoch 26: training loss 77191007593.412
Test Loss of 63424280310.229004, Test MSE of 63424279858.632423
Epoch 27: training loss 72754133037.176
Test Loss of 60787099307.377281, Test MSE of 60787098274.426674
Epoch 28: training loss 69959991732.706
Test Loss of 60103886283.769608, Test MSE of 60103885465.014626
Epoch 29: training loss 66711816350.118
Test Loss of 55797270028.198936, Test MSE of 55797270271.974884
Epoch 30: training loss 63035319649.882
Test Loss of 51214713168.359009, Test MSE of 51214711658.343681
Epoch 31: training loss 60194147734.588
Test Loss of 51531616826.389084, Test MSE of 51531616528.519211
Epoch 32: training loss 57307453688.471
Test Loss of 48783850816.962296, Test MSE of 48783850469.537949
Epoch 33: training loss 54400667482.353
Test Loss of 48270212273.891281, Test MSE of 48270213302.749496
Epoch 34: training loss 51966976722.824
Test Loss of 44824573940.156372, Test MSE of 44824574482.402878
Epoch 35: training loss 49151136320.000
Test Loss of 43014897882.870232, Test MSE of 43014898288.417542
Epoch 36: training loss 46690479488.000
Test Loss of 40107264764.150818, Test MSE of 40107264590.294121
Epoch 37: training loss 44821575800.471
Test Loss of 36750886284.287766, Test MSE of 36750886429.635445
Epoch 38: training loss 42061654328.471
Test Loss of 37449491917.664581, Test MSE of 37449491655.934105
Epoch 39: training loss 40466521534.118
Test Loss of 37543459741.224152, Test MSE of 37543460096.433540
Epoch 40: training loss 38184481325.176
Test Loss of 35577763661.398102, Test MSE of 35577763878.730179
Epoch 41: training loss 36458059527.529
Test Loss of 32410019965.542446, Test MSE of 32410019900.473778
Epoch 42: training loss 34351936481.882
Test Loss of 31461824537.108490, Test MSE of 31461824655.565804
Epoch 43: training loss 33074904824.471
Test Loss of 28824156822.769375, Test MSE of 28824157125.728611
Epoch 44: training loss 31804381033.412
Test Loss of 29061709199.603977, Test MSE of 29061709082.014900
Epoch 45: training loss 30369016214.588
Test Loss of 29163681265.669212, Test MSE of 29163681285.886387
Epoch 46: training loss 28864209581.176
Test Loss of 25545318935.331944, Test MSE of 25545318540.159412
Epoch 47: training loss 27827641208.471
Test Loss of 26964765318.662041, Test MSE of 26964765150.193584
Epoch 48: training loss 26401400086.588
Test Loss of 25902324005.248207, Test MSE of 25902323985.137081
Epoch 49: training loss 25127202439.529
Test Loss of 27167567258.973862, Test MSE of 27167567027.602200
Epoch 50: training loss 24152924050.824
Test Loss of 25268053462.191998, Test MSE of 25268053252.086132
Epoch 51: training loss 23266251294.118
Test Loss of 23127962150.491787, Test MSE of 23127962185.192039
Epoch 52: training loss 22814461500.235
Test Loss of 24802221604.359936, Test MSE of 24802221004.757023
Epoch 53: training loss 21670105208.471
Test Loss of 22181557595.492020, Test MSE of 22181557486.951973
Epoch 54: training loss 20945333421.176
Test Loss of 24261946545.891277, Test MSE of 24261946751.166565
Epoch 55: training loss 19946106989.176
Test Loss of 22005429408.362713, Test MSE of 22005429577.561306
Epoch 56: training loss 19465320248.471
Test Loss of 23838031307.295860, Test MSE of 23838030809.658443
Epoch 57: training loss 18821311875.765
Test Loss of 21352033528.005550, Test MSE of 21352033525.792057
Epoch 58: training loss 18102649040.941
Test Loss of 20098077297.343510, Test MSE of 20098077533.492287
Epoch 59: training loss 17195712286.118
Test Loss of 20579403741.416611, Test MSE of 20579403668.831097
Epoch 60: training loss 17398131320.471
Test Loss of 19751150924.332176, Test MSE of 19751150839.356064
Epoch 61: training loss 16397828039.529
Test Loss of 20751685012.815174, Test MSE of 20751684670.299225
Epoch 62: training loss 16163925063.529
Test Loss of 18980781634.205875, Test MSE of 18980781904.447250
Epoch 63: training loss 15547512617.412
Test Loss of 19943775144.120285, Test MSE of 19943775108.084671
Epoch 64: training loss 15203104698.353
Test Loss of 20926197562.448299, Test MSE of 20926198001.810028
Epoch 65: training loss 14685482582.588
Test Loss of 19240886023.757576, Test MSE of 19240886814.618877
Epoch 66: training loss 14319009889.882
Test Loss of 20125729313.280594, Test MSE of 20125729699.826618
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20125729699.82662, 'MSE - std': 0.0, 'R2 - mean': 0.8432787592023877, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005321 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917138522.353
Test Loss of 424555866608.958618, Test MSE of 424555864621.662415
Epoch 2: training loss 427895188540.235
Test Loss of 424538690044.802246, Test MSE of 424538696306.322632
Epoch 3: training loss 427865705532.235
Test Loss of 424515176143.381897, Test MSE of 424515178195.345581
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885868694.588
Test Loss of 424520302186.474182, Test MSE of 424520292159.709595
Epoch 2: training loss 427874609272.471
Test Loss of 424521706571.562317, Test MSE of 424521706806.766663
Epoch 3: training loss 427874053180.235
Test Loss of 424522062160.832764, Test MSE of 424522060555.177673
Epoch 4: training loss 422111015875.765
Test Loss of 407013115072.577393, Test MSE of 407013122138.973999
Epoch 5: training loss 384962842624.000
Test Loss of 352700431565.368469, Test MSE of 352700434378.441101
Epoch 6: training loss 316183512244.706
Test Loss of 276007328327.180176, Test MSE of 276007337826.946838
Epoch 7: training loss 224189811471.059
Test Loss of 179595635768.612549, Test MSE of 179595636956.023376
Epoch 8: training loss 158941863363.765
Test Loss of 141351215723.658569, Test MSE of 141351214042.819641
Epoch 9: training loss 139642015412.706
Test Loss of 131455557884.032379, Test MSE of 131455554147.799149
Epoch 10: training loss 134708420156.235
Test Loss of 127639223452.098999, Test MSE of 127639221950.722992
Epoch 11: training loss 130405284984.471
Test Loss of 123626284522.089294, Test MSE of 123626280880.549179
Epoch 12: training loss 128194753957.647
Test Loss of 121018965688.168396, Test MSE of 121018964554.945068
Epoch 13: training loss 123949805417.412
Test Loss of 117378148151.605835, Test MSE of 117378147205.718414
Epoch 14: training loss 120390783126.588
Test Loss of 114247734810.411285, Test MSE of 114247732715.456711
Epoch 15: training loss 117168084419.765
Test Loss of 109374178913.473053, Test MSE of 109374176596.463669
Epoch 16: training loss 111524607909.647
Test Loss of 105965765504.325699, Test MSE of 105965764027.218246
Epoch 17: training loss 108330010593.882
Test Loss of 102541401624.042557, Test MSE of 102541404425.121277
Epoch 18: training loss 104482163380.706
Test Loss of 99617108098.753647, Test MSE of 99617106368.145004
Epoch 19: training loss 100417130526.118
Test Loss of 95176773029.396255, Test MSE of 95176772633.057571
Epoch 20: training loss 95297175552.000
Test Loss of 91911243811.530884, Test MSE of 91911245200.208252
Epoch 21: training loss 92775202740.706
Test Loss of 86877478096.210968, Test MSE of 86877477413.177979
Epoch 22: training loss 88425684796.235
Test Loss of 83686119583.652100, Test MSE of 83686119085.445404
Epoch 23: training loss 84576972122.353
Test Loss of 79816404294.647232, Test MSE of 79816404409.123123
Epoch 24: training loss 80323921408.000
Test Loss of 78383129106.594498, Test MSE of 78383130880.972366
Epoch 25: training loss 75999383687.529
Test Loss of 77652500399.463333, Test MSE of 77652496054.408539
Epoch 26: training loss 72139801148.235
Test Loss of 71901117888.873474, Test MSE of 71901117488.349167
Epoch 27: training loss 68777642842.353
Test Loss of 63747331152.773537, Test MSE of 63747332101.837456
Epoch 28: training loss 66489480673.882
Test Loss of 64435969403.706688, Test MSE of 64435970266.255524
Epoch 29: training loss 63188082416.941
Test Loss of 58302363306.429794, Test MSE of 58302362989.614243
Epoch 30: training loss 59225147858.824
Test Loss of 57559128001.939392, Test MSE of 57559127738.751579
Epoch 31: training loss 56225306669.176
Test Loss of 55124995921.188065, Test MSE of 55124996738.600655
Epoch 32: training loss 53031084837.647
Test Loss of 51347312182.362251, Test MSE of 51347310939.741714
Epoch 33: training loss 50506675350.588
Test Loss of 51360701685.163078, Test MSE of 51360701274.650734
Epoch 34: training loss 47859433938.824
Test Loss of 44940214052.182281, Test MSE of 44940214536.920807
Epoch 35: training loss 45197794906.353
Test Loss of 45738860995.479065, Test MSE of 45738861301.160164
Epoch 36: training loss 42259830490.353
Test Loss of 44937693392.447838, Test MSE of 44937694359.089279
Epoch 37: training loss 40360233660.235
Test Loss of 39577352053.192688, Test MSE of 39577352880.804039
Epoch 38: training loss 38507811267.765
Test Loss of 39133815535.833450, Test MSE of 39133815875.394043
Epoch 39: training loss 36795229824.000
Test Loss of 36268308021.414757, Test MSE of 36268307378.207611
Epoch 40: training loss 34137156728.471
Test Loss of 40003010388.504280, Test MSE of 40003009462.613556
Epoch 41: training loss 32914977106.824
Test Loss of 35359776538.233635, Test MSE of 35359776805.911880
Epoch 42: training loss 31297594473.412
Test Loss of 32501506505.164005, Test MSE of 32501506568.013985
Epoch 43: training loss 29318552643.765
Test Loss of 32644890365.808929, Test MSE of 32644890425.090488
Epoch 44: training loss 27678531072.000
Test Loss of 31892692992.473743, Test MSE of 31892693118.025047
Epoch 45: training loss 26797038034.824
Test Loss of 31515952994.242886, Test MSE of 31515953281.783459
Epoch 46: training loss 25293814068.706
Test Loss of 29391586594.642609, Test MSE of 29391586396.653358
Epoch 47: training loss 24041592681.412
Test Loss of 27893309687.294933, Test MSE of 27893309721.600082
Epoch 48: training loss 22743274119.529
Test Loss of 30275898272.777237, Test MSE of 30275896947.597313
Epoch 49: training loss 21393827226.353
Test Loss of 29925868723.312515, Test MSE of 29925868961.795246
Epoch 50: training loss 20968826326.588
Test Loss of 28718504131.182976, Test MSE of 28718504499.563782
Epoch 51: training loss 20533618228.706
Test Loss of 26589810731.821419, Test MSE of 26589811190.165039
Epoch 52: training loss 19123219245.176
Test Loss of 24924478837.311127, Test MSE of 24924479003.230389
Epoch 53: training loss 18584103269.647
Test Loss of 27912876385.176960, Test MSE of 27912876363.608032
Epoch 54: training loss 17592439762.824
Test Loss of 28007836848.943790, Test MSE of 28007836531.343224
Epoch 55: training loss 17002026962.824
Test Loss of 26602694040.368263, Test MSE of 26602693649.447639
Epoch 56: training loss 16546826153.412
Test Loss of 26447401281.909786, Test MSE of 26447400718.118252
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23286565208.972435, 'MSE - std': 3160835509.145817, 'R2 - mean': 0.8272309386273711, 'R2 - std': 0.016047820575016636} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003548 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927133424.941
Test Loss of 447257231356.210022, Test MSE of 447257234487.608032
Epoch 2: training loss 421906618849.882
Test Loss of 447238311190.325256, Test MSE of 447238310173.447083
Epoch 3: training loss 421879282507.294
Test Loss of 447213006183.809387, Test MSE of 447213013488.696594
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899252434.824
Test Loss of 447221434010.322449, Test MSE of 447221433811.897339
Epoch 2: training loss 421885727201.882
Test Loss of 447221985525.399963, Test MSE of 447221992760.196045
Epoch 3: training loss 421885073769.412
Test Loss of 447220997907.127441, Test MSE of 447221005430.566589
Epoch 4: training loss 416728012559.059
Test Loss of 430134543757.235229, Test MSE of 430134547219.360168
Epoch 5: training loss 380858394624.000
Test Loss of 375719872802.405762, Test MSE of 375719878758.326660
Epoch 6: training loss 312664130017.882
Test Loss of 296554987936.895691, Test MSE of 296554994673.227783
Epoch 7: training loss 221442017460.706
Test Loss of 195749344671.237579, Test MSE of 195749343448.592926
Epoch 8: training loss 157479080749.176
Test Loss of 154639743580.024994, Test MSE of 154639743770.972229
Epoch 9: training loss 137456343311.059
Test Loss of 142767411019.029388, Test MSE of 142767410344.598572
Epoch 10: training loss 133245646697.412
Test Loss of 137601585713.861664, Test MSE of 137601585029.782471
Epoch 11: training loss 128641927860.706
Test Loss of 134372095228.269257, Test MSE of 134372097278.012207
Epoch 12: training loss 125804357029.647
Test Loss of 131882062510.930374, Test MSE of 131882062354.101318
Epoch 13: training loss 121889642164.706
Test Loss of 127535922824.793900, Test MSE of 127535923170.580154
Epoch 14: training loss 118212391092.706
Test Loss of 124001761546.718475, Test MSE of 124001762715.070236
Epoch 15: training loss 115297882503.529
Test Loss of 120924038012.535736, Test MSE of 120924036733.715683
Epoch 16: training loss 112409507026.824
Test Loss of 119034809720.864212, Test MSE of 119034808407.604385
Epoch 17: training loss 106781120000.000
Test Loss of 114591437164.783722, Test MSE of 114591435859.022842
Epoch 18: training loss 103952768813.176
Test Loss of 109505477587.231094, Test MSE of 109505476919.888672
Epoch 19: training loss 101009826334.118
Test Loss of 107097321997.857040, Test MSE of 107097322343.734360
Epoch 20: training loss 95569426552.471
Test Loss of 103065328574.386307, Test MSE of 103065331109.342117
Epoch 21: training loss 93629914066.824
Test Loss of 100712410532.448761, Test MSE of 100712412117.175247
Epoch 22: training loss 89441359631.059
Test Loss of 96170284686.005096, Test MSE of 96170285601.488724
Epoch 23: training loss 85464388427.294
Test Loss of 91024189692.506134, Test MSE of 91024189965.721298
Epoch 24: training loss 81875975845.647
Test Loss of 90564531198.341888, Test MSE of 90564530373.642059
Epoch 25: training loss 78089770947.765
Test Loss of 84988436150.273422, Test MSE of 84988436156.605194
Epoch 26: training loss 74454410074.353
Test Loss of 82171184716.628265, Test MSE of 82171186113.546066
Epoch 27: training loss 71869762108.235
Test Loss of 78219439848.490402, Test MSE of 78219438352.351578
Epoch 28: training loss 67308600229.647
Test Loss of 74661247490.960907, Test MSE of 74661248451.015579
Epoch 29: training loss 65242355034.353
Test Loss of 71624528051.075638, Test MSE of 71624528756.431030
Epoch 30: training loss 62120873426.824
Test Loss of 71142448160.925278, Test MSE of 71142447918.359451
Epoch 31: training loss 58993795222.588
Test Loss of 66182615520.377518, Test MSE of 66182614583.599365
Epoch 32: training loss 55587432824.471
Test Loss of 62831849001.097389, Test MSE of 62831850611.071175
Epoch 33: training loss 53010237778.824
Test Loss of 59580392941.879250, Test MSE of 59580393444.916016
Epoch 34: training loss 50932529618.824
Test Loss of 55080234880.088829, Test MSE of 55080234607.713409
Epoch 35: training loss 48003999269.647
Test Loss of 55585303172.767059, Test MSE of 55585303290.457970
Epoch 36: training loss 45959184820.706
Test Loss of 53561546893.649780, Test MSE of 53561546889.560020
Epoch 37: training loss 43575066691.765
Test Loss of 47958836610.812859, Test MSE of 47958837770.278740
Epoch 38: training loss 41219635222.588
Test Loss of 48098370765.842239, Test MSE of 48098371081.747925
Epoch 39: training loss 39459252954.353
Test Loss of 43909987467.044182, Test MSE of 43909988644.010208
Epoch 40: training loss 37412213150.118
Test Loss of 44114971599.441132, Test MSE of 44114971530.516991
Epoch 41: training loss 35303782287.059
Test Loss of 41302981439.422623, Test MSE of 41302981355.718582
Epoch 42: training loss 33979232752.941
Test Loss of 38632395128.153595, Test MSE of 38632395806.362160
Epoch 43: training loss 32160862817.882
Test Loss of 38826689348.633820, Test MSE of 38826689883.323013
Epoch 44: training loss 30653148980.706
Test Loss of 35185808449.613693, Test MSE of 35185807856.722870
Epoch 45: training loss 29325537837.176
Test Loss of 35545372172.198936, Test MSE of 35545372932.251740
Epoch 46: training loss 28447032252.235
Test Loss of 33499127897.064075, Test MSE of 33499128540.851341
Epoch 47: training loss 26805980555.294
Test Loss of 30346915588.915104, Test MSE of 30346916003.023788
Epoch 48: training loss 26136179410.824
Test Loss of 30172210019.900993, Test MSE of 30172209512.270679
Epoch 49: training loss 24770429929.412
Test Loss of 32551519115.695583, Test MSE of 32551519219.720646
Epoch 50: training loss 23885501048.471
Test Loss of 29018858491.499420, Test MSE of 29018858758.628197
Epoch 51: training loss 23321667136.000
Test Loss of 28031322621.275967, Test MSE of 28031322538.011059
Epoch 52: training loss 21784818435.765
Test Loss of 27310486827.643764, Test MSE of 27310486569.913635
Epoch 53: training loss 21323595384.471
Test Loss of 27659808108.546841, Test MSE of 27659808396.849052
Epoch 54: training loss 20303386078.118
Test Loss of 24975243024.284988, Test MSE of 24975243225.207088
Epoch 55: training loss 19544259346.824
Test Loss of 24964928149.348137, Test MSE of 24964928191.471336
Epoch 56: training loss 18959548434.824
Test Loss of 26347130689.554478, Test MSE of 26347130614.757786
Epoch 57: training loss 17786215280.941
Test Loss of 25963823632.699512, Test MSE of 25963824049.051903
Epoch 58: training loss 17466508427.294
Test Loss of 26020547369.630348, Test MSE of 26020547628.113159
Epoch 59: training loss 16785238460.235
Test Loss of 24721300157.616470, Test MSE of 24721300071.414028
Epoch 60: training loss 16608798027.294
Test Loss of 24656947048.875317, Test MSE of 24656947355.512737
Epoch 61: training loss 16043812600.471
Test Loss of 24731620330.918343, Test MSE of 24731620596.492588
Epoch 62: training loss 15373830132.706
Test Loss of 24854946703.011799, Test MSE of 24854946585.422691
Epoch 63: training loss 15030813903.059
Test Loss of 24211595862.813786, Test MSE of 24211595863.617340
Epoch 64: training loss 14724498010.353
Test Loss of 25251515677.431412, Test MSE of 25251515791.428543
Epoch 65: training loss 14218805993.412
Test Loss of 23284804402.631504, Test MSE of 23284804250.479233
Epoch 66: training loss 13734248621.176
Test Loss of 21899507764.111958, Test MSE of 21899507882.909046
Epoch 67: training loss 13569784391.529
Test Loss of 23035390940.705990, Test MSE of 23035390963.780670
Epoch 68: training loss 12896961953.882
Test Loss of 22286211996.276661, Test MSE of 22286211746.046917
Epoch 69: training loss 12975070832.941
Test Loss of 23110551357.764515, Test MSE of 23110552000.379795
Epoch 70: training loss 12633338132.706
Test Loss of 21327785376.658802, Test MSE of 21327785653.508358
Epoch 71: training loss 12195786650.353
Test Loss of 21595811917.694195, Test MSE of 21595812226.745655
Epoch 72: training loss 11748532679.529
Test Loss of 21249456293.810780, Test MSE of 21249455799.224529
Epoch 73: training loss 11500358181.647
Test Loss of 21919657155.419846, Test MSE of 21919657278.824520
Epoch 74: training loss 11418044363.294
Test Loss of 23058458743.857506, Test MSE of 23058458964.746880
Epoch 75: training loss 11081752941.176
Test Loss of 21473328952.079575, Test MSE of 21473328904.125797
Epoch 76: training loss 10864814578.824
Test Loss of 22464012493.131622, Test MSE of 22464012808.184425
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23012381075.376434, 'MSE - std': 2609778015.237371, 'R2 - mean': 0.834973475494854, 'R2 - std': 0.017075775758676904} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005595 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110540980.706
Test Loss of 410765129258.883850, Test MSE of 410765125957.523071
Epoch 2: training loss 430089682221.176
Test Loss of 410747149968.288757, Test MSE of 410747152400.708252
Epoch 3: training loss 430062521886.118
Test Loss of 410723477537.169800, Test MSE of 410723474476.525940
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077027990.588
Test Loss of 410725763806.474792, Test MSE of 410725765914.290710
Epoch 2: training loss 430066779557.647
Test Loss of 410728061736.396118, Test MSE of 410728058401.081787
Epoch 3: training loss 430066302253.176
Test Loss of 410728775975.685303, Test MSE of 410728773167.345093
Epoch 4: training loss 424679727826.824
Test Loss of 393714834087.507629, Test MSE of 393714839250.229614
Epoch 5: training loss 388240937441.882
Test Loss of 338991878293.738098, Test MSE of 338991879069.955688
Epoch 6: training loss 319634018544.941
Test Loss of 262274006168.107361, Test MSE of 262274005863.557251
Epoch 7: training loss 228383192606.118
Test Loss of 164605071138.235992, Test MSE of 164605068439.448303
Epoch 8: training loss 164224537991.529
Test Loss of 125257567238.160110, Test MSE of 125257568068.656158
Epoch 9: training loss 145128016112.941
Test Loss of 113684490826.632111, Test MSE of 113684493432.092438
Epoch 10: training loss 138071929313.882
Test Loss of 110080342202.698746, Test MSE of 110080340615.260406
Epoch 11: training loss 135711198238.118
Test Loss of 107954375098.580292, Test MSE of 107954376470.935120
Epoch 12: training loss 132665893135.059
Test Loss of 104966495687.374359, Test MSE of 104966496724.914047
Epoch 13: training loss 129259311164.235
Test Loss of 102027252574.415543, Test MSE of 102027250875.517303
Epoch 14: training loss 124198121773.176
Test Loss of 98638583960.581207, Test MSE of 98638584391.432175
Epoch 15: training loss 121570386462.118
Test Loss of 96002761439.422485, Test MSE of 96002761780.150543
Epoch 16: training loss 117265019994.353
Test Loss of 93846361097.003235, Test MSE of 93846360665.318771
Epoch 17: training loss 113363669293.176
Test Loss of 89622151581.201294, Test MSE of 89622151736.556732
Epoch 18: training loss 109099315019.294
Test Loss of 86969676460.246185, Test MSE of 86969677168.476974
Epoch 19: training loss 105340821232.941
Test Loss of 82767121925.923187, Test MSE of 82767121023.595673
Epoch 20: training loss 101416596464.941
Test Loss of 78717645427.383621, Test MSE of 78717643157.706528
Epoch 21: training loss 97343234921.412
Test Loss of 77757378819.672379, Test MSE of 77757378826.813248
Epoch 22: training loss 94231725733.647
Test Loss of 73740398093.504852, Test MSE of 73740397269.262024
Epoch 23: training loss 89750951318.588
Test Loss of 69205295937.984268, Test MSE of 69205295220.717819
Epoch 24: training loss 85229462347.294
Test Loss of 66930185240.166588, Test MSE of 66930185272.177406
Epoch 25: training loss 82752659983.059
Test Loss of 65520543182.008331, Test MSE of 65520542935.996269
Epoch 26: training loss 78940187617.882
Test Loss of 61911624884.538643, Test MSE of 61911624535.123505
Epoch 27: training loss 75624964547.765
Test Loss of 59755367543.885239, Test MSE of 59755368594.856842
Epoch 28: training loss 71984297426.824
Test Loss of 58036959824.318375, Test MSE of 58036958742.714958
Epoch 29: training loss 68545446686.118
Test Loss of 54452983524.634888, Test MSE of 54452983677.064644
Epoch 30: training loss 64852813161.412
Test Loss of 51923757386.750580, Test MSE of 51923756589.246727
Epoch 31: training loss 62297508216.471
Test Loss of 47755817303.070801, Test MSE of 47755816720.573936
Epoch 32: training loss 59913016033.882
Test Loss of 47622381640.025917, Test MSE of 47622380955.678307
Epoch 33: training loss 57273739339.294
Test Loss of 45098319492.916245, Test MSE of 45098319034.584724
Epoch 34: training loss 54451999066.353
Test Loss of 42487798583.559464, Test MSE of 42487798871.363869
Epoch 35: training loss 51845376384.000
Test Loss of 40669053892.768166, Test MSE of 40669053008.833611
Epoch 36: training loss 48725837756.235
Test Loss of 37099647034.757980, Test MSE of 37099646740.735329
Epoch 37: training loss 46768405097.412
Test Loss of 35849619013.419716, Test MSE of 35849619530.875938
Epoch 38: training loss 44122377065.412
Test Loss of 35496328603.779732, Test MSE of 35496328797.941574
Epoch 39: training loss 41991472474.353
Test Loss of 32847536105.254974, Test MSE of 32847535459.254265
Epoch 40: training loss 39384443346.824
Test Loss of 31856871542.463673, Test MSE of 31856871557.401035
Epoch 41: training loss 37926776869.647
Test Loss of 30810822376.425728, Test MSE of 30810822484.830894
Epoch 42: training loss 36268092001.882
Test Loss of 29699535835.513187, Test MSE of 29699535858.334511
Epoch 43: training loss 34090937735.529
Test Loss of 27595898489.543728, Test MSE of 27595898482.773434
Epoch 44: training loss 32721598012.235
Test Loss of 25490235480.136974, Test MSE of 25490235676.311428
Epoch 45: training loss 30878636280.471
Test Loss of 25015830596.235077, Test MSE of 25015830422.301918
Epoch 46: training loss 29822994612.706
Test Loss of 25414859954.643219, Test MSE of 25414860521.976425
Epoch 47: training loss 28827461952.000
Test Loss of 24595152189.008793, Test MSE of 24595151838.380215
Epoch 48: training loss 27484294656.000
Test Loss of 24178664127.200371, Test MSE of 24178664173.487743
Epoch 49: training loss 26394166144.000
Test Loss of 23705647005.438225, Test MSE of 23705646626.040306
Epoch 50: training loss 25469241472.000
Test Loss of 22481860390.974548, Test MSE of 22481860858.317314
Epoch 51: training loss 24098633370.353
Test Loss of 21058371815.241093, Test MSE of 21058371964.242329
Epoch 52: training loss 23467794654.118
Test Loss of 20708267416.936604, Test MSE of 20708267296.432339
Epoch 53: training loss 22390166716.235
Test Loss of 20994495026.939381, Test MSE of 20994495041.702446
Epoch 54: training loss 21932897584.941
Test Loss of 20156355805.290142, Test MSE of 20156355939.786880
Epoch 55: training loss 20738820374.588
Test Loss of 21431137046.389633, Test MSE of 21431136719.858913
Epoch 56: training loss 20505485108.706
Test Loss of 19725805290.321148, Test MSE of 19725805295.693226
Epoch 57: training loss 19215250262.588
Test Loss of 21090506723.568718, Test MSE of 21090506736.697594
Epoch 58: training loss 18614262117.647
Test Loss of 18466629766.574734, Test MSE of 18466629783.676941
Epoch 59: training loss 18321436559.059
Test Loss of 20471173481.077278, Test MSE of 20471173481.282482
Epoch 60: training loss 17876718004.706
Test Loss of 17915012125.852844, Test MSE of 17915011905.072678
Epoch 61: training loss 17331521743.059
Test Loss of 19404404536.981026, Test MSE of 19404404444.537022
Epoch 62: training loss 16762769328.941
Test Loss of 18430053655.100418, Test MSE of 18430053448.494194
Epoch 63: training loss 16122902497.882
Test Loss of 18787725335.692734, Test MSE of 18787725273.095356
Epoch 64: training loss 15823794322.824
Test Loss of 17372956367.785286, Test MSE of 17372956379.952991
Epoch 65: training loss 15333641581.176
Test Loss of 18578095516.727440, Test MSE of 18578095883.707405
Epoch 66: training loss 14927227945.412
Test Loss of 17401822850.073112, Test MSE of 17401822789.739906
Epoch 67: training loss 14565400384.000
Test Loss of 18762502354.391487, Test MSE of 18762502693.274742
Epoch 68: training loss 14326322974.118
Test Loss of 17418765737.521519, Test MSE of 17418765812.498192
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21613977259.656876, 'MSE - std': 3312824423.6707306, 'R2 - mean': 0.8402885685637511, 'R2 - std': 0.01741944979939305} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003922 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043114375.529
Test Loss of 431612393306.150879, Test MSE of 431612396138.783752
Epoch 2: training loss 424023720538.353
Test Loss of 431593064674.502563, Test MSE of 431593066078.541321
Epoch 3: training loss 423996575021.176
Test Loss of 431566330565.834351, Test MSE of 431566334072.168457
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011374230.588
Test Loss of 431569554991.622375, Test MSE of 431569555108.160889
Epoch 2: training loss 424000492242.824
Test Loss of 431572034411.683472, Test MSE of 431572039611.004761
Epoch 3: training loss 423999924585.412
Test Loss of 431572908836.605286, Test MSE of 431572904735.480835
Epoch 4: training loss 418830678979.765
Test Loss of 414423230246.974548, Test MSE of 414423226928.559448
Epoch 5: training loss 382769822298.353
Test Loss of 359295142167.574280, Test MSE of 359295142028.884338
Epoch 6: training loss 314934492099.765
Test Loss of 280609399765.826904, Test MSE of 280609401443.736084
Epoch 7: training loss 223729634063.059
Test Loss of 180511548479.970398, Test MSE of 180511549350.394440
Epoch 8: training loss 160310624948.706
Test Loss of 138372784651.135590, Test MSE of 138372783891.344513
Epoch 9: training loss 140776771041.882
Test Loss of 127070333685.219803, Test MSE of 127070334049.466461
Epoch 10: training loss 135231232828.235
Test Loss of 121737984304.214722, Test MSE of 121737985272.726654
Epoch 11: training loss 132210958095.059
Test Loss of 117933167954.806107, Test MSE of 117933169180.609818
Epoch 12: training loss 128530746458.353
Test Loss of 114287680156.135117, Test MSE of 114287680974.869736
Epoch 13: training loss 124683689170.824
Test Loss of 111051250343.507629, Test MSE of 111051251124.547791
Epoch 14: training loss 121098844340.706
Test Loss of 108353985225.625168, Test MSE of 108353984717.514954
Epoch 15: training loss 117692398622.118
Test Loss of 105291909141.797318, Test MSE of 105291908634.476318
Epoch 16: training loss 113285593012.706
Test Loss of 100510585561.736237, Test MSE of 100510584705.501587
Epoch 17: training loss 109680522782.118
Test Loss of 97281970996.716339, Test MSE of 97281969165.959137
Epoch 18: training loss 105545101552.941
Test Loss of 92078851704.596024, Test MSE of 92078853494.928070
Epoch 19: training loss 102473695442.824
Test Loss of 90738434749.304947, Test MSE of 90738434066.550186
Epoch 20: training loss 99373439322.353
Test Loss of 85513766452.360947, Test MSE of 85513766924.076324
Epoch 21: training loss 94928208474.353
Test Loss of 84335460727.292923, Test MSE of 84335459880.261002
Epoch 22: training loss 91735818480.941
Test Loss of 78796430622.682098, Test MSE of 78796429268.912048
Epoch 23: training loss 87426316062.118
Test Loss of 75332039173.449326, Test MSE of 75332038405.243561
Epoch 24: training loss 84030727348.706
Test Loss of 72256065994.691345, Test MSE of 72256065263.828522
Epoch 25: training loss 80215778063.059
Test Loss of 70232605371.409531, Test MSE of 70232604095.359268
Epoch 26: training loss 76991936165.647
Test Loss of 66477294508.127716, Test MSE of 66477294773.807121
Epoch 27: training loss 72829774351.059
Test Loss of 62890449994.395187, Test MSE of 62890449345.412750
Epoch 28: training loss 70266353513.412
Test Loss of 57585433871.518738, Test MSE of 57585434408.967484
Epoch 29: training loss 67445441551.059
Test Loss of 57441709033.728828, Test MSE of 57441709648.816650
Epoch 30: training loss 63386652235.294
Test Loss of 54005544106.113838, Test MSE of 54005544119.093178
Epoch 31: training loss 60758529957.647
Test Loss of 52148982539.017120, Test MSE of 52148984227.780952
Epoch 32: training loss 57553654091.294
Test Loss of 50237514173.423416, Test MSE of 50237515393.161430
Epoch 33: training loss 54531196167.529
Test Loss of 48828565395.013420, Test MSE of 48828564652.345062
Epoch 34: training loss 52973498556.235
Test Loss of 46688324011.416939, Test MSE of 46688323370.677521
Epoch 35: training loss 50569558829.176
Test Loss of 43735959837.260529, Test MSE of 43735960390.321571
Epoch 36: training loss 47436646076.235
Test Loss of 40914789402.062012, Test MSE of 40914789095.752113
Epoch 37: training loss 45502228675.765
Test Loss of 37947950578.021286, Test MSE of 37947951478.919189
Epoch 38: training loss 43161043847.529
Test Loss of 35556477302.345207, Test MSE of 35556477472.319733
Epoch 39: training loss 40812901428.706
Test Loss of 36467761733.893570, Test MSE of 36467760949.606544
Epoch 40: training loss 38865606467.765
Test Loss of 32746265666.813511, Test MSE of 32746266008.798450
Epoch 41: training loss 37566435011.765
Test Loss of 33102988991.674225, Test MSE of 33102989824.176250
Epoch 42: training loss 35466969780.706
Test Loss of 32220863598.408146, Test MSE of 32220863400.066761
Epoch 43: training loss 33898368865.882
Test Loss of 31045501830.219341, Test MSE of 31045501083.988453
Epoch 44: training loss 32352392372.706
Test Loss of 30479318589.838039, Test MSE of 30479318252.853340
Epoch 45: training loss 30924352986.353
Test Loss of 27198084604.446091, Test MSE of 27198084625.800491
Epoch 46: training loss 29437093601.882
Test Loss of 27243767470.615456, Test MSE of 27243767672.046047
Epoch 47: training loss 28752941052.235
Test Loss of 27220320893.808422, Test MSE of 27220320866.467651
Epoch 48: training loss 27213461417.412
Test Loss of 26231388304.051826, Test MSE of 26231387985.167816
Epoch 49: training loss 25784746277.647
Test Loss of 25614537957.819527, Test MSE of 25614538002.587399
Epoch 50: training loss 24733290273.882
Test Loss of 25886435890.939381, Test MSE of 25886436086.462524
Epoch 51: training loss 24173138797.176
Test Loss of 24262039216.510876, Test MSE of 24262039187.849724
Epoch 52: training loss 23309838584.471
Test Loss of 22822149889.066174, Test MSE of 22822149990.850639
Epoch 53: training loss 22302339489.882
Test Loss of 20920668385.554836, Test MSE of 20920668414.066692
Epoch 54: training loss 21668557522.824
Test Loss of 23577927695.163349, Test MSE of 23577927357.226204
Epoch 55: training loss 20784488752.941
Test Loss of 24728186118.989357, Test MSE of 24728186121.460911
Epoch 56: training loss 19884846701.176
Test Loss of 22137353727.289219, Test MSE of 22137353425.236023
Epoch 57: training loss 19429290420.706
Test Loss of 21901690230.819065, Test MSE of 21901689787.917660
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21671519765.309032, 'MSE - std': 2965314332.3233643, 'R2 - mean': 0.8395183283539179, 'R2 - std': 0.015656400128197894} 
 

Saving model.....
Results After CV: {'MSE - mean': 21671519765.309032, 'MSE - std': 2965314332.3233643, 'R2 - mean': 0.8395183283539179, 'R2 - std': 0.015656400128197894}
Train time: 99.23010549779937
Inference time: 0.07369274019874865
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 66 finished with value: 21671519765.309032 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005528 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526001603.765
Test Loss of 418112522462.186462, Test MSE of 418112522904.631226
Epoch 2: training loss 427506145039.059
Test Loss of 418094790244.315552, Test MSE of 418094788495.386536
Epoch 3: training loss 427479131678.118
Test Loss of 418071504604.646790, Test MSE of 418071512070.746216
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496128993.882
Test Loss of 418075895804.683777, Test MSE of 418075897218.200317
Epoch 2: training loss 427484101571.765
Test Loss of 418077980616.571838, Test MSE of 418077984839.393066
Epoch 3: training loss 427483582704.941
Test Loss of 418077622806.384460, Test MSE of 418077616668.420654
Epoch 4: training loss 427483199969.882
Test Loss of 418077221244.891052, Test MSE of 418077223324.540710
Epoch 5: training loss 427482953246.118
Test Loss of 418075527966.260437, Test MSE of 418075532449.834229
Epoch 6: training loss 419035700645.647
Test Loss of 391567589902.804504, Test MSE of 391567597648.788696
Epoch 7: training loss 364744428001.882
Test Loss of 314200817451.762207, Test MSE of 314200816138.379517
Epoch 8: training loss 277487272658.824
Test Loss of 224139179358.808228, Test MSE of 224139181200.305878
Epoch 9: training loss 199757934501.647
Test Loss of 159920429707.399506, Test MSE of 159920429090.988251
Epoch 10: training loss 156353792496.941
Test Loss of 130442027631.448532, Test MSE of 130442029307.374680
Epoch 11: training loss 139856090081.882
Test Loss of 119010857997.501740, Test MSE of 119010857772.329254
Epoch 12: training loss 136478173244.235
Test Loss of 115359586815.170944, Test MSE of 115359586402.912125
Epoch 13: training loss 132188800090.353
Test Loss of 112424692083.889893, Test MSE of 112424692985.729614
Epoch 14: training loss 130358605447.529
Test Loss of 109604759575.213516, Test MSE of 109604761119.951385
Epoch 15: training loss 126127794838.588
Test Loss of 107108757673.600739, Test MSE of 107108756457.150192
Epoch 16: training loss 122635184670.118
Test Loss of 103671444489.711777, Test MSE of 103671445724.867371
Epoch 17: training loss 118963332336.941
Test Loss of 100330776999.054367, Test MSE of 100330779723.138016
Epoch 18: training loss 115319209539.765
Test Loss of 97038915399.713165, Test MSE of 97038918134.262283
Epoch 19: training loss 111842349793.882
Test Loss of 94079782816.303497, Test MSE of 94079782403.506393
Epoch 20: training loss 107154144677.647
Test Loss of 91512442996.541290, Test MSE of 91512444462.993256
Epoch 21: training loss 102737040572.235
Test Loss of 89038556464.854965, Test MSE of 89038559227.658554
Epoch 22: training loss 99881226345.412
Test Loss of 83562375494.410370, Test MSE of 83562375067.259201
Epoch 23: training loss 97002088214.588
Test Loss of 83284915780.100861, Test MSE of 83284914620.031662
Epoch 24: training loss 93679946337.882
Test Loss of 79347625344.444138, Test MSE of 79347626720.445953
Epoch 25: training loss 91011836796.235
Test Loss of 75672314833.809860, Test MSE of 75672313658.622635
Epoch 26: training loss 87054650322.824
Test Loss of 74325478333.438812, Test MSE of 74325479280.355164
Epoch 27: training loss 83766457795.765
Test Loss of 71795538853.751556, Test MSE of 71795539746.443695
Epoch 28: training loss 81932518475.294
Test Loss of 67328548109.797829, Test MSE of 67328549103.818222
Epoch 29: training loss 77911935427.765
Test Loss of 66481862850.709229, Test MSE of 66481862646.826714
Epoch 30: training loss 74698305513.412
Test Loss of 64122722110.238258, Test MSE of 64122721820.991585
Epoch 31: training loss 71607617596.235
Test Loss of 59181365531.773308, Test MSE of 59181365169.967117
Epoch 32: training loss 69699409483.294
Test Loss of 58556891183.374512, Test MSE of 58556891384.331596
Epoch 33: training loss 67273255845.647
Test Loss of 56821443013.374046, Test MSE of 56821444175.147537
Epoch 34: training loss 63905037876.706
Test Loss of 51566060455.172798, Test MSE of 51566059893.104179
Epoch 35: training loss 61095572419.765
Test Loss of 51127942415.692802, Test MSE of 51127942345.802795
Epoch 36: training loss 58332045191.529
Test Loss of 50059294341.477676, Test MSE of 50059294822.911964
Epoch 37: training loss 55858566166.588
Test Loss of 47029469119.570671, Test MSE of 47029469375.104965
Epoch 38: training loss 53925206076.235
Test Loss of 44694978186.688873, Test MSE of 44694978487.041100
Epoch 39: training loss 51651636291.765
Test Loss of 43886223378.002312, Test MSE of 43886223429.314369
Epoch 40: training loss 49412863472.941
Test Loss of 39020404821.274117, Test MSE of 39020404674.712456
Epoch 41: training loss 46868614799.059
Test Loss of 41162127836.113808, Test MSE of 41162128762.810402
Epoch 42: training loss 44523539162.353
Test Loss of 36778415463.335648, Test MSE of 36778415206.860672
Epoch 43: training loss 43037774080.000
Test Loss of 36616178028.309967, Test MSE of 36616178987.650826
Epoch 44: training loss 41008232944.941
Test Loss of 36784696945.106636, Test MSE of 36784696585.378250
Epoch 45: training loss 39478822588.235
Test Loss of 32333269853.742310, Test MSE of 32333269349.504669
Epoch 46: training loss 37668378917.647
Test Loss of 32330628119.450382, Test MSE of 32330627836.382671
Epoch 47: training loss 35740751360.000
Test Loss of 33797888372.837383, Test MSE of 33797889391.223209
Epoch 48: training loss 34016501293.176
Test Loss of 30445761771.214436, Test MSE of 30445761569.551373
Epoch 49: training loss 32623378838.588
Test Loss of 28381283231.592876, Test MSE of 28381283338.213158
Epoch 50: training loss 30971281626.353
Test Loss of 27468390921.119595, Test MSE of 27468391503.521442
Epoch 51: training loss 30175757718.588
Test Loss of 30717436536.923431, Test MSE of 30717436508.048191
Epoch 52: training loss 28838622787.765
Test Loss of 25416980065.473053, Test MSE of 25416980438.376827
Epoch 53: training loss 27548173982.118
Test Loss of 23508416117.370346, Test MSE of 23508415920.881413
Epoch 54: training loss 26003764457.412
Test Loss of 26293398523.025677, Test MSE of 26293398532.301628
Epoch 55: training loss 25430776530.824
Test Loss of 23596213332.089752, Test MSE of 23596213367.500782
Epoch 56: training loss 24523568534.588
Test Loss of 24104631080.682858, Test MSE of 24104631298.040085
Epoch 57: training loss 23308643501.176
Test Loss of 23092827032.012955, Test MSE of 23092827448.056870
Epoch 58: training loss 22460010292.706
Test Loss of 21858037286.254917, Test MSE of 21858037306.339973
Epoch 59: training loss 22041432771.765
Test Loss of 20505354099.297710, Test MSE of 20505354372.017006
Epoch 60: training loss 20977370710.588
Test Loss of 23195206456.790192, Test MSE of 23195206519.051640
Epoch 61: training loss 20390247691.294
Test Loss of 21520592928.214664, Test MSE of 21520593015.563229
Epoch 62: training loss 19734041577.412
Test Loss of 20633585384.016655, Test MSE of 20633585347.845745
Epoch 63: training loss 19000868039.529
Test Loss of 21777179675.714088, Test MSE of 21777180046.475475
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21777180046.475475, 'MSE - std': 0.0, 'R2 - mean': 0.8304187361720334, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005759 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917629560.471
Test Loss of 424555258577.987488, Test MSE of 424555254649.585022
Epoch 2: training loss 427896021955.765
Test Loss of 424538439164.565369, Test MSE of 424538441661.035767
Epoch 3: training loss 427867149010.824
Test Loss of 424516009388.976196, Test MSE of 424516015231.796875
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887949462.588
Test Loss of 424523384493.272278, Test MSE of 424523384007.687012
Epoch 2: training loss 427875190181.647
Test Loss of 424524105070.915588, Test MSE of 424524109450.244629
Epoch 3: training loss 427874694083.765
Test Loss of 424522815301.818176, Test MSE of 424522823425.952881
Epoch 4: training loss 427874353874.824
Test Loss of 424522500992.799438, Test MSE of 424522501521.011536
Epoch 5: training loss 427874121848.471
Test Loss of 424522413421.968079, Test MSE of 424522415642.079346
Epoch 6: training loss 419849448267.294
Test Loss of 399037814760.786499, Test MSE of 399037815210.670776
Epoch 7: training loss 366218236867.765
Test Loss of 323507299291.284729, Test MSE of 323507294036.631714
Epoch 8: training loss 278888434386.824
Test Loss of 234086136264.216522, Test MSE of 234086141034.058472
Epoch 9: training loss 199149007329.882
Test Loss of 169834121678.848938, Test MSE of 169834122104.730621
Epoch 10: training loss 155074096368.941
Test Loss of 139525779420.942871, Test MSE of 139525779889.004120
Epoch 11: training loss 137677930255.059
Test Loss of 129136708454.269714, Test MSE of 129136708668.957916
Epoch 12: training loss 134187164882.824
Test Loss of 126151866071.909317, Test MSE of 126151867488.134674
Epoch 13: training loss 130451393746.824
Test Loss of 123052939144.142487, Test MSE of 123052940477.133102
Epoch 14: training loss 127503915730.824
Test Loss of 120150798388.111954, Test MSE of 120150799421.352829
Epoch 15: training loss 123184282593.882
Test Loss of 117370020205.731201, Test MSE of 117370019697.983917
Epoch 16: training loss 119576484833.882
Test Loss of 114859548571.566040, Test MSE of 114859549220.709213
Epoch 17: training loss 116265383755.294
Test Loss of 110654917787.151520, Test MSE of 110654917604.470673
Epoch 18: training loss 111124396965.647
Test Loss of 106731900795.351379, Test MSE of 106731902925.527725
Epoch 19: training loss 108145731644.235
Test Loss of 102907861888.799438, Test MSE of 102907863284.958969
Epoch 20: training loss 103901566403.765
Test Loss of 98908914001.306503, Test MSE of 98908916267.293228
Epoch 21: training loss 100647388250.353
Test Loss of 95957860513.073334, Test MSE of 95957864772.890060
Epoch 22: training loss 96884986608.941
Test Loss of 92969906432.769836, Test MSE of 92969904977.526398
Epoch 23: training loss 93065062430.118
Test Loss of 88052168655.204254, Test MSE of 88052168594.782242
Epoch 24: training loss 89814063646.118
Test Loss of 87379188847.566971, Test MSE of 87379186497.468414
Epoch 25: training loss 86291283275.294
Test Loss of 84165722670.071716, Test MSE of 84165721685.481461
Epoch 26: training loss 82543667019.294
Test Loss of 79490599258.307663, Test MSE of 79490599381.825867
Epoch 27: training loss 79868169502.118
Test Loss of 75248218189.694199, Test MSE of 75248218867.412567
Epoch 28: training loss 76784232538.353
Test Loss of 74025126126.293777, Test MSE of 74025125591.546265
Epoch 29: training loss 72672216244.706
Test Loss of 69419325175.176498, Test MSE of 69419327359.088852
Epoch 30: training loss 71007029187.765
Test Loss of 68683826276.670830, Test MSE of 68683825306.530128
Epoch 31: training loss 68028363399.529
Test Loss of 63231575458.080040, Test MSE of 63231574571.077301
Epoch 32: training loss 64522418959.059
Test Loss of 63366154351.330093, Test MSE of 63366155377.108635
Epoch 33: training loss 61882760869.647
Test Loss of 57847448461.590561, Test MSE of 57847447624.077896
Epoch 34: training loss 59246797071.059
Test Loss of 57662478568.608833, Test MSE of 57662479115.970169
Epoch 35: training loss 56507549259.294
Test Loss of 54290645856.111031, Test MSE of 54290646778.542442
Epoch 36: training loss 53921687416.471
Test Loss of 53082781246.889664, Test MSE of 53082782173.057907
Epoch 37: training loss 51153887548.235
Test Loss of 52988702382.693497, Test MSE of 52988703789.015915
Epoch 38: training loss 49134360666.353
Test Loss of 45717115453.231552, Test MSE of 45717116203.477127
Epoch 39: training loss 46875894272.000
Test Loss of 47298061904.418228, Test MSE of 47298062862.556503
Epoch 40: training loss 44078693654.588
Test Loss of 45365070590.519547, Test MSE of 45365070783.325325
Epoch 41: training loss 42672447826.824
Test Loss of 42690935023.715012, Test MSE of 42690935542.983101
Epoch 42: training loss 40844515531.294
Test Loss of 41387021412.197083, Test MSE of 41387020779.431625
Epoch 43: training loss 38343336312.471
Test Loss of 42156912742.328941, Test MSE of 42156912001.578766
Epoch 44: training loss 36148005760.000
Test Loss of 37464092555.932457, Test MSE of 37464094141.972084
Epoch 45: training loss 34915510640.941
Test Loss of 37708013468.039787, Test MSE of 37708013429.424606
Epoch 46: training loss 32694874059.294
Test Loss of 37769494526.578766, Test MSE of 37769495746.216125
Epoch 47: training loss 31652700310.588
Test Loss of 36486871660.132317, Test MSE of 36486871249.540268
Epoch 48: training loss 29730470031.059
Test Loss of 34902793207.235718, Test MSE of 34902792862.644432
Epoch 49: training loss 28529719898.353
Test Loss of 31319794395.462410, Test MSE of 31319793925.562241
Epoch 50: training loss 27161730055.529
Test Loss of 31548030395.662273, Test MSE of 31548029224.709282
Epoch 51: training loss 25502857163.294
Test Loss of 29258926799.618782, Test MSE of 29258927016.441643
Epoch 52: training loss 24704987083.294
Test Loss of 30549827887.433727, Test MSE of 30549827499.530910
Epoch 53: training loss 23315871789.176
Test Loss of 30151150283.355076, Test MSE of 30151150091.563126
Epoch 54: training loss 22453032316.235
Test Loss of 28490578550.791580, Test MSE of 28490577614.815620
Epoch 55: training loss 21498144764.235
Test Loss of 30201561820.409901, Test MSE of 30201561407.372074
Epoch 56: training loss 20484204754.824
Test Loss of 28879764320.111034, Test MSE of 28879764679.141266
Epoch 57: training loss 19919167883.294
Test Loss of 27684805394.890587, Test MSE of 27684806065.396992
Epoch 58: training loss 19156636867.765
Test Loss of 26194119267.131161, Test MSE of 26194118965.458111
Epoch 59: training loss 18247998486.588
Test Loss of 28599222425.730278, Test MSE of 28599222778.484978
Epoch 60: training loss 17875970401.882
Test Loss of 27949888682.785103, Test MSE of 27949889240.702106
Epoch 61: training loss 16836465306.353
Test Loss of 27661386006.088364, Test MSE of 27661386157.509533
Epoch 62: training loss 16369835237.647
Test Loss of 27378225483.147816, Test MSE of 27378225899.513081
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24577702972.994278, 'MSE - std': 2800522926.5188026, 'R2 - mean': 0.8174781902152384, 'R2 - std': 0.012940545956795058} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005545 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927200647.529
Test Loss of 447259436704.007385, Test MSE of 447259438019.312927
Epoch 2: training loss 421906391401.412
Test Loss of 447241081833.497131, Test MSE of 447241085175.720947
Epoch 3: training loss 421879266002.824
Test Loss of 447216804937.193604, Test MSE of 447216814864.110474
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902432376.471
Test Loss of 447224579950.560242, Test MSE of 447224583931.648315
Epoch 2: training loss 421889138206.118
Test Loss of 447224361618.505676, Test MSE of 447224359992.403992
Epoch 3: training loss 421888518746.353
Test Loss of 447223954637.368469, Test MSE of 447223961927.574646
Epoch 4: training loss 421888162394.353
Test Loss of 447224233002.400208, Test MSE of 447224236098.864807
Epoch 5: training loss 421887947233.882
Test Loss of 447223906201.671082, Test MSE of 447223911815.532410
Epoch 6: training loss 413889366136.471
Test Loss of 421639769362.535278, Test MSE of 421639770247.885620
Epoch 7: training loss 360631854140.235
Test Loss of 342957277869.982910, Test MSE of 342957280693.888245
Epoch 8: training loss 274277531407.059
Test Loss of 251491222125.553558, Test MSE of 251491223254.636292
Epoch 9: training loss 195591776557.176
Test Loss of 184938804362.570435, Test MSE of 184938805357.370270
Epoch 10: training loss 151609263194.353
Test Loss of 152231632988.854034, Test MSE of 152231633541.211761
Epoch 11: training loss 135549768583.529
Test Loss of 140342354710.206787, Test MSE of 140342357370.392792
Epoch 12: training loss 131153917921.882
Test Loss of 136419720924.173035, Test MSE of 136419719069.639984
Epoch 13: training loss 128988547553.882
Test Loss of 132956884216.005554, Test MSE of 132956884829.706345
Epoch 14: training loss 123869709854.118
Test Loss of 129967048581.300018, Test MSE of 129967048524.310776
Epoch 15: training loss 121505362913.882
Test Loss of 126851983076.226700, Test MSE of 126851984490.521500
Epoch 16: training loss 118579178977.882
Test Loss of 123384352738.627808, Test MSE of 123384349719.535995
Epoch 17: training loss 113030923444.706
Test Loss of 119230174477.324081, Test MSE of 119230172195.554108
Epoch 18: training loss 110447604254.118
Test Loss of 115967405252.841080, Test MSE of 115967405371.913895
Epoch 19: training loss 106099276257.882
Test Loss of 112724484962.006012, Test MSE of 112724486540.562408
Epoch 20: training loss 103292169035.294
Test Loss of 107259406776.109177, Test MSE of 107259408912.058823
Epoch 21: training loss 99457946684.235
Test Loss of 104444128278.502884, Test MSE of 104444126092.965439
Epoch 22: training loss 95098724261.647
Test Loss of 102889082096.425629, Test MSE of 102889083379.157669
Epoch 23: training loss 92946979614.118
Test Loss of 97580554580.622711, Test MSE of 97580552602.439194
Epoch 24: training loss 89540505163.294
Test Loss of 96107404190.171646, Test MSE of 96107405277.807999
Epoch 25: training loss 85594803169.882
Test Loss of 92132335765.703445, Test MSE of 92132337337.129776
Epoch 26: training loss 81963171719.529
Test Loss of 88448917100.369186, Test MSE of 88448916104.763992
Epoch 27: training loss 79783714243.765
Test Loss of 86672058503.017349, Test MSE of 86672059959.672424
Epoch 28: training loss 76586999175.529
Test Loss of 79941877068.332184, Test MSE of 79941876721.950439
Epoch 29: training loss 73442158983.529
Test Loss of 78676001048.457092, Test MSE of 78676001881.348907
Epoch 30: training loss 69963346718.118
Test Loss of 73847065895.380066, Test MSE of 73847065215.997894
Epoch 31: training loss 68066593159.529
Test Loss of 72052476740.160080, Test MSE of 72052477648.260880
Epoch 32: training loss 64451880688.941
Test Loss of 67293550028.717094, Test MSE of 67293550288.425690
Epoch 33: training loss 61912863563.294
Test Loss of 61615230078.726807, Test MSE of 61615231098.070137
Epoch 34: training loss 59352089509.647
Test Loss of 59416488713.415680, Test MSE of 59416488420.722153
Epoch 35: training loss 57179993630.118
Test Loss of 59741059255.339348, Test MSE of 59741059684.175591
Epoch 36: training loss 54201747636.706
Test Loss of 58034586827.947258, Test MSE of 58034586691.460548
Epoch 37: training loss 51937440045.176
Test Loss of 54620255427.419846, Test MSE of 54620256228.467560
Epoch 38: training loss 50028965240.471
Test Loss of 50347603850.037476, Test MSE of 50347603971.211624
Epoch 39: training loss 48153991137.882
Test Loss of 48400728575.644691, Test MSE of 48400729624.878159
Epoch 40: training loss 44743800048.941
Test Loss of 46308039739.455009, Test MSE of 46308039986.004555
Epoch 41: training loss 43142434657.882
Test Loss of 45771961274.122597, Test MSE of 45771962414.813889
Epoch 42: training loss 41632483282.824
Test Loss of 42714958103.035858, Test MSE of 42714958047.016846
Epoch 43: training loss 39629943326.118
Test Loss of 42626434457.552628, Test MSE of 42626434841.622169
Epoch 44: training loss 37800601246.118
Test Loss of 41525608003.627113, Test MSE of 41525608017.267044
Epoch 45: training loss 35746712033.882
Test Loss of 38432626537.822807, Test MSE of 38432626222.505684
Epoch 46: training loss 34392394744.471
Test Loss of 37227839083.421700, Test MSE of 37227839187.940834
Epoch 47: training loss 32709733767.529
Test Loss of 37038500074.503815, Test MSE of 37038500085.324387
Epoch 48: training loss 30967102727.529
Test Loss of 32883193114.588943, Test MSE of 32883193121.523438
Epoch 49: training loss 30158529746.824
Test Loss of 32005047429.832985, Test MSE of 32005047781.229324
Epoch 50: training loss 28542792560.941
Test Loss of 28275111847.409668, Test MSE of 28275112142.545780
Epoch 51: training loss 27511672636.235
Test Loss of 28914404574.423317, Test MSE of 28914405101.696270
Epoch 52: training loss 25902017893.647
Test Loss of 30143926982.617626, Test MSE of 30143927175.596066
Epoch 53: training loss 25243689675.294
Test Loss of 30631600574.267868, Test MSE of 30631600625.857601
Epoch 54: training loss 23994320794.353
Test Loss of 28215460367.752026, Test MSE of 28215460450.477474
Epoch 55: training loss 22895743435.294
Test Loss of 27205356987.662273, Test MSE of 27205356688.634876
Epoch 56: training loss 21999742870.588
Test Loss of 27684457667.419846, Test MSE of 27684457719.631840
Epoch 57: training loss 21044398949.647
Test Loss of 24966431663.226463, Test MSE of 24966431707.001266
Epoch 58: training loss 20482117135.059
Test Loss of 24982646905.752487, Test MSE of 24982647653.794128
Epoch 59: training loss 20173469812.706
Test Loss of 24280618043.928753, Test MSE of 24280617347.064556
Epoch 60: training loss 19222389270.588
Test Loss of 26705530254.656490, Test MSE of 26705530272.523647
Epoch 61: training loss 18781556717.176
Test Loss of 26716211134.149433, Test MSE of 26716211019.969681
Epoch 62: training loss 17728589376.000
Test Loss of 25593495259.699284, Test MSE of 25593495367.959629
Epoch 63: training loss 17282702490.353
Test Loss of 21633354612.482071, Test MSE of 21633354441.306637
Epoch 64: training loss 17000339979.294
Test Loss of 22681891643.158916, Test MSE of 22681892141.008152
Epoch 65: training loss 16124037327.059
Test Loss of 23342891270.928520, Test MSE of 23342890984.538334
Epoch 66: training loss 15885253703.529
Test Loss of 22297457782.673145, Test MSE of 22297457851.783581
Epoch 67: training loss 15263388653.176
Test Loss of 21094868022.717556, Test MSE of 21094868135.693745
Epoch 68: training loss 15385464154.353
Test Loss of 24632903691.843628, Test MSE of 24632903862.754555
Epoch 69: training loss 14544850450.824
Test Loss of 22381644318.911865, Test MSE of 22381644080.315609
Epoch 70: training loss 14236249253.647
Test Loss of 22700211223.924126, Test MSE of 22700211568.514755
Epoch 71: training loss 13829155787.294
Test Loss of 21777283417.360168, Test MSE of 21777283244.821117
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23644229730.269894, 'MSE - std': 2640334012.532579, 'R2 - mean': 0.829995481014486, 'R2 - std': 0.02061561603119852} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005522 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110298112.000
Test Loss of 410765843619.479858, Test MSE of 410765848314.564392
Epoch 2: training loss 430089119864.471
Test Loss of 410747605824.088867, Test MSE of 410747606190.517761
Epoch 3: training loss 430061258872.471
Test Loss of 410723960323.080078, Test MSE of 410723959511.822937
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080013733.647
Test Loss of 410728643781.597412, Test MSE of 410728647103.540222
Epoch 2: training loss 430067130127.059
Test Loss of 410729210132.731140, Test MSE of 410729210905.930359
Epoch 3: training loss 430066668001.882
Test Loss of 410730349673.669617, Test MSE of 410730348688.441711
Epoch 4: training loss 430066312131.765
Test Loss of 410729682744.033325, Test MSE of 410729679680.945374
Epoch 5: training loss 430066041072.941
Test Loss of 410729279580.875549, Test MSE of 410729271443.741516
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 120415490158.6378, 'MSE - std': 167628336081.5179, 'R2 - mean': 0.025005927449249565, 'R2 - std': 1.3943971087053126} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005503 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043156781.176
Test Loss of 431613715171.213318, Test MSE of 431613718760.496399
Epoch 2: training loss 424023040481.882
Test Loss of 431593701717.175354, Test MSE of 431593693442.369568
Epoch 3: training loss 423995859787.294
Test Loss of 431566421515.609436, Test MSE of 431566421791.448730
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012891196.235
Test Loss of 431571950649.336426, Test MSE of 431571956467.761963
Epoch 2: training loss 424000650420.706
Test Loss of 431574239648.044434, Test MSE of 431574236516.671204
Epoch 3: training loss 424000137216.000
Test Loss of 431573362448.229553, Test MSE of 431573364841.147217
Epoch 4: training loss 423999793513.412
Test Loss of 431572462291.102295, Test MSE of 431572461847.793030
Epoch 5: training loss 423999568715.294
Test Loss of 431572146711.455811, Test MSE of 431572150830.683105
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 182646822293.04688, 'MSE - std': 194859852311.49896, 'R2 - mean': -0.42459462713768586, 'R2 - std': 1.5375426082395598} 
 

Saving model.....
Results After CV: {'MSE - mean': 182646822293.04688, 'MSE - std': 194859852311.49896, 'R2 - mean': -0.42459462713768586, 'R2 - std': 1.5375426082395598}
Train time: 65.71942215300078
Inference time: 0.07424953940062551
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 67 finished with value: 182646822293.04688 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005613 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524462832.941
Test Loss of 418111145076.778137, Test MSE of 418111139453.871399
Epoch 2: training loss 427503026778.353
Test Loss of 418092531200.355286, Test MSE of 418092522130.979309
Epoch 3: training loss 427475673690.353
Test Loss of 418068257722.596375, Test MSE of 418068264424.023132
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491488105.412
Test Loss of 418075340476.905823, Test MSE of 418075339934.184265
Epoch 2: training loss 427480851998.118
Test Loss of 418076295615.215332, Test MSE of 418076296559.331604
Epoch 3: training loss 423932635497.412
Test Loss of 406871306917.455444, Test MSE of 406871306151.894165
Epoch 4: training loss 398061492224.000
Test Loss of 366259996488.660645, Test MSE of 366259995970.020874
Epoch 5: training loss 332846250947.765
Test Loss of 280429030502.092041, Test MSE of 280429035429.011780
Epoch 6: training loss 250015935488.000
Test Loss of 201617846187.673370, Test MSE of 201617847972.796326
Epoch 7: training loss 180925061104.941
Test Loss of 141437597466.233643, Test MSE of 141437599512.949799
Epoch 8: training loss 146519021537.882
Test Loss of 122260399571.349533, Test MSE of 122260398888.388992
Epoch 9: training loss 137343250868.706
Test Loss of 116591064592.936386, Test MSE of 116591062897.736847
Epoch 10: training loss 133473780254.118
Test Loss of 113875854987.162613, Test MSE of 113875855859.499786
Epoch 11: training loss 131114153321.412
Test Loss of 110977960029.327774, Test MSE of 110977960149.775070
Epoch 12: training loss 127807462896.941
Test Loss of 107707902956.813324, Test MSE of 107707904357.503067
Epoch 13: training loss 124553634439.529
Test Loss of 105062906533.929214, Test MSE of 105062909083.941879
Epoch 14: training loss 120402968048.941
Test Loss of 100852036227.345825, Test MSE of 100852036366.912857
Epoch 15: training loss 116167738789.647
Test Loss of 98168413425.373123, Test MSE of 98168414240.451645
Epoch 16: training loss 112287442898.824
Test Loss of 95463713443.086746, Test MSE of 95463714437.607269
Epoch 17: training loss 109078206524.235
Test Loss of 90997874246.469574, Test MSE of 90997874357.319138
Epoch 18: training loss 104011996867.765
Test Loss of 88026558370.198471, Test MSE of 88026559434.993835
Epoch 19: training loss 100610775265.882
Test Loss of 84632553218.546387, Test MSE of 84632552255.141449
Epoch 20: training loss 96698091896.471
Test Loss of 82771094537.238022, Test MSE of 82771094248.472641
Epoch 21: training loss 93179421364.706
Test Loss of 79211015654.773071, Test MSE of 79211014647.334778
Epoch 22: training loss 89700216591.059
Test Loss of 75495662638.663895, Test MSE of 75495664406.662704
Epoch 23: training loss 85363491508.706
Test Loss of 74032046993.380524, Test MSE of 74032046747.554932
Epoch 24: training loss 82506567243.294
Test Loss of 67680390795.636360, Test MSE of 67680391529.967758
Epoch 25: training loss 78986260288.000
Test Loss of 65348820620.583855, Test MSE of 65348820102.514282
Epoch 26: training loss 75163102192.941
Test Loss of 65009344839.831596, Test MSE of 65009345490.552864
Epoch 27: training loss 72335738834.824
Test Loss of 60619562981.470276, Test MSE of 60619562549.360008
Epoch 28: training loss 69307825483.294
Test Loss of 58186597265.380524, Test MSE of 58186597297.050880
Epoch 29: training loss 66838349131.294
Test Loss of 55964129210.359474, Test MSE of 55964130983.334747
Epoch 30: training loss 63626972973.176
Test Loss of 55108586909.816330, Test MSE of 55108588066.056160
Epoch 31: training loss 60356140754.824
Test Loss of 50431116956.217445, Test MSE of 50431117009.066666
Epoch 32: training loss 57627113679.059
Test Loss of 48722545319.824196, Test MSE of 48722544741.166435
Epoch 33: training loss 55535145709.176
Test Loss of 46619353430.043953, Test MSE of 46619354360.008194
Epoch 34: training loss 52357192297.412
Test Loss of 48701329711.196854, Test MSE of 48701329403.671059
Epoch 35: training loss 50503528252.235
Test Loss of 42910549991.838997, Test MSE of 42910549301.911263
Epoch 36: training loss 47768030960.941
Test Loss of 38198600988.483925, Test MSE of 38198600702.175613
Epoch 37: training loss 46386484515.765
Test Loss of 40247098852.878098, Test MSE of 40247098886.722679
Epoch 38: training loss 44183592380.235
Test Loss of 38206337607.653946, Test MSE of 38206338449.045486
Epoch 39: training loss 42079516340.706
Test Loss of 37741888998.299332, Test MSE of 37741889736.824905
Epoch 40: training loss 39484650409.412
Test Loss of 35983836144.129539, Test MSE of 35983836938.288353
Epoch 41: training loss 37606979681.882
Test Loss of 32382666354.527874, Test MSE of 32382666716.549377
Epoch 42: training loss 37033741944.471
Test Loss of 31007482465.236179, Test MSE of 31007483307.299671
Epoch 43: training loss 35111214192.941
Test Loss of 30014292532.704140, Test MSE of 30014292770.861622
Epoch 44: training loss 33526791326.118
Test Loss of 30785930713.034466, Test MSE of 30785931353.948429
Epoch 45: training loss 31816641995.294
Test Loss of 27113313746.402035, Test MSE of 27113313743.992561
Epoch 46: training loss 30136410010.353
Test Loss of 30051844213.488781, Test MSE of 30051844003.715027
Epoch 47: training loss 28868509967.059
Test Loss of 26063775480.597733, Test MSE of 26063775322.892090
Epoch 48: training loss 27703028547.765
Test Loss of 22736080748.191532, Test MSE of 22736081138.079865
Epoch 49: training loss 26796920026.353
Test Loss of 24929515703.813091, Test MSE of 24929515678.415577
Epoch 50: training loss 25733406362.353
Test Loss of 22813546748.979874, Test MSE of 22813546734.784599
Epoch 51: training loss 24714968609.882
Test Loss of 24082995462.928520, Test MSE of 24082995456.308109
Epoch 52: training loss 23691093820.235
Test Loss of 24169437147.521629, Test MSE of 24169437487.640450
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24169437487.64045, 'MSE - std': 0.0, 'R2 - mean': 0.8117899679197238, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005470 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917562096.941
Test Loss of 424557282189.590576, Test MSE of 424557287390.248230
Epoch 2: training loss 427895938108.235
Test Loss of 424540580924.402527, Test MSE of 424540584577.410889
Epoch 3: training loss 427867423683.765
Test Loss of 424518090335.104309, Test MSE of 424518088212.111816
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890231898.353
Test Loss of 424523827289.537842, Test MSE of 424523828067.355530
Epoch 2: training loss 427877678140.235
Test Loss of 424524891730.313232, Test MSE of 424524897630.787048
Epoch 3: training loss 424144216545.882
Test Loss of 412846493071.604004, Test MSE of 412846482719.869385
Epoch 4: training loss 397660906917.647
Test Loss of 372308028665.900513, Test MSE of 372308031295.576538
Epoch 5: training loss 331463839503.059
Test Loss of 287356021282.228088, Test MSE of 287356020227.481140
Epoch 6: training loss 248367549982.118
Test Loss of 210886445657.182526, Test MSE of 210886445262.724487
Epoch 7: training loss 179413291610.353
Test Loss of 152075348927.333801, Test MSE of 152075347379.516144
Epoch 8: training loss 143803720101.647
Test Loss of 133986027249.491562, Test MSE of 133986030290.884918
Epoch 9: training loss 134736459083.294
Test Loss of 128854814683.047882, Test MSE of 128854814360.126099
Epoch 10: training loss 132403592161.882
Test Loss of 126321444239.130234, Test MSE of 126321445045.024353
Epoch 11: training loss 129799766528.000
Test Loss of 123331298902.103165, Test MSE of 123331298188.256226
Epoch 12: training loss 123805481622.588
Test Loss of 119446452594.942398, Test MSE of 119446456631.370255
Epoch 13: training loss 121951767853.176
Test Loss of 116031794314.096695, Test MSE of 116031792097.414871
Epoch 14: training loss 117911390569.412
Test Loss of 112766371073.006714, Test MSE of 112766367548.749481
Epoch 15: training loss 114124850928.941
Test Loss of 109631071666.187363, Test MSE of 109631072488.645966
Epoch 16: training loss 109521878407.529
Test Loss of 105178170613.636826, Test MSE of 105178170366.312088
Epoch 17: training loss 105671769328.941
Test Loss of 102331161421.634979, Test MSE of 102331162898.576950
Epoch 18: training loss 102783581123.765
Test Loss of 96931453692.150818, Test MSE of 96931456135.637604
Epoch 19: training loss 97826752391.529
Test Loss of 92951206553.611847, Test MSE of 92951206548.082886
Epoch 20: training loss 94895073972.706
Test Loss of 91983125751.058060, Test MSE of 91983127435.226517
Epoch 21: training loss 91275496764.235
Test Loss of 86648091384.834610, Test MSE of 86648092031.872681
Epoch 22: training loss 86556684739.765
Test Loss of 81729954546.202179, Test MSE of 81729955166.369339
Epoch 23: training loss 83885682160.941
Test Loss of 80126922576.714325, Test MSE of 80126919137.875580
Epoch 24: training loss 79756909206.588
Test Loss of 78431259559.646545, Test MSE of 78431259673.917007
Epoch 25: training loss 76319878927.059
Test Loss of 74560768444.846634, Test MSE of 74560768450.247803
Epoch 26: training loss 73176788931.765
Test Loss of 70688113751.405975, Test MSE of 70688113154.019745
Epoch 27: training loss 69662113039.059
Test Loss of 66888213039.966690, Test MSE of 66888213828.689888
Epoch 28: training loss 66386728899.765
Test Loss of 62781137260.783714, Test MSE of 62781137813.172707
Epoch 29: training loss 63250634571.294
Test Loss of 62482308605.512840, Test MSE of 62482308906.243729
Epoch 30: training loss 60713098240.000
Test Loss of 56016685804.043488, Test MSE of 56016685315.615616
Epoch 31: training loss 57866421022.118
Test Loss of 56250283878.506592, Test MSE of 56250281857.038818
Epoch 32: training loss 55100646023.529
Test Loss of 55758922593.058525, Test MSE of 55758922945.384270
Epoch 33: training loss 51780883832.471
Test Loss of 52204379759.211662, Test MSE of 52204378986.380424
Epoch 34: training loss 49518494441.412
Test Loss of 51767278904.434883, Test MSE of 51767279601.149071
Epoch 35: training loss 47553446159.059
Test Loss of 50135845348.167473, Test MSE of 50135844523.059319
Epoch 36: training loss 44487622482.824
Test Loss of 47904391700.963219, Test MSE of 47904391257.887566
Epoch 37: training loss 42907796766.118
Test Loss of 40485195784.527412, Test MSE of 40485195924.782417
Epoch 38: training loss 40321396615.529
Test Loss of 40622011837.557251, Test MSE of 40622010284.653946
Epoch 39: training loss 38596161965.176
Test Loss of 40571230104.249825, Test MSE of 40571229682.350510
Epoch 40: training loss 36517855081.412
Test Loss of 40021568834.383530, Test MSE of 40021568874.590767
Epoch 41: training loss 35264077387.294
Test Loss of 35789863086.338188, Test MSE of 35789863955.230026
Epoch 42: training loss 32806864993.882
Test Loss of 36717043554.716629, Test MSE of 36717043418.972656
Epoch 43: training loss 31582264666.353
Test Loss of 35012103678.934074, Test MSE of 35012102929.814354
Epoch 44: training loss 30055421741.176
Test Loss of 35945043769.027061, Test MSE of 35945043747.931152
Epoch 45: training loss 28871592410.353
Test Loss of 30425873888.614388, Test MSE of 30425874525.411350
Epoch 46: training loss 27431161780.706
Test Loss of 33783916518.891510, Test MSE of 33783916615.127098
Epoch 47: training loss 26471033532.235
Test Loss of 30669527545.486004, Test MSE of 30669526981.514248
Epoch 48: training loss 25055050262.588
Test Loss of 32735579075.123756, Test MSE of 32735578721.384514
Epoch 49: training loss 23928346074.353
Test Loss of 28077020860.432106, Test MSE of 28077020853.487202
Epoch 50: training loss 22696778496.000
Test Loss of 30050282126.241962, Test MSE of 30050281441.403683
Epoch 51: training loss 21886796920.471
Test Loss of 27927104182.036549, Test MSE of 27927103886.143147
Epoch 52: training loss 20724828363.294
Test Loss of 28258435660.154522, Test MSE of 28258435172.079983
Epoch 53: training loss 19869651192.471
Test Loss of 28742160718.464031, Test MSE of 28742161577.714508
Epoch 54: training loss 19176166720.000
Test Loss of 28794496480.140644, Test MSE of 28794494158.078655
Epoch 55: training loss 18238277812.706
Test Loss of 25096251519.437428, Test MSE of 25096252165.219273
Epoch 56: training loss 17856674398.118
Test Loss of 25619553042.179966, Test MSE of 25619552950.159958
Epoch 57: training loss 17289546093.176
Test Loss of 26534802743.961140, Test MSE of 26534803663.887177
Epoch 58: training loss 16468084276.706
Test Loss of 27959150739.571594, Test MSE of 27959150434.978088
Epoch 59: training loss 16086904286.118
Test Loss of 27241140733.749710, Test MSE of 27241140643.072617
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25705289065.356533, 'MSE - std': 1535851577.7160835, 'R2 - mean': 0.8086531549471804, 'R2 - std': 0.0031368129725434235} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005568 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927412073.412
Test Loss of 447257663457.206543, Test MSE of 447257667981.498047
Epoch 2: training loss 421906435614.118
Test Loss of 447239498569.845032, Test MSE of 447239502836.616760
Epoch 3: training loss 421879189263.059
Test Loss of 447215517396.593079, Test MSE of 447215520727.230957
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900717718.588
Test Loss of 447223402424.938232, Test MSE of 447223403503.094543
Epoch 2: training loss 421888131072.000
Test Loss of 447223547838.149414, Test MSE of 447223549802.383972
Epoch 3: training loss 418096425080.471
Test Loss of 435166155355.551208, Test MSE of 435166154971.582703
Epoch 4: training loss 391720626898.824
Test Loss of 393303199900.335876, Test MSE of 393303201650.231567
Epoch 5: training loss 325932429312.000
Test Loss of 306200538873.545227, Test MSE of 306200535091.481506
Epoch 6: training loss 242910886731.294
Test Loss of 227195767635.319916, Test MSE of 227195767042.354614
Epoch 7: training loss 174444913965.176
Test Loss of 165241837290.859131, Test MSE of 165241835517.613281
Epoch 8: training loss 141642281743.059
Test Loss of 145455182876.661591, Test MSE of 145455183258.227692
Epoch 9: training loss 132625266085.647
Test Loss of 138546858623.082123, Test MSE of 138546858046.070343
Epoch 10: training loss 129678904470.588
Test Loss of 135856323107.175568, Test MSE of 135856323201.403915
Epoch 11: training loss 125701495024.941
Test Loss of 131892825085.868149, Test MSE of 131892825228.068680
Epoch 12: training loss 122248241603.765
Test Loss of 128866295173.181580, Test MSE of 128866294454.884521
Epoch 13: training loss 120512633795.765
Test Loss of 124513998737.617401, Test MSE of 124514001496.139923
Epoch 14: training loss 115441748419.765
Test Loss of 121397671080.890121, Test MSE of 121397668503.760208
Epoch 15: training loss 112136725624.471
Test Loss of 118407193188.078644, Test MSE of 118407193406.921295
Epoch 16: training loss 108110325278.118
Test Loss of 114169080809.023361, Test MSE of 114169081366.791260
Epoch 17: training loss 104300719194.353
Test Loss of 111962571532.731903, Test MSE of 111962572211.050354
Epoch 18: training loss 100790377622.588
Test Loss of 107006706588.750412, Test MSE of 107006705521.030701
Epoch 19: training loss 96585666334.118
Test Loss of 101191136809.571136, Test MSE of 101191133774.388672
Epoch 20: training loss 92175329219.765
Test Loss of 97493336831.467041, Test MSE of 97493335600.719650
Epoch 21: training loss 89934546477.176
Test Loss of 93000586662.817490, Test MSE of 93000588903.671112
Epoch 22: training loss 85196105336.471
Test Loss of 92997627203.804764, Test MSE of 92997626461.382523
Epoch 23: training loss 82937450480.941
Test Loss of 86201950729.119598, Test MSE of 86201951143.963242
Epoch 24: training loss 79563502049.882
Test Loss of 85220918246.654633, Test MSE of 85220920382.190323
Epoch 25: training loss 75001793054.118
Test Loss of 81321756155.144119, Test MSE of 81321756000.561615
Epoch 26: training loss 72523342441.412
Test Loss of 76700271277.982880, Test MSE of 76700270950.336044
Epoch 27: training loss 69315280173.176
Test Loss of 75688318469.566498, Test MSE of 75688318468.541763
Epoch 28: training loss 66266120086.588
Test Loss of 72101125749.844086, Test MSE of 72101125873.390747
Epoch 29: training loss 62956527744.000
Test Loss of 66083354470.032845, Test MSE of 66083354546.283051
Epoch 30: training loss 60126463744.000
Test Loss of 65095824516.885498, Test MSE of 65095823938.512581
Epoch 31: training loss 57769374569.412
Test Loss of 61419350471.269028, Test MSE of 61419350154.607742
Epoch 32: training loss 54997179512.471
Test Loss of 58412615941.981033, Test MSE of 58412614000.489487
Epoch 33: training loss 51868285236.706
Test Loss of 55003919547.839928, Test MSE of 55003919824.527237
Epoch 34: training loss 50345416192.000
Test Loss of 50638602800.914177, Test MSE of 50638603565.822182
Epoch 35: training loss 47800901338.353
Test Loss of 51436450624.843857, Test MSE of 51436451235.080757
Epoch 36: training loss 45308861552.941
Test Loss of 47380863399.054359, Test MSE of 47380863968.284569
Epoch 37: training loss 43820560948.706
Test Loss of 47294014085.240807, Test MSE of 47294014008.446213
Epoch 38: training loss 40819368944.941
Test Loss of 44695909741.020584, Test MSE of 44695909746.040840
Epoch 39: training loss 38973589993.412
Test Loss of 40908348944.462639, Test MSE of 40908349014.103317
Epoch 40: training loss 37208350682.353
Test Loss of 39764629229.701599, Test MSE of 39764629494.114922
Epoch 41: training loss 36448902113.882
Test Loss of 39222080479.074715, Test MSE of 39222081483.343552
Epoch 42: training loss 34310874021.647
Test Loss of 39993681273.337959, Test MSE of 39993680741.900085
Epoch 43: training loss 32482817716.706
Test Loss of 34379337039.174644, Test MSE of 34379337590.500298
Epoch 44: training loss 31425260016.941
Test Loss of 38286888148.000923, Test MSE of 38286887948.970596
Epoch 45: training loss 29163095461.647
Test Loss of 34929651703.472588, Test MSE of 34929652202.924751
Epoch 46: training loss 28567881728.000
Test Loss of 30824749498.951653, Test MSE of 30824749225.951523
Epoch 47: training loss 27023680933.647
Test Loss of 31110598493.979179, Test MSE of 31110598708.009369
Epoch 48: training loss 25627748013.176
Test Loss of 29427783904.081425, Test MSE of 29427784114.984066
Epoch 49: training loss 24996956449.882
Test Loss of 30896919013.114967, Test MSE of 30896918877.555115
Epoch 50: training loss 23786434725.647
Test Loss of 28566222856.527412, Test MSE of 28566223386.695801
Epoch 51: training loss 22753347757.176
Test Loss of 27666957084.365486, Test MSE of 27666957058.915466
Epoch 52: training loss 21824140928.000
Test Loss of 25684469755.736294, Test MSE of 25684470507.355450
Epoch 53: training loss 21040685244.235
Test Loss of 25444569721.870922, Test MSE of 25444569488.521194
Epoch 54: training loss 20578297825.882
Test Loss of 26442141250.679619, Test MSE of 26442140745.219242
Epoch 55: training loss 19790809916.235
Test Loss of 25524167013.677540, Test MSE of 25524167490.113781
Epoch 56: training loss 18802799454.118
Test Loss of 24540234330.130001, Test MSE of 24540234806.181828
Epoch 57: training loss 18296648832.000
Test Loss of 23263047349.325932, Test MSE of 23263047334.848713
Epoch 58: training loss 17657598622.118
Test Loss of 22722760149.718250, Test MSE of 22722760221.025917
Epoch 59: training loss 17297840990.118
Test Loss of 23905366241.028915, Test MSE of 23905366386.616875
Epoch 60: training loss 16503806433.882
Test Loss of 22939843201.450844, Test MSE of 22939843510.578316
Epoch 61: training loss 15943844698.353
Test Loss of 24588449898.592644, Test MSE of 24588449655.677113
Epoch 62: training loss 15713283380.706
Test Loss of 25588289366.399261, Test MSE of 25588289184.738602
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25666289105.150555, 'MSE - std': 1255229875.2183964, 'R2 - mean': 0.815655585823163, 'R2 - std': 0.010228773473017644} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003841 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430111165500.235
Test Loss of 410765102116.486816, Test MSE of 410765105722.634766
Epoch 2: training loss 430090374445.176
Test Loss of 410746428875.165222, Test MSE of 410746427588.247742
Epoch 3: training loss 430062549353.412
Test Loss of 410721709369.217957, Test MSE of 410721708359.642761
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079295488.000
Test Loss of 410728071077.019897, Test MSE of 410728075640.155701
Epoch 2: training loss 430065333428.706
Test Loss of 410728309642.010193, Test MSE of 410728312038.170898
Epoch 3: training loss 426089910753.882
Test Loss of 398436663007.896362, Test MSE of 398436665215.887512
Epoch 4: training loss 399162487145.412
Test Loss of 356956933899.490967, Test MSE of 356956936642.189026
Epoch 5: training loss 332835957217.882
Test Loss of 270722483335.996307, Test MSE of 270722483677.489410
Epoch 6: training loss 250197869146.353
Test Loss of 194168496988.520142, Test MSE of 194168497459.712677
Epoch 7: training loss 181294759695.059
Test Loss of 135224057298.746872, Test MSE of 135224058011.066177
Epoch 8: training loss 148325600677.647
Test Loss of 116770423291.972229, Test MSE of 116770423187.985596
Epoch 9: training loss 139741073920.000
Test Loss of 111905757476.842209, Test MSE of 111905757439.320999
Epoch 10: training loss 137602267587.765
Test Loss of 109055926992.259140, Test MSE of 109055928542.459396
Epoch 11: training loss 133512001957.647
Test Loss of 106170502237.349380, Test MSE of 106170500209.488464
Epoch 12: training loss 130259717089.882
Test Loss of 103297823643.068954, Test MSE of 103297823136.806046
Epoch 13: training loss 127400660600.471
Test Loss of 99410867914.099030, Test MSE of 99410865599.327499
Epoch 14: training loss 122586025472.000
Test Loss of 98167260640.962524, Test MSE of 98167262213.689178
Epoch 15: training loss 119365976154.353
Test Loss of 94751811742.267471, Test MSE of 94751811684.647903
Epoch 16: training loss 116063234319.059
Test Loss of 92021057292.438690, Test MSE of 92021054506.983414
Epoch 17: training loss 112120396227.765
Test Loss of 89172890212.220276, Test MSE of 89172891127.307007
Epoch 18: training loss 107314334599.529
Test Loss of 85755560614.559921, Test MSE of 85755560173.032272
Epoch 19: training loss 103617619847.529
Test Loss of 82214318244.901428, Test MSE of 82214319743.941727
Epoch 20: training loss 100098617976.471
Test Loss of 79707217438.563629, Test MSE of 79707216987.960281
Epoch 21: training loss 96764423424.000
Test Loss of 77511399333.967606, Test MSE of 77511400728.222229
Epoch 22: training loss 94032272745.412
Test Loss of 73461461236.509018, Test MSE of 73461461227.290054
Epoch 23: training loss 88924698804.706
Test Loss of 71366396790.108276, Test MSE of 71366396832.645309
Epoch 24: training loss 85590274213.647
Test Loss of 70555413350.944931, Test MSE of 70555414364.044800
Epoch 25: training loss 81327240207.059
Test Loss of 67331781114.076813, Test MSE of 67331781398.174370
Epoch 26: training loss 79700381003.294
Test Loss of 62130824047.474319, Test MSE of 62130824047.183975
Epoch 27: training loss 76184083742.118
Test Loss of 61149597959.463211, Test MSE of 61149597396.577827
Epoch 28: training loss 72864386966.588
Test Loss of 57222884623.518738, Test MSE of 57222884795.538162
Epoch 29: training loss 69764269191.529
Test Loss of 57534067630.023140, Test MSE of 57534067962.332626
Epoch 30: training loss 66961230817.882
Test Loss of 52669180103.966682, Test MSE of 52669179293.027786
Epoch 31: training loss 64013183021.176
Test Loss of 50436908619.105972, Test MSE of 50436908092.362167
Epoch 32: training loss 61478684024.471
Test Loss of 47042425739.905602, Test MSE of 47042425508.095718
Epoch 33: training loss 58845909797.647
Test Loss of 47218635534.807961, Test MSE of 47218635311.426682
Epoch 34: training loss 55859824496.941
Test Loss of 45987168769.658493, Test MSE of 45987168414.350052
Epoch 35: training loss 53631560041.412
Test Loss of 43536645162.173065, Test MSE of 43536645789.292557
Epoch 36: training loss 52409133989.647
Test Loss of 40899741185.184639, Test MSE of 40899740656.782059
Epoch 37: training loss 49002105494.588
Test Loss of 39400428426.010178, Test MSE of 39400428999.254219
Epoch 38: training loss 46456437360.941
Test Loss of 38021543631.785286, Test MSE of 38021544030.422531
Epoch 39: training loss 44531276769.882
Test Loss of 35071282629.478943, Test MSE of 35071282736.216408
Epoch 40: training loss 42474967755.294
Test Loss of 36488367610.550674, Test MSE of 36488368383.300194
Epoch 41: training loss 40776354582.588
Test Loss of 33447502800.140675, Test MSE of 33447502066.736145
Epoch 42: training loss 39013574249.412
Test Loss of 35492397486.260063, Test MSE of 35492397534.964561
Epoch 43: training loss 37016198287.059
Test Loss of 30941581088.340584, Test MSE of 30941581427.283035
Epoch 44: training loss 35608846870.588
Test Loss of 31058283152.762611, Test MSE of 31058283125.942822
Epoch 45: training loss 33966671984.941
Test Loss of 28370160485.049515, Test MSE of 28370161244.710712
Epoch 46: training loss 32577945682.824
Test Loss of 28638408586.484035, Test MSE of 28638408539.830059
Epoch 47: training loss 31004066281.412
Test Loss of 27067403625.551132, Test MSE of 27067403647.245335
Epoch 48: training loss 29824193084.235
Test Loss of 26063621844.523830, Test MSE of 26063621819.988647
Epoch 49: training loss 28213962699.294
Test Loss of 24791699007.733456, Test MSE of 24791698602.185860
Epoch 50: training loss 27167781624.471
Test Loss of 22891047333.730679, Test MSE of 22891047216.127010
Epoch 51: training loss 26029844656.941
Test Loss of 24642115199.229984, Test MSE of 24642115263.856258
Epoch 52: training loss 25074938236.235
Test Loss of 24080013188.797779, Test MSE of 24080012809.633438
Epoch 53: training loss 24144228724.706
Test Loss of 22268305780.449791, Test MSE of 22268305772.213718
Epoch 54: training loss 22935358656.000
Test Loss of 21561077006.571033, Test MSE of 21561076747.360497
Epoch 55: training loss 22623716762.353
Test Loss of 21655580907.979641, Test MSE of 21655581118.543514
Epoch 56: training loss 21409842665.412
Test Loss of 20800437701.478947, Test MSE of 20800437472.805328
Epoch 57: training loss 20857260333.176
Test Loss of 19655819662.985653, Test MSE of 19655819475.565411
Epoch 58: training loss 20137019264.000
Test Loss of 20414169175.189262, Test MSE of 20414169288.799976
Epoch 59: training loss 19843740242.824
Test Loss of 18663731916.942158, Test MSE of 18663731948.933681
Epoch 60: training loss 18703893232.941
Test Loss of 19828230410.306339, Test MSE of 19828230275.443684
Epoch 61: training loss 18446618612.706
Test Loss of 19494113933.919483, Test MSE of 19494113750.801655
Epoch 62: training loss 17601156975.059
Test Loss of 22341181671.241093, Test MSE of 22341181075.192120
Epoch 63: training loss 17252139766.588
Test Loss of 19835493847.011570, Test MSE of 19835493925.060799
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24208590310.128113, 'MSE - std': 2748883202.458811, 'R2 - mean': 0.8208135221557112, 'R2 - std': 0.012581088057222804} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005387 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043750219.294
Test Loss of 431612441010.050903, Test MSE of 431612440289.677856
Epoch 2: training loss 424023373342.118
Test Loss of 431592730095.178162, Test MSE of 431592733999.482849
Epoch 3: training loss 423995901349.647
Test Loss of 431565253952.799622, Test MSE of 431565251242.027954
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012732777.412
Test Loss of 431567705957.523376, Test MSE of 431567703911.644165
Epoch 2: training loss 423999617987.765
Test Loss of 431569168189.719604, Test MSE of 431569167580.898315
Epoch 3: training loss 420383087194.353
Test Loss of 419726543309.534485, Test MSE of 419726542107.591980
Epoch 4: training loss 394416937200.941
Test Loss of 378300016422.974548, Test MSE of 378300021111.249512
Epoch 5: training loss 329076265321.412
Test Loss of 290981774418.924561, Test MSE of 290981775757.123352
Epoch 6: training loss 246806929106.824
Test Loss of 212029836529.192047, Test MSE of 212029835907.615387
Epoch 7: training loss 179461225472.000
Test Loss of 148162216474.298950, Test MSE of 148162214640.056732
Epoch 8: training loss 145114150610.824
Test Loss of 128771002517.738083, Test MSE of 128771003061.809326
Epoch 9: training loss 136930748265.412
Test Loss of 122421000958.696899, Test MSE of 122421002119.536697
Epoch 10: training loss 131755872888.471
Test Loss of 119596950936.936600, Test MSE of 119596949541.874344
Epoch 11: training loss 129211963361.882
Test Loss of 115836446980.620087, Test MSE of 115836445706.434845
Epoch 12: training loss 126164239992.471
Test Loss of 112459769513.876907, Test MSE of 112459768895.184341
Epoch 13: training loss 123080953765.647
Test Loss of 110103617482.928268, Test MSE of 110103615543.429459
Epoch 14: training loss 117991495243.294
Test Loss of 107898860302.807953, Test MSE of 107898859400.602829
Epoch 15: training loss 115919831461.647
Test Loss of 104621044475.853775, Test MSE of 104621045012.500229
Epoch 16: training loss 111978276638.118
Test Loss of 99929405918.593246, Test MSE of 99929408643.636810
Epoch 17: training loss 108190700875.294
Test Loss of 96622052578.976395, Test MSE of 96622053526.694199
Epoch 18: training loss 103989855141.647
Test Loss of 92756480676.664505, Test MSE of 92756481978.722290
Epoch 19: training loss 99867677771.294
Test Loss of 90119695646.682098, Test MSE of 90119697526.781494
Epoch 20: training loss 95074286516.706
Test Loss of 84467738384.229523, Test MSE of 84467738312.710861
Epoch 21: training loss 93226675787.294
Test Loss of 83288530727.922256, Test MSE of 83288530361.884140
Epoch 22: training loss 88482481799.529
Test Loss of 76701987807.304031, Test MSE of 76701987477.577942
Epoch 23: training loss 85089131196.235
Test Loss of 73313723708.061081, Test MSE of 73313724507.498642
Epoch 24: training loss 82585019128.471
Test Loss of 71473817049.380844, Test MSE of 71473815391.528549
Epoch 25: training loss 78585302196.706
Test Loss of 68542002270.297081, Test MSE of 68542002494.492783
Epoch 26: training loss 75446577889.882
Test Loss of 66692580224.533089, Test MSE of 66692579243.519653
Epoch 27: training loss 72495021522.824
Test Loss of 64288083704.536789, Test MSE of 64288082746.102943
Epoch 28: training loss 69102423687.529
Test Loss of 60587334165.560387, Test MSE of 60587334436.205452
Epoch 29: training loss 66385123343.059
Test Loss of 56973036866.695045, Test MSE of 56973036586.680374
Epoch 30: training loss 63244371019.294
Test Loss of 55601710274.754280, Test MSE of 55601710122.237419
Epoch 31: training loss 59960118723.765
Test Loss of 52514130533.167976, Test MSE of 52514130068.976891
Epoch 32: training loss 57935256176.941
Test Loss of 50178402665.077278, Test MSE of 50178402626.448563
Epoch 33: training loss 54709140449.882
Test Loss of 49901662408.914391, Test MSE of 49901663155.186295
Epoch 34: training loss 53274697057.882
Test Loss of 43859406222.511803, Test MSE of 43859406634.198380
Epoch 35: training loss 50525662230.588
Test Loss of 46009788085.249420, Test MSE of 46009787217.397522
Epoch 36: training loss 48334508958.118
Test Loss of 42304235828.005554, Test MSE of 42304235669.559799
Epoch 37: training loss 45776531930.353
Test Loss of 39247874378.750580, Test MSE of 39247873830.207748
Epoch 38: training loss 43870524009.412
Test Loss of 39619217810.776489, Test MSE of 39619217413.783905
Epoch 39: training loss 42099692016.941
Test Loss of 38546808200.825546, Test MSE of 38546807258.740524
Epoch 40: training loss 40212185758.118
Test Loss of 32224994488.329475, Test MSE of 32224994744.836941
Epoch 41: training loss 38363325274.353
Test Loss of 33775866314.691345, Test MSE of 33775866384.996487
Epoch 42: training loss 36606809697.882
Test Loss of 33145420386.324848, Test MSE of 33145420786.271526
Epoch 43: training loss 34645533244.235
Test Loss of 27828203722.809811, Test MSE of 27828202836.440964
Epoch 44: training loss 33702880768.000
Test Loss of 27555296287.748264, Test MSE of 27555296699.275654
Epoch 45: training loss 31493431747.765
Test Loss of 28879259351.366959, Test MSE of 28879259078.713734
Epoch 46: training loss 30600039529.412
Test Loss of 31042899680.844055, Test MSE of 31042899411.227909
Epoch 47: training loss 29349799326.118
Test Loss of 28254870972.001850, Test MSE of 28254870769.102703
Epoch 48: training loss 27764391390.118
Test Loss of 29072662656.888477, Test MSE of 29072662494.936657
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25181404747.08982, 'MSE - std': 3135372195.394459, 'R2 - mean': 0.8132276731446005, 'R2 - std': 0.01888934732171412} 
 

Saving model.....
Results After CV: {'MSE - mean': 25181404747.08982, 'MSE - std': 3135372195.394459, 'R2 - mean': 0.8132276731446005, 'R2 - std': 0.01888934732171412}
Train time: 89.06254548160068
Inference time: 0.07527373939956306
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 68 finished with value: 25181404747.08982 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003655 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524954834.824
Test Loss of 418111662702.027283, Test MSE of 418111669036.016418
Epoch 2: training loss 427503677440.000
Test Loss of 418092425900.087891, Test MSE of 418092432734.354858
Epoch 3: training loss 427476113287.529
Test Loss of 418068140521.852417, Test MSE of 418068143979.635559
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492861470.118
Test Loss of 418073224503.961121, Test MSE of 418073225658.237793
Epoch 2: training loss 427481249792.000
Test Loss of 418074478215.372681, Test MSE of 418074470243.296692
Epoch 3: training loss 427480739237.647
Test Loss of 418074045060.767090, Test MSE of 418074049897.399719
Epoch 4: training loss 427480362044.235
Test Loss of 418073944050.735107, Test MSE of 418073944192.779297
Epoch 5: training loss 420363641193.412
Test Loss of 395944465828.922485, Test MSE of 395944469747.357605
Epoch 6: training loss 374319828751.059
Test Loss of 328952130696.201721, Test MSE of 328952126670.459778
Epoch 7: training loss 295651218371.765
Test Loss of 243072827892.274811, Test MSE of 243072830386.436401
Epoch 8: training loss 216932604807.529
Test Loss of 173861395492.715240, Test MSE of 173861393957.599060
Epoch 9: training loss 159131182878.118
Test Loss of 125951391237.566498, Test MSE of 125951389169.551926
Epoch 10: training loss 139274071311.059
Test Loss of 118523101077.881104, Test MSE of 118523099764.043686
Epoch 11: training loss 135650005022.118
Test Loss of 115705993537.672913, Test MSE of 115705992087.948700
Epoch 12: training loss 132885029918.118
Test Loss of 112941080699.173721, Test MSE of 112941080584.657745
Epoch 13: training loss 128859661583.059
Test Loss of 109234848045.301880, Test MSE of 109234845028.223755
Epoch 14: training loss 124481904730.353
Test Loss of 106221106653.771912, Test MSE of 106221106644.706894
Epoch 15: training loss 121751632640.000
Test Loss of 102521061628.032379, Test MSE of 102521061895.737656
Epoch 16: training loss 118627050194.824
Test Loss of 99827312615.839005, Test MSE of 99827313800.785248
Epoch 17: training loss 113750772736.000
Test Loss of 95597705242.055984, Test MSE of 95597704389.805634
Epoch 18: training loss 109279324807.529
Test Loss of 91913080550.595413, Test MSE of 91913080961.389526
Epoch 19: training loss 105687889829.647
Test Loss of 88500904523.443909, Test MSE of 88500905399.395416
Epoch 20: training loss 101607821658.353
Test Loss of 86380764693.200089, Test MSE of 86380762048.567886
Epoch 21: training loss 97820728545.882
Test Loss of 82545504527.929672, Test MSE of 82545505991.151016
Epoch 22: training loss 94003115715.765
Test Loss of 78694673110.251221, Test MSE of 78694673547.331635
Epoch 23: training loss 89961086878.118
Test Loss of 74524591508.815170, Test MSE of 74524591812.083252
Epoch 24: training loss 87362221650.824
Test Loss of 70905989652.963226, Test MSE of 70905990433.029816
Epoch 25: training loss 82659318949.647
Test Loss of 69865428201.319458, Test MSE of 69865428548.112534
Epoch 26: training loss 78856233170.824
Test Loss of 65478436210.705528, Test MSE of 65478435863.516190
Epoch 27: training loss 76065308122.353
Test Loss of 65118270655.629890, Test MSE of 65118270105.241859
Epoch 28: training loss 72713020054.588
Test Loss of 58480584837.122368, Test MSE of 58480586421.342033
Epoch 29: training loss 68970232320.000
Test Loss of 56695801916.165627, Test MSE of 56695803273.582733
Epoch 30: training loss 65861870644.706
Test Loss of 53662854909.335182, Test MSE of 53662855248.456673
Epoch 31: training loss 63280890277.647
Test Loss of 53013631490.487160, Test MSE of 53013630606.843208
Epoch 32: training loss 59689779493.647
Test Loss of 52867343546.892433, Test MSE of 52867343899.581543
Epoch 33: training loss 56983515708.235
Test Loss of 48732506576.743927, Test MSE of 48732507453.763649
Epoch 34: training loss 54425616135.529
Test Loss of 46596273012.955818, Test MSE of 46596273511.655533
Epoch 35: training loss 50519580837.647
Test Loss of 43939206022.958130, Test MSE of 43939206593.047348
Epoch 36: training loss 49288374128.941
Test Loss of 39849523438.293777, Test MSE of 39849522680.649902
Epoch 37: training loss 46022116321.882
Test Loss of 38208284346.537125, Test MSE of 38208284601.676338
Epoch 38: training loss 43873609901.176
Test Loss of 39591947613.387001, Test MSE of 39591947930.416840
Epoch 39: training loss 42092261187.765
Test Loss of 33940690853.514690, Test MSE of 33940691239.869240
Epoch 40: training loss 39664062934.588
Test Loss of 36477846803.956512, Test MSE of 36477846973.723244
Epoch 41: training loss 37356042684.235
Test Loss of 31964071935.526257, Test MSE of 31964072285.309132
Epoch 42: training loss 36191059960.471
Test Loss of 31989795094.088364, Test MSE of 31989795075.211014
Epoch 43: training loss 33389541511.529
Test Loss of 29270701974.354847, Test MSE of 29270702881.565929
Epoch 44: training loss 32427779203.765
Test Loss of 28407988788.704140, Test MSE of 28407988896.975544
Epoch 45: training loss 30990564491.294
Test Loss of 30496792219.743698, Test MSE of 30496792025.643257
Epoch 46: training loss 28991532822.588
Test Loss of 24860299820.176727, Test MSE of 24860299837.665264
Epoch 47: training loss 28142631695.059
Test Loss of 24933329496.235023, Test MSE of 24933329639.446362
Epoch 48: training loss 26683704688.941
Test Loss of 22650659546.041176, Test MSE of 22650659401.125614
Epoch 49: training loss 25616535555.765
Test Loss of 25892277926.639832, Test MSE of 25892278132.599365
Epoch 50: training loss 24334175186.824
Test Loss of 23594859176.297943, Test MSE of 23594858834.551659
Epoch 51: training loss 23641589737.412
Test Loss of 24713473488.033310, Test MSE of 24713473824.539371
Epoch 52: training loss 22457706100.706
Test Loss of 23078170749.068703, Test MSE of 23078171421.371944
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23078171421.371944, 'MSE - std': 0.0, 'R2 - mean': 0.8202877751792241, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005510 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918441532.235
Test Loss of 424555695143.557739, Test MSE of 424555703096.442200
Epoch 2: training loss 427897659392.000
Test Loss of 424538967113.667358, Test MSE of 424538969896.684753
Epoch 3: training loss 427869422049.882
Test Loss of 424516420031.215332, Test MSE of 424516427305.775696
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886348649.412
Test Loss of 424521809946.529724, Test MSE of 424521806619.342651
Epoch 2: training loss 427875856143.059
Test Loss of 424521782022.099487, Test MSE of 424521788059.952698
Epoch 3: training loss 427875311856.941
Test Loss of 424520869323.769592, Test MSE of 424520865577.119141
Epoch 4: training loss 427874880090.353
Test Loss of 424520782412.391418, Test MSE of 424520778945.096008
Epoch 5: training loss 421295052679.529
Test Loss of 403963649837.657166, Test MSE of 403963647406.473633
Epoch 6: training loss 376143210736.941
Test Loss of 338638426704.181335, Test MSE of 338638430030.552612
Epoch 7: training loss 297407034789.647
Test Loss of 254512837418.104095, Test MSE of 254512839541.947144
Epoch 8: training loss 217963511567.059
Test Loss of 185312923554.198486, Test MSE of 185312924537.675018
Epoch 9: training loss 158418058059.294
Test Loss of 137904875006.934082, Test MSE of 137904874057.448425
Epoch 10: training loss 138258009720.471
Test Loss of 130396567060.726349, Test MSE of 130396568882.043777
Epoch 11: training loss 132899768862.118
Test Loss of 127462759691.902847, Test MSE of 127462758792.105255
Epoch 12: training loss 129721000658.824
Test Loss of 123921097535.896362, Test MSE of 123921101440.716629
Epoch 13: training loss 127554517925.647
Test Loss of 121851909825.169556, Test MSE of 121851907392.773453
Epoch 14: training loss 122184988762.353
Test Loss of 117276614972.461716, Test MSE of 117276615932.744080
Epoch 15: training loss 118188314654.118
Test Loss of 114644684130.124451, Test MSE of 114644686371.000916
Epoch 16: training loss 115927872271.059
Test Loss of 110549247752.231323, Test MSE of 110549247278.721832
Epoch 17: training loss 111277060547.765
Test Loss of 106570924751.145035, Test MSE of 106570922770.718216
Epoch 18: training loss 108179478829.176
Test Loss of 103500998428.128616, Test MSE of 103500997841.314545
Epoch 19: training loss 103310923806.118
Test Loss of 99087343804.550537, Test MSE of 99087341482.286209
Epoch 20: training loss 100801836634.353
Test Loss of 96367567158.776779, Test MSE of 96367566548.036469
Epoch 21: training loss 96162483712.000
Test Loss of 90297744207.529953, Test MSE of 90297744965.167068
Epoch 22: training loss 92149094610.824
Test Loss of 86246335150.930374, Test MSE of 86246334743.926147
Epoch 23: training loss 87962675200.000
Test Loss of 84516366674.727737, Test MSE of 84516366214.637009
Epoch 24: training loss 85420413048.471
Test Loss of 80722464803.057129, Test MSE of 80722464739.563202
Epoch 25: training loss 81676484065.882
Test Loss of 76278794185.519318, Test MSE of 76278794493.966110
Epoch 26: training loss 78575986191.059
Test Loss of 72503010122.318756, Test MSE of 72503008893.052734
Epoch 27: training loss 74471553822.118
Test Loss of 69175801508.744858, Test MSE of 69175802147.731491
Epoch 28: training loss 70531527499.294
Test Loss of 66689628905.201019, Test MSE of 66689630896.869560
Epoch 29: training loss 67287083038.118
Test Loss of 64969714998.539902, Test MSE of 64969716911.678841
Epoch 30: training loss 64432657031.529
Test Loss of 59408507476.918808, Test MSE of 59408507945.835403
Epoch 31: training loss 60700151009.882
Test Loss of 59240535455.237564, Test MSE of 59240537144.452301
Epoch 32: training loss 57383814008.471
Test Loss of 53974966819.175575, Test MSE of 53974964653.338921
Epoch 33: training loss 55311587644.235
Test Loss of 53643837683.031227, Test MSE of 53643838501.397644
Epoch 34: training loss 51982419079.529
Test Loss of 49343172968.046265, Test MSE of 49343171667.636848
Epoch 35: training loss 50239764495.059
Test Loss of 48420562277.440666, Test MSE of 48420563074.191933
Epoch 36: training loss 46617925511.529
Test Loss of 45148885904.433029, Test MSE of 45148885786.759125
Epoch 37: training loss 43544145799.529
Test Loss of 46665436452.063843, Test MSE of 46665436599.924820
Epoch 38: training loss 42079799228.235
Test Loss of 42610877322.274345, Test MSE of 42610877994.636223
Epoch 39: training loss 39589520293.647
Test Loss of 40522593349.877403, Test MSE of 40522593889.928406
Epoch 40: training loss 37121256734.118
Test Loss of 35794846390.984039, Test MSE of 35794847507.009003
Epoch 41: training loss 35083429376.000
Test Loss of 36593143858.216980, Test MSE of 36593141987.693512
Epoch 42: training loss 33590930168.471
Test Loss of 34973927226.448303, Test MSE of 34973927903.602676
Epoch 43: training loss 31916656880.941
Test Loss of 32950543140.892899, Test MSE of 32950543720.438816
Epoch 44: training loss 30022615311.059
Test Loss of 31266314567.357853, Test MSE of 31266313754.153336
Epoch 45: training loss 28605205029.647
Test Loss of 31743332544.340504, Test MSE of 31743332745.285828
Epoch 46: training loss 27231241238.588
Test Loss of 29186104962.161461, Test MSE of 29186105263.821384
Epoch 47: training loss 25684383194.353
Test Loss of 29889148874.940552, Test MSE of 29889149245.239582
Epoch 48: training loss 23933712188.235
Test Loss of 29377957733.085358, Test MSE of 29377956736.566036
Epoch 49: training loss 23521372476.235
Test Loss of 27894668240.862366, Test MSE of 27894668276.344574
Epoch 50: training loss 22477670275.765
Test Loss of 27266064244.718945, Test MSE of 27266064031.968597
Epoch 51: training loss 21056323365.647
Test Loss of 25637100240.803146, Test MSE of 25637100400.429989
Epoch 52: training loss 20482755542.588
Test Loss of 24991617461.740459, Test MSE of 24991617153.154369
Epoch 53: training loss 19446793980.235
Test Loss of 27102706814.726810, Test MSE of 27102706905.648029
Epoch 54: training loss 18797545630.118
Test Loss of 27551008668.750404, Test MSE of 27551008308.043812
Epoch 55: training loss 17769058699.294
Test Loss of 23258242104.849411, Test MSE of 23258242216.535149
Epoch 56: training loss 17242082198.588
Test Loss of 26410736154.648163, Test MSE of 26410735674.652393
Epoch 57: training loss 16463897897.412
Test Loss of 24172546278.950729, Test MSE of 24172545824.048012
Epoch 58: training loss 15952212032.000
Test Loss of 24999598933.688641, Test MSE of 24999598808.679962
Epoch 59: training loss 15593219817.412
Test Loss of 26351887740.891048, Test MSE of 26351887793.415115
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24715029607.39353, 'MSE - std': 1636858186.0215855, 'R2 - mean': 0.8160763960501092, 'R2 - std': 0.004211379129114956} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005458 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927018134.588
Test Loss of 447258147920.299805, Test MSE of 447258153609.679626
Epoch 2: training loss 421906117210.353
Test Loss of 447239708260.789246, Test MSE of 447239719343.227905
Epoch 3: training loss 421878308743.529
Test Loss of 447215080626.365051, Test MSE of 447215084447.653870
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897029752.471
Test Loss of 447223662673.957886, Test MSE of 447223668198.879150
Epoch 2: training loss 421887972653.176
Test Loss of 447224543521.458252, Test MSE of 447224543042.025574
Epoch 3: training loss 421887453184.000
Test Loss of 447224235064.138794, Test MSE of 447224231699.319946
Epoch 4: training loss 421887108156.235
Test Loss of 447224072580.944702, Test MSE of 447224076608.780579
Epoch 5: training loss 415102363166.118
Test Loss of 425542470877.475830, Test MSE of 425542475321.675659
Epoch 6: training loss 370211611949.176
Test Loss of 358124021029.011353, Test MSE of 358124018735.783936
Epoch 7: training loss 291664556393.412
Test Loss of 271365691186.868378, Test MSE of 271365687286.121277
Epoch 8: training loss 214363046550.588
Test Loss of 200120188992.429321, Test MSE of 200120187478.165344
Epoch 9: training loss 153336411708.235
Test Loss of 149190353852.965057, Test MSE of 149190353818.902832
Epoch 10: training loss 133767018496.000
Test Loss of 140295021019.640076, Test MSE of 140295018547.871918
Epoch 11: training loss 131720666503.529
Test Loss of 136694046748.424698, Test MSE of 136694051366.145447
Epoch 12: training loss 129173187674.353
Test Loss of 133395348818.017120, Test MSE of 133395348145.462357
Epoch 13: training loss 125411898789.647
Test Loss of 129743588133.840393, Test MSE of 129743589051.732742
Epoch 14: training loss 120740443708.235
Test Loss of 125389104523.340271, Test MSE of 125389106272.459396
Epoch 15: training loss 117831875614.118
Test Loss of 123211504381.098312, Test MSE of 123211504341.891266
Epoch 16: training loss 114490672399.059
Test Loss of 119852167762.076340, Test MSE of 119852167883.476166
Epoch 17: training loss 109798063856.941
Test Loss of 116372422800.018509, Test MSE of 116372423506.509796
Epoch 18: training loss 105716289716.706
Test Loss of 110898761963.925049, Test MSE of 110898760723.260483
Epoch 19: training loss 102305895303.529
Test Loss of 108606634444.006470, Test MSE of 108606636040.725143
Epoch 20: training loss 97640574584.471
Test Loss of 103241216897.273193, Test MSE of 103241217175.269409
Epoch 21: training loss 94877362718.118
Test Loss of 102537188122.944244, Test MSE of 102537188012.209366
Epoch 22: training loss 91162388992.000
Test Loss of 93604099013.492477, Test MSE of 93604099340.992752
Epoch 23: training loss 86301864583.529
Test Loss of 92280624499.653015, Test MSE of 92280625732.672989
Epoch 24: training loss 83256179471.059
Test Loss of 88177921397.074249, Test MSE of 88177922675.304413
Epoch 25: training loss 80976896451.765
Test Loss of 84507049773.657181, Test MSE of 84507049418.727982
Epoch 26: training loss 76514625008.941
Test Loss of 84072628919.457779, Test MSE of 84072630056.744110
Epoch 27: training loss 73092282503.529
Test Loss of 78590978262.606522, Test MSE of 78590978588.738541
Epoch 28: training loss 68739565703.529
Test Loss of 73906814644.378448, Test MSE of 73906815248.948563
Epoch 29: training loss 67325260634.353
Test Loss of 71342168209.439743, Test MSE of 71342168975.026642
Epoch 30: training loss 63180903619.765
Test Loss of 68216275165.712700, Test MSE of 68216275485.273048
Epoch 31: training loss 60577048380.235
Test Loss of 70055889338.004166, Test MSE of 70055888743.531982
Epoch 32: training loss 57708567792.941
Test Loss of 60968539162.766594, Test MSE of 60968539493.409485
Epoch 33: training loss 54634811934.118
Test Loss of 61608299110.447372, Test MSE of 61608299555.620026
Epoch 34: training loss 51861210902.588
Test Loss of 56288160189.794121, Test MSE of 56288159725.428986
Epoch 35: training loss 49096690484.706
Test Loss of 52692600813.523941, Test MSE of 52692600683.427353
Epoch 36: training loss 46541729106.824
Test Loss of 47493034154.548233, Test MSE of 47493034894.741837
Epoch 37: training loss 44826543744.000
Test Loss of 49455444205.820030, Test MSE of 49455443974.930954
Epoch 38: training loss 42109052092.235
Test Loss of 46528211171.871384, Test MSE of 46528211420.495064
Epoch 39: training loss 39772327371.294
Test Loss of 45960910524.432106, Test MSE of 45960911099.022537
Epoch 40: training loss 37779116009.412
Test Loss of 44649376745.023361, Test MSE of 44649377265.586891
Epoch 41: training loss 35969311683.765
Test Loss of 40994135019.865837, Test MSE of 40994135456.726402
Epoch 42: training loss 34216097264.941
Test Loss of 38119894413.709000, Test MSE of 38119894526.262321
Epoch 43: training loss 32644147568.941
Test Loss of 39819449384.978951, Test MSE of 39819449597.304504
Epoch 44: training loss 31454969374.118
Test Loss of 37945142986.170715, Test MSE of 37945143113.579536
Epoch 45: training loss 28792846245.647
Test Loss of 35440196173.812630, Test MSE of 35440197059.970490
Epoch 46: training loss 28345441362.824
Test Loss of 31273759278.071709, Test MSE of 31273758465.211018
Epoch 47: training loss 26674217061.647
Test Loss of 33387147556.774464, Test MSE of 33387147597.763748
Epoch 48: training loss 25463728218.353
Test Loss of 32621299823.330093, Test MSE of 32621300570.060619
Epoch 49: training loss 24609074066.824
Test Loss of 30881166459.410595, Test MSE of 30881165702.841373
Epoch 50: training loss 22995855092.706
Test Loss of 28258580275.815868, Test MSE of 28258580582.893345
Epoch 51: training loss 21935873445.647
Test Loss of 26757607039.318993, Test MSE of 26757606961.320217
Epoch 52: training loss 21223325007.059
Test Loss of 28433660305.262085, Test MSE of 28433660319.016445
Epoch 53: training loss 20758113392.941
Test Loss of 27679197167.418922, Test MSE of 27679197105.532833
Epoch 54: training loss 19550219858.824
Test Loss of 25719360214.724960, Test MSE of 25719360117.514393
Epoch 55: training loss 18988464481.882
Test Loss of 25116742944.273884, Test MSE of 25116742889.274044
Epoch 56: training loss 18558290819.765
Test Loss of 23785082035.786259, Test MSE of 23785082524.299294
Epoch 57: training loss 17787577792.000
Test Loss of 27805424443.632664, Test MSE of 27805424217.673138
Epoch 58: training loss 17201294219.294
Test Loss of 25127135190.784176, Test MSE of 25127135498.510635
Epoch 59: training loss 16660483659.294
Test Loss of 24479405003.651169, Test MSE of 24479405254.750404
Epoch 60: training loss 16300960783.059
Test Loss of 24509366253.997688, Test MSE of 24509366411.279362
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24646475208.68881, 'MSE - std': 1340000954.6989567, 'R2 - mean': 0.822998519125217, 'R2 - std': 0.010375711309591078} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005424 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109130752.000
Test Loss of 410763564370.806091, Test MSE of 410763557504.938782
Epoch 2: training loss 430087420265.412
Test Loss of 410745038866.006470, Test MSE of 410745040360.328430
Epoch 3: training loss 430059973933.176
Test Loss of 410722149758.400757, Test MSE of 410722153326.677063
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076742475.294
Test Loss of 410726049544.647827, Test MSE of 410726047385.939514
Epoch 2: training loss 430065373906.824
Test Loss of 410727329725.660339, Test MSE of 410727337503.451538
Epoch 3: training loss 430064926720.000
Test Loss of 410727269542.322998, Test MSE of 410727272813.877563
Epoch 4: training loss 430064575668.706
Test Loss of 410726927684.590454, Test MSE of 410726927868.186035
Epoch 5: training loss 422871532001.882
Test Loss of 388160087323.838989, Test MSE of 388160087306.289795
Epoch 6: training loss 376747897434.353
Test Loss of 321315282985.699219, Test MSE of 321315283449.878235
Epoch 7: training loss 297594661707.294
Test Loss of 236122482334.504395, Test MSE of 236122479045.548370
Epoch 8: training loss 219766289468.235
Test Loss of 166942651764.923645, Test MSE of 166942651243.993622
Epoch 9: training loss 161086889532.235
Test Loss of 119648349247.022675, Test MSE of 119648349387.396957
Epoch 10: training loss 141283883971.765
Test Loss of 111860885324.882919, Test MSE of 111860885730.135559
Epoch 11: training loss 138222217095.529
Test Loss of 109230590864.644150, Test MSE of 109230590238.344666
Epoch 12: training loss 133630508423.529
Test Loss of 106486316612.472000, Test MSE of 106486316066.436249
Epoch 13: training loss 130788414704.941
Test Loss of 103466871673.899124, Test MSE of 103466872219.525558
Epoch 14: training loss 126791212333.176
Test Loss of 99430491177.225357, Test MSE of 99430491999.259521
Epoch 15: training loss 122591945005.176
Test Loss of 96471681921.480789, Test MSE of 96471681568.767761
Epoch 16: training loss 119219061729.882
Test Loss of 94019081793.628876, Test MSE of 94019081726.188461
Epoch 17: training loss 114928988491.294
Test Loss of 90220996607.052292, Test MSE of 90220997318.653610
Epoch 18: training loss 109017749925.647
Test Loss of 86720963367.448410, Test MSE of 86720961874.415665
Epoch 19: training loss 105897827900.235
Test Loss of 84076832069.538177, Test MSE of 84076832491.330612
Epoch 20: training loss 102588108197.647
Test Loss of 79649326401.747345, Test MSE of 79649327192.623367
Epoch 21: training loss 99018084111.059
Test Loss of 77449794273.317902, Test MSE of 77449795827.162766
Epoch 22: training loss 93990033438.118
Test Loss of 73420168196.264694, Test MSE of 73420167971.375595
Epoch 23: training loss 90697274548.706
Test Loss of 69995689086.519211, Test MSE of 69995689735.764313
Epoch 24: training loss 86648540958.118
Test Loss of 67438502570.350761, Test MSE of 67438502130.189034
Epoch 25: training loss 81852214241.882
Test Loss of 65159134742.981956, Test MSE of 65159133963.310646
Epoch 26: training loss 79064809592.471
Test Loss of 63314379377.488197, Test MSE of 63314379187.213921
Epoch 27: training loss 75995288545.882
Test Loss of 58222377772.186951, Test MSE of 58222378201.600273
Epoch 28: training loss 72028225596.235
Test Loss of 57489934293.353081, Test MSE of 57489934645.851936
Epoch 29: training loss 68692814848.000
Test Loss of 54202512537.055069, Test MSE of 54202511709.375923
Epoch 30: training loss 64248371019.294
Test Loss of 50972612741.627022, Test MSE of 50972613450.152069
Epoch 31: training loss 61860655585.882
Test Loss of 50682244932.353539, Test MSE of 50682244823.924454
Epoch 32: training loss 58229040474.353
Test Loss of 44971245731.006012, Test MSE of 44971246244.998604
Epoch 33: training loss 56390880527.059
Test Loss of 45196926968.418327, Test MSE of 45196927455.137421
Epoch 34: training loss 53201769961.412
Test Loss of 42525257136.629341, Test MSE of 42525256210.387695
Epoch 35: training loss 50434192414.118
Test Loss of 39541689128.396111, Test MSE of 39541688650.440392
Epoch 36: training loss 47752306816.000
Test Loss of 39304585557.175385, Test MSE of 39304584738.197060
Epoch 37: training loss 44792802928.941
Test Loss of 38524491494.530312, Test MSE of 38524491395.035255
Epoch 38: training loss 42816051629.176
Test Loss of 34154167395.035633, Test MSE of 34154167914.613663
Epoch 39: training loss 40780870204.235
Test Loss of 36173093511.285515, Test MSE of 36173093522.188805
Epoch 40: training loss 38370484796.235
Test Loss of 32003531178.469227, Test MSE of 32003531001.455486
Epoch 41: training loss 36735709364.706
Test Loss of 31263417637.316059, Test MSE of 31263417739.731850
Epoch 42: training loss 35009766535.529
Test Loss of 29831427755.772327, Test MSE of 29831427396.540779
Epoch 43: training loss 33311985807.059
Test Loss of 29099345337.158722, Test MSE of 29099344933.070518
Epoch 44: training loss 30802576611.765
Test Loss of 28453462523.972237, Test MSE of 28453462343.850437
Epoch 45: training loss 29618866477.176
Test Loss of 26006710161.118000, Test MSE of 26006710386.157734
Epoch 46: training loss 28283838298.353
Test Loss of 24755993133.253124, Test MSE of 24755992833.937969
Epoch 47: training loss 27312079936.000
Test Loss of 23866091439.444702, Test MSE of 23866091765.728367
Epoch 48: training loss 25903081935.059
Test Loss of 25115676719.385471, Test MSE of 25115676484.524933
Epoch 49: training loss 24612580254.118
Test Loss of 24116338310.811661, Test MSE of 24116338125.639114
Epoch 50: training loss 23367248796.235
Test Loss of 20449482549.190189, Test MSE of 20449483061.645924
Epoch 51: training loss 22879942430.118
Test Loss of 23870467465.773254, Test MSE of 23870467057.038422
Epoch 52: training loss 22052140340.706
Test Loss of 21386002767.489124, Test MSE of 21386002638.548725
Epoch 53: training loss 20808102219.294
Test Loss of 20475332789.960205, Test MSE of 20475332814.631210
Epoch 54: training loss 19731250273.882
Test Loss of 20448532371.961128, Test MSE of 20448532033.552383
Epoch 55: training loss 19204391875.765
Test Loss of 20114504816.303562, Test MSE of 20114504788.154457
Epoch 56: training loss 18869203207.529
Test Loss of 22252200957.630726, Test MSE of 22252200597.491879
Epoch 57: training loss 18356453842.824
Test Loss of 18982461649.443775, Test MSE of 18982461797.054829
Epoch 58: training loss 17709714055.529
Test Loss of 19886233854.933826, Test MSE of 19886234082.130806
Epoch 59: training loss 17120524976.941
Test Loss of 20488904871.744564, Test MSE of 20488904744.269451
Epoch 60: training loss 16743319762.824
Test Loss of 19825518106.772789, Test MSE of 19825517738.576851
Epoch 61: training loss 16118214223.059
Test Loss of 18750236677.686256, Test MSE of 18750236795.150063
Epoch 62: training loss 15784265694.118
Test Loss of 17560991905.110596, Test MSE of 17560991992.411987
Epoch 63: training loss 14933523004.235
Test Loss of 19398340856.299862, Test MSE of 19398340759.700581
Epoch 64: training loss 15087398339.765
Test Loss of 20413960580.560852, Test MSE of 20413960988.535789
Epoch 65: training loss 14340378552.471
Test Loss of 18919153719.914856, Test MSE of 18919153820.414299
Epoch 66: training loss 13974595365.647
Test Loss of 20418671025.103191, Test MSE of 20418671395.328342
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23589524255.34869, 'MSE - std': 2167518828.5791955, 'R2 - mean': 0.8251174052344039, 'R2 - std': 0.009706213160450266} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005425 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043012577.882
Test Loss of 431611847774.297058, Test MSE of 431611860478.153259
Epoch 2: training loss 424022827248.941
Test Loss of 431591819611.809326, Test MSE of 431591826951.115540
Epoch 3: training loss 423995825091.765
Test Loss of 431565020664.655273, Test MSE of 431565023759.277283
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008687616.000
Test Loss of 431569127218.347046, Test MSE of 431569130955.890564
Epoch 2: training loss 423999144658.824
Test Loss of 431571069461.560364, Test MSE of 431571066916.723511
Epoch 3: training loss 423998598324.706
Test Loss of 431570211776.977295, Test MSE of 431570212553.962708
Epoch 4: training loss 423998147041.882
Test Loss of 431569287965.023621, Test MSE of 431569283423.169922
Epoch 5: training loss 417227920564.706
Test Loss of 409388226193.236450, Test MSE of 409388225118.011108
Epoch 6: training loss 371793265121.882
Test Loss of 342003319436.024048, Test MSE of 342003319739.210754
Epoch 7: training loss 293176296809.412
Test Loss of 255005586342.441467, Test MSE of 255005580355.186615
Epoch 8: training loss 215211365376.000
Test Loss of 183154370449.118011, Test MSE of 183154371276.384766
Epoch 9: training loss 157083245477.647
Test Loss of 132814771542.596939, Test MSE of 132814770888.903259
Epoch 10: training loss 138238730029.176
Test Loss of 124063468455.389175, Test MSE of 124063468596.641953
Epoch 11: training loss 134191642458.353
Test Loss of 120803133954.606201, Test MSE of 120803133838.887955
Epoch 12: training loss 131063522755.765
Test Loss of 117168293727.837112, Test MSE of 117168290539.005859
Epoch 13: training loss 127202826315.294
Test Loss of 112913465052.105499, Test MSE of 112913466623.201614
Epoch 14: training loss 124310672670.118
Test Loss of 110412108547.435440, Test MSE of 110412108852.780899
Epoch 15: training loss 119662246881.882
Test Loss of 106523836995.998154, Test MSE of 106523834665.255585
Epoch 16: training loss 116480799472.941
Test Loss of 101599993272.211014, Test MSE of 101599991234.917419
Epoch 17: training loss 112044961551.059
Test Loss of 99516845156.931046, Test MSE of 99516845534.923447
Epoch 18: training loss 108610791875.765
Test Loss of 95465527509.234619, Test MSE of 95465528683.290039
Epoch 19: training loss 105232997616.941
Test Loss of 91402451138.280426, Test MSE of 91402449121.347153
Epoch 20: training loss 101001812299.294
Test Loss of 88301790923.994446, Test MSE of 88301790220.187439
Epoch 21: training loss 96763324626.824
Test Loss of 83002062833.784363, Test MSE of 83002063101.204437
Epoch 22: training loss 92632664425.412
Test Loss of 82779353859.435440, Test MSE of 82779353227.805420
Epoch 23: training loss 89055847363.765
Test Loss of 77464297015.204071, Test MSE of 77464295989.195389
Epoch 24: training loss 85056025780.706
Test Loss of 72385480864.162888, Test MSE of 72385482033.724655
Epoch 25: training loss 81252630046.118
Test Loss of 68312137460.745949, Test MSE of 68312137342.991844
Epoch 26: training loss 77628107038.118
Test Loss of 65408815230.993057, Test MSE of 65408816400.159691
Epoch 27: training loss 74142866492.235
Test Loss of 67109414459.942619, Test MSE of 67109414445.649956
Epoch 28: training loss 71841048365.176
Test Loss of 60140789619.739006, Test MSE of 60140789912.921890
Epoch 29: training loss 67793806923.294
Test Loss of 59874541928.129570, Test MSE of 59874541765.833687
Epoch 30: training loss 64336564043.294
Test Loss of 56191619284.286903, Test MSE of 56191619436.584213
Epoch 31: training loss 61923170432.000
Test Loss of 51652181906.065712, Test MSE of 51652181996.396294
Epoch 32: training loss 58604979079.529
Test Loss of 51334576864.370201, Test MSE of 51334576182.141747
Epoch 33: training loss 55592401543.529
Test Loss of 51734921619.724205, Test MSE of 51734924238.683754
Epoch 34: training loss 52613660995.765
Test Loss of 47466463703.959282, Test MSE of 47466463691.790802
Epoch 35: training loss 49826132389.647
Test Loss of 45680445397.353081, Test MSE of 45680445102.529724
Epoch 36: training loss 47809762070.588
Test Loss of 41218183672.181396, Test MSE of 41218184326.231865
Epoch 37: training loss 44880737987.765
Test Loss of 40339380605.926888, Test MSE of 40339381246.569206
Epoch 38: training loss 42829042123.294
Test Loss of 36103158152.351692, Test MSE of 36103159545.375504
Epoch 39: training loss 40859677056.000
Test Loss of 34115676468.005554, Test MSE of 34115676428.431854
Epoch 40: training loss 38878573884.235
Test Loss of 30303632304.866264, Test MSE of 30303632120.394249
Epoch 41: training loss 36980065528.471
Test Loss of 34066024402.509949, Test MSE of 34066023839.485653
Epoch 42: training loss 35211347704.471
Test Loss of 30965589729.791763, Test MSE of 30965589664.540333
Epoch 43: training loss 33575173353.412
Test Loss of 29389701174.019436, Test MSE of 29389701504.969883
Epoch 44: training loss 31560249999.059
Test Loss of 28790650170.165665, Test MSE of 28790649708.939274
Epoch 45: training loss 30048668416.000
Test Loss of 29011661009.443775, Test MSE of 29011661805.739586
Epoch 46: training loss 28836848768.000
Test Loss of 29380873225.950947, Test MSE of 29380872844.989479
Epoch 47: training loss 27499974580.706
Test Loss of 28657620854.582138, Test MSE of 28657621202.830982
Epoch 48: training loss 26471367220.706
Test Loss of 28118020020.183247, Test MSE of 28118019537.021839
Epoch 49: training loss 25005454686.118
Test Loss of 26036691081.417862, Test MSE of 26036691311.576172
Epoch 50: training loss 23989398475.294
Test Loss of 25549708138.735771, Test MSE of 25549707563.235100
Epoch 51: training loss 23040502535.529
Test Loss of 23460742583.263306, Test MSE of 23460743117.283947
Epoch 52: training loss 22105379813.647
Test Loss of 22711336451.080055, Test MSE of 22711336322.859615
Epoch 53: training loss 21422922450.824
Test Loss of 23509518462.045349, Test MSE of 23509518450.467278
Epoch 54: training loss 20805734972.235
Test Loss of 22396651023.400276, Test MSE of 22396650823.519493
Epoch 55: training loss 20073263578.353
Test Loss of 22896598123.565018, Test MSE of 22896598195.523571
Epoch 56: training loss 19144327160.471
Test Loss of 23349757879.026375, Test MSE of 23349758573.431644
Epoch 57: training loss 18682854802.824
Test Loss of 20839044389.316059, Test MSE of 20839044092.946529
Epoch 58: training loss 18129765842.824
Test Loss of 20908595671.011570, Test MSE of 20908595528.317123
Epoch 59: training loss 17712996378.353
Test Loss of 22420362203.513187, Test MSE of 22420362701.620148
Epoch 60: training loss 17259287830.588
Test Loss of 20361312049.399353, Test MSE of 20361312241.200176
Epoch 61: training loss 16431492498.824
Test Loss of 20653727652.072189, Test MSE of 20653727369.656372
Epoch 62: training loss 16235964084.706
Test Loss of 20491782087.137436, Test MSE of 20491781697.296093
Epoch 63: training loss 16086516359.529
Test Loss of 20286337589.782509, Test MSE of 20286338172.068043
Epoch 64: training loss 15340229767.529
Test Loss of 21569087825.384544, Test MSE of 21569087587.171009
Epoch 65: training loss 15124375431.529
Test Loss of 19906686483.191116, Test MSE of 19906686599.468277
Epoch 66: training loss 14745177720.471
Test Loss of 20210793734.989357, Test MSE of 20210793746.270275
Epoch 67: training loss 14417010718.118
Test Loss of 19382970802.998611, Test MSE of 19382970955.389988
Epoch 68: training loss 14003115166.118
Test Loss of 20456299522.369274, Test MSE of 20456299485.878918
Epoch 69: training loss 13616413632.000
Test Loss of 19485638696.751503, Test MSE of 19485639034.467266
Epoch 70: training loss 13169765722.353
Test Loss of 19696858093.519669, Test MSE of 19696857848.150730
Epoch 71: training loss 12802687040.000
Test Loss of 20130251920.525684, Test MSE of 20130252271.599247
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22897669858.5988, 'MSE - std': 2381839692.9694643, 'R2 - mean': 0.8300272299185298, 'R2 - std': 0.013107019983515865} 
 

Saving model.....
Results After CV: {'MSE - mean': 22897669858.5988, 'MSE - std': 2381839692.9694643, 'R2 - mean': 0.8300272299185298, 'R2 - std': 0.013107019983515865}
Train time: 96.30203492460132
Inference time: 0.07477605439926265
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 69 finished with value: 22897669858.5988 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005515 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525877639.529
Test Loss of 418111912278.517700, Test MSE of 418111913766.213623
Epoch 2: training loss 427506274424.471
Test Loss of 418094981576.690247, Test MSE of 418094984371.388184
Epoch 3: training loss 427479792459.294
Test Loss of 418071739354.811035, Test MSE of 418071733638.261169
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495942023.529
Test Loss of 418076344270.730530, Test MSE of 418076346411.919983
Epoch 2: training loss 427484008448.000
Test Loss of 418077807187.971313, Test MSE of 418077808032.680603
Epoch 3: training loss 427483548732.235
Test Loss of 418077548856.671753, Test MSE of 418077547215.832520
Epoch 4: training loss 427483227678.118
Test Loss of 418077211330.353943, Test MSE of 418077216134.753540
Epoch 5: training loss 427483030949.647
Test Loss of 418076107436.087891, Test MSE of 418076109027.842957
Epoch 6: training loss 427482871326.118
Test Loss of 418076341390.360413, Test MSE of 418076340876.385559
Epoch 7: training loss 417823772551.529
Test Loss of 387532249709.079834, Test MSE of 387532252702.740540
Epoch 8: training loss 356925239777.882
Test Loss of 301511961004.739319, Test MSE of 301511959360.483337
Epoch 9: training loss 263572752805.647
Test Loss of 208351543837.964386, Test MSE of 208351542608.187775
Epoch 10: training loss 187084216711.529
Test Loss of 148801104399.278290, Test MSE of 148801102420.792664
Epoch 11: training loss 150955662576.941
Test Loss of 125992190478.804535, Test MSE of 125992192132.975266
Epoch 12: training loss 138977405515.294
Test Loss of 119467778625.969009, Test MSE of 119467777398.474014
Epoch 13: training loss 135420834665.412
Test Loss of 115411533278.245667, Test MSE of 115411531796.160019
Epoch 14: training loss 132984179440.941
Test Loss of 111593097832.342361, Test MSE of 111593096726.342484
Epoch 15: training loss 129138730827.294
Test Loss of 108927932752.832748, Test MSE of 108927932976.749161
Epoch 16: training loss 125190965428.706
Test Loss of 106261112500.852188, Test MSE of 106261112854.730057
Epoch 17: training loss 123181138492.235
Test Loss of 103274817955.264404, Test MSE of 103274816552.498993
Epoch 18: training loss 119086259215.059
Test Loss of 99228529038.893356, Test MSE of 99228529555.356613
Epoch 19: training loss 115316886317.176
Test Loss of 96444927601.106644, Test MSE of 96444925514.861740
Epoch 20: training loss 110865620630.588
Test Loss of 92974060205.035385, Test MSE of 92974061042.867905
Epoch 21: training loss 108158341526.588
Test Loss of 90136998986.141113, Test MSE of 90137000983.728973
Epoch 22: training loss 103188033385.412
Test Loss of 86761918021.522095, Test MSE of 86761919392.317200
Epoch 23: training loss 100285663653.647
Test Loss of 83349490068.104553, Test MSE of 83349490337.546021
Epoch 24: training loss 96205581891.765
Test Loss of 82255364283.603058, Test MSE of 82255364730.539261
Epoch 25: training loss 92915325906.824
Test Loss of 78767101739.288452, Test MSE of 78767100173.367493
Epoch 26: training loss 89760266450.824
Test Loss of 73868000880.159149, Test MSE of 73868000553.658844
Epoch 27: training loss 85836015179.294
Test Loss of 72354815105.569275, Test MSE of 72354814484.438339
Epoch 28: training loss 82978987459.765
Test Loss of 71599676731.277359, Test MSE of 71599675959.220123
Epoch 29: training loss 79697680007.529
Test Loss of 68446073257.423088, Test MSE of 68446073429.862946
Epoch 30: training loss 75812590765.176
Test Loss of 64929427682.450150, Test MSE of 64929427590.852707
Epoch 31: training loss 74765233046.588
Test Loss of 61828843099.788109, Test MSE of 61828842853.055275
Epoch 32: training loss 69905449788.235
Test Loss of 59586854068.733749, Test MSE of 59586854792.899925
Epoch 33: training loss 68697781142.588
Test Loss of 56559217419.310661, Test MSE of 56559218286.318573
Epoch 34: training loss 65017504824.471
Test Loss of 55005293959.076569, Test MSE of 55005294233.055809
Epoch 35: training loss 62623168993.882
Test Loss of 50178998287.159843, Test MSE of 50178998081.066803
Epoch 36: training loss 60247271664.941
Test Loss of 47564533944.760582, Test MSE of 47564533756.157745
Epoch 37: training loss 57399699523.765
Test Loss of 51041196835.945412, Test MSE of 51041196766.790787
Epoch 38: training loss 54781075809.882
Test Loss of 44631625341.424011, Test MSE of 44631625690.917564
Epoch 39: training loss 51902691169.882
Test Loss of 43707258700.450615, Test MSE of 43707259215.718483
Epoch 40: training loss 50471163083.294
Test Loss of 42897399408.159149, Test MSE of 42897399555.949387
Epoch 41: training loss 48432106036.706
Test Loss of 43214391670.732361, Test MSE of 43214391106.526749
Epoch 42: training loss 46547719589.647
Test Loss of 40266435814.950729, Test MSE of 40266436059.622154
Epoch 43: training loss 44119561140.706
Test Loss of 35912665706.237335, Test MSE of 35912666159.180847
Epoch 44: training loss 42238345681.882
Test Loss of 36399417751.894516, Test MSE of 36399417659.674110
Epoch 45: training loss 40103315275.294
Test Loss of 33347686076.905853, Test MSE of 33347686603.330730
Epoch 46: training loss 38700360357.647
Test Loss of 29320344316.624565, Test MSE of 29320344993.186974
Epoch 47: training loss 37100329050.353
Test Loss of 29859378348.680084, Test MSE of 29859378940.422131
Epoch 48: training loss 34865857622.588
Test Loss of 30858174951.009945, Test MSE of 30858174809.587814
Epoch 49: training loss 33268015220.706
Test Loss of 28522424987.506824, Test MSE of 28522425003.984798
Epoch 50: training loss 31427190644.706
Test Loss of 27590327450.204025, Test MSE of 27590327896.937580
Epoch 51: training loss 30843043614.118
Test Loss of 26287769663.955585, Test MSE of 26287769724.640881
Epoch 52: training loss 28839622072.471
Test Loss of 25187384214.117973, Test MSE of 25187384103.349880
Epoch 53: training loss 27947697264.941
Test Loss of 22523993110.029148, Test MSE of 22523993424.359261
Epoch 54: training loss 27231394217.412
Test Loss of 24833774415.056210, Test MSE of 24833774438.892593
Epoch 55: training loss 25661239465.412
Test Loss of 22788796154.966457, Test MSE of 22788796183.053391
Epoch 56: training loss 25156712485.647
Test Loss of 22809933338.411289, Test MSE of 22809933682.953419
Epoch 57: training loss 24015796468.706
Test Loss of 21430598573.805229, Test MSE of 21430598339.805626
Epoch 58: training loss 23007053748.706
Test Loss of 22806897487.056210, Test MSE of 22806897755.552162
Epoch 59: training loss 22247220114.824
Test Loss of 23544606545.661808, Test MSE of 23544606685.683746
Epoch 60: training loss 21249400075.294
Test Loss of 19177385171.527180, Test MSE of 19177385042.198517
Epoch 61: training loss 20448098770.824
Test Loss of 20790103064.397873, Test MSE of 20790103037.129570
Epoch 62: training loss 19825827384.471
Test Loss of 17587675150.686096, Test MSE of 17587675262.271309
Epoch 63: training loss 19001320365.176
Test Loss of 18517132776.668056, Test MSE of 18517132830.682693
Epoch 64: training loss 18558459316.706
Test Loss of 19158675671.554012, Test MSE of 19158676069.810696
Epoch 65: training loss 18133979309.176
Test Loss of 20508301876.704140, Test MSE of 20508301559.290012
Epoch 66: training loss 17330705084.235
Test Loss of 19216496901.981033, Test MSE of 19216497047.106853
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19216497047.106853, 'MSE - std': 0.0, 'R2 - mean': 0.850359052520109, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005560 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917446927.059
Test Loss of 424556480340.030518, Test MSE of 424556472910.873352
Epoch 2: training loss 427896522631.529
Test Loss of 424540439892.385864, Test MSE of 424540439963.506836
Epoch 3: training loss 427867814249.412
Test Loss of 424518340836.345154, Test MSE of 424518345063.247253
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886013018.353
Test Loss of 424525882956.628296, Test MSE of 424525887716.003113
Epoch 2: training loss 427877859568.941
Test Loss of 424526547367.054382, Test MSE of 424526546610.621704
Epoch 3: training loss 427877306608.941
Test Loss of 424526642532.966919, Test MSE of 424526643183.572388
Epoch 4: training loss 427876904719.059
Test Loss of 424526729565.623901, Test MSE of 424526740786.278503
Epoch 5: training loss 427876615830.588
Test Loss of 424526504623.877869, Test MSE of 424526507605.286133
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221871502326.1965, 'MSE - std': 202655005279.08966, 'R2 - mean': -0.5902391782941528, 'R2 - std': 1.440598230814262} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926980909.176
Test Loss of 447259089877.126099, Test MSE of 447259082566.161255
Epoch 2: training loss 421906887981.176
Test Loss of 447241930436.012024, Test MSE of 447241927963.026489
Epoch 3: training loss 421881057882.353
Test Loss of 447219201071.611389, Test MSE of 447219211168.327148
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901443312.941
Test Loss of 447228593503.755737, Test MSE of 447228595530.745422
Epoch 2: training loss 421891520150.588
Test Loss of 447229080877.538757, Test MSE of 447229081138.399475
Epoch 3: training loss 421891086817.882
Test Loss of 447229130556.343262, Test MSE of 447229130966.474915
Epoch 4: training loss 421890744681.412
Test Loss of 447228810016.629211, Test MSE of 447228809142.797852
Epoch 5: training loss 421890490970.353
Test Loss of 447228261200.003723, Test MSE of 447228264863.213623
Epoch 6: training loss 421890334840.471
Test Loss of 447228175016.297913, Test MSE of 447228167474.292114
Epoch 7: training loss 412357844028.235
Test Loss of 416501987086.863770, Test MSE of 416501975858.667297
Epoch 8: training loss 351616668009.412
Test Loss of 330108768685.213074, Test MSE of 330108764774.295532
Epoch 9: training loss 258181098616.471
Test Loss of 234040371374.811951, Test MSE of 234040376027.439972
Epoch 10: training loss 182444962273.882
Test Loss of 173614560519.639130, Test MSE of 173614561263.394623
Epoch 11: training loss 146265139471.059
Test Loss of 148973732063.607666, Test MSE of 148973731467.367462
Epoch 12: training loss 134442653093.647
Test Loss of 141143221006.626892, Test MSE of 141143220844.155609
Epoch 13: training loss 130410683783.529
Test Loss of 136117654505.497116, Test MSE of 136117654971.156250
Epoch 14: training loss 127158867004.235
Test Loss of 133004885121.569275, Test MSE of 133004886233.929733
Epoch 15: training loss 123442785852.235
Test Loss of 129429649837.686798, Test MSE of 129429650938.389343
Epoch 16: training loss 119896634036.706
Test Loss of 126431259958.539902, Test MSE of 126431260337.146423
Epoch 17: training loss 117758919890.824
Test Loss of 122266364651.095993, Test MSE of 122266365272.924118
Epoch 18: training loss 113566223028.706
Test Loss of 118517361197.597961, Test MSE of 118517360236.527115
Epoch 19: training loss 110082354447.059
Test Loss of 113855949740.383987, Test MSE of 113855952229.436310
Epoch 20: training loss 105481033517.176
Test Loss of 112151658230.702759, Test MSE of 112151659280.042252
Epoch 21: training loss 102110668107.294
Test Loss of 110613239947.754807, Test MSE of 110613241371.203857
Epoch 22: training loss 98629788220.235
Test Loss of 105741098232.953049, Test MSE of 105741097868.451630
Epoch 23: training loss 94935576967.529
Test Loss of 100144861482.459396, Test MSE of 100144861263.913101
Epoch 24: training loss 91299631194.353
Test Loss of 99234026244.204483, Test MSE of 99234025066.214417
Epoch 25: training loss 87291149131.294
Test Loss of 91876022874.130005, Test MSE of 91876023441.471283
Epoch 26: training loss 84588579749.647
Test Loss of 88575631420.639374, Test MSE of 88575630640.485870
Epoch 27: training loss 80501759623.529
Test Loss of 88990642597.633118, Test MSE of 88990645110.004837
Epoch 28: training loss 77398075045.647
Test Loss of 81220984703.141342, Test MSE of 81220984673.540833
Epoch 29: training loss 74614857923.765
Test Loss of 80446887003.195923, Test MSE of 80446886904.284683
Epoch 30: training loss 71318123459.765
Test Loss of 75737582197.607224, Test MSE of 75737581814.226685
Epoch 31: training loss 68337502554.353
Test Loss of 71757012046.404816, Test MSE of 71757013502.904800
Epoch 32: training loss 65567738322.824
Test Loss of 70021193644.620865, Test MSE of 70021193564.614395
Epoch 33: training loss 62416492574.118
Test Loss of 67833179901.335182, Test MSE of 67833179097.266235
Epoch 34: training loss 60229311638.588
Test Loss of 60261515737.745087, Test MSE of 60261516020.913811
Epoch 35: training loss 57644494320.941
Test Loss of 60967129517.213043, Test MSE of 60967130131.900604
Epoch 36: training loss 54724683489.882
Test Loss of 58965370478.737915, Test MSE of 58965371501.865036
Epoch 37: training loss 52304605357.176
Test Loss of 54232482267.640068, Test MSE of 54232481809.144775
Epoch 38: training loss 50314254057.412
Test Loss of 52420628583.276428, Test MSE of 52420628412.020134
Epoch 39: training loss 47669680783.059
Test Loss of 52827605859.427246, Test MSE of 52827605632.208061
Epoch 40: training loss 46217889280.000
Test Loss of 48751421423.418922, Test MSE of 48751422476.763893
Epoch 41: training loss 43876424854.588
Test Loss of 50900540031.082115, Test MSE of 50900540591.606743
Epoch 42: training loss 41460183047.529
Test Loss of 44455335781.085358, Test MSE of 44455336048.122238
Epoch 43: training loss 39597625246.118
Test Loss of 41902795655.431877, Test MSE of 41902795947.830414
Epoch 44: training loss 37482048730.353
Test Loss of 40659134095.900070, Test MSE of 40659134213.223251
Epoch 45: training loss 35743884905.412
Test Loss of 38790604253.771919, Test MSE of 38790604451.876587
Epoch 46: training loss 34309022908.235
Test Loss of 36377857115.669670, Test MSE of 36377856789.735474
Epoch 47: training loss 33076913965.176
Test Loss of 37233982092.346985, Test MSE of 37233982156.118538
Epoch 48: training loss 30827865163.294
Test Loss of 33322106184.542217, Test MSE of 33322106376.767914
Epoch 49: training loss 29802557056.000
Test Loss of 32254663018.414989, Test MSE of 32254663084.600437
Epoch 50: training loss 28068085824.000
Test Loss of 31899599407.966690, Test MSE of 31899599199.452423
Epoch 51: training loss 27477172668.235
Test Loss of 30153122242.531574, Test MSE of 30153122252.100746
Epoch 52: training loss 26080080451.765
Test Loss of 31519735275.510525, Test MSE of 31519735193.086105
Epoch 53: training loss 25133385611.294
Test Loss of 28789611067.810318, Test MSE of 28789611084.572865
Epoch 54: training loss 24536131576.471
Test Loss of 27325383385.093685, Test MSE of 27325383856.842327
Epoch 55: training loss 22934733022.118
Test Loss of 28955220502.384457, Test MSE of 28955220646.785393
Epoch 56: training loss 22248656651.294
Test Loss of 27037440265.770992, Test MSE of 27037440119.646523
Epoch 57: training loss 21365550953.412
Test Loss of 26716261568.577377, Test MSE of 26716261471.747238
Epoch 58: training loss 20378305524.706
Test Loss of 25203814253.612770, Test MSE of 25203814226.752178
Epoch 59: training loss 20099042872.471
Test Loss of 25101233159.343048, Test MSE of 25101232655.857456
Epoch 60: training loss 18973971772.235
Test Loss of 27991463430.040249, Test MSE of 27991462677.426735
Epoch 61: training loss 18649745276.235
Test Loss of 23896885252.737450, Test MSE of 23896885480.784790
Epoch 62: training loss 17911475715.765
Test Loss of 25021393164.613464, Test MSE of 25021393214.094204
Epoch 63: training loss 17253337807.059
Test Loss of 25844023011.752949, Test MSE of 25844023542.856754
Epoch 64: training loss 17043368105.412
Test Loss of 23520477060.352531, Test MSE of 23520476886.772408
Epoch 65: training loss 16301408666.353
Test Loss of 23481229060.915104, Test MSE of 23481229071.882191
Epoch 66: training loss 15586424466.824
Test Loss of 24570841103.633587, Test MSE of 24570841022.603718
Epoch 67: training loss 15537522439.529
Test Loss of 21077872960.251678, Test MSE of 21077873498.665379
Epoch 68: training loss 14828525372.235
Test Loss of 22475703499.947258, Test MSE of 22475703635.576710
Epoch 69: training loss 14380082119.529
Test Loss of 22863774277.285217, Test MSE of 22863774258.683472
Epoch 70: training loss 14155295092.706
Test Loss of 21320705894.980339, Test MSE of 21320705859.721252
Epoch 71: training loss 14114088026.353
Test Loss of 22672255245.560951, Test MSE of 22672255588.703621
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 155471753413.69888, 'MSE - std': 190255672068.92245, 'R2 - mean': -0.1104686886018641, 'R2 - std': 1.357905846260807} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005639 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110478817.882
Test Loss of 410764518236.520142, Test MSE of 410764513490.947937
Epoch 2: training loss 430089907501.176
Test Loss of 410746473409.925049, Test MSE of 410746476810.198181
Epoch 3: training loss 430062403584.000
Test Loss of 410722967775.659424, Test MSE of 410722969462.832886
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077608899.765
Test Loss of 410728511466.202698, Test MSE of 410728515643.334961
Epoch 2: training loss 430066192624.941
Test Loss of 410727899852.468323, Test MSE of 410727895786.138977
Epoch 3: training loss 430065732909.176
Test Loss of 410727471442.332275, Test MSE of 410727473757.469971
Epoch 4: training loss 430065374147.765
Test Loss of 410727401888.044434, Test MSE of 410727400991.660156
Epoch 5: training loss 430065128146.824
Test Loss of 410726992553.403076, Test MSE of 410726988313.453735
Epoch 6: training loss 430064961897.412
Test Loss of 410726759841.466003, Test MSE of 410726759932.865784
Epoch 7: training loss 420746005805.176
Test Loss of 380938151969.169800, Test MSE of 380938151506.460754
Epoch 8: training loss 360129044961.882
Test Loss of 295974427963.113403, Test MSE of 295974427689.981628
Epoch 9: training loss 266547187712.000
Test Loss of 202902061730.769073, Test MSE of 202902063378.686371
Epoch 10: training loss 190134807672.471
Test Loss of 142906259062.700592, Test MSE of 142906260908.497498
Epoch 11: training loss 153115502983.529
Test Loss of 120009397336.610825, Test MSE of 120009397409.068375
Epoch 12: training loss 143218053541.647
Test Loss of 113795711082.143448, Test MSE of 113795708625.906647
Epoch 13: training loss 138213251553.882
Test Loss of 109958228509.142059, Test MSE of 109958228248.164627
Epoch 14: training loss 134924041216.000
Test Loss of 107098105282.635818, Test MSE of 107098106619.779633
Epoch 15: training loss 131719326147.765
Test Loss of 104272834171.439148, Test MSE of 104272834757.087540
Epoch 16: training loss 127189683772.235
Test Loss of 100861265016.832947, Test MSE of 100861263108.409821
Epoch 17: training loss 124303199081.412
Test Loss of 98587629906.332260, Test MSE of 98587627293.164215
Epoch 18: training loss 121475762718.118
Test Loss of 96893868025.366028, Test MSE of 96893866175.112137
Epoch 19: training loss 116825410499.765
Test Loss of 92682152628.775574, Test MSE of 92682154276.239014
Epoch 20: training loss 113352782848.000
Test Loss of 89594111218.139755, Test MSE of 89594110539.636856
Epoch 21: training loss 110322537110.588
Test Loss of 86428927419.054138, Test MSE of 86428928349.045670
Epoch 22: training loss 105678814147.765
Test Loss of 84266289837.193893, Test MSE of 84266289267.213531
Epoch 23: training loss 101972647574.588
Test Loss of 79914440761.810272, Test MSE of 79914440098.706680
Epoch 24: training loss 98068637364.706
Test Loss of 77619785498.654327, Test MSE of 77619784272.831238
Epoch 25: training loss 94613526377.412
Test Loss of 74904211571.620544, Test MSE of 74904213250.912659
Epoch 26: training loss 91294124062.118
Test Loss of 71269098965.590012, Test MSE of 71269098930.505722
Epoch 27: training loss 88975841536.000
Test Loss of 69831659313.399353, Test MSE of 69831659537.227646
Epoch 28: training loss 85510930853.647
Test Loss of 65552559367.937065, Test MSE of 65552560645.520653
Epoch 29: training loss 81831832289.882
Test Loss of 64371319443.605736, Test MSE of 64371318246.260025
Epoch 30: training loss 79055574166.588
Test Loss of 62938471614.015732, Test MSE of 62938472365.513985
Epoch 31: training loss 75194836389.647
Test Loss of 59901821379.109673, Test MSE of 59901822295.877586
Epoch 32: training loss 73184193761.882
Test Loss of 56592961977.632576, Test MSE of 56592962977.056458
Epoch 33: training loss 69735767898.353
Test Loss of 55385849651.294769, Test MSE of 55385849293.994812
Epoch 34: training loss 66563014098.824
Test Loss of 51656097100.172142, Test MSE of 51656097071.168327
Epoch 35: training loss 63840275109.647
Test Loss of 49638862377.936142, Test MSE of 49638863082.323578
Epoch 36: training loss 61508701093.647
Test Loss of 48369218798.348915, Test MSE of 48369218774.998070
Epoch 37: training loss 58458188453.647
Test Loss of 48356431232.296158, Test MSE of 48356430964.638466
Epoch 38: training loss 56496203888.941
Test Loss of 45776870881.910225, Test MSE of 45776870708.733482
Epoch 39: training loss 53696506337.882
Test Loss of 43817460993.303101, Test MSE of 43817460631.474602
Epoch 40: training loss 52044926441.412
Test Loss of 41976231125.708466, Test MSE of 41976231500.148865
Epoch 41: training loss 49890002936.471
Test Loss of 40427350011.261452, Test MSE of 40427349870.575188
Epoch 42: training loss 47290576105.412
Test Loss of 38982580878.867188, Test MSE of 38982581319.478516
Epoch 43: training loss 44921679284.706
Test Loss of 35458195266.458122, Test MSE of 35458195976.600662
Epoch 44: training loss 43060736790.588
Test Loss of 36657432421.523369, Test MSE of 36657431994.206390
Epoch 45: training loss 41385181854.118
Test Loss of 32236168923.157799, Test MSE of 32236169150.155846
Epoch 46: training loss 39580828928.000
Test Loss of 33239334521.543728, Test MSE of 33239334330.643299
Epoch 47: training loss 37819439397.647
Test Loss of 28816242143.067097, Test MSE of 28816242113.592384
Epoch 48: training loss 35951732773.647
Test Loss of 29916637234.228600, Test MSE of 29916637852.257854
Epoch 49: training loss 34026667504.941
Test Loss of 27465176604.668209, Test MSE of 27465176692.186569
Epoch 50: training loss 32900439732.706
Test Loss of 26222506148.901436, Test MSE of 26222506265.676121
Epoch 51: training loss 31658803139.765
Test Loss of 26135900815.814899, Test MSE of 26135900851.679916
Epoch 52: training loss 29930383134.118
Test Loss of 24088096657.118000, Test MSE of 24088096993.339043
Epoch 53: training loss 28942357142.588
Test Loss of 24292716455.863026, Test MSE of 24292716501.488350
Epoch 54: training loss 27890863548.235
Test Loss of 24065750610.687645, Test MSE of 24065750903.449409
Epoch 55: training loss 27039428649.412
Test Loss of 24480018094.615456, Test MSE of 24480018122.342957
Epoch 56: training loss 25340652156.235
Test Loss of 20992763561.876907, Test MSE of 20992763645.600960
Epoch 57: training loss 24153086667.294
Test Loss of 22153130245.567791, Test MSE of 22153130095.475620
Epoch 58: training loss 23496226272.000
Test Loss of 20785188715.209625, Test MSE of 20785188869.786366
Epoch 59: training loss 22698421530.353
Test Loss of 19036183455.807495, Test MSE of 19036183681.910099
Epoch 60: training loss 21939745558.588
Test Loss of 22268143938.221195, Test MSE of 22268143853.422569
Epoch 61: training loss 21442219026.824
Test Loss of 19687208742.974548, Test MSE of 19687208687.441395
Epoch 62: training loss 20794399585.882
Test Loss of 19896955764.212864, Test MSE of 19896955952.078369
Epoch 63: training loss 19592345362.824
Test Loss of 20238753898.143452, Test MSE of 20238754038.917858
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 121663503570.00362, 'MSE - std': 174862542696.48254, 'R2 - mean': 0.12538823736617816, 'R2 - std': 1.2449163361701228} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 6, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005358 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042204943.059
Test Loss of 431612582286.985657, Test MSE of 431612574213.708557
Epoch 2: training loss 424021827704.471
Test Loss of 431592806761.077271, Test MSE of 431592804609.155823
Epoch 3: training loss 423993925270.588
Test Loss of 431565784563.916687, Test MSE of 431565784158.932373
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012273061.647
Test Loss of 431568570116.856995, Test MSE of 431568569522.083496
Epoch 2: training loss 424000159382.588
Test Loss of 431570169592.536804, Test MSE of 431570166134.927673
Epoch 3: training loss 423999541248.000
Test Loss of 431570208274.243408, Test MSE of 431570210250.335632
Epoch 4: training loss 423999083218.824
Test Loss of 431569956501.975037, Test MSE of 431569960177.151489
Epoch 5: training loss 423998823002.353
Test Loss of 431570063813.005066, Test MSE of 431570069081.155701
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 183644816672.23404, 'MSE - std': 199570187568.5099, 'R2 - mean': -0.344285669887591, 'R2 - std': 1.4567867599174158} 
 

Saving model.....
Results After CV: {'MSE - mean': 183644816672.23404, 'MSE - std': 199570187568.5099, 'R2 - mean': -0.344285669887591, 'R2 - std': 1.4567867599174158}
Train time: 67.14236554320087
Inference time: 0.0754276646002836
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 70 finished with value: 183644816672.23404 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 6, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005498 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524845327.059
Test Loss of 418110684308.519104, Test MSE of 418110689788.899597
Epoch 2: training loss 427503252781.176
Test Loss of 418092381383.209778, Test MSE of 418092383274.480530
Epoch 3: training loss 427474948216.471
Test Loss of 418067858503.298645, Test MSE of 418067862146.906433
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491694230.588
Test Loss of 418075408218.662964, Test MSE of 418075410750.070557
Epoch 2: training loss 427480257716.706
Test Loss of 418075732525.597961, Test MSE of 418075725645.157715
Epoch 3: training loss 427479824624.941
Test Loss of 418075240679.661316, Test MSE of 418075240915.675110
Epoch 4: training loss 422233934185.412
Test Loss of 401567119361.421265, Test MSE of 401567111810.082031
Epoch 5: training loss 386312276088.471
Test Loss of 347251727176.897522, Test MSE of 347251723722.091736
Epoch 6: training loss 318275737961.412
Test Loss of 269470966648.745789, Test MSE of 269470969982.539093
Epoch 7: training loss 226515650379.294
Test Loss of 171389422764.206329, Test MSE of 171389421475.620880
Epoch 8: training loss 160494808816.941
Test Loss of 129899167296.784637, Test MSE of 129899168481.467636
Epoch 9: training loss 141343762883.765
Test Loss of 119527476555.384689, Test MSE of 119527475967.508011
Epoch 10: training loss 136771847439.059
Test Loss of 115403537260.902145, Test MSE of 115403539107.402267
Epoch 11: training loss 133071257750.588
Test Loss of 112398091579.751099, Test MSE of 112398090150.743546
Epoch 12: training loss 129340210951.529
Test Loss of 108797910530.250290, Test MSE of 108797908375.690933
Epoch 13: training loss 125206448941.176
Test Loss of 105693563001.752487, Test MSE of 105693563318.080002
Epoch 14: training loss 121791542211.765
Test Loss of 102350539451.721497, Test MSE of 102350540012.485077
Epoch 15: training loss 118059934464.000
Test Loss of 100016739942.210495, Test MSE of 100016741750.483963
Epoch 16: training loss 113976725699.765
Test Loss of 96192620679.017349, Test MSE of 96192622074.038086
Epoch 17: training loss 109679329295.059
Test Loss of 92176013499.129303, Test MSE of 92176012273.085693
Epoch 18: training loss 105821203832.471
Test Loss of 89298859870.452927, Test MSE of 89298861718.549011
Epoch 19: training loss 102192304850.824
Test Loss of 85725686601.845016, Test MSE of 85725685773.137802
Epoch 20: training loss 97585422411.294
Test Loss of 81965115792.314590, Test MSE of 81965116301.875168
Epoch 21: training loss 94386498876.235
Test Loss of 79093444281.115891, Test MSE of 79093445391.227295
Epoch 22: training loss 90291970394.353
Test Loss of 75434423480.049973, Test MSE of 75434423426.921494
Epoch 23: training loss 86372522059.294
Test Loss of 73033628871.683548, Test MSE of 73033628213.453506
Epoch 24: training loss 82649717022.118
Test Loss of 68430038318.486237, Test MSE of 68430037943.402283
Epoch 25: training loss 79252033837.176
Test Loss of 68076255900.454315, Test MSE of 68076256969.484787
Epoch 26: training loss 76926467787.294
Test Loss of 63501265641.201019, Test MSE of 63501266047.719193
Epoch 27: training loss 72812006377.412
Test Loss of 61032190089.386078, Test MSE of 61032189562.373459
Epoch 28: training loss 68814913784.471
Test Loss of 57433870342.632431, Test MSE of 57433871166.759857
Epoch 29: training loss 65673581696.000
Test Loss of 55293946677.947723, Test MSE of 55293945128.498817
Epoch 30: training loss 63724143864.471
Test Loss of 54344288128.562569, Test MSE of 54344288678.272018
Epoch 31: training loss 60462928926.118
Test Loss of 49946681553.395325, Test MSE of 49946682283.179649
Epoch 32: training loss 56559491154.824
Test Loss of 48694670725.655334, Test MSE of 48694670490.517349
Epoch 33: training loss 54229366189.176
Test Loss of 42164732879.914871, Test MSE of 42164733453.428726
Epoch 34: training loss 51094815216.941
Test Loss of 44152982298.470505, Test MSE of 44152982721.310219
Epoch 35: training loss 49439068318.118
Test Loss of 41945329238.576912, Test MSE of 41945328917.539703
Epoch 36: training loss 46400468803.765
Test Loss of 36401085121.406433, Test MSE of 36401085943.342773
Epoch 37: training loss 43720337972.706
Test Loss of 36264399721.585938, Test MSE of 36264400361.080109
Epoch 38: training loss 41611418624.000
Test Loss of 35856878835.268105, Test MSE of 35856878637.282555
Epoch 39: training loss 40195399732.706
Test Loss of 36037860715.599350, Test MSE of 36037860702.191284
Epoch 40: training loss 37897373944.471
Test Loss of 36603823281.417534, Test MSE of 36603822818.436707
Epoch 41: training loss 35883669357.176
Test Loss of 31177221559.872311, Test MSE of 31177221942.556992
Epoch 42: training loss 34143787045.647
Test Loss of 30669465507.619709, Test MSE of 30669465517.877415
Epoch 43: training loss 32968424259.765
Test Loss of 29456512675.797363, Test MSE of 29456512502.136955
Epoch 44: training loss 31381295314.824
Test Loss of 28398563657.726578, Test MSE of 28398563453.219475
Epoch 45: training loss 29962043320.471
Test Loss of 28606828009.615543, Test MSE of 28606827762.107941
Epoch 46: training loss 28462533680.941
Test Loss of 24711559163.736294, Test MSE of 24711558890.935925
Epoch 47: training loss 27189933961.412
Test Loss of 24180975617.894981, Test MSE of 24180975946.079582
Epoch 48: training loss 25845957940.706
Test Loss of 25015517727.859356, Test MSE of 25015518139.495781
Epoch 49: training loss 24656761163.294
Test Loss of 24837955057.669212, Test MSE of 24837955106.961597
Epoch 50: training loss 24012901906.824
Test Loss of 22304117086.097618, Test MSE of 22304117305.136070
Epoch 51: training loss 23058689046.588
Test Loss of 23635344632.953041, Test MSE of 23635344462.221878
Epoch 52: training loss 22176169856.000
Test Loss of 20640608891.765903, Test MSE of 20640608739.087799
Epoch 53: training loss 21146676216.471
Test Loss of 21995158103.761276, Test MSE of 21995157950.509895
Epoch 54: training loss 20715659026.824
Test Loss of 22390308823.731667, Test MSE of 22390308629.925827
Epoch 55: training loss 19836411881.412
Test Loss of 22497094294.295628, Test MSE of 22497094585.491512
Epoch 56: training loss 18964902569.412
Test Loss of 20231008545.931992, Test MSE of 20231008503.688362
Epoch 57: training loss 18428790290.824
Test Loss of 20343746564.263706, Test MSE of 20343746421.039066
Epoch 58: training loss 17999272568.471
Test Loss of 20327087112.290539, Test MSE of 20327087295.328587
Epoch 59: training loss 17366208553.412
Test Loss of 19378251033.167709, Test MSE of 19378251292.690010
Epoch 60: training loss 16942850307.765
Test Loss of 18822979833.189915, Test MSE of 18822979830.505653
Epoch 61: training loss 16725029763.765
Test Loss of 18391333410.701828, Test MSE of 18391333796.257671
Epoch 62: training loss 15999254821.647
Test Loss of 20008023471.344898, Test MSE of 20008023574.059669
Epoch 63: training loss 15445363004.235
Test Loss of 18491944926.364098, Test MSE of 18491944760.930988
Epoch 64: training loss 15196166812.235
Test Loss of 18214453426.601898, Test MSE of 18214453209.516640
Epoch 65: training loss 14470291045.647
Test Loss of 18008863381.348137, Test MSE of 18008863409.800819
Epoch 66: training loss 14371560534.588
Test Loss of 17270236443.062687, Test MSE of 17270236479.259281
Epoch 67: training loss 13777859211.294
Test Loss of 18798619411.364330, Test MSE of 18798619457.377102
Epoch 68: training loss 13595973138.824
Test Loss of 17755606922.984962, Test MSE of 17755606961.233852
Epoch 69: training loss 13065106016.000
Test Loss of 18597167335.187603, Test MSE of 18597167512.567749
Epoch 70: training loss 12705619166.118
Test Loss of 17300911237.832985, Test MSE of 17300911008.860237
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 17300911008.860237, 'MSE - std': 0.0, 'R2 - mean': 0.8652759288394396, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917899896.471
Test Loss of 424557047651.664124, Test MSE of 424557039219.575684
Epoch 2: training loss 427896125801.412
Test Loss of 424539612089.175110, Test MSE of 424539617702.534241
Epoch 3: training loss 427867273818.353
Test Loss of 424516426633.326843, Test MSE of 424516428953.423706
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885892306.824
Test Loss of 424521496622.900757, Test MSE of 424521508397.446045
Epoch 2: training loss 427875928425.412
Test Loss of 424522371997.697876, Test MSE of 424522363556.982422
Epoch 3: training loss 427875436664.471
Test Loss of 424522044401.077026, Test MSE of 424522048119.593018
Epoch 4: training loss 422455122281.412
Test Loss of 407791449677.575745, Test MSE of 407791448729.344543
Epoch 5: training loss 385715380705.882
Test Loss of 353713079230.860046, Test MSE of 353713079260.945984
Epoch 6: training loss 318242305566.118
Test Loss of 277606632180.570923, Test MSE of 277606631940.102234
Epoch 7: training loss 224872106164.706
Test Loss of 181060110550.606537, Test MSE of 181060113643.989136
Epoch 8: training loss 158227246200.471
Test Loss of 142004171402.925751, Test MSE of 142004170796.935699
Epoch 9: training loss 140158955550.118
Test Loss of 131592872743.261627, Test MSE of 131592871536.691360
Epoch 10: training loss 133694962748.235
Test Loss of 127795482443.029373, Test MSE of 127795482122.576828
Epoch 11: training loss 130597312391.529
Test Loss of 124550551987.371735, Test MSE of 124550551394.457779
Epoch 12: training loss 128067916739.765
Test Loss of 121887015615.037704, Test MSE of 121887016190.496033
Epoch 13: training loss 124228825931.294
Test Loss of 117833941007.159836, Test MSE of 117833938765.201385
Epoch 14: training loss 120683935924.706
Test Loss of 115858513455.729813, Test MSE of 115858513266.766663
Epoch 15: training loss 116762898070.588
Test Loss of 111805754727.809387, Test MSE of 111805754923.334457
Epoch 16: training loss 112151053312.000
Test Loss of 107272564819.379135, Test MSE of 107272567395.576187
Epoch 17: training loss 107832219015.529
Test Loss of 103955069718.680542, Test MSE of 103955070220.148285
Epoch 18: training loss 104705070893.176
Test Loss of 100397508005.869995, Test MSE of 100397505598.197189
Epoch 19: training loss 100957571102.118
Test Loss of 95270168470.354843, Test MSE of 95270169941.630920
Epoch 20: training loss 96023075177.412
Test Loss of 91057134015.689102, Test MSE of 91057136461.107193
Epoch 21: training loss 92910593234.824
Test Loss of 87416929115.847336, Test MSE of 87416930532.263535
Epoch 22: training loss 88564886256.941
Test Loss of 83697565930.977554, Test MSE of 83697567461.026047
Epoch 23: training loss 84883650544.941
Test Loss of 81683538369.110336, Test MSE of 81683536849.741302
Epoch 24: training loss 81525568647.529
Test Loss of 78915678046.926666, Test MSE of 78915680074.764923
Epoch 25: training loss 77648878592.000
Test Loss of 74989619816.342361, Test MSE of 74989619210.201599
Epoch 26: training loss 73948880128.000
Test Loss of 72029931877.677536, Test MSE of 72029933355.844528
Epoch 27: training loss 70815141029.647
Test Loss of 68073031273.052971, Test MSE of 68073032737.937035
Epoch 28: training loss 66972099328.000
Test Loss of 64513591174.247513, Test MSE of 64513590119.535683
Epoch 29: training loss 64310575179.294
Test Loss of 62593738518.443672, Test MSE of 62593738899.206963
Epoch 30: training loss 60871054411.294
Test Loss of 56533864514.798058, Test MSE of 56533864555.229538
Epoch 31: training loss 57882883117.176
Test Loss of 52575665190.136482, Test MSE of 52575665617.788139
Epoch 32: training loss 54666153351.529
Test Loss of 52736169066.829514, Test MSE of 52736169303.943237
Epoch 33: training loss 51791029624.471
Test Loss of 50172411545.374969, Test MSE of 50172411589.466545
Epoch 34: training loss 49648846945.882
Test Loss of 46749532006.032845, Test MSE of 46749530600.074554
Epoch 35: training loss 46669575363.765
Test Loss of 46404206190.737915, Test MSE of 46404206254.196945
Epoch 36: training loss 44180710535.529
Test Loss of 45260536259.479065, Test MSE of 45260538236.872864
Epoch 37: training loss 41428712907.294
Test Loss of 44146261277.905159, Test MSE of 44146260705.589134
Epoch 38: training loss 39693419301.647
Test Loss of 37701798165.140877, Test MSE of 37701797525.796471
Epoch 39: training loss 37845104835.765
Test Loss of 43844402253.457321, Test MSE of 43844402138.591431
Epoch 40: training loss 35300661443.765
Test Loss of 38520512593.484154, Test MSE of 38520511105.233536
Epoch 41: training loss 33648090179.765
Test Loss of 40902499585.006706, Test MSE of 40902500771.638184
Epoch 42: training loss 32010217524.706
Test Loss of 36405477870.116119, Test MSE of 36405478135.542336
Epoch 43: training loss 29967574603.294
Test Loss of 33183589534.941475, Test MSE of 33183589220.774128
Epoch 44: training loss 28736080647.529
Test Loss of 33199503002.796207, Test MSE of 33199503733.451206
Epoch 45: training loss 27148292254.118
Test Loss of 31715326475.488319, Test MSE of 31715326632.432110
Epoch 46: training loss 26015742317.176
Test Loss of 31580845118.297478, Test MSE of 31580845242.360493
Epoch 47: training loss 24761834142.118
Test Loss of 32949497240.605137, Test MSE of 32949496996.818001
Epoch 48: training loss 23678130416.941
Test Loss of 30911906783.548462, Test MSE of 30911906709.077904
Epoch 49: training loss 22333650913.882
Test Loss of 29411891391.866760, Test MSE of 29411892049.082207
Epoch 50: training loss 21530344165.647
Test Loss of 30223558112.140644, Test MSE of 30223558304.528801
Epoch 51: training loss 20671848105.412
Test Loss of 27853821107.312515, Test MSE of 27853821264.802597
Epoch 52: training loss 19776596754.824
Test Loss of 29068723418.159611, Test MSE of 29068724431.367897
Epoch 53: training loss 18864242631.529
Test Loss of 27959675413.910709, Test MSE of 27959675968.173122
Epoch 54: training loss 18014503721.412
Test Loss of 27748595834.699978, Test MSE of 27748597000.670258
Epoch 55: training loss 17623041310.118
Test Loss of 26591808908.761509, Test MSE of 26591808478.676247
Epoch 56: training loss 17322939840.000
Test Loss of 26345316376.634743, Test MSE of 26345316510.948971
Epoch 57: training loss 16329918972.235
Test Loss of 26291393565.135323, Test MSE of 26291393353.486176
Epoch 58: training loss 16160038821.647
Test Loss of 25186317856.569973, Test MSE of 25186317873.479347
Epoch 59: training loss 15513585453.176
Test Loss of 25367165030.802685, Test MSE of 25367164731.625854
Epoch 60: training loss 14882609701.647
Test Loss of 26335844330.207726, Test MSE of 26335844209.235180
Epoch 61: training loss 14670589451.294
Test Loss of 24293655600.085125, Test MSE of 24293655941.529385
Epoch 62: training loss 13863101120.000
Test Loss of 27680649658.241035, Test MSE of 27680650167.985249
Epoch 63: training loss 13381329091.765
Test Loss of 23902831324.173027, Test MSE of 23902830725.387447
Epoch 64: training loss 13271217344.000
Test Loss of 25104299453.557251, Test MSE of 25104299010.028931
Epoch 65: training loss 12706312353.882
Test Loss of 26222613321.134396, Test MSE of 26222613430.639282
Epoch 66: training loss 12540504176.941
Test Loss of 27503258066.165165, Test MSE of 27503258542.071404
Epoch 67: training loss 12165311841.882
Test Loss of 26447005856.599583, Test MSE of 26447006107.278156
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21873958558.0692, 'MSE - std': 4573047549.20896, 'R2 - mean': 0.8382309320755995, 'R2 - std': 0.02704499676384009} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005690 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926905615.059
Test Loss of 447258763334.114258, Test MSE of 447258774116.133728
Epoch 2: training loss 421906741609.412
Test Loss of 447241346584.990051, Test MSE of 447241342951.680298
Epoch 3: training loss 421879918592.000
Test Loss of 447217345127.868591, Test MSE of 447217347627.095093
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900350885.647
Test Loss of 447223533875.934326, Test MSE of 447223535497.377991
Epoch 2: training loss 421889663939.765
Test Loss of 447224515369.630371, Test MSE of 447224511364.874756
Epoch 3: training loss 421889210729.412
Test Loss of 447225211661.205627, Test MSE of 447225212154.170959
Epoch 4: training loss 416645854629.647
Test Loss of 430472074721.561890, Test MSE of 430472076152.625000
Epoch 5: training loss 380535667410.824
Test Loss of 374700704486.832275, Test MSE of 374700700373.789612
Epoch 6: training loss 313014255856.941
Test Loss of 296823653981.683105, Test MSE of 296823654903.558899
Epoch 7: training loss 220780706093.176
Test Loss of 196294295294.282684, Test MSE of 196294289845.321869
Epoch 8: training loss 157092255744.000
Test Loss of 154402604800.651398, Test MSE of 154402605837.109528
Epoch 9: training loss 137015246516.706
Test Loss of 142722310695.439270, Test MSE of 142722310546.544067
Epoch 10: training loss 133077755361.882
Test Loss of 137644446629.277802, Test MSE of 137644445929.851105
Epoch 11: training loss 129705106944.000
Test Loss of 134091883098.603745, Test MSE of 134091883698.341003
Epoch 12: training loss 127174786288.941
Test Loss of 130815516279.739075, Test MSE of 130815515301.673172
Epoch 13: training loss 121935681897.412
Test Loss of 127292770219.199631, Test MSE of 127292770962.080811
Epoch 14: training loss 118388822678.588
Test Loss of 123190248560.988205, Test MSE of 123190250912.167572
Epoch 15: training loss 115087516310.588
Test Loss of 119945157032.001846, Test MSE of 119945158671.811020
Epoch 16: training loss 111728270155.294
Test Loss of 117765555836.002777, Test MSE of 117765554230.777359
Epoch 17: training loss 107891155877.647
Test Loss of 113067173274.500122, Test MSE of 113067175472.395477
Epoch 18: training loss 103232137246.118
Test Loss of 109506846862.123520, Test MSE of 109506847766.731186
Epoch 19: training loss 99874214972.235
Test Loss of 105837790972.861435, Test MSE of 105837789654.139664
Epoch 20: training loss 95969854945.882
Test Loss of 102067290612.274811, Test MSE of 102067290598.197144
Epoch 21: training loss 91929581989.647
Test Loss of 97510875077.255615, Test MSE of 97510874963.190186
Epoch 22: training loss 88265452890.353
Test Loss of 96425718299.121902, Test MSE of 96425718781.736908
Epoch 23: training loss 85205277997.176
Test Loss of 88523603391.925980, Test MSE of 88523603353.850800
Epoch 24: training loss 82303612988.235
Test Loss of 86038793623.183899, Test MSE of 86038795147.776611
Epoch 25: training loss 78075402209.882
Test Loss of 81452620145.047424, Test MSE of 81452619024.580994
Epoch 26: training loss 74335994006.588
Test Loss of 77746789497.989365, Test MSE of 77746788624.225998
Epoch 27: training loss 71144160557.176
Test Loss of 72022725215.341202, Test MSE of 72022724279.075562
Epoch 28: training loss 68143694923.294
Test Loss of 72100046552.856812, Test MSE of 72100046598.152267
Epoch 29: training loss 65327205089.882
Test Loss of 66815048145.691422, Test MSE of 66815049863.983521
Epoch 30: training loss 61297290480.941
Test Loss of 65530405915.714088, Test MSE of 65530404931.472832
Epoch 31: training loss 58964607728.941
Test Loss of 59929846618.426094, Test MSE of 59929846507.868446
Epoch 32: training loss 55703999887.059
Test Loss of 57022852514.553780, Test MSE of 57022852289.387871
Epoch 33: training loss 52944389300.706
Test Loss of 55531693364.171181, Test MSE of 55531694226.124596
Epoch 34: training loss 50856935710.118
Test Loss of 52739222245.411057, Test MSE of 52739222590.701897
Epoch 35: training loss 47789160485.647
Test Loss of 47315840139.044182, Test MSE of 47315840840.123871
Epoch 36: training loss 46104200854.588
Test Loss of 45984968614.935921, Test MSE of 45984968102.367233
Epoch 37: training loss 43641570311.529
Test Loss of 47175135898.085587, Test MSE of 47175135252.967583
Epoch 38: training loss 41489011998.118
Test Loss of 42624341404.395096, Test MSE of 42624340946.981010
Epoch 39: training loss 38970507038.118
Test Loss of 40750853271.361557, Test MSE of 40750852353.764801
Epoch 40: training loss 37159412796.235
Test Loss of 40308347148.139717, Test MSE of 40308346528.560883
Epoch 41: training loss 35592547456.000
Test Loss of 37671298544.721718, Test MSE of 37671299140.626854
Epoch 42: training loss 33542457728.000
Test Loss of 36729462277.092758, Test MSE of 36729461612.853264
Epoch 43: training loss 32278275719.529
Test Loss of 35244000585.015961, Test MSE of 35244001506.860641
Epoch 44: training loss 30311495725.176
Test Loss of 34736972694.117973, Test MSE of 34736972876.321823
Epoch 45: training loss 29381606708.706
Test Loss of 29241419353.419384, Test MSE of 29241419765.408722
Epoch 46: training loss 27830378232.471
Test Loss of 30011160901.462872, Test MSE of 30011160569.086254
Epoch 47: training loss 26658359853.176
Test Loss of 29626901102.501041, Test MSE of 29626901112.052761
Epoch 48: training loss 25210251471.059
Test Loss of 28705362137.922737, Test MSE of 28705362227.978401
Epoch 49: training loss 24451644965.647
Test Loss of 28183200258.960907, Test MSE of 28183200182.030113
Epoch 50: training loss 23339871442.824
Test Loss of 30641910080.251678, Test MSE of 30641910656.901192
Epoch 51: training loss 22488072990.118
Test Loss of 29796360458.244736, Test MSE of 29796360520.936306
Epoch 52: training loss 21394128749.176
Test Loss of 26676152947.712238, Test MSE of 26676152869.668846
Epoch 53: training loss 20569893150.118
Test Loss of 25894493685.932919, Test MSE of 25894493811.503155
Epoch 54: training loss 19923427425.882
Test Loss of 25595411370.489014, Test MSE of 25595412013.578362
Epoch 55: training loss 19423755794.824
Test Loss of 24860250582.902615, Test MSE of 24860251089.312832
Epoch 56: training loss 18870749662.118
Test Loss of 23456337683.601204, Test MSE of 23456337478.258057
Epoch 57: training loss 17833752387.765
Test Loss of 23289744947.519779, Test MSE of 23289745230.618755
Epoch 58: training loss 17495113193.412
Test Loss of 23036488709.448067, Test MSE of 23036489114.089123
Epoch 59: training loss 17020322123.294
Test Loss of 23851044286.504742, Test MSE of 23851044686.314251
Epoch 60: training loss 16632689415.529
Test Loss of 25052148308.918808, Test MSE of 25052148609.566109
Epoch 61: training loss 15903591879.529
Test Loss of 23469021459.245895, Test MSE of 23469021864.631172
Epoch 62: training loss 15324399062.588
Test Loss of 25531437129.904232, Test MSE of 25531437637.341141
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23093118251.159847, 'MSE - std': 4112729422.9093747, 'R2 - mean': 0.8355002562418271, 'R2 - std': 0.022417279369325277} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005501 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109712865.882
Test Loss of 410764172254.356323, Test MSE of 410764179003.634521
Epoch 2: training loss 430088470287.059
Test Loss of 410746779557.967590, Test MSE of 410746784970.900513
Epoch 3: training loss 430061459817.412
Test Loss of 410724820145.695496, Test MSE of 410724821596.792175
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080825946.353
Test Loss of 410730505780.360962, Test MSE of 410730507396.628662
Epoch 2: training loss 430067769825.882
Test Loss of 410730800161.643677, Test MSE of 410730806904.694885
Epoch 3: training loss 430067326494.118
Test Loss of 410730080842.632141, Test MSE of 410730069720.908936
Epoch 4: training loss 424801569370.353
Test Loss of 393909358809.973145, Test MSE of 393909361065.228821
Epoch 5: training loss 388578955264.000
Test Loss of 339618099095.751953, Test MSE of 339618102447.787720
Epoch 6: training loss 320336786010.353
Test Loss of 262059734760.425720, Test MSE of 262059742590.636871
Epoch 7: training loss 229765025310.118
Test Loss of 164995428270.496979, Test MSE of 164995427214.649872
Epoch 8: training loss 164003921588.706
Test Loss of 124599919725.460434, Test MSE of 124599921515.832306
Epoch 9: training loss 143202260570.353
Test Loss of 113948520290.680237, Test MSE of 113948519934.950241
Epoch 10: training loss 137984115922.824
Test Loss of 109958352211.753815, Test MSE of 109958352243.317184
Epoch 11: training loss 135434195245.176
Test Loss of 107152367877.567795, Test MSE of 107152366017.930893
Epoch 12: training loss 131856094268.235
Test Loss of 104759708719.859329, Test MSE of 104759709672.584244
Epoch 13: training loss 128497700924.235
Test Loss of 101733355737.973160, Test MSE of 101733356541.466263
Epoch 14: training loss 124567365541.647
Test Loss of 98008293216.784821, Test MSE of 98008292190.266830
Epoch 15: training loss 120790382561.882
Test Loss of 94897950665.032852, Test MSE of 94897949887.470245
Epoch 16: training loss 116260691606.588
Test Loss of 92145065661.778809, Test MSE of 92145066190.771408
Epoch 17: training loss 113870817159.529
Test Loss of 88781394926.467377, Test MSE of 88781394786.806000
Epoch 18: training loss 108074688662.588
Test Loss of 86348037184.918091, Test MSE of 86348037385.799835
Epoch 19: training loss 103878815141.647
Test Loss of 81181007417.099487, Test MSE of 81181007082.996964
Epoch 20: training loss 101278974945.882
Test Loss of 78745904188.179550, Test MSE of 78745905003.798035
Epoch 21: training loss 97610268039.529
Test Loss of 76934468908.897736, Test MSE of 76934469347.856262
Epoch 22: training loss 92341273072.941
Test Loss of 75115986178.250809, Test MSE of 75115984830.234818
Epoch 23: training loss 89079659113.412
Test Loss of 71418498315.727905, Test MSE of 71418497342.454819
Epoch 24: training loss 85242340020.706
Test Loss of 69588303003.898193, Test MSE of 69588302773.259979
Epoch 25: training loss 81712204408.471
Test Loss of 66708058413.371590, Test MSE of 66708057490.745949
Epoch 26: training loss 78934109289.412
Test Loss of 62395096887.085609, Test MSE of 62395096756.119560
Epoch 27: training loss 74665063529.412
Test Loss of 58364191281.043961, Test MSE of 58364191715.322693
Epoch 28: training loss 71369694976.000
Test Loss of 55114593554.361870, Test MSE of 55114594507.097610
Epoch 29: training loss 67632509304.471
Test Loss of 54867053445.271637, Test MSE of 54867053305.618294
Epoch 30: training loss 65384057027.765
Test Loss of 51737208624.451645, Test MSE of 51737209579.535027
Epoch 31: training loss 61970053812.706
Test Loss of 50082573530.447014, Test MSE of 50082574193.971138
Epoch 32: training loss 59133661440.000
Test Loss of 49620264464.347984, Test MSE of 49620264583.501991
Epoch 33: training loss 56423459501.176
Test Loss of 44107010046.104584, Test MSE of 44107010281.857994
Epoch 34: training loss 53017627655.529
Test Loss of 41863574923.668671, Test MSE of 41863574791.490501
Epoch 35: training loss 49914941123.765
Test Loss of 40036362578.332253, Test MSE of 40036362171.116119
Epoch 36: training loss 47618929159.529
Test Loss of 38941440990.356316, Test MSE of 38941441588.733185
Epoch 37: training loss 46021755264.000
Test Loss of 38674127379.664970, Test MSE of 38674127554.120605
Epoch 38: training loss 43261432756.706
Test Loss of 35908954645.560387, Test MSE of 35908955148.470200
Epoch 39: training loss 41487779410.824
Test Loss of 34788545245.527069, Test MSE of 34788545144.472412
Epoch 40: training loss 39285673268.706
Test Loss of 33215511535.415085, Test MSE of 33215511528.053280
Epoch 41: training loss 37097182915.765
Test Loss of 30885169753.795464, Test MSE of 30885169485.670544
Epoch 42: training loss 35676338898.824
Test Loss of 30030948962.324848, Test MSE of 30030949628.233650
Epoch 43: training loss 33818352865.882
Test Loss of 28672488219.128181, Test MSE of 28672488909.921776
Epoch 44: training loss 32099495943.529
Test Loss of 25708708948.819992, Test MSE of 25708709019.269543
Epoch 45: training loss 30381123614.118
Test Loss of 26907466869.042110, Test MSE of 26907466605.678520
Epoch 46: training loss 29547603184.941
Test Loss of 24955232083.516891, Test MSE of 24955232360.831188
Epoch 47: training loss 28232485835.294
Test Loss of 26644185557.116150, Test MSE of 26644185732.288242
Epoch 48: training loss 26949485504.000
Test Loss of 25322646238.474781, Test MSE of 25322645915.319355
Epoch 49: training loss 25868439288.471
Test Loss of 26355963989.293846, Test MSE of 26355963819.244678
Epoch 50: training loss 24562910874.353
Test Loss of 23772396021.338268, Test MSE of 23772396060.190384
Epoch 51: training loss 23390078870.588
Test Loss of 23580128185.395649, Test MSE of 23580128161.619781
Epoch 52: training loss 22525299090.824
Test Loss of 22328214839.322536, Test MSE of 22328214828.204777
Epoch 53: training loss 22015955776.000
Test Loss of 23318582908.386856, Test MSE of 23318582994.241310
Epoch 54: training loss 21068228318.118
Test Loss of 22559357443.553909, Test MSE of 22559357863.134884
Epoch 55: training loss 20539725609.412
Test Loss of 22461999992.003700, Test MSE of 22462000425.491272
Epoch 56: training loss 19820610605.176
Test Loss of 20822769655.470615, Test MSE of 20822769068.690762
Epoch 57: training loss 18837431977.412
Test Loss of 20530350069.575195, Test MSE of 20530350164.616577
Epoch 58: training loss 18518654000.941
Test Loss of 19616988694.034245, Test MSE of 19616988804.149605
Epoch 59: training loss 17962164267.294
Test Loss of 21691452050.658028, Test MSE of 21691451876.549908
Epoch 60: training loss 17190435531.294
Test Loss of 21158305513.373440, Test MSE of 21158305480.632996
Epoch 61: training loss 16799708510.118
Test Loss of 19934826572.764462, Test MSE of 19934826613.365425
Epoch 62: training loss 16508079051.294
Test Loss of 17935477867.565018, Test MSE of 17935477865.338924
Epoch 63: training loss 15972321483.294
Test Loss of 18660196178.569180, Test MSE of 18660196104.249950
Epoch 64: training loss 15288593168.941
Test Loss of 18928918874.861637, Test MSE of 18928918839.776493
Epoch 65: training loss 14793395629.176
Test Loss of 19790575397.079128, Test MSE of 19790575304.089081
Epoch 66: training loss 14589491926.588
Test Loss of 18330596885.560390, Test MSE of 18330597044.993881
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21902487949.618355, 'MSE - std': 4115666291.6707306, 'R2 - mean': 0.8388021995308463, 'R2 - std': 0.02023880676308458} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005389 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042763685.647
Test Loss of 431610741456.259155, Test MSE of 431610735777.384521
Epoch 2: training loss 424021951186.824
Test Loss of 431590820240.881104, Test MSE of 431590818787.975464
Epoch 3: training loss 423994452329.412
Test Loss of 431563599403.831543, Test MSE of 431563601776.131836
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009529825.882
Test Loss of 431565905006.408142, Test MSE of 431565912891.636230
Epoch 2: training loss 423997846949.647
Test Loss of 431569267661.297546, Test MSE of 431569269309.108459
Epoch 3: training loss 423997367476.706
Test Loss of 431568881845.012512, Test MSE of 431568886388.220703
Epoch 4: training loss 418863446016.000
Test Loss of 414790230376.129578, Test MSE of 414790231241.666382
Epoch 5: training loss 382869553995.294
Test Loss of 359131547101.171692, Test MSE of 359131545429.553528
Epoch 6: training loss 315900603934.118
Test Loss of 280952397281.910217, Test MSE of 280952401283.366516
Epoch 7: training loss 223863502215.529
Test Loss of 179852774300.016663, Test MSE of 179852776000.246643
Epoch 8: training loss 159389208967.529
Test Loss of 137169148610.043503, Test MSE of 137169149527.432175
Epoch 9: training loss 139938625867.294
Test Loss of 125822078837.634430, Test MSE of 125822079554.769180
Epoch 10: training loss 134082690499.765
Test Loss of 120320313938.213791, Test MSE of 120320316365.824173
Epoch 11: training loss 133010496602.353
Test Loss of 118174629099.505783, Test MSE of 118174629622.937088
Epoch 12: training loss 127458477447.529
Test Loss of 114572046735.933365, Test MSE of 114572045338.148346
Epoch 13: training loss 124050863706.353
Test Loss of 109656233321.551132, Test MSE of 109656233049.434891
Epoch 14: training loss 120791069831.529
Test Loss of 108244304163.894501, Test MSE of 108244305100.632980
Epoch 15: training loss 116886133624.471
Test Loss of 103053161427.931519, Test MSE of 103053158989.417450
Epoch 16: training loss 113481205428.706
Test Loss of 100941259718.663574, Test MSE of 100941259772.708847
Epoch 17: training loss 107898416670.118
Test Loss of 96545369219.257751, Test MSE of 96545369890.739456
Epoch 18: training loss 105226185276.235
Test Loss of 95311185134.348907, Test MSE of 95311185487.766937
Epoch 19: training loss 100937853394.824
Test Loss of 88529569112.492371, Test MSE of 88529568914.649338
Epoch 20: training loss 97213260528.941
Test Loss of 87044459382.108276, Test MSE of 87044460531.327637
Epoch 21: training loss 92947218959.059
Test Loss of 83552370752.444244, Test MSE of 83552372296.920868
Epoch 22: training loss 89222600884.706
Test Loss of 78347712685.430817, Test MSE of 78347712452.284760
Epoch 23: training loss 85920231936.000
Test Loss of 75323075690.617310, Test MSE of 75323075427.478485
Epoch 24: training loss 82384520869.647
Test Loss of 69088829877.841736, Test MSE of 69088829925.490860
Epoch 25: training loss 79084261270.588
Test Loss of 66767535659.831558, Test MSE of 66767535612.169510
Epoch 26: training loss 75153516374.588
Test Loss of 64634130081.347527, Test MSE of 64634128157.785370
Epoch 27: training loss 70817869056.000
Test Loss of 63209822347.313278, Test MSE of 63209822785.691124
Epoch 28: training loss 68482367329.882
Test Loss of 59424060555.787132, Test MSE of 59424059276.658203
Epoch 29: training loss 64678106729.412
Test Loss of 55229057915.794540, Test MSE of 55229057834.231232
Epoch 30: training loss 62064634051.765
Test Loss of 53095321387.713097, Test MSE of 53095322076.513130
Epoch 31: training loss 59165929893.647
Test Loss of 50099078497.969460, Test MSE of 50099077814.204391
Epoch 32: training loss 56318859407.059
Test Loss of 49734142694.056458, Test MSE of 49734144224.317474
Epoch 33: training loss 53343706774.588
Test Loss of 42668153403.468765, Test MSE of 42668152407.753883
Epoch 34: training loss 50947732690.824
Test Loss of 41830330059.046738, Test MSE of 41830331162.697578
Epoch 35: training loss 48224220408.471
Test Loss of 41048097429.501160, Test MSE of 41048097870.696411
Epoch 36: training loss 45248743435.294
Test Loss of 38652471018.321144, Test MSE of 38652471214.417473
Epoch 37: training loss 43717910957.176
Test Loss of 35220462642.702454, Test MSE of 35220463004.264343
Epoch 38: training loss 41161919337.412
Test Loss of 35247714133.412308, Test MSE of 35247713969.147888
Epoch 39: training loss 39152885263.059
Test Loss of 32367392278.981953, Test MSE of 32367391586.595486
Epoch 40: training loss 36991230810.353
Test Loss of 32939115349.886166, Test MSE of 32939115847.219120
Epoch 41: training loss 35177607597.176
Test Loss of 33649254589.541878, Test MSE of 33649254693.718533
Epoch 42: training loss 33705499648.000
Test Loss of 30615258233.780659, Test MSE of 30615257698.727123
Epoch 43: training loss 32380482085.647
Test Loss of 31437674558.548820, Test MSE of 31437674304.602753
Epoch 44: training loss 30963822866.824
Test Loss of 29160942696.721889, Test MSE of 29160942038.862484
Epoch 45: training loss 29179543883.294
Test Loss of 28490012911.770477, Test MSE of 28490013336.888924
Epoch 46: training loss 28006134994.824
Test Loss of 29144713158.189728, Test MSE of 29144713615.947277
Epoch 47: training loss 26851906529.882
Test Loss of 27164257364.819992, Test MSE of 27164256849.318111
Epoch 48: training loss 25766255344.941
Test Loss of 27118035268.116611, Test MSE of 27118034602.035362
Epoch 49: training loss 24620527442.824
Test Loss of 27831492951.070801, Test MSE of 27831492144.899555
Epoch 50: training loss 23748437116.235
Test Loss of 25563624983.929661, Test MSE of 25563624646.478851
Epoch 51: training loss 22643970029.176
Test Loss of 22742942820.931049, Test MSE of 22742942765.572334
Epoch 52: training loss 21799346337.882
Test Loss of 22023324646.885700, Test MSE of 22023324421.345772
Epoch 53: training loss 21509960391.529
Test Loss of 21741648997.878761, Test MSE of 21741649447.801373
Epoch 54: training loss 20165342772.706
Test Loss of 23130541301.930588, Test MSE of 23130541170.095703
Epoch 55: training loss 19780792884.706
Test Loss of 23352852492.320221, Test MSE of 23352852614.260612
Epoch 56: training loss 19110990979.765
Test Loss of 21442258310.930126, Test MSE of 21442258243.791100
Epoch 57: training loss 18586731290.353
Test Loss of 20452842359.055992, Test MSE of 20452842155.111423
Epoch 58: training loss 18068402311.529
Test Loss of 20682445489.932438, Test MSE of 20682445427.078037
Epoch 59: training loss 17689026608.941
Test Loss of 19127796299.579823, Test MSE of 19127796590.858616
Epoch 60: training loss 17054063254.588
Test Loss of 23572488121.869503, Test MSE of 23572487994.309757
Epoch 61: training loss 16537603877.647
Test Loss of 21514118727.315132, Test MSE of 21514118939.458923
Epoch 62: training loss 16222015363.765
Test Loss of 22418334661.715874, Test MSE of 22418334195.394733
Epoch 63: training loss 15654935495.529
Test Loss of 20129512037.167976, Test MSE of 20129511903.362522
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21547892740.367188, 'MSE - std': 3748855594.606343, 'R2 - mean': 0.8409761711751796, 'R2 - std': 0.01861698284352873} 
 

Saving model.....
Results After CV: {'MSE - mean': 21547892740.367188, 'MSE - std': 3748855594.606343, 'R2 - mean': 0.8409761711751796, 'R2 - std': 0.01861698284352873}
Train time: 102.11929506460073
Inference time: 0.0750632819996099
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 71 finished with value: 21547892740.367188 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005556 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525497554.824
Test Loss of 418112224328.719849, Test MSE of 418112226414.505310
Epoch 2: training loss 427505157541.647
Test Loss of 418093458725.011353, Test MSE of 418093460956.276489
Epoch 3: training loss 427478253688.471
Test Loss of 418068839951.515137, Test MSE of 418068839717.752380
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493322270.118
Test Loss of 418070676604.358093, Test MSE of 418070669336.107117
Epoch 2: training loss 427481488444.235
Test Loss of 418071932991.481812, Test MSE of 418071930940.456909
Epoch 3: training loss 427480935725.176
Test Loss of 418070699162.204041, Test MSE of 418070699544.106934
Epoch 4: training loss 422256915034.353
Test Loss of 401480216889.619263, Test MSE of 401480218696.218079
Epoch 5: training loss 386265059930.353
Test Loss of 346644678518.140198, Test MSE of 346644674400.091003
Epoch 6: training loss 318148933993.412
Test Loss of 269535810639.826050, Test MSE of 269535809510.933960
Epoch 7: training loss 226527971207.529
Test Loss of 171953581192.201721, Test MSE of 171953583364.639099
Epoch 8: training loss 161317406117.647
Test Loss of 130453734164.311813, Test MSE of 130453734716.430923
Epoch 9: training loss 141098978680.471
Test Loss of 120509195488.792038, Test MSE of 120509195383.223434
Epoch 10: training loss 136631008677.647
Test Loss of 116161696228.167480, Test MSE of 116161696958.546494
Epoch 11: training loss 134341196423.529
Test Loss of 113706197458.165161, Test MSE of 113706198220.085739
Epoch 12: training loss 130319922898.824
Test Loss of 110103815680.829056, Test MSE of 110103816134.273224
Epoch 13: training loss 126607140261.647
Test Loss of 106735320076.080505, Test MSE of 106735318088.797623
Epoch 14: training loss 123390809359.059
Test Loss of 104495866473.763596, Test MSE of 104495867232.459198
Epoch 15: training loss 119051079077.647
Test Loss of 101095727572.060150, Test MSE of 101095726931.988724
Epoch 16: training loss 116409718919.529
Test Loss of 97999800708.944717, Test MSE of 97999800609.486923
Epoch 17: training loss 111959231939.765
Test Loss of 94133629893.492477, Test MSE of 94133630326.238754
Epoch 18: training loss 107674936952.471
Test Loss of 91633602562.842468, Test MSE of 91633601366.813889
Epoch 19: training loss 103856375642.353
Test Loss of 86592683510.880402, Test MSE of 86592683371.057571
Epoch 20: training loss 100187789086.118
Test Loss of 84226417223.653946, Test MSE of 84226419009.738663
Epoch 21: training loss 95989049916.235
Test Loss of 80656134584.582932, Test MSE of 80656136066.663254
Epoch 22: training loss 91737753871.059
Test Loss of 78920027853.960678, Test MSE of 78920025688.218674
Epoch 23: training loss 88427538793.412
Test Loss of 71701789421.938461, Test MSE of 71701788862.739487
Epoch 24: training loss 85251653165.176
Test Loss of 72947002327.968536, Test MSE of 72947002523.471313
Epoch 25: training loss 82247978962.824
Test Loss of 70122047713.976410, Test MSE of 70122046800.182037
Epoch 26: training loss 77664382991.059
Test Loss of 66074185006.959984, Test MSE of 66074185432.197792
Epoch 27: training loss 75451843998.118
Test Loss of 65026359917.553551, Test MSE of 65026360877.424156
Epoch 28: training loss 70772200914.824
Test Loss of 59608203425.073326, Test MSE of 59608204576.173592
Epoch 29: training loss 67216667964.235
Test Loss of 55236910931.556786, Test MSE of 55236911177.802940
Epoch 30: training loss 64347323911.529
Test Loss of 54718968642.738838, Test MSE of 54718967950.779343
Epoch 31: training loss 63106082138.353
Test Loss of 51180512838.943329, Test MSE of 51180513437.071915
Epoch 32: training loss 59182874789.647
Test Loss of 49036780999.979645, Test MSE of 49036781118.799416
Epoch 33: training loss 56274689588.706
Test Loss of 48615322428.106407, Test MSE of 48615322982.969810
Epoch 34: training loss 53493147979.294
Test Loss of 43697590128.928986, Test MSE of 43697590383.066315
Epoch 35: training loss 51210664131.765
Test Loss of 41811767254.310432, Test MSE of 41811767141.715950
Epoch 36: training loss 48851875282.824
Test Loss of 43213319560.971550, Test MSE of 43213319143.895012
Epoch 37: training loss 46755244438.588
Test Loss of 39380819635.430954, Test MSE of 39380819865.427086
Epoch 38: training loss 44101199292.235
Test Loss of 38117118504.386772, Test MSE of 38117118688.162514
Epoch 39: training loss 41596630806.588
Test Loss of 34357802957.309277, Test MSE of 34357803458.362442
Epoch 40: training loss 39465055096.471
Test Loss of 33721939074.043026, Test MSE of 33721939547.148464
Epoch 41: training loss 38120146695.529
Test Loss of 31343209677.368496, Test MSE of 31343209625.129894
Epoch 42: training loss 36033104210.824
Test Loss of 32839002528.421928, Test MSE of 32839002682.437138
Epoch 43: training loss 34701652988.235
Test Loss of 28590345480.112885, Test MSE of 28590345184.638859
Epoch 44: training loss 32539654347.294
Test Loss of 25394005105.935692, Test MSE of 25394004967.248272
Epoch 45: training loss 30936420668.235
Test Loss of 25977653479.661346, Test MSE of 25977652557.438541
Epoch 46: training loss 29771915151.059
Test Loss of 27079487889.735832, Test MSE of 27079488095.495289
Epoch 47: training loss 28773885240.471
Test Loss of 24078733037.701595, Test MSE of 24078733260.186752
Epoch 48: training loss 27258504161.882
Test Loss of 23895025914.611149, Test MSE of 23895025790.183697
Epoch 49: training loss 26309864440.471
Test Loss of 24509341052.180431, Test MSE of 24509341151.266518
Epoch 50: training loss 25222468054.588
Test Loss of 23889003527.816795, Test MSE of 23889003451.944157
Epoch 51: training loss 24222345441.882
Test Loss of 24180823327.800140, Test MSE of 24180823184.006092
Epoch 52: training loss 23423234213.647
Test Loss of 22709957451.739994, Test MSE of 22709957428.720631
Epoch 53: training loss 22187210842.353
Test Loss of 21999749780.400646, Test MSE of 21999749915.110691
Epoch 54: training loss 21675949850.353
Test Loss of 21502486534.158688, Test MSE of 21502486797.749660
Epoch 55: training loss 20734129566.118
Test Loss of 21440893138.342819, Test MSE of 21440893285.994812
Epoch 56: training loss 20015826183.529
Test Loss of 19906654343.964840, Test MSE of 19906654578.339436
Epoch 57: training loss 19240468528.941
Test Loss of 20481088913.025215, Test MSE of 20481089125.729652
Epoch 58: training loss 18867347689.412
Test Loss of 20407419247.626186, Test MSE of 20407419120.903538
Epoch 59: training loss 17840668848.941
Test Loss of 20317435612.646774, Test MSE of 20317435750.747551
Epoch 60: training loss 17659788743.529
Test Loss of 19481902073.604443, Test MSE of 19481902064.116280
Epoch 61: training loss 17195242330.353
Test Loss of 19732327051.399490, Test MSE of 19732327286.631104
Epoch 62: training loss 16949248000.000
Test Loss of 19428857129.275040, Test MSE of 19428857348.556339
Epoch 63: training loss 16175163038.118
Test Loss of 21740615087.818645, Test MSE of 21740615198.123409
Epoch 64: training loss 15617836464.941
Test Loss of 20471197877.918114, Test MSE of 20471198018.751141
Epoch 65: training loss 15130083753.412
Test Loss of 20759329101.042793, Test MSE of 20759328953.328960
Epoch 66: training loss 14729012498.824
Test Loss of 20191688395.118206, Test MSE of 20191688559.063309
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20191688559.06331, 'MSE - std': 0.0, 'R2 - mean': 0.8427651304090298, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005528 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918464180.706
Test Loss of 424556819920.507080, Test MSE of 424556818305.573242
Epoch 2: training loss 427898242951.529
Test Loss of 424541458997.651611, Test MSE of 424541462058.405579
Epoch 3: training loss 427870996118.588
Test Loss of 424519969246.008789, Test MSE of 424519964773.486694
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427891470336.000
Test Loss of 424526493185.776550, Test MSE of 424526485481.718567
Epoch 2: training loss 427879450744.471
Test Loss of 424527509880.627319, Test MSE of 424527514248.831177
Epoch 3: training loss 427878900193.882
Test Loss of 424527306875.173706, Test MSE of 424527313668.365601
Epoch 4: training loss 422411611196.235
Test Loss of 407733993790.356689, Test MSE of 407733992707.443237
Epoch 5: training loss 386011920384.000
Test Loss of 353778494414.493652, Test MSE of 353778490226.190674
Epoch 6: training loss 317458777630.118
Test Loss of 277266298275.027527, Test MSE of 277266295344.251282
Epoch 7: training loss 224397061240.471
Test Loss of 180843387618.568573, Test MSE of 180843390348.193054
Epoch 8: training loss 158694120387.765
Test Loss of 141822071004.765198, Test MSE of 141822071329.766418
Epoch 9: training loss 139359623439.059
Test Loss of 131581731668.741150, Test MSE of 131581733510.069855
Epoch 10: training loss 135062293744.941
Test Loss of 127706933043.815872, Test MSE of 127706931732.865723
Epoch 11: training loss 131445950132.706
Test Loss of 125384258528.022202, Test MSE of 125384260638.008423
Epoch 12: training loss 127805540713.412
Test Loss of 121975205254.129074, Test MSE of 121975204070.203232
Epoch 13: training loss 124975909044.706
Test Loss of 118521961635.678925, Test MSE of 118521962258.495773
Epoch 14: training loss 120687024007.529
Test Loss of 114915183600.603287, Test MSE of 114915184429.210999
Epoch 15: training loss 117259637037.176
Test Loss of 111961350342.972931, Test MSE of 111961351194.844467
Epoch 16: training loss 112368218714.353
Test Loss of 108041571791.085815, Test MSE of 108041570090.361618
Epoch 17: training loss 108674791664.941
Test Loss of 104710526573.790421, Test MSE of 104710526457.005646
Epoch 18: training loss 104875153016.471
Test Loss of 100731827819.895447, Test MSE of 100731829208.058197
Epoch 19: training loss 101088712704.000
Test Loss of 96449507695.626190, Test MSE of 96449508418.488419
Epoch 20: training loss 96726492912.941
Test Loss of 93243102025.371277, Test MSE of 93243100279.086258
Epoch 21: training loss 93538603670.588
Test Loss of 89514132376.960449, Test MSE of 89514132305.954758
Epoch 22: training loss 89510112045.176
Test Loss of 88497683857.498962, Test MSE of 88497682920.958191
Epoch 23: training loss 87265029240.471
Test Loss of 80870875750.921112, Test MSE of 80870876659.218170
Epoch 24: training loss 81202394518.588
Test Loss of 79547584219.225540, Test MSE of 79547584987.702774
Epoch 25: training loss 78855958558.118
Test Loss of 76884538654.378906, Test MSE of 76884539852.980804
Epoch 26: training loss 75336727175.529
Test Loss of 73536129091.745544, Test MSE of 73536128903.311600
Epoch 27: training loss 72210960128.000
Test Loss of 67105272225.843163, Test MSE of 67105269856.123268
Epoch 28: training loss 68161317918.118
Test Loss of 67224862009.382370, Test MSE of 67224860776.103645
Epoch 29: training loss 64830906684.235
Test Loss of 61497030033.025215, Test MSE of 61497030985.521683
Epoch 30: training loss 62378652822.588
Test Loss of 56589707481.448997, Test MSE of 56589708223.314560
Epoch 31: training loss 58679028043.294
Test Loss of 55321273074.202171, Test MSE of 55321272951.438156
Epoch 32: training loss 56481813127.529
Test Loss of 55697661662.068008, Test MSE of 55697662786.483086
Epoch 33: training loss 52631466488.471
Test Loss of 53472548425.312050, Test MSE of 53472549570.088608
Epoch 34: training loss 49994639043.765
Test Loss of 50393650325.940323, Test MSE of 50393649530.661720
Epoch 35: training loss 48045758110.118
Test Loss of 48256107885.020584, Test MSE of 48256108771.188606
Epoch 36: training loss 45600085014.588
Test Loss of 45484217622.325233, Test MSE of 45484216726.176414
Epoch 37: training loss 42825878264.471
Test Loss of 42155897353.593338, Test MSE of 42155897604.347908
Epoch 38: training loss 40941115776.000
Test Loss of 41414309898.422394, Test MSE of 41414310262.313080
Epoch 39: training loss 37902303058.824
Test Loss of 40504060869.492485, Test MSE of 40504063441.694702
Epoch 40: training loss 36806071152.941
Test Loss of 39232625342.090218, Test MSE of 39232625551.638161
Epoch 41: training loss 34150392719.059
Test Loss of 37426375383.198708, Test MSE of 37426375803.297958
Epoch 42: training loss 33002355817.412
Test Loss of 34627610887.876015, Test MSE of 34627610263.856400
Epoch 43: training loss 31122424749.176
Test Loss of 33457437058.575989, Test MSE of 33457436509.061604
Epoch 44: training loss 29752082123.294
Test Loss of 32949494050.405735, Test MSE of 32949493597.160831
Epoch 45: training loss 28423687604.706
Test Loss of 31241548147.416145, Test MSE of 31241547786.608238
Epoch 46: training loss 26857694320.941
Test Loss of 31935748319.133934, Test MSE of 31935748545.752644
Epoch 47: training loss 25503390471.529
Test Loss of 31325850625.184364, Test MSE of 31325850637.464531
Epoch 48: training loss 24399206290.824
Test Loss of 28221696581.995834, Test MSE of 28221697192.289673
Epoch 49: training loss 23264939369.412
Test Loss of 28732725554.276196, Test MSE of 28732726278.207096
Epoch 50: training loss 22196918144.000
Test Loss of 26419911127.139488, Test MSE of 26419911586.930824
Epoch 51: training loss 20945616097.882
Test Loss of 26998381910.754570, Test MSE of 26998381914.180763
Epoch 52: training loss 20495406543.059
Test Loss of 27555248747.184826, Test MSE of 27555248688.400906
Epoch 53: training loss 19634416820.706
Test Loss of 27733210575.559566, Test MSE of 27733211740.984871
Epoch 54: training loss 19126381658.353
Test Loss of 24897985256.016655, Test MSE of 24897985093.991138
Epoch 55: training loss 17993098352.941
Test Loss of 27390006556.957668, Test MSE of 27390005302.209915
Epoch 56: training loss 17673050590.118
Test Loss of 26309549482.133705, Test MSE of 26309549936.001488
Epoch 57: training loss 16656877552.941
Test Loss of 24115606961.003006, Test MSE of 24115606830.550327
Epoch 58: training loss 16454733568.000
Test Loss of 23774684014.797131, Test MSE of 23774683676.044891
Epoch 59: training loss 15946536786.824
Test Loss of 23797592616.386768, Test MSE of 23797592869.315041
Epoch 60: training loss 15182691764.706
Test Loss of 25638258618.833218, Test MSE of 25638258679.775448
Epoch 61: training loss 15030044905.412
Test Loss of 24509844337.639603, Test MSE of 24509844496.419609
Epoch 62: training loss 14368589782.588
Test Loss of 24185716205.168633, Test MSE of 24185716678.524887
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22188702618.794098, 'MSE - std': 1997014059.7307892, 'R2 - mean': 0.8350475854539385, 'R2 - std': 0.007717544955091249} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005543 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421928035629.176
Test Loss of 447258248187.499451, Test MSE of 447258256817.571045
Epoch 2: training loss 421907474552.471
Test Loss of 447239748661.770081, Test MSE of 447239753921.810669
Epoch 3: training loss 421880073396.706
Test Loss of 447215075987.453186, Test MSE of 447215077053.586121
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898913430.588
Test Loss of 447223712534.680542, Test MSE of 447223728065.542908
Epoch 2: training loss 421887612084.706
Test Loss of 447224425259.525330, Test MSE of 447224432553.720886
Epoch 3: training loss 421887173571.765
Test Loss of 447224698424.730957, Test MSE of 447224697788.852783
Epoch 4: training loss 416516594868.706
Test Loss of 429742660170.970154, Test MSE of 429742668868.320496
Epoch 5: training loss 380161048937.412
Test Loss of 374818702403.508667, Test MSE of 374818700380.033630
Epoch 6: training loss 312609866209.882
Test Loss of 295706571402.925720, Test MSE of 295706569523.318848
Epoch 7: training loss 221095212815.059
Test Loss of 196064257135.093231, Test MSE of 196064255511.414459
Epoch 8: training loss 155685664707.765
Test Loss of 154276164185.656250, Test MSE of 154276162580.046387
Epoch 9: training loss 137399332412.235
Test Loss of 142655147028.607910, Test MSE of 142655146894.642456
Epoch 10: training loss 131438000278.588
Test Loss of 137592690990.723114, Test MSE of 137592692767.056396
Epoch 11: training loss 129726139632.941
Test Loss of 133650916326.654633, Test MSE of 133650918894.293488
Epoch 12: training loss 125523219184.941
Test Loss of 131419019623.572525, Test MSE of 131419020068.334457
Epoch 13: training loss 122048315392.000
Test Loss of 127995879099.958359, Test MSE of 127995877077.092270
Epoch 14: training loss 118192928225.882
Test Loss of 124826363627.806610, Test MSE of 124826364057.508789
Epoch 15: training loss 115420585321.412
Test Loss of 121367265389.908859, Test MSE of 121367266526.486557
Epoch 16: training loss 111375351717.647
Test Loss of 116537597063.254227, Test MSE of 116537595254.288452
Epoch 17: training loss 107867193554.824
Test Loss of 113671029554.631500, Test MSE of 113671027096.240372
Epoch 18: training loss 103658968606.118
Test Loss of 110535783357.912567, Test MSE of 110535782314.896347
Epoch 19: training loss 99455988419.765
Test Loss of 105860540541.542450, Test MSE of 105860540947.368103
Epoch 20: training loss 96513528982.588
Test Loss of 103819076283.247742, Test MSE of 103819076337.295288
Epoch 21: training loss 92219890597.647
Test Loss of 100916742034.564880, Test MSE of 100916740992.021225
Epoch 22: training loss 88698968967.529
Test Loss of 98515633164.554245, Test MSE of 98515635495.453827
Epoch 23: training loss 84722543811.765
Test Loss of 88699841990.558411, Test MSE of 88699841410.145630
Epoch 24: training loss 81599780442.353
Test Loss of 88100373700.367340, Test MSE of 88100372824.516235
Epoch 25: training loss 78394335518.118
Test Loss of 84676409760.421936, Test MSE of 84676409204.795273
Epoch 26: training loss 74954921155.765
Test Loss of 85962480777.149200, Test MSE of 85962480827.712357
Epoch 27: training loss 71651351988.706
Test Loss of 76195913043.912094, Test MSE of 76195913102.389023
Epoch 28: training loss 68059291060.706
Test Loss of 69967544975.663193, Test MSE of 69967547059.193726
Epoch 29: training loss 64225233618.824
Test Loss of 71568689546.629654, Test MSE of 71568690290.392914
Epoch 30: training loss 61928762593.882
Test Loss of 66363177287.357857, Test MSE of 66363176369.683022
Epoch 31: training loss 59092792756.706
Test Loss of 66641481533.764519, Test MSE of 66641481534.139664
Epoch 32: training loss 56033142106.353
Test Loss of 60432886617.004860, Test MSE of 60432888089.235992
Epoch 33: training loss 53000126855.529
Test Loss of 62038628840.431183, Test MSE of 62038628484.148750
Epoch 34: training loss 51503498209.882
Test Loss of 54966325487.715012, Test MSE of 54966325371.178284
Epoch 35: training loss 47982189274.353
Test Loss of 54807075825.550774, Test MSE of 54807075651.328041
Epoch 36: training loss 45526859821.176
Test Loss of 50884452622.034698, Test MSE of 50884452603.971008
Epoch 37: training loss 43830887710.118
Test Loss of 49959751861.918114, Test MSE of 49959750873.006493
Epoch 38: training loss 41162269432.471
Test Loss of 48544398638.012489, Test MSE of 48544398921.884621
Epoch 39: training loss 39563569114.353
Test Loss of 45877237330.313210, Test MSE of 45877236995.660645
Epoch 40: training loss 38222554925.176
Test Loss of 41342622276.337730, Test MSE of 41342621756.153458
Epoch 41: training loss 35097775405.176
Test Loss of 37658519658.592644, Test MSE of 37658520162.828140
Epoch 42: training loss 33790515087.059
Test Loss of 40698521279.274574, Test MSE of 40698521456.083694
Epoch 43: training loss 31874997609.412
Test Loss of 37262324042.674072, Test MSE of 37262324243.968857
Epoch 44: training loss 30460994657.882
Test Loss of 34282055925.873699, Test MSE of 34282055670.881107
Epoch 45: training loss 29441865336.471
Test Loss of 37246589315.049736, Test MSE of 37246589905.523888
Epoch 46: training loss 28365437568.000
Test Loss of 29698046744.575527, Test MSE of 29698046542.188568
Epoch 47: training loss 26631248067.765
Test Loss of 31670334788.278511, Test MSE of 31670334438.820072
Epoch 48: training loss 25827140141.176
Test Loss of 29039139393.969002, Test MSE of 29039139159.418049
Epoch 49: training loss 24667329479.529
Test Loss of 30250997327.470737, Test MSE of 30250997568.724518
Epoch 50: training loss 23727313946.353
Test Loss of 26308399097.841312, Test MSE of 26308398941.127457
Epoch 51: training loss 22964452193.882
Test Loss of 22456422804.578300, Test MSE of 22456422769.309731
Epoch 52: training loss 22156554733.176
Test Loss of 28580685761.939392, Test MSE of 28580685823.602242
Epoch 53: training loss 20636000813.176
Test Loss of 26468874700.006477, Test MSE of 26468874794.665955
Epoch 54: training loss 20409496858.353
Test Loss of 26128804948.089752, Test MSE of 26128804513.119274
Epoch 55: training loss 19460810507.294
Test Loss of 24447450599.246819, Test MSE of 24447450695.384407
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22941618644.3242, 'MSE - std': 1947427838.5026383, 'R2 - mean': 0.8357833683393153, 'R2 - std': 0.006386685603508524} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005484 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110229925.647
Test Loss of 410765371417.588135, Test MSE of 410765377396.517212
Epoch 2: training loss 430088200914.824
Test Loss of 410746441384.929199, Test MSE of 410746438459.387695
Epoch 3: training loss 430060311250.824
Test Loss of 410722609435.365112, Test MSE of 410722606342.628784
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075938213.647
Test Loss of 410728315646.696899, Test MSE of 410728313845.372742
Epoch 2: training loss 430065732186.353
Test Loss of 410728186333.645508, Test MSE of 410728186319.294861
Epoch 3: training loss 430065257532.235
Test Loss of 410728248427.565002, Test MSE of 410728251095.335754
Epoch 4: training loss 424471408399.059
Test Loss of 393397711312.377625, Test MSE of 393397721118.993835
Epoch 5: training loss 387288280726.588
Test Loss of 338180632638.548828, Test MSE of 338180634022.902710
Epoch 6: training loss 318905599518.118
Test Loss of 261047485644.231384, Test MSE of 261047487708.061432
Epoch 7: training loss 227821553724.235
Test Loss of 162917980551.403992, Test MSE of 162917982022.359985
Epoch 8: training loss 163020371305.412
Test Loss of 124560873294.778351, Test MSE of 124560871567.319244
Epoch 9: training loss 143712367405.176
Test Loss of 114491576879.148544, Test MSE of 114491577720.197510
Epoch 10: training loss 139191583111.529
Test Loss of 110100852472.536789, Test MSE of 110100853575.824493
Epoch 11: training loss 135623891968.000
Test Loss of 107389645423.592789, Test MSE of 107389643323.294098
Epoch 12: training loss 132219882074.353
Test Loss of 104500845246.252655, Test MSE of 104500844952.579193
Epoch 13: training loss 127855443516.235
Test Loss of 101347124238.689499, Test MSE of 101347125776.258560
Epoch 14: training loss 123392653191.529
Test Loss of 98859477802.291534, Test MSE of 98859476642.860352
Epoch 15: training loss 120066131696.941
Test Loss of 94211919964.875519, Test MSE of 94211921212.748154
Epoch 16: training loss 116174144783.059
Test Loss of 92447649558.863495, Test MSE of 92447650651.026093
Epoch 17: training loss 112986092092.235
Test Loss of 89055531950.023132, Test MSE of 89055530151.908524
Epoch 18: training loss 109704253379.765
Test Loss of 86655148111.607590, Test MSE of 86655148840.498962
Epoch 19: training loss 104778387907.765
Test Loss of 82270175978.794998, Test MSE of 82270176284.568253
Epoch 20: training loss 99741604020.706
Test Loss of 79217607326.978256, Test MSE of 79217606494.664536
Epoch 21: training loss 97797741086.118
Test Loss of 75980517173.190186, Test MSE of 75980518683.339081
Epoch 22: training loss 92304427248.941
Test Loss of 73104086430.622864, Test MSE of 73104086632.884949
Epoch 23: training loss 88973010974.118
Test Loss of 68358911030.493294, Test MSE of 68358909911.705666
Epoch 24: training loss 85924869270.588
Test Loss of 67152906299.705688, Test MSE of 67152907920.012550
Epoch 25: training loss 81886159932.235
Test Loss of 65105966479.459511, Test MSE of 65105965887.889069
Epoch 26: training loss 78024020510.118
Test Loss of 63016160610.917168, Test MSE of 63016160672.459709
Epoch 27: training loss 74755513780.706
Test Loss of 60015137744.140678, Test MSE of 60015138123.946434
Epoch 28: training loss 70894430915.765
Test Loss of 55898306955.194817, Test MSE of 55898305779.986404
Epoch 29: training loss 68053349616.941
Test Loss of 52067337236.375755, Test MSE of 52067338311.537842
Epoch 30: training loss 64971892976.941
Test Loss of 52374102114.561775, Test MSE of 52374103220.515518
Epoch 31: training loss 61469327209.412
Test Loss of 48610438449.636276, Test MSE of 48610438675.859230
Epoch 32: training loss 58256445108.706
Test Loss of 48001160729.825081, Test MSE of 48001160248.148430
Epoch 33: training loss 56361842168.471
Test Loss of 43556638138.106430, Test MSE of 43556637649.313919
Epoch 34: training loss 53273558550.588
Test Loss of 42509822659.938919, Test MSE of 42509822769.389900
Epoch 35: training loss 50514863104.000
Test Loss of 39543367161.602959, Test MSE of 39543367265.293709
Epoch 36: training loss 47897448493.176
Test Loss of 39585899058.939377, Test MSE of 39585899515.818359
Epoch 37: training loss 45058990953.412
Test Loss of 34599369607.167053, Test MSE of 34599370221.217484
Epoch 38: training loss 43334843678.118
Test Loss of 33887778335.511337, Test MSE of 33887778581.277664
Epoch 39: training loss 41289396148.706
Test Loss of 32985962142.504395, Test MSE of 32985961652.920242
Epoch 40: training loss 39043975936.000
Test Loss of 30899815292.742249, Test MSE of 30899814830.324120
Epoch 41: training loss 37473922138.353
Test Loss of 30396418016.725590, Test MSE of 30396418397.724262
Epoch 42: training loss 35413910083.765
Test Loss of 30655008929.110596, Test MSE of 30655009289.301147
Epoch 43: training loss 33683638113.882
Test Loss of 31003683387.468765, Test MSE of 31003683326.265205
Epoch 44: training loss 32261248045.176
Test Loss of 27700786520.492363, Test MSE of 27700786682.453667
Epoch 45: training loss 30788407243.294
Test Loss of 24706326660.679314, Test MSE of 24706326842.204498
Epoch 46: training loss 29424891595.294
Test Loss of 24496073805.238316, Test MSE of 24496074396.411781
Epoch 47: training loss 27634047205.647
Test Loss of 24499143960.048126, Test MSE of 24499143652.151287
Epoch 48: training loss 26725764269.176
Test Loss of 22670419251.531700, Test MSE of 22670419693.792809
Epoch 49: training loss 26104953592.471
Test Loss of 21842326039.455807, Test MSE of 21842326181.744701
Epoch 50: training loss 24952273863.529
Test Loss of 21456974796.349838, Test MSE of 21456974580.117832
Epoch 51: training loss 23931847232.000
Test Loss of 23163534859.609440, Test MSE of 23163535562.248653
Epoch 52: training loss 22978977761.882
Test Loss of 20518823561.654789, Test MSE of 20518823503.184311
Epoch 53: training loss 21731368869.647
Test Loss of 20873228531.561314, Test MSE of 20873228208.594276
Epoch 54: training loss 21409561249.882
Test Loss of 20819039683.583527, Test MSE of 20819039772.393208
Epoch 55: training loss 20352245921.882
Test Loss of 19667261838.985653, Test MSE of 19667262051.417717
Epoch 56: training loss 19681354533.647
Test Loss of 20731555378.465527, Test MSE of 20731555307.582291
Epoch 57: training loss 19056958987.294
Test Loss of 20441691102.356316, Test MSE of 20441691402.494518
Epoch 58: training loss 18618900363.294
Test Loss of 19798671567.548359, Test MSE of 19798671536.126808
Epoch 59: training loss 17963242511.059
Test Loss of 17587511861.782509, Test MSE of 17587511920.951889
Epoch 60: training loss 17215236963.765
Test Loss of 19084758407.877834, Test MSE of 19084758406.005535
Epoch 61: training loss 16932456914.824
Test Loss of 18445956602.550671, Test MSE of 18445956887.116619
Epoch 62: training loss 16596124585.412
Test Loss of 20650747201.747337, Test MSE of 20650747205.302338
Epoch 63: training loss 15854562838.588
Test Loss of 17300353986.872746, Test MSE of 17300353959.457466
Epoch 64: training loss 15593268995.765
Test Loss of 19416599837.734383, Test MSE of 19416599805.089832
Epoch 65: training loss 15204873135.059
Test Loss of 17856784879.652012, Test MSE of 17856784920.133694
Epoch 66: training loss 14469899672.471
Test Loss of 18925926262.582138, Test MSE of 18925926739.907772
Epoch 67: training loss 14426247224.471
Test Loss of 18583784639.911152, Test MSE of 18583784622.593178
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21852160138.89144, 'MSE - std': 2530833043.6460276, 'R2 - mean': 0.8384921113466166, 'R2 - std': 0.0072528739674255025} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005452 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042990531.765
Test Loss of 431612069477.641846, Test MSE of 431612069128.207886
Epoch 2: training loss 424021522311.529
Test Loss of 431589934109.852844, Test MSE of 431589930253.926392
Epoch 3: training loss 423992822723.765
Test Loss of 431561150795.698303, Test MSE of 431561145413.045593
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424005063017.412
Test Loss of 431562757246.519226, Test MSE of 431562749402.196472
Epoch 2: training loss 423995571019.294
Test Loss of 431567088900.146240, Test MSE of 431567079197.931702
Epoch 3: training loss 423995050827.294
Test Loss of 431566979026.509949, Test MSE of 431566969622.632446
Epoch 4: training loss 418505900393.412
Test Loss of 413968766396.949585, Test MSE of 413968766574.410522
Epoch 5: training loss 381807355422.118
Test Loss of 358166035630.852356, Test MSE of 358166040996.278809
Epoch 6: training loss 314048278287.059
Test Loss of 279394169538.991211, Test MSE of 279394171586.713074
Epoch 7: training loss 223185162571.294
Test Loss of 179198782731.727905, Test MSE of 179198784285.361694
Epoch 8: training loss 159375942776.471
Test Loss of 137523331072.000000, Test MSE of 137523331422.585724
Epoch 9: training loss 141043409317.647
Test Loss of 126677447366.782043, Test MSE of 126677448926.123703
Epoch 10: training loss 134737820250.353
Test Loss of 121463103764.731140, Test MSE of 121463101797.075912
Epoch 11: training loss 131110692171.294
Test Loss of 117876193895.063400, Test MSE of 117876196523.767319
Epoch 12: training loss 128626217923.765
Test Loss of 115375854737.473389, Test MSE of 115375855839.287857
Epoch 13: training loss 124213153671.529
Test Loss of 110634084635.365112, Test MSE of 110634081248.327255
Epoch 14: training loss 120325364043.294
Test Loss of 107300668169.121704, Test MSE of 107300667349.946930
Epoch 15: training loss 118113448598.588
Test Loss of 105583341994.469223, Test MSE of 105583344033.841660
Epoch 16: training loss 113131868190.118
Test Loss of 100363290331.157791, Test MSE of 100363290657.031357
Epoch 17: training loss 108838446019.765
Test Loss of 95336627048.840347, Test MSE of 95336627830.034973
Epoch 18: training loss 104370830486.588
Test Loss of 93101464258.043503, Test MSE of 93101465792.222443
Epoch 19: training loss 101380542644.706
Test Loss of 89366097921.895416, Test MSE of 89366097851.194824
Epoch 20: training loss 96981510324.706
Test Loss of 82478385928.173996, Test MSE of 82478386778.902908
Epoch 21: training loss 93902741639.529
Test Loss of 84184607473.428970, Test MSE of 84184607302.008331
Epoch 22: training loss 89128842857.412
Test Loss of 80780264413.408600, Test MSE of 80780263894.473923
Epoch 23: training loss 85885514337.882
Test Loss of 75221582906.284134, Test MSE of 75221582948.728226
Epoch 24: training loss 82631080432.941
Test Loss of 71231075985.236465, Test MSE of 71231075692.777512
Epoch 25: training loss 78700191156.706
Test Loss of 68497571889.280891, Test MSE of 68497572578.949760
Epoch 26: training loss 75171114285.176
Test Loss of 64022537892.190651, Test MSE of 64022538763.718849
Epoch 27: training loss 71795882450.824
Test Loss of 60118997400.462746, Test MSE of 60118998022.979187
Epoch 28: training loss 68095597658.353
Test Loss of 56815562149.256828, Test MSE of 56815562135.341537
Epoch 29: training loss 65098292404.706
Test Loss of 56987552160.044426, Test MSE of 56987552060.444977
Epoch 30: training loss 62576104990.118
Test Loss of 53645411793.799164, Test MSE of 53645413104.225922
Epoch 31: training loss 59385844826.353
Test Loss of 51566648426.617310, Test MSE of 51566647727.779411
Epoch 32: training loss 55333206558.118
Test Loss of 48618172808.351692, Test MSE of 48618173313.943779
Epoch 33: training loss 53767276092.235
Test Loss of 44441611266.369270, Test MSE of 44441611698.989876
Epoch 34: training loss 50765943047.529
Test Loss of 44718859809.406754, Test MSE of 44718860635.851166
Epoch 35: training loss 48408888794.353
Test Loss of 42535526151.700142, Test MSE of 42535526849.062531
Epoch 36: training loss 46442026262.588
Test Loss of 42529305845.930588, Test MSE of 42529306846.392746
Epoch 37: training loss 43467744203.294
Test Loss of 34675364803.820450, Test MSE of 34675364434.412628
Epoch 38: training loss 41410840937.412
Test Loss of 35525699952.658951, Test MSE of 35525699864.748032
Epoch 39: training loss 39066544218.353
Test Loss of 33722416723.635353, Test MSE of 33722416761.173149
Epoch 40: training loss 37345050526.118
Test Loss of 30794137784.803333, Test MSE of 30794137699.454830
Epoch 41: training loss 35321683840.000
Test Loss of 31392888313.602962, Test MSE of 31392888704.175396
Epoch 42: training loss 33610317748.706
Test Loss of 28513412077.519669, Test MSE of 28513411722.877548
Epoch 43: training loss 32399167691.294
Test Loss of 28880467142.545116, Test MSE of 28880467302.079403
Epoch 44: training loss 30519617426.824
Test Loss of 24799656642.043499, Test MSE of 24799656455.505814
Epoch 45: training loss 29323399928.471
Test Loss of 26786371101.615917, Test MSE of 26786371184.388092
Epoch 46: training loss 28189578488.471
Test Loss of 24225010305.599258, Test MSE of 24225010389.557201
Epoch 47: training loss 26662720041.412
Test Loss of 23869581660.757057, Test MSE of 23869581162.255112
Epoch 48: training loss 25668386770.824
Test Loss of 26415353274.106434, Test MSE of 26415353536.032589
Epoch 49: training loss 24869225566.118
Test Loss of 24852518099.813049, Test MSE of 24852518199.479725
Epoch 50: training loss 23773373387.294
Test Loss of 23307410078.030540, Test MSE of 23307410045.741707
Epoch 51: training loss 23056977167.059
Test Loss of 21931905622.952335, Test MSE of 21931905732.715988
Epoch 52: training loss 22301629364.706
Test Loss of 24641077533.260529, Test MSE of 24641077250.445107
Epoch 53: training loss 21418618970.353
Test Loss of 21822055406.467377, Test MSE of 21822055088.369873
Epoch 54: training loss 20691016828.235
Test Loss of 20816120496.037022, Test MSE of 20816120746.116489
Epoch 55: training loss 20074988709.647
Test Loss of 21321118937.499306, Test MSE of 21321118715.426247
Epoch 56: training loss 19145472877.176
Test Loss of 21097449719.352150, Test MSE of 21097449534.614109
Epoch 57: training loss 18671388144.941
Test Loss of 21251570818.310043, Test MSE of 21251571051.614590
Epoch 58: training loss 18274567819.294
Test Loss of 20267899437.253124, Test MSE of 20267899474.195988
Epoch 59: training loss 17584838034.824
Test Loss of 20280411459.642757, Test MSE of 20280411484.616505
Epoch 60: training loss 17328277665.882
Test Loss of 19005395892.657104, Test MSE of 19005395939.432213
Epoch 61: training loss 16505707764.706
Test Loss of 21534566277.745487, Test MSE of 21534566029.969936
Epoch 62: training loss 16137838362.353
Test Loss of 21453380621.741787, Test MSE of 21453380811.504330
Epoch 63: training loss 15708256135.529
Test Loss of 19480036454.826469, Test MSE of 19480036487.170269
Epoch 64: training loss 15545806000.941
Test Loss of 19754756808.677464, Test MSE of 19754756999.196701
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21432679510.95249, 'MSE - std': 2414114476.306522, 'R2 - mean': 0.8412878373353807, 'R2 - std': 0.008564326000736582} 
 

Saving model.....
Results After CV: {'MSE - mean': 21432679510.95249, 'MSE - std': 2414114476.306522, 'R2 - mean': 0.8412878373353807, 'R2 - std': 0.008564326000736582}
Train time: 98.14335135540023
Inference time: 0.07502354579992243
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 72 finished with value: 21432679510.95249 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002611 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525123132.235
Test Loss of 418111426253.960693, Test MSE of 418111431121.023804
Epoch 2: training loss 427504890096.941
Test Loss of 418093806764.680054, Test MSE of 418093805506.459167
Epoch 3: training loss 427478019734.588
Test Loss of 418070499886.071716, Test MSE of 418070500880.067200
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427489042432.000
Test Loss of 418077290411.673401, Test MSE of 418077295007.091309
Epoch 2: training loss 427482763023.059
Test Loss of 418078647263.074707, Test MSE of 418078649225.863220
Epoch 3: training loss 427482363422.118
Test Loss of 418078224250.403870, Test MSE of 418078229747.269714
Epoch 4: training loss 422057975687.529
Test Loss of 401042063598.767517, Test MSE of 401042072144.619507
Epoch 5: training loss 385654367171.765
Test Loss of 346491657514.222534, Test MSE of 346491662763.068848
Epoch 6: training loss 317217245545.412
Test Loss of 268262247383.968536, Test MSE of 268262248266.332977
Epoch 7: training loss 225551115294.118
Test Loss of 170880628017.328705, Test MSE of 170880628573.647461
Epoch 8: training loss 161900816173.176
Test Loss of 130890050086.018036, Test MSE of 130890049673.420776
Epoch 9: training loss 141739701157.647
Test Loss of 120144576682.548233, Test MSE of 120144575463.066147
Epoch 10: training loss 135470267301.647
Test Loss of 115907375796.615311, Test MSE of 115907374643.088715
Epoch 11: training loss 132786745223.529
Test Loss of 112696081611.473511, Test MSE of 112696083914.352692
Epoch 12: training loss 128625792165.647
Test Loss of 110027801705.408279, Test MSE of 110027799942.301620
Epoch 13: training loss 126697727352.471
Test Loss of 106861459897.056671, Test MSE of 106861456969.211945
Epoch 14: training loss 121893020205.176
Test Loss of 103109410654.926666, Test MSE of 103109410119.341721
Epoch 15: training loss 118672986081.882
Test Loss of 99438083519.925980, Test MSE of 99438083327.139999
Epoch 16: training loss 115165411297.882
Test Loss of 97295704450.575989, Test MSE of 97295703400.345993
Epoch 17: training loss 111089199902.118
Test Loss of 92400642935.324539, Test MSE of 92400642796.911316
Epoch 18: training loss 107838220679.529
Test Loss of 90498214913.658112, Test MSE of 90498214417.654572
Epoch 19: training loss 103201276129.882
Test Loss of 86876954942.356689, Test MSE of 86876954584.365738
Epoch 20: training loss 99803086464.000
Test Loss of 83831029708.835526, Test MSE of 83831030578.996124
Epoch 21: training loss 95657996589.176
Test Loss of 80619650302.637985, Test MSE of 80619649866.355316
Epoch 22: training loss 92044733018.353
Test Loss of 77876318028.924362, Test MSE of 77876318576.226288
Epoch 23: training loss 87317468988.235
Test Loss of 74346599047.135788, Test MSE of 74346600679.154144
Epoch 24: training loss 84808280591.059
Test Loss of 71840219281.913483, Test MSE of 71840219281.092468
Epoch 25: training loss 81620628528.941
Test Loss of 68728085550.190140, Test MSE of 68728085622.600281
Epoch 26: training loss 77363956856.471
Test Loss of 66447833648.440437, Test MSE of 66447835110.881523
Epoch 27: training loss 73967763832.471
Test Loss of 62146166454.273422, Test MSE of 62146165680.187958
Epoch 28: training loss 71235206648.471
Test Loss of 58454097482.259544, Test MSE of 58454097274.972847
Epoch 29: training loss 67870494200.471
Test Loss of 58274089011.638214, Test MSE of 58274089176.150673
Epoch 30: training loss 63973064357.647
Test Loss of 53875652196.315521, Test MSE of 53875651185.060440
Epoch 31: training loss 60818860675.765
Test Loss of 51157888247.058060, Test MSE of 51157888782.064003
Epoch 32: training loss 58464397654.588
Test Loss of 51382805458.757347, Test MSE of 51382805424.508026
Epoch 33: training loss 55615086832.941
Test Loss of 45397942422.650940, Test MSE of 45397943097.446640
Epoch 34: training loss 52582817916.235
Test Loss of 44818203032.368263, Test MSE of 44818202647.460197
Epoch 35: training loss 49599161253.647
Test Loss of 44661484108.865143, Test MSE of 44661483674.417984
Epoch 36: training loss 47808567243.294
Test Loss of 40115719058.801758, Test MSE of 40115719023.242325
Epoch 37: training loss 45383177264.941
Test Loss of 39837523974.632431, Test MSE of 39837524064.060448
Epoch 38: training loss 43344587949.176
Test Loss of 36731643949.005783, Test MSE of 36731643246.037552
Epoch 39: training loss 41038550806.588
Test Loss of 32631942450.749943, Test MSE of 32631941792.300701
Epoch 40: training loss 39319679841.882
Test Loss of 31454971825.832062, Test MSE of 31454971711.336212
Epoch 41: training loss 37620061071.059
Test Loss of 30694483073.095535, Test MSE of 30694482804.292721
Epoch 42: training loss 35298648837.647
Test Loss of 29816173105.861671, Test MSE of 29816172602.234493
Epoch 43: training loss 33881140088.471
Test Loss of 27467521177.256535, Test MSE of 27467521219.441055
Epoch 44: training loss 32518041607.529
Test Loss of 28683918289.336109, Test MSE of 28683918405.091015
Epoch 45: training loss 30535936301.176
Test Loss of 23547630101.910709, Test MSE of 23547629732.700016
Epoch 46: training loss 29385699873.882
Test Loss of 30043553691.566044, Test MSE of 30043553631.097431
Epoch 47: training loss 28160898115.765
Test Loss of 23728056274.520473, Test MSE of 23728056331.350037
Epoch 48: training loss 27191327292.235
Test Loss of 24676647771.847328, Test MSE of 24676648207.340664
Epoch 49: training loss 25960444920.471
Test Loss of 24705371486.808235, Test MSE of 24705371043.921474
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24705371043.921474, 'MSE - std': 0.0, 'R2 - mean': 0.807616595168615, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918106864.941
Test Loss of 424555862098.905396, Test MSE of 424555864558.085571
Epoch 2: training loss 427896291568.941
Test Loss of 424537940102.543579, Test MSE of 424537938489.944397
Epoch 3: training loss 427867746785.882
Test Loss of 424514762150.817505, Test MSE of 424514761261.910339
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884253424.941
Test Loss of 424520121132.472839, Test MSE of 424520125900.585999
Epoch 2: training loss 427874134377.412
Test Loss of 424520555765.163086, Test MSE of 424520548284.325195
Epoch 3: training loss 427873607920.941
Test Loss of 424519609639.380066, Test MSE of 424519607777.916382
Epoch 4: training loss 422161539553.882
Test Loss of 407061777205.000244, Test MSE of 407061784464.779297
Epoch 5: training loss 384644477289.412
Test Loss of 352170097971.697449, Test MSE of 352170097868.527649
Epoch 6: training loss 315361396013.176
Test Loss of 275317598235.003479, Test MSE of 275317600252.858582
Epoch 7: training loss 224508694347.294
Test Loss of 179345948990.356689, Test MSE of 179345946404.223633
Epoch 8: training loss 157908196472.471
Test Loss of 141584142090.600037, Test MSE of 141584144087.323029
Epoch 9: training loss 138900182558.118
Test Loss of 131872557101.953278, Test MSE of 131872556254.739700
Epoch 10: training loss 133931024323.765
Test Loss of 127113450676.260010, Test MSE of 127113449961.154404
Epoch 11: training loss 129363393837.176
Test Loss of 124193802583.465179, Test MSE of 124193804346.695877
Epoch 12: training loss 127250765733.647
Test Loss of 121870561218.413132, Test MSE of 121870560349.160645
Epoch 13: training loss 123507404167.529
Test Loss of 117417473399.442978, Test MSE of 117417472999.287308
Epoch 14: training loss 119511641600.000
Test Loss of 113175978834.846176, Test MSE of 113175979637.591629
Epoch 15: training loss 115448121344.000
Test Loss of 110867860419.123764, Test MSE of 110867860905.491364
Epoch 16: training loss 111439349850.353
Test Loss of 106959534022.676849, Test MSE of 106959536281.254974
Epoch 17: training loss 107752948976.941
Test Loss of 103653236131.501266, Test MSE of 103653238321.613510
Epoch 18: training loss 103617558678.588
Test Loss of 100017824522.836914, Test MSE of 100017826660.162186
Epoch 19: training loss 99432713878.588
Test Loss of 95534498091.406891, Test MSE of 95534499718.032074
Epoch 20: training loss 95130898311.529
Test Loss of 93536822258.024521, Test MSE of 93536822550.416718
Epoch 21: training loss 89958472764.235
Test Loss of 89058554437.758957, Test MSE of 89058554791.040054
Epoch 22: training loss 87223294313.412
Test Loss of 83871426334.023590, Test MSE of 83871424720.247971
Epoch 23: training loss 83848199273.412
Test Loss of 80619521227.947266, Test MSE of 80619520287.720306
Epoch 24: training loss 80269379779.765
Test Loss of 76100139153.439743, Test MSE of 76100140201.455978
Epoch 25: training loss 76227895958.588
Test Loss of 74239357286.151291, Test MSE of 74239356386.077591
Epoch 26: training loss 72410618970.353
Test Loss of 70605920380.594955, Test MSE of 70605919036.676071
Epoch 27: training loss 69984118362.353
Test Loss of 69457885674.326157, Test MSE of 69457885897.986267
Epoch 28: training loss 65692835267.765
Test Loss of 65644634242.753647, Test MSE of 65644635920.958000
Epoch 29: training loss 62492409539.765
Test Loss of 62027942289.262085, Test MSE of 62027942507.267075
Epoch 30: training loss 59181377280.000
Test Loss of 58578766830.708305, Test MSE of 58578765522.859329
Epoch 31: training loss 55880882507.294
Test Loss of 53017203400.986351, Test MSE of 53017202020.301888
Epoch 32: training loss 52953119518.118
Test Loss of 53738848553.511909, Test MSE of 53738848617.603989
Epoch 33: training loss 50554446825.412
Test Loss of 51759075159.346748, Test MSE of 51759076230.351334
Epoch 34: training loss 47563007149.176
Test Loss of 47065955352.871620, Test MSE of 47065956736.249413
Epoch 35: training loss 44945543785.412
Test Loss of 46821145171.023827, Test MSE of 46821144653.272926
Epoch 36: training loss 43109825264.941
Test Loss of 43496286429.712700, Test MSE of 43496285933.317482
Epoch 37: training loss 40089229424.941
Test Loss of 43126390786.842468, Test MSE of 43126391565.755348
Epoch 38: training loss 38194418650.353
Test Loss of 39206722674.409439, Test MSE of 39206722546.316444
Epoch 39: training loss 36622122661.647
Test Loss of 39119387438.367798, Test MSE of 39119388436.702721
Epoch 40: training loss 34510715053.176
Test Loss of 36846751992.716171, Test MSE of 36846751664.936745
Epoch 41: training loss 31906699504.941
Test Loss of 35811206100.652328, Test MSE of 35811204726.467064
Epoch 42: training loss 30943921611.294
Test Loss of 34555401752.990051, Test MSE of 34555401664.751305
Epoch 43: training loss 28670341059.765
Test Loss of 34333595819.969467, Test MSE of 34333595392.709766
Epoch 44: training loss 27858692171.294
Test Loss of 34009867890.054131, Test MSE of 34009867249.042439
Epoch 45: training loss 26278847804.235
Test Loss of 33048893984.333103, Test MSE of 33048893910.058083
Epoch 46: training loss 25142760696.471
Test Loss of 28967481432.590332, Test MSE of 28967481695.335110
Epoch 47: training loss 23565761584.941
Test Loss of 28919239391.726116, Test MSE of 28919239857.475094
Epoch 48: training loss 22529816252.235
Test Loss of 29553050573.546150, Test MSE of 29553051992.266426
Epoch 49: training loss 21734141138.824
Test Loss of 32431468809.297249, Test MSE of 32431469742.739929
Epoch 50: training loss 20806481355.294
Test Loss of 31258105998.597271, Test MSE of 31258106737.479176
Epoch 51: training loss 19869409957.647
Test Loss of 28972629093.618320, Test MSE of 28972630296.658012
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26839000670.28974, 'MSE - std': 2133629626.368269, 'R2 - mean': 0.8003856253008801, 'R2 - std': 0.0072309698677349155} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005453 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927153302.588
Test Loss of 447258220369.188049, Test MSE of 447258217029.427429
Epoch 2: training loss 421906743536.941
Test Loss of 447239854295.554016, Test MSE of 447239851520.493774
Epoch 3: training loss 421879690420.706
Test Loss of 447215857780.541321, Test MSE of 447215859324.740906
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897773417.412
Test Loss of 447223230618.440918, Test MSE of 447223228406.053406
Epoch 2: training loss 421888338040.471
Test Loss of 447224370714.411316, Test MSE of 447224374868.344482
Epoch 3: training loss 421887829052.235
Test Loss of 447223532693.466553, Test MSE of 447223535247.483704
Epoch 4: training loss 416779823947.294
Test Loss of 430702849831.024780, Test MSE of 430702842550.957520
Epoch 5: training loss 380957105212.235
Test Loss of 375061603401.193604, Test MSE of 375061597316.911011
Epoch 6: training loss 312911714544.941
Test Loss of 296911640351.918579, Test MSE of 296911640971.588928
Epoch 7: training loss 221730256052.706
Test Loss of 196478075990.458466, Test MSE of 196478076537.595764
Epoch 8: training loss 156408007348.706
Test Loss of 154105258065.484161, Test MSE of 154105257302.402435
Epoch 9: training loss 137971467203.765
Test Loss of 142746042326.310425, Test MSE of 142746041448.005096
Epoch 10: training loss 132449441310.118
Test Loss of 138038195312.040710, Test MSE of 138038194896.196503
Epoch 11: training loss 128525818699.294
Test Loss of 133823417554.105942, Test MSE of 133823416708.197464
Epoch 12: training loss 126052235023.059
Test Loss of 131348906605.553558, Test MSE of 131348908140.461945
Epoch 13: training loss 123039951299.765
Test Loss of 126601906652.113815, Test MSE of 126601905386.473083
Epoch 14: training loss 118535186221.176
Test Loss of 124218434705.202866, Test MSE of 124218434866.116913
Epoch 15: training loss 114274423657.412
Test Loss of 121137181178.907242, Test MSE of 121137182372.663910
Epoch 16: training loss 111249293974.588
Test Loss of 116545242827.591949, Test MSE of 116545244108.175919
Epoch 17: training loss 107410678332.235
Test Loss of 113851396799.985199, Test MSE of 113851398477.916260
Epoch 18: training loss 104059111032.471
Test Loss of 111170857388.739304, Test MSE of 111170857667.724503
Epoch 19: training loss 99951262388.706
Test Loss of 105525196596.763351, Test MSE of 105525196002.242569
Epoch 20: training loss 96448302260.706
Test Loss of 101534757886.341888, Test MSE of 101534757848.468506
Epoch 21: training loss 92189047958.588
Test Loss of 97606830478.419617, Test MSE of 97606831642.567841
Epoch 22: training loss 88808884871.529
Test Loss of 95712422904.183212, Test MSE of 95712424120.657898
Epoch 23: training loss 85012556393.412
Test Loss of 92848670726.158691, Test MSE of 92848672349.482681
Epoch 24: training loss 81703944146.824
Test Loss of 82840455143.839005, Test MSE of 82840456889.794815
Epoch 25: training loss 77163945532.235
Test Loss of 81141758385.476746, Test MSE of 81141757089.367950
Epoch 26: training loss 73736495164.235
Test Loss of 77648792537.863525, Test MSE of 77648792505.485077
Epoch 27: training loss 70865857355.294
Test Loss of 73536040149.895905, Test MSE of 73536042820.212006
Epoch 28: training loss 67447980905.412
Test Loss of 72800199656.549622, Test MSE of 72800201693.181778
Epoch 29: training loss 64027776286.118
Test Loss of 68122902232.383064, Test MSE of 68122901115.808853
Epoch 30: training loss 61355955456.000
Test Loss of 65771621745.284294, Test MSE of 65771622982.918587
Epoch 31: training loss 58089706044.235
Test Loss of 60505592873.926437, Test MSE of 60505592454.519890
Epoch 32: training loss 55753973481.412
Test Loss of 56453324623.766830, Test MSE of 56453325538.169426
Epoch 33: training loss 53254689355.294
Test Loss of 56992839308.110107, Test MSE of 56992838812.812363
Epoch 34: training loss 50613575559.529
Test Loss of 55744873111.243118, Test MSE of 55744873223.297493
Epoch 35: training loss 47628387395.765
Test Loss of 48552592850.402039, Test MSE of 48552593400.668335
Epoch 36: training loss 45492061018.353
Test Loss of 50689682265.952347, Test MSE of 50689682036.554924
Epoch 37: training loss 42816038016.000
Test Loss of 46106598888.668053, Test MSE of 46106598882.258041
Epoch 38: training loss 40811712790.588
Test Loss of 48164508695.924126, Test MSE of 48164509338.996300
Epoch 39: training loss 38537195060.706
Test Loss of 41776018800.810547, Test MSE of 41776019545.727837
Epoch 40: training loss 36874076032.000
Test Loss of 39382981755.884338, Test MSE of 39382982945.564400
Epoch 41: training loss 34591168368.941
Test Loss of 41675722712.205414, Test MSE of 41675723597.814804
Epoch 42: training loss 33390202435.765
Test Loss of 35793257071.448532, Test MSE of 35793257775.996315
Epoch 43: training loss 31904457193.412
Test Loss of 34194897293.472126, Test MSE of 34194897214.662617
Epoch 44: training loss 30848029650.824
Test Loss of 31335086118.373352, Test MSE of 31335086190.023777
Epoch 45: training loss 29002296862.118
Test Loss of 32952580731.292160, Test MSE of 32952580562.762043
Epoch 46: training loss 27425811764.706
Test Loss of 31868719308.657875, Test MSE of 31868719836.358665
Epoch 47: training loss 26760076837.647
Test Loss of 33327045869.583160, Test MSE of 33327045852.999279
Epoch 48: training loss 25269671969.882
Test Loss of 29132847839.726116, Test MSE of 29132847906.182442
Epoch 49: training loss 24168930409.412
Test Loss of 29545490983.913025, Test MSE of 29545491306.790401
Epoch 50: training loss 23341249792.000
Test Loss of 28357123303.898220, Test MSE of 28357122886.661758
Epoch 51: training loss 22641044747.294
Test Loss of 29070591489.776543, Test MSE of 29070592015.817596
Epoch 52: training loss 21510086351.059
Test Loss of 25012729429.155678, Test MSE of 25012729166.878586
Epoch 53: training loss 20666744602.353
Test Loss of 28211765261.738609, Test MSE of 28211765376.946205
Epoch 54: training loss 20172268129.882
Test Loss of 26204713312.466343, Test MSE of 26204713375.194729
Epoch 55: training loss 19353669967.059
Test Loss of 25765523587.227390, Test MSE of 25765523834.344662
Epoch 56: training loss 18823001592.471
Test Loss of 24756263416.064770, Test MSE of 24756263404.862774
Epoch 57: training loss 18070018187.294
Test Loss of 25276985076.334026, Test MSE of 25276985227.966270
Epoch 58: training loss 17220620751.059
Test Loss of 25943880013.516541, Test MSE of 25943879589.790943
Epoch 59: training loss 16671540784.941
Test Loss of 24121352775.180199, Test MSE of 24121352907.392288
Epoch 60: training loss 16248888022.588
Test Loss of 21640452937.371269, Test MSE of 21640452783.657219
Epoch 61: training loss 15923140058.353
Test Loss of 22265918621.993984, Test MSE of 22265918917.820030
Epoch 62: training loss 15521988784.941
Test Loss of 24047223132.202637, Test MSE of 24047223300.276493
Epoch 63: training loss 15008052096.000
Test Loss of 23786608404.548695, Test MSE of 23786608711.603481
Epoch 64: training loss 14742559943.529
Test Loss of 22771937266.735138, Test MSE of 22771937423.316250
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25483312921.29858, 'MSE - std': 2590501006.988235, 'R2 - mean': 0.8163933219966258, 'R2 - std': 0.023395526434727552} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005526 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110376899.765
Test Loss of 410764614599.611267, Test MSE of 410764612743.035706
Epoch 2: training loss 430089598855.529
Test Loss of 410746634156.601562, Test MSE of 410746631143.037964
Epoch 3: training loss 430062125056.000
Test Loss of 410723198637.667725, Test MSE of 410723195837.826477
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079744843.294
Test Loss of 410726432404.079590, Test MSE of 410726429761.598022
Epoch 2: training loss 430066516931.765
Test Loss of 410727750372.161011, Test MSE of 410727751556.846375
Epoch 3: training loss 430066043241.412
Test Loss of 410727179032.285034, Test MSE of 410727178765.106873
Epoch 4: training loss 424766016572.235
Test Loss of 394142131491.420654, Test MSE of 394142129840.085754
Epoch 5: training loss 388361723783.529
Test Loss of 338862751114.720947, Test MSE of 338862749401.411804
Epoch 6: training loss 320464239555.765
Test Loss of 262371641540.175842, Test MSE of 262371640147.033539
Epoch 7: training loss 228515087962.353
Test Loss of 164964803966.400726, Test MSE of 164964806683.503601
Epoch 8: training loss 163775631239.529
Test Loss of 124762351110.870895, Test MSE of 124762351536.729767
Epoch 9: training loss 144644398441.412
Test Loss of 114478481112.314667, Test MSE of 114478479416.366440
Epoch 10: training loss 139370516389.647
Test Loss of 110044243451.498383, Test MSE of 110044242799.663208
Epoch 11: training loss 134746905780.706
Test Loss of 107386873475.968536, Test MSE of 107386874115.649521
Epoch 12: training loss 131802015954.824
Test Loss of 104539297610.987503, Test MSE of 104539296072.909729
Epoch 13: training loss 128199550192.941
Test Loss of 101632442652.312820, Test MSE of 101632441181.720642
Epoch 14: training loss 124827179158.588
Test Loss of 98549808606.119385, Test MSE of 98549808483.973831
Epoch 15: training loss 121582810081.882
Test Loss of 95731961410.576584, Test MSE of 95731961757.971313
Epoch 16: training loss 116379927130.353
Test Loss of 92982908720.451645, Test MSE of 92982910521.128326
Epoch 17: training loss 113022524355.765
Test Loss of 89592441483.076355, Test MSE of 89592442341.033829
Epoch 18: training loss 110009274488.471
Test Loss of 86498192534.211945, Test MSE of 86498195170.670776
Epoch 19: training loss 105479655966.118
Test Loss of 83498346764.201752, Test MSE of 83498348114.364548
Epoch 20: training loss 101231039066.353
Test Loss of 81647435036.786667, Test MSE of 81647435565.801804
Epoch 21: training loss 98411131241.412
Test Loss of 78598310235.809341, Test MSE of 78598308546.373230
Epoch 22: training loss 94471439902.118
Test Loss of 73149844387.598328, Test MSE of 73149845153.042267
Epoch 23: training loss 91011678343.529
Test Loss of 72641145467.913010, Test MSE of 72641146522.439667
Epoch 24: training loss 86606083177.412
Test Loss of 68007315993.351227, Test MSE of 68007315632.828163
Epoch 25: training loss 82909668291.765
Test Loss of 67074987631.118927, Test MSE of 67074987348.608139
Epoch 26: training loss 79289555486.118
Test Loss of 62253043622.915314, Test MSE of 62253043443.602676
Epoch 27: training loss 75518095691.294
Test Loss of 61952320473.617767, Test MSE of 61952320360.146431
Epoch 28: training loss 73043948724.706
Test Loss of 56139333707.816750, Test MSE of 56139333812.376549
Epoch 29: training loss 69613279668.706
Test Loss of 53091938017.791763, Test MSE of 53091938568.476662
Epoch 30: training loss 66561386375.529
Test Loss of 52691027233.525223, Test MSE of 52691027293.622581
Epoch 31: training loss 62504539075.765
Test Loss of 50996817847.974083, Test MSE of 50996819098.148659
Epoch 32: training loss 59687660890.353
Test Loss of 50069408940.956963, Test MSE of 50069409355.619286
Epoch 33: training loss 58071413172.706
Test Loss of 47032070360.077744, Test MSE of 47032070337.795479
Epoch 34: training loss 55001868950.588
Test Loss of 44218704533.501160, Test MSE of 44218704566.683510
Epoch 35: training loss 52523274721.882
Test Loss of 42111045034.943085, Test MSE of 42111045932.867050
Epoch 36: training loss 48659628303.059
Test Loss of 40192185279.081909, Test MSE of 40192185015.321251
Epoch 37: training loss 47232555226.353
Test Loss of 39162974062.052750, Test MSE of 39162974375.581528
Epoch 38: training loss 44907494000.941
Test Loss of 38062718219.727905, Test MSE of 38062717789.154663
Epoch 39: training loss 42548456146.824
Test Loss of 34148492560.466450, Test MSE of 34148492753.065739
Epoch 40: training loss 39930212133.647
Test Loss of 34714863973.286438, Test MSE of 34714864094.815712
Epoch 41: training loss 38725041054.118
Test Loss of 32589362658.857937, Test MSE of 32589362560.120647
Epoch 42: training loss 36632752067.765
Test Loss of 32638904341.797318, Test MSE of 32638904506.184029
Epoch 43: training loss 34929426085.647
Test Loss of 32631779043.687183, Test MSE of 32631778765.461143
Epoch 44: training loss 33614089976.471
Test Loss of 30502787630.200832, Test MSE of 30502787897.646812
Epoch 45: training loss 32069076314.353
Test Loss of 27493969167.518742, Test MSE of 27493969585.798172
Epoch 46: training loss 30979036604.235
Test Loss of 27361381014.448868, Test MSE of 27361380987.340298
Epoch 47: training loss 28971553227.294
Test Loss of 26324072062.282276, Test MSE of 26324071791.741291
Epoch 48: training loss 28049001438.118
Test Loss of 24852928102.589542, Test MSE of 24852928755.207928
Epoch 49: training loss 26672531365.647
Test Loss of 25114016513.540028, Test MSE of 25114016579.714897
Epoch 50: training loss 25275734618.353
Test Loss of 23833842785.140213, Test MSE of 23833842544.817085
Epoch 51: training loss 24283455544.471
Test Loss of 22477948457.936142, Test MSE of 22477948521.819073
Epoch 52: training loss 23464057001.412
Test Loss of 24790921556.227673, Test MSE of 24790921294.762428
Epoch 53: training loss 22460187294.118
Test Loss of 24031423761.414162, Test MSE of 24031424070.422989
Epoch 54: training loss 22099894046.118
Test Loss of 22407101767.433594, Test MSE of 22407101821.310623
Epoch 55: training loss 20766853613.176
Test Loss of 22315249657.366035, Test MSE of 22315249288.463966
Epoch 56: training loss 20147128521.412
Test Loss of 20501930273.999073, Test MSE of 20501930190.342304
Epoch 57: training loss 19874366004.706
Test Loss of 22010559475.205925, Test MSE of 22010559725.700321
Epoch 58: training loss 19005816353.882
Test Loss of 21368283114.202682, Test MSE of 21368283470.852821
Epoch 59: training loss 18697165541.647
Test Loss of 18717171881.639980, Test MSE of 18717172296.358776
Epoch 60: training loss 17891913592.471
Test Loss of 20585602655.955578, Test MSE of 20585602866.556225
Epoch 61: training loss 17295465560.471
Test Loss of 20167308121.677002, Test MSE of 20167308026.495220
Epoch 62: training loss 17016421492.706
Test Loss of 18414772806.841278, Test MSE of 18414772683.944477
Epoch 63: training loss 16527687360.000
Test Loss of 18719289269.130959, Test MSE of 18719289434.636974
Epoch 64: training loss 15823468152.471
Test Loss of 20387895674.136047, Test MSE of 20387895386.084675
Epoch 65: training loss 15541880673.882
Test Loss of 18435771500.512726, Test MSE of 18435771467.804203
Epoch 66: training loss 14936476608.000
Test Loss of 18426463254.745026, Test MSE of 18426463038.158825
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23719100450.51364, 'MSE - std': 3790825573.226047, 'R2 - mean': 0.8242741908461468, 'R2 - std': 0.024430253259812497} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005490 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043355075.765
Test Loss of 431611927351.559448, Test MSE of 431611932829.472534
Epoch 2: training loss 424023295156.706
Test Loss of 431591566717.453003, Test MSE of 431591566752.533691
Epoch 3: training loss 423996387328.000
Test Loss of 431563976228.249878, Test MSE of 431563977590.322144
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008047917.176
Test Loss of 431564555195.291077, Test MSE of 431564549908.107239
Epoch 2: training loss 423997965733.647
Test Loss of 431567632900.501648, Test MSE of 431567635181.821716
Epoch 3: training loss 423997369765.647
Test Loss of 431567562679.026367, Test MSE of 431567562880.892151
Epoch 4: training loss 418735535284.706
Test Loss of 414342506514.006470, Test MSE of 414342504324.137695
Epoch 5: training loss 382656489712.941
Test Loss of 359114964703.896362, Test MSE of 359114960059.768127
Epoch 6: training loss 315205104218.353
Test Loss of 280302354416.836670, Test MSE of 280302350646.891357
Epoch 7: training loss 223440902113.882
Test Loss of 178731908035.820465, Test MSE of 178731906138.089142
Epoch 8: training loss 159358537246.118
Test Loss of 136974905906.939377, Test MSE of 136974904762.035629
Epoch 9: training loss 140531193675.294
Test Loss of 126423897244.845901, Test MSE of 126423900293.325317
Epoch 10: training loss 134479427674.353
Test Loss of 122196029566.993057, Test MSE of 122196031132.388763
Epoch 11: training loss 132238694761.412
Test Loss of 117240239154.702454, Test MSE of 117240236877.089142
Epoch 12: training loss 127901959770.353
Test Loss of 114783230089.417862, Test MSE of 114783229368.881424
Epoch 13: training loss 124769918614.588
Test Loss of 112518063285.486343, Test MSE of 112518062126.326492
Epoch 14: training loss 121004116239.059
Test Loss of 106936536892.771866, Test MSE of 106936535836.000122
Epoch 15: training loss 116911045948.235
Test Loss of 103947304163.924103, Test MSE of 103947305223.797333
Epoch 16: training loss 113025664361.412
Test Loss of 98463598412.882919, Test MSE of 98463598970.995804
Epoch 17: training loss 108162899456.000
Test Loss of 95997251301.582596, Test MSE of 95997251139.362976
Epoch 18: training loss 105498007220.706
Test Loss of 94119777906.909760, Test MSE of 94119778133.009079
Epoch 19: training loss 101984765891.765
Test Loss of 90545915702.137894, Test MSE of 90545916818.777069
Epoch 20: training loss 98336643599.059
Test Loss of 87947955065.899124, Test MSE of 87947954239.312057
Epoch 21: training loss 92730055288.471
Test Loss of 83311155917.416016, Test MSE of 83311155441.467255
Epoch 22: training loss 90004402251.294
Test Loss of 80487164979.650162, Test MSE of 80487166871.807693
Epoch 23: training loss 86146870332.235
Test Loss of 75504796047.933365, Test MSE of 75504794690.008072
Epoch 24: training loss 82324254192.941
Test Loss of 72042311431.700134, Test MSE of 72042310914.222031
Epoch 25: training loss 78065329935.059
Test Loss of 66358877188.264694, Test MSE of 66358876796.670235
Epoch 26: training loss 75633195158.588
Test Loss of 64970079630.511803, Test MSE of 64970078425.797600
Epoch 27: training loss 72913016184.471
Test Loss of 62291181359.503937, Test MSE of 62291180028.010788
Epoch 28: training loss 68774007710.118
Test Loss of 58612386045.038406, Test MSE of 58612386282.630806
Epoch 29: training loss 65206060077.176
Test Loss of 54717281472.858864, Test MSE of 54717281656.754517
Epoch 30: training loss 62498769106.824
Test Loss of 53504752405.441925, Test MSE of 53504752322.149467
Epoch 31: training loss 59639700028.235
Test Loss of 53800054446.615456, Test MSE of 53800053927.727371
Epoch 32: training loss 56682209061.647
Test Loss of 49172665790.844978, Test MSE of 49172665444.113007
Epoch 33: training loss 54154824229.647
Test Loss of 45592640601.558540, Test MSE of 45592639322.913124
Epoch 34: training loss 52015001871.059
Test Loss of 44112854389.871353, Test MSE of 44112853975.464592
Epoch 35: training loss 48649571670.588
Test Loss of 43287410179.080055, Test MSE of 43287409674.760300
Epoch 36: training loss 46341638731.294
Test Loss of 39111022283.046738, Test MSE of 39111021854.358490
Epoch 37: training loss 43767483700.706
Test Loss of 36246546114.517357, Test MSE of 36246546598.752762
Epoch 38: training loss 41696259267.765
Test Loss of 36193557766.989357, Test MSE of 36193557488.549316
Epoch 39: training loss 39454243855.059
Test Loss of 35133978986.024986, Test MSE of 35133978722.308945
Epoch 40: training loss 38161950840.471
Test Loss of 33050447453.112450, Test MSE of 33050447077.719269
Epoch 41: training loss 35932790528.000
Test Loss of 33405158433.643684, Test MSE of 33405158822.126560
Epoch 42: training loss 34305605737.412
Test Loss of 30123755776.829247, Test MSE of 30123755896.557152
Epoch 43: training loss 32638474729.412
Test Loss of 29466443451.883389, Test MSE of 29466443942.635891
Epoch 44: training loss 31007541240.471
Test Loss of 26157183491.553909, Test MSE of 26157183337.967472
Epoch 45: training loss 29995614960.941
Test Loss of 25578171444.597870, Test MSE of 25578171227.589802
Epoch 46: training loss 29078689106.824
Test Loss of 26549925295.681629, Test MSE of 26549925076.995079
Epoch 47: training loss 27330007936.000
Test Loss of 24589445446.959740, Test MSE of 24589445312.723732
Epoch 48: training loss 26182433965.176
Test Loss of 23787720139.165203, Test MSE of 23787720081.445808
Epoch 49: training loss 25372815954.824
Test Loss of 22583778651.335491, Test MSE of 22583778600.552383
Epoch 50: training loss 24234358140.235
Test Loss of 23572141555.916706, Test MSE of 23572141365.746277
Epoch 51: training loss 23377735307.294
Test Loss of 22313473581.726978, Test MSE of 22313473646.001713
Epoch 52: training loss 22449749771.294
Test Loss of 19873630315.091160, Test MSE of 19873630520.012493
Epoch 53: training loss 21500028536.471
Test Loss of 22208785114.683941, Test MSE of 22208784874.468979
Epoch 54: training loss 20806022953.412
Test Loss of 20699351813.330864, Test MSE of 20699351804.768410
Epoch 55: training loss 20445083568.941
Test Loss of 20088828265.077278, Test MSE of 20088827969.390778
Epoch 56: training loss 19544291813.647
Test Loss of 22183189134.867191, Test MSE of 22183189069.920986
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23411918174.39511, 'MSE - std': 3445828002.7222943, 'R2 - mean': 0.8262863767619762, 'R2 - std': 0.022218582045448167} 
 

Saving model.....
Results After CV: {'MSE - mean': 23411918174.39511, 'MSE - std': 3445828002.7222943, 'R2 - mean': 0.8262863767619762, 'R2 - std': 0.022218582045448167}
Train time: 88.70831380960007
Inference time: 0.10073680020068423
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 73 finished with value: 23411918174.39511 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005911 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524565473.882
Test Loss of 418110884615.520691, Test MSE of 418110876895.256775
Epoch 2: training loss 427502261669.647
Test Loss of 418090946943.733521, Test MSE of 418090948414.582092
Epoch 3: training loss 427473778085.647
Test Loss of 418065468038.662048, Test MSE of 418065468311.758728
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427487237903.059
Test Loss of 418071274819.330994, Test MSE of 418071276795.747375
Epoch 2: training loss 427478172732.235
Test Loss of 418071559589.396240, Test MSE of 418071562610.730408
Epoch 3: training loss 427477624952.471
Test Loss of 418071430427.536438, Test MSE of 418071435414.627930
Epoch 4: training loss 422487797398.588
Test Loss of 402064950654.075439, Test MSE of 402064960024.969238
Epoch 5: training loss 386810288248.471
Test Loss of 347662712074.007874, Test MSE of 347662718210.262634
Epoch 6: training loss 319045111928.471
Test Loss of 269227411253.710846, Test MSE of 269227413887.181946
Epoch 7: training loss 226380945709.176
Test Loss of 170477259902.726807, Test MSE of 170477262334.605377
Epoch 8: training loss 161097825099.294
Test Loss of 131361264803.205185, Test MSE of 131361265219.259506
Epoch 9: training loss 142458977280.000
Test Loss of 120179898255.722412, Test MSE of 120179898298.574478
Epoch 10: training loss 134743291663.059
Test Loss of 115636468787.875092, Test MSE of 115636466784.235229
Epoch 11: training loss 131614909229.176
Test Loss of 112961113495.183899, Test MSE of 112961114205.664230
Epoch 12: training loss 129480586390.588
Test Loss of 109669828182.103165, Test MSE of 109669827747.415085
Epoch 13: training loss 125686214686.118
Test Loss of 106144949901.057602, Test MSE of 106144948076.275284
Epoch 14: training loss 122405885936.941
Test Loss of 102250458460.439514, Test MSE of 102250457871.092010
Epoch 15: training loss 117275199126.588
Test Loss of 100009931052.354385, Test MSE of 100009932244.777390
Epoch 16: training loss 113934805232.941
Test Loss of 96242107055.877869, Test MSE of 96242109350.912338
Epoch 17: training loss 110018853925.647
Test Loss of 92078211970.220673, Test MSE of 92078212325.197922
Epoch 18: training loss 106547216685.176
Test Loss of 90989140316.202637, Test MSE of 90989140924.745422
Epoch 19: training loss 101949756536.471
Test Loss of 84702530663.987045, Test MSE of 84702530926.484650
Epoch 20: training loss 98785406102.588
Test Loss of 82922319461.736755, Test MSE of 82922319742.062317
Epoch 21: training loss 94727542016.000
Test Loss of 80743732481.954193, Test MSE of 80743732458.807678
Epoch 22: training loss 90472990170.353
Test Loss of 76549637932.235947, Test MSE of 76549636448.336868
Epoch 23: training loss 85860905200.941
Test Loss of 74617420472.879013, Test MSE of 74617419519.183807
Epoch 24: training loss 82750620491.294
Test Loss of 71784191669.799683, Test MSE of 71784191157.888260
Epoch 25: training loss 78829317225.412
Test Loss of 67636873169.099236, Test MSE of 67636872906.027313
Epoch 26: training loss 75641802194.824
Test Loss of 65106526289.484154, Test MSE of 65106526422.264931
Epoch 27: training loss 72428329878.588
Test Loss of 63513481351.491096, Test MSE of 63513481091.145508
Epoch 28: training loss 68520290861.176
Test Loss of 60280001314.524170, Test MSE of 60280001240.076958
Epoch 29: training loss 65899438795.294
Test Loss of 55919194099.682625, Test MSE of 55919194778.887611
Epoch 30: training loss 62341598162.824
Test Loss of 51815364513.724731, Test MSE of 51815363738.052795
Epoch 31: training loss 60228370533.647
Test Loss of 50982191749.240807, Test MSE of 50982192027.777412
Epoch 32: training loss 56650131151.059
Test Loss of 48513368727.243118, Test MSE of 48513368301.667572
Epoch 33: training loss 53813206309.647
Test Loss of 45843765887.082115, Test MSE of 45843766190.218178
Epoch 34: training loss 50629987004.235
Test Loss of 46839078346.348366, Test MSE of 46839079040.707138
Epoch 35: training loss 48987728993.882
Test Loss of 42919099606.843399, Test MSE of 42919098980.420052
Epoch 36: training loss 45944651949.176
Test Loss of 38198802352.884567, Test MSE of 38198802034.799866
Epoch 37: training loss 43468413022.118
Test Loss of 35120073907.786263, Test MSE of 35120073871.628281
Epoch 38: training loss 41284417317.647
Test Loss of 37158758408.053665, Test MSE of 37158757682.748901
Epoch 39: training loss 39426127149.176
Test Loss of 31183473893.055748, Test MSE of 31183474764.687695
Epoch 40: training loss 37782403491.765
Test Loss of 30702669998.338192, Test MSE of 30702670108.500767
Epoch 41: training loss 35741371450.353
Test Loss of 31733827754.074486, Test MSE of 31733827644.159435
Epoch 42: training loss 33909896500.706
Test Loss of 26724365081.996761, Test MSE of 26724365422.014793
Epoch 43: training loss 32122930194.824
Test Loss of 27940360967.046959, Test MSE of 27940360770.582836
Epoch 44: training loss 31192699919.059
Test Loss of 28031844169.134396, Test MSE of 28031844555.925182
Epoch 45: training loss 29805420570.353
Test Loss of 29140647437.620171, Test MSE of 29140647623.940102
Epoch 46: training loss 28313645658.353
Test Loss of 23207056950.835995, Test MSE of 23207056229.906696
Epoch 47: training loss 26911292928.000
Test Loss of 25754390076.047188, Test MSE of 25754389884.186691
Epoch 48: training loss 25748207397.647
Test Loss of 22225628937.178810, Test MSE of 22225628881.410789
Epoch 49: training loss 24930828257.882
Test Loss of 23167234161.698822, Test MSE of 23167234694.068924
Epoch 50: training loss 23697985938.824
Test Loss of 23843421784.945641, Test MSE of 23843422077.947086
Epoch 51: training loss 22898384195.765
Test Loss of 21206366216.053665, Test MSE of 21206366191.158043
Epoch 52: training loss 21928901522.824
Test Loss of 19424664068.855888, Test MSE of 19424664348.036533
Epoch 53: training loss 21297611960.471
Test Loss of 20691722594.361324, Test MSE of 20691722642.293461
Epoch 54: training loss 20552907090.824
Test Loss of 20993141077.570206, Test MSE of 20993141100.938938
Epoch 55: training loss 19907617517.176
Test Loss of 20447556008.475597, Test MSE of 20447555840.321793
Epoch 56: training loss 19025588459.294
Test Loss of 18144921259.851028, Test MSE of 18144921135.723530
Epoch 57: training loss 18486744000.000
Test Loss of 19896222410.881332, Test MSE of 19896222699.018547
Epoch 58: training loss 18016125056.000
Test Loss of 19342230569.926441, Test MSE of 19342230625.220421
Epoch 59: training loss 17185391653.647
Test Loss of 18384259437.257462, Test MSE of 18384259236.157570
Epoch 60: training loss 16673522251.294
Test Loss of 19536834772.237797, Test MSE of 19536834951.234299
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19536834951.2343, 'MSE - std': 0.0, 'R2 - mean': 0.8478645465042703, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005477 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918466108.235
Test Loss of 424556721169.291687, Test MSE of 424556718762.600525
Epoch 2: training loss 427898277888.000
Test Loss of 424541046475.591980, Test MSE of 424541048304.457275
Epoch 3: training loss 427870586036.706
Test Loss of 424518761643.258850, Test MSE of 424518765395.656372
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890086369.882
Test Loss of 424524282706.372437, Test MSE of 424524280281.330688
Epoch 2: training loss 427878208451.765
Test Loss of 424524497407.644714, Test MSE of 424524498883.058472
Epoch 3: training loss 427877728496.941
Test Loss of 424525407672.819824, Test MSE of 424525403847.920654
Epoch 4: training loss 422702491768.471
Test Loss of 408194828623.885254, Test MSE of 408194836825.320618
Epoch 5: training loss 386681279668.706
Test Loss of 354843459174.684265, Test MSE of 354843458810.858459
Epoch 6: training loss 318321940961.882
Test Loss of 277921183347.475342, Test MSE of 277921184388.249817
Epoch 7: training loss 226318101684.706
Test Loss of 180729339716.396942, Test MSE of 180729338440.868835
Epoch 8: training loss 159446631845.647
Test Loss of 141971258239.851959, Test MSE of 141971260527.524139
Epoch 9: training loss 140577120888.471
Test Loss of 131579761556.222992, Test MSE of 131579759790.296844
Epoch 10: training loss 134654142765.176
Test Loss of 127320702405.610916, Test MSE of 127320701288.284180
Epoch 11: training loss 131122442571.294
Test Loss of 125155918581.992142, Test MSE of 125155920812.629135
Epoch 12: training loss 127646206524.235
Test Loss of 121655258432.014801, Test MSE of 121655257510.858002
Epoch 13: training loss 124983830407.529
Test Loss of 119190121727.585480, Test MSE of 119190121866.000031
Epoch 14: training loss 120817925270.588
Test Loss of 114329482365.305573, Test MSE of 114329481454.680664
Epoch 15: training loss 117273196303.059
Test Loss of 111922443751.246826, Test MSE of 111922446596.201309
Epoch 16: training loss 112702715090.824
Test Loss of 108133617041.498962, Test MSE of 108133617009.458832
Epoch 17: training loss 107992489351.529
Test Loss of 105028109853.253754, Test MSE of 105028109874.219055
Epoch 18: training loss 105923144222.118
Test Loss of 100475022827.510529, Test MSE of 100475023225.938065
Epoch 19: training loss 100897785494.588
Test Loss of 96696338903.376358, Test MSE of 96696340302.831131
Epoch 20: training loss 97191453786.353
Test Loss of 91588311095.428177, Test MSE of 91588310805.125000
Epoch 21: training loss 93949079642.353
Test Loss of 90052807987.697433, Test MSE of 90052809311.662399
Epoch 22: training loss 89563568263.529
Test Loss of 86242111168.695816, Test MSE of 86242113258.465302
Epoch 23: training loss 86192092175.059
Test Loss of 82239128801.739532, Test MSE of 82239129032.800491
Epoch 24: training loss 82330284393.412
Test Loss of 78970943972.641220, Test MSE of 78970944627.940460
Epoch 25: training loss 78814657114.353
Test Loss of 75163416989.579453, Test MSE of 75163417699.261856
Epoch 26: training loss 75120920681.412
Test Loss of 72702408063.022903, Test MSE of 72702407160.231445
Epoch 27: training loss 71890069805.176
Test Loss of 67587637297.269485, Test MSE of 67587639303.305634
Epoch 28: training loss 68458847247.059
Test Loss of 63854152792.827202, Test MSE of 63854152939.081413
Epoch 29: training loss 64767426017.882
Test Loss of 64534822983.061760, Test MSE of 64534823304.923164
Epoch 30: training loss 61822722063.059
Test Loss of 60432139468.894753, Test MSE of 60432138159.703995
Epoch 31: training loss 58442864745.412
Test Loss of 56181363072.681007, Test MSE of 56181362932.169426
Epoch 32: training loss 55854981165.176
Test Loss of 53923864559.892670, Test MSE of 53923864004.403236
Epoch 33: training loss 53269577185.882
Test Loss of 50359568654.508446, Test MSE of 50359569262.568443
Epoch 34: training loss 50256785679.059
Test Loss of 48975971104.629196, Test MSE of 48975973413.204384
Epoch 35: training loss 47747238023.529
Test Loss of 48241363054.382607, Test MSE of 48241363595.210274
Epoch 36: training loss 44736896278.588
Test Loss of 47418343229.290771, Test MSE of 47418341616.697937
Epoch 37: training loss 43149163738.353
Test Loss of 44793782490.870232, Test MSE of 44793782462.992744
Epoch 38: training loss 40253444510.118
Test Loss of 42662968224.066620, Test MSE of 42662968266.858421
Epoch 39: training loss 38164633600.000
Test Loss of 43339324971.939857, Test MSE of 43339324380.839333
Epoch 40: training loss 36355080071.529
Test Loss of 38653270375.098778, Test MSE of 38653270022.913116
Epoch 41: training loss 34667457505.882
Test Loss of 36408176601.626648, Test MSE of 36408177009.418732
Epoch 42: training loss 33023127092.706
Test Loss of 34567227240.401573, Test MSE of 34567227339.406494
Epoch 43: training loss 31267140675.765
Test Loss of 33642350432.347908, Test MSE of 33642350094.688164
Epoch 44: training loss 29459367499.294
Test Loss of 31883622834.424244, Test MSE of 31883622589.378792
Epoch 45: training loss 27946985795.765
Test Loss of 33455983603.445755, Test MSE of 33455983254.376003
Epoch 46: training loss 26643866819.765
Test Loss of 32123777872.477448, Test MSE of 32123777649.100224
Epoch 47: training loss 25766302377.412
Test Loss of 29770291223.213509, Test MSE of 29770291342.381245
Epoch 48: training loss 24345351521.882
Test Loss of 31534314329.952347, Test MSE of 31534313895.833904
Epoch 49: training loss 23142106992.941
Test Loss of 30794870723.834373, Test MSE of 30794869998.975128
Epoch 50: training loss 22198986601.412
Test Loss of 31960877855.207958, Test MSE of 31960878838.726696
Epoch 51: training loss 21423427655.529
Test Loss of 28262913236.000927, Test MSE of 28262913536.111725
Epoch 52: training loss 20320085270.588
Test Loss of 28877248302.841545, Test MSE of 28877247506.848530
Epoch 53: training loss 19651246656.000
Test Loss of 29215528821.666435, Test MSE of 29215529162.344887
Epoch 54: training loss 18706587497.412
Test Loss of 24889059975.372658, Test MSE of 24889060561.038723
Epoch 55: training loss 18112390121.412
Test Loss of 27486960231.631737, Test MSE of 27486960899.050407
Epoch 56: training loss 17530586891.294
Test Loss of 26480395132.535740, Test MSE of 26480394554.540150
Epoch 57: training loss 16756420566.588
Test Loss of 26239799600.854961, Test MSE of 26239799044.420586
Epoch 58: training loss 16435904304.941
Test Loss of 26572078507.318066, Test MSE of 26572079253.562637
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23054457102.398468, 'MSE - std': 3517622151.1641693, 'R2 - mean': 0.8290787712955845, 'R2 - std': 0.018785775208685795} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003540 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927302565.647
Test Loss of 447259327492.500549, Test MSE of 447259321241.987427
Epoch 2: training loss 421907020016.941
Test Loss of 447241056243.682617, Test MSE of 447241060777.580566
Epoch 3: training loss 421879690782.118
Test Loss of 447216123987.379150, Test MSE of 447216132983.973389
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900585923.765
Test Loss of 447223683321.663635, Test MSE of 447223692382.762085
Epoch 2: training loss 421888662106.353
Test Loss of 447224549085.120544, Test MSE of 447224553719.303955
Epoch 3: training loss 421888184801.882
Test Loss of 447224452941.398132, Test MSE of 447224453634.334656
Epoch 4: training loss 416599350693.647
Test Loss of 430087072885.251892, Test MSE of 430087073027.299438
Epoch 5: training loss 380346179824.941
Test Loss of 374646306233.767273, Test MSE of 374646311542.815613
Epoch 6: training loss 312325772468.706
Test Loss of 296831967935.037720, Test MSE of 296831967742.897766
Epoch 7: training loss 221487116288.000
Test Loss of 196795179006.105011, Test MSE of 196795178389.638123
Epoch 8: training loss 157033951804.235
Test Loss of 154088049711.137634, Test MSE of 154088049483.961304
Epoch 9: training loss 137063318377.412
Test Loss of 142312516569.863525, Test MSE of 142312518062.947998
Epoch 10: training loss 132232659365.647
Test Loss of 137293605611.095993, Test MSE of 137293603170.751846
Epoch 11: training loss 128314925146.353
Test Loss of 134086691906.087433, Test MSE of 134086692283.194427
Epoch 12: training loss 124636962936.471
Test Loss of 130742848244.570892, Test MSE of 130742851016.228546
Epoch 13: training loss 122165850714.353
Test Loss of 126999203162.544525, Test MSE of 126999201494.608688
Epoch 14: training loss 117299781571.765
Test Loss of 122390652125.002075, Test MSE of 122390651299.798462
Epoch 15: training loss 113950422678.588
Test Loss of 119068117581.812637, Test MSE of 119068118307.094040
Epoch 16: training loss 110632071017.412
Test Loss of 116414646376.697662, Test MSE of 116414645282.381104
Epoch 17: training loss 107175393581.176
Test Loss of 111589816291.101547, Test MSE of 111589817261.425995
Epoch 18: training loss 103521897200.941
Test Loss of 107166367308.154526, Test MSE of 107166367372.056412
Epoch 19: training loss 99979339489.882
Test Loss of 104159663195.195923, Test MSE of 104159662615.273270
Epoch 20: training loss 95619562285.176
Test Loss of 100133584437.414764, Test MSE of 100133583523.827377
Epoch 21: training loss 91179874650.353
Test Loss of 99654317874.868378, Test MSE of 99654316007.933960
Epoch 22: training loss 88218378104.471
Test Loss of 95247492174.878555, Test MSE of 95247494151.356491
Epoch 23: training loss 85202979644.235
Test Loss of 88920940951.420776, Test MSE of 88920941602.776230
Epoch 24: training loss 80272690672.941
Test Loss of 84903764384.895676, Test MSE of 84903763923.178207
Epoch 25: training loss 77119519307.294
Test Loss of 83771414891.362473, Test MSE of 83771416062.771561
Epoch 26: training loss 74067213658.353
Test Loss of 80325360492.902145, Test MSE of 80325359958.178665
Epoch 27: training loss 70510389458.824
Test Loss of 72513854811.492020, Test MSE of 72513853697.647415
Epoch 28: training loss 68047699847.529
Test Loss of 72740459791.219055, Test MSE of 72740461570.765991
Epoch 29: training loss 63740658206.118
Test Loss of 68491438976.799446, Test MSE of 68491437524.350121
Epoch 30: training loss 60563538944.000
Test Loss of 66546101475.160767, Test MSE of 66546101187.014664
Epoch 31: training loss 58081954680.471
Test Loss of 65049978109.216751, Test MSE of 65049978253.268661
Epoch 32: training loss 55322723824.941
Test Loss of 56988541068.228546, Test MSE of 56988540568.775650
Epoch 33: training loss 52663058913.882
Test Loss of 54472303899.536430, Test MSE of 54472304201.411713
Epoch 34: training loss 49926329690.353
Test Loss of 49429320628.437660, Test MSE of 49429320382.029579
Epoch 35: training loss 47967063100.235
Test Loss of 53086620335.404114, Test MSE of 53086619183.461021
Epoch 36: training loss 45231301398.588
Test Loss of 48325877493.755264, Test MSE of 48325877674.023018
Epoch 37: training loss 42891462625.882
Test Loss of 44173228884.267410, Test MSE of 44173229332.866295
Epoch 38: training loss 41021427862.588
Test Loss of 45484461807.122833, Test MSE of 45484461616.435013
Epoch 39: training loss 38359373801.412
Test Loss of 42218903583.740921, Test MSE of 42218903975.223633
Epoch 40: training loss 36783391826.824
Test Loss of 41624358833.358315, Test MSE of 41624358778.359047
Epoch 41: training loss 34798442209.882
Test Loss of 38910007738.004166, Test MSE of 38910007805.667854
Epoch 42: training loss 33623217641.412
Test Loss of 41416291911.890816, Test MSE of 41416292769.066750
Epoch 43: training loss 31770876845.176
Test Loss of 35318584291.812164, Test MSE of 35318584442.584175
Epoch 44: training loss 30225176960.000
Test Loss of 33743454652.372890, Test MSE of 33743454726.962482
Epoch 45: training loss 28572759416.471
Test Loss of 35677954307.612305, Test MSE of 35677954316.431252
Epoch 46: training loss 27190513882.353
Test Loss of 29940028267.007172, Test MSE of 29940028353.514145
Epoch 47: training loss 26103501967.059
Test Loss of 33007667122.779552, Test MSE of 33007667610.487190
Epoch 48: training loss 25282431698.824
Test Loss of 28997308852.792969, Test MSE of 28997309122.545303
Epoch 49: training loss 24369113799.529
Test Loss of 29056308501.851494, Test MSE of 29056308972.924644
Epoch 50: training loss 23027912101.647
Test Loss of 26347495684.086052, Test MSE of 26347495808.388569
Epoch 51: training loss 21918312632.471
Test Loss of 29217064082.624104, Test MSE of 29217064744.551159
Epoch 52: training loss 21481934061.176
Test Loss of 27596423366.499191, Test MSE of 27596423377.833088
Epoch 53: training loss 20573794202.353
Test Loss of 25482463728.011105, Test MSE of 25482463748.926640
Epoch 54: training loss 19587718799.059
Test Loss of 25490209257.378674, Test MSE of 25490209500.515369
Epoch 55: training loss 19310309443.765
Test Loss of 26175254682.914642, Test MSE of 26175254403.817612
Epoch 56: training loss 18623117812.706
Test Loss of 27375996438.621326, Test MSE of 27375996678.087490
Epoch 57: training loss 17916099663.059
Test Loss of 24859894865.721027, Test MSE of 24859894961.746124
Epoch 58: training loss 17564877605.647
Test Loss of 25030946591.918575, Test MSE of 25030946273.913784
Epoch 59: training loss 17105576060.235
Test Loss of 24109374399.807541, Test MSE of 24109374936.531349
Epoch 60: training loss 16444590174.118
Test Loss of 24136302487.776081, Test MSE of 24136302483.461143
Epoch 61: training loss 16023735134.118
Test Loss of 23325789869.746010, Test MSE of 23325789936.960560
Epoch 62: training loss 15624695427.765
Test Loss of 20885798641.017811, Test MSE of 20885799050.757130
Epoch 63: training loss 15061355143.529
Test Loss of 23319751192.279434, Test MSE of 23319751795.442280
Epoch 64: training loss 14649198810.353
Test Loss of 19015993945.182510, Test MSE of 19015993977.747612
Epoch 65: training loss 14312345524.706
Test Loss of 25907913844.778164, Test MSE of 25907914660.998451
Epoch 66: training loss 13904045014.588
Test Loss of 21498765377.376823, Test MSE of 21498765333.541218
Epoch 67: training loss 13524920658.824
Test Loss of 23103245340.187832, Test MSE of 23103245472.308361
Epoch 68: training loss 13172666736.941
Test Loss of 23171371615.814945, Test MSE of 23171371680.944977
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23093428628.58064, 'MSE - std': 2872655210.534874, 'R2 - mean': 0.8346357501621947, 'R2 - std': 0.017234565898471492} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005395 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110413763.765
Test Loss of 410764917851.453979, Test MSE of 410764917122.532471
Epoch 2: training loss 430090060498.824
Test Loss of 410747299251.472473, Test MSE of 410747294350.411499
Epoch 3: training loss 430062943533.176
Test Loss of 410724041735.107849, Test MSE of 410724044560.220886
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078858661.647
Test Loss of 410728282181.182800, Test MSE of 410728278534.953125
Epoch 2: training loss 430067330108.235
Test Loss of 410729515628.749634, Test MSE of 410729517972.868652
Epoch 3: training loss 430066906774.588
Test Loss of 410729507379.887085, Test MSE of 410729512875.868958
Epoch 4: training loss 424707169822.118
Test Loss of 393883323640.773743, Test MSE of 393883329553.870239
Epoch 5: training loss 388341138371.765
Test Loss of 339441510554.476624, Test MSE of 339441510065.137939
Epoch 6: training loss 320205726057.412
Test Loss of 262209610800.807037, Test MSE of 262209607033.772736
Epoch 7: training loss 228868225746.824
Test Loss of 164831640083.664978, Test MSE of 164831642242.584351
Epoch 8: training loss 164317743134.118
Test Loss of 124559842634.276718, Test MSE of 124559842307.927536
Epoch 9: training loss 145018920749.176
Test Loss of 114605638048.992126, Test MSE of 114605637075.091644
Epoch 10: training loss 139353450857.412
Test Loss of 110226956197.019897, Test MSE of 110226956297.809891
Epoch 11: training loss 134894238448.941
Test Loss of 107136596609.599258, Test MSE of 107136596367.821518
Epoch 12: training loss 131428295800.471
Test Loss of 104176888167.181854, Test MSE of 104176888108.789566
Epoch 13: training loss 128168173146.353
Test Loss of 101067368090.713562, Test MSE of 101067367036.307068
Epoch 14: training loss 125284257520.941
Test Loss of 98658869774.926422, Test MSE of 98658868785.321426
Epoch 15: training loss 120954376914.824
Test Loss of 95181474200.462753, Test MSE of 95181473142.580215
Epoch 16: training loss 116800289611.294
Test Loss of 93299023602.850525, Test MSE of 93299022575.722412
Epoch 17: training loss 112714552410.353
Test Loss of 89173450941.541885, Test MSE of 89173450855.263428
Epoch 18: training loss 108948910802.824
Test Loss of 86414164066.561783, Test MSE of 86414164860.545990
Epoch 19: training loss 104647595881.412
Test Loss of 82893874175.526138, Test MSE of 82893873625.656097
Epoch 20: training loss 100570655472.941
Test Loss of 80149184765.986115, Test MSE of 80149184631.475479
Epoch 21: training loss 97009834767.059
Test Loss of 76307053765.597412, Test MSE of 76307054444.382202
Epoch 22: training loss 93373609592.471
Test Loss of 73657214258.583984, Test MSE of 73657214036.548370
Epoch 23: training loss 89552885007.059
Test Loss of 72078062259.827850, Test MSE of 72078064052.712799
Epoch 24: training loss 86134108461.176
Test Loss of 67825759392.636742, Test MSE of 67825758894.967865
Epoch 25: training loss 81931901229.176
Test Loss of 66187750705.636276, Test MSE of 66187749543.460114
Epoch 26: training loss 78208177543.529
Test Loss of 60848344267.283669, Test MSE of 60848344629.426163
Epoch 27: training loss 75321406569.412
Test Loss of 61534437394.954185, Test MSE of 61534438705.615120
Epoch 28: training loss 71610054264.471
Test Loss of 55467838841.662193, Test MSE of 55467837795.779938
Epoch 29: training loss 68591824489.412
Test Loss of 55883533909.530769, Test MSE of 55883534212.739876
Epoch 30: training loss 66047068958.118
Test Loss of 49762904495.207771, Test MSE of 49762905768.605721
Epoch 31: training loss 61898233374.118
Test Loss of 47710874757.153168, Test MSE of 47710874078.591743
Epoch 32: training loss 59235867550.118
Test Loss of 46137276689.888016, Test MSE of 46137277246.592926
Epoch 33: training loss 55955365812.706
Test Loss of 43853008300.838501, Test MSE of 43853008224.052368
Epoch 34: training loss 54002301168.941
Test Loss of 44093422340.383156, Test MSE of 44093422690.398689
Epoch 35: training loss 50294260269.176
Test Loss of 39845553405.512260, Test MSE of 39845552729.193634
Epoch 36: training loss 49173233701.647
Test Loss of 37782096907.846367, Test MSE of 37782096814.798241
Epoch 37: training loss 46451134727.529
Test Loss of 36047819168.992134, Test MSE of 36047819494.573624
Epoch 38: training loss 43539275964.235
Test Loss of 35603389357.075432, Test MSE of 35603388914.592163
Epoch 39: training loss 41523474168.471
Test Loss of 32176662790.515503, Test MSE of 32176662972.471260
Epoch 40: training loss 39327341485.176
Test Loss of 31635054720.888477, Test MSE of 31635054281.440536
Epoch 41: training loss 37649019407.059
Test Loss of 28623850219.742710, Test MSE of 28623849667.069080
Epoch 42: training loss 36255899858.824
Test Loss of 28835672479.096714, Test MSE of 28835672150.953297
Epoch 43: training loss 34127820672.000
Test Loss of 27884682399.689034, Test MSE of 27884682083.433479
Epoch 44: training loss 32645792557.176
Test Loss of 27227533363.176308, Test MSE of 27227533981.899590
Epoch 45: training loss 31255050804.706
Test Loss of 24680294746.861637, Test MSE of 24680294664.602833
Epoch 46: training loss 29647240933.647
Test Loss of 26339332013.549282, Test MSE of 26339332039.812450
Epoch 47: training loss 28400289031.529
Test Loss of 23371184852.997684, Test MSE of 23371185060.385197
Epoch 48: training loss 27120742407.529
Test Loss of 23615219118.733921, Test MSE of 23615220103.403091
Epoch 49: training loss 25952453172.706
Test Loss of 24571994175.496529, Test MSE of 24571993696.389286
Epoch 50: training loss 25162035512.471
Test Loss of 21060799481.839890, Test MSE of 21060799645.307373
Epoch 51: training loss 23982444988.235
Test Loss of 21185643681.584450, Test MSE of 21185643490.855347
Epoch 52: training loss 23027008338.824
Test Loss of 23547463211.831558, Test MSE of 23547463484.573040
Epoch 53: training loss 22094379508.706
Test Loss of 22605654245.345673, Test MSE of 22605654158.511517
Epoch 54: training loss 21065040854.588
Test Loss of 23250113173.501156, Test MSE of 23250113645.389606
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23132599882.78288, 'MSE - std': 2488717366.5217996, 'R2 - mean': 0.8280029863804772, 'R2 - std': 0.018834897462788606} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005557 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043402300.235
Test Loss of 431613496424.721863, Test MSE of 431613490228.683594
Epoch 2: training loss 424024593950.118
Test Loss of 431594964035.761230, Test MSE of 431594966189.477234
Epoch 3: training loss 423998430388.706
Test Loss of 431569411728.288757, Test MSE of 431569412273.034607
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424016290273.882
Test Loss of 431572470555.128174, Test MSE of 431572466579.067444
Epoch 2: training loss 424003342817.882
Test Loss of 431573681378.976379, Test MSE of 431573684972.154358
Epoch 3: training loss 424002695649.882
Test Loss of 431573499373.282715, Test MSE of 431573503075.600830
Epoch 4: training loss 418542892694.588
Test Loss of 414205899527.226257, Test MSE of 414205908874.227661
Epoch 5: training loss 382178279303.529
Test Loss of 358476773467.927795, Test MSE of 358476777888.445068
Epoch 6: training loss 314613656214.588
Test Loss of 279404627959.470642, Test MSE of 279404628207.256165
Epoch 7: training loss 224178597586.824
Test Loss of 180058828036.146240, Test MSE of 180058826560.032379
Epoch 8: training loss 160788416090.353
Test Loss of 138889361048.818146, Test MSE of 138889361988.452423
Epoch 9: training loss 141254434785.882
Test Loss of 126393787342.245255, Test MSE of 126393787303.628036
Epoch 10: training loss 134307538883.765
Test Loss of 122113884698.298935, Test MSE of 122113884077.310181
Epoch 11: training loss 133290896896.000
Test Loss of 119043429203.043030, Test MSE of 119043427970.585068
Epoch 12: training loss 128947082541.176
Test Loss of 115818418268.875519, Test MSE of 115818416692.703232
Epoch 13: training loss 126033224553.412
Test Loss of 111670783774.445160, Test MSE of 111670784957.240463
Epoch 14: training loss 121505407623.529
Test Loss of 108450210773.826935, Test MSE of 108450210087.648087
Epoch 15: training loss 117893778672.941
Test Loss of 104904674780.223969, Test MSE of 104904676594.263733
Epoch 16: training loss 115692672421.647
Test Loss of 102104598492.934753, Test MSE of 102104598839.919220
Epoch 17: training loss 110040400143.059
Test Loss of 97430846517.545578, Test MSE of 97430847594.160492
Epoch 18: training loss 107432996306.824
Test Loss of 93387914164.657104, Test MSE of 93387914755.423813
Epoch 19: training loss 103004550053.647
Test Loss of 91541046370.087921, Test MSE of 91541048267.726944
Epoch 20: training loss 99594369204.706
Test Loss of 90399921172.375748, Test MSE of 90399920650.410751
Epoch 21: training loss 96841079928.471
Test Loss of 84647164052.790375, Test MSE of 84647161515.398651
Epoch 22: training loss 91455599013.647
Test Loss of 81535166944.488663, Test MSE of 81535167777.326462
Epoch 23: training loss 89246022113.882
Test Loss of 80629914215.063400, Test MSE of 80629912253.090286
Epoch 24: training loss 84585403824.941
Test Loss of 75294005233.784363, Test MSE of 75294004221.891586
Epoch 25: training loss 82253692235.294
Test Loss of 68658053893.330864, Test MSE of 68658054211.498741
Epoch 26: training loss 77907363704.471
Test Loss of 66687476913.695511, Test MSE of 66687475599.474548
Epoch 27: training loss 74232688135.529
Test Loss of 62747012638.563629, Test MSE of 62747013296.451538
Epoch 28: training loss 71117048282.353
Test Loss of 60502769880.077744, Test MSE of 60502770578.718880
Epoch 29: training loss 68829792873.412
Test Loss of 57100378561.214256, Test MSE of 57100378602.176277
Epoch 30: training loss 65260987828.706
Test Loss of 55875061027.420639, Test MSE of 55875060235.944054
Epoch 31: training loss 61971147971.765
Test Loss of 53739073774.822769, Test MSE of 53739074414.275757
Epoch 32: training loss 58600444069.647
Test Loss of 53341496093.971306, Test MSE of 53341497335.586388
Epoch 33: training loss 56667683418.353
Test Loss of 44925782223.548355, Test MSE of 44925782391.269028
Epoch 34: training loss 53809631638.588
Test Loss of 42898285305.958351, Test MSE of 42898284759.195351
Epoch 35: training loss 51388467787.294
Test Loss of 43450707880.810738, Test MSE of 43450708284.930382
Epoch 36: training loss 48389068416.000
Test Loss of 44075623133.527069, Test MSE of 44075623533.926933
Epoch 37: training loss 46827317797.647
Test Loss of 39350314859.209625, Test MSE of 39350315182.949722
Epoch 38: training loss 43880585645.176
Test Loss of 40059761420.912544, Test MSE of 40059760334.922791
Epoch 39: training loss 41999097208.471
Test Loss of 35453550108.668213, Test MSE of 35453550156.529930
Epoch 40: training loss 40493415348.706
Test Loss of 36660433498.743172, Test MSE of 36660433503.511696
Epoch 41: training loss 38806487258.353
Test Loss of 33698531090.124943, Test MSE of 33698531452.825920
Epoch 42: training loss 36724624082.824
Test Loss of 31892034209.821381, Test MSE of 31892034143.847561
Epoch 43: training loss 34916079781.647
Test Loss of 30277592307.087460, Test MSE of 30277592476.098572
Epoch 44: training loss 33709084310.588
Test Loss of 29390507141.153172, Test MSE of 29390507206.096287
Epoch 45: training loss 31672475482.353
Test Loss of 27200715132.979176, Test MSE of 27200714364.316555
Epoch 46: training loss 30641902027.294
Test Loss of 29272836938.513653, Test MSE of 29272837051.838688
Epoch 47: training loss 28889573082.353
Test Loss of 25457105393.547432, Test MSE of 25457105838.528584
Epoch 48: training loss 28411105445.647
Test Loss of 24244153731.139286, Test MSE of 24244153547.849144
Epoch 49: training loss 26681147809.882
Test Loss of 22133517967.341045, Test MSE of 22133518485.678707
Epoch 50: training loss 25887041148.235
Test Loss of 25015613766.012032, Test MSE of 25015613774.110241
Epoch 51: training loss 24501434729.412
Test Loss of 23320809121.347523, Test MSE of 23320809264.535450
Epoch 52: training loss 23522479653.647
Test Loss of 25236248115.413235, Test MSE of 25236247789.032291
Epoch 53: training loss 22764111777.882
Test Loss of 22374410259.901897, Test MSE of 22374410610.799728
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22980962028.38625, 'MSE - std': 2246541221.4971294, 'R2 - mean': 0.8289838032760783, 'R2 - std': 0.016960268190031547} 
 

Saving model.....
Results After CV: {'MSE - mean': 22980962028.38625, 'MSE - std': 2246541221.4971294, 'R2 - mean': 0.8289838032760783, 'R2 - std': 0.016960268190031547}
Train time: 91.98822335280056
Inference time: 0.07453828979996616
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 74 finished with value: 22980962028.38625 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005662 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525411056.941
Test Loss of 418112711096.109192, Test MSE of 418112710699.301086
Epoch 2: training loss 427504998881.882
Test Loss of 418094909241.500793, Test MSE of 418094911432.685913
Epoch 3: training loss 427477565078.588
Test Loss of 418071516838.639832, Test MSE of 418071511870.772034
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493846919.529
Test Loss of 418076232277.392578, Test MSE of 418076229936.727844
Epoch 2: training loss 427483921227.294
Test Loss of 418077263753.089966, Test MSE of 418077268238.707214
Epoch 3: training loss 427483467776.000
Test Loss of 418076685867.466125, Test MSE of 418076683021.958130
Epoch 4: training loss 427483163708.235
Test Loss of 418076205224.653259, Test MSE of 418076203721.291504
Epoch 5: training loss 421057475764.706
Test Loss of 397367785393.832031, Test MSE of 397367791527.962769
Epoch 6: training loss 376624871664.941
Test Loss of 331035382333.231567, Test MSE of 331035383947.507996
Epoch 7: training loss 297833642767.059
Test Loss of 246100665239.065460, Test MSE of 246100663890.920441
Epoch 8: training loss 218223558234.353
Test Loss of 175160661052.876251, Test MSE of 175160662517.435913
Epoch 9: training loss 160560652378.353
Test Loss of 126243087368.764282, Test MSE of 126243085030.959900
Epoch 10: training loss 138842267768.471
Test Loss of 117840752019.393936, Test MSE of 117840754540.387238
Epoch 11: training loss 134209163384.471
Test Loss of 115327986142.008789, Test MSE of 115327985326.771362
Epoch 12: training loss 131523968963.765
Test Loss of 111776163027.527176, Test MSE of 111776163631.709351
Epoch 13: training loss 127900467561.412
Test Loss of 108285534518.776779, Test MSE of 108285535849.387329
Epoch 14: training loss 125174144421.647
Test Loss of 106070374365.416611, Test MSE of 106070374635.483368
Epoch 15: training loss 121626762616.471
Test Loss of 102970652040.497803, Test MSE of 102970653583.545105
Epoch 16: training loss 117294957447.529
Test Loss of 98012622877.372192, Test MSE of 98012623631.425858
Epoch 17: training loss 113035684562.824
Test Loss of 95610300647.661346, Test MSE of 95610299347.286804
Epoch 18: training loss 109033876954.353
Test Loss of 92018845749.533188, Test MSE of 92018844809.255234
Epoch 19: training loss 106172658913.882
Test Loss of 87588615717.781174, Test MSE of 87588614793.830429
Epoch 20: training loss 101082152794.353
Test Loss of 85868819899.662277, Test MSE of 85868820432.205872
Epoch 21: training loss 97684083154.824
Test Loss of 82421976740.271103, Test MSE of 82421977511.905594
Epoch 22: training loss 94020234812.235
Test Loss of 78612980363.873230, Test MSE of 78612981229.874481
Epoch 23: training loss 88960830358.588
Test Loss of 77673254033.676620, Test MSE of 77673254150.624542
Epoch 24: training loss 85504849724.235
Test Loss of 73308473590.821182, Test MSE of 73308470985.267487
Epoch 25: training loss 81972275154.824
Test Loss of 69381364922.418686, Test MSE of 69381365873.781204
Epoch 26: training loss 78218474661.647
Test Loss of 66868287400.357162, Test MSE of 66868288697.134834
Epoch 27: training loss 74810301741.176
Test Loss of 63690904163.131157, Test MSE of 63690904145.875282
Epoch 28: training loss 71390031424.000
Test Loss of 61292157907.231087, Test MSE of 61292158162.077515
Epoch 29: training loss 69055030076.235
Test Loss of 58643022834.024521, Test MSE of 58643021694.966171
Epoch 30: training loss 64694615943.529
Test Loss of 55091627350.280823, Test MSE of 55091627254.994553
Epoch 31: training loss 61838514657.882
Test Loss of 51955332295.209808, Test MSE of 51955333096.805275
Epoch 32: training loss 59280298744.471
Test Loss of 51222051440.869766, Test MSE of 51222051239.767349
Epoch 33: training loss 56005556743.529
Test Loss of 50506786824.053665, Test MSE of 50506787384.642349
Epoch 34: training loss 53411810334.118
Test Loss of 44358069108.482071, Test MSE of 44358069011.638229
Epoch 35: training loss 50413792399.059
Test Loss of 43535695497.741386, Test MSE of 43535694870.901817
Epoch 36: training loss 47893014279.529
Test Loss of 41619659561.630348, Test MSE of 41619658651.750023
Epoch 37: training loss 46136184380.235
Test Loss of 40555017976.123985, Test MSE of 40555017894.119492
Epoch 38: training loss 43193647344.941
Test Loss of 37443591142.180893, Test MSE of 37443591186.626663
Epoch 39: training loss 40882454046.118
Test Loss of 36130235217.898682, Test MSE of 36130235357.239922
Epoch 40: training loss 39049849494.588
Test Loss of 34928451926.043953, Test MSE of 34928451163.247650
Epoch 41: training loss 36909666729.412
Test Loss of 31433448569.041870, Test MSE of 31433448775.417912
Epoch 42: training loss 34793995572.706
Test Loss of 32014598478.464031, Test MSE of 32014598907.817600
Epoch 43: training loss 33003860480.000
Test Loss of 26218775325.786724, Test MSE of 26218774533.206944
Epoch 44: training loss 32086307260.235
Test Loss of 31183503432.956741, Test MSE of 31183503374.739525
Epoch 45: training loss 30165668404.706
Test Loss of 30060219922.357620, Test MSE of 30060219553.238724
Epoch 46: training loss 29228642710.588
Test Loss of 26094895720.816101, Test MSE of 26094895835.851303
Epoch 47: training loss 27879485963.294
Test Loss of 26056423524.907703, Test MSE of 26056423763.460777
Epoch 48: training loss 26366941906.824
Test Loss of 25739343038.682396, Test MSE of 25739343630.641727
Epoch 49: training loss 25102277526.588
Test Loss of 24259418197.747860, Test MSE of 24259418289.086636
Epoch 50: training loss 24116204912.941
Test Loss of 20787516456.505203, Test MSE of 20787516919.085915
Epoch 51: training loss 22927785114.353
Test Loss of 22214206524.876244, Test MSE of 22214206430.339607
Epoch 52: training loss 22450201520.941
Test Loss of 21072390183.320843, Test MSE of 21072390340.472553
Epoch 53: training loss 21423091922.824
Test Loss of 22117731087.574371, Test MSE of 22117731271.774776
Epoch 54: training loss 20540814283.294
Test Loss of 19432294668.139717, Test MSE of 19432294618.510120
Epoch 55: training loss 19859217596.235
Test Loss of 19038477378.561184, Test MSE of 19038477562.666340
Epoch 56: training loss 19019346002.824
Test Loss of 20315171997.520241, Test MSE of 20315172064.064514
Epoch 57: training loss 18224027941.647
Test Loss of 19236185021.438816, Test MSE of 19236184877.030384
Epoch 58: training loss 17897751134.118
Test Loss of 20939101750.835995, Test MSE of 20939102240.956215
Epoch 59: training loss 17104604532.706
Test Loss of 18802455308.731899, Test MSE of 18802455045.473614
Epoch 60: training loss 16911066800.941
Test Loss of 19253042753.732132, Test MSE of 19253042964.319004
Epoch 61: training loss 16201970985.412
Test Loss of 19477188202.474209, Test MSE of 19477188617.725571
Epoch 62: training loss 15582584180.706
Test Loss of 17831394026.622253, Test MSE of 17831393908.572918
Epoch 63: training loss 15344124965.647
Test Loss of 17907192553.201019, Test MSE of 17907192994.481403
Epoch 64: training loss 14740386281.412
Test Loss of 19449686604.391396, Test MSE of 19449686589.409550
Epoch 65: training loss 14566540525.176
Test Loss of 17763297152.088829, Test MSE of 17763297064.121830
Epoch 66: training loss 14261297370.353
Test Loss of 18129846942.823040, Test MSE of 18129847277.593296
Epoch 67: training loss 13675109296.941
Test Loss of 18596685030.003239, Test MSE of 18596685114.105541
Epoch 68: training loss 13589504203.294
Test Loss of 18079447091.638214, Test MSE of 18079447169.057613
Epoch 69: training loss 12958776033.882
Test Loss of 19653584650.126301, Test MSE of 19653584514.042301
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19653584514.0423, 'MSE - std': 0.0, 'R2 - mean': 0.8469554049914532, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005628 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917560410.353
Test Loss of 424556206181.855164, Test MSE of 424556202656.263123
Epoch 2: training loss 427896283136.000
Test Loss of 424539473353.874634, Test MSE of 424539481696.405396
Epoch 3: training loss 427867771120.941
Test Loss of 424516741670.018066, Test MSE of 424516734045.105652
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427885236464.941
Test Loss of 424523477642.925720, Test MSE of 424523480196.553528
Epoch 2: training loss 427876057328.941
Test Loss of 424524713132.206360, Test MSE of 424524717180.445923
Epoch 3: training loss 427875607491.765
Test Loss of 424524365873.269470, Test MSE of 424524362699.795044
Epoch 4: training loss 427875257645.176
Test Loss of 424523932097.110352, Test MSE of 424523938102.209473
Epoch 5: training loss 420994520726.588
Test Loss of 402832228905.334229, Test MSE of 402832225764.318298
Epoch 6: training loss 375142530469.647
Test Loss of 337172949754.255859, Test MSE of 337172950198.664795
Epoch 7: training loss 296610701552.941
Test Loss of 252966911711.962982, Test MSE of 252966914782.246429
Epoch 8: training loss 216530956528.941
Test Loss of 183827325164.872528, Test MSE of 183827325310.102142
Epoch 9: training loss 156123810575.059
Test Loss of 137023383793.136246, Test MSE of 137023385354.864883
Epoch 10: training loss 136486016000.000
Test Loss of 129508092093.971771, Test MSE of 129508093522.346786
Epoch 11: training loss 133125893029.647
Test Loss of 126210264827.440201, Test MSE of 126210261431.529846
Epoch 12: training loss 129129523200.000
Test Loss of 123371232137.326859, Test MSE of 123371234141.673630
Epoch 13: training loss 127548527495.529
Test Loss of 119776646216.246124, Test MSE of 119776646432.815094
Epoch 14: training loss 122314791183.059
Test Loss of 116333400669.446213, Test MSE of 116333403936.568268
Epoch 15: training loss 118296943435.294
Test Loss of 112388681401.115891, Test MSE of 112388680444.078308
Epoch 16: training loss 114408775770.353
Test Loss of 110264410805.562805, Test MSE of 110264410071.308960
Epoch 17: training loss 111065529705.412
Test Loss of 105968913984.784637, Test MSE of 105968916798.072495
Epoch 18: training loss 107016513054.118
Test Loss of 102021205647.426331, Test MSE of 102021204296.014511
Epoch 19: training loss 101607426048.000
Test Loss of 98501293897.608139, Test MSE of 98501295607.513519
Epoch 20: training loss 99035647186.824
Test Loss of 95335413221.351837, Test MSE of 95335413288.028244
Epoch 21: training loss 94112844047.059
Test Loss of 92032304412.483917, Test MSE of 92032302939.247864
Epoch 22: training loss 91274907888.941
Test Loss of 87871692913.698822, Test MSE of 87871695649.320618
Epoch 23: training loss 86242726881.882
Test Loss of 83680133231.803833, Test MSE of 83680133787.216034
Epoch 24: training loss 83286900239.059
Test Loss of 78993827317.459167, Test MSE of 78993827124.447754
Epoch 25: training loss 78899927416.471
Test Loss of 75616901798.876709, Test MSE of 75616902617.242874
Epoch 26: training loss 75522633908.706
Test Loss of 73051385614.389999, Test MSE of 73051385189.250351
Epoch 27: training loss 72035946827.294
Test Loss of 69116406845.349991, Test MSE of 69116406358.611282
Epoch 28: training loss 68179949929.412
Test Loss of 65721293563.203331, Test MSE of 65721294296.710182
Epoch 29: training loss 65183931000.471
Test Loss of 62578908182.502892, Test MSE of 62578909467.131561
Epoch 30: training loss 61090098672.941
Test Loss of 60022037277.312981, Test MSE of 60022039011.037430
Epoch 31: training loss 57807991808.000
Test Loss of 56995869368.405273, Test MSE of 56995871385.036522
Epoch 32: training loss 56046723403.294
Test Loss of 55992427184.351608, Test MSE of 55992426681.247398
Epoch 33: training loss 52478136402.824
Test Loss of 49919430132.037933, Test MSE of 49919430282.750580
Epoch 34: training loss 50230037232.941
Test Loss of 48330891102.452927, Test MSE of 48330891767.274803
Epoch 35: training loss 47178627599.059
Test Loss of 45225724882.283600, Test MSE of 45225724987.284264
Epoch 36: training loss 43967421763.765
Test Loss of 44912576656.965996, Test MSE of 44912575655.639664
Epoch 37: training loss 42131217008.941
Test Loss of 43405694592.266479, Test MSE of 43405695259.001747
Epoch 38: training loss 39279103405.176
Test Loss of 40908914115.242195, Test MSE of 40908913927.822906
Epoch 39: training loss 37740263175.529
Test Loss of 37844390193.091835, Test MSE of 37844389145.514801
Epoch 40: training loss 35156383179.294
Test Loss of 37893907045.026138, Test MSE of 37893906763.534676
Epoch 41: training loss 33616122849.882
Test Loss of 35464338255.529953, Test MSE of 35464337042.343399
Epoch 42: training loss 31517940728.471
Test Loss of 32448212204.635670, Test MSE of 32448211888.155571
Epoch 43: training loss 29570627117.176
Test Loss of 28693166265.234329, Test MSE of 28693166602.647675
Epoch 44: training loss 28237183111.529
Test Loss of 29972553304.471893, Test MSE of 29972554080.425602
Epoch 45: training loss 26946539090.824
Test Loss of 32407549322.866528, Test MSE of 32407549377.740070
Epoch 46: training loss 25151464568.471
Test Loss of 32830510926.345592, Test MSE of 32830510308.928001
Epoch 47: training loss 23736679303.529
Test Loss of 29281485870.190147, Test MSE of 29281486936.450172
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24467535725.24624, 'MSE - std': 4813951211.203936, 'R2 - mean': 0.8189525145375842, 'R2 - std': 0.02800289045386889} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002423 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927259798.588
Test Loss of 447258365282.124451, Test MSE of 447258367462.284912
Epoch 2: training loss 421906612464.941
Test Loss of 447239232620.724487, Test MSE of 447239237282.398132
Epoch 3: training loss 421878921456.941
Test Loss of 447214329804.835510, Test MSE of 447214335876.203003
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896840613.647
Test Loss of 447222717776.122131, Test MSE of 447222720136.913269
Epoch 2: training loss 421887029488.941
Test Loss of 447224309635.405029, Test MSE of 447224305876.571594
Epoch 3: training loss 421886550136.471
Test Loss of 447223901259.562317, Test MSE of 447223897125.906006
Epoch 4: training loss 421886181616.941
Test Loss of 447223327035.040466, Test MSE of 447223329426.478882
Epoch 5: training loss 414669571011.765
Test Loss of 424236277614.086487, Test MSE of 424236281342.621704
Epoch 6: training loss 368490629120.000
Test Loss of 356496094562.598206, Test MSE of 356496103815.138977
Epoch 7: training loss 289908588303.059
Test Loss of 269812994204.335876, Test MSE of 269812995086.532623
Epoch 8: training loss 211807029187.765
Test Loss of 197663533803.096008, Test MSE of 197663535906.471619
Epoch 9: training loss 153247927205.647
Test Loss of 148535871304.660645, Test MSE of 148535872189.651123
Epoch 10: training loss 133614038166.588
Test Loss of 140437721024.518158, Test MSE of 140437722987.700531
Epoch 11: training loss 132286264470.588
Test Loss of 137078146589.253754, Test MSE of 137078143888.317001
Epoch 12: training loss 127706974689.882
Test Loss of 134759631024.706909, Test MSE of 134759632651.936935
Epoch 13: training loss 124961864854.588
Test Loss of 130432567593.985657, Test MSE of 130432567704.634338
Epoch 14: training loss 121592582204.235
Test Loss of 127174267608.619934, Test MSE of 127174270805.231949
Epoch 15: training loss 118578343424.000
Test Loss of 123259470343.224609, Test MSE of 123259472146.896713
Epoch 16: training loss 113645329347.765
Test Loss of 120325582411.207031, Test MSE of 120325581158.935455
Epoch 17: training loss 110715744346.353
Test Loss of 116226496568.375671, Test MSE of 116226496666.817215
Epoch 18: training loss 106134085903.059
Test Loss of 110799472521.089981, Test MSE of 110799473148.740997
Epoch 19: training loss 102408202511.059
Test Loss of 108807305729.065933, Test MSE of 108807306960.007248
Epoch 20: training loss 98824953584.941
Test Loss of 107451772540.476517, Test MSE of 107451771707.289505
Epoch 21: training loss 94500425351.529
Test Loss of 100456439449.848709, Test MSE of 100456438500.758881
Epoch 22: training loss 91087579572.706
Test Loss of 98405862570.311356, Test MSE of 98405863959.306351
Epoch 23: training loss 87730673242.353
Test Loss of 94189120651.991669, Test MSE of 94189120803.466843
Epoch 24: training loss 83440451749.647
Test Loss of 88956708612.441360, Test MSE of 88956710977.173096
Epoch 25: training loss 80335757101.176
Test Loss of 85686729643.436508, Test MSE of 85686728776.434937
Epoch 26: training loss 77107827531.294
Test Loss of 84470147705.397171, Test MSE of 84470149084.371414
Epoch 27: training loss 73763540329.412
Test Loss of 80779814586.537125, Test MSE of 80779814317.193329
Epoch 28: training loss 69746692547.765
Test Loss of 77668262886.654633, Test MSE of 77668264839.842224
Epoch 29: training loss 67223339414.588
Test Loss of 71401061901.620178, Test MSE of 71401061943.203979
Epoch 30: training loss 63200099614.118
Test Loss of 68179390680.975250, Test MSE of 68179390328.614128
Epoch 31: training loss 60390284619.294
Test Loss of 66928744100.744850, Test MSE of 66928743703.280968
Epoch 32: training loss 58809811576.471
Test Loss of 63539008976.507057, Test MSE of 63539009688.765488
Epoch 33: training loss 54829819166.118
Test Loss of 59729971575.679855, Test MSE of 59729971087.979675
Epoch 34: training loss 52919226699.294
Test Loss of 57118741295.789032, Test MSE of 57118741483.046646
Epoch 35: training loss 49368488576.000
Test Loss of 53770798647.783485, Test MSE of 53770798304.602058
Epoch 36: training loss 46955040956.235
Test Loss of 53763757291.214432, Test MSE of 53763757383.454544
Epoch 37: training loss 44476552545.882
Test Loss of 47543179861.392555, Test MSE of 47543179551.594521
Epoch 38: training loss 42436563260.235
Test Loss of 42763380991.348602, Test MSE of 42763382113.784584
Epoch 39: training loss 40200398697.412
Test Loss of 45050887301.359238, Test MSE of 45050886752.094658
Epoch 40: training loss 38194001912.471
Test Loss of 44104716563.719643, Test MSE of 44104716894.378136
Epoch 41: training loss 36310773639.529
Test Loss of 39634554819.360626, Test MSE of 39634555564.609138
Epoch 42: training loss 34268460483.765
Test Loss of 37341857434.322464, Test MSE of 37341857613.339592
Epoch 43: training loss 32858296244.706
Test Loss of 37485490696.172104, Test MSE of 37485490553.980766
Epoch 44: training loss 30884710671.059
Test Loss of 35648130769.750633, Test MSE of 35648131299.707809
Epoch 45: training loss 29546155949.176
Test Loss of 36220561795.049736, Test MSE of 36220562804.406570
Epoch 46: training loss 28200650744.471
Test Loss of 30684928977.336109, Test MSE of 30684928749.606022
Epoch 47: training loss 26934329712.941
Test Loss of 30506254092.495026, Test MSE of 30506254752.876385
Epoch 48: training loss 25384703525.647
Test Loss of 30417822161.217674, Test MSE of 30417822722.293503
Epoch 49: training loss 24106701865.412
Test Loss of 29088058129.943096, Test MSE of 29088058483.529427
Epoch 50: training loss 23127406381.176
Test Loss of 27766220173.709000, Test MSE of 27766219972.342194
Epoch 51: training loss 22159879902.118
Test Loss of 28538561703.468887, Test MSE of 28538561701.128605
Epoch 52: training loss 21561993566.118
Test Loss of 29663160761.056675, Test MSE of 29663160600.261616
Epoch 53: training loss 20556817291.294
Test Loss of 25674029204.282211, Test MSE of 25674029225.652641
Epoch 54: training loss 19867734433.882
Test Loss of 25569615018.311359, Test MSE of 25569615143.001575
Epoch 55: training loss 18813551883.294
Test Loss of 25926309439.363403, Test MSE of 25926309656.678379
Epoch 56: training loss 18280399646.118
Test Loss of 26260193820.543140, Test MSE of 26260194062.773155
Epoch 57: training loss 18015327661.176
Test Loss of 23516784546.198475, Test MSE of 23516784691.638714
Epoch 58: training loss 17677793024.000
Test Loss of 22365956103.579922, Test MSE of 22365955770.131557
Epoch 59: training loss 16521094102.588
Test Loss of 25353748686.552856, Test MSE of 25353748391.235882
Epoch 60: training loss 16148673242.353
Test Loss of 23009801517.065002, Test MSE of 23009801531.892620
Epoch 61: training loss 15768122300.235
Test Loss of 24877994048.903076, Test MSE of 24877994169.741673
Epoch 62: training loss 15034334633.412
Test Loss of 23363369914.596344, Test MSE of 23363370459.501041
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24099480636.664505, 'MSE - std': 3964889229.940273, 'R2 - mean': 0.827458871257512, 'R2 - std': 0.025835843160376612} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005483 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110735420.235
Test Loss of 410767037384.559021, Test MSE of 410767038878.554016
Epoch 2: training loss 430089963399.529
Test Loss of 410748151916.986572, Test MSE of 410748156053.582703
Epoch 3: training loss 430062484540.235
Test Loss of 410724535317.323486, Test MSE of 410724541804.780151
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076180600.471
Test Loss of 410729399477.486328, Test MSE of 410729401666.162659
Epoch 2: training loss 430067428653.176
Test Loss of 410730195234.946777, Test MSE of 410730200682.186584
Epoch 3: training loss 430067070614.588
Test Loss of 410729834195.576111, Test MSE of 410729836274.800903
Epoch 4: training loss 430066765101.176
Test Loss of 410729770918.915344, Test MSE of 410729771773.376160
Epoch 5: training loss 423220313871.059
Test Loss of 388972307512.388733, Test MSE of 388972308920.537048
Epoch 6: training loss 377975276845.176
Test Loss of 322422218065.858398, Test MSE of 322422211401.249268
Epoch 7: training loss 298982223028.706
Test Loss of 238124287263.629791, Test MSE of 238124286293.132263
Epoch 8: training loss 220474911924.706
Test Loss of 167861358438.471069, Test MSE of 167861359894.202850
Epoch 9: training loss 161373541797.647
Test Loss of 120554783144.573807, Test MSE of 120554781785.978256
Epoch 10: training loss 143276622998.588
Test Loss of 112844423964.075897, Test MSE of 112844422498.248474
Epoch 11: training loss 137974981812.706
Test Loss of 109801552244.449799, Test MSE of 109801552717.659470
Epoch 12: training loss 136594346917.647
Test Loss of 106978770659.687180, Test MSE of 106978769704.845978
Epoch 13: training loss 131643658992.941
Test Loss of 104027256264.322067, Test MSE of 104027255449.315460
Epoch 14: training loss 127667032997.647
Test Loss of 101075667128.803329, Test MSE of 101075664969.845703
Epoch 15: training loss 122572514725.647
Test Loss of 98146462036.227676, Test MSE of 98146462211.724884
Epoch 16: training loss 120501168700.235
Test Loss of 95280895841.732529, Test MSE of 95280895576.009338
Epoch 17: training loss 116560117519.059
Test Loss of 91370472963.080063, Test MSE of 91370473191.320892
Epoch 18: training loss 112289161547.294
Test Loss of 87329142072.270248, Test MSE of 87329142147.228607
Epoch 19: training loss 108700965496.471
Test Loss of 85814959976.366501, Test MSE of 85814958498.916138
Epoch 20: training loss 103774281456.941
Test Loss of 84159070371.006012, Test MSE of 84159070628.151108
Epoch 21: training loss 99619219395.765
Test Loss of 80482747581.068024, Test MSE of 80482747372.804214
Epoch 22: training loss 95984079164.235
Test Loss of 77576719540.538635, Test MSE of 77576718935.243866
Epoch 23: training loss 93675530541.176
Test Loss of 74121466176.325775, Test MSE of 74121465570.017212
Epoch 24: training loss 89554651256.471
Test Loss of 71308067744.755203, Test MSE of 71308067858.691528
Epoch 25: training loss 85863826477.176
Test Loss of 69743127732.538635, Test MSE of 69743127173.932632
Epoch 26: training loss 81356783510.588
Test Loss of 65450330490.136047, Test MSE of 65450331394.398613
Epoch 27: training loss 79013325387.294
Test Loss of 62260374280.173996, Test MSE of 62260376355.831345
Epoch 28: training loss 75313680956.235
Test Loss of 59019044142.793152, Test MSE of 59019044688.803368
Epoch 29: training loss 71602826300.235
Test Loss of 57041376454.545120, Test MSE of 57041376567.175644
Epoch 30: training loss 67693003715.765
Test Loss of 53534747474.095329, Test MSE of 53534745837.569801
Epoch 31: training loss 64660237733.647
Test Loss of 53010037307.468765, Test MSE of 53010037203.777840
Epoch 32: training loss 61806229067.294
Test Loss of 49624566232.906990, Test MSE of 49624566926.705055
Epoch 33: training loss 59262300784.941
Test Loss of 46183286125.341972, Test MSE of 46183286710.864723
Epoch 34: training loss 55986695431.529
Test Loss of 45315249682.243408, Test MSE of 45315249245.086967
Epoch 35: training loss 53588673520.941
Test Loss of 43652634460.046272, Test MSE of 43652634647.343208
Epoch 36: training loss 50825530624.000
Test Loss of 40068740882.124939, Test MSE of 40068739731.564613
Epoch 37: training loss 48463304907.294
Test Loss of 38135465107.368813, Test MSE of 38135465521.202080
Epoch 38: training loss 45915206979.765
Test Loss of 37521458501.538177, Test MSE of 37521458873.860519
Epoch 39: training loss 43450214128.941
Test Loss of 36465069414.708008, Test MSE of 36465068571.690628
Epoch 40: training loss 40660967928.471
Test Loss of 35027292783.592781, Test MSE of 35027291836.200661
Epoch 41: training loss 40004044190.118
Test Loss of 32146599009.140213, Test MSE of 32146599668.878155
Epoch 42: training loss 37323055375.059
Test Loss of 32629803543.455807, Test MSE of 32629803403.340603
Epoch 43: training loss 34585482608.941
Test Loss of 30874104967.996300, Test MSE of 30874105210.296486
Epoch 44: training loss 33441609524.706
Test Loss of 28799750068.183247, Test MSE of 28799749465.474419
Epoch 45: training loss 32600207804.235
Test Loss of 28188902302.859787, Test MSE of 28188902182.260529
Epoch 46: training loss 30625377174.588
Test Loss of 27897061719.070801, Test MSE of 27897062325.392189
Epoch 47: training loss 29167048436.706
Test Loss of 27201379870.563629, Test MSE of 27201379575.664814
Epoch 48: training loss 27544160993.882
Test Loss of 24626264384.325775, Test MSE of 24626265148.531055
Epoch 49: training loss 26535255792.941
Test Loss of 23498211356.431282, Test MSE of 23498211740.117908
Epoch 50: training loss 25515285733.647
Test Loss of 23471656455.818604, Test MSE of 23471656321.654060
Epoch 51: training loss 24187387809.882
Test Loss of 23069392668.549744, Test MSE of 23069392608.108551
Epoch 52: training loss 23301170763.294
Test Loss of 19748796034.073112, Test MSE of 19748795861.888790
Epoch 53: training loss 22305723294.118
Test Loss of 23916187167.511337, Test MSE of 23916186588.577530
Epoch 54: training loss 21462831518.118
Test Loss of 22670793445.108746, Test MSE of 22670793587.327908
Epoch 55: training loss 20696101637.647
Test Loss of 21466599899.750114, Test MSE of 21466599713.590855
Epoch 56: training loss 20381103273.412
Test Loss of 20507805164.335030, Test MSE of 20507805405.487240
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23201561828.87019, 'MSE - std': 3769487300.74059, 'R2 - mean': 0.8282787519738755, 'R2 - std': 0.022419516208492942} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005591 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042914635.294
Test Loss of 431613230019.820435, Test MSE of 431613227545.937622
Epoch 2: training loss 424022175503.059
Test Loss of 431591535147.357727, Test MSE of 431591537888.716248
Epoch 3: training loss 423994046223.059
Test Loss of 431562688844.645996, Test MSE of 431562688797.855042
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010596713.412
Test Loss of 431569017738.957886, Test MSE of 431569028040.867920
Epoch 2: training loss 423997636728.471
Test Loss of 431570899904.977295, Test MSE of 431570903651.355835
Epoch 3: training loss 423996901616.941
Test Loss of 431570098840.344299, Test MSE of 431570097715.076233
Epoch 4: training loss 423996418529.882
Test Loss of 431569663257.943542, Test MSE of 431569667803.231262
Epoch 5: training loss 417191539531.294
Test Loss of 409457736022.123108, Test MSE of 409457734072.568604
Epoch 6: training loss 371786630565.647
Test Loss of 341446675591.048584, Test MSE of 341446677564.347168
Epoch 7: training loss 293472741315.765
Test Loss of 255040923044.309113, Test MSE of 255040917935.771973
Epoch 8: training loss 215170477537.882
Test Loss of 183769678043.868591, Test MSE of 183769677806.000031
Epoch 9: training loss 156755415401.412
Test Loss of 133008569834.439606, Test MSE of 133008569811.042435
Epoch 10: training loss 137905105438.118
Test Loss of 124198912310.848679, Test MSE of 124198913756.935913
Epoch 11: training loss 133151559092.706
Test Loss of 120830318638.437759, Test MSE of 120830316396.683121
Epoch 12: training loss 131784197782.588
Test Loss of 117872016426.173065, Test MSE of 117872018925.204651
Epoch 13: training loss 128115824850.824
Test Loss of 114992032823.440994, Test MSE of 114992034283.998077
Epoch 14: training loss 124701179241.412
Test Loss of 111031744448.503464, Test MSE of 111031742783.106461
Epoch 15: training loss 119563508028.235
Test Loss of 108153284129.406754, Test MSE of 108153282609.656204
Epoch 16: training loss 116393476833.882
Test Loss of 103485582580.982880, Test MSE of 103485583097.273315
Epoch 17: training loss 113144943299.765
Test Loss of 100189529849.484497, Test MSE of 100189529046.313187
Epoch 18: training loss 107990976271.059
Test Loss of 96564670970.076813, Test MSE of 96564672841.552124
Epoch 19: training loss 104881212235.294
Test Loss of 94747752202.543274, Test MSE of 94747751576.987366
Epoch 20: training loss 101267116197.647
Test Loss of 89878805948.001846, Test MSE of 89878803486.582825
Epoch 21: training loss 97809131821.176
Test Loss of 87058157721.528915, Test MSE of 87058157704.072922
Epoch 22: training loss 93944371983.059
Test Loss of 82372891323.409531, Test MSE of 82372891267.706589
Epoch 23: training loss 89604319646.118
Test Loss of 78292437370.136047, Test MSE of 78292435112.778320
Epoch 24: training loss 85560773044.706
Test Loss of 74716032699.883392, Test MSE of 74716032221.762878
Epoch 25: training loss 81822418326.588
Test Loss of 73338014661.715866, Test MSE of 73338014031.132660
Epoch 26: training loss 78879420777.412
Test Loss of 72398844692.494217, Test MSE of 72398844453.861603
Epoch 27: training loss 75713973955.765
Test Loss of 63304621009.088387, Test MSE of 63304621025.206169
Epoch 28: training loss 71269714816.000
Test Loss of 62686016348.520126, Test MSE of 62686017943.330894
Epoch 29: training loss 68112579840.000
Test Loss of 63391733304.151779, Test MSE of 63391734913.314125
Epoch 30: training loss 65725360655.059
Test Loss of 55603267410.569183, Test MSE of 55603267285.871918
Epoch 31: training loss 61998002703.059
Test Loss of 56101247460.279503, Test MSE of 56101248027.943909
Epoch 32: training loss 58826086580.706
Test Loss of 50739753911.500229, Test MSE of 50739753578.767265
Epoch 33: training loss 56655639868.235
Test Loss of 45673424258.191582, Test MSE of 45673423798.153793
Epoch 34: training loss 53119523358.118
Test Loss of 44082563063.470619, Test MSE of 44082561939.589973
Epoch 35: training loss 50838014080.000
Test Loss of 43691126524.327629, Test MSE of 43691125602.162422
Epoch 36: training loss 48964571200.000
Test Loss of 42435042454.211937, Test MSE of 42435042068.302971
Epoch 37: training loss 45666345456.941
Test Loss of 39664854021.212402, Test MSE of 39664853893.482269
Epoch 38: training loss 43375545607.529
Test Loss of 39566113523.324387, Test MSE of 39566112493.957298
Epoch 39: training loss 41416441803.294
Test Loss of 36920138924.956963, Test MSE of 36920139756.727531
Epoch 40: training loss 40139359879.529
Test Loss of 34725534530.931976, Test MSE of 34725535358.114952
Epoch 41: training loss 37470203218.824
Test Loss of 30200481028.146229, Test MSE of 30200480954.431446
Epoch 42: training loss 35295302565.647
Test Loss of 31037468176.347988, Test MSE of 31037468214.133915
Epoch 43: training loss 33671703981.176
Test Loss of 26548110284.349838, Test MSE of 26548110411.432854
Epoch 44: training loss 32148497611.294
Test Loss of 29245116050.184174, Test MSE of 29245116398.045063
Epoch 45: training loss 31312237349.647
Test Loss of 28789545160.914391, Test MSE of 28789545415.646286
Epoch 46: training loss 29539977494.588
Test Loss of 25956678123.861176, Test MSE of 25956678321.890114
Epoch 47: training loss 27987758607.059
Test Loss of 25517022143.081905, Test MSE of 25517022345.090424
Epoch 48: training loss 26685134460.235
Test Loss of 24627344273.591854, Test MSE of 24627344842.132431
Epoch 49: training loss 25812718750.118
Test Loss of 23622129757.823231, Test MSE of 23622129680.456615
Epoch 50: training loss 24284111126.588
Test Loss of 25258679833.825081, Test MSE of 25258680229.522190
Epoch 51: training loss 23503337543.529
Test Loss of 21934098497.391949, Test MSE of 21934098602.474937
Epoch 52: training loss 22785782550.588
Test Loss of 24523807876.205460, Test MSE of 24523807305.473877
Epoch 53: training loss 21676938447.059
Test Loss of 23307585039.400276, Test MSE of 23307584937.802963
Epoch 54: training loss 20950839247.059
Test Loss of 24295519665.577049, Test MSE of 24295519495.332325
Epoch 55: training loss 20225761400.471
Test Loss of 21769781513.358631, Test MSE of 21769781546.368176
Epoch 56: training loss 19325101974.588
Test Loss of 21652666616.299862, Test MSE of 21652666568.582794
Epoch 57: training loss 19169802887.529
Test Loss of 22584961045.797318, Test MSE of 22584961353.490433
Epoch 58: training loss 18269468555.294
Test Loss of 21691529649.103191, Test MSE of 21691529537.207767
Epoch 59: training loss 17636578070.588
Test Loss of 20595241202.613605, Test MSE of 20595241085.237137
Epoch 60: training loss 17277976715.294
Test Loss of 20020556806.160110, Test MSE of 20020557197.459068
Epoch 61: training loss 16667239951.059
Test Loss of 20239557877.930588, Test MSE of 20239557954.093540
Epoch 62: training loss 16227173199.059
Test Loss of 22162404384.695972, Test MSE of 22162404997.841434
Epoch 63: training loss 15665870110.118
Test Loss of 19339996563.250347, Test MSE of 19339996662.994591
Epoch 64: training loss 15802196634.353
Test Loss of 20840479538.820915, Test MSE of 20840480060.325623
Epoch 65: training loss 15182426127.059
Test Loss of 21816775569.591854, Test MSE of 21816775511.136765
Epoch 66: training loss 14521675668.706
Test Loss of 19304154904.758907, Test MSE of 19304154765.609352
Epoch 67: training loss 14319075147.294
Test Loss of 20342985038.541416, Test MSE of 20342985240.084312
Epoch 68: training loss 14068786560.000
Test Loss of 21788401778.672836, Test MSE of 21788401924.497360
Epoch 69: training loss 13727139809.882
Test Loss of 18469766121.728828, Test MSE of 18469765909.604912
Epoch 70: training loss 13379705633.882
Test Loss of 20215880563.265156, Test MSE of 20215880441.232494
Epoch 71: training loss 13209966817.882
Test Loss of 19672556507.513187, Test MSE of 19672556458.762932
Epoch 72: training loss 12528799808.000
Test Loss of 19034414846.223045, Test MSE of 19034414892.364925
Epoch 73: training loss 12499268122.353
Test Loss of 20446343594.469227, Test MSE of 20446343419.831970
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22650518147.062546, 'MSE - std': 3547086715.234935, 'R2 - mean': 0.8320841912239884, 'R2 - std': 0.021448385420900575} 
 

Saving model.....
Results After CV: {'MSE - mean': 22650518147.062546, 'MSE - std': 3547086715.234935, 'R2 - mean': 0.8320841912239884, 'R2 - std': 0.021448385420900575}
Train time: 95.90676169240032
Inference time: 0.07489745959974244
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 75 finished with value: 22650518147.062546 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005502 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526821285.647
Test Loss of 418113234133.422180, Test MSE of 418113236309.485168
Epoch 2: training loss 427507313001.412
Test Loss of 418095149737.245422, Test MSE of 418095149625.120117
Epoch 3: training loss 427480867418.353
Test Loss of 418071497274.862854, Test MSE of 418071500701.410828
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495022712.471
Test Loss of 418077852955.536438, Test MSE of 418077854155.057617
Epoch 2: training loss 427484292517.647
Test Loss of 418079066758.662048, Test MSE of 418079073350.873474
Epoch 3: training loss 427483814128.941
Test Loss of 418077494667.814026, Test MSE of 418077498725.151550
Epoch 4: training loss 422143703280.941
Test Loss of 401182984615.054382, Test MSE of 401182986280.091248
Epoch 5: training loss 385703743849.412
Test Loss of 346781930412.857727, Test MSE of 346781929102.950623
Epoch 6: training loss 317889508592.941
Test Loss of 269659322547.549377, Test MSE of 269659322744.406616
Epoch 7: training loss 226443121483.294
Test Loss of 171307428208.099945, Test MSE of 171307432798.373779
Epoch 8: training loss 161680338040.471
Test Loss of 131773198354.002319, Test MSE of 131773200952.124252
Epoch 9: training loss 142399266273.882
Test Loss of 120524978723.175568, Test MSE of 120524979258.492218
Epoch 10: training loss 137310030336.000
Test Loss of 116817929075.060837, Test MSE of 116817930135.774658
Epoch 11: training loss 133755526068.706
Test Loss of 114076840397.427719, Test MSE of 114076840710.193771
Epoch 12: training loss 130809394657.882
Test Loss of 111213416784.359009, Test MSE of 111213415094.275452
Epoch 13: training loss 127323820272.941
Test Loss of 106950425615.870453, Test MSE of 106950425755.080582
Epoch 14: training loss 123286104801.882
Test Loss of 104251866588.350677, Test MSE of 104251867854.062714
Epoch 15: training loss 118993978774.588
Test Loss of 101464630076.106415, Test MSE of 101464632006.936432
Epoch 16: training loss 116662669086.118
Test Loss of 97957025626.662964, Test MSE of 97957026978.421066
Epoch 17: training loss 111643154597.647
Test Loss of 94421103584.022202, Test MSE of 94421103610.006851
Epoch 18: training loss 108666774603.294
Test Loss of 91733723087.204254, Test MSE of 91733722717.467072
Epoch 19: training loss 104497670595.765
Test Loss of 88942411402.688873, Test MSE of 88942411986.091782
Epoch 20: training loss 101193439969.882
Test Loss of 85606329133.420303, Test MSE of 85606326703.551437
Epoch 21: training loss 96761664210.824
Test Loss of 82711134418.342819, Test MSE of 82711135583.080780
Epoch 22: training loss 93921642526.118
Test Loss of 79083456988.113815, Test MSE of 79083457717.035706
Epoch 23: training loss 89128399585.882
Test Loss of 75012298120.497803, Test MSE of 75012296970.738373
Epoch 24: training loss 86769262080.000
Test Loss of 71030566052.626419, Test MSE of 71030565846.508820
Epoch 25: training loss 82919261108.706
Test Loss of 68943267556.226700, Test MSE of 68943267231.953812
Epoch 26: training loss 80019801825.882
Test Loss of 66851670620.972473, Test MSE of 66851672011.861382
Epoch 27: training loss 76200985984.000
Test Loss of 63447273490.002312, Test MSE of 63447272472.709366
Epoch 28: training loss 72419465750.588
Test Loss of 62206997689.944946, Test MSE of 62206998253.686951
Epoch 29: training loss 68916444476.235
Test Loss of 60591749648.225769, Test MSE of 60591749747.498833
Epoch 30: training loss 66901241916.235
Test Loss of 56637614697.289848, Test MSE of 56637615967.976112
Epoch 31: training loss 63351585972.706
Test Loss of 52671009331.756653, Test MSE of 52671008745.679108
Epoch 32: training loss 60300266601.412
Test Loss of 51387738593.798752, Test MSE of 51387738824.304581
Epoch 33: training loss 56914440131.765
Test Loss of 48244497844.556099, Test MSE of 48244497720.154030
Epoch 34: training loss 54948447201.882
Test Loss of 48278000242.291000, Test MSE of 48277998543.494423
Epoch 35: training loss 52063517549.176
Test Loss of 44354127615.703911, Test MSE of 44354127676.359505
Epoch 36: training loss 50237945961.412
Test Loss of 41022897956.182281, Test MSE of 41022898151.848778
Epoch 37: training loss 47920550106.353
Test Loss of 39235111243.147812, Test MSE of 39235111228.851944
Epoch 38: training loss 44902595245.176
Test Loss of 35568002264.264633, Test MSE of 35568002154.849762
Epoch 39: training loss 43426401543.529
Test Loss of 35243006147.656723, Test MSE of 35243005200.342545
Epoch 40: training loss 40713737560.471
Test Loss of 37071478825.689568, Test MSE of 37071479341.271797
Epoch 41: training loss 39401852687.059
Test Loss of 31766463745.006710, Test MSE of 31766463839.112396
Epoch 42: training loss 37251093274.353
Test Loss of 33401844413.142723, Test MSE of 33401844138.094131
Epoch 43: training loss 35624459233.882
Test Loss of 31792446991.515152, Test MSE of 31792447097.834423
Epoch 44: training loss 34303796491.294
Test Loss of 29584899441.284294, Test MSE of 29584900155.985073
Epoch 45: training loss 32489473400.471
Test Loss of 28857320363.910248, Test MSE of 28857320221.325954
Epoch 46: training loss 30824399954.824
Test Loss of 27908487754.496414, Test MSE of 27908487765.298824
Epoch 47: training loss 29407448967.529
Test Loss of 27637755641.545223, Test MSE of 27637755937.521736
Epoch 48: training loss 28236865611.294
Test Loss of 25513807775.356003, Test MSE of 25513807798.448296
Epoch 49: training loss 27170914032.941
Test Loss of 27192330829.812630, Test MSE of 27192330659.533855
Epoch 50: training loss 26209868803.765
Test Loss of 25319864618.933147, Test MSE of 25319864853.313560
Epoch 51: training loss 24853831476.706
Test Loss of 24010337154.694424, Test MSE of 24010336798.617729
Epoch 52: training loss 24408720602.353
Test Loss of 24876761764.507980, Test MSE of 24876761885.958298
Epoch 53: training loss 23299986789.647
Test Loss of 23274144446.563961, Test MSE of 23274144889.414516
Epoch 54: training loss 22023223928.471
Test Loss of 24145568028.720795, Test MSE of 24145568132.935768
Epoch 55: training loss 21711566008.471
Test Loss of 22220626048.384918, Test MSE of 22220626138.166256
Epoch 56: training loss 20634060999.529
Test Loss of 20527978361.456394, Test MSE of 20527978800.766781
Epoch 57: training loss 19867791529.412
Test Loss of 21311867278.893360, Test MSE of 21311867357.868584
Epoch 58: training loss 19513191397.647
Test Loss of 19554364187.417995, Test MSE of 19554364199.842854
Epoch 59: training loss 18915649999.059
Test Loss of 19742948457.408279, Test MSE of 19742947990.740768
Epoch 60: training loss 17785556867.765
Test Loss of 18347444467.031227, Test MSE of 18347444321.702278
Epoch 61: training loss 17529546469.647
Test Loss of 18328574782.948879, Test MSE of 18328574885.680027
Epoch 62: training loss 16890955079.529
Test Loss of 19517678573.287067, Test MSE of 19517678416.208397
Epoch 63: training loss 16441859919.059
Test Loss of 19876847647.267174, Test MSE of 19876847882.713558
Epoch 64: training loss 16127613368.471
Test Loss of 19149325500.787415, Test MSE of 19149325566.786942
Epoch 65: training loss 15493592587.294
Test Loss of 17867610191.115429, Test MSE of 17867609893.454834
Epoch 66: training loss 15288198904.471
Test Loss of 18891242188.776314, Test MSE of 18891242272.785732
Epoch 67: training loss 15007707602.824
Test Loss of 19468012728.049965, Test MSE of 19468012341.962479
Epoch 68: training loss 14522347294.118
Test Loss of 18815906452.874393, Test MSE of 18815906191.290806
Epoch 69: training loss 14032531738.353
Test Loss of 19500071337.896832, Test MSE of 19500071595.841373
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19500071595.841373, 'MSE - std': 0.0, 'R2 - mean': 0.8481508267415088, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005543 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918336000.000
Test Loss of 424557217563.181152, Test MSE of 424557218962.938904
Epoch 2: training loss 427897554582.588
Test Loss of 424540973329.350891, Test MSE of 424540978529.455933
Epoch 3: training loss 427869864899.765
Test Loss of 424518858529.813538, Test MSE of 424518856840.547791
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886512730.353
Test Loss of 424525820172.139709, Test MSE of 424525809402.975403
Epoch 2: training loss 427878178816.000
Test Loss of 424526184106.903564, Test MSE of 424526186818.069702
Epoch 3: training loss 427877642480.941
Test Loss of 424525628854.924805, Test MSE of 424525633934.684692
Epoch 4: training loss 422491531866.353
Test Loss of 407836463328.555176, Test MSE of 407836463864.724792
Epoch 5: training loss 385891186688.000
Test Loss of 353337493256.231323, Test MSE of 353337490720.786743
Epoch 6: training loss 317242841088.000
Test Loss of 276832000780.258179, Test MSE of 276832000518.216187
Epoch 7: training loss 224698936018.824
Test Loss of 180141600456.275726, Test MSE of 180141602538.345612
Epoch 8: training loss 160227852619.294
Test Loss of 142233524583.572510, Test MSE of 142233522333.043549
Epoch 9: training loss 139996663265.882
Test Loss of 132249259237.055756, Test MSE of 132249257885.059677
Epoch 10: training loss 134348461266.824
Test Loss of 127982347251.919495, Test MSE of 127982346911.565460
Epoch 11: training loss 130613944500.706
Test Loss of 124933458542.027298, Test MSE of 124933459651.850220
Epoch 12: training loss 128204315166.118
Test Loss of 122051929862.336334, Test MSE of 122051928660.894913
Epoch 13: training loss 123648914401.882
Test Loss of 119281446314.133698, Test MSE of 119281444109.541122
Epoch 14: training loss 120277417532.235
Test Loss of 115730219650.398331, Test MSE of 115730217877.334549
Epoch 15: training loss 115468659109.647
Test Loss of 112046389535.563263, Test MSE of 112046393072.608002
Epoch 16: training loss 112547420491.294
Test Loss of 108001230490.796204, Test MSE of 108001231001.547531
Epoch 17: training loss 108634259727.059
Test Loss of 104077066025.867218, Test MSE of 104077065986.174225
Epoch 18: training loss 105146750162.824
Test Loss of 100161625320.845703, Test MSE of 100161624945.379654
Epoch 19: training loss 101984210552.471
Test Loss of 96802122895.781631, Test MSE of 96802122542.802429
Epoch 20: training loss 95829023412.706
Test Loss of 94409557543.676147, Test MSE of 94409558209.556931
Epoch 21: training loss 93527256756.706
Test Loss of 88871298344.801300, Test MSE of 88871298105.498764
Epoch 22: training loss 89447417374.118
Test Loss of 86635244552.290543, Test MSE of 86635243598.767319
Epoch 23: training loss 85775058552.471
Test Loss of 82011781971.319916, Test MSE of 82011782292.140930
Epoch 24: training loss 81948272865.882
Test Loss of 77754582268.269257, Test MSE of 77754581551.082550
Epoch 25: training loss 79143640937.412
Test Loss of 74002459328.458939, Test MSE of 74002461568.664093
Epoch 26: training loss 75252086226.824
Test Loss of 74460560759.916718, Test MSE of 74460562700.334824
Epoch 27: training loss 71801245500.235
Test Loss of 69164586387.867691, Test MSE of 69164584175.091110
Epoch 28: training loss 67687407826.824
Test Loss of 65254529598.652786, Test MSE of 65254528645.030495
Epoch 29: training loss 64715395312.941
Test Loss of 62501274938.803612, Test MSE of 62501275233.219597
Epoch 30: training loss 61410995877.647
Test Loss of 59555224732.099007, Test MSE of 59555224112.207268
Epoch 31: training loss 58577964227.765
Test Loss of 55395653870.293777, Test MSE of 55395654796.963867
Epoch 32: training loss 55160810179.765
Test Loss of 53850613569.080734, Test MSE of 53850611989.998001
Epoch 33: training loss 53150303141.647
Test Loss of 54057841698.346519, Test MSE of 54057842399.171211
Epoch 34: training loss 49483342283.294
Test Loss of 48182454161.617393, Test MSE of 48182451812.619308
Epoch 35: training loss 47094971038.118
Test Loss of 47547303136.318298, Test MSE of 47547302179.799225
Epoch 36: training loss 44815007969.882
Test Loss of 44520534468.426552, Test MSE of 44520534544.893242
Epoch 37: training loss 41965963045.647
Test Loss of 44006624230.180893, Test MSE of 44006623468.465378
Epoch 38: training loss 40486458036.706
Test Loss of 43795512166.743462, Test MSE of 43795512709.865158
Epoch 39: training loss 38280624143.059
Test Loss of 40733181949.157532, Test MSE of 40733182028.575500
Epoch 40: training loss 36188128519.529
Test Loss of 38337717480.371964, Test MSE of 38337718376.108963
Epoch 41: training loss 33919589398.588
Test Loss of 35484701023.045105, Test MSE of 35484701781.227959
Epoch 42: training loss 32024340562.824
Test Loss of 35716731779.168167, Test MSE of 35716732071.134529
Epoch 43: training loss 30558604581.647
Test Loss of 33126999525.351837, Test MSE of 33126998704.396572
Epoch 44: training loss 29602560617.412
Test Loss of 35422308983.028450, Test MSE of 35422308902.958496
Epoch 45: training loss 28247478671.059
Test Loss of 33520191739.084896, Test MSE of 33520191959.925949
Epoch 46: training loss 26504679695.059
Test Loss of 31666964581.855194, Test MSE of 31666964912.918957
Epoch 47: training loss 25047556080.941
Test Loss of 30042939463.061764, Test MSE of 30042940093.098457
Epoch 48: training loss 23731339414.588
Test Loss of 32768387547.166321, Test MSE of 32768387216.382229
Epoch 49: training loss 23046527563.294
Test Loss of 29502819717.655331, Test MSE of 29502819214.105698
Epoch 50: training loss 21748016711.529
Test Loss of 32474095706.485310, Test MSE of 32474094896.355133
Epoch 51: training loss 20699255875.765
Test Loss of 28839582755.530880, Test MSE of 28839583407.930988
Epoch 52: training loss 19883603041.882
Test Loss of 30555253046.303032, Test MSE of 30555253320.056362
Epoch 53: training loss 19275387290.353
Test Loss of 27392236288.414528, Test MSE of 27392235636.073402
Epoch 54: training loss 18554716905.412
Test Loss of 29618144397.176037, Test MSE of 29618144790.062527
Epoch 55: training loss 17863468638.118
Test Loss of 26373486843.084896, Test MSE of 26373487081.973881
Epoch 56: training loss 17185717680.941
Test Loss of 28548614211.982418, Test MSE of 28548613847.500237
Epoch 57: training loss 16329118302.118
Test Loss of 28422615411.889893, Test MSE of 28422615793.480824
Epoch 58: training loss 15941917925.647
Test Loss of 27146894986.925747, Test MSE of 27146895595.760563
Epoch 59: training loss 15636818085.647
Test Loss of 28908119283.031227, Test MSE of 28908118958.485691
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24204095277.163532, 'MSE - std': 4704023681.322159, 'R2 - mean': 0.8208830251491482, 'R2 - std': 0.02726780159236053} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005369 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927437733.647
Test Loss of 447258940461.716431, Test MSE of 447258935983.475952
Epoch 2: training loss 421907063627.294
Test Loss of 447240906646.354858, Test MSE of 447240915055.003601
Epoch 3: training loss 421880109296.941
Test Loss of 447216316350.623169, Test MSE of 447216329542.277649
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901263329.882
Test Loss of 447225918064.395996, Test MSE of 447225920594.266602
Epoch 2: training loss 421888794142.118
Test Loss of 447226602379.695557, Test MSE of 447226598958.493042
Epoch 3: training loss 421888244675.765
Test Loss of 447226145280.118408, Test MSE of 447226142123.717773
Epoch 4: training loss 416458899215.059
Test Loss of 429792568942.737915, Test MSE of 429792565288.437866
Epoch 5: training loss 379974509748.706
Test Loss of 374261214423.790894, Test MSE of 374261226050.545837
Epoch 6: training loss 311803830994.824
Test Loss of 295500959911.705750, Test MSE of 295500966285.589966
Epoch 7: training loss 220613633927.529
Test Loss of 196012053519.396729, Test MSE of 196012051974.969696
Epoch 8: training loss 157047495800.471
Test Loss of 153862938629.684937, Test MSE of 153862937962.976440
Epoch 9: training loss 136959621120.000
Test Loss of 142654694332.965057, Test MSE of 142654694365.168884
Epoch 10: training loss 133264690326.588
Test Loss of 137702834081.250977, Test MSE of 137702834232.970123
Epoch 11: training loss 128538407755.294
Test Loss of 133637650350.042099, Test MSE of 133637652630.661636
Epoch 12: training loss 126307572344.471
Test Loss of 130978657410.990509, Test MSE of 130978657807.260330
Epoch 13: training loss 123435979233.882
Test Loss of 127662527613.305573, Test MSE of 127662527133.932159
Epoch 14: training loss 118580337332.706
Test Loss of 124023022028.480225, Test MSE of 124023022089.051895
Epoch 15: training loss 114445232459.294
Test Loss of 121959077883.736298, Test MSE of 121959079326.619049
Epoch 16: training loss 112424097099.294
Test Loss of 116815081975.117279, Test MSE of 116815083454.857330
Epoch 17: training loss 106654343951.059
Test Loss of 115006621516.450607, Test MSE of 115006622712.238968
Epoch 18: training loss 103823419663.059
Test Loss of 110445417661.734909, Test MSE of 110445418405.211349
Epoch 19: training loss 99043712512.000
Test Loss of 106692675245.272263, Test MSE of 106692674643.509491
Epoch 20: training loss 96393505641.412
Test Loss of 102781394354.661118, Test MSE of 102781393981.449173
Epoch 21: training loss 92383396412.235
Test Loss of 97469019295.415222, Test MSE of 97469021888.554108
Epoch 22: training loss 88514737980.235
Test Loss of 95895716925.823730, Test MSE of 95895715790.197006
Epoch 23: training loss 84867666386.824
Test Loss of 89488408066.250290, Test MSE of 89488408398.729660
Epoch 24: training loss 80771511717.647
Test Loss of 87852125051.114502, Test MSE of 87852126303.657394
Epoch 25: training loss 77277698484.706
Test Loss of 83695742334.549149, Test MSE of 83695740920.215042
Epoch 26: training loss 73813670204.235
Test Loss of 80991902017.672913, Test MSE of 80991902110.888092
Epoch 27: training loss 71432922789.647
Test Loss of 75599562218.089294, Test MSE of 75599562258.382111
Epoch 28: training loss 67754588897.882
Test Loss of 72973317503.022903, Test MSE of 72973317793.149368
Epoch 29: training loss 64711604931.765
Test Loss of 69671287768.916031, Test MSE of 69671286944.585312
Epoch 30: training loss 61932700476.235
Test Loss of 65454688347.432800, Test MSE of 65454687799.956367
Epoch 31: training loss 58833174528.000
Test Loss of 62864608439.576218, Test MSE of 62864607682.617424
Epoch 32: training loss 55164593633.882
Test Loss of 60218374490.307655, Test MSE of 60218375068.393959
Epoch 33: training loss 53069545208.471
Test Loss of 54812785056.421928, Test MSE of 54812785803.662910
Epoch 34: training loss 50587175461.647
Test Loss of 54603561289.252831, Test MSE of 54603561602.728806
Epoch 35: training loss 47618101760.000
Test Loss of 53934453362.291000, Test MSE of 53934454341.002441
Epoch 36: training loss 45120097264.941
Test Loss of 50456303258.322464, Test MSE of 50456304115.725540
Epoch 37: training loss 42929087939.765
Test Loss of 44657858755.656723, Test MSE of 44657858680.157051
Epoch 38: training loss 40710655804.235
Test Loss of 44565398378.533424, Test MSE of 44565398014.509834
Epoch 39: training loss 38940001686.588
Test Loss of 43888170031.611382, Test MSE of 43888170361.727966
Epoch 40: training loss 37202002319.059
Test Loss of 40656982878.216057, Test MSE of 40656982879.269890
Epoch 41: training loss 35352005564.235
Test Loss of 40117160574.134628, Test MSE of 40117161055.530144
Epoch 42: training loss 33444315482.353
Test Loss of 38714498352.854965, Test MSE of 38714498614.334755
Epoch 43: training loss 31804196480.000
Test Loss of 34438447599.537361, Test MSE of 34438448034.299812
Epoch 44: training loss 30297509187.765
Test Loss of 33949143045.684940, Test MSE of 33949143542.932129
Epoch 45: training loss 29302615604.706
Test Loss of 35741215917.153831, Test MSE of 35741216093.594788
Epoch 46: training loss 27850628773.647
Test Loss of 33867717342.778625, Test MSE of 33867716943.981667
Epoch 47: training loss 26579150049.882
Test Loss of 33150686945.384224, Test MSE of 33150687206.418316
Epoch 48: training loss 25350349899.294
Test Loss of 30222653290.770298, Test MSE of 30222653392.013878
Epoch 49: training loss 24384878268.235
Test Loss of 30723342505.600739, Test MSE of 30723342976.525917
Epoch 50: training loss 23105047902.118
Test Loss of 27670200908.865139, Test MSE of 27670200879.689529
Epoch 51: training loss 22327214147.765
Test Loss of 26921070098.357620, Test MSE of 26921070078.003941
Epoch 52: training loss 21648737302.588
Test Loss of 27825049004.502430, Test MSE of 27825049446.736660
Epoch 53: training loss 20613862245.647
Test Loss of 27420630643.949108, Test MSE of 27420630801.462067
Epoch 54: training loss 20024063887.059
Test Loss of 25698799098.196621, Test MSE of 25698799354.566387
Epoch 55: training loss 18952285180.235
Test Loss of 28870897655.472588, Test MSE of 28870897968.480377
Epoch 56: training loss 18404234590.118
Test Loss of 28378488845.738609, Test MSE of 28378488707.339783
Epoch 57: training loss 18212714755.765
Test Loss of 24920354691.878788, Test MSE of 24920354881.377541
Epoch 58: training loss 17386134373.647
Test Loss of 24301882679.961140, Test MSE of 24301882449.446327
Epoch 59: training loss 17068568067.765
Test Loss of 21478600997.721951, Test MSE of 21478601269.343918
Epoch 60: training loss 16379679928.471
Test Loss of 24758703353.426788, Test MSE of 24758703906.177940
Epoch 61: training loss 15779312297.412
Test Loss of 23454616089.226925, Test MSE of 23454616699.284931
Epoch 62: training loss 15499636818.824
Test Loss of 22811530896.373814, Test MSE of 22811530603.409225
Epoch 63: training loss 14908567503.059
Test Loss of 21456422343.505898, Test MSE of 21456422345.345985
Epoch 64: training loss 14410362816.000
Test Loss of 23061549226.074486, Test MSE of 23061549667.359299
Epoch 65: training loss 14124216677.647
Test Loss of 21041424746.888733, Test MSE of 21041425147.392994
Epoch 66: training loss 13764966640.941
Test Loss of 20404628920.346054, Test MSE of 20404628599.988228
Epoch 67: training loss 13506316122.353
Test Loss of 20815461023.296783, Test MSE of 20815461326.459484
Epoch 68: training loss 13309871152.941
Test Loss of 24855525258.511219, Test MSE of 24855524954.692543
Epoch 69: training loss 12750547403.294
Test Loss of 22552270730.511219, Test MSE of 22552270840.964592
Epoch 70: training loss 12463580841.412
Test Loss of 22314790361.981956, Test MSE of 22314789985.234741
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23574326846.5206, 'MSE - std': 3942728634.0928116, 'R2 - mean': 0.831072655337552, 'R2 - std': 0.02652066733461066} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005620 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110079578.353
Test Loss of 410765301408.399841, Test MSE of 410765304044.561096
Epoch 2: training loss 430088783992.471
Test Loss of 410747066918.619141, Test MSE of 410747057984.777100
Epoch 3: training loss 430060873607.529
Test Loss of 410723156301.593689, Test MSE of 410723149647.551758
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075608847.059
Test Loss of 410728739295.540955, Test MSE of 410728736779.756042
Epoch 2: training loss 430066313336.471
Test Loss of 410729833240.285034, Test MSE of 410729834808.189148
Epoch 3: training loss 430065877473.882
Test Loss of 410729161412.886597, Test MSE of 410729166692.968323
Epoch 4: training loss 424856389993.412
Test Loss of 394270773716.168457, Test MSE of 394270772033.586853
Epoch 5: training loss 388670665185.882
Test Loss of 339631770478.526611, Test MSE of 339631772865.457886
Epoch 6: training loss 320721156336.941
Test Loss of 262659215615.881531, Test MSE of 262659213963.699738
Epoch 7: training loss 228551529953.882
Test Loss of 165301771526.041656, Test MSE of 165301771942.284485
Epoch 8: training loss 163396555745.882
Test Loss of 124914587159.929657, Test MSE of 124914585317.210587
Epoch 9: training loss 144092637123.765
Test Loss of 114014551336.159180, Test MSE of 114014551351.257050
Epoch 10: training loss 138239070960.941
Test Loss of 110119894668.497925, Test MSE of 110119895462.982178
Epoch 11: training loss 135328275907.765
Test Loss of 107111034711.307724, Test MSE of 107111037109.806549
Epoch 12: training loss 132808567808.000
Test Loss of 104607880743.093018, Test MSE of 104607878113.960922
Epoch 13: training loss 127297540096.000
Test Loss of 101459304288.784821, Test MSE of 101459303282.868362
Epoch 14: training loss 124358050213.647
Test Loss of 98554456347.365112, Test MSE of 98554455085.941360
Epoch 15: training loss 119879595791.059
Test Loss of 95361494901.634430, Test MSE of 95361494044.319901
Epoch 16: training loss 117078534927.059
Test Loss of 92116559400.988434, Test MSE of 92116557043.176041
Epoch 17: training loss 113237060788.706
Test Loss of 88674024634.224899, Test MSE of 88674023324.546906
Epoch 18: training loss 109019682002.824
Test Loss of 85507544910.778351, Test MSE of 85507546221.471054
Epoch 19: training loss 105243655860.706
Test Loss of 83151360801.762146, Test MSE of 83151359790.737137
Epoch 20: training loss 101523506266.353
Test Loss of 79593046353.384537, Test MSE of 79593045629.758865
Epoch 21: training loss 98066585840.941
Test Loss of 76092520001.628876, Test MSE of 76092521531.896606
Epoch 22: training loss 93153704387.765
Test Loss of 73031178347.091156, Test MSE of 73031177563.955261
Epoch 23: training loss 89569434819.765
Test Loss of 69907876309.116150, Test MSE of 69907876349.229401
Epoch 24: training loss 85398641031.529
Test Loss of 67162543711.955574, Test MSE of 67162543398.795578
Epoch 25: training loss 81968926479.059
Test Loss of 63049025379.627953, Test MSE of 63049023825.203835
Epoch 26: training loss 79092078667.294
Test Loss of 61167323161.588150, Test MSE of 61167323520.691902
Epoch 27: training loss 75398088327.529
Test Loss of 58587177831.892639, Test MSE of 58587177065.719086
Epoch 28: training loss 72249242744.471
Test Loss of 56180052743.226288, Test MSE of 56180053228.854568
Epoch 29: training loss 68464737280.000
Test Loss of 53764929438.385933, Test MSE of 53764928965.292656
Epoch 30: training loss 67093053831.529
Test Loss of 54293970189.623322, Test MSE of 54293969598.741402
Epoch 31: training loss 62043258669.176
Test Loss of 49250073930.276726, Test MSE of 49250074062.933334
Epoch 32: training loss 58983667162.353
Test Loss of 46745890000.969925, Test MSE of 46745889804.599548
Epoch 33: training loss 56837092909.176
Test Loss of 44742615927.055992, Test MSE of 44742614600.480278
Epoch 34: training loss 53143695600.941
Test Loss of 42713815440.881073, Test MSE of 42713815722.093445
Epoch 35: training loss 51085939832.471
Test Loss of 38967461839.666824, Test MSE of 38967461963.681610
Epoch 36: training loss 47963175220.706
Test Loss of 39311043129.573349, Test MSE of 39311042518.957314
Epoch 37: training loss 46088926426.353
Test Loss of 38082634853.878761, Test MSE of 38082635190.215340
Epoch 38: training loss 43417042319.059
Test Loss of 35072881648.362793, Test MSE of 35072881729.625740
Epoch 39: training loss 41950691937.882
Test Loss of 34458575765.382690, Test MSE of 34458576178.923050
Epoch 40: training loss 39965682853.647
Test Loss of 31652980719.888939, Test MSE of 31652981165.002583
Epoch 41: training loss 37598083162.353
Test Loss of 33220996297.388245, Test MSE of 33220996289.503498
Epoch 42: training loss 36081270904.471
Test Loss of 30480963317.693661, Test MSE of 30480962953.279324
Epoch 43: training loss 34612273558.588
Test Loss of 29719258900.020359, Test MSE of 29719259002.310539
Epoch 44: training loss 32374173093.647
Test Loss of 27288943762.421101, Test MSE of 27288943739.410797
Epoch 45: training loss 30853804250.353
Test Loss of 27602068625.473392, Test MSE of 27602068921.485218
Epoch 46: training loss 29353617310.118
Test Loss of 24607426008.433132, Test MSE of 24607425444.632847
Epoch 47: training loss 28997486371.765
Test Loss of 23938157788.342434, Test MSE of 23938157991.762604
Epoch 48: training loss 27088048026.353
Test Loss of 24647130084.990284, Test MSE of 24647130194.330040
Epoch 49: training loss 26096991363.765
Test Loss of 25488691514.639519, Test MSE of 25488691923.641434
Epoch 50: training loss 25172527408.941
Test Loss of 21571408803.598335, Test MSE of 21571408803.616020
Epoch 51: training loss 23927433389.176
Test Loss of 22866197141.501156, Test MSE of 22866197017.347164
Epoch 52: training loss 23300331471.059
Test Loss of 24175628688.881073, Test MSE of 24175628399.165916
Epoch 53: training loss 22019232276.706
Test Loss of 22603051943.389172, Test MSE of 22603051963.820114
Epoch 54: training loss 21347805443.765
Test Loss of 23545513226.780193, Test MSE of 23545513489.583382
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23567123507.286293, 'MSE - std': 3414525951.850403, 'R2 - mean': 0.824721143045753, 'R2 - std': 0.02546633998209081} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005403 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043067151.059
Test Loss of 431612507652.975464, Test MSE of 431612510939.747131
Epoch 2: training loss 424023007834.353
Test Loss of 431592011701.130981, Test MSE of 431592004379.818359
Epoch 3: training loss 423995443922.824
Test Loss of 431564680672.014832, Test MSE of 431564688771.364624
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010509131.294
Test Loss of 431567494322.643250, Test MSE of 431567497087.445251
Epoch 2: training loss 423999060208.941
Test Loss of 431569938139.157776, Test MSE of 431569944924.926453
Epoch 3: training loss 423998521464.471
Test Loss of 431569572990.519226, Test MSE of 431569567061.818298
Epoch 4: training loss 418522094893.176
Test Loss of 413637681876.049988, Test MSE of 413637685836.777649
Epoch 5: training loss 381936954428.235
Test Loss of 357988675836.090698, Test MSE of 357988677644.169556
Epoch 6: training loss 313975758245.647
Test Loss of 279428616000.562683, Test MSE of 279428616193.538879
Epoch 7: training loss 222535343646.118
Test Loss of 179473936325.242004, Test MSE of 179473940004.462646
Epoch 8: training loss 159245305675.294
Test Loss of 138021261143.307739, Test MSE of 138021262370.774689
Epoch 9: training loss 140039330800.941
Test Loss of 126782305099.461365, Test MSE of 126782303726.920914
Epoch 10: training loss 135769000267.294
Test Loss of 120956053412.072189, Test MSE of 120956053160.795807
Epoch 11: training loss 132092276976.941
Test Loss of 118186478776.329483, Test MSE of 118186477803.777466
Epoch 12: training loss 128738041042.824
Test Loss of 114466314847.481720, Test MSE of 114466315821.714050
Epoch 13: training loss 124664133782.588
Test Loss of 110967923461.330872, Test MSE of 110967919461.273834
Epoch 14: training loss 120703550343.529
Test Loss of 108016082868.183243, Test MSE of 108016082460.424973
Epoch 15: training loss 118017920015.059
Test Loss of 104151885735.863022, Test MSE of 104151887241.934631
Epoch 16: training loss 113433081524.706
Test Loss of 99420028768.784821, Test MSE of 99420031045.469711
Epoch 17: training loss 110028949835.294
Test Loss of 96574920545.732529, Test MSE of 96574920114.033371
Epoch 18: training loss 105728809170.824
Test Loss of 92670394470.352615, Test MSE of 92670394855.901779
Epoch 19: training loss 101624889705.412
Test Loss of 89126251537.532623, Test MSE of 89126250136.998642
Epoch 20: training loss 97686303668.706
Test Loss of 84227153484.053680, Test MSE of 84227153151.065109
Epoch 21: training loss 95360236559.059
Test Loss of 83460565969.088379, Test MSE of 83460568716.958344
Epoch 22: training loss 90338798607.059
Test Loss of 77297128040.011108, Test MSE of 77297129523.578171
Epoch 23: training loss 87492956882.824
Test Loss of 74524964360.766312, Test MSE of 74524965717.250488
Epoch 24: training loss 83537505596.235
Test Loss of 72429358510.733917, Test MSE of 72429358258.118759
Epoch 25: training loss 80426230497.882
Test Loss of 70274686663.255905, Test MSE of 70274686108.159180
Epoch 26: training loss 75996681984.000
Test Loss of 67688978072.818138, Test MSE of 67688977666.760033
Epoch 27: training loss 72828121728.000
Test Loss of 64564391843.124481, Test MSE of 64564391825.456528
Epoch 28: training loss 69268245172.706
Test Loss of 58069446347.046738, Test MSE of 58069445413.664322
Epoch 29: training loss 66572649622.588
Test Loss of 55563374356.968071, Test MSE of 55563373717.578049
Epoch 30: training loss 63346591021.176
Test Loss of 56056524458.824615, Test MSE of 56056524161.203560
Epoch 31: training loss 60865706149.647
Test Loss of 53324759108.235077, Test MSE of 53324757793.870461
Epoch 32: training loss 57228067433.412
Test Loss of 47621196779.624245, Test MSE of 47621196085.298721
Epoch 33: training loss 55042031856.941
Test Loss of 49373408269.741783, Test MSE of 49373408333.665436
Epoch 34: training loss 51693782505.412
Test Loss of 44498219239.241089, Test MSE of 44498217793.730537
Epoch 35: training loss 49759825408.000
Test Loss of 45208836210.672836, Test MSE of 45208836051.112938
Epoch 36: training loss 46757788668.235
Test Loss of 40954027882.261917, Test MSE of 40954028340.225166
Epoch 37: training loss 45341573955.765
Test Loss of 35541552605.171677, Test MSE of 35541552720.153114
Epoch 38: training loss 42937828638.118
Test Loss of 34883683073.540031, Test MSE of 34883682952.802750
Epoch 39: training loss 40497226405.647
Test Loss of 35870861551.770477, Test MSE of 35870861300.306160
Epoch 40: training loss 38786031856.941
Test Loss of 35909146272.399818, Test MSE of 35909145756.265053
Epoch 41: training loss 36441775732.706
Test Loss of 32937452666.728367, Test MSE of 32937453214.927269
Epoch 42: training loss 35238261368.471
Test Loss of 34022718019.050438, Test MSE of 34022719279.323128
Epoch 43: training loss 33313943254.588
Test Loss of 32787877957.182785, Test MSE of 32787878209.373653
Epoch 44: training loss 32368613217.882
Test Loss of 29316971511.944469, Test MSE of 29316971078.370064
Epoch 45: training loss 30390183296.000
Test Loss of 27623245835.372513, Test MSE of 27623245633.130459
Epoch 46: training loss 29318494064.941
Test Loss of 27833610063.726051, Test MSE of 27833609822.763203
Epoch 47: training loss 27788700848.941
Test Loss of 26738915962.965294, Test MSE of 26738916451.645298
Epoch 48: training loss 26603547237.647
Test Loss of 25191812036.768162, Test MSE of 25191811928.350628
Epoch 49: training loss 25674778789.647
Test Loss of 22469701256.707081, Test MSE of 22469701273.012581
Epoch 50: training loss 24615652001.882
Test Loss of 26134244083.324387, Test MSE of 26134243817.853714
Epoch 51: training loss 23930349594.353
Test Loss of 24702107865.025452, Test MSE of 24702107666.137207
Epoch 52: training loss 22808500133.647
Test Loss of 23679478560.814438, Test MSE of 23679478696.281399
Epoch 53: training loss 22333222960.941
Test Loss of 24962463537.399353, Test MSE of 24962463491.126598
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23846191504.054356, 'MSE - std': 3104626510.2212334, 'R2 - mean': 0.8224927936087919, 'R2 - std': 0.02320969070279844} 
 

Saving model.....
Results After CV: {'MSE - mean': 23846191504.054356, 'MSE - std': 3104626510.2212334, 'R2 - mean': 0.8224927936087919, 'R2 - std': 0.02320969070279844}
Train time: 95.46453372460019
Inference time: 0.0747648261996801
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 76 finished with value: 23846191504.054356 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525598147.765
Test Loss of 418111486953.970825, Test MSE of 418111490504.538940
Epoch 2: training loss 427506895811.765
Test Loss of 418094041476.944702, Test MSE of 418094037658.139099
Epoch 3: training loss 427481793355.294
Test Loss of 418071719465.097412, Test MSE of 418071717946.890930
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493637180.235
Test Loss of 418073558020.500549, Test MSE of 418073559880.999207
Epoch 2: training loss 427483693537.882
Test Loss of 418074613001.534119, Test MSE of 418074614333.502075
Epoch 3: training loss 427483369712.941
Test Loss of 418074668948.933594, Test MSE of 418074666567.110291
Epoch 4: training loss 422453691693.176
Test Loss of 401942617537.820984, Test MSE of 401942612517.502747
Epoch 5: training loss 386877841889.882
Test Loss of 347923745266.853577, Test MSE of 347923742846.721497
Epoch 6: training loss 319119535766.588
Test Loss of 269566326652.772614, Test MSE of 269566325722.360046
Epoch 7: training loss 227824249916.235
Test Loss of 172184247427.464264, Test MSE of 172184249450.845428
Epoch 8: training loss 161561340717.176
Test Loss of 131285202668.754105, Test MSE of 131285199893.886246
Epoch 9: training loss 141306790927.059
Test Loss of 120330557708.613464, Test MSE of 120330558860.724442
Epoch 10: training loss 136320705204.706
Test Loss of 115776857015.280136, Test MSE of 115776857750.764847
Epoch 11: training loss 134262042834.824
Test Loss of 113347876306.402039, Test MSE of 113347876492.939743
Epoch 12: training loss 128734284709.647
Test Loss of 110113296858.692581, Test MSE of 110113299480.711868
Epoch 13: training loss 125869054192.941
Test Loss of 106023324023.679855, Test MSE of 106023322221.288803
Epoch 14: training loss 123399603937.882
Test Loss of 103726837533.549850, Test MSE of 103726837932.931320
Epoch 15: training loss 118162775898.353
Test Loss of 99567191099.928757, Test MSE of 99567189627.373383
Epoch 16: training loss 114403858597.647
Test Loss of 95930120278.695343, Test MSE of 95930121477.655396
Epoch 17: training loss 111102485684.706
Test Loss of 94335454067.534576, Test MSE of 94335456351.291168
Epoch 18: training loss 107097889490.824
Test Loss of 89988007792.455246, Test MSE of 89988007103.556396
Epoch 19: training loss 102925367386.353
Test Loss of 87543764341.548004, Test MSE of 87543764910.499191
Epoch 20: training loss 99065001200.941
Test Loss of 84265033820.854034, Test MSE of 84265034786.660583
Epoch 21: training loss 95600044453.647
Test Loss of 80625725514.851730, Test MSE of 80625726370.614822
Epoch 22: training loss 91881811079.529
Test Loss of 78982669707.340271, Test MSE of 78982669595.068237
Epoch 23: training loss 87660824530.824
Test Loss of 75149602797.997681, Test MSE of 75149602826.439194
Epoch 24: training loss 84566685500.235
Test Loss of 70085038177.591492, Test MSE of 70085037863.780075
Epoch 25: training loss 80707553701.647
Test Loss of 68951965921.502655, Test MSE of 68951965446.850891
Epoch 26: training loss 76999427192.471
Test Loss of 64537262534.321533, Test MSE of 64537263021.398476
Epoch 27: training loss 73573759608.471
Test Loss of 59766624609.413834, Test MSE of 59766625114.936340
Epoch 28: training loss 69788967055.059
Test Loss of 59461944558.056908, Test MSE of 59461944168.879219
Epoch 29: training loss 66441112997.647
Test Loss of 55514670589.986580, Test MSE of 55514671457.808060
Epoch 30: training loss 63618855258.353
Test Loss of 54238453232.247978, Test MSE of 54238453417.756775
Epoch 31: training loss 60602451211.294
Test Loss of 51139629157.381447, Test MSE of 51139629271.667046
Epoch 32: training loss 57518803817.412
Test Loss of 48988297115.092300, Test MSE of 48988297324.230988
Epoch 33: training loss 54504669711.059
Test Loss of 45621460615.846405, Test MSE of 45621460495.405624
Epoch 34: training loss 52684645869.176
Test Loss of 45033081597.098312, Test MSE of 45033080857.931969
Epoch 35: training loss 50291395636.706
Test Loss of 39945475468.287766, Test MSE of 39945475703.002647
Epoch 36: training loss 47058268265.412
Test Loss of 38881944869.011337, Test MSE of 38881945192.116554
Epoch 37: training loss 44946490947.765
Test Loss of 37695545364.844780, Test MSE of 37695545324.825005
Epoch 38: training loss 42608191107.765
Test Loss of 33981035260.624565, Test MSE of 33981035272.850323
Epoch 39: training loss 39916657336.471
Test Loss of 32519474765.102013, Test MSE of 32519474680.313278
Epoch 40: training loss 38011241697.882
Test Loss of 31643755903.022900, Test MSE of 31643755729.080414
Epoch 41: training loss 36783092544.000
Test Loss of 31239612559.544762, Test MSE of 31239612974.284519
Epoch 42: training loss 35194508208.941
Test Loss of 28559663228.121212, Test MSE of 28559663212.289837
Epoch 43: training loss 33241774181.647
Test Loss of 26406874771.216286, Test MSE of 26406875079.059059
Epoch 44: training loss 31296699542.588
Test Loss of 28049374962.912792, Test MSE of 28049375130.024677
Epoch 45: training loss 30327201588.706
Test Loss of 23834465905.106640, Test MSE of 23834465832.378124
Epoch 46: training loss 29190116743.529
Test Loss of 24747104726.902615, Test MSE of 24747105102.073639
Epoch 47: training loss 28056299346.824
Test Loss of 24925617348.367336, Test MSE of 24925617244.414810
Epoch 48: training loss 26655591864.471
Test Loss of 23889204795.099701, Test MSE of 23889204986.275208
Epoch 49: training loss 25851459674.353
Test Loss of 23318983364.248901, Test MSE of 23318983507.053562
Epoch 50: training loss 24507698458.353
Test Loss of 22173406002.157761, Test MSE of 22173406081.783909
Epoch 51: training loss 23590780879.059
Test Loss of 22057035584.606987, Test MSE of 22057035982.207348
Epoch 52: training loss 22596686558.118
Test Loss of 21267932615.032154, Test MSE of 21267932609.496681
Epoch 53: training loss 21418508592.941
Test Loss of 20952167452.661579, Test MSE of 20952167552.254215
Epoch 54: training loss 20878967725.176
Test Loss of 19029081944.294239, Test MSE of 19029082052.212978
Epoch 55: training loss 20325909748.706
Test Loss of 20018868013.183437, Test MSE of 20018867930.491318
Epoch 56: training loss 19880728801.882
Test Loss of 19859892151.990746, Test MSE of 19859892460.744217
Epoch 57: training loss 19185782302.118
Test Loss of 19237928533.155678, Test MSE of 19237928526.944469
Epoch 58: training loss 18178058447.059
Test Loss of 21416878380.828129, Test MSE of 21416878813.580284
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21416878813.580284, 'MSE - std': 0.0, 'R2 - mean': 0.8332244409649747, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005455 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918247333.647
Test Loss of 424557206913.391602, Test MSE of 424557196530.507568
Epoch 2: training loss 427898321498.353
Test Loss of 424541279633.972717, Test MSE of 424541286767.299805
Epoch 3: training loss 427871071533.176
Test Loss of 424519458321.410156, Test MSE of 424519461498.677673
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888351352.471
Test Loss of 424526929205.592407, Test MSE of 424526934619.924377
Epoch 2: training loss 427878159058.824
Test Loss of 424527042821.033569, Test MSE of 424527036643.420654
Epoch 3: training loss 427877808489.412
Test Loss of 424526709547.051575, Test MSE of 424526720136.296814
Epoch 4: training loss 422629993291.294
Test Loss of 407607148778.977539, Test MSE of 407607149275.161743
Epoch 5: training loss 386234029959.529
Test Loss of 353549677304.834595, Test MSE of 353549677105.553955
Epoch 6: training loss 317703100054.588
Test Loss of 277356014557.653503, Test MSE of 277356014067.879028
Epoch 7: training loss 225364890262.588
Test Loss of 181041114058.703674, Test MSE of 181041114482.102692
Epoch 8: training loss 158942489118.118
Test Loss of 141261974567.557709, Test MSE of 141261974688.972565
Epoch 9: training loss 139692125756.235
Test Loss of 130976036789.622025, Test MSE of 130976036513.350906
Epoch 10: training loss 134139642880.000
Test Loss of 127183855337.437897, Test MSE of 127183858108.648071
Epoch 11: training loss 131166245225.412
Test Loss of 124156344385.850571, Test MSE of 124156344429.535675
Epoch 12: training loss 127153030294.588
Test Loss of 120698190374.965530, Test MSE of 120698190973.101181
Epoch 13: training loss 124579719288.471
Test Loss of 117153801891.797363, Test MSE of 117153805963.588531
Epoch 14: training loss 119938504282.353
Test Loss of 113797481497.819107, Test MSE of 113797481930.567627
Epoch 15: training loss 115079425536.000
Test Loss of 110803422101.644226, Test MSE of 110803421406.080841
Epoch 16: training loss 112217616474.353
Test Loss of 105128999155.031235, Test MSE of 105129000173.572617
Epoch 17: training loss 108858293037.176
Test Loss of 101319829171.904694, Test MSE of 101319825765.490662
Epoch 18: training loss 103553546962.824
Test Loss of 96552257950.526947, Test MSE of 96552258372.001617
Epoch 19: training loss 99964546349.176
Test Loss of 92869290121.859818, Test MSE of 92869289831.128586
Epoch 20: training loss 96302789993.412
Test Loss of 90833785515.140411, Test MSE of 90833784011.147858
Epoch 21: training loss 91639919811.765
Test Loss of 88679712602.662964, Test MSE of 88679712711.563522
Epoch 22: training loss 89117908660.706
Test Loss of 84265003482.218826, Test MSE of 84265004948.820343
Epoch 23: training loss 83845692626.824
Test Loss of 78405322386.742538, Test MSE of 78405319824.709366
Epoch 24: training loss 80415378763.294
Test Loss of 73651669803.525330, Test MSE of 73651669765.467300
Epoch 25: training loss 76822107813.647
Test Loss of 72902861840.344208, Test MSE of 72902859909.825516
Epoch 26: training loss 73548188250.353
Test Loss of 70189773722.855423, Test MSE of 70189771204.946976
Epoch 27: training loss 70432921389.176
Test Loss of 66721038940.498726, Test MSE of 66721038241.909897
Epoch 28: training loss 67245489769.412
Test Loss of 62919126365.387001, Test MSE of 62919126288.547005
Epoch 29: training loss 63703558234.353
Test Loss of 62579127030.939629, Test MSE of 62579126565.413223
Epoch 30: training loss 60922298428.235
Test Loss of 56438797934.737915, Test MSE of 56438797647.170265
Epoch 31: training loss 56935078836.706
Test Loss of 55690918412.909554, Test MSE of 55690917681.969452
Epoch 32: training loss 54575856188.235
Test Loss of 54436337001.230629, Test MSE of 54436336293.067909
Epoch 33: training loss 51616218955.294
Test Loss of 50993548005.884804, Test MSE of 50993547546.857773
Epoch 34: training loss 49063743013.647
Test Loss of 47104810733.701599, Test MSE of 47104811691.183403
Epoch 35: training loss 46814429507.765
Test Loss of 45778902955.673378, Test MSE of 45778902249.998756
Epoch 36: training loss 44003864816.941
Test Loss of 44043811920.299789, Test MSE of 44043811707.441612
Epoch 37: training loss 40830177325.176
Test Loss of 38233655516.765213, Test MSE of 38233655842.531876
Epoch 38: training loss 38749481908.706
Test Loss of 40258428266.651863, Test MSE of 40258427635.153587
Epoch 39: training loss 36891230064.941
Test Loss of 39158996855.561417, Test MSE of 39158997604.318222
Epoch 40: training loss 35379274270.118
Test Loss of 37139893164.383995, Test MSE of 37139892341.946144
Epoch 41: training loss 33020286757.647
Test Loss of 36650956120.649551, Test MSE of 36650955806.656334
Epoch 42: training loss 31786814772.706
Test Loss of 36627602747.277351, Test MSE of 36627602691.649834
Epoch 43: training loss 29758255683.765
Test Loss of 33184459486.778625, Test MSE of 33184458291.931347
Epoch 44: training loss 28528951657.412
Test Loss of 35147695460.493179, Test MSE of 35147694371.727982
Epoch 45: training loss 26917776203.294
Test Loss of 30147406429.446217, Test MSE of 30147407520.179546
Epoch 46: training loss 25662885775.059
Test Loss of 30713552075.473515, Test MSE of 30713552338.293468
Epoch 47: training loss 24549976737.882
Test Loss of 31537598720.296089, Test MSE of 31537599079.003387
Epoch 48: training loss 23458129182.118
Test Loss of 33155015684.026833, Test MSE of 33155015233.346272
Epoch 49: training loss 22136687318.588
Test Loss of 30073570669.968079, Test MSE of 30073569818.557159
Epoch 50: training loss 21503105588.706
Test Loss of 28680022212.130466, Test MSE of 28680022143.646511
Epoch 51: training loss 20258412672.000
Test Loss of 29213510492.794819, Test MSE of 29213510044.487465
Epoch 52: training loss 19438799672.471
Test Loss of 26640221666.035622, Test MSE of 26640222025.004700
Epoch 53: training loss 18807524875.294
Test Loss of 29312687202.302105, Test MSE of 29312687765.437943
Epoch 54: training loss 18055814878.118
Test Loss of 26208370879.393013, Test MSE of 26208370673.529701
Epoch 55: training loss 17390222501.647
Test Loss of 26645125714.076336, Test MSE of 26645126040.353065
Epoch 56: training loss 16757886595.765
Test Loss of 24991329200.647697, Test MSE of 24991328780.802097
Epoch 57: training loss 16373183823.059
Test Loss of 28564084122.500114, Test MSE of 28564084372.081532
Epoch 58: training loss 15632034458.353
Test Loss of 26771445966.552856, Test MSE of 26771446249.460289
Epoch 59: training loss 15221147109.647
Test Loss of 22452965124.915104, Test MSE of 22452964926.622150
Epoch 60: training loss 14459259836.235
Test Loss of 26502160464.536663, Test MSE of 26502160969.182388
Epoch 61: training loss 14269815333.647
Test Loss of 26043375684.693039, Test MSE of 26043375712.743832
Epoch 62: training loss 14051218036.706
Test Loss of 27517068729.767292, Test MSE of 27517068337.540497
Epoch 63: training loss 13464019230.118
Test Loss of 25844575101.009483, Test MSE of 25844575145.062775
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23630726979.32153, 'MSE - std': 2213848165.7412453, 'R2 - mean': 0.8243556666879718, 'R2 - std': 0.008868774277002944} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005567 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926967657.412
Test Loss of 447257727750.099487, Test MSE of 447257726649.553101
Epoch 2: training loss 421906227320.471
Test Loss of 447239108977.047424, Test MSE of 447239110143.236206
Epoch 3: training loss 421879179866.353
Test Loss of 447214634153.837585, Test MSE of 447214646409.012085
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896700024.471
Test Loss of 447225575451.950989, Test MSE of 447225579875.121155
Epoch 2: training loss 421886777705.412
Test Loss of 447225447593.837585, Test MSE of 447225449341.343628
Epoch 3: training loss 421886497611.294
Test Loss of 447225131920.433044, Test MSE of 447225128789.134399
Epoch 4: training loss 416393316231.529
Test Loss of 429770927145.452698, Test MSE of 429770928480.386719
Epoch 5: training loss 379687942866.824
Test Loss of 374303761757.860718, Test MSE of 374303757592.518433
Epoch 6: training loss 311871955666.824
Test Loss of 295709973355.007141, Test MSE of 295709976783.314758
Epoch 7: training loss 220488341684.706
Test Loss of 195767854968.035156, Test MSE of 195767853346.115143
Epoch 8: training loss 155920487574.588
Test Loss of 153838270685.238953, Test MSE of 153838268982.319611
Epoch 9: training loss 135948994078.118
Test Loss of 142045733915.714081, Test MSE of 142045733225.398834
Epoch 10: training loss 130288734840.471
Test Loss of 137391459303.128387, Test MSE of 137391459915.186005
Epoch 11: training loss 127438967175.529
Test Loss of 133392219580.136017, Test MSE of 133392219690.884506
Epoch 12: training loss 124842360711.529
Test Loss of 130611859919.322693, Test MSE of 130611861968.335709
Epoch 13: training loss 120821083346.824
Test Loss of 126894503403.036774, Test MSE of 126894500378.773941
Epoch 14: training loss 117286138036.706
Test Loss of 122622807363.804764, Test MSE of 122622807857.967758
Epoch 15: training loss 113290645744.941
Test Loss of 119176162163.534576, Test MSE of 119176162554.419708
Epoch 16: training loss 108331411275.294
Test Loss of 115566797726.645386, Test MSE of 115566797605.022903
Epoch 17: training loss 107560314096.941
Test Loss of 110787938022.121674, Test MSE of 110787937081.866180
Epoch 18: training loss 102394805609.412
Test Loss of 107870174777.204712, Test MSE of 107870174965.428207
Epoch 19: training loss 98415964581.647
Test Loss of 101799000819.623413, Test MSE of 101799002680.797989
Epoch 20: training loss 94245579956.706
Test Loss of 100287240561.047424, Test MSE of 100287239945.125488
Epoch 21: training loss 89831620909.176
Test Loss of 95288588010.859131, Test MSE of 95288587256.471771
Epoch 22: training loss 86253589202.824
Test Loss of 89899688459.014572, Test MSE of 89899690280.067596
Epoch 23: training loss 82451268818.824
Test Loss of 86187758821.529495, Test MSE of 86187758832.646317
Epoch 24: training loss 79867570853.647
Test Loss of 85582847897.671066, Test MSE of 85582849704.947021
Epoch 25: training loss 75332269357.176
Test Loss of 81045540643.234787, Test MSE of 81045541579.582001
Epoch 26: training loss 72493289878.588
Test Loss of 75755223052.791122, Test MSE of 75755224007.766998
Epoch 27: training loss 68283162608.941
Test Loss of 72133837792.969696, Test MSE of 72133837633.030746
Epoch 28: training loss 65284491339.294
Test Loss of 65522924882.017120, Test MSE of 65522924226.244591
Epoch 29: training loss 62101315207.529
Test Loss of 63077151801.086281, Test MSE of 63077150795.684418
Epoch 30: training loss 58745912003.765
Test Loss of 63066609703.320839, Test MSE of 63066610557.861183
Epoch 31: training loss 55660702494.118
Test Loss of 61865336303.300484, Test MSE of 61865336970.339264
Epoch 32: training loss 52818379467.294
Test Loss of 54035890924.517235, Test MSE of 54035892187.827286
Epoch 33: training loss 50993700502.588
Test Loss of 55564025422.049500, Test MSE of 55564026495.046684
Epoch 34: training loss 47996402936.471
Test Loss of 51638229967.441132, Test MSE of 51638229760.418533
Epoch 35: training loss 45492817438.118
Test Loss of 50347505558.354843, Test MSE of 50347504964.748322
Epoch 36: training loss 43773703017.412
Test Loss of 45652084161.347214, Test MSE of 45652084476.802727
Epoch 37: training loss 40696212848.941
Test Loss of 41996248403.438354, Test MSE of 41996248807.317192
Epoch 38: training loss 38442897837.176
Test Loss of 41886042603.273651, Test MSE of 41886043017.248131
Epoch 39: training loss 36451048711.529
Test Loss of 37525856668.158226, Test MSE of 37525855766.902847
Epoch 40: training loss 35329719416.471
Test Loss of 38159939769.234329, Test MSE of 38159939845.230431
Epoch 41: training loss 33781584030.118
Test Loss of 35765699330.309509, Test MSE of 35765700094.250755
Epoch 42: training loss 31518173974.588
Test Loss of 30925188046.493637, Test MSE of 30925188262.452297
Epoch 43: training loss 30088657242.353
Test Loss of 33833683659.828823, Test MSE of 33833683775.567703
Epoch 44: training loss 28387963730.824
Test Loss of 32871023846.476982, Test MSE of 32871023664.943745
Epoch 45: training loss 27355353155.765
Test Loss of 32131920614.832291, Test MSE of 32131920516.438091
Epoch 46: training loss 25983236461.176
Test Loss of 28513249238.310432, Test MSE of 28513249598.085922
Epoch 47: training loss 25079437756.235
Test Loss of 28111266377.548923, Test MSE of 28111266096.217770
Epoch 48: training loss 23777426996.706
Test Loss of 30186469602.923897, Test MSE of 30186469975.039497
Epoch 49: training loss 22960957312.000
Test Loss of 29242388935.742771, Test MSE of 29242389053.865131
Epoch 50: training loss 21708789191.529
Test Loss of 27991397828.189682, Test MSE of 27991397545.814026
Epoch 51: training loss 20775861037.176
Test Loss of 26032783854.589867, Test MSE of 26032783929.099350
Epoch 52: training loss 20344114763.294
Test Loss of 25806619939.116356, Test MSE of 25806619581.231022
Epoch 53: training loss 19470865182.118
Test Loss of 25352118621.623871, Test MSE of 25352119042.388947
Epoch 54: training loss 19164940950.588
Test Loss of 25140918528.769836, Test MSE of 25140918243.584576
Epoch 55: training loss 18078681536.000
Test Loss of 25372147144.453388, Test MSE of 25372146830.709484
Epoch 56: training loss 17524933982.118
Test Loss of 24589620052.267406, Test MSE of 24589620096.691814
Epoch 57: training loss 17323957281.882
Test Loss of 23762143922.720333, Test MSE of 23762144363.671516
Epoch 58: training loss 16434675670.588
Test Loss of 24330818943.022900, Test MSE of 24330819212.501759
Epoch 59: training loss 16091932574.118
Test Loss of 22179312168.860512, Test MSE of 22179312547.605713
Epoch 60: training loss 15753623680.000
Test Loss of 22620296480.273884, Test MSE of 22620296587.095455
Epoch 61: training loss 15423026518.588
Test Loss of 24925043573.903309, Test MSE of 24925043414.458961
Epoch 62: training loss 14591151137.882
Test Loss of 22419949571.789959, Test MSE of 22419949741.238403
Epoch 63: training loss 14227230592.000
Test Loss of 23360830446.234558, Test MSE of 23360830580.388664
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23540761513.010574, 'MSE - std': 1812071568.9216986, 'R2 - mean': 0.8310666086269819, 'R2 - std': 0.011937765907760108} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005621 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430111147670.588
Test Loss of 410765552179.887085, Test MSE of 410765543909.291321
Epoch 2: training loss 430090349628.235
Test Loss of 410745819338.809814, Test MSE of 410745817154.832764
Epoch 3: training loss 430062529837.176
Test Loss of 410720907682.887573, Test MSE of 410720901462.724182
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077564084.706
Test Loss of 410727060060.638611, Test MSE of 410727058629.253296
Epoch 2: training loss 430064297863.529
Test Loss of 410727661120.681152, Test MSE of 410727653907.405945
Epoch 3: training loss 430064031382.588
Test Loss of 410727621498.846802, Test MSE of 410727625671.543701
Epoch 4: training loss 424635300924.235
Test Loss of 393932986463.718628, Test MSE of 393932988885.319397
Epoch 5: training loss 388134709850.353
Test Loss of 339356357455.252197, Test MSE of 339356357430.438721
Epoch 6: training loss 320287394876.235
Test Loss of 262337086918.426666, Test MSE of 262337090456.127838
Epoch 7: training loss 228989405424.941
Test Loss of 165209275346.983795, Test MSE of 165209275378.606781
Epoch 8: training loss 164424161460.706
Test Loss of 124702351303.137436, Test MSE of 124702349418.275513
Epoch 9: training loss 143138140792.471
Test Loss of 114095724609.391953, Test MSE of 114095727028.200684
Epoch 10: training loss 138313297950.118
Test Loss of 110514054969.928741, Test MSE of 110514057001.159668
Epoch 11: training loss 135879934012.235
Test Loss of 107316071248.199905, Test MSE of 107316069945.714722
Epoch 12: training loss 132735161464.471
Test Loss of 104864994218.232300, Test MSE of 104864994363.270035
Epoch 13: training loss 129121905874.824
Test Loss of 102184874584.847763, Test MSE of 102184873274.806702
Epoch 14: training loss 124511617084.235
Test Loss of 98007303133.408600, Test MSE of 98007301914.323929
Epoch 15: training loss 120853289592.471
Test Loss of 94849966413.593704, Test MSE of 94849965774.602158
Epoch 16: training loss 115906391431.529
Test Loss of 93356400730.980103, Test MSE of 93356401501.607620
Epoch 17: training loss 113460177709.176
Test Loss of 88266352476.046280, Test MSE of 88266354168.992950
Epoch 18: training loss 108615720478.118
Test Loss of 86577617439.037476, Test MSE of 86577618417.269775
Epoch 19: training loss 105218757421.176
Test Loss of 82516797234.347061, Test MSE of 82516798524.390457
Epoch 20: training loss 101076918603.294
Test Loss of 79268342232.906982, Test MSE of 79268343745.564117
Epoch 21: training loss 97648073231.059
Test Loss of 77714178085.908371, Test MSE of 77714177289.836273
Epoch 22: training loss 93191844065.882
Test Loss of 74983751228.890335, Test MSE of 74983751022.683365
Epoch 23: training loss 90011950155.294
Test Loss of 69974164665.277191, Test MSE of 69974164367.226746
Epoch 24: training loss 86712936824.471
Test Loss of 67518558698.913467, Test MSE of 67518560352.419464
Epoch 25: training loss 81723416982.588
Test Loss of 62916426244.501617, Test MSE of 62916425380.778244
Epoch 26: training loss 78783833871.059
Test Loss of 63507212480.385010, Test MSE of 63507211448.372192
Epoch 27: training loss 74888041110.588
Test Loss of 61056515296.133270, Test MSE of 61056514743.961464
Epoch 28: training loss 71274057110.588
Test Loss of 58040371316.568253, Test MSE of 58040370371.197029
Epoch 29: training loss 69104781176.471
Test Loss of 53482202054.663582, Test MSE of 53482201826.542244
Epoch 30: training loss 64934854625.882
Test Loss of 52813780746.543266, Test MSE of 52813781110.655457
Epoch 31: training loss 61964907520.000
Test Loss of 49948356873.358627, Test MSE of 49948356399.935257
Epoch 32: training loss 59091838162.824
Test Loss of 47228654455.055992, Test MSE of 47228655451.292755
Epoch 33: training loss 56799765692.235
Test Loss of 45445991862.315598, Test MSE of 45445991706.159088
Epoch 34: training loss 53747638708.706
Test Loss of 42985403719.433594, Test MSE of 42985403561.737350
Epoch 35: training loss 50452190938.353
Test Loss of 40068919936.651550, Test MSE of 40068918479.142113
Epoch 36: training loss 48156241897.412
Test Loss of 40209223892.286903, Test MSE of 40209223792.249908
Epoch 37: training loss 46724638396.235
Test Loss of 37478489152.918091, Test MSE of 37478489368.645218
Epoch 38: training loss 43127047047.529
Test Loss of 36259790076.564552, Test MSE of 36259789590.842850
Epoch 39: training loss 41510237552.941
Test Loss of 34155304502.256363, Test MSE of 34155304691.721989
Epoch 40: training loss 39442205771.294
Test Loss of 32804568227.479870, Test MSE of 32804568583.982964
Epoch 41: training loss 37918513468.235
Test Loss of 29559089137.784359, Test MSE of 29559090251.633156
Epoch 42: training loss 36246879073.882
Test Loss of 29228009666.280426, Test MSE of 29228009449.236282
Epoch 43: training loss 34122417355.294
Test Loss of 28203029308.298012, Test MSE of 28203028955.919743
Epoch 44: training loss 32747407864.471
Test Loss of 28168625769.432671, Test MSE of 28168626528.938725
Epoch 45: training loss 31048800173.176
Test Loss of 26356927343.474316, Test MSE of 26356927722.304691
Epoch 46: training loss 29972304399.059
Test Loss of 24017936157.497456, Test MSE of 24017935782.022755
Epoch 47: training loss 28556083486.118
Test Loss of 24349307345.325314, Test MSE of 24349308107.009167
Epoch 48: training loss 27006492156.235
Test Loss of 24322831221.160572, Test MSE of 24322831030.091198
Epoch 49: training loss 26068354006.588
Test Loss of 25252565058.339657, Test MSE of 25252565748.587357
Epoch 50: training loss 24843718185.412
Test Loss of 22992792416.310966, Test MSE of 22992792443.497021
Epoch 51: training loss 24008522375.529
Test Loss of 24787165224.751503, Test MSE of 24787165466.925552
Epoch 52: training loss 23194065163.294
Test Loss of 21390528508.209164, Test MSE of 21390528613.941586
Epoch 53: training loss 22117780732.235
Test Loss of 20587705779.472466, Test MSE of 20587705841.897346
Epoch 54: training loss 21500749844.706
Test Loss of 20895242845.586304, Test MSE of 20895242787.730057
Epoch 55: training loss 20678756397.176
Test Loss of 19520754542.526608, Test MSE of 19520754688.723206
Epoch 56: training loss 20025761332.706
Test Loss of 21438731114.735771, Test MSE of 21438731115.189503
Epoch 57: training loss 19397234740.706
Test Loss of 19685412338.968994, Test MSE of 19685412150.793430
Epoch 58: training loss 18828528813.176
Test Loss of 19001924952.492363, Test MSE of 19001924999.077744
Epoch 59: training loss 18363328451.765
Test Loss of 20304229391.637203, Test MSE of 20304229533.561634
Epoch 60: training loss 17615516766.118
Test Loss of 19416365850.180473, Test MSE of 19416366232.385105
Epoch 61: training loss 16908681359.059
Test Loss of 17966437554.643219, Test MSE of 17966437575.572384
Epoch 62: training loss 16863017385.412
Test Loss of 18697535274.765385, Test MSE of 18697535325.617050
Epoch 63: training loss 16085041283.765
Test Loss of 18281700588.453495, Test MSE of 18281700534.147499
Epoch 64: training loss 15745537664.000
Test Loss of 19900654670.186024, Test MSE of 19900654897.484314
Epoch 65: training loss 15632206912.000
Test Loss of 16493375965.645535, Test MSE of 16493375758.098091
Epoch 66: training loss 14764530744.471
Test Loss of 18190190179.746414, Test MSE of 18190190151.628223
Epoch 67: training loss 14445782049.882
Test Loss of 19964586201.973160, Test MSE of 19964585870.868919
Epoch 68: training loss 14143401856.000
Test Loss of 18501462356.227673, Test MSE of 18501462305.318150
Epoch 69: training loss 13990327913.412
Test Loss of 17282548960.133270, Test MSE of 17282548724.467636
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21976208315.87484, 'MSE - std': 3131482500.058671, 'R2 - mean': 0.8376394860674797, 'R2 - std': 0.015378258831700617} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005263 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424044359559.529
Test Loss of 431613353540.471985, Test MSE of 431613358650.352356
Epoch 2: training loss 424025738541.176
Test Loss of 431593605126.633972, Test MSE of 431593603142.229248
Epoch 3: training loss 423999772431.059
Test Loss of 431567074388.820007, Test MSE of 431567070971.480591
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424014729336.471
Test Loss of 431569793677.919495, Test MSE of 431569794374.774780
Epoch 2: training loss 424000568139.294
Test Loss of 431571242338.917175, Test MSE of 431571242846.689087
Epoch 3: training loss 424000202631.529
Test Loss of 431572023494.071289, Test MSE of 431572026365.147278
Epoch 4: training loss 418814662053.647
Test Loss of 414580470307.776001, Test MSE of 414580473360.406128
Epoch 5: training loss 382820286584.471
Test Loss of 359332325022.504395, Test MSE of 359332336710.408264
Epoch 6: training loss 315498005443.765
Test Loss of 280951368342.448853, Test MSE of 280951366494.404724
Epoch 7: training loss 224565400696.471
Test Loss of 180786874427.705688, Test MSE of 180786872809.700500
Epoch 8: training loss 160267610322.824
Test Loss of 138132924271.000458, Test MSE of 138132923565.192383
Epoch 9: training loss 140347845391.059
Test Loss of 126356482024.307266, Test MSE of 126356483161.785263
Epoch 10: training loss 135493477722.353
Test Loss of 121516673261.875061, Test MSE of 121516672039.545929
Epoch 11: training loss 131947848643.765
Test Loss of 118850074431.614990, Test MSE of 118850073803.636475
Epoch 12: training loss 129719071713.882
Test Loss of 115219450067.339188, Test MSE of 115219452247.534973
Epoch 13: training loss 125380559841.882
Test Loss of 111707035679.274414, Test MSE of 111707033338.344086
Epoch 14: training loss 121375830196.706
Test Loss of 108457062291.961136, Test MSE of 108457060813.338959
Epoch 15: training loss 116870338288.941
Test Loss of 105082085478.352615, Test MSE of 105082085221.545792
Epoch 16: training loss 113842601532.235
Test Loss of 100641997107.057846, Test MSE of 100641996645.900574
Epoch 17: training loss 110052216079.059
Test Loss of 97151698802.791306, Test MSE of 97151700711.886276
Epoch 18: training loss 105620991563.294
Test Loss of 93211391413.367889, Test MSE of 93211391007.122971
Epoch 19: training loss 101790642236.235
Test Loss of 91565057921.954651, Test MSE of 91565056006.472702
Epoch 20: training loss 98903194518.588
Test Loss of 87893115361.436371, Test MSE of 87893116700.397827
Epoch 21: training loss 95113673547.294
Test Loss of 83502977291.254044, Test MSE of 83502977987.941177
Epoch 22: training loss 91025197056.000
Test Loss of 78187772075.535400, Test MSE of 78187772572.450745
Epoch 23: training loss 88555349654.588
Test Loss of 77823174929.888016, Test MSE of 77823175280.784470
Epoch 24: training loss 84165350550.588
Test Loss of 74970813485.016190, Test MSE of 74970814289.498871
Epoch 25: training loss 80832319382.588
Test Loss of 67013962703.192963, Test MSE of 67013962910.788872
Epoch 26: training loss 76913760918.588
Test Loss of 64867639273.254974, Test MSE of 64867641286.966087
Epoch 27: training loss 73708390324.706
Test Loss of 62621056910.274872, Test MSE of 62621056415.037987
Epoch 28: training loss 70307383913.412
Test Loss of 60476395523.790840, Test MSE of 60476395388.328728
Epoch 29: training loss 66855386458.353
Test Loss of 60398493663.777878, Test MSE of 60398493156.074326
Epoch 30: training loss 63651319808.000
Test Loss of 54765297590.078667, Test MSE of 54765297149.826485
Epoch 31: training loss 61161124397.176
Test Loss of 50619236477.097641, Test MSE of 50619236430.732765
Epoch 32: training loss 57996543841.882
Test Loss of 49513211994.032394, Test MSE of 49513213061.021790
Epoch 33: training loss 55798718102.588
Test Loss of 50601720398.422951, Test MSE of 50601720080.287163
Epoch 34: training loss 52583462490.353
Test Loss of 45028033835.002312, Test MSE of 45028034263.519005
Epoch 35: training loss 50342858864.941
Test Loss of 46432749794.502548, Test MSE of 46432748825.275612
Epoch 36: training loss 47717271198.118
Test Loss of 43293979004.979179, Test MSE of 43293979306.322060
Epoch 37: training loss 46148918603.294
Test Loss of 35699066034.643219, Test MSE of 35699065761.550743
Epoch 38: training loss 43320816165.647
Test Loss of 37685661349.138359, Test MSE of 37685661375.719513
Epoch 39: training loss 41024852080.941
Test Loss of 36158214094.719109, Test MSE of 36158213800.378036
Epoch 40: training loss 39203352869.647
Test Loss of 32754725229.815826, Test MSE of 32754726227.712055
Epoch 41: training loss 37386991141.647
Test Loss of 32385321518.200832, Test MSE of 32385321733.983559
Epoch 42: training loss 35808718223.059
Test Loss of 31602016690.998611, Test MSE of 31602016513.580734
Epoch 43: training loss 34128867448.471
Test Loss of 31868190228.138824, Test MSE of 31868190265.924812
Epoch 44: training loss 32571433193.412
Test Loss of 27818959193.913929, Test MSE of 27818959097.259682
Epoch 45: training loss 31080783149.176
Test Loss of 26786027930.358166, Test MSE of 26786028158.768661
Epoch 46: training loss 29922369829.647
Test Loss of 25824608654.037945, Test MSE of 25824608899.222713
Epoch 47: training loss 28536459350.588
Test Loss of 28652459173.375290, Test MSE of 28652458867.848328
Epoch 48: training loss 27336235911.529
Test Loss of 24440096443.883389, Test MSE of 24440096712.971432
Epoch 49: training loss 25633023868.235
Test Loss of 24619427396.472004, Test MSE of 24619427168.221481
Epoch 50: training loss 24801890032.941
Test Loss of 22815483156.257290, Test MSE of 22815482966.692078
Epoch 51: training loss 24063583085.176
Test Loss of 24612380858.698750, Test MSE of 24612380707.559982
Epoch 52: training loss 23122504086.588
Test Loss of 23430149153.643684, Test MSE of 23430149131.316479
Epoch 53: training loss 22535819896.471
Test Loss of 24872608349.586304, Test MSE of 24872608789.909801
Epoch 54: training loss 21434943442.824
Test Loss of 20945866101.871357, Test MSE of 20945865821.374275
Epoch 55: training loss 21193794390.588
Test Loss of 22713557243.142990, Test MSE of 22713557488.195599
Epoch 56: training loss 20070123531.294
Test Loss of 21008049378.028690, Test MSE of 21008049455.747261
Epoch 57: training loss 19099067154.824
Test Loss of 21822839340.305412, Test MSE of 21822839609.098782
Epoch 58: training loss 18693122496.000
Test Loss of 21872172373.649235, Test MSE of 21872172361.890785
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21955401125.078026, 'MSE - std': 2801192223.910095, 'R2 - mean': 0.8374431498035946, 'R2 - std': 0.01376033674987236} 
 

Saving model.....
Results After CV: {'MSE - mean': 21955401125.078026, 'MSE - std': 2801192223.910095, 'R2 - mean': 0.8374431498035946, 'R2 - std': 0.01376033674987236}
Train time: 99.18792510659986
Inference time: 0.07521841479974682
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 77 finished with value: 21955401125.078026 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005540 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524943390.118
Test Loss of 418111910565.455444, Test MSE of 418111907088.589417
Epoch 2: training loss 427504384602.353
Test Loss of 418093858755.360657, Test MSE of 418093857173.417725
Epoch 3: training loss 427478231280.941
Test Loss of 418070857074.705505, Test MSE of 418070855803.669312
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496409208.471
Test Loss of 418072485075.053406, Test MSE of 418072487739.717346
Epoch 2: training loss 427482798682.353
Test Loss of 418074036298.377991, Test MSE of 418074036514.883179
Epoch 3: training loss 427482393660.235
Test Loss of 418074363212.805908, Test MSE of 418074365538.553406
Epoch 4: training loss 427482129106.824
Test Loss of 418074866049.628479, Test MSE of 418074865427.330261
Epoch 5: training loss 421048888500.706
Test Loss of 397392500129.132568, Test MSE of 397392501002.740662
Epoch 6: training loss 376378401972.706
Test Loss of 331077191201.043701, Test MSE of 331077188839.836670
Epoch 7: training loss 297887490048.000
Test Loss of 245976539524.470978, Test MSE of 245976540348.442993
Epoch 8: training loss 218974351631.059
Test Loss of 175324261360.603271, Test MSE of 175324261186.824249
Epoch 9: training loss 158917471171.765
Test Loss of 126795549565.009491, Test MSE of 126795550145.525452
Epoch 10: training loss 139611488662.588
Test Loss of 118770634415.640991, Test MSE of 118770633978.047226
Epoch 11: training loss 135777806064.941
Test Loss of 115682096376.479294, Test MSE of 115682095844.189117
Epoch 12: training loss 133758147162.353
Test Loss of 112713853815.324539, Test MSE of 112713854708.414719
Epoch 13: training loss 129871070328.471
Test Loss of 109341619061.903305, Test MSE of 109341615847.315216
Epoch 14: training loss 125760050944.000
Test Loss of 105818786810.078186, Test MSE of 105818787455.796356
Epoch 15: training loss 120979158671.059
Test Loss of 102797254426.707382, Test MSE of 102797253457.653748
Epoch 16: training loss 118707176568.471
Test Loss of 99640630822.254913, Test MSE of 99640631549.322235
Epoch 17: training loss 113885068227.765
Test Loss of 96105324849.091827, Test MSE of 96105325792.489136
Epoch 18: training loss 110239890432.000
Test Loss of 93491984034.849869, Test MSE of 93491984734.155701
Epoch 19: training loss 107020071574.588
Test Loss of 90092940371.142258, Test MSE of 90092940266.745377
Epoch 20: training loss 103138230234.353
Test Loss of 86602757974.873001, Test MSE of 86602756487.089005
Epoch 21: training loss 98865637225.412
Test Loss of 81685455835.284760, Test MSE of 81685456244.835526
Epoch 22: training loss 96283120805.647
Test Loss of 78895941374.756424, Test MSE of 78895941014.827942
Epoch 23: training loss 91403078046.118
Test Loss of 76685140305.306503, Test MSE of 76685139335.208145
Epoch 24: training loss 87903300864.000
Test Loss of 75439084415.378204, Test MSE of 75439083758.850571
Epoch 25: training loss 83833738932.706
Test Loss of 70075678829.908859, Test MSE of 70075678616.870560
Epoch 26: training loss 80747085312.000
Test Loss of 67116120133.640526, Test MSE of 67116119113.921349
Epoch 27: training loss 76814892077.176
Test Loss of 63531185512.283134, Test MSE of 63531184251.080063
Epoch 28: training loss 74031896508.235
Test Loss of 63299475008.310898, Test MSE of 63299476188.709541
Epoch 29: training loss 69888264026.353
Test Loss of 60349806710.910019, Test MSE of 60349806392.361168
Epoch 30: training loss 67610499937.882
Test Loss of 57283479073.043716, Test MSE of 57283478535.895485
Epoch 31: training loss 64261381225.412
Test Loss of 54671495963.891739, Test MSE of 54671495326.793976
Epoch 32: training loss 61429877270.588
Test Loss of 51234578505.904236, Test MSE of 51234577572.579788
Epoch 33: training loss 58427539915.294
Test Loss of 50200359807.378212, Test MSE of 50200359814.920662
Epoch 34: training loss 55168118573.176
Test Loss of 46170241705.719177, Test MSE of 46170241848.943184
Epoch 35: training loss 53054722123.294
Test Loss of 40232388969.467499, Test MSE of 40232389278.680763
Epoch 36: training loss 50122342716.235
Test Loss of 42352127315.912102, Test MSE of 42352127616.046387
Epoch 37: training loss 47634067614.118
Test Loss of 41753149657.922737, Test MSE of 41753150353.755043
Epoch 38: training loss 45385094701.176
Test Loss of 39153765612.635666, Test MSE of 39153765941.192520
Epoch 39: training loss 42765635824.941
Test Loss of 38876019592.379364, Test MSE of 38876019166.168129
Epoch 40: training loss 40549356687.059
Test Loss of 35213934010.241035, Test MSE of 35213933694.490768
Epoch 41: training loss 38421203456.000
Test Loss of 36461371786.155907, Test MSE of 36461372217.067108
Epoch 42: training loss 36809136643.765
Test Loss of 30800502088.542217, Test MSE of 30800502215.222916
Epoch 43: training loss 35000422912.000
Test Loss of 29368354552.123989, Test MSE of 29368354725.556499
Epoch 44: training loss 33786623969.882
Test Loss of 29715256417.828362, Test MSE of 29715256835.175892
Epoch 45: training loss 32044412096.000
Test Loss of 27835217333.029839, Test MSE of 27835217582.715027
Epoch 46: training loss 30397194947.765
Test Loss of 26228815986.172565, Test MSE of 26228816462.973251
Epoch 47: training loss 29278112406.588
Test Loss of 26859588406.895210, Test MSE of 26859588582.068150
Epoch 48: training loss 27634096956.235
Test Loss of 23485943958.650936, Test MSE of 23485943513.379265
Epoch 49: training loss 25882488429.176
Test Loss of 23223218369.287994, Test MSE of 23223218589.167236
Epoch 50: training loss 25274242078.118
Test Loss of 24683703354.507519, Test MSE of 24683703799.658859
Epoch 51: training loss 24007751457.882
Test Loss of 24049968901.862595, Test MSE of 24049968760.985195
Epoch 52: training loss 23254033438.118
Test Loss of 21414835035.847328, Test MSE of 21414835580.746605
Epoch 53: training loss 22488406415.059
Test Loss of 21781640778.733288, Test MSE of 21781641057.186012
Epoch 54: training loss 21539717044.706
Test Loss of 21420385146.167015, Test MSE of 21420385827.914062
Epoch 55: training loss 20795402217.412
Test Loss of 19829875166.482536, Test MSE of 19829875445.302200
Epoch 56: training loss 19921421338.353
Test Loss of 22475639933.779320, Test MSE of 22475639895.504539
Epoch 57: training loss 19501364464.941
Test Loss of 23357495551.348602, Test MSE of 23357495514.439926
Epoch 58: training loss 18457724830.118
Test Loss of 19418879124.045338, Test MSE of 19418879412.998375
Epoch 59: training loss 17932817031.529
Test Loss of 19054254559.903770, Test MSE of 19054254682.790047
Epoch 60: training loss 17666174200.471
Test Loss of 19621558019.493870, Test MSE of 19621558318.295433
Epoch 61: training loss 16774118181.647
Test Loss of 18049537992.571827, Test MSE of 18049538035.504009
Epoch 62: training loss 16091609472.000
Test Loss of 18348015963.728893, Test MSE of 18348015756.176903
Epoch 63: training loss 16012817615.059
Test Loss of 18836589474.909092, Test MSE of 18836589672.312401
Epoch 64: training loss 15645917191.529
Test Loss of 18942498302.934074, Test MSE of 18942498327.653046
Epoch 65: training loss 14927280225.882
Test Loss of 20185460872.912331, Test MSE of 20185461115.575462
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20185461115.575462, 'MSE - std': 0.0, 'R2 - mean': 0.8428136241871422, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005372 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916360764.235
Test Loss of 424556233613.590576, Test MSE of 424556234869.303833
Epoch 2: training loss 427894605221.647
Test Loss of 424539954049.746948, Test MSE of 424539955163.035950
Epoch 3: training loss 427867776180.706
Test Loss of 424518533783.243103, Test MSE of 424518528644.325134
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890769438.118
Test Loss of 424523956762.174438, Test MSE of 424523960294.696716
Epoch 2: training loss 427877559115.294
Test Loss of 424523695353.426758, Test MSE of 424523690482.007751
Epoch 3: training loss 427877190716.235
Test Loss of 424523391989.814453, Test MSE of 424523394080.495178
Epoch 4: training loss 427876887853.176
Test Loss of 424523312180.822571, Test MSE of 424523314878.033081
Epoch 5: training loss 420862131983.059
Test Loss of 402761477858.805481, Test MSE of 402761474330.897522
Epoch 6: training loss 374735644431.059
Test Loss of 335689459174.299316, Test MSE of 335689454734.877869
Epoch 7: training loss 295243983088.941
Test Loss of 251687873410.694427, Test MSE of 251687875648.101379
Epoch 8: training loss 215893244566.588
Test Loss of 183634717237.177887, Test MSE of 183634717746.354584
Epoch 9: training loss 156743219410.824
Test Loss of 137343756661.784866, Test MSE of 137343760424.134369
Epoch 10: training loss 137464488568.471
Test Loss of 130243542410.866531, Test MSE of 130243541548.769867
Epoch 11: training loss 133362667128.471
Test Loss of 126997684087.561417, Test MSE of 126997684630.351028
Epoch 12: training loss 129777501786.353
Test Loss of 124145946280.061066, Test MSE of 124145947484.203232
Epoch 13: training loss 127106364777.412
Test Loss of 120241370197.510986, Test MSE of 120241368018.589478
Epoch 14: training loss 122959929374.118
Test Loss of 117290239593.052979, Test MSE of 117290237401.336990
Epoch 15: training loss 119199927928.471
Test Loss of 113032516437.688644, Test MSE of 113032517937.927597
Epoch 16: training loss 114546286411.294
Test Loss of 109665566468.915100, Test MSE of 109665568934.554672
Epoch 17: training loss 111107478016.000
Test Loss of 105285241009.654404, Test MSE of 105285241767.876801
Epoch 18: training loss 106669149093.647
Test Loss of 102734888820.718948, Test MSE of 102734889105.627609
Epoch 19: training loss 102307097479.529
Test Loss of 99166102770.794357, Test MSE of 99166103028.806992
Epoch 20: training loss 99201624214.588
Test Loss of 94971050508.435806, Test MSE of 94971051856.350357
Epoch 21: training loss 94700158072.471
Test Loss of 92301861728.111038, Test MSE of 92301860876.640991
Epoch 22: training loss 91512767457.882
Test Loss of 87627612557.945877, Test MSE of 87627613391.828156
Epoch 23: training loss 86694996208.941
Test Loss of 84854849722.655563, Test MSE of 84854849031.825653
Epoch 24: training loss 83083256048.941
Test Loss of 80269150736.225769, Test MSE of 80269149206.315048
Epoch 25: training loss 79936453722.353
Test Loss of 77750505391.463333, Test MSE of 77750506395.833282
Epoch 26: training loss 75984474112.000
Test Loss of 73345884274.646317, Test MSE of 73345883679.041092
Epoch 27: training loss 72462430689.882
Test Loss of 71226648468.459869, Test MSE of 71226646673.666580
Epoch 28: training loss 68861729867.294
Test Loss of 66189032410.811012, Test MSE of 66189032562.186111
Epoch 29: training loss 65347684758.588
Test Loss of 63908185600.829056, Test MSE of 63908185005.224335
Epoch 30: training loss 62102621846.588
Test Loss of 61997543947.014572, Test MSE of 61997544290.408005
Epoch 31: training loss 59421105859.765
Test Loss of 53960965829.907005, Test MSE of 53960965402.278214
Epoch 32: training loss 56276554285.176
Test Loss of 57501585144.597733, Test MSE of 57501586090.001656
Epoch 33: training loss 53444747143.529
Test Loss of 51188601081.189911, Test MSE of 51188602024.281998
Epoch 34: training loss 50283536263.529
Test Loss of 50744844612.041641, Test MSE of 50744843576.531723
Epoch 35: training loss 47891960892.235
Test Loss of 46789009685.614616, Test MSE of 46789009082.570274
Epoch 36: training loss 45058054663.529
Test Loss of 46753305524.911407, Test MSE of 46753304568.556305
Epoch 37: training loss 42415745249.882
Test Loss of 44376897197.272263, Test MSE of 44376898032.423790
Epoch 38: training loss 40367935834.353
Test Loss of 43686013712.521858, Test MSE of 43686013562.192947
Epoch 39: training loss 38757106017.882
Test Loss of 41993473773.227852, Test MSE of 41993473996.202019
Epoch 40: training loss 35824098763.294
Test Loss of 39481684831.637291, Test MSE of 39481684420.309677
Epoch 41: training loss 34064505690.353
Test Loss of 40030852391.380058, Test MSE of 40030852944.546120
Epoch 42: training loss 32521847732.706
Test Loss of 34588144133.803375, Test MSE of 34588144374.567886
Epoch 43: training loss 30357393370.353
Test Loss of 33378025250.761047, Test MSE of 33378024574.328571
Epoch 44: training loss 29438502972.235
Test Loss of 33004598588.224842, Test MSE of 33004599395.280186
Epoch 45: training loss 27066130258.824
Test Loss of 30799065828.937313, Test MSE of 30799065417.802418
Epoch 46: training loss 25994508980.706
Test Loss of 32248740931.982418, Test MSE of 32248740921.739407
Epoch 47: training loss 24128767134.118
Test Loss of 28607214257.772842, Test MSE of 28607214253.645893
Epoch 48: training loss 23365372355.765
Test Loss of 28019305374.408512, Test MSE of 28019306208.543018
Epoch 49: training loss 22487717492.706
Test Loss of 29093653899.340275, Test MSE of 29093653032.527515
Epoch 50: training loss 21103276096.000
Test Loss of 29286594317.679390, Test MSE of 29286594193.909935
Epoch 51: training loss 20221601238.588
Test Loss of 28517164106.141106, Test MSE of 28517163577.968380
Epoch 52: training loss 19554903431.529
Test Loss of 28848219337.104790, Test MSE of 28848217448.939144
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24516839282.2573, 'MSE - std': 4331378166.681841, 'R2 - mean': 0.8184282523758903, 'R2 - std': 0.024385371811251944} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005401 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421925307934.118
Test Loss of 447255504544.007385, Test MSE of 447255513869.599670
Epoch 2: training loss 421902620672.000
Test Loss of 447236291414.162415, Test MSE of 447236292347.581238
Epoch 3: training loss 421874881475.765
Test Loss of 447211405440.858643, Test MSE of 447211409741.852905
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896658582.588
Test Loss of 447218736195.034912, Test MSE of 447218742785.522217
Epoch 2: training loss 421884786447.059
Test Loss of 447220451440.277588, Test MSE of 447220454007.567566
Epoch 3: training loss 421884249148.235
Test Loss of 447220438296.693970, Test MSE of 447220439693.187866
Epoch 4: training loss 421883880869.647
Test Loss of 447220354675.001648, Test MSE of 447220356633.551880
Epoch 5: training loss 415106597707.294
Test Loss of 425552780950.058777, Test MSE of 425552777546.159241
Epoch 6: training loss 369975267809.882
Test Loss of 358186602557.586853, Test MSE of 358186601918.723083
Epoch 7: training loss 292297885936.941
Test Loss of 271650377956.345123, Test MSE of 271650374793.633942
Epoch 8: training loss 212799462881.882
Test Loss of 200150104455.787170, Test MSE of 200150105443.169922
Epoch 9: training loss 154233040203.294
Test Loss of 148881502434.450165, Test MSE of 148881501226.928253
Epoch 10: training loss 135229143732.706
Test Loss of 139844413008.418213, Test MSE of 139844411478.344391
Epoch 11: training loss 130713224161.882
Test Loss of 136835681850.152206, Test MSE of 136835679685.904236
Epoch 12: training loss 128223734512.941
Test Loss of 133270147910.528793, Test MSE of 133270148196.308517
Epoch 13: training loss 124140448045.176
Test Loss of 129801664301.183441, Test MSE of 129801665270.456757
Epoch 14: training loss 121089731614.118
Test Loss of 126241108156.550537, Test MSE of 126241104397.043030
Epoch 15: training loss 116916645767.529
Test Loss of 122238586994.646317, Test MSE of 122238589353.313461
Epoch 16: training loss 113114021135.059
Test Loss of 119606586261.644226, Test MSE of 119606585594.688339
Epoch 17: training loss 109179787324.235
Test Loss of 114701164652.013885, Test MSE of 114701164184.664581
Epoch 18: training loss 105880047284.706
Test Loss of 112731823875.967621, Test MSE of 112731823903.368576
Epoch 19: training loss 101932878275.765
Test Loss of 108018254123.406891, Test MSE of 108018252201.777206
Epoch 20: training loss 97508784399.059
Test Loss of 104254232052.511688, Test MSE of 104254232066.221130
Epoch 21: training loss 94231457852.235
Test Loss of 100047351412.896606, Test MSE of 100047352038.936584
Epoch 22: training loss 90138306334.118
Test Loss of 95531878233.715469, Test MSE of 95531877283.829803
Epoch 23: training loss 86712043414.588
Test Loss of 90502701154.538971, Test MSE of 90502701070.134750
Epoch 24: training loss 82963475983.059
Test Loss of 88016360901.137177, Test MSE of 88016362178.689270
Epoch 25: training loss 78794847397.647
Test Loss of 87045504995.575302, Test MSE of 87045504889.430283
Epoch 26: training loss 74670566189.176
Test Loss of 81541509153.162155, Test MSE of 81541507667.493744
Epoch 27: training loss 71776530477.176
Test Loss of 74903553804.021286, Test MSE of 74903554280.495346
Epoch 28: training loss 68248077808.941
Test Loss of 71509985526.821182, Test MSE of 71509984563.090866
Epoch 29: training loss 65474321784.471
Test Loss of 70721213152.199860, Test MSE of 70721213775.685150
Epoch 30: training loss 62089625103.059
Test Loss of 67405570660.078651, Test MSE of 67405569234.235023
Epoch 31: training loss 58278090706.824
Test Loss of 65403789554.557480, Test MSE of 65403788893.834229
Epoch 32: training loss 55879667847.529
Test Loss of 63817780460.635666, Test MSE of 63817781672.343269
Epoch 33: training loss 52598623201.882
Test Loss of 55921389675.066391, Test MSE of 55921390437.549217
Epoch 34: training loss 50322267648.000
Test Loss of 57740026502.188293, Test MSE of 57740027374.300606
Epoch 35: training loss 47638444822.588
Test Loss of 54634023378.638908, Test MSE of 54634023478.409348
Epoch 36: training loss 44825315689.412
Test Loss of 50504209752.175804, Test MSE of 50504209175.248917
Epoch 37: training loss 42799494234.353
Test Loss of 46590983583.711311, Test MSE of 46590983093.123238
Epoch 38: training loss 40694511450.353
Test Loss of 46806476307.778854, Test MSE of 46806476329.515739
Epoch 39: training loss 38541387919.059
Test Loss of 42057010738.572289, Test MSE of 42057010375.391075
Epoch 40: training loss 36603067482.353
Test Loss of 39332886609.010406, Test MSE of 39332886081.506264
Epoch 41: training loss 35029929148.235
Test Loss of 41524998452.881798, Test MSE of 41524997993.425903
Epoch 42: training loss 33341721155.765
Test Loss of 38314386025.763588, Test MSE of 38314385912.386177
Epoch 43: training loss 31424812212.706
Test Loss of 34766762221.820030, Test MSE of 34766762206.144203
Epoch 44: training loss 29055918494.118
Test Loss of 35630311870.741615, Test MSE of 35630313400.115311
Epoch 45: training loss 28233785630.118
Test Loss of 32944603372.872543, Test MSE of 32944603209.949703
Epoch 46: training loss 27305347098.353
Test Loss of 34139910038.828590, Test MSE of 34139910641.014126
Epoch 47: training loss 26275341131.294
Test Loss of 31841603620.952118, Test MSE of 31841603416.722401
Epoch 48: training loss 24477593359.059
Test Loss of 32310251785.297249, Test MSE of 32310252089.522873
Epoch 49: training loss 23342287608.471
Test Loss of 26870391941.832985, Test MSE of 26870391954.689682
Epoch 50: training loss 22514359672.471
Test Loss of 29641185391.330093, Test MSE of 29641185284.196552
Epoch 51: training loss 21499043343.059
Test Loss of 29674086148.915104, Test MSE of 29674085671.726139
Epoch 52: training loss 20595845214.118
Test Loss of 26752504502.036549, Test MSE of 26752505009.967701
Epoch 53: training loss 19760070893.176
Test Loss of 26541812223.170944, Test MSE of 26541811715.392673
Epoch 54: training loss 18977551529.412
Test Loss of 26110800134.454777, Test MSE of 26110800220.377823
Epoch 55: training loss 18626575311.059
Test Loss of 26968280129.613693, Test MSE of 26968279692.400387
Epoch 56: training loss 17676262832.941
Test Loss of 23260167885.960674, Test MSE of 23260167545.202145
Epoch 57: training loss 17080007333.647
Test Loss of 25116418536.431183, Test MSE of 25116418931.336937
Epoch 58: training loss 16529200500.706
Test Loss of 23335643563.791813, Test MSE of 23335643767.254486
Epoch 59: training loss 16190234217.412
Test Loss of 24860996519.646542, Test MSE of 24860996273.807362
Epoch 60: training loss 15558841641.412
Test Loss of 25201039793.476753, Test MSE of 25201039754.409698
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24744906106.3081, 'MSE - std': 3551232673.453543, 'R2 - mean': 0.8230316154094678, 'R2 - std': 0.02094785927057503} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005330 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430106112481.882
Test Loss of 410761240936.129578, Test MSE of 410761245431.277405
Epoch 2: training loss 430083317278.118
Test Loss of 410742159846.174927, Test MSE of 410742162546.940002
Epoch 3: training loss 430057008911.059
Test Loss of 410718974264.744080, Test MSE of 410718973080.211365
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076207585.882
Test Loss of 410723694236.608948, Test MSE of 410723697324.399109
Epoch 2: training loss 430061200082.824
Test Loss of 410723924840.840332, Test MSE of 410723933862.967834
Epoch 3: training loss 430060814576.941
Test Loss of 410723584605.586304, Test MSE of 410723591285.503479
Epoch 4: training loss 430060598452.706
Test Loss of 410723439371.017151, Test MSE of 410723436210.393005
Epoch 5: training loss 423242798501.647
Test Loss of 389116437901.564087, Test MSE of 389116432747.081543
Epoch 6: training loss 377539363418.353
Test Loss of 322734688179.235535, Test MSE of 322734694425.512634
Epoch 7: training loss 299080656293.647
Test Loss of 236730267398.752441, Test MSE of 236730268626.486237
Epoch 8: training loss 219545580363.294
Test Loss of 168168182250.913452, Test MSE of 168168180908.414978
Epoch 9: training loss 161393468717.176
Test Loss of 120229380543.318832, Test MSE of 120229380882.565414
Epoch 10: training loss 141608211546.353
Test Loss of 112298537947.513184, Test MSE of 112298537519.847977
Epoch 11: training loss 137119794928.941
Test Loss of 109429588631.396576, Test MSE of 109429589891.768127
Epoch 12: training loss 135040191307.294
Test Loss of 107178790413.504852, Test MSE of 107178788457.001999
Epoch 13: training loss 132526155113.412
Test Loss of 103767709071.459503, Test MSE of 103767708469.429764
Epoch 14: training loss 127761707158.588
Test Loss of 100265073770.617310, Test MSE of 100265073809.421997
Epoch 15: training loss 124139435791.059
Test Loss of 97596940185.647385, Test MSE of 97596938567.338791
Epoch 16: training loss 119856073667.765
Test Loss of 95268134165.205002, Test MSE of 95268134794.789963
Epoch 17: training loss 117129652976.941
Test Loss of 91116891005.689957, Test MSE of 91116890469.725128
Epoch 18: training loss 112238138428.235
Test Loss of 88316053927.626099, Test MSE of 88316053552.763550
Epoch 19: training loss 108293485176.471
Test Loss of 85666291973.567795, Test MSE of 85666292481.469971
Epoch 20: training loss 103589894686.118
Test Loss of 83354084306.036087, Test MSE of 83354085295.580872
Epoch 21: training loss 99800277172.706
Test Loss of 79116415914.232300, Test MSE of 79116415152.866989
Epoch 22: training loss 96250723117.176
Test Loss of 76630372588.927353, Test MSE of 76630373389.194824
Epoch 23: training loss 92457841633.882
Test Loss of 71869428593.369736, Test MSE of 71869429630.352234
Epoch 24: training loss 88882270554.353
Test Loss of 70042401519.533554, Test MSE of 70042401589.906708
Epoch 25: training loss 84967868928.000
Test Loss of 67603744149.145767, Test MSE of 67603745529.936287
Epoch 26: training loss 81583470456.471
Test Loss of 65252554366.282280, Test MSE of 65252554875.824127
Epoch 27: training loss 77728109778.824
Test Loss of 61132278460.357239, Test MSE of 61132279960.729271
Epoch 28: training loss 75309283343.059
Test Loss of 58066274094.556221, Test MSE of 58066274896.426353
Epoch 29: training loss 69962553042.824
Test Loss of 54645439473.310501, Test MSE of 54645439729.141022
Epoch 30: training loss 67940031036.235
Test Loss of 53732079086.230446, Test MSE of 53732080447.513443
Epoch 31: training loss 64823121633.882
Test Loss of 51452106523.602036, Test MSE of 51452106863.358391
Epoch 32: training loss 61978035832.471
Test Loss of 49332055548.446091, Test MSE of 49332056289.052406
Epoch 33: training loss 59478055574.588
Test Loss of 49035749659.365112, Test MSE of 49035750182.303696
Epoch 34: training loss 55903581643.294
Test Loss of 45258116196.931053, Test MSE of 45258116520.130280
Epoch 35: training loss 52535027553.882
Test Loss of 42676978007.070801, Test MSE of 42676977865.286949
Epoch 36: training loss 49971851971.765
Test Loss of 42396774661.567795, Test MSE of 42396774088.268204
Epoch 37: training loss 47915716028.235
Test Loss of 39908785226.395187, Test MSE of 39908784911.642967
Epoch 38: training loss 45279604690.824
Test Loss of 37175564205.549286, Test MSE of 37175565074.905060
Epoch 39: training loss 43794211561.412
Test Loss of 34475906348.897736, Test MSE of 34475906685.808884
Epoch 40: training loss 40720085955.765
Test Loss of 33703836352.148079, Test MSE of 33703836554.368080
Epoch 41: training loss 39135194104.471
Test Loss of 33469284080.955112, Test MSE of 33469283195.399643
Epoch 42: training loss 36735553167.059
Test Loss of 31470233890.946785, Test MSE of 31470233600.303741
Epoch 43: training loss 34703830520.471
Test Loss of 28857801844.094402, Test MSE of 28857802070.933662
Epoch 44: training loss 33390381093.647
Test Loss of 29301763430.708004, Test MSE of 29301764350.505711
Epoch 45: training loss 32150179252.706
Test Loss of 29279657722.432209, Test MSE of 29279657124.899536
Epoch 46: training loss 30376595888.941
Test Loss of 26895596054.508099, Test MSE of 26895596462.570156
Epoch 47: training loss 28505468574.118
Test Loss of 26949667447.174458, Test MSE of 26949667568.814735
Epoch 48: training loss 27559688421.647
Test Loss of 23889952159.570568, Test MSE of 23889952211.146126
Epoch 49: training loss 26027667444.706
Test Loss of 21670413766.426655, Test MSE of 21670413211.465916
Epoch 50: training loss 25104523297.882
Test Loss of 23326636136.248032, Test MSE of 23326636593.305641
Epoch 51: training loss 24069845225.412
Test Loss of 23549061343.659416, Test MSE of 23549061290.764839
Epoch 52: training loss 23140173624.471
Test Loss of 21616699935.985191, Test MSE of 21616699832.706059
Epoch 53: training loss 22205953935.059
Test Loss of 21116000757.338268, Test MSE of 21116000938.472210
Epoch 54: training loss 21629651207.529
Test Loss of 20453910251.268856, Test MSE of 20453910530.775612
Epoch 55: training loss 20626962187.294
Test Loss of 20876475527.048588, Test MSE of 20876475447.687370
Epoch 56: training loss 19805868235.294
Test Loss of 20097003407.222584, Test MSE of 20097003176.158821
Epoch 57: training loss 19382797839.059
Test Loss of 20823060570.980103, Test MSE of 20823060734.139545
Epoch 58: training loss 19088234541.176
Test Loss of 19173317775.577972, Test MSE of 19173317817.063320
Epoch 59: training loss 17881842294.588
Test Loss of 20103084523.861176, Test MSE of 20103084632.964657
Epoch 60: training loss 17355767992.471
Test Loss of 18879507765.427116, Test MSE of 18879507627.818893
Epoch 61: training loss 16817618465.882
Test Loss of 19257963437.075428, Test MSE of 19257963263.900097
Epoch 62: training loss 16368710262.588
Test Loss of 18555039468.216568, Test MSE of 18555039785.226646
Epoch 63: training loss 16220506437.647
Test Loss of 18821814177.229061, Test MSE of 18821814143.075241
Epoch 64: training loss 15740707821.176
Test Loss of 19417441397.989819, Test MSE of 19417441622.882793
Epoch 65: training loss 15141166765.176
Test Loss of 19772087623.433594, Test MSE of 19772088069.732662
Epoch 66: training loss 14711726435.765
Test Loss of 20354745044.049976, Test MSE of 20354744784.434978
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23647365775.83982, 'MSE - std': 3615553132.892154, 'R2 - mean': 0.8252741323565297, 'R2 - std': 0.018552526857235404} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005416 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043720342.588
Test Loss of 431612504392.855164, Test MSE of 431612513484.497925
Epoch 2: training loss 424024313615.059
Test Loss of 431593076213.812134, Test MSE of 431593083806.901428
Epoch 3: training loss 423997568180.706
Test Loss of 431566530267.157776, Test MSE of 431566539900.421997
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012687239.529
Test Loss of 431569530017.584473, Test MSE of 431569531297.710083
Epoch 2: training loss 424000752459.294
Test Loss of 431571133375.081909, Test MSE of 431571137287.217102
Epoch 3: training loss 424000157214.118
Test Loss of 431570367246.807983, Test MSE of 431570370585.560608
Epoch 4: training loss 423999706413.176
Test Loss of 431570893126.959717, Test MSE of 431570900797.976990
Epoch 5: training loss 417382134362.353
Test Loss of 409838999405.105042, Test MSE of 409839001328.782410
Epoch 6: training loss 372945281144.471
Test Loss of 342873513646.615479, Test MSE of 342873514179.509033
Epoch 7: training loss 294780195599.059
Test Loss of 254925517954.310028, Test MSE of 254925518633.340424
Epoch 8: training loss 216431394514.824
Test Loss of 183672040632.329468, Test MSE of 183672039253.204590
Epoch 9: training loss 157320009035.294
Test Loss of 132628941519.311432, Test MSE of 132628941507.719452
Epoch 10: training loss 138329718151.529
Test Loss of 124300308205.164276, Test MSE of 124300310362.562241
Epoch 11: training loss 134906412272.941
Test Loss of 120804946782.415543, Test MSE of 120804946110.804062
Epoch 12: training loss 131039740747.294
Test Loss of 118189071216.422028, Test MSE of 118189071596.847031
Epoch 13: training loss 128050914800.941
Test Loss of 113685538003.813049, Test MSE of 113685537646.148865
Epoch 14: training loss 123880296387.765
Test Loss of 109621576695.944473, Test MSE of 109621580778.871796
Epoch 15: training loss 120579694501.647
Test Loss of 106854873214.045349, Test MSE of 106854874416.921783
Epoch 16: training loss 116508002544.941
Test Loss of 102660670156.468307, Test MSE of 102660667230.796432
Epoch 17: training loss 111535979128.471
Test Loss of 100481039989.752899, Test MSE of 100481040426.391647
Epoch 18: training loss 108622762752.000
Test Loss of 94987186879.200363, Test MSE of 94987185631.365448
Epoch 19: training loss 104715857377.882
Test Loss of 92076129007.533554, Test MSE of 92076126367.988831
Epoch 20: training loss 101306010563.765
Test Loss of 89100712501.782501, Test MSE of 89100711624.046661
Epoch 21: training loss 97812008628.706
Test Loss of 85288971766.759827, Test MSE of 85288972077.945557
Epoch 22: training loss 93862723689.412
Test Loss of 80655928515.701996, Test MSE of 80655928086.095551
Epoch 23: training loss 89341325191.529
Test Loss of 77375952392.766312, Test MSE of 77375953069.225723
Epoch 24: training loss 85725523712.000
Test Loss of 77682102535.463211, Test MSE of 77682100927.519363
Epoch 25: training loss 82679327849.412
Test Loss of 72327454272.681168, Test MSE of 72327453266.689560
Epoch 26: training loss 78723707783.529
Test Loss of 70054087301.390091, Test MSE of 70054086664.597717
Epoch 27: training loss 75091309507.765
Test Loss of 62767505133.164276, Test MSE of 62767504896.321709
Epoch 28: training loss 72032063420.235
Test Loss of 64315135955.457657, Test MSE of 64315135180.302742
Epoch 29: training loss 69026090270.118
Test Loss of 60368906866.435913, Test MSE of 60368909144.978943
Epoch 30: training loss 65447745129.412
Test Loss of 55036060457.343819, Test MSE of 55036059797.895027
Epoch 31: training loss 62581551510.588
Test Loss of 53297801685.116150, Test MSE of 53297800901.670044
Epoch 32: training loss 58930580329.412
Test Loss of 50421605982.060158, Test MSE of 50421606200.848602
Epoch 33: training loss 56330711634.824
Test Loss of 50869332382.149002, Test MSE of 50869332906.649559
Epoch 34: training loss 54149011651.765
Test Loss of 46601112512.029617, Test MSE of 46601112648.088264
Epoch 35: training loss 50621796856.471
Test Loss of 38665850641.177231, Test MSE of 38665851182.738182
Epoch 36: training loss 48893856188.235
Test Loss of 40017103836.460899, Test MSE of 40017103420.008041
Epoch 37: training loss 46309818270.118
Test Loss of 37987099858.865341, Test MSE of 37987099306.638084
Epoch 38: training loss 43730479826.824
Test Loss of 36252227541.353081, Test MSE of 36252227595.178734
Epoch 39: training loss 41463913675.294
Test Loss of 31264773123.316982, Test MSE of 31264773175.630619
Epoch 40: training loss 40005264918.588
Test Loss of 34577102953.195740, Test MSE of 34577103414.907639
Epoch 41: training loss 37254874142.118
Test Loss of 29117806413.830635, Test MSE of 29117806891.002964
Epoch 42: training loss 35800928399.059
Test Loss of 30251614794.632114, Test MSE of 30251615248.900749
Epoch 43: training loss 33986686012.235
Test Loss of 28467793657.484497, Test MSE of 28467793844.218521
Epoch 44: training loss 32787640478.118
Test Loss of 28210899519.259602, Test MSE of 28210899223.651104
Epoch 45: training loss 30558853360.941
Test Loss of 23476620476.120316, Test MSE of 23476620881.584942
Epoch 46: training loss 29410502407.529
Test Loss of 28997776231.892643, Test MSE of 28997776475.540382
Epoch 47: training loss 27844090522.353
Test Loss of 22657153274.195278, Test MSE of 22657153039.484692
Epoch 48: training loss 26805901131.294
Test Loss of 23513773144.610828, Test MSE of 23513773189.010620
Epoch 49: training loss 25747982863.059
Test Loss of 21062169706.617306, Test MSE of 21062170038.856152
Epoch 50: training loss 24498239792.941
Test Loss of 23177412676.235077, Test MSE of 23177412294.526184
Epoch 51: training loss 23756865769.412
Test Loss of 24155200376.951412, Test MSE of 24155200331.544788
Epoch 52: training loss 22796089140.706
Test Loss of 21342788430.304489, Test MSE of 21342788462.737888
Epoch 53: training loss 22076276483.765
Test Loss of 20395562512.347988, Test MSE of 20395562577.990284
Epoch 54: training loss 20974669473.882
Test Loss of 20709972841.788059, Test MSE of 20709972707.742317
Epoch 55: training loss 20037654362.353
Test Loss of 20859381170.524757, Test MSE of 20859381003.250332
Epoch 56: training loss 19650810548.706
Test Loss of 21696624811.535400, Test MSE of 21696625043.839085
Epoch 57: training loss 19178742185.412
Test Loss of 20130828664.240631, Test MSE of 20130829177.214054
Epoch 58: training loss 18636702083.765
Test Loss of 20914747108.634892, Test MSE of 20914747260.327969
Epoch 59: training loss 17933766629.647
Test Loss of 20695356167.226284, Test MSE of 20695356010.614220
Epoch 60: training loss 17196995956.706
Test Loss of 19946844849.932438, Test MSE of 19946844975.405621
Epoch 61: training loss 17044311762.824
Test Loss of 19427458117.656639, Test MSE of 19427458295.047596
Epoch 62: training loss 16321073558.588
Test Loss of 21036889031.611290, Test MSE of 21036889160.655777
Epoch 63: training loss 15958874593.882
Test Loss of 19728109357.608513, Test MSE of 19728109335.471233
Epoch 64: training loss 15654903393.882
Test Loss of 18287109357.401203, Test MSE of 18287109047.751907
Epoch 65: training loss 15346178465.882
Test Loss of 19613539560.662655, Test MSE of 19613539348.814754
Epoch 66: training loss 14860673453.176
Test Loss of 18843750753.495605, Test MSE of 18843750523.096367
Epoch 67: training loss 14418535002.353
Test Loss of 19896058986.617306, Test MSE of 19896058667.744709
Epoch 68: training loss 14128997387.294
Test Loss of 18849652352.177696, Test MSE of 18849652472.296963
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22687823115.13125, 'MSE - std': 3760407961.417452, 'R2 - mean': 0.8320653249854949, 'R2 - std': 0.02144383807823591} 
 

Saving model.....
Results After CV: {'MSE - mean': 22687823115.13125, 'MSE - std': 3760407961.417452, 'R2 - mean': 0.8320653249854949, 'R2 - std': 0.02144383807823591}
Train time: 98.12460738260052
Inference time: 0.07557615719997557
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 78 finished with value: 22687823115.13125 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525364314.353
Test Loss of 418111329890.420532, Test MSE of 418111333469.023132
Epoch 2: training loss 427504871544.471
Test Loss of 418093896321.450867, Test MSE of 418093901853.339355
Epoch 3: training loss 427477334738.824
Test Loss of 418070297608.290527, Test MSE of 418070290618.367126
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494489750.588
Test Loss of 418076825322.859131, Test MSE of 418076824860.772278
Epoch 2: training loss 427482805187.765
Test Loss of 418077781196.421021, Test MSE of 418077782869.521606
Epoch 3: training loss 427482304150.588
Test Loss of 418076417795.967590, Test MSE of 418076411853.534668
Epoch 4: training loss 427481959845.647
Test Loss of 418076941136.477417, Test MSE of 418076943582.149658
Epoch 5: training loss 427481730951.529
Test Loss of 418075232258.368713, Test MSE of 418075228249.133789
Epoch 6: training loss 419351311299.765
Test Loss of 392289427131.958374, Test MSE of 392289433847.501831
Epoch 7: training loss 366441578977.882
Test Loss of 316056411661.620178, Test MSE of 316056405705.672119
Epoch 8: training loss 279352561423.059
Test Loss of 225471820870.351135, Test MSE of 225471823029.026611
Epoch 9: training loss 200449841423.059
Test Loss of 159114196898.672211, Test MSE of 159114194753.909821
Epoch 10: training loss 156650223736.471
Test Loss of 130096587485.357391, Test MSE of 130096588081.070084
Epoch 11: training loss 139829213274.353
Test Loss of 118150465504.259079, Test MSE of 118150463521.520081
Epoch 12: training loss 134670303713.882
Test Loss of 114027463205.544296, Test MSE of 114027463631.433701
Epoch 13: training loss 131519299312.941
Test Loss of 111650266838.488083, Test MSE of 111650268940.141159
Epoch 14: training loss 128341474002.824
Test Loss of 108677494017.954193, Test MSE of 108677497589.976837
Epoch 15: training loss 125251258172.235
Test Loss of 105499406939.551239, Test MSE of 105499407439.273590
Epoch 16: training loss 121176999695.059
Test Loss of 101916617642.489014, Test MSE of 101916619058.716003
Epoch 17: training loss 117782637718.588
Test Loss of 98283331320.597733, Test MSE of 98283330897.428650
Epoch 18: training loss 112422094802.824
Test Loss of 96454714594.687027, Test MSE of 96454715865.268997
Epoch 19: training loss 109570757632.000
Test Loss of 92752475005.483231, Test MSE of 92752474569.652939
Epoch 20: training loss 104873772679.529
Test Loss of 89609447159.176498, Test MSE of 89609447563.157211
Epoch 21: training loss 101619603636.706
Test Loss of 86993284664.257233, Test MSE of 86993284387.298294
Epoch 22: training loss 97849481743.059
Test Loss of 82894137994.451996, Test MSE of 82894138315.179626
Epoch 23: training loss 93852242040.471
Test Loss of 79177992588.524643, Test MSE of 79177993440.036026
Epoch 24: training loss 89845922477.176
Test Loss of 76778293509.981033, Test MSE of 76778293141.496277
Epoch 25: training loss 86944593754.353
Test Loss of 72030830842.137405, Test MSE of 72030831300.886902
Epoch 26: training loss 82156475196.235
Test Loss of 70502474006.562103, Test MSE of 70502474310.325226
Epoch 27: training loss 77714045485.176
Test Loss of 68320875978.822113, Test MSE of 68320877119.559669
Epoch 28: training loss 75220201261.176
Test Loss of 64146938761.326859, Test MSE of 64146937803.014702
Epoch 29: training loss 71775702226.824
Test Loss of 60156094544.062920, Test MSE of 60156095779.256111
Epoch 30: training loss 69011444736.000
Test Loss of 56947651839.111725, Test MSE of 56947652791.283211
Epoch 31: training loss 64963372739.765
Test Loss of 55700625888.851257, Test MSE of 55700625493.616013
Epoch 32: training loss 62666224519.529
Test Loss of 54178865553.735832, Test MSE of 54178866775.555374
Epoch 33: training loss 59435089054.118
Test Loss of 49985874076.809624, Test MSE of 49985875486.793922
Epoch 34: training loss 56098117214.118
Test Loss of 46663871201.621094, Test MSE of 46663870517.115547
Epoch 35: training loss 53356772517.647
Test Loss of 43926753355.799217, Test MSE of 43926753822.082718
Epoch 36: training loss 50626288173.176
Test Loss of 44726912081.721024, Test MSE of 44726910829.324905
Epoch 37: training loss 48476395399.529
Test Loss of 41555965521.365715, Test MSE of 41555965801.746300
Epoch 38: training loss 45995816353.882
Test Loss of 38477101671.394867, Test MSE of 38477100991.997398
Epoch 39: training loss 43605699817.412
Test Loss of 39879709408.199860, Test MSE of 39879710114.289223
Epoch 40: training loss 41599869906.824
Test Loss of 34807483459.982422, Test MSE of 34807483494.771805
Epoch 41: training loss 39109486885.647
Test Loss of 33933702702.308582, Test MSE of 33933702564.584679
Epoch 42: training loss 36990735105.882
Test Loss of 34521504882.409439, Test MSE of 34521504697.181625
Epoch 43: training loss 35422417822.118
Test Loss of 29898409596.950268, Test MSE of 29898409631.094208
Epoch 44: training loss 33978732457.412
Test Loss of 28423143369.045570, Test MSE of 28423143192.439747
Epoch 45: training loss 32198994025.412
Test Loss of 27643966086.188293, Test MSE of 27643966337.238411
Epoch 46: training loss 30983091403.294
Test Loss of 28693226345.585938, Test MSE of 28693226241.537350
Epoch 47: training loss 28896608768.000
Test Loss of 27039497561.597038, Test MSE of 27039497826.526585
Epoch 48: training loss 27568665012.706
Test Loss of 25498795450.241035, Test MSE of 25498795700.972401
Epoch 49: training loss 26274122804.706
Test Loss of 24345321284.396946, Test MSE of 24345321847.450756
Epoch 50: training loss 25553970390.588
Test Loss of 23387294378.192921, Test MSE of 23387294407.528511
Epoch 51: training loss 24432242959.059
Test Loss of 24392688978.017117, Test MSE of 24392688716.285603
Epoch 52: training loss 23307125007.059
Test Loss of 21381943367.535507, Test MSE of 21381943751.644428
Epoch 53: training loss 22534239634.824
Test Loss of 21570889682.520473, Test MSE of 21570889464.900013
Epoch 54: training loss 21471504877.176
Test Loss of 22353765529.256535, Test MSE of 22353765543.127274
Epoch 55: training loss 20452434804.706
Test Loss of 21309055182.552856, Test MSE of 21309054872.093040
Epoch 56: training loss 20078801144.471
Test Loss of 21751020033.539673, Test MSE of 21751020002.728981
Epoch 57: training loss 18978812995.765
Test Loss of 24556831801.086281, Test MSE of 24556832073.109669
Epoch 58: training loss 18580889814.588
Test Loss of 21527596975.463337, Test MSE of 21527596724.007713
Epoch 59: training loss 17812502332.235
Test Loss of 19998372475.292160, Test MSE of 19998372531.711342
Epoch 60: training loss 17077435136.000
Test Loss of 20974915260.905853, Test MSE of 20974915356.609879
Epoch 61: training loss 16551976391.529
Test Loss of 21163353020.965069, Test MSE of 21163352651.065121
Epoch 62: training loss 16550768075.294
Test Loss of 17149449335.146889, Test MSE of 17149449146.515953
Epoch 63: training loss 15575877605.647
Test Loss of 18995672501.029839, Test MSE of 18995672381.838875
Epoch 64: training loss 15128943220.706
Test Loss of 19640444294.602821, Test MSE of 19640444168.608109
Epoch 65: training loss 14950125059.765
Test Loss of 20683879397.707150, Test MSE of 20683879119.587601
Epoch 66: training loss 14344597505.882
Test Loss of 18957862941.845940, Test MSE of 18957862917.927387
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18957862917.927387, 'MSE - std': 0.0, 'R2 - mean': 0.852373064545619, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005890 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917943747.765
Test Loss of 424555957621.074280, Test MSE of 424555961586.889832
Epoch 2: training loss 427896714902.588
Test Loss of 424540026354.616699, Test MSE of 424540023465.102356
Epoch 3: training loss 427868036156.235
Test Loss of 424517291343.174622, Test MSE of 424517292012.549622
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887958618.353
Test Loss of 424524052572.380310, Test MSE of 424524053196.899963
Epoch 2: training loss 427876537524.706
Test Loss of 424525538714.500122, Test MSE of 424525533642.959961
Epoch 3: training loss 427875903367.529
Test Loss of 424525396121.019653, Test MSE of 424525399717.980469
Epoch 4: training loss 427875468709.647
Test Loss of 424525142845.527649, Test MSE of 424525141863.361511
Epoch 5: training loss 427875164882.824
Test Loss of 424524744983.035828, Test MSE of 424524745122.734558
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 221741304020.33096, 'MSE - std': 202783441102.40356, 'R2 - mean': -0.5892258808037698, 'R2 - std': 1.441598945349389} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005479 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421925784274.824
Test Loss of 447257781870.737915, Test MSE of 447257784608.360657
Epoch 2: training loss 421904582174.118
Test Loss of 447239804844.147095, Test MSE of 447239801386.558167
Epoch 3: training loss 421877974678.588
Test Loss of 447215709365.681213, Test MSE of 447215701635.480591
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899610112.000
Test Loss of 447221905314.672241, Test MSE of 447221907055.614136
Epoch 2: training loss 421888120832.000
Test Loss of 447222910093.886658, Test MSE of 447222911154.038025
Epoch 3: training loss 421887665332.706
Test Loss of 447222512193.495239, Test MSE of 447222518712.489136
Epoch 4: training loss 421887361144.471
Test Loss of 447223123696.070312, Test MSE of 447223125589.853943
Epoch 5: training loss 421887162488.471
Test Loss of 447223077640.468201, Test MSE of 447223082085.824097
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 296901896708.8287, 'MSE - std': 196754446057.6698, 'R2 - mean': -1.0518621016454313, 'R2 - std': 1.3466759911566042} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005708 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110334494.118
Test Loss of 410763824270.630249, Test MSE of 410763828298.414368
Epoch 2: training loss 430089891358.118
Test Loss of 410746325673.403076, Test MSE of 410746320619.078125
Epoch 3: training loss 430063483482.353
Test Loss of 410723953014.345215, Test MSE of 410723950775.185730
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079557872.941
Test Loss of 410729693752.625610, Test MSE of 410729694972.559021
Epoch 2: training loss 430066859791.059
Test Loss of 410730293099.209595, Test MSE of 410730294835.549744
Epoch 3: training loss 430066386823.529
Test Loss of 410729451381.634399, Test MSE of 410729448108.290588
Epoch 4: training loss 430066017701.647
Test Loss of 410728917813.664062, Test MSE of 410728917107.244812
Epoch 5: training loss 430065781579.294
Test Loss of 410728273507.746399, Test MSE of 410728278021.990845
Epoch 6: training loss 421948734765.176
Test Loss of 385125143276.690430, Test MSE of 385125142401.654297
Epoch 7: training loss 368626757511.529
Test Loss of 308644373939.472473, Test MSE of 308644374037.848999
Epoch 8: training loss 282072778149.647
Test Loss of 217884167371.283661, Test MSE of 217884167503.063507
Epoch 9: training loss 203205487194.353
Test Loss of 152999979662.867188, Test MSE of 152999978057.848114
Epoch 10: training loss 159537681739.294
Test Loss of 124082945198.852386, Test MSE of 124082945093.018478
Epoch 11: training loss 142828650827.294
Test Loss of 112317747311.829712, Test MSE of 112317746906.312988
Epoch 12: training loss 136285677778.824
Test Loss of 108850548188.223969, Test MSE of 108850548713.177139
Epoch 13: training loss 134667166870.588
Test Loss of 106261088527.992599, Test MSE of 106261086736.284760
Epoch 14: training loss 130200384391.529
Test Loss of 103896446294.596939, Test MSE of 103896448514.505402
Epoch 15: training loss 127586028212.706
Test Loss of 100845163582.074966, Test MSE of 100845163260.318192
Epoch 16: training loss 122771128169.412
Test Loss of 97920970499.909302, Test MSE of 97920969930.349472
Epoch 17: training loss 119727272146.824
Test Loss of 94291932316.845901, Test MSE of 94291932797.339157
Epoch 18: training loss 114977686287.059
Test Loss of 91927977146.224899, Test MSE of 91927977138.508545
Epoch 19: training loss 111034700950.588
Test Loss of 87626841080.892181, Test MSE of 87626840827.774826
Epoch 20: training loss 108555965711.059
Test Loss of 84909591645.349380, Test MSE of 84909591063.188599
Epoch 21: training loss 103445899986.824
Test Loss of 81741255025.606659, Test MSE of 81741256625.894119
Epoch 22: training loss 98921822328.471
Test Loss of 78012485535.333649, Test MSE of 78012486763.211029
Epoch 23: training loss 95934836720.941
Test Loss of 76950004602.846832, Test MSE of 76950005306.039597
Epoch 24: training loss 90982648560.941
Test Loss of 73958016602.269318, Test MSE of 73958016101.649994
Epoch 25: training loss 87805656244.706
Test Loss of 70124333191.522446, Test MSE of 70124333307.744141
Epoch 26: training loss 84582471966.118
Test Loss of 67459486217.240166, Test MSE of 67459485525.276924
Epoch 27: training loss 80812380626.824
Test Loss of 64437093739.446556, Test MSE of 64437095057.109543
Epoch 28: training loss 77223941391.059
Test Loss of 60302666295.677925, Test MSE of 60302667264.995590
Epoch 29: training loss 73547637594.353
Test Loss of 58918014287.962982, Test MSE of 58918015215.676216
Epoch 30: training loss 70639259919.059
Test Loss of 55065760085.175385, Test MSE of 55065760018.401779
Epoch 31: training loss 67179714951.529
Test Loss of 55697253938.465523, Test MSE of 55697253901.444740
Epoch 32: training loss 64547187975.529
Test Loss of 51250088668.579361, Test MSE of 51250089176.319122
Epoch 33: training loss 60998165187.765
Test Loss of 49181294415.726051, Test MSE of 49181293784.263214
Epoch 34: training loss 57838820352.000
Test Loss of 47601999092.982880, Test MSE of 47601999418.353180
Epoch 35: training loss 55663975408.941
Test Loss of 43335844090.195282, Test MSE of 43335844229.328018
Epoch 36: training loss 52130040929.882
Test Loss of 44684322862.437759, Test MSE of 44684322435.845657
Epoch 37: training loss 50070381221.647
Test Loss of 40567268025.514114, Test MSE of 40567268049.339371
Epoch 38: training loss 47571515745.882
Test Loss of 40251219575.648308, Test MSE of 40251219566.870010
Epoch 39: training loss 44854729253.647
Test Loss of 36385112668.638596, Test MSE of 36385113453.073936
Epoch 40: training loss 42377326253.176
Test Loss of 35231927275.624245, Test MSE of 35231927509.438370
Epoch 41: training loss 40350582038.588
Test Loss of 34373551941.301247, Test MSE of 34373552224.721649
Epoch 42: training loss 38472096037.647
Test Loss of 34368782246.915314, Test MSE of 34368781899.732971
Epoch 43: training loss 36062952749.176
Test Loss of 29178075589.952801, Test MSE of 29178075822.399940
Epoch 44: training loss 35091249054.118
Test Loss of 30898557715.546505, Test MSE of 30898557403.213623
Epoch 45: training loss 32610651241.412
Test Loss of 27098796043.372513, Test MSE of 27098796460.605621
Epoch 46: training loss 31128165812.706
Test Loss of 25885343841.614067, Test MSE of 25885343698.204136
Epoch 47: training loss 29929127408.941
Test Loss of 26763873294.689495, Test MSE of 26763873343.659538
Epoch 48: training loss 28598210529.882
Test Loss of 26027712398.274872, Test MSE of 26027712586.633083
Epoch 49: training loss 27077886147.765
Test Loss of 24691160690.909763, Test MSE of 24691160844.186951
Epoch 50: training loss 25777732807.529
Test Loss of 21231382647.411385, Test MSE of 21231382761.250149
Epoch 51: training loss 24424692939.294
Test Loss of 22525631387.068951, Test MSE of 22525631789.526081
Epoch 52: training loss 23793903845.647
Test Loss of 20424879380.257290, Test MSE of 20424879460.471157
Epoch 53: training loss 22639250889.412
Test Loss of 21867345335.263306, Test MSE of 21867345560.582535
Epoch 54: training loss 21847115550.118
Test Loss of 22925737363.250347, Test MSE of 22925737245.382259
Epoch 55: training loss 21017783898.353
Test Loss of 20700924863.081905, Test MSE of 20700925069.402771
Epoch 56: training loss 20217347045.647
Test Loss of 19695528232.633041, Test MSE of 19695528035.999939
Epoch 57: training loss 19813327631.059
Test Loss of 23363295745.658493, Test MSE of 23363295825.440292
Epoch 58: training loss 19169267105.882
Test Loss of 19152098903.426193, Test MSE of 19152099214.577190
Epoch 59: training loss 18347671439.059
Test Loss of 19401613293.045811, Test MSE of 19401613370.993130
Epoch 60: training loss 17422978145.882
Test Loss of 17111255712.399815, Test MSE of 17111255623.684362
Epoch 61: training loss 17395823907.765
Test Loss of 20972079167.496529, Test MSE of 20972079345.940136
Epoch 62: training loss 16663105404.235
Test Loss of 18977317795.598335, Test MSE of 18977317794.357903
Epoch 63: training loss 16289360583.529
Test Loss of 18745168746.261917, Test MSE of 18745168794.006550
Epoch 64: training loss 15829118482.824
Test Loss of 17322075951.030079, Test MSE of 17322075980.111206
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 227006941526.64932, 'MSE - std': 209021884768.92072, 'R2 - mean': -0.5746386063970514, 'R2 - std': 1.4294680696397029} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005263 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043272071.529
Test Loss of 431611163240.011108, Test MSE of 431611156900.838623
Epoch 2: training loss 424023145773.176
Test Loss of 431590530984.810730, Test MSE of 431590535234.230469
Epoch 3: training loss 423995690526.118
Test Loss of 431562713606.397034, Test MSE of 431562711891.859558
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009940630.588
Test Loss of 431565180016.303589, Test MSE of 431565182578.980652
Epoch 2: training loss 423997114608.941
Test Loss of 431567334243.154114, Test MSE of 431567340336.871643
Epoch 3: training loss 423996541891.765
Test Loss of 431566777975.648315, Test MSE of 431566784363.595642
Epoch 4: training loss 423996119642.353
Test Loss of 431566641641.965759, Test MSE of 431566629009.012878
Epoch 5: training loss 423995852920.471
Test Loss of 431567081091.020813, Test MSE of 431567073987.719360
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 267918968018.86334, 'MSE - std': 204076687293.4781, 'R2 - mean': -0.9043026714043781, 'R2 - std': 1.4385467502741043} 
 

Saving model.....
Results After CV: {'MSE - mean': 267918968018.86334, 'MSE - std': 204076687293.4781, 'R2 - mean': -0.9043026714043781, 'R2 - std': 1.4385467502741043}
Train time: 48.00661845599971
Inference time: 0.07930040340070263
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 79 finished with value: 267918968018.86334 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005485 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525925586.824
Test Loss of 418112869994.000488, Test MSE of 418112873002.986816
Epoch 2: training loss 427505102848.000
Test Loss of 418095016739.945435, Test MSE of 418095017237.825806
Epoch 3: training loss 427477407021.176
Test Loss of 418071456873.882019, Test MSE of 418071453145.689026
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494517097.412
Test Loss of 418075446353.247253, Test MSE of 418075448626.927063
Epoch 2: training loss 427483969415.529
Test Loss of 418075849930.999756, Test MSE of 418075846809.268921
Epoch 3: training loss 427483485967.059
Test Loss of 418075396788.141541, Test MSE of 418075394665.983337
Epoch 4: training loss 421791290187.294
Test Loss of 400199330664.164673, Test MSE of 400199329598.459839
Epoch 5: training loss 384694501014.588
Test Loss of 344927816015.885254, Test MSE of 344927820110.922607
Epoch 6: training loss 315648332498.824
Test Loss of 267590441039.115417, Test MSE of 267590439289.767303
Epoch 7: training loss 224396176835.765
Test Loss of 170569503092.363647, Test MSE of 170569503521.886078
Epoch 8: training loss 161254229744.941
Test Loss of 130234733424.218369, Test MSE of 130234736453.362564
Epoch 9: training loss 140557329257.412
Test Loss of 120515723457.287994, Test MSE of 120515723693.080658
Epoch 10: training loss 137057161472.000
Test Loss of 116028916843.303268, Test MSE of 116028916990.051483
Epoch 11: training loss 133277339934.118
Test Loss of 113093754876.210037, Test MSE of 113093755449.419312
Epoch 12: training loss 130180530507.294
Test Loss of 110226658726.817490, Test MSE of 110226659572.433441
Epoch 13: training loss 125637699945.412
Test Loss of 106819695932.698593, Test MSE of 106819697222.512726
Epoch 14: training loss 122814110388.706
Test Loss of 103450577662.045807, Test MSE of 103450577716.767258
Epoch 15: training loss 119008542192.941
Test Loss of 101792509640.275742, Test MSE of 101792509831.177429
Epoch 16: training loss 114674881310.118
Test Loss of 98006576347.580841, Test MSE of 98006575660.534531
Epoch 17: training loss 109918379926.588
Test Loss of 93074934637.849640, Test MSE of 93074936779.839142
Epoch 18: training loss 107396540416.000
Test Loss of 91813636731.292160, Test MSE of 91813635780.104523
Epoch 19: training loss 102485033953.882
Test Loss of 86898102418.860977, Test MSE of 86898102791.471420
Epoch 20: training loss 98700377840.941
Test Loss of 83234116608.236877, Test MSE of 83234116232.831604
Epoch 21: training loss 96676986849.882
Test Loss of 81305146087.779785, Test MSE of 81305147719.113434
Epoch 22: training loss 92318478983.529
Test Loss of 77118638713.397171, Test MSE of 77118639366.229034
Epoch 23: training loss 87561403648.000
Test Loss of 74476531688.312744, Test MSE of 74476533310.551682
Epoch 24: training loss 84872478855.529
Test Loss of 72098720595.319916, Test MSE of 72098721344.099792
Epoch 25: training loss 81560016301.176
Test Loss of 68498200188.950264, Test MSE of 68498199984.863434
Epoch 26: training loss 77234195712.000
Test Loss of 65771649840.262779, Test MSE of 65771650840.978889
Epoch 27: training loss 74456223789.176
Test Loss of 63670470216.838310, Test MSE of 63670470806.736526
Epoch 28: training loss 70520971602.824
Test Loss of 61169795216.018509, Test MSE of 61169794116.643509
Epoch 29: training loss 67824016323.765
Test Loss of 58432910756.922508, Test MSE of 58432911034.104172
Epoch 30: training loss 63766349673.412
Test Loss of 55571105370.840622, Test MSE of 55571106091.092735
Epoch 31: training loss 61856949089.882
Test Loss of 52518208532.134163, Test MSE of 52518209380.761902
Epoch 32: training loss 58524269628.235
Test Loss of 49528518111.666901, Test MSE of 49528519361.075409
Epoch 33: training loss 55664483870.118
Test Loss of 46728980872.734673, Test MSE of 46728980082.241646
Epoch 34: training loss 53067548656.941
Test Loss of 46512183876.337730, Test MSE of 46512184239.984100
Epoch 35: training loss 50457932020.706
Test Loss of 46416240151.095070, Test MSE of 46416239877.776688
Epoch 36: training loss 48170275373.176
Test Loss of 41522051388.935463, Test MSE of 41522051099.570488
Epoch 37: training loss 45167063623.529
Test Loss of 40725436100.012032, Test MSE of 40725435626.314972
Epoch 38: training loss 42658232591.059
Test Loss of 37432765862.343742, Test MSE of 37432766817.394211
Epoch 39: training loss 40224518810.353
Test Loss of 33195236840.668056, Test MSE of 33195237417.814583
Epoch 40: training loss 39302623646.118
Test Loss of 34936373392.965996, Test MSE of 34936373874.651596
Epoch 41: training loss 37207992387.765
Test Loss of 33929407061.392551, Test MSE of 33929407492.630581
Epoch 42: training loss 35239857005.176
Test Loss of 32340822970.596344, Test MSE of 32340822999.146778
Epoch 43: training loss 33738095706.353
Test Loss of 32698530572.258156, Test MSE of 32698530601.268707
Epoch 44: training loss 32373841152.000
Test Loss of 28449898620.121212, Test MSE of 28449898393.135967
Epoch 45: training loss 30582342791.529
Test Loss of 26952196276.023132, Test MSE of 26952196811.307678
Epoch 46: training loss 29206070497.882
Test Loss of 26428986097.254684, Test MSE of 26428986506.803421
Epoch 47: training loss 28206631130.353
Test Loss of 25990159157.947723, Test MSE of 25990159505.039497
Epoch 48: training loss 26608341929.412
Test Loss of 25562683945.808002, Test MSE of 25562683889.790028
Epoch 49: training loss 25402863461.647
Test Loss of 23756792936.934536, Test MSE of 23756793010.776546
Epoch 50: training loss 24978893722.353
Test Loss of 24013423980.073097, Test MSE of 24013423988.931538
Epoch 51: training loss 23795991043.765
Test Loss of 23015419651.020126, Test MSE of 23015419819.014771
Epoch 52: training loss 22693554251.294
Test Loss of 21629154597.011333, Test MSE of 21629154785.502651
Epoch 53: training loss 22195143375.059
Test Loss of 21434896012.583855, Test MSE of 21434896040.859043
Epoch 54: training loss 21140331749.647
Test Loss of 19119283681.561878, Test MSE of 19119283610.543968
Epoch 55: training loss 20464527668.706
Test Loss of 19089896464.817951, Test MSE of 19089896423.233883
Epoch 56: training loss 19669636257.882
Test Loss of 20818078051.308815, Test MSE of 20818078221.649586
Epoch 57: training loss 18940634796.235
Test Loss of 19245135418.152210, Test MSE of 19245135148.840405
Epoch 58: training loss 18237189236.706
Test Loss of 19983630823.483692, Test MSE of 19983630748.519489
Epoch 59: training loss 17755300506.353
Test Loss of 19835082757.684940, Test MSE of 19835083099.744324
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19835083099.744324, 'MSE - std': 0.0, 'R2 - mean': 0.8455420558121448, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005505 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918316724.706
Test Loss of 424556720146.002319, Test MSE of 424556724817.276062
Epoch 2: training loss 427897850458.353
Test Loss of 424540981599.045105, Test MSE of 424540977780.235718
Epoch 3: training loss 427870597120.000
Test Loss of 424519997973.910706, Test MSE of 424520003106.659973
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887321810.824
Test Loss of 424527582875.269958, Test MSE of 424527588098.392639
Epoch 2: training loss 427879079213.176
Test Loss of 424527487951.914856, Test MSE of 424527494489.567383
Epoch 3: training loss 427878619979.294
Test Loss of 424527098518.295654, Test MSE of 424527095098.610657
Epoch 4: training loss 422320108724.706
Test Loss of 407444912917.733032, Test MSE of 407444919059.893738
Epoch 5: training loss 385169177178.353
Test Loss of 352806837675.081177, Test MSE of 352806840289.931885
Epoch 6: training loss 316546781665.882
Test Loss of 276499522102.599121, Test MSE of 276499521958.060242
Epoch 7: training loss 224805947934.118
Test Loss of 180014924662.850800, Test MSE of 180014923983.756653
Epoch 8: training loss 159313712730.353
Test Loss of 141822381654.576904, Test MSE of 141822383718.999237
Epoch 9: training loss 139625971531.294
Test Loss of 131335847765.451767, Test MSE of 131335846897.940140
Epoch 10: training loss 134548928240.941
Test Loss of 126489885027.782562, Test MSE of 126489885794.088852
Epoch 11: training loss 129862517940.706
Test Loss of 124273344248.597733, Test MSE of 124273345106.503647
Epoch 12: training loss 126580158343.529
Test Loss of 121484160793.759888, Test MSE of 121484158698.661453
Epoch 13: training loss 123831264195.765
Test Loss of 117921185941.703445, Test MSE of 117921184688.991882
Epoch 14: training loss 118288929551.059
Test Loss of 114328338540.013885, Test MSE of 114328340271.555695
Epoch 15: training loss 116004392176.941
Test Loss of 109612009163.591949, Test MSE of 109612010648.663193
Epoch 16: training loss 111714252679.529
Test Loss of 105919495175.343048, Test MSE of 105919492517.619202
Epoch 17: training loss 107557359766.588
Test Loss of 102685494950.166092, Test MSE of 102685497080.581970
Epoch 18: training loss 103584129596.235
Test Loss of 98751923348.992828, Test MSE of 98751920129.068100
Epoch 19: training loss 99951595339.294
Test Loss of 94813578501.744156, Test MSE of 94813579715.985199
Epoch 20: training loss 95442067275.294
Test Loss of 89540068766.290070, Test MSE of 89540070505.167572
Epoch 21: training loss 92402636589.176
Test Loss of 87860738661.499878, Test MSE of 87860738086.135147
Epoch 22: training loss 87276612623.059
Test Loss of 82763665414.632431, Test MSE of 82763666199.189377
Epoch 23: training loss 83586454031.059
Test Loss of 80046465495.850098, Test MSE of 80046465284.758850
Epoch 24: training loss 80368316596.706
Test Loss of 77008710335.511444, Test MSE of 77008711392.824600
Epoch 25: training loss 76654365244.235
Test Loss of 71935873985.702515, Test MSE of 71935873423.044724
Epoch 26: training loss 72294227260.235
Test Loss of 72012884268.828125, Test MSE of 72012883978.447052
Epoch 27: training loss 69990484766.118
Test Loss of 64928479348.067543, Test MSE of 64928480180.843124
Epoch 28: training loss 65925039992.471
Test Loss of 62196171040.984505, Test MSE of 62196169244.282883
Epoch 29: training loss 63207570447.059
Test Loss of 60167397549.390701, Test MSE of 60167396730.743729
Epoch 30: training loss 59265687160.471
Test Loss of 57497088580.811470, Test MSE of 57497088227.164680
Epoch 31: training loss 56636252928.000
Test Loss of 53808430879.918571, Test MSE of 53808431415.919113
Epoch 32: training loss 53873945810.824
Test Loss of 55908866542.589867, Test MSE of 55908866939.548340
Epoch 33: training loss 50845722089.412
Test Loss of 50045394157.109413, Test MSE of 50045394026.247124
Epoch 34: training loss 47560277074.824
Test Loss of 46546530207.829750, Test MSE of 46546530556.254646
Epoch 35: training loss 46001071555.765
Test Loss of 45913196500.415451, Test MSE of 45913195902.670929
Epoch 36: training loss 43088123535.059
Test Loss of 44975845344.259079, Test MSE of 44975845202.530334
Epoch 37: training loss 40570181579.294
Test Loss of 38481736918.369652, Test MSE of 38481736766.011551
Epoch 38: training loss 38305897637.647
Test Loss of 39085205774.508446, Test MSE of 39085204833.246498
Epoch 39: training loss 36349088677.647
Test Loss of 36729532099.064537, Test MSE of 36729531387.004120
Epoch 40: training loss 34584900826.353
Test Loss of 36442524506.426094, Test MSE of 36442524651.229240
Epoch 41: training loss 32453606987.294
Test Loss of 36338454345.608139, Test MSE of 36338453711.580063
Epoch 42: training loss 30951028713.412
Test Loss of 35508175695.529953, Test MSE of 35508176119.559433
Epoch 43: training loss 29563555576.471
Test Loss of 33004133292.857738, Test MSE of 33004132999.142319
Epoch 44: training loss 27877372792.471
Test Loss of 32483238826.962757, Test MSE of 32483238544.479881
Epoch 45: training loss 25929849366.588
Test Loss of 31902273685.229702, Test MSE of 31902273422.528648
Epoch 46: training loss 25409087435.294
Test Loss of 29441131926.236408, Test MSE of 29441132266.504662
Epoch 47: training loss 24046886102.588
Test Loss of 30968149384.024055, Test MSE of 30968149798.332359
Epoch 48: training loss 23297954048.000
Test Loss of 28019062395.529030, Test MSE of 28019062243.909248
Epoch 49: training loss 22074902991.059
Test Loss of 32877369878.384457, Test MSE of 32877369153.410225
Epoch 50: training loss 20944111600.941
Test Loss of 28286638460.417301, Test MSE of 28286639304.346729
Epoch 51: training loss 20146538876.235
Test Loss of 31558190666.733288, Test MSE of 31558189581.344486
Epoch 52: training loss 19357860257.882
Test Loss of 28123090358.687946, Test MSE of 28123089271.554115
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23979086185.64922, 'MSE - std': 4144003085.904896, 'R2 - mean': 0.8223809350650708, 'R2 - std': 0.02316112074707405} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005903 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926371689.412
Test Loss of 447257848210.209595, Test MSE of 447257850455.006714
Epoch 2: training loss 421905339331.765
Test Loss of 447238621451.665955, Test MSE of 447238624533.204346
Epoch 3: training loss 421877862881.882
Test Loss of 447214047376.966003, Test MSE of 447214049990.894531
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897791849.412
Test Loss of 447222912625.580383, Test MSE of 447222910546.989685
Epoch 2: training loss 421887180318.118
Test Loss of 447223695934.652771, Test MSE of 447223697676.819458
Epoch 3: training loss 421886571700.706
Test Loss of 447222854730.141113, Test MSE of 447222857570.474976
Epoch 4: training loss 416072556905.412
Test Loss of 429188972862.356689, Test MSE of 429188969870.153992
Epoch 5: training loss 378886496135.529
Test Loss of 372277102404.870667, Test MSE of 372277102854.469788
Epoch 6: training loss 309902279378.824
Test Loss of 293424674707.512390, Test MSE of 293424675581.279175
Epoch 7: training loss 218845708167.529
Test Loss of 194221612288.532959, Test MSE of 194221615721.370148
Epoch 8: training loss 156201833381.647
Test Loss of 153868104584.853119, Test MSE of 153868103769.965179
Epoch 9: training loss 137052294746.353
Test Loss of 142376239107.789948, Test MSE of 142376239896.466492
Epoch 10: training loss 131472696410.353
Test Loss of 138078109805.908875, Test MSE of 138078109665.107513
Epoch 11: training loss 128837330040.471
Test Loss of 134262942822.092072, Test MSE of 134262939630.035416
Epoch 12: training loss 126316333266.824
Test Loss of 130969034583.346756, Test MSE of 130969037131.964844
Epoch 13: training loss 121906000534.588
Test Loss of 126799468316.128616, Test MSE of 126799468099.833069
Epoch 14: training loss 118404724886.588
Test Loss of 124294976417.487854, Test MSE of 124294975000.611130
Epoch 15: training loss 114195391307.294
Test Loss of 121135157078.399261, Test MSE of 121135158937.780701
Epoch 16: training loss 110001030083.765
Test Loss of 116248816798.941483, Test MSE of 116248814110.067856
Epoch 17: training loss 106663209712.941
Test Loss of 113925125372.269257, Test MSE of 113925126484.100342
Epoch 18: training loss 102625572954.353
Test Loss of 109022657381.322235, Test MSE of 109022659149.032700
Epoch 19: training loss 97652601253.647
Test Loss of 106248037813.029846, Test MSE of 106248037103.365829
Epoch 20: training loss 95105510264.471
Test Loss of 102467435604.089752, Test MSE of 102467437186.262070
Epoch 21: training loss 91506521615.059
Test Loss of 96884570018.672226, Test MSE of 96884572547.695496
Epoch 22: training loss 87354753566.118
Test Loss of 93692037385.770996, Test MSE of 93692037203.668839
Epoch 23: training loss 83453751627.294
Test Loss of 88137852761.715469, Test MSE of 88137851955.569336
Epoch 24: training loss 79806498800.941
Test Loss of 85855754036.763351, Test MSE of 85855755207.559891
Epoch 25: training loss 76784818763.294
Test Loss of 81375404032.000000, Test MSE of 81375404718.977341
Epoch 26: training loss 72673155960.471
Test Loss of 78011822804.829987, Test MSE of 78011823167.375870
Epoch 27: training loss 69953639047.529
Test Loss of 78174822934.384460, Test MSE of 78174821722.616928
Epoch 28: training loss 66891896696.471
Test Loss of 73460858601.437897, Test MSE of 73460858072.559326
Epoch 29: training loss 62817667568.941
Test Loss of 69484145133.168640, Test MSE of 69484147336.251587
Epoch 30: training loss 59490822580.706
Test Loss of 65154415421.527641, Test MSE of 65154416720.849976
Epoch 31: training loss 57391900310.588
Test Loss of 59482605716.282211, Test MSE of 59482606179.071548
Epoch 32: training loss 53844548065.882
Test Loss of 57356191924.496880, Test MSE of 57356191705.433464
Epoch 33: training loss 51587140946.824
Test Loss of 60140580438.576912, Test MSE of 60140579648.566963
Epoch 34: training loss 48354106631.529
Test Loss of 52964196297.045570, Test MSE of 52964198149.670280
Epoch 35: training loss 46296595847.529
Test Loss of 49950054889.378670, Test MSE of 49950054359.123299
Epoch 36: training loss 43932267648.000
Test Loss of 47296513311.326393, Test MSE of 47296513282.349411
Epoch 37: training loss 41533376045.176
Test Loss of 49199812171.207031, Test MSE of 49199812901.269142
Epoch 38: training loss 39786900728.471
Test Loss of 43297237268.193382, Test MSE of 43297237444.531792
Epoch 39: training loss 37708814366.118
Test Loss of 40833998759.883415, Test MSE of 40833999018.189125
Epoch 40: training loss 35451298800.941
Test Loss of 42553306788.271111, Test MSE of 42553306702.687241
Epoch 41: training loss 34396925312.000
Test Loss of 40590132644.922508, Test MSE of 40590133245.406502
Epoch 42: training loss 32044672896.000
Test Loss of 37422336519.698357, Test MSE of 37422337062.836182
Epoch 43: training loss 30699457807.059
Test Loss of 35456185756.395096, Test MSE of 35456186540.007301
Epoch 44: training loss 29093271841.882
Test Loss of 34514721938.624107, Test MSE of 34514722463.036087
Epoch 45: training loss 28105271416.471
Test Loss of 33719078658.072636, Test MSE of 33719078645.363693
Epoch 46: training loss 26755646912.000
Test Loss of 35316261362.379829, Test MSE of 35316261691.334686
Epoch 47: training loss 25556877982.118
Test Loss of 32554742507.569744, Test MSE of 32554742849.725342
Epoch 48: training loss 24366378955.294
Test Loss of 31083393049.582233, Test MSE of 31083392870.542641
Epoch 49: training loss 23549630407.529
Test Loss of 28821466757.477676, Test MSE of 28821466490.265732
Epoch 50: training loss 22690094177.882
Test Loss of 29280881017.574833, Test MSE of 29280880853.986637
Epoch 51: training loss 21531146526.118
Test Loss of 27710138105.071480, Test MSE of 27710138206.374134
Epoch 52: training loss 20665841912.471
Test Loss of 26737707067.691879, Test MSE of 26737706880.025276
Epoch 53: training loss 19698771990.588
Test Loss of 27300803361.102940, Test MSE of 27300803802.887608
Epoch 54: training loss 19050537765.647
Test Loss of 25257699531.236641, Test MSE of 25257699745.705048
Epoch 55: training loss 18942035087.059
Test Loss of 26925628884.297016, Test MSE of 26925629208.840488
Epoch 56: training loss 17880994454.588
Test Loss of 27253284294.558407, Test MSE of 27253284380.799786
Epoch 57: training loss 17326811557.647
Test Loss of 22668551427.849178, Test MSE of 22668551352.048695
Epoch 58: training loss 16756315572.706
Test Loss of 25328635598.908165, Test MSE of 25328635353.845997
Epoch 59: training loss 16347947004.235
Test Loss of 24458154309.462872, Test MSE of 24458154409.231010
Epoch 60: training loss 15901227640.471
Test Loss of 22953607248.062920, Test MSE of 22953606795.783417
Epoch 61: training loss 15540928794.353
Test Loss of 25425294764.028683, Test MSE of 25425295407.493065
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24461155926.263836, 'MSE - std': 3451563440.9581237, 'R2 - mean': 0.8251691188412514, 'R2 - std': 0.01931768482625554} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005342 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109203757.176
Test Loss of 410765643190.315613, Test MSE of 410765638783.017822
Epoch 2: training loss 430087391593.412
Test Loss of 410746251918.867188, Test MSE of 410746248059.967590
Epoch 3: training loss 430059360015.059
Test Loss of 410721950041.913940, Test MSE of 410721948750.601196
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077649618.824
Test Loss of 410728763496.248047, Test MSE of 410728760000.600281
Epoch 2: training loss 430065193200.941
Test Loss of 410728898359.085632, Test MSE of 410728903494.588013
Epoch 3: training loss 430064658793.412
Test Loss of 410727933894.189697, Test MSE of 410727938497.728455
Epoch 4: training loss 425130940536.471
Test Loss of 394796192826.284119, Test MSE of 394796188012.777893
Epoch 5: training loss 389710129152.000
Test Loss of 340726752047.030090, Test MSE of 340726753828.307617
Epoch 6: training loss 321538323395.765
Test Loss of 263296144472.610840, Test MSE of 263296147274.177521
Epoch 7: training loss 229912802304.000
Test Loss of 165944688677.908386, Test MSE of 165944690030.607025
Epoch 8: training loss 163237726689.882
Test Loss of 125162960472.373901, Test MSE of 125162960315.589630
Epoch 9: training loss 142926813455.059
Test Loss of 114426776195.494675, Test MSE of 114426779433.570526
Epoch 10: training loss 136945957677.176
Test Loss of 109418711782.056458, Test MSE of 109418713388.612396
Epoch 11: training loss 133758393434.353
Test Loss of 106771485826.310043, Test MSE of 106771485194.592941
Epoch 12: training loss 130525533756.235
Test Loss of 104313497280.621933, Test MSE of 104313499441.574585
Epoch 13: training loss 127725525413.647
Test Loss of 100246033871.903748, Test MSE of 100246034917.800522
Epoch 14: training loss 122759360752.941
Test Loss of 97236013345.999069, Test MSE of 97236011430.542145
Epoch 15: training loss 119564656429.176
Test Loss of 94395399126.774643, Test MSE of 94395399183.094116
Epoch 16: training loss 114949240560.941
Test Loss of 91865922264.788528, Test MSE of 91865924203.517838
Epoch 17: training loss 111724925500.235
Test Loss of 88386267430.737625, Test MSE of 88386267298.587677
Epoch 18: training loss 108256954548.706
Test Loss of 85336163953.014343, Test MSE of 85336164614.600250
Epoch 19: training loss 103717261312.000
Test Loss of 81031325088.518280, Test MSE of 81031325404.261368
Epoch 20: training loss 99043138258.824
Test Loss of 78343582167.959274, Test MSE of 78343582277.845184
Epoch 21: training loss 96352573229.176
Test Loss of 76477883649.303101, Test MSE of 76477884043.931595
Epoch 22: training loss 91485402352.941
Test Loss of 71835220962.147156, Test MSE of 71835221126.017365
Epoch 23: training loss 88835113818.353
Test Loss of 68431255306.543266, Test MSE of 68431255028.291183
Epoch 24: training loss 84071727570.824
Test Loss of 65351103106.546967, Test MSE of 65351103820.505913
Epoch 25: training loss 80538644148.706
Test Loss of 63145162051.642761, Test MSE of 63145160570.064735
Epoch 26: training loss 76954718313.412
Test Loss of 60376750566.174919, Test MSE of 60376749524.515381
Epoch 27: training loss 73328136207.059
Test Loss of 60042887793.488197, Test MSE of 60042886279.320564
Epoch 28: training loss 70129352237.176
Test Loss of 55891348091.913002, Test MSE of 55891347631.829506
Epoch 29: training loss 66397229221.647
Test Loss of 51655144745.106895, Test MSE of 51655144370.567947
Epoch 30: training loss 62792981571.765
Test Loss of 51853806818.028694, Test MSE of 51853806733.811691
Epoch 31: training loss 60125931316.706
Test Loss of 48675682145.732529, Test MSE of 48675681081.649498
Epoch 32: training loss 57879307565.176
Test Loss of 47184713319.537254, Test MSE of 47184714254.884476
Epoch 33: training loss 53917485831.529
Test Loss of 44428088963.968536, Test MSE of 44428089098.661110
Epoch 34: training loss 52350464406.588
Test Loss of 42981965962.365570, Test MSE of 42981965288.301506
Epoch 35: training loss 49004514258.824
Test Loss of 41024910943.955574, Test MSE of 41024911580.190971
Epoch 36: training loss 46229725304.471
Test Loss of 37026001745.147614, Test MSE of 37026001329.529526
Epoch 37: training loss 44184873660.235
Test Loss of 36448174678.952339, Test MSE of 36448174434.345337
Epoch 38: training loss 41825969746.824
Test Loss of 36068176493.697365, Test MSE of 36068176226.726387
Epoch 39: training loss 39551014881.882
Test Loss of 33341666669.341972, Test MSE of 33341666958.510197
Epoch 40: training loss 37776379998.118
Test Loss of 33445457895.359554, Test MSE of 33445458071.545307
Epoch 41: training loss 36184506405.647
Test Loss of 29776283046.204536, Test MSE of 29776282516.018513
Epoch 42: training loss 34115028043.294
Test Loss of 31506705806.511799, Test MSE of 31506705939.149021
Epoch 43: training loss 32528790279.529
Test Loss of 30407075816.307266, Test MSE of 30407075993.623852
Epoch 44: training loss 30988520997.647
Test Loss of 26787778149.641834, Test MSE of 26787777538.539856
Epoch 45: training loss 29450908483.765
Test Loss of 29737108496.584915, Test MSE of 29737108294.498543
Epoch 46: training loss 28523774622.118
Test Loss of 27255754063.962978, Test MSE of 27255754133.576210
Epoch 47: training loss 27069975363.765
Test Loss of 27482576982.241554, Test MSE of 27482577059.413456
Epoch 48: training loss 25564552082.824
Test Loss of 26499758189.934288, Test MSE of 26499757823.793865
Epoch 49: training loss 24861611297.882
Test Loss of 23711256824.299862, Test MSE of 23711256843.074230
Epoch 50: training loss 23892175815.529
Test Loss of 23452031970.147156, Test MSE of 23452031517.349686
Epoch 51: training loss 22697824455.529
Test Loss of 25153469590.211941, Test MSE of 25153469745.560757
Epoch 52: training loss 21805398580.706
Test Loss of 22827566825.373440, Test MSE of 22827566942.643829
Epoch 53: training loss 21160155328.000
Test Loss of 21604572588.838501, Test MSE of 21604572844.499668
Epoch 54: training loss 20793676001.882
Test Loss of 20795444041.565941, Test MSE of 20795444257.567661
Epoch 55: training loss 19793591570.824
Test Loss of 23057184883.620544, Test MSE of 23057185300.908962
Epoch 56: training loss 18883439732.706
Test Loss of 21296125380.531235, Test MSE of 21296125526.269066
Epoch 57: training loss 18162088756.706
Test Loss of 21265260516.516426, Test MSE of 21265260435.140293
Epoch 58: training loss 17958781440.000
Test Loss of 21262579036.283203, Test MSE of 21262578910.954857
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23661511672.436592, 'MSE - std': 3294428696.7933917, 'R2 - mean': 0.8250040528869212, 'R2 - std': 0.016732048607171344} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005453 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043347365.647
Test Loss of 431613102435.391052, Test MSE of 431613100900.149902
Epoch 2: training loss 424023967382.588
Test Loss of 431593248287.511353, Test MSE of 431593253685.927734
Epoch 3: training loss 423996855597.176
Test Loss of 431566280026.387756, Test MSE of 431566284096.954407
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011233761.882
Test Loss of 431568785830.678406, Test MSE of 431568784898.764404
Epoch 2: training loss 423999867361.882
Test Loss of 431569031871.200378, Test MSE of 431569034859.554382
Epoch 3: training loss 423999305366.588
Test Loss of 431568878509.075439, Test MSE of 431568881401.153687
Epoch 4: training loss 419012061545.412
Test Loss of 415148163246.378540, Test MSE of 415148159829.924438
Epoch 5: training loss 383300833400.471
Test Loss of 359944958674.154541, Test MSE of 359944959785.830383
Epoch 6: training loss 315743734121.412
Test Loss of 281142467421.467834, Test MSE of 281142467627.418335
Epoch 7: training loss 224917354435.765
Test Loss of 181223844836.042572, Test MSE of 181223843916.923828
Epoch 8: training loss 160585095439.059
Test Loss of 138493822853.745483, Test MSE of 138493821792.088776
Epoch 9: training loss 141749927002.353
Test Loss of 127007381262.334106, Test MSE of 127007382541.846954
Epoch 10: training loss 135273023036.235
Test Loss of 121845829520.170288, Test MSE of 121845830724.447678
Epoch 11: training loss 131774477161.412
Test Loss of 117867260968.277649, Test MSE of 117867261757.267624
Epoch 12: training loss 129537559145.412
Test Loss of 114742845697.776962, Test MSE of 114742846514.112274
Epoch 13: training loss 125808429206.588
Test Loss of 112443830474.809814, Test MSE of 112443830621.069382
Epoch 14: training loss 121474885089.882
Test Loss of 107336450878.193436, Test MSE of 107336450349.432877
Epoch 15: training loss 118444962048.000
Test Loss of 104768811752.899582, Test MSE of 104768812321.571045
Epoch 16: training loss 114715413383.529
Test Loss of 100383449341.986115, Test MSE of 100383448088.048218
Epoch 17: training loss 110289588208.941
Test Loss of 98385162708.168442, Test MSE of 98385161441.233429
Epoch 18: training loss 106248311868.235
Test Loss of 94738696058.372971, Test MSE of 94738698234.919266
Epoch 19: training loss 102823754721.882
Test Loss of 92299256547.213333, Test MSE of 92299254600.403793
Epoch 20: training loss 98660665088.000
Test Loss of 88018969425.147614, Test MSE of 88018970544.114670
Epoch 21: training loss 95621389116.235
Test Loss of 84514943554.102737, Test MSE of 84514943875.005463
Epoch 22: training loss 91031813180.235
Test Loss of 81516228393.817673, Test MSE of 81516228676.985855
Epoch 23: training loss 88283594872.471
Test Loss of 76930932908.483109, Test MSE of 76930932546.552994
Epoch 24: training loss 85061735378.824
Test Loss of 75956896748.571960, Test MSE of 75956897533.210938
Epoch 25: training loss 81321858168.471
Test Loss of 71283386274.176773, Test MSE of 71283387094.463577
Epoch 26: training loss 76205385140.706
Test Loss of 67801118190.230446, Test MSE of 67801118421.989418
Epoch 27: training loss 72997471488.000
Test Loss of 65613884841.047661, Test MSE of 65613882752.694397
Epoch 28: training loss 70325739881.412
Test Loss of 60463549929.491905, Test MSE of 60463552291.624214
Epoch 29: training loss 67168180148.706
Test Loss of 59608824521.625175, Test MSE of 59608825306.826515
Epoch 30: training loss 63590529694.118
Test Loss of 56753108687.785286, Test MSE of 56753108365.698250
Epoch 31: training loss 61248875572.706
Test Loss of 51881758721.895416, Test MSE of 51881760772.800529
Epoch 32: training loss 57927179625.412
Test Loss of 47336023856.451645, Test MSE of 47336024980.887291
Epoch 33: training loss 56305907471.059
Test Loss of 45654013515.105972, Test MSE of 45654013889.335648
Epoch 34: training loss 52685388890.353
Test Loss of 45883591183.874130, Test MSE of 45883591260.629181
Epoch 35: training loss 49780591179.294
Test Loss of 42949703853.430817, Test MSE of 42949703926.383507
Epoch 36: training loss 47740545904.941
Test Loss of 40779991806.696899, Test MSE of 40779990775.225594
Epoch 37: training loss 45811956434.824
Test Loss of 40583037479.566864, Test MSE of 40583036863.274635
Epoch 38: training loss 43071733089.882
Test Loss of 37954956557.623322, Test MSE of 37954956941.081375
Epoch 39: training loss 41245687280.941
Test Loss of 37220344734.859787, Test MSE of 37220344823.070801
Epoch 40: training loss 39358508416.000
Test Loss of 33070014264.981026, Test MSE of 33070014203.273289
Epoch 41: training loss 37257734588.235
Test Loss of 33777184243.916706, Test MSE of 33777184478.532276
Epoch 42: training loss 35254262264.471
Test Loss of 30845788216.862564, Test MSE of 30845788084.611847
Epoch 43: training loss 33947123501.176
Test Loss of 27417535365.271633, Test MSE of 27417535653.216213
Epoch 44: training loss 32660949790.118
Test Loss of 28583439097.958351, Test MSE of 28583438258.322205
Epoch 45: training loss 30807929848.471
Test Loss of 31376680039.774178, Test MSE of 31376680122.622993
Epoch 46: training loss 29522362616.471
Test Loss of 26354633236.612679, Test MSE of 26354632524.249512
Epoch 47: training loss 28869570051.765
Test Loss of 25030855947.727905, Test MSE of 25030855508.676662
Epoch 48: training loss 27104825321.412
Test Loss of 24448920626.702454, Test MSE of 24448920694.306564
Epoch 49: training loss 26101708126.118
Test Loss of 25912565515.490978, Test MSE of 25912565625.458878
Epoch 50: training loss 25007552677.647
Test Loss of 22842916320.962517, Test MSE of 22842916049.801785
Epoch 51: training loss 24070352888.471
Test Loss of 22990876521.314205, Test MSE of 22990876314.674038
Epoch 52: training loss 23114599100.235
Test Loss of 23914809729.717724, Test MSE of 23914809946.337769
Epoch 53: training loss 22273369189.647
Test Loss of 25016632782.956039, Test MSE of 25016633053.424149
Epoch 54: training loss 21916918279.529
Test Loss of 25460252618.928272, Test MSE of 25460252546.093037
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24021259847.16788, 'MSE - std': 3033196885.712271, 'R2 - mean': 0.8219756200529222, 'R2 - std': 0.01614480666327013} 
 

Saving model.....
Results After CV: {'MSE - mean': 24021259847.16788, 'MSE - std': 3033196885.712271, 'R2 - mean': 0.8219756200529222, 'R2 - std': 0.01614480666327013}
Train time: 89.06332019420006
Inference time: 0.07333241480009747
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 80 finished with value: 24021259847.16788 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005460 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525074221.176
Test Loss of 418110045525.807068, Test MSE of 418110046146.462402
Epoch 2: training loss 427503986928.941
Test Loss of 418091646964.630127, Test MSE of 418091647741.776917
Epoch 3: training loss 427476642032.941
Test Loss of 418067435285.969910, Test MSE of 418067429250.166687
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490966829.176
Test Loss of 418073081865.001160, Test MSE of 418073089474.540222
Epoch 2: training loss 427479771497.412
Test Loss of 418074471666.320618, Test MSE of 418074473160.526245
Epoch 3: training loss 423805654678.588
Test Loss of 406444951417.930115, Test MSE of 406444955924.037476
Epoch 4: training loss 397495589827.765
Test Loss of 365161906456.457092, Test MSE of 365161911289.175720
Epoch 5: training loss 331874570601.412
Test Loss of 279248064884.363647, Test MSE of 279248072534.998413
Epoch 6: training loss 248164328448.000
Test Loss of 200967230903.872314, Test MSE of 200967231273.338501
Epoch 7: training loss 179891274029.176
Test Loss of 140958181512.912323, Test MSE of 140958182752.956757
Epoch 8: training loss 148039063401.412
Test Loss of 123020947849.208420, Test MSE of 123020949654.426102
Epoch 9: training loss 137369388724.706
Test Loss of 116591094139.469818, Test MSE of 116591095636.372101
Epoch 10: training loss 134100377027.765
Test Loss of 113285204995.553085, Test MSE of 113285203969.261078
Epoch 11: training loss 132582441652.706
Test Loss of 110898778880.414520, Test MSE of 110898779742.236877
Epoch 12: training loss 127475448560.941
Test Loss of 108320500697.863525, Test MSE of 108320500773.986755
Epoch 13: training loss 124721613161.412
Test Loss of 104302972203.170013, Test MSE of 104302973805.773224
Epoch 14: training loss 120367673163.294
Test Loss of 101784342517.577606, Test MSE of 101784341008.994141
Epoch 15: training loss 116345551525.647
Test Loss of 98859909790.823044, Test MSE of 98859912914.087585
Epoch 16: training loss 113298086460.235
Test Loss of 95064766021.995834, Test MSE of 95064767132.613754
Epoch 17: training loss 109136232719.059
Test Loss of 92139455040.074020, Test MSE of 92139456032.636185
Epoch 18: training loss 104870004419.765
Test Loss of 88956839849.304657, Test MSE of 88956840783.203354
Epoch 19: training loss 102004997029.647
Test Loss of 84734427136.000000, Test MSE of 84734426754.409988
Epoch 20: training loss 97858089050.353
Test Loss of 82540565754.374283, Test MSE of 82540565315.580780
Epoch 21: training loss 93333955568.941
Test Loss of 78849958327.635437, Test MSE of 78849956665.001877
Epoch 22: training loss 90199768591.059
Test Loss of 74105934867.186676, Test MSE of 74105935118.021484
Epoch 23: training loss 86420282680.471
Test Loss of 74109635153.602585, Test MSE of 74109634592.487488
Epoch 24: training loss 83329820099.765
Test Loss of 70031467115.658569, Test MSE of 70031468702.457611
Epoch 25: training loss 79228686599.529
Test Loss of 68693298088.594032, Test MSE of 68693298859.036263
Epoch 26: training loss 76791671853.176
Test Loss of 65199149604.596809, Test MSE of 65199148969.137970
Epoch 27: training loss 72665578104.471
Test Loss of 62042424306.024521, Test MSE of 62042424818.238594
Epoch 28: training loss 69636939128.471
Test Loss of 56066665970.142960, Test MSE of 56066664575.501442
Epoch 29: training loss 67365273042.824
Test Loss of 54509579005.098312, Test MSE of 54509577830.109764
Epoch 30: training loss 63717949296.941
Test Loss of 53986113691.862129, Test MSE of 53986114090.242485
Epoch 31: training loss 61197833483.294
Test Loss of 50269836032.414528, Test MSE of 50269836094.479782
Epoch 32: training loss 58995208749.176
Test Loss of 49823847333.277817, Test MSE of 49823847700.824242
Epoch 33: training loss 55635319250.824
Test Loss of 48925645446.662041, Test MSE of 48925645111.910149
Epoch 34: training loss 53217260062.118
Test Loss of 43957799545.397179, Test MSE of 43957799714.764702
Epoch 35: training loss 50926648824.471
Test Loss of 44197355509.577606, Test MSE of 44197354550.445274
Epoch 36: training loss 48918518738.824
Test Loss of 42659768674.835068, Test MSE of 42659768986.591866
Epoch 37: training loss 46607630987.294
Test Loss of 41175221693.557251, Test MSE of 41175221754.211250
Epoch 38: training loss 44292529842.824
Test Loss of 38171419676.661575, Test MSE of 38171419114.619453
Epoch 39: training loss 42612344500.706
Test Loss of 35594291293.327782, Test MSE of 35594291531.316353
Epoch 40: training loss 39961785517.176
Test Loss of 32900255334.684246, Test MSE of 32900255504.165829
Epoch 41: training loss 38978485632.000
Test Loss of 32262902009.189915, Test MSE of 32262902697.887680
Epoch 42: training loss 36449059143.529
Test Loss of 33894402242.709229, Test MSE of 33894402591.500317
Epoch 43: training loss 35322458273.882
Test Loss of 31165749486.293777, Test MSE of 31165749297.834469
Epoch 44: training loss 33787196431.059
Test Loss of 30688550879.785336, Test MSE of 30688551058.722343
Epoch 45: training loss 32067744670.118
Test Loss of 26657429172.852184, Test MSE of 26657429327.835995
Epoch 46: training loss 30934612751.059
Test Loss of 27397494685.934769, Test MSE of 27397494327.359440
Epoch 47: training loss 29416868483.765
Test Loss of 26121024534.976635, Test MSE of 26121024383.509068
Epoch 48: training loss 27854016267.294
Test Loss of 27949483039.504047, Test MSE of 27949482787.238441
Epoch 49: training loss 27764338401.882
Test Loss of 24649647864.123989, Test MSE of 24649648346.964920
Epoch 50: training loss 25686244969.412
Test Loss of 24340651804.602360, Test MSE of 24340652191.512157
Epoch 51: training loss 24744957440.000
Test Loss of 22169996054.680546, Test MSE of 22169996117.190147
Epoch 52: training loss 23920824489.412
Test Loss of 23561422058.266945, Test MSE of 23561422561.103615
Epoch 53: training loss 22468684683.294
Test Loss of 22218542159.352303, Test MSE of 22218542154.222763
Epoch 54: training loss 22481158787.765
Test Loss of 20405116152.005550, Test MSE of 20405116304.998337
Epoch 55: training loss 21457996336.941
Test Loss of 22098060823.568817, Test MSE of 22098060864.666969
Epoch 56: training loss 20338254784.000
Test Loss of 20777454763.258846, Test MSE of 20777454729.008167
Epoch 57: training loss 19943356619.294
Test Loss of 19647833089.184364, Test MSE of 19647833392.586048
Epoch 58: training loss 19307898032.941
Test Loss of 20841545615.722416, Test MSE of 20841545859.864288
Epoch 59: training loss 18578996766.118
Test Loss of 21310439114.644459, Test MSE of 21310438862.724831
Epoch 60: training loss 17985015480.471
Test Loss of 20138428324.567200, Test MSE of 20138428103.705753
Epoch 61: training loss 17205951702.588
Test Loss of 19822622684.469120, Test MSE of 19822622470.155376
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19822622470.155376, 'MSE - std': 0.0, 'R2 - mean': 0.845639088086722, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005511 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918017475.765
Test Loss of 424556549938.868408, Test MSE of 424556547415.733643
Epoch 2: training loss 427897245214.118
Test Loss of 424539966306.479736, Test MSE of 424539961752.657166
Epoch 3: training loss 427869059192.471
Test Loss of 424516899764.437683, Test MSE of 424516903763.114380
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889557022.118
Test Loss of 424525138236.935486, Test MSE of 424525140497.631653
Epoch 2: training loss 427876284295.529
Test Loss of 424525865401.530396, Test MSE of 424525867036.888000
Epoch 3: training loss 423887651297.882
Test Loss of 412351803967.126526, Test MSE of 412351800513.054504
Epoch 4: training loss 396736756073.412
Test Loss of 371292098409.349060, Test MSE of 371292101200.135193
Epoch 5: training loss 329749068619.294
Test Loss of 285667600528.492249, Test MSE of 285667598874.721375
Epoch 6: training loss 246426060920.471
Test Loss of 209871281965.657196, Test MSE of 209871280458.602142
Epoch 7: training loss 178566468367.059
Test Loss of 152044246325.118652, Test MSE of 152044248713.713104
Epoch 8: training loss 143863060570.353
Test Loss of 134113819624.075867, Test MSE of 134113814767.437897
Epoch 9: training loss 135702621334.588
Test Loss of 128574219875.368027, Test MSE of 128574220126.467270
Epoch 10: training loss 132687656297.412
Test Loss of 126107692329.275040, Test MSE of 126107690523.359665
Epoch 11: training loss 130195117899.294
Test Loss of 122304911526.284531, Test MSE of 122304909310.057678
Epoch 12: training loss 126707790908.235
Test Loss of 119198809894.077255, Test MSE of 119198811310.285599
Epoch 13: training loss 122098690740.706
Test Loss of 114903800783.441132, Test MSE of 114903799237.004166
Epoch 14: training loss 117969783205.647
Test Loss of 113088318010.862823, Test MSE of 113088319071.409698
Epoch 15: training loss 114277231826.824
Test Loss of 109893016692.304413, Test MSE of 109893015789.813873
Epoch 16: training loss 111019490966.588
Test Loss of 106433224928.555176, Test MSE of 106433224895.810974
Epoch 17: training loss 107332225807.059
Test Loss of 101500622293.244507, Test MSE of 101500621496.256714
Epoch 18: training loss 102950754273.882
Test Loss of 98157076498.002319, Test MSE of 98157075510.640930
Epoch 19: training loss 98796263273.412
Test Loss of 94709885068.702286, Test MSE of 94709884180.008133
Epoch 20: training loss 95933488459.294
Test Loss of 91228299191.043259, Test MSE of 91228298577.877747
Epoch 21: training loss 91578463171.765
Test Loss of 87368060125.475830, Test MSE of 87368059547.529694
Epoch 22: training loss 87436119130.353
Test Loss of 84977569502.068008, Test MSE of 84977570858.743805
Epoch 23: training loss 83951938785.882
Test Loss of 78926604687.367111, Test MSE of 78926605402.651566
Epoch 24: training loss 81240537027.765
Test Loss of 73390711190.473282, Test MSE of 73390714399.057571
Epoch 25: training loss 77388380536.471
Test Loss of 74270282310.706451, Test MSE of 74270283643.503311
Epoch 26: training loss 74075136120.471
Test Loss of 70655912055.857513, Test MSE of 70655912212.870438
Epoch 27: training loss 71506029824.000
Test Loss of 67771734934.591721, Test MSE of 67771734237.879570
Epoch 28: training loss 67968114642.824
Test Loss of 66024783737.456398, Test MSE of 66024784514.300270
Epoch 29: training loss 64771132792.471
Test Loss of 64117571250.483459, Test MSE of 64117570268.926598
Epoch 30: training loss 61944651956.706
Test Loss of 60323295369.386078, Test MSE of 60323297560.073265
Epoch 31: training loss 59339210270.118
Test Loss of 59584563425.028915, Test MSE of 59584562210.443985
Epoch 32: training loss 56128676126.118
Test Loss of 54717137068.916954, Test MSE of 54717137572.070908
Epoch 33: training loss 54192174290.824
Test Loss of 56170950683.950958, Test MSE of 56170951383.425102
Epoch 34: training loss 51447679804.235
Test Loss of 52005258910.586166, Test MSE of 52005258344.069557
Epoch 35: training loss 49049401103.059
Test Loss of 50217526717.083504, Test MSE of 50217525798.600639
Epoch 36: training loss 46375016342.588
Test Loss of 44689571768.464493, Test MSE of 44689572098.568558
Epoch 37: training loss 44139806268.235
Test Loss of 46420721327.640991, Test MSE of 46420721289.689880
Epoch 38: training loss 42231200722.824
Test Loss of 42257000600.072174, Test MSE of 42257000954.163719
Epoch 39: training loss 40341789221.647
Test Loss of 42436012962.435349, Test MSE of 42436015126.197830
Epoch 40: training loss 38072869955.765
Test Loss of 37319282297.160309, Test MSE of 37319282077.694458
Epoch 41: training loss 36438154631.529
Test Loss of 40401770795.880638, Test MSE of 40401771641.613838
Epoch 42: training loss 34385154612.706
Test Loss of 40023882872.094376, Test MSE of 40023883505.148827
Epoch 43: training loss 32593373500.235
Test Loss of 35969413628.328476, Test MSE of 35969413148.750244
Epoch 44: training loss 31300106149.647
Test Loss of 35012088889.560028, Test MSE of 35012090327.876030
Epoch 45: training loss 29542632598.588
Test Loss of 32561911586.287300, Test MSE of 32561911719.965534
Epoch 46: training loss 28295765519.059
Test Loss of 34770118011.706688, Test MSE of 34770117343.260246
Epoch 47: training loss 27164466070.588
Test Loss of 34142115988.755955, Test MSE of 34142115503.516293
Epoch 48: training loss 26110856470.588
Test Loss of 31513791145.245430, Test MSE of 31513791530.839260
Epoch 49: training loss 24557527243.294
Test Loss of 31159320939.125607, Test MSE of 31159320781.620163
Epoch 50: training loss 23257382576.941
Test Loss of 33331298218.489014, Test MSE of 33331297434.493668
Epoch 51: training loss 22189747346.824
Test Loss of 33897000084.045338, Test MSE of 33897001295.467239
Epoch 52: training loss 21256868261.647
Test Loss of 31173250801.965302, Test MSE of 31173250298.349041
Epoch 53: training loss 20887559329.882
Test Loss of 28584729397.000233, Test MSE of 28584729597.626202
Epoch 54: training loss 19676583506.824
Test Loss of 28042910616.723572, Test MSE of 28042910717.185272
Epoch 55: training loss 18874274074.353
Test Loss of 28439079146.503819, Test MSE of 28439079335.941063
Epoch 56: training loss 18083573929.412
Test Loss of 28476684549.981033, Test MSE of 28476685592.075981
Epoch 57: training loss 17768326697.412
Test Loss of 29558371451.410595, Test MSE of 29558372247.637047
Epoch 58: training loss 17136509040.941
Test Loss of 26549366835.164471, Test MSE of 26549367362.655479
Epoch 59: training loss 16642082032.941
Test Loss of 28669686430.586166, Test MSE of 28669685724.047005
Epoch 60: training loss 15919556502.588
Test Loss of 27956998743.287533, Test MSE of 27956998868.442871
Epoch 61: training loss 15541052739.765
Test Loss of 26658732450.080036, Test MSE of 26658731836.795940
Epoch 62: training loss 15095258654.118
Test Loss of 25667245081.819107, Test MSE of 25667245663.598072
Epoch 63: training loss 14429170153.412
Test Loss of 27466440483.471664, Test MSE of 27466441164.982674
Epoch 64: training loss 13785902400.000
Test Loss of 25297407750.573212, Test MSE of 25297407547.684399
Epoch 65: training loss 13691211640.471
Test Loss of 24873367945.208420, Test MSE of 24873368473.535069
Epoch 66: training loss 13126085978.353
Test Loss of 27416317614.930374, Test MSE of 27416317359.000420
Epoch 67: training loss 13219703420.235
Test Loss of 25721664713.578533, Test MSE of 25721664609.301670
Epoch 68: training loss 12430367318.588
Test Loss of 27121053919.844551, Test MSE of 27121054021.842106
Epoch 69: training loss 12107925067.294
Test Loss of 27473710746.796207, Test MSE of 27473711480.849037
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23648166975.502205, 'MSE - std': 3825544505.3468304, 'R2 - mean': 0.8247475143450065, 'R2 - std': 0.020891573741715408} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003565 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926992233.412
Test Loss of 447258772778.696289, Test MSE of 447258778747.958252
Epoch 2: training loss 421906746066.824
Test Loss of 447240478062.441833, Test MSE of 447240485335.638306
Epoch 3: training loss 421880161460.706
Test Loss of 447216120000.340515, Test MSE of 447216130172.684814
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421899200150.588
Test Loss of 447222958650.862854, Test MSE of 447222956149.703308
Epoch 2: training loss 421888490315.294
Test Loss of 447224226711.065491, Test MSE of 447224227091.164368
Epoch 3: training loss 418381081057.882
Test Loss of 435889185543.046936, Test MSE of 435889181308.652283
Epoch 4: training loss 392604326008.471
Test Loss of 394710701041.077026, Test MSE of 394710708765.184753
Epoch 5: training loss 327388412506.353
Test Loss of 307070458818.176270, Test MSE of 307070455489.656616
Epoch 6: training loss 244625471488.000
Test Loss of 228924293559.635437, Test MSE of 228924286851.534027
Epoch 7: training loss 176175381383.529
Test Loss of 166301065445.292633, Test MSE of 166301064618.665344
Epoch 8: training loss 141942554563.765
Test Loss of 145836452617.415680, Test MSE of 145836454303.494781
Epoch 9: training loss 133238368707.765
Test Loss of 139166574217.267639, Test MSE of 139166573808.779907
Epoch 10: training loss 129212862373.647
Test Loss of 135831649705.186218, Test MSE of 135831651845.775467
Epoch 11: training loss 126616508958.118
Test Loss of 132599523696.099930, Test MSE of 132599521053.550369
Epoch 12: training loss 123890004630.588
Test Loss of 129399541783.687256, Test MSE of 129399540569.458282
Epoch 13: training loss 119793071375.059
Test Loss of 126006875670.621323, Test MSE of 126006877155.278839
Epoch 14: training loss 117216922112.000
Test Loss of 121255879321.138092, Test MSE of 121255879601.107208
Epoch 15: training loss 113505598735.059
Test Loss of 119131729543.846405, Test MSE of 119131730300.472961
Epoch 16: training loss 110312756766.118
Test Loss of 113764079187.260696, Test MSE of 113764079321.037277
Epoch 17: training loss 105276498672.941
Test Loss of 111961693273.774689, Test MSE of 111961694403.981705
Epoch 18: training loss 102358830802.824
Test Loss of 104617352562.231781, Test MSE of 104617353450.537369
Epoch 19: training loss 100036194785.882
Test Loss of 102683782879.015503, Test MSE of 102683782222.692337
Epoch 20: training loss 95341875712.000
Test Loss of 99167255702.414062, Test MSE of 99167257131.535797
Epoch 21: training loss 89938342068.706
Test Loss of 92191959250.816559, Test MSE of 92191957096.772934
Epoch 22: training loss 87317243331.765
Test Loss of 90080705250.568588, Test MSE of 90080704297.083450
Epoch 23: training loss 84411449600.000
Test Loss of 87882332568.842010, Test MSE of 87882331729.142960
Epoch 24: training loss 80782120764.235
Test Loss of 84994064321.465652, Test MSE of 84994064329.071686
Epoch 25: training loss 76337089144.471
Test Loss of 79912482250.822113, Test MSE of 79912482132.091293
Epoch 26: training loss 73353171907.765
Test Loss of 76971325147.699280, Test MSE of 76971325430.384979
Epoch 27: training loss 70369567819.294
Test Loss of 78174452668.491333, Test MSE of 78174453172.470581
Epoch 28: training loss 68025454607.059
Test Loss of 71676421587.349533, Test MSE of 71676421883.108887
Epoch 29: training loss 65368609972.706
Test Loss of 67562401620.504280, Test MSE of 67562401692.144058
Epoch 30: training loss 62022461560.471
Test Loss of 64661132873.785797, Test MSE of 64661134260.800301
Epoch 31: training loss 59398616094.118
Test Loss of 64842296160.584778, Test MSE of 64842295273.192223
Epoch 32: training loss 56489020792.471
Test Loss of 62989795551.607681, Test MSE of 62989795420.112488
Epoch 33: training loss 54361455028.706
Test Loss of 57481532666.374275, Test MSE of 57481533220.222122
Epoch 34: training loss 52215846430.118
Test Loss of 58979202254.789726, Test MSE of 58979203053.811607
Epoch 35: training loss 49317480681.412
Test Loss of 53050928055.753876, Test MSE of 53050926894.272926
Epoch 36: training loss 47527850187.294
Test Loss of 49380226253.842239, Test MSE of 49380225428.743675
Epoch 37: training loss 45496482123.294
Test Loss of 50775913652.260002, Test MSE of 50775913105.120247
Epoch 38: training loss 43142002816.000
Test Loss of 45234758823.942635, Test MSE of 45234758998.808441
Epoch 39: training loss 40831072173.176
Test Loss of 42807057743.174644, Test MSE of 42807058767.155479
Epoch 40: training loss 39630540528.941
Test Loss of 46805925633.125145, Test MSE of 46805925812.789291
Epoch 41: training loss 37565308175.059
Test Loss of 44513686771.741844, Test MSE of 44513687175.038109
Epoch 42: training loss 35768340336.941
Test Loss of 40999783288.272034, Test MSE of 40999783744.732422
Epoch 43: training loss 34210150226.824
Test Loss of 37887929897.097389, Test MSE of 37887929549.805054
Epoch 44: training loss 32891732449.882
Test Loss of 37471252766.142029, Test MSE of 37471253701.963478
Epoch 45: training loss 31102419215.059
Test Loss of 36037385829.736755, Test MSE of 36037385429.215614
Epoch 46: training loss 29483430174.118
Test Loss of 36344392890.418694, Test MSE of 36344392814.145027
Epoch 47: training loss 28746432353.882
Test Loss of 36663948543.111725, Test MSE of 36663948804.653641
Epoch 48: training loss 27278449773.176
Test Loss of 30405837982.467731, Test MSE of 30405838444.269627
Epoch 49: training loss 25918546695.529
Test Loss of 36325071033.234329, Test MSE of 36325070847.095375
Epoch 50: training loss 24992891730.824
Test Loss of 31442199962.500114, Test MSE of 31442200061.096874
Epoch 51: training loss 24219247420.235
Test Loss of 29729826906.485310, Test MSE of 29729827550.025105
Epoch 52: training loss 23003367879.529
Test Loss of 31622463877.418461, Test MSE of 31622464343.055660
Epoch 53: training loss 22244703653.647
Test Loss of 27100752965.877399, Test MSE of 27100752570.063583
Epoch 54: training loss 21270147870.118
Test Loss of 29328274322.328014, Test MSE of 29328274160.507156
Epoch 55: training loss 20474101488.941
Test Loss of 27954212021.681240, Test MSE of 27954212070.090225
Epoch 56: training loss 20020930710.588
Test Loss of 23233288836.767059, Test MSE of 23233288883.876488
Epoch 57: training loss 19098170492.235
Test Loss of 29738684722.039326, Test MSE of 29738685333.052128
Epoch 58: training loss 18422819531.294
Test Loss of 25734452448.792042, Test MSE of 25734452601.192886
Epoch 59: training loss 18172517835.294
Test Loss of 25331302372.522785, Test MSE of 25331302498.698284
Epoch 60: training loss 17177061703.529
Test Loss of 25521112450.812862, Test MSE of 25521112385.145020
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24272482112.04981, 'MSE - std': 3245930676.149927, 'R2 - mean': 0.8265342225955862, 'R2 - std': 0.017244029546939845} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005552 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109815988.706
Test Loss of 410765674745.247559, Test MSE of 410765676687.553467
Epoch 2: training loss 430088202842.353
Test Loss of 410746782166.063843, Test MSE of 410746778382.235107
Epoch 3: training loss 430060482319.059
Test Loss of 410722262452.420166, Test MSE of 410722257530.517334
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079070930.824
Test Loss of 410727044897.288269, Test MSE of 410727041629.834961
Epoch 2: training loss 430065676288.000
Test Loss of 410727785793.747314, Test MSE of 410727785280.290100
Epoch 3: training loss 426401468416.000
Test Loss of 399215665633.436401, Test MSE of 399215669134.228027
Epoch 4: training loss 400187760519.529
Test Loss of 357962709190.071289, Test MSE of 357962710906.576660
Epoch 5: training loss 334276454400.000
Test Loss of 272516852723.205933, Test MSE of 272516855788.024689
Epoch 6: training loss 250655572269.176
Test Loss of 195285874089.995361, Test MSE of 195285875089.747284
Epoch 7: training loss 183155817532.235
Test Loss of 135960599707.898193, Test MSE of 135960602233.582550
Epoch 8: training loss 148899909511.529
Test Loss of 117062878586.609909, Test MSE of 117062878970.168640
Epoch 9: training loss 139036267640.471
Test Loss of 111330236557.208694, Test MSE of 111330236812.996826
Epoch 10: training loss 136605378228.706
Test Loss of 108453859576.299866, Test MSE of 108453858206.781403
Epoch 11: training loss 132384793389.176
Test Loss of 105996978830.867188, Test MSE of 105996978601.519943
Epoch 12: training loss 129776610906.353
Test Loss of 102490504620.838501, Test MSE of 102490504149.189545
Epoch 13: training loss 126547984504.471
Test Loss of 100399660460.838501, Test MSE of 100399660309.855774
Epoch 14: training loss 122246236099.765
Test Loss of 96417972292.235077, Test MSE of 96417971699.867950
Epoch 15: training loss 118580751691.294
Test Loss of 93887114553.217957, Test MSE of 93887115090.990143
Epoch 16: training loss 114513327495.529
Test Loss of 90640868681.329010, Test MSE of 90640868902.935883
Epoch 17: training loss 109392244208.941
Test Loss of 87424211293.704773, Test MSE of 87424210319.687759
Epoch 18: training loss 105998745720.471
Test Loss of 84508339797.530777, Test MSE of 84508341506.714584
Epoch 19: training loss 102195259693.176
Test Loss of 82009879530.676544, Test MSE of 82009878905.080612
Epoch 20: training loss 98740706032.941
Test Loss of 79186518675.605743, Test MSE of 79186519541.746170
Epoch 21: training loss 95770150098.824
Test Loss of 74699052560.347992, Test MSE of 74699051714.464539
Epoch 22: training loss 91264122518.588
Test Loss of 71250691746.295227, Test MSE of 71250691674.667267
Epoch 23: training loss 87440968553.412
Test Loss of 69746428892.460892, Test MSE of 69746429561.271225
Epoch 24: training loss 83879657216.000
Test Loss of 66663584659.487274, Test MSE of 66663584072.940216
Epoch 25: training loss 80154711868.235
Test Loss of 64368865243.987045, Test MSE of 64368865932.080429
Epoch 26: training loss 76137662840.471
Test Loss of 59803820483.109673, Test MSE of 59803820634.510208
Epoch 27: training loss 73453450240.000
Test Loss of 57475243061.545578, Test MSE of 57475242605.066460
Epoch 28: training loss 70828589342.118
Test Loss of 56316029131.757523, Test MSE of 56316028481.044373
Epoch 29: training loss 67816218051.765
Test Loss of 53217558145.125404, Test MSE of 53217557815.282867
Epoch 30: training loss 64431288515.765
Test Loss of 51207991712.992134, Test MSE of 51207991381.619591
Epoch 31: training loss 62051463981.176
Test Loss of 50236345964.275795, Test MSE of 50236345495.617325
Epoch 32: training loss 58762656768.000
Test Loss of 46867613695.052292, Test MSE of 46867614148.568764
Epoch 33: training loss 56176183130.353
Test Loss of 45628383014.026840, Test MSE of 45628383221.307617
Epoch 34: training loss 53535533206.588
Test Loss of 45823651320.655251, Test MSE of 45823651957.089874
Epoch 35: training loss 51211456963.765
Test Loss of 43218870192.392410, Test MSE of 43218870277.750290
Epoch 36: training loss 49674874450.824
Test Loss of 40837387942.086075, Test MSE of 40837388237.126961
Epoch 37: training loss 46830629022.118
Test Loss of 39330731792.229523, Test MSE of 39330730930.590103
Epoch 38: training loss 44666653334.588
Test Loss of 36730099197.393799, Test MSE of 36730098729.307816
Epoch 39: training loss 43135052596.706
Test Loss of 35785241442.206383, Test MSE of 35785241364.424988
Epoch 40: training loss 41046831405.176
Test Loss of 34210614295.218880, Test MSE of 34210614875.392540
Epoch 41: training loss 38921427373.176
Test Loss of 32437639311.104118, Test MSE of 32437639720.787693
Epoch 42: training loss 37137853040.941
Test Loss of 31372830811.927811, Test MSE of 31372831264.526794
Epoch 43: training loss 35552998633.412
Test Loss of 29259910605.060619, Test MSE of 29259910194.662346
Epoch 44: training loss 33756326912.000
Test Loss of 31634374964.953262, Test MSE of 31634375480.738102
Epoch 45: training loss 32547794831.059
Test Loss of 26318803889.813976, Test MSE of 26318804142.662548
Epoch 46: training loss 30878330955.294
Test Loss of 26927761626.920872, Test MSE of 26927762094.674110
Epoch 47: training loss 29344795632.941
Test Loss of 25268105912.566406, Test MSE of 25268106634.395927
Epoch 48: training loss 28440011708.235
Test Loss of 23498655324.164738, Test MSE of 23498655562.750866
Epoch 49: training loss 27207180626.824
Test Loss of 24524601748.671909, Test MSE of 24524601956.710335
Epoch 50: training loss 26196525488.941
Test Loss of 22273726966.285980, Test MSE of 22273726948.072659
Epoch 51: training loss 24725386755.765
Test Loss of 21936824734.149006, Test MSE of 21936824851.490063
Epoch 52: training loss 23754732615.529
Test Loss of 22920991789.016197, Test MSE of 22920991617.021858
Epoch 53: training loss 22689176316.235
Test Loss of 20693163415.988895, Test MSE of 20693163245.219341
Epoch 54: training loss 22125791883.294
Test Loss of 19813496689.843590, Test MSE of 19813496968.052673
Epoch 55: training loss 21180011523.765
Test Loss of 21884826617.839890, Test MSE of 21884826719.708092
Epoch 56: training loss 20439379395.765
Test Loss of 20048781447.522442, Test MSE of 20048781605.994724
Epoch 57: training loss 19991490416.941
Test Loss of 20237147259.676075, Test MSE of 20237146885.581745
Epoch 58: training loss 19010311107.765
Test Loss of 17695055785.758446, Test MSE of 17695055910.009644
Epoch 59: training loss 18738589129.412
Test Loss of 19787232263.107819, Test MSE of 19787232321.382332
Epoch 60: training loss 17977924784.941
Test Loss of 18123869147.513187, Test MSE of 18123869434.273605
Epoch 61: training loss 17762533248.000
Test Loss of 17489352293.641834, Test MSE of 17489352123.514519
Epoch 62: training loss 17067438042.353
Test Loss of 18810329438.652477, Test MSE of 18810329570.309021
Epoch 63: training loss 16596144700.235
Test Loss of 18642430221.149467, Test MSE of 18642430375.154343
Epoch 64: training loss 16275194435.765
Test Loss of 18404156515.983341, Test MSE of 18404156792.033455
Epoch 65: training loss 15697183905.882
Test Loss of 17456955699.057842, Test MSE of 17456955673.666534
Epoch 66: training loss 15175150689.882
Test Loss of 16850739849.654789, Test MSE of 16850740292.923328
Epoch 67: training loss 14778823753.412
Test Loss of 18236278955.535400, Test MSE of 18236278956.610645
Epoch 68: training loss 14436824489.412
Test Loss of 18887819585.273483, Test MSE of 18887819749.550091
Epoch 69: training loss 14008326994.824
Test Loss of 17382457902.674686, Test MSE of 17382458083.505688
Epoch 70: training loss 13553852877.176
Test Loss of 18021478675.783432, Test MSE of 18021478882.328411
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22709731304.61946, 'MSE - std': 3902386413.9973345, 'R2 - mean': 0.8327155026203572, 'R2 - std': 0.018375039714125737} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005705 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043540720.941
Test Loss of 431613618792.958801, Test MSE of 431613625271.977112
Epoch 2: training loss 424024136764.235
Test Loss of 431593160097.466003, Test MSE of 431593155889.032166
Epoch 3: training loss 423997152436.706
Test Loss of 431565539129.928711, Test MSE of 431565539356.901062
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011478196.706
Test Loss of 431566727648.488647, Test MSE of 431566727532.648987
Epoch 2: training loss 423999665694.118
Test Loss of 431568849804.853333, Test MSE of 431568850421.928528
Epoch 3: training loss 420428081392.941
Test Loss of 419366293570.339661, Test MSE of 419366291816.897400
Epoch 4: training loss 394503110535.529
Test Loss of 378040683298.709839, Test MSE of 378040678553.916504
Epoch 5: training loss 329244352030.118
Test Loss of 290944823335.803772, Test MSE of 290944820150.634888
Epoch 6: training loss 246946819674.353
Test Loss of 212403069879.500244, Test MSE of 212403066672.559052
Epoch 7: training loss 179276673897.412
Test Loss of 150300964334.230438, Test MSE of 150300964223.457367
Epoch 8: training loss 143805158701.176
Test Loss of 129430535532.394257, Test MSE of 129430535277.566376
Epoch 9: training loss 135987918697.412
Test Loss of 122186420332.038864, Test MSE of 122186420965.238403
Epoch 10: training loss 132205013624.471
Test Loss of 120181397641.417862, Test MSE of 120181399040.245712
Epoch 11: training loss 130651077662.118
Test Loss of 116602865162.661728, Test MSE of 116602866079.672989
Epoch 12: training loss 126186589756.235
Test Loss of 113881504851.872284, Test MSE of 113881503614.918091
Epoch 13: training loss 122697220336.941
Test Loss of 110559314037.515961, Test MSE of 110559311276.227692
Epoch 14: training loss 119378266955.294
Test Loss of 107435727500.971771, Test MSE of 107435726302.282425
Epoch 15: training loss 116551830016.000
Test Loss of 104637339660.320221, Test MSE of 104637336767.894714
Epoch 16: training loss 113139584647.529
Test Loss of 100785491060.094406, Test MSE of 100785491118.178879
Epoch 17: training loss 107717462588.235
Test Loss of 95026524544.770020, Test MSE of 95026524637.079636
Epoch 18: training loss 104908377660.235
Test Loss of 92374033714.583984, Test MSE of 92374036075.284683
Epoch 19: training loss 101982067561.412
Test Loss of 90432619595.342896, Test MSE of 90432620374.494797
Epoch 20: training loss 98073971395.765
Test Loss of 87538359148.631195, Test MSE of 87538360460.041077
Epoch 21: training loss 93976800075.294
Test Loss of 83240746333.230911, Test MSE of 83240747624.589020
Epoch 22: training loss 90527899196.235
Test Loss of 79493374833.369736, Test MSE of 79493374204.246506
Epoch 23: training loss 86286424756.706
Test Loss of 74114136660.109207, Test MSE of 74114137618.484100
Epoch 24: training loss 83098346450.824
Test Loss of 73942453733.701065, Test MSE of 73942453926.661514
Epoch 25: training loss 79690460144.941
Test Loss of 70189845258.069412, Test MSE of 70189846136.486847
Epoch 26: training loss 76701139425.882
Test Loss of 65547408430.911613, Test MSE of 65547410514.623802
Epoch 27: training loss 72505382964.706
Test Loss of 61473316306.746880, Test MSE of 61473316927.435188
Epoch 28: training loss 69653230592.000
Test Loss of 60625409783.589081, Test MSE of 60625410185.109932
Epoch 29: training loss 67064419117.176
Test Loss of 53349573758.519203, Test MSE of 53349574644.439865
Epoch 30: training loss 64509883196.235
Test Loss of 54372067109.079132, Test MSE of 54372066644.508804
Epoch 31: training loss 61066992233.412
Test Loss of 52281587440.007401, Test MSE of 52281587907.726082
Epoch 32: training loss 58626615205.647
Test Loss of 51324460815.281815, Test MSE of 51324460606.845657
Epoch 33: training loss 56746226055.529
Test Loss of 48513815473.813972, Test MSE of 48513816221.994354
Epoch 34: training loss 53387752937.412
Test Loss of 46768420084.035172, Test MSE of 46768419519.149422
Epoch 35: training loss 51305474928.941
Test Loss of 44455572142.141602, Test MSE of 44455571565.327690
Epoch 36: training loss 49559567924.706
Test Loss of 42384332222.844978, Test MSE of 42384332046.601456
Epoch 37: training loss 46806192165.647
Test Loss of 40582861306.076813, Test MSE of 40582861366.361328
Epoch 38: training loss 44922147358.118
Test Loss of 38766629235.502083, Test MSE of 38766628585.744308
Epoch 39: training loss 43487382550.588
Test Loss of 34979637599.600182, Test MSE of 34979637778.512917
Epoch 40: training loss 40903206829.176
Test Loss of 34967227260.742249, Test MSE of 34967227957.889030
Epoch 41: training loss 39275433991.529
Test Loss of 33419082469.108746, Test MSE of 33419082846.865868
Epoch 42: training loss 37625756995.765
Test Loss of 32129091101.615917, Test MSE of 32129090913.910984
Epoch 43: training loss 35859399973.647
Test Loss of 33246133560.270245, Test MSE of 33246133340.945881
Epoch 44: training loss 34137653820.235
Test Loss of 31713685533.378990, Test MSE of 31713686027.443481
Epoch 45: training loss 32093577667.765
Test Loss of 31159657301.886166, Test MSE of 31159657053.896656
Epoch 46: training loss 31537408843.294
Test Loss of 29261492389.849144, Test MSE of 29261492516.768471
Epoch 47: training loss 29933478987.294
Test Loss of 25176388012.364647, Test MSE of 25176387794.543037
Epoch 48: training loss 28437553769.412
Test Loss of 22737551941.893566, Test MSE of 22737551535.040962
Epoch 49: training loss 27192764641.882
Test Loss of 24462442442.928272, Test MSE of 24462442750.940147
Epoch 50: training loss 26962387286.588
Test Loss of 23763164807.759369, Test MSE of 23763164751.690685
Epoch 51: training loss 25756102720.000
Test Loss of 22800158336.177696, Test MSE of 22800158790.457405
Epoch 52: training loss 24422571301.647
Test Loss of 24993992410.210087, Test MSE of 24993992466.216522
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23166583536.938873, 'MSE - std': 3608012143.5524163, 'R2 - mean': 0.8288411893571641, 'R2 - std': 0.018170164573882392} 
 

Saving model.....
Results After CV: {'MSE - mean': 23166583536.938873, 'MSE - std': 3608012143.5524163, 'R2 - mean': 0.8288411893571641, 'R2 - std': 0.018170164573882392}
Train time: 97.28128234720134
Inference time: 0.07330473019974307
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 81 finished with value: 23166583536.938873 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003945 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525374433.882
Test Loss of 418111907669.925537, Test MSE of 418111904387.548828
Epoch 2: training loss 427504095472.941
Test Loss of 418093619533.042786, Test MSE of 418093620126.486145
Epoch 3: training loss 427476147742.118
Test Loss of 418069791883.517944, Test MSE of 418069788278.197144
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493566464.000
Test Loss of 418074572168.497803, Test MSE of 418074573376.928589
Epoch 2: training loss 427482002733.176
Test Loss of 418076928826.685181, Test MSE of 418076935582.623352
Epoch 3: training loss 424014993889.882
Test Loss of 406886051668.504272, Test MSE of 406886060770.140381
Epoch 4: training loss 398170217773.176
Test Loss of 366328463250.091125, Test MSE of 366328467292.070923
Epoch 5: training loss 332922009961.412
Test Loss of 279921218128.181335, Test MSE of 279921217908.348145
Epoch 6: training loss 249921483776.000
Test Loss of 203037310862.301178, Test MSE of 203037309992.243347
Epoch 7: training loss 180831317925.647
Test Loss of 141950632697.782104, Test MSE of 141950633902.012787
Epoch 8: training loss 145390367232.000
Test Loss of 122610333735.320847, Test MSE of 122610333528.341415
Epoch 9: training loss 137210055288.471
Test Loss of 116398156200.949341, Test MSE of 116398156192.781174
Epoch 10: training loss 133961381647.059
Test Loss of 114138553289.756195, Test MSE of 114138551209.594727
Epoch 11: training loss 128899686490.353
Test Loss of 111156766957.346283, Test MSE of 111156765998.392471
Epoch 12: training loss 126583502456.471
Test Loss of 106988902001.580383, Test MSE of 106988902091.742859
Epoch 13: training loss 123274245827.765
Test Loss of 104306664517.403656, Test MSE of 104306666496.452362
Epoch 14: training loss 120429053997.176
Test Loss of 101910732781.050201, Test MSE of 101910731578.148697
Epoch 15: training loss 115477808564.706
Test Loss of 97451889188.596802, Test MSE of 97451887304.134048
Epoch 16: training loss 111596572310.588
Test Loss of 94106494303.755722, Test MSE of 94106495446.167847
Epoch 17: training loss 107543012660.706
Test Loss of 91977766921.001160, Test MSE of 91977766858.448959
Epoch 18: training loss 103002599273.412
Test Loss of 87200092363.710388, Test MSE of 87200092287.397934
Epoch 19: training loss 100837931926.588
Test Loss of 84034043521.213974, Test MSE of 84034043544.318192
Epoch 20: training loss 96813172720.941
Test Loss of 80669816967.964844, Test MSE of 80669817975.584610
Epoch 21: training loss 92144099629.176
Test Loss of 77806941565.127914, Test MSE of 77806941166.841919
Epoch 22: training loss 88241639077.647
Test Loss of 75041358221.945877, Test MSE of 75041357721.082779
Epoch 23: training loss 85112045530.353
Test Loss of 71344167408.011108, Test MSE of 71344167600.058075
Epoch 24: training loss 81281074537.412
Test Loss of 67843351841.221375, Test MSE of 67843352275.884338
Epoch 25: training loss 77495725274.353
Test Loss of 65722740249.937546, Test MSE of 65722740214.625549
Epoch 26: training loss 74572494561.882
Test Loss of 63601387573.533195, Test MSE of 63601387187.689438
Epoch 27: training loss 71981945012.706
Test Loss of 62349067536.166550, Test MSE of 62349067442.458076
Epoch 28: training loss 68269818315.294
Test Loss of 55700941746.068932, Test MSE of 55700942651.666031
Epoch 29: training loss 65454573680.941
Test Loss of 55617688852.903999, Test MSE of 55617689641.166908
Epoch 30: training loss 62796570307.765
Test Loss of 53991798728.571823, Test MSE of 53991797941.380188
Epoch 31: training loss 59939350859.294
Test Loss of 51780731457.021515, Test MSE of 51780730848.433250
Epoch 32: training loss 57302965360.941
Test Loss of 47446688627.060837, Test MSE of 47446689236.034904
Epoch 33: training loss 54523091794.824
Test Loss of 45830005796.241501, Test MSE of 45830005938.210777
Epoch 34: training loss 51987631435.294
Test Loss of 42177971158.547302, Test MSE of 42177970681.001938
Epoch 35: training loss 50124392948.706
Test Loss of 41474240622.856346, Test MSE of 41474240867.420059
Epoch 36: training loss 47301186691.765
Test Loss of 35928266804.585701, Test MSE of 35928266767.876938
Epoch 37: training loss 44991335009.882
Test Loss of 36954175994.433495, Test MSE of 36954176099.410133
Epoch 38: training loss 42917773722.353
Test Loss of 35362732829.312981, Test MSE of 35362733142.732567
Epoch 39: training loss 41439830753.882
Test Loss of 33647523633.920887, Test MSE of 33647524029.652050
Epoch 40: training loss 39361927341.176
Test Loss of 33311174331.010872, Test MSE of 33311174975.790451
Epoch 41: training loss 37195864207.059
Test Loss of 33853919510.562111, Test MSE of 33853919407.794594
Epoch 42: training loss 35792134140.235
Test Loss of 27728377591.176498, Test MSE of 27728377853.965012
Epoch 43: training loss 33875311198.118
Test Loss of 31980711865.885727, Test MSE of 31980712057.244232
Epoch 44: training loss 32626694437.647
Test Loss of 28614081580.295166, Test MSE of 28614081613.919609
Epoch 45: training loss 31249441882.353
Test Loss of 26604834797.997688, Test MSE of 26604834598.315559
Epoch 46: training loss 29566450631.529
Test Loss of 25471243048.209114, Test MSE of 25471243658.095642
Epoch 47: training loss 28362722913.882
Test Loss of 25789484408.864216, Test MSE of 25789484653.457062
Epoch 48: training loss 27129553242.353
Test Loss of 23428546416.928986, Test MSE of 23428546000.341873
Epoch 49: training loss 25847164118.588
Test Loss of 23536025901.301872, Test MSE of 23536025578.980106
Epoch 50: training loss 24816564698.353
Test Loss of 24491862415.367107, Test MSE of 24491862281.650669
Epoch 51: training loss 23700536376.471
Test Loss of 21310632651.828823, Test MSE of 21310632757.446209
Epoch 52: training loss 23072746816.000
Test Loss of 22172520257.080730, Test MSE of 22172520277.942307
Epoch 53: training loss 22180004261.647
Test Loss of 20227337471.585472, Test MSE of 20227337952.712605
Epoch 54: training loss 21127254497.882
Test Loss of 20664496421.721951, Test MSE of 20664496408.425217
Epoch 55: training loss 20437786191.059
Test Loss of 21135185312.658802, Test MSE of 21135185498.676590
Epoch 56: training loss 19561702735.059
Test Loss of 19596664470.058754, Test MSE of 19596664114.377686
Epoch 57: training loss 19266124190.118
Test Loss of 19950812598.924820, Test MSE of 19950813045.374287
Epoch 58: training loss 18422194800.941
Test Loss of 21833406366.882259, Test MSE of 21833406604.159008
Epoch 59: training loss 18143379376.941
Test Loss of 20546240303.552162, Test MSE of 20546240296.940147
Epoch 60: training loss 17718075376.941
Test Loss of 18842029625.678463, Test MSE of 18842029924.416595
Epoch 61: training loss 16791137313.882
Test Loss of 17726360639.244968, Test MSE of 17726360668.216686
Epoch 62: training loss 16227898767.059
Test Loss of 18323594706.402035, Test MSE of 18323594946.029682
Epoch 63: training loss 16178501007.059
Test Loss of 19699543848.919731, Test MSE of 19699543999.704559
Epoch 64: training loss 15523523599.059
Test Loss of 20294665515.643764, Test MSE of 20294665506.696609
Epoch 65: training loss 15068476815.059
Test Loss of 19188448852.208187, Test MSE of 19188449138.998348
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19188449138.99835, 'MSE - std': 0.0, 'R2 - mean': 0.8505774646237247, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005452 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918439363.765
Test Loss of 424558331349.244507, Test MSE of 424558323591.378784
Epoch 2: training loss 427897985144.471
Test Loss of 424542643148.124939, Test MSE of 424542644514.700806
Epoch 3: training loss 427870467975.529
Test Loss of 424520422025.030762, Test MSE of 424520426571.262756
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890305385.412
Test Loss of 424527877749.370361, Test MSE of 424527892909.791931
Epoch 2: training loss 427879838659.765
Test Loss of 424528992126.667603, Test MSE of 424528986399.392334
Epoch 3: training loss 423880871936.000
Test Loss of 412278679389.505432, Test MSE of 412278678672.329041
Epoch 4: training loss 396773614411.294
Test Loss of 371208865885.327759, Test MSE of 371208863126.891602
Epoch 5: training loss 330261326667.294
Test Loss of 285337664982.665771, Test MSE of 285337668014.749512
Epoch 6: training loss 246927009430.588
Test Loss of 209941694614.177185, Test MSE of 209941695830.458435
Epoch 7: training loss 177189201076.706
Test Loss of 152063225811.467957, Test MSE of 152063228069.846008
Epoch 8: training loss 145099756544.000
Test Loss of 134526442870.021744, Test MSE of 134526443535.991760
Epoch 9: training loss 135616012318.118
Test Loss of 128574128849.987503, Test MSE of 128574126809.357727
Epoch 10: training loss 132846115960.471
Test Loss of 126300760632.967850, Test MSE of 126300763594.052170
Epoch 11: training loss 128372602066.824
Test Loss of 122933104548.093460, Test MSE of 122933106288.445587
Epoch 12: training loss 124850834191.059
Test Loss of 120084927835.018280, Test MSE of 120084930187.067169
Epoch 13: training loss 122371416425.412
Test Loss of 116902369709.686798, Test MSE of 116902370226.269287
Epoch 14: training loss 119219936045.176
Test Loss of 113693445182.297485, Test MSE of 113693448244.313370
Epoch 15: training loss 114531783318.588
Test Loss of 109982040831.467041, Test MSE of 109982041767.083221
Epoch 16: training loss 112437073347.765
Test Loss of 106610915750.106873, Test MSE of 106610915724.506592
Epoch 17: training loss 108817435437.176
Test Loss of 101702947358.911865, Test MSE of 101702947586.873550
Epoch 18: training loss 103858005564.235
Test Loss of 97454432625.521164, Test MSE of 97454434841.274872
Epoch 19: training loss 99378055951.059
Test Loss of 96134360031.548462, Test MSE of 96134360699.002441
Epoch 20: training loss 96170098477.176
Test Loss of 90220045521.632202, Test MSE of 90220045066.569397
Epoch 21: training loss 91557072173.176
Test Loss of 87309209230.478836, Test MSE of 87309210103.373871
Epoch 22: training loss 88012403576.471
Test Loss of 85851096206.360397, Test MSE of 85851097531.085968
Epoch 23: training loss 85194647657.412
Test Loss of 81639052803.671524, Test MSE of 81639053493.436035
Epoch 24: training loss 81425724235.294
Test Loss of 79301752506.537125, Test MSE of 79301753561.506134
Epoch 25: training loss 77713752997.647
Test Loss of 76473720836.737457, Test MSE of 76473718326.184067
Epoch 26: training loss 74508073170.824
Test Loss of 72799896096.806854, Test MSE of 72799895648.085587
Epoch 27: training loss 71910567875.765
Test Loss of 69404592313.234329, Test MSE of 69404592044.700378
Epoch 28: training loss 68923789703.529
Test Loss of 67540081810.624107, Test MSE of 67540082139.801659
Epoch 29: training loss 64805084370.824
Test Loss of 62425469361.003006, Test MSE of 62425469429.308479
Epoch 30: training loss 62404010300.235
Test Loss of 59051628169.267639, Test MSE of 59051627381.226021
Epoch 31: training loss 59919421319.529
Test Loss of 58724623538.365021, Test MSE of 58724623584.649292
Epoch 32: training loss 57041534102.588
Test Loss of 55227627321.974556, Test MSE of 55227626769.552475
Epoch 33: training loss 54990858676.706
Test Loss of 50316423490.146660, Test MSE of 50316423092.732445
Epoch 34: training loss 51468026819.765
Test Loss of 51128834345.511909, Test MSE of 51128834120.974701
Epoch 35: training loss 49522202398.118
Test Loss of 50719970243.597504, Test MSE of 50719969822.249435
Epoch 36: training loss 47596398486.588
Test Loss of 50876580162.146660, Test MSE of 50876579160.366035
Epoch 37: training loss 44976357436.235
Test Loss of 46408224955.129311, Test MSE of 46408225403.257774
Epoch 38: training loss 43129962368.000
Test Loss of 44443411024.655098, Test MSE of 44443411305.740913
Epoch 39: training loss 40776089389.176
Test Loss of 44051762288.514458, Test MSE of 44051761939.077217
Epoch 40: training loss 38536613722.353
Test Loss of 38725788542.904465, Test MSE of 38725787513.124413
Epoch 41: training loss 37060100540.235
Test Loss of 36463549136.092529, Test MSE of 36463549559.560638
Epoch 42: training loss 34845159695.059
Test Loss of 35819590625.917191, Test MSE of 35819590123.835136
Epoch 43: training loss 33507717391.059
Test Loss of 38160385157.359238, Test MSE of 38160384460.898064
Epoch 44: training loss 31240390083.765
Test Loss of 34891968936.475594, Test MSE of 34891969474.054016
Epoch 45: training loss 30617259858.824
Test Loss of 33960192152.072170, Test MSE of 33960191693.396858
Epoch 46: training loss 28483063205.647
Test Loss of 30178311650.035622, Test MSE of 30178310541.762085
Epoch 47: training loss 27686972581.647
Test Loss of 31303403956.319221, Test MSE of 31303403433.467327
Epoch 48: training loss 26572102369.882
Test Loss of 32051422814.156834, Test MSE of 32051423394.747166
Epoch 49: training loss 24934982317.176
Test Loss of 28557044605.483231, Test MSE of 28557044934.385185
Epoch 50: training loss 24301368033.882
Test Loss of 29929491972.145271, Test MSE of 29929492475.362179
Epoch 51: training loss 22882831269.647
Test Loss of 29013579971.656719, Test MSE of 29013579697.840382
Epoch 52: training loss 22201544858.353
Test Loss of 29905431971.027527, Test MSE of 29905430945.768581
Epoch 53: training loss 20461978145.882
Test Loss of 29133794879.837151, Test MSE of 29133795344.682903
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24161122241.840626, 'MSE - std': 4972673102.842278, 'R2 - mean': 0.8212907543080101, 'R2 - std': 0.029286710315714615} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005533 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927366656.000
Test Loss of 447260127401.600769, Test MSE of 447260125785.203857
Epoch 2: training loss 421906716431.059
Test Loss of 447241166304.140625, Test MSE of 447241168958.453491
Epoch 3: training loss 421878859535.059
Test Loss of 447216074202.455688, Test MSE of 447216073545.963440
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897556329.412
Test Loss of 447222970990.974792, Test MSE of 447222966285.740662
Epoch 2: training loss 421888510433.882
Test Loss of 447224488855.065491, Test MSE of 447224493007.629211
Epoch 3: training loss 418287014972.235
Test Loss of 435620222630.639832, Test MSE of 435620214594.557129
Epoch 4: training loss 392258220634.353
Test Loss of 394384992593.780273, Test MSE of 394384986374.503113
Epoch 5: training loss 327178394563.765
Test Loss of 305276579888.322021, Test MSE of 305276580248.737122
Epoch 6: training loss 244852621131.294
Test Loss of 227983884128.821655, Test MSE of 227983886255.650208
Epoch 7: training loss 175033216722.824
Test Loss of 165751287844.952118, Test MSE of 165751292033.353485
Epoch 8: training loss 141827640771.765
Test Loss of 145258377957.884796, Test MSE of 145258377958.769012
Epoch 9: training loss 133998888929.882
Test Loss of 138743627597.871857, Test MSE of 138743626157.258942
Epoch 10: training loss 130289015416.471
Test Loss of 135395005913.981964, Test MSE of 135395006206.767349
Epoch 11: training loss 126459686189.176
Test Loss of 132619728624.544067, Test MSE of 132619732573.742508
Epoch 12: training loss 123606174629.647
Test Loss of 129107259193.974548, Test MSE of 129107258424.839294
Epoch 13: training loss 120058583311.059
Test Loss of 125771422787.034927, Test MSE of 125771423248.210953
Epoch 14: training loss 116065099896.471
Test Loss of 122076439651.723343, Test MSE of 122076439604.819046
Epoch 15: training loss 113053896854.588
Test Loss of 120181763905.554474, Test MSE of 120181765507.222122
Epoch 16: training loss 108311828028.235
Test Loss of 115515033299.171875, Test MSE of 115515031405.010590
Epoch 17: training loss 104092856410.353
Test Loss of 110302490558.623184, Test MSE of 110302490781.727707
Epoch 18: training loss 102785026108.235
Test Loss of 108264556633.537827, Test MSE of 108264554380.346588
Epoch 19: training loss 98010998573.176
Test Loss of 103574366905.826508, Test MSE of 103574368754.248322
Epoch 20: training loss 93986221673.412
Test Loss of 99639719545.160309, Test MSE of 99639719754.434082
Epoch 21: training loss 90633409174.588
Test Loss of 99003101188.737457, Test MSE of 99003100947.216690
Epoch 22: training loss 86659537874.824
Test Loss of 92637910397.127914, Test MSE of 92637911201.422623
Epoch 23: training loss 83609676800.000
Test Loss of 87255263518.615784, Test MSE of 87255263814.001007
Epoch 24: training loss 79609219312.941
Test Loss of 84882371318.939621, Test MSE of 84882370990.989670
Epoch 25: training loss 76782044084.706
Test Loss of 83088849475.390244, Test MSE of 83088849225.554001
Epoch 26: training loss 73358862953.412
Test Loss of 78076794665.393478, Test MSE of 78076795005.505875
Epoch 27: training loss 69984556754.824
Test Loss of 74266837556.704147, Test MSE of 74266837863.422165
Epoch 28: training loss 67382386040.471
Test Loss of 69964457107.097855, Test MSE of 69964457735.575760
Epoch 29: training loss 64721133703.529
Test Loss of 68354808629.947723, Test MSE of 68354808477.728302
Epoch 30: training loss 60775458334.118
Test Loss of 65890824702.697205, Test MSE of 65890823567.920555
Epoch 31: training loss 58758945551.059
Test Loss of 60999931026.150360, Test MSE of 60999930708.451042
Epoch 32: training loss 55396541861.647
Test Loss of 56608016937.808006, Test MSE of 56608016582.316261
Epoch 33: training loss 53291396261.647
Test Loss of 56967851847.239418, Test MSE of 56967851113.284348
Epoch 34: training loss 50520588408.471
Test Loss of 53915137719.694656, Test MSE of 53915137100.043602
Epoch 35: training loss 48340233607.529
Test Loss of 55585192236.828125, Test MSE of 55585192164.104080
Epoch 36: training loss 46781477654.588
Test Loss of 45985027392.488548, Test MSE of 45985027813.116669
Epoch 37: training loss 43941205360.941
Test Loss of 48067497073.461945, Test MSE of 48067497547.632843
Epoch 38: training loss 42500242816.000
Test Loss of 43670216861.283371, Test MSE of 43670217188.723061
Epoch 39: training loss 40090560858.353
Test Loss of 42298204304.018509, Test MSE of 42298204169.358467
Epoch 40: training loss 38517456828.235
Test Loss of 41929294717.483231, Test MSE of 41929295040.301285
Epoch 41: training loss 36549959183.059
Test Loss of 41092705148.061996, Test MSE of 41092704504.118042
Epoch 42: training loss 34636386334.118
Test Loss of 37019920415.740921, Test MSE of 37019921310.739189
Epoch 43: training loss 33853435128.471
Test Loss of 37410034666.444595, Test MSE of 37410034310.124626
Epoch 44: training loss 32024121833.412
Test Loss of 35613358719.082115, Test MSE of 35613359857.823059
Epoch 45: training loss 29906087905.882
Test Loss of 33289521258.592644, Test MSE of 33289521311.240746
Epoch 46: training loss 29254629172.706
Test Loss of 34737291184.410828, Test MSE of 34737292077.750633
Epoch 47: training loss 27500222720.000
Test Loss of 32283320060.387695, Test MSE of 32283320155.363270
Epoch 48: training loss 26865386560.000
Test Loss of 29762923896.864216, Test MSE of 29762924034.064342
Epoch 49: training loss 25385304583.529
Test Loss of 29384624092.469120, Test MSE of 29384623995.687672
Epoch 50: training loss 24217462166.588
Test Loss of 30383580307.097847, Test MSE of 30383580338.190769
Epoch 51: training loss 23101712150.588
Test Loss of 27094000639.289383, Test MSE of 27094000461.786251
Epoch 52: training loss 22249163226.353
Test Loss of 28788479177.104790, Test MSE of 28788479046.132526
Epoch 53: training loss 21651039469.176
Test Loss of 27616787486.082813, Test MSE of 27616787389.250168
Epoch 54: training loss 20901884054.588
Test Loss of 26749855755.843628, Test MSE of 26749856139.326851
Epoch 55: training loss 20227043855.059
Test Loss of 24050018635.147816, Test MSE of 24050018663.440125
Epoch 56: training loss 19602709910.588
Test Loss of 28724932535.043259, Test MSE of 28724932363.196522
Epoch 57: training loss 18554105984.000
Test Loss of 25945463500.302567, Test MSE of 25945463161.714306
Epoch 58: training loss 17923714191.059
Test Loss of 23219056614.417767, Test MSE of 23219056659.718102
Epoch 59: training loss 17592196178.824
Test Loss of 24896135501.516541, Test MSE of 24896135247.076183
Epoch 60: training loss 16924573029.647
Test Loss of 23431132225.139950, Test MSE of 23431132500.364765
Epoch 61: training loss 16478511096.471
Test Loss of 24863951656.209114, Test MSE of 24863951579.477409
Epoch 62: training loss 15923067904.000
Test Loss of 22572392429.997688, Test MSE of 22572392558.530743
Epoch 63: training loss 15543263936.000
Test Loss of 22017710301.238953, Test MSE of 22017710056.024673
Epoch 64: training loss 15170599691.294
Test Loss of 24860176222.926670, Test MSE of 24860176663.211784
Epoch 65: training loss 14541364133.647
Test Loss of 23551397193.963451, Test MSE of 23551397359.136356
Epoch 66: training loss 14190241219.765
Test Loss of 24694888313.456394, Test MSE of 24694888498.388729
Epoch 67: training loss 13733553396.706
Test Loss of 21978484924.550545, Test MSE of 21978485339.126080
Epoch 68: training loss 13625593871.059
Test Loss of 24822271508.252602, Test MSE of 24822271635.550159
Epoch 69: training loss 13246434951.529
Test Loss of 22348552508.461716, Test MSE of 22348552781.498596
Epoch 70: training loss 12851352158.118
Test Loss of 24219140890.944252, Test MSE of 24219141090.426987
Epoch 71: training loss 12731522736.941
Test Loss of 22370135586.938702, Test MSE of 22370135054.446011
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23564126512.709087, 'MSE - std': 4147022183.81133, 'R2 - mean': 0.8312216652856974, 'R2 - std': 0.027731815419079417} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005382 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109873814.588
Test Loss of 410763770744.003723, Test MSE of 410763773275.806824
Epoch 2: training loss 430088159713.882
Test Loss of 410745624277.471558, Test MSE of 410745626042.259277
Epoch 3: training loss 430060468585.412
Test Loss of 410721692765.349365, Test MSE of 410721686175.672852
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076243486.118
Test Loss of 410726060174.156433, Test MSE of 410726062273.172058
Epoch 2: training loss 430064808417.882
Test Loss of 410726559290.994934, Test MSE of 410726556822.338257
Epoch 3: training loss 426496377554.824
Test Loss of 399448433038.037964, Test MSE of 399448440825.262939
Epoch 4: training loss 400372317726.118
Test Loss of 358614433519.059692, Test MSE of 358614430944.730591
Epoch 5: training loss 334391678373.647
Test Loss of 272914533298.761688, Test MSE of 272914532510.329712
Epoch 6: training loss 251940341458.824
Test Loss of 195042434165.989807, Test MSE of 195042435193.476807
Epoch 7: training loss 183535257660.235
Test Loss of 135173117744.451645, Test MSE of 135173117660.911072
Epoch 8: training loss 148497577381.647
Test Loss of 116808201952.844055, Test MSE of 116808203599.973419
Epoch 9: training loss 139124410307.765
Test Loss of 111038930265.440079, Test MSE of 111038931473.452072
Epoch 10: training loss 135238958622.118
Test Loss of 108404562675.324387, Test MSE of 108404560928.955948
Epoch 11: training loss 132408378789.647
Test Loss of 104760047491.850067, Test MSE of 104760048605.112152
Epoch 12: training loss 129090253583.059
Test Loss of 102567712943.326233, Test MSE of 102567712311.362442
Epoch 13: training loss 124571785788.235
Test Loss of 99076708716.868118, Test MSE of 99076708789.193970
Epoch 14: training loss 122144860521.412
Test Loss of 96158113541.804718, Test MSE of 96158115474.820007
Epoch 15: training loss 118354642432.000
Test Loss of 92059433605.390091, Test MSE of 92059434601.380493
Epoch 16: training loss 114385190430.118
Test Loss of 90819724799.289215, Test MSE of 90819727157.566971
Epoch 17: training loss 109834548826.353
Test Loss of 88105896276.227676, Test MSE of 88105895039.393600
Epoch 18: training loss 105602225603.765
Test Loss of 83921514346.261917, Test MSE of 83921511767.076767
Epoch 19: training loss 102575455924.706
Test Loss of 81755083083.698288, Test MSE of 81755082333.712341
Epoch 20: training loss 98185709748.706
Test Loss of 77806517024.814438, Test MSE of 77806516349.859299
Epoch 21: training loss 94130526720.000
Test Loss of 74478355193.958359, Test MSE of 74478356009.422775
Epoch 22: training loss 91970655247.059
Test Loss of 72097484528.955109, Test MSE of 72097484295.300705
Epoch 23: training loss 87563272026.353
Test Loss of 70959195203.761215, Test MSE of 70959195285.304840
Epoch 24: training loss 84561651922.824
Test Loss of 64714588565.619621, Test MSE of 64714587430.562210
Epoch 25: training loss 79776299188.706
Test Loss of 62054862455.648308, Test MSE of 62054862607.268364
Epoch 26: training loss 76771885010.824
Test Loss of 59796788475.616844, Test MSE of 59796789070.143997
Epoch 27: training loss 73773803926.588
Test Loss of 60141016372.479408, Test MSE of 60141015807.207718
Epoch 28: training loss 69752697027.765
Test Loss of 54886607607.115227, Test MSE of 54886608357.073898
Epoch 29: training loss 67213892623.059
Test Loss of 53603275062.848679, Test MSE of 53603273893.168282
Epoch 30: training loss 63988262618.353
Test Loss of 50615404450.176773, Test MSE of 50615404786.321594
Epoch 31: training loss 60736663913.412
Test Loss of 48345272009.151321, Test MSE of 48345272517.738068
Epoch 32: training loss 58607615405.176
Test Loss of 48349964565.678856, Test MSE of 48349965061.516037
Epoch 33: training loss 55741753871.059
Test Loss of 44306041828.042572, Test MSE of 44306042075.112816
Epoch 34: training loss 53144638938.353
Test Loss of 43650930167.707542, Test MSE of 43650929171.448357
Epoch 35: training loss 51182280033.882
Test Loss of 40346362475.801941, Test MSE of 40346362668.701920
Epoch 36: training loss 48338878765.176
Test Loss of 40822798765.312355, Test MSE of 40822798720.010468
Epoch 37: training loss 46794071860.706
Test Loss of 38940697466.846832, Test MSE of 38940698343.059555
Epoch 38: training loss 44397860487.529
Test Loss of 36157894182.145302, Test MSE of 36157894053.044441
Epoch 39: training loss 42136860348.235
Test Loss of 34971963744.547897, Test MSE of 34971964054.990227
Epoch 40: training loss 40226222592.000
Test Loss of 33601139008.325775, Test MSE of 33601138394.328232
Epoch 41: training loss 37867595422.118
Test Loss of 30424275132.594170, Test MSE of 30424275201.866997
Epoch 42: training loss 36419610322.824
Test Loss of 32991529154.280426, Test MSE of 32991530006.124344
Epoch 43: training loss 34957608854.588
Test Loss of 30924726524.090698, Test MSE of 30924727067.518169
Epoch 44: training loss 33153832839.529
Test Loss of 28544192957.423416, Test MSE of 28544192103.484203
Epoch 45: training loss 32100538669.176
Test Loss of 26928040468.612679, Test MSE of 26928040649.291809
Epoch 46: training loss 30476554797.176
Test Loss of 26914955406.156410, Test MSE of 26914956083.043121
Epoch 47: training loss 29044161874.824
Test Loss of 25253217540.146229, Test MSE of 25253217712.238148
Epoch 48: training loss 28212355689.412
Test Loss of 23790308344.892181, Test MSE of 23790308482.585609
Epoch 49: training loss 26396890661.647
Test Loss of 24472607465.847294, Test MSE of 24472607541.807896
Epoch 50: training loss 25627730624.000
Test Loss of 23993186203.542805, Test MSE of 23993186026.365395
Epoch 51: training loss 24496763922.824
Test Loss of 24258085527.396576, Test MSE of 24258085738.446922
Epoch 52: training loss 23696250010.353
Test Loss of 24224871588.427578, Test MSE of 24224871479.764870
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23729312754.47303, 'MSE - std': 3602805049.9814386, 'R2 - mean': 0.823431126619921, 'R2 - std': 0.027547552848434437} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005386 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043484340.706
Test Loss of 431612963038.711731, Test MSE of 431612967442.456482
Epoch 2: training loss 424023428276.706
Test Loss of 431592678327.500244, Test MSE of 431592677969.225830
Epoch 3: training loss 423996315045.647
Test Loss of 431564744631.026367, Test MSE of 431564753541.417603
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011214968.471
Test Loss of 431567906811.261475, Test MSE of 431567909112.472900
Epoch 2: training loss 423998833121.882
Test Loss of 431570980164.590454, Test MSE of 431570979772.436157
Epoch 3: training loss 420527958738.824
Test Loss of 420107784690.495117, Test MSE of 420107781216.352539
Epoch 4: training loss 394850926712.471
Test Loss of 378913040970.632141, Test MSE of 378913042833.650574
Epoch 5: training loss 329743760384.000
Test Loss of 290598883568.718201, Test MSE of 290598881150.423279
Epoch 6: training loss 247703139207.529
Test Loss of 213136540717.016205, Test MSE of 213136541132.169647
Epoch 7: training loss 179596376967.529
Test Loss of 149031077780.908844, Test MSE of 149031073623.478363
Epoch 8: training loss 144409743600.941
Test Loss of 129673111168.177689, Test MSE of 129673111516.349136
Epoch 9: training loss 136412557522.824
Test Loss of 123705765996.038864, Test MSE of 123705764691.164566
Epoch 10: training loss 133947666341.647
Test Loss of 119466990361.232758, Test MSE of 119466991315.300537
Epoch 11: training loss 129086944496.941
Test Loss of 117549226629.390091, Test MSE of 117549228066.073792
Epoch 12: training loss 127723171990.588
Test Loss of 113451205570.872742, Test MSE of 113451203964.210678
Epoch 13: training loss 122082419952.941
Test Loss of 109040675588.383163, Test MSE of 109040676167.173752
Epoch 14: training loss 118842626108.235
Test Loss of 107159939912.618225, Test MSE of 107159941416.400040
Epoch 15: training loss 114765557910.588
Test Loss of 103158244385.643677, Test MSE of 103158246959.636505
Epoch 16: training loss 111903295728.941
Test Loss of 100726684960.577515, Test MSE of 100726683056.840057
Epoch 17: training loss 108500797259.294
Test Loss of 96823497543.670517, Test MSE of 96823497922.153107
Epoch 18: training loss 104669243120.941
Test Loss of 93136455770.032394, Test MSE of 93136454750.916565
Epoch 19: training loss 100170749409.882
Test Loss of 88148864430.733917, Test MSE of 88148864975.436172
Epoch 20: training loss 96253989285.647
Test Loss of 86308293054.371124, Test MSE of 86308292287.669907
Epoch 21: training loss 94009998637.176
Test Loss of 82557412723.028229, Test MSE of 82557413428.540771
Epoch 22: training loss 90284294068.706
Test Loss of 78504901511.167053, Test MSE of 78504901381.517120
Epoch 23: training loss 86047620615.529
Test Loss of 75103245284.990280, Test MSE of 75103244979.815063
Epoch 24: training loss 82581937001.412
Test Loss of 74674390153.891724, Test MSE of 74674389615.953690
Epoch 25: training loss 79110796905.412
Test Loss of 66633218337.051369, Test MSE of 66633218059.654182
Epoch 26: training loss 76353926625.882
Test Loss of 63713807834.328552, Test MSE of 63713808200.134079
Epoch 27: training loss 72644590772.706
Test Loss of 60442226477.608513, Test MSE of 60442227540.545898
Epoch 28: training loss 70425893014.588
Test Loss of 60526475761.547432, Test MSE of 60526474023.531090
Epoch 29: training loss 66942151454.118
Test Loss of 56404034025.018044, Test MSE of 56404033022.261581
Epoch 30: training loss 64093293116.235
Test Loss of 55359861505.540031, Test MSE of 55359860846.094414
Epoch 31: training loss 61619983676.235
Test Loss of 53354136740.427582, Test MSE of 53354136542.557106
Epoch 32: training loss 58575052656.941
Test Loss of 48318975080.721886, Test MSE of 48318976650.090218
Epoch 33: training loss 56147077549.176
Test Loss of 48749368734.622856, Test MSE of 48749369317.556328
Epoch 34: training loss 53799965891.765
Test Loss of 41512150021.212402, Test MSE of 41512149849.365326
Epoch 35: training loss 51462413884.235
Test Loss of 44731709691.142990, Test MSE of 44731710418.240372
Epoch 36: training loss 49176235399.529
Test Loss of 41453470327.174454, Test MSE of 41453469263.227890
Epoch 37: training loss 46802399058.824
Test Loss of 37976564138.469231, Test MSE of 37976565002.021431
Epoch 38: training loss 45216790964.706
Test Loss of 35234108334.970848, Test MSE of 35234108827.836578
Epoch 39: training loss 43718716291.765
Test Loss of 36175793630.119392, Test MSE of 36175794184.254311
Epoch 40: training loss 41198941515.294
Test Loss of 34861669529.055069, Test MSE of 34861669474.540443
Epoch 41: training loss 39099913569.882
Test Loss of 36609722721.495605, Test MSE of 36609722092.847748
Epoch 42: training loss 37600940973.176
Test Loss of 33208214584.388710, Test MSE of 33208215140.343853
Epoch 43: training loss 35918798576.941
Test Loss of 31839738379.135586, Test MSE of 31839738788.919868
Epoch 44: training loss 34375495175.529
Test Loss of 30159609226.720963, Test MSE of 30159608674.066353
Epoch 45: training loss 32949207356.235
Test Loss of 31624927667.946320, Test MSE of 31624927754.805801
Epoch 46: training loss 31427076811.294
Test Loss of 29372720741.641834, Test MSE of 29372720925.248150
Epoch 47: training loss 29682048240.941
Test Loss of 26372874931.827858, Test MSE of 26372874455.892807
Epoch 48: training loss 28958882906.353
Test Loss of 26346098316.497917, Test MSE of 26346098493.829697
Epoch 49: training loss 27595512124.235
Test Loss of 22823108728.359093, Test MSE of 22823109356.613960
Epoch 50: training loss 26444025136.941
Test Loss of 22948677254.811661, Test MSE of 22948677557.632374
Epoch 51: training loss 25550562153.412
Test Loss of 23422951391.777882, Test MSE of 23422951115.141808
Epoch 52: training loss 24280931851.294
Test Loss of 21743125570.813511, Test MSE of 21743125515.622711
Epoch 53: training loss 23487334942.118
Test Loss of 23511796567.781582, Test MSE of 23511796549.159763
Epoch 54: training loss 22526696824.471
Test Loss of 20165050435.287369, Test MSE of 20165050230.545044
Epoch 55: training loss 21976172024.471
Test Loss of 23874292509.023602, Test MSE of 23874291755.369164
Epoch 56: training loss 21173985995.294
Test Loss of 20539009247.659416, Test MSE of 20539008955.761459
Epoch 57: training loss 20431067580.235
Test Loss of 20256761806.719112, Test MSE of 20256761788.004921
Epoch 58: training loss 19784666187.294
Test Loss of 19604430933.293846, Test MSE of 19604430880.440010
Epoch 59: training loss 18751984271.059
Test Loss of 19711951888.584915, Test MSE of 19711951876.543434
Epoch 60: training loss 18631518076.235
Test Loss of 20045618091.180008, Test MSE of 20045618022.035736
Epoch 61: training loss 17657527130.353
Test Loss of 20209992473.232761, Test MSE of 20209991783.731228
Epoch 62: training loss 17531715056.941
Test Loss of 19057296819.472466, Test MSE of 19057296743.090157
Epoch 63: training loss 16948065660.235
Test Loss of 18799621074.036095, Test MSE of 18799621328.412540
Epoch 64: training loss 16755229816.471
Test Loss of 20055736376.862564, Test MSE of 20055736298.593502
Epoch 65: training loss 15969359781.647
Test Loss of 21162953054.652477, Test MSE of 21162952768.439400
Epoch 66: training loss 15720752342.588
Test Loss of 18420058367.881535, Test MSE of 18420058435.360027
Epoch 67: training loss 15141757500.235
Test Loss of 20431356504.847755, Test MSE of 20431356830.119629
Epoch 68: training loss 14768278373.647
Test Loss of 20465952462.837574, Test MSE of 20465952604.013691
Epoch 69: training loss 14868555550.118
Test Loss of 20054183608.092548, Test MSE of 20054183980.901299
Epoch 70: training loss 14528416327.529
Test Loss of 19102993809.354927, Test MSE of 19102994055.009602
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22804049014.580345, 'MSE - std': 3715994528.518336, 'R2 - mean': 0.8302125275270937, 'R2 - std': 0.02812549959387463} 
 

Saving model.....
Results After CV: {'MSE - mean': 22804049014.580345, 'MSE - std': 3715994528.518336, 'R2 - mean': 0.8302125275270937, 'R2 - std': 0.02812549959387463}
Train time: 97.19270912399952
Inference time: 0.07405668599967612
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 82 finished with value: 22804049014.580345 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005703 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525027719.529
Test Loss of 418112802995.075623, Test MSE of 418112812145.342590
Epoch 2: training loss 427503823932.235
Test Loss of 418094786082.938721, Test MSE of 418094794752.932312
Epoch 3: training loss 427476421451.294
Test Loss of 418071004344.997437, Test MSE of 418071000228.651123
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494914891.294
Test Loss of 418078436450.302124, Test MSE of 418078434824.399170
Epoch 2: training loss 427483912553.412
Test Loss of 418079594935.161682, Test MSE of 418079599471.404907
Epoch 3: training loss 427483440549.647
Test Loss of 418077582397.823730, Test MSE of 418077581961.497009
Epoch 4: training loss 421665926445.176
Test Loss of 400109515956.260010, Test MSE of 400109514661.062683
Epoch 5: training loss 384270905705.412
Test Loss of 344468484781.035400, Test MSE of 344468484237.478149
Epoch 6: training loss 316239011719.529
Test Loss of 267297119306.614838, Test MSE of 267297124207.410645
Epoch 7: training loss 223375474145.882
Test Loss of 169970666978.983124, Test MSE of 169970665947.798737
Epoch 8: training loss 160240335360.000
Test Loss of 130593844564.859589, Test MSE of 130593843625.378265
Epoch 9: training loss 141412872583.529
Test Loss of 120230287916.650467, Test MSE of 120230286433.374710
Epoch 10: training loss 137106469737.412
Test Loss of 115914179844.796661, Test MSE of 115914177321.715393
Epoch 11: training loss 133361448960.000
Test Loss of 112847228394.563034, Test MSE of 112847228375.046692
Epoch 12: training loss 129586321889.882
Test Loss of 110529409862.528793, Test MSE of 110529407997.204956
Epoch 13: training loss 126701338910.118
Test Loss of 107169752366.486237, Test MSE of 107169753107.162018
Epoch 14: training loss 122127060239.059
Test Loss of 104019630511.581772, Test MSE of 104019627980.764114
Epoch 15: training loss 118042828649.412
Test Loss of 100678808574.341888, Test MSE of 100678810000.395264
Epoch 16: training loss 114824009140.706
Test Loss of 96376277244.506134, Test MSE of 96376277325.950912
Epoch 17: training loss 111580970977.882
Test Loss of 93849885423.833450, Test MSE of 93849885093.925522
Epoch 18: training loss 107084232463.059
Test Loss of 90999603006.712006, Test MSE of 90999603620.373383
Epoch 19: training loss 102895903412.706
Test Loss of 86682999278.116119, Test MSE of 86682999171.382401
Epoch 20: training loss 98833871766.588
Test Loss of 82722992737.236176, Test MSE of 82722992802.198105
Epoch 21: training loss 94774891181.176
Test Loss of 80472884102.010635, Test MSE of 80472884046.477646
Epoch 22: training loss 91434816150.588
Test Loss of 76821794466.376129, Test MSE of 76821796042.045990
Epoch 23: training loss 87480059565.176
Test Loss of 73200441087.703903, Test MSE of 73200441016.794708
Epoch 24: training loss 83048314691.765
Test Loss of 71754863606.761978, Test MSE of 71754864411.368378
Epoch 25: training loss 79612099358.118
Test Loss of 67096342203.010872, Test MSE of 67096343001.234589
Epoch 26: training loss 76955309138.824
Test Loss of 64400835761.417534, Test MSE of 64400836160.714058
Epoch 27: training loss 73395639936.000
Test Loss of 63103766473.993057, Test MSE of 63103766067.891495
Epoch 28: training loss 69932799036.235
Test Loss of 60013615236.174881, Test MSE of 60013615876.970978
Epoch 29: training loss 66888174373.647
Test Loss of 55887455626.392784, Test MSE of 55887454363.456200
Epoch 30: training loss 64532432564.706
Test Loss of 51561730355.934303, Test MSE of 51561731442.036400
Epoch 31: training loss 60869616361.412
Test Loss of 52219910990.582466, Test MSE of 52219911653.444344
Epoch 32: training loss 57893068370.824
Test Loss of 48272720079.500343, Test MSE of 48272719431.577026
Epoch 33: training loss 54479674819.765
Test Loss of 48024316057.967155, Test MSE of 48024316134.109764
Epoch 34: training loss 52093508525.176
Test Loss of 44684083358.230858, Test MSE of 44684083252.922539
Epoch 35: training loss 49129555689.412
Test Loss of 40048926840.331253, Test MSE of 40048928008.460335
Epoch 36: training loss 47019674744.471
Test Loss of 41349497658.211426, Test MSE of 41349498260.057205
Epoch 37: training loss 45014579501.176
Test Loss of 38362894689.413834, Test MSE of 38362895408.075752
Epoch 38: training loss 42681320094.118
Test Loss of 34012084789.177887, Test MSE of 34012084822.694721
Epoch 39: training loss 40357097837.176
Test Loss of 33735856132.737450, Test MSE of 33735856295.602722
Epoch 40: training loss 38348854482.824
Test Loss of 34961760383.911171, Test MSE of 34961760303.938065
Epoch 41: training loss 36112617765.647
Test Loss of 31032105381.159382, Test MSE of 31032105609.911697
Epoch 42: training loss 35174406603.294
Test Loss of 30978427630.885960, Test MSE of 30978427436.065819
Epoch 43: training loss 33497993622.588
Test Loss of 28830670585.545223, Test MSE of 28830671151.555225
Epoch 44: training loss 31843756784.941
Test Loss of 27892129088.014805, Test MSE of 27892128701.354305
Epoch 45: training loss 30535322831.059
Test Loss of 26493629592.782791, Test MSE of 26493629453.562248
Epoch 46: training loss 28604960075.294
Test Loss of 23537690938.566734, Test MSE of 23537690543.768280
Epoch 47: training loss 27682137667.765
Test Loss of 24135830334.948879, Test MSE of 24135830445.969231
Epoch 48: training loss 26681972615.529
Test Loss of 22611743592.164700, Test MSE of 22611743905.154400
Epoch 49: training loss 25134494930.824
Test Loss of 22806702239.652092, Test MSE of 22806701861.426311
Epoch 50: training loss 24396727939.765
Test Loss of 21351209644.087902, Test MSE of 21351209429.206566
Epoch 51: training loss 23433410443.294
Test Loss of 21278038960.173954, Test MSE of 21278038647.681046
Epoch 52: training loss 22542782768.941
Test Loss of 21476324130.524174, Test MSE of 21476324311.190647
Epoch 53: training loss 21782983732.706
Test Loss of 20360228077.583160, Test MSE of 20360228049.757801
Epoch 54: training loss 21007416417.882
Test Loss of 20063173105.432339, Test MSE of 20063173149.879086
Epoch 55: training loss 20246939881.412
Test Loss of 21311795774.179043, Test MSE of 21311796076.761875
Epoch 56: training loss 19535536933.647
Test Loss of 19670968835.434650, Test MSE of 19670968625.538795
Epoch 57: training loss 18665982836.706
Test Loss of 20475350378.651863, Test MSE of 20475350750.587231
Epoch 58: training loss 17983510874.353
Test Loss of 21191700418.413139, Test MSE of 21191700496.979050
Epoch 59: training loss 17698495785.412
Test Loss of 20821231371.073792, Test MSE of 20821231613.560947
Epoch 60: training loss 16876079115.294
Test Loss of 19567883316.822578, Test MSE of 19567883217.301712
Epoch 61: training loss 16542064094.118
Test Loss of 17719691874.183670, Test MSE of 17719692343.014320
Epoch 62: training loss 16309391664.941
Test Loss of 18520430023.979645, Test MSE of 18520430039.235744
Epoch 63: training loss 15711138680.471
Test Loss of 21881239821.087208, Test MSE of 21881239680.845821
Epoch 64: training loss 15309381104.941
Test Loss of 19318110408.867916, Test MSE of 19318110259.383644
Epoch 65: training loss 14674388592.941
Test Loss of 18403370181.314827, Test MSE of 18403370419.317467
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18403370419.317467, 'MSE - std': 0.0, 'R2 - mean': 0.856690957794273, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005518 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917805688.471
Test Loss of 424556405541.366638, Test MSE of 424556404240.648621
Epoch 2: training loss 427895810650.353
Test Loss of 424539191411.120056, Test MSE of 424539192815.721863
Epoch 3: training loss 427866733387.294
Test Loss of 424516023457.310181, Test MSE of 424516021541.693298
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888653492.706
Test Loss of 424524356747.044189, Test MSE of 424524364122.856934
Epoch 2: training loss 427875187290.353
Test Loss of 424525393619.645630, Test MSE of 424525398705.922424
Epoch 3: training loss 427874585178.353
Test Loss of 424524973502.504761, Test MSE of 424524968259.103271
Epoch 4: training loss 422808834770.824
Test Loss of 408496281519.937073, Test MSE of 408496284343.295166
Epoch 5: training loss 386789812705.882
Test Loss of 355056852660.378418, Test MSE of 355056852794.529846
Epoch 6: training loss 318396247582.118
Test Loss of 278444402317.057617, Test MSE of 278444405299.934204
Epoch 7: training loss 225754895721.412
Test Loss of 180946049451.791809, Test MSE of 180946047878.873322
Epoch 8: training loss 159481240937.412
Test Loss of 141895789251.301422, Test MSE of 141895790163.347626
Epoch 9: training loss 139622040124.235
Test Loss of 131611598224.551468, Test MSE of 131611601015.069534
Epoch 10: training loss 134169679209.412
Test Loss of 127237348098.783249, Test MSE of 127237348012.978745
Epoch 11: training loss 130768846004.706
Test Loss of 123947956081.402725, Test MSE of 123947954718.090668
Epoch 12: training loss 126567095657.412
Test Loss of 120866032810.311356, Test MSE of 120866034953.148468
Epoch 13: training loss 123323874243.765
Test Loss of 117111101873.476746, Test MSE of 117111102388.148560
Epoch 14: training loss 119791064124.235
Test Loss of 114170061603.471664, Test MSE of 114170064105.408508
Epoch 15: training loss 115229686061.176
Test Loss of 109420174709.074249, Test MSE of 109420175874.393188
Epoch 16: training loss 111949303687.529
Test Loss of 106068391999.008102, Test MSE of 106068393951.555328
Epoch 17: training loss 107690481573.647
Test Loss of 103531686814.171646, Test MSE of 103531686617.165024
Epoch 18: training loss 104440054000.941
Test Loss of 99117018615.827896, Test MSE of 99117017719.152374
Epoch 19: training loss 97604591616.000
Test Loss of 95393785824.259079, Test MSE of 95393784877.349976
Epoch 20: training loss 95517871826.824
Test Loss of 92301287793.521164, Test MSE of 92301287414.322342
Epoch 21: training loss 91508089193.412
Test Loss of 88411129393.151047, Test MSE of 88411131030.771408
Epoch 22: training loss 88418561867.294
Test Loss of 84039342101.318527, Test MSE of 84039340763.274002
Epoch 23: training loss 84083820634.353
Test Loss of 79297070906.448303, Test MSE of 79297069128.096985
Epoch 24: training loss 80103441784.471
Test Loss of 77898143415.694656, Test MSE of 77898145462.663635
Epoch 25: training loss 76782488440.471
Test Loss of 74882577886.719406, Test MSE of 74882580293.391800
Epoch 26: training loss 72705956487.529
Test Loss of 69805241528.049973, Test MSE of 69805243277.007690
Epoch 27: training loss 69477035866.353
Test Loss of 67707059741.727501, Test MSE of 67707061202.011261
Epoch 28: training loss 65255943032.471
Test Loss of 64190934846.712006, Test MSE of 64190935338.468681
Epoch 29: training loss 62871557541.647
Test Loss of 59839749093.233406, Test MSE of 59839749220.463898
Epoch 30: training loss 60560508581.647
Test Loss of 57879629932.961372, Test MSE of 57879630321.949913
Epoch 31: training loss 56829363847.529
Test Loss of 54976063778.405739, Test MSE of 54976063533.708763
Epoch 32: training loss 53386537697.882
Test Loss of 50801184045.065002, Test MSE of 50801183592.645584
Epoch 33: training loss 50505019919.059
Test Loss of 49905815939.523476, Test MSE of 49905815211.114716
Epoch 34: training loss 48048939384.471
Test Loss of 47150098744.198013, Test MSE of 47150098590.640411
Epoch 35: training loss 45872496624.941
Test Loss of 50153849178.781403, Test MSE of 50153848482.915825
Epoch 36: training loss 42643905400.471
Test Loss of 41957431315.897293, Test MSE of 41957432029.867104
Epoch 37: training loss 39809539425.882
Test Loss of 42826391216.588478, Test MSE of 42826391255.341606
Epoch 38: training loss 38397936752.941
Test Loss of 40545289153.228775, Test MSE of 40545289920.251976
Epoch 39: training loss 36300620762.353
Test Loss of 38219339389.424011, Test MSE of 38219338917.622299
Epoch 40: training loss 34124807168.000
Test Loss of 38275283395.479065, Test MSE of 38275285416.087387
Epoch 41: training loss 32312245330.824
Test Loss of 35703447492.308121, Test MSE of 35703447928.381340
Epoch 42: training loss 30841291941.647
Test Loss of 33620131585.598888, Test MSE of 33620131836.477215
Epoch 43: training loss 28917267433.412
Test Loss of 36993864223.859360, Test MSE of 36993864847.874672
Epoch 44: training loss 27806414629.647
Test Loss of 30487183611.795513, Test MSE of 30487183606.815269
Epoch 45: training loss 26553665483.294
Test Loss of 31804582232.886421, Test MSE of 31804581990.062618
Epoch 46: training loss 25369827395.765
Test Loss of 33032277958.203098, Test MSE of 33032277715.519176
Epoch 47: training loss 23764343552.000
Test Loss of 30600540326.758270, Test MSE of 30600540248.561550
Epoch 48: training loss 22777538149.647
Test Loss of 29917598697.023365, Test MSE of 29917597843.950432
Epoch 49: training loss 21798584421.647
Test Loss of 32547724746.822113, Test MSE of 32547724302.158283
Epoch 50: training loss 21089101481.412
Test Loss of 28693027430.684246, Test MSE of 28693028485.599483
Epoch 51: training loss 19907835855.059
Test Loss of 29580726049.102940, Test MSE of 29580726225.232487
Epoch 52: training loss 18996085496.471
Test Loss of 28936613320.453388, Test MSE of 28936613692.565037
Epoch 53: training loss 18017904775.529
Test Loss of 30127368866.612999, Test MSE of 30127368293.028278
Epoch 54: training loss 17683109432.471
Test Loss of 29457628142.945175, Test MSE of 29457628010.849674
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23930499215.083572, 'MSE - std': 5527128795.766104, 'R2 - mean': 0.8231915257759164, 'R2 - std': 0.03349943201835659} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005906 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926966452.706
Test Loss of 447258991686.824890, Test MSE of 447258985061.548706
Epoch 2: training loss 421906319360.000
Test Loss of 447240290247.150574, Test MSE of 447240287506.611694
Epoch 3: training loss 421879017833.412
Test Loss of 447215802250.037476, Test MSE of 447215807108.629761
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421895536399.059
Test Loss of 447226332958.971069, Test MSE of 447226337139.460083
Epoch 2: training loss 421888363821.176
Test Loss of 447227171192.153625, Test MSE of 447227170015.277466
Epoch 3: training loss 421887920489.412
Test Loss of 447227471084.161926, Test MSE of 447227466781.523132
Epoch 4: training loss 416482222923.294
Test Loss of 430048696698.522339, Test MSE of 430048695610.485046
Epoch 5: training loss 379953001773.176
Test Loss of 374671474453.022461, Test MSE of 374671474199.800232
Epoch 6: training loss 311955251681.882
Test Loss of 296162469873.313904, Test MSE of 296162468666.159851
Epoch 7: training loss 220907286407.529
Test Loss of 195836525197.294464, Test MSE of 195836526168.540955
Epoch 8: training loss 156712259915.294
Test Loss of 154629823926.451080, Test MSE of 154629824518.829285
Epoch 9: training loss 138137604126.118
Test Loss of 142876609973.503571, Test MSE of 142876611103.383942
Epoch 10: training loss 132154569246.118
Test Loss of 137341698612.704147, Test MSE of 137341702261.782181
Epoch 11: training loss 129271851188.706
Test Loss of 134238728949.044647, Test MSE of 134238728490.106812
Epoch 12: training loss 124650624512.000
Test Loss of 131003370562.087433, Test MSE of 131003372619.876389
Epoch 13: training loss 122711892931.765
Test Loss of 128549638516.363632, Test MSE of 128549636926.660172
Epoch 14: training loss 117714045921.882
Test Loss of 123961413905.824661, Test MSE of 123961412206.511826
Epoch 15: training loss 115060124039.529
Test Loss of 120364206616.753174, Test MSE of 120364207471.338394
Epoch 16: training loss 110917044344.471
Test Loss of 117874353956.656021, Test MSE of 117874354771.298782
Epoch 17: training loss 106153197417.412
Test Loss of 112484128403.690033, Test MSE of 112484128394.381454
Epoch 18: training loss 103181827945.412
Test Loss of 110586821693.349991, Test MSE of 110586822479.615845
Epoch 19: training loss 99605865050.353
Test Loss of 105333884437.673843, Test MSE of 105333883265.756729
Epoch 20: training loss 95839667049.412
Test Loss of 102476515675.255142, Test MSE of 102476516350.592514
Epoch 21: training loss 92751204246.588
Test Loss of 100013545625.019669, Test MSE of 100013547182.597061
Epoch 22: training loss 88034878659.765
Test Loss of 94912431313.632202, Test MSE of 94912432825.623840
Epoch 23: training loss 85474201238.588
Test Loss of 89465882429.290771, Test MSE of 89465883274.040787
Epoch 24: training loss 81273458838.588
Test Loss of 87684150194.542679, Test MSE of 87684149836.292664
Epoch 25: training loss 77690557891.765
Test Loss of 79807813775.544754, Test MSE of 79807815494.835098
Epoch 26: training loss 74116067448.471
Test Loss of 79657368927.281982, Test MSE of 79657370499.294754
Epoch 27: training loss 70482165714.824
Test Loss of 77599181789.653488, Test MSE of 77599182735.714691
Epoch 28: training loss 67658664990.118
Test Loss of 73490111157.089066, Test MSE of 73490110716.060104
Epoch 29: training loss 65024885970.824
Test Loss of 67423416419.486465, Test MSE of 67423416450.274147
Epoch 30: training loss 61076529136.941
Test Loss of 65552397539.634514, Test MSE of 65552398635.931923
Epoch 31: training loss 58514872741.647
Test Loss of 63264163218.920197, Test MSE of 63264162519.686012
Epoch 32: training loss 55778127555.765
Test Loss of 64285886439.838997, Test MSE of 64285888201.439125
Epoch 33: training loss 52837072692.706
Test Loss of 55545251091.956512, Test MSE of 55545250711.085205
Epoch 34: training loss 50016113310.118
Test Loss of 55229105969.920891, Test MSE of 55229106877.231506
Epoch 35: training loss 47653394243.765
Test Loss of 48962112598.932220, Test MSE of 48962112515.598526
Epoch 36: training loss 45949231224.471
Test Loss of 49184512300.828125, Test MSE of 49184512651.087570
Epoch 37: training loss 42888091414.588
Test Loss of 44234402158.204948, Test MSE of 44234402829.206772
Epoch 38: training loss 41071932754.824
Test Loss of 43270015900.276657, Test MSE of 43270016288.235153
Epoch 39: training loss 38985180574.118
Test Loss of 43759713439.888969, Test MSE of 43759714513.506035
Epoch 40: training loss 37114405029.647
Test Loss of 40425657379.294006, Test MSE of 40425658170.725899
Epoch 41: training loss 35254971527.529
Test Loss of 38834207548.580154, Test MSE of 38834208063.134422
Epoch 42: training loss 33669840158.118
Test Loss of 36688852014.663887, Test MSE of 36688851986.272781
Epoch 43: training loss 32326926012.235
Test Loss of 36465263104.592178, Test MSE of 36465263154.325287
Epoch 44: training loss 29958619399.529
Test Loss of 33454029443.819569, Test MSE of 33454029736.631172
Epoch 45: training loss 29444389842.824
Test Loss of 30650143856.040714, Test MSE of 30650144155.194157
Epoch 46: training loss 28098728719.059
Test Loss of 32447475097.078880, Test MSE of 32447475038.212208
Epoch 47: training loss 26067000120.471
Test Loss of 32106143568.714317, Test MSE of 32106143056.134464
Epoch 48: training loss 25533778891.294
Test Loss of 28070000360.016655, Test MSE of 28069999983.325439
Epoch 49: training loss 24233808956.235
Test Loss of 29004507723.917648, Test MSE of 29004507491.341835
Epoch 50: training loss 23442134117.647
Test Loss of 28454349781.599815, Test MSE of 28454349952.665123
Epoch 51: training loss 22547450646.588
Test Loss of 27219113118.941475, Test MSE of 27219112636.771217
Epoch 52: training loss 21628117857.882
Test Loss of 29202909679.063614, Test MSE of 29202910108.164738
Epoch 53: training loss 20542563083.294
Test Loss of 28301772827.003471, Test MSE of 28301772624.881367
Epoch 54: training loss 19913368805.647
Test Loss of 28140821033.334259, Test MSE of 28140821039.503345
Epoch 55: training loss 19355599838.118
Test Loss of 26042132851.416145, Test MSE of 26042133067.076012
Epoch 56: training loss 18512144380.235
Test Loss of 27227214128.144344, Test MSE of 27227214198.192898
Epoch 57: training loss 17867816267.294
Test Loss of 25714082090.459404, Test MSE of 25714082273.275776
Epoch 58: training loss 17385349473.882
Test Loss of 25978106978.775848, Test MSE of 25978106847.769875
Epoch 59: training loss 16763571915.294
Test Loss of 24069247485.275967, Test MSE of 24069247796.974396
Epoch 60: training loss 16532751201.882
Test Loss of 22126951555.464260, Test MSE of 22126951613.594582
Epoch 61: training loss 15837055043.765
Test Loss of 25148049449.215824, Test MSE of 25148049797.840515
Epoch 62: training loss 15420920075.294
Test Loss of 22897369187.486469, Test MSE of 22897369242.369267
Epoch 63: training loss 15044911984.941
Test Loss of 24066762548.289612, Test MSE of 24066762424.124256
Epoch 64: training loss 14696853820.235
Test Loss of 23729438996.667130, Test MSE of 23729439360.088985
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23863479263.418713, 'MSE - std': 4513876954.957925, 'R2 - mean': 0.8294725799121334, 'R2 - std': 0.028758382763277492} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005401 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110247032.471
Test Loss of 410764994881.273499, Test MSE of 410764995450.746765
Epoch 2: training loss 430089020837.647
Test Loss of 410747034984.603455, Test MSE of 410747036375.694458
Epoch 3: training loss 430061259113.412
Test Loss of 410723961490.658020, Test MSE of 410723960673.745728
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077755151.059
Test Loss of 410730908655.415100, Test MSE of 410730899679.574646
Epoch 2: training loss 430067146511.059
Test Loss of 410731457538.369263, Test MSE of 410731458119.372192
Epoch 3: training loss 430066643907.765
Test Loss of 410731072525.741760, Test MSE of 410731070389.840576
Epoch 4: training loss 425159917086.118
Test Loss of 395014793719.233704, Test MSE of 395014793841.544678
Epoch 5: training loss 389554370439.529
Test Loss of 341100400941.371582, Test MSE of 341100405850.390076
Epoch 6: training loss 322128875158.588
Test Loss of 264234161483.224426, Test MSE of 264234161882.080231
Epoch 7: training loss 230084164969.412
Test Loss of 165650394015.807495, Test MSE of 165650395076.504456
Epoch 8: training loss 165545302377.412
Test Loss of 125150127630.452576, Test MSE of 125150128921.670395
Epoch 9: training loss 143665001050.353
Test Loss of 114189275998.415543, Test MSE of 114189276126.106079
Epoch 10: training loss 139606888658.824
Test Loss of 110187520128.888474, Test MSE of 110187520652.717651
Epoch 11: training loss 134567756950.588
Test Loss of 107133206691.006012, Test MSE of 107133209054.731216
Epoch 12: training loss 130892072357.647
Test Loss of 105036151315.664963, Test MSE of 105036150267.023575
Epoch 13: training loss 127838427497.412
Test Loss of 101127619670.715408, Test MSE of 101127619848.406311
Epoch 14: training loss 124740083290.353
Test Loss of 97727843739.779724, Test MSE of 97727844498.933060
Epoch 15: training loss 120396050251.294
Test Loss of 95216310761.965759, Test MSE of 95216310951.343750
Epoch 16: training loss 116973103856.941
Test Loss of 93090634324.109207, Test MSE of 93090633175.156555
Epoch 17: training loss 112027978992.941
Test Loss of 88968320062.074966, Test MSE of 88968319505.430588
Epoch 18: training loss 108802346164.706
Test Loss of 85869983790.437759, Test MSE of 85869985135.418701
Epoch 19: training loss 104843062000.941
Test Loss of 83144161220.294312, Test MSE of 83144160947.688309
Epoch 20: training loss 101215572419.765
Test Loss of 80677350478.186020, Test MSE of 80677351708.582993
Epoch 21: training loss 95906150746.353
Test Loss of 78014046473.358627, Test MSE of 78014046754.990311
Epoch 22: training loss 93317929788.235
Test Loss of 74796988496.555298, Test MSE of 74796988762.620834
Epoch 23: training loss 89374512323.765
Test Loss of 69481314627.168900, Test MSE of 69481314301.894287
Epoch 24: training loss 85412504169.412
Test Loss of 69650045129.862106, Test MSE of 69650044911.200867
Epoch 25: training loss 82290047036.235
Test Loss of 66004368915.191116, Test MSE of 66004367701.984322
Epoch 26: training loss 78092831171.765
Test Loss of 63478431402.824615, Test MSE of 63478431745.573242
Epoch 27: training loss 75185704372.706
Test Loss of 60252575479.589081, Test MSE of 60252575662.109558
Epoch 28: training loss 71832896391.529
Test Loss of 57371968666.950485, Test MSE of 57371967794.385704
Epoch 29: training loss 67329415318.588
Test Loss of 56082974794.869041, Test MSE of 56082973477.868690
Epoch 30: training loss 64467864455.529
Test Loss of 52607186370.635818, Test MSE of 52607186154.844345
Epoch 31: training loss 61775827862.588
Test Loss of 47445309539.509483, Test MSE of 47445309445.103081
Epoch 32: training loss 58075538928.941
Test Loss of 47658637149.941696, Test MSE of 47658637049.147064
Epoch 33: training loss 55827794070.588
Test Loss of 46977497033.980568, Test MSE of 46977496993.897835
Epoch 34: training loss 53182228728.471
Test Loss of 42471434396.372047, Test MSE of 42471433789.424522
Epoch 35: training loss 50389763305.412
Test Loss of 42021189974.123093, Test MSE of 42021189412.711304
Epoch 36: training loss 47644143698.824
Test Loss of 37970153854.874596, Test MSE of 37970154362.241714
Epoch 37: training loss 45652412438.588
Test Loss of 34073124193.021748, Test MSE of 34073123037.169628
Epoch 38: training loss 43153860246.588
Test Loss of 37041696766.104584, Test MSE of 37041697865.401268
Epoch 39: training loss 40995790034.824
Test Loss of 36333003400.233223, Test MSE of 36333003133.420143
Epoch 40: training loss 39310837632.000
Test Loss of 32625477041.577049, Test MSE of 32625476599.076630
Epoch 41: training loss 37269666544.941
Test Loss of 30653827651.050438, Test MSE of 30653827407.616829
Epoch 42: training loss 35547267855.059
Test Loss of 28908654320.955112, Test MSE of 28908654457.745987
Epoch 43: training loss 33588184771.765
Test Loss of 28002426406.145302, Test MSE of 28002426368.541653
Epoch 44: training loss 32198125176.471
Test Loss of 28757015967.570568, Test MSE of 28757015228.452824
Epoch 45: training loss 30959648304.941
Test Loss of 25908047947.816753, Test MSE of 25908047602.904942
Epoch 46: training loss 29176853643.294
Test Loss of 25917930734.348911, Test MSE of 25917930491.755142
Epoch 47: training loss 28120478204.235
Test Loss of 24839079404.335030, Test MSE of 24839078972.218571
Epoch 48: training loss 27000120606.118
Test Loss of 22380194462.978252, Test MSE of 22380194738.007954
Epoch 49: training loss 25883782204.235
Test Loss of 20865217492.879223, Test MSE of 20865217566.540863
Epoch 50: training loss 25033791141.647
Test Loss of 23225419204.531235, Test MSE of 23225419412.969315
Epoch 51: training loss 23779566343.529
Test Loss of 23967138056.410919, Test MSE of 23967137701.777050
Epoch 52: training loss 22863826036.706
Test Loss of 21443935469.401203, Test MSE of 21443935452.867290
Epoch 53: training loss 22031943514.353
Test Loss of 21627641638.026840, Test MSE of 21627641942.087955
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23304519933.08602, 'MSE - std': 4027234841.957081, 'R2 - mean': 0.8274783848269562, 'R2 - std': 0.02514386359497314} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003693 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043286287.059
Test Loss of 431613078340.827393, Test MSE of 431613071332.442871
Epoch 2: training loss 424023812939.294
Test Loss of 431593167542.670959, Test MSE of 431593174744.824402
Epoch 3: training loss 423996566347.294
Test Loss of 431565257576.840332, Test MSE of 431565253480.178650
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010746337.882
Test Loss of 431569797574.900513, Test MSE of 431569798697.713013
Epoch 2: training loss 423999624011.294
Test Loss of 431571976487.685303, Test MSE of 431571983380.345215
Epoch 3: training loss 423998970578.824
Test Loss of 431571388589.430847, Test MSE of 431571396433.848083
Epoch 4: training loss 418931669714.824
Test Loss of 414882513540.442383, Test MSE of 414882516655.136902
Epoch 5: training loss 383261070275.765
Test Loss of 359864703185.443787, Test MSE of 359864709543.081787
Epoch 6: training loss 315520972679.529
Test Loss of 280780843640.122192, Test MSE of 280780846364.396667
Epoch 7: training loss 224351222573.176
Test Loss of 180655795232.695984, Test MSE of 180655790715.186127
Epoch 8: training loss 160369933884.235
Test Loss of 138384049932.438690, Test MSE of 138384051992.762573
Epoch 9: training loss 140617535307.294
Test Loss of 127139419171.539108, Test MSE of 127139417991.302521
Epoch 10: training loss 136118442646.588
Test Loss of 121227978322.687637, Test MSE of 121227977463.018478
Epoch 11: training loss 131438134934.588
Test Loss of 118401450545.043961, Test MSE of 118401452045.746094
Epoch 12: training loss 129512910049.882
Test Loss of 115500904309.634430, Test MSE of 115500905613.862244
Epoch 13: training loss 125061560320.000
Test Loss of 110920274638.363724, Test MSE of 110920273397.483749
Epoch 14: training loss 120977757967.059
Test Loss of 107770216000.681168, Test MSE of 107770214189.253845
Epoch 15: training loss 117663462430.118
Test Loss of 104774652683.017120, Test MSE of 104774653270.114761
Epoch 16: training loss 113522758987.294
Test Loss of 101037169406.696899, Test MSE of 101037170477.379303
Epoch 17: training loss 108687081908.706
Test Loss of 97041931715.583527, Test MSE of 97041932840.089172
Epoch 18: training loss 106412384466.824
Test Loss of 93359409014.582138, Test MSE of 93359407991.236084
Epoch 19: training loss 102084775514.353
Test Loss of 89211086286.008331, Test MSE of 89211084576.234039
Epoch 20: training loss 98260887883.294
Test Loss of 86849193183.185562, Test MSE of 86849193629.444824
Epoch 21: training loss 95121028035.765
Test Loss of 81911654899.442856, Test MSE of 81911653834.100937
Epoch 22: training loss 91361470885.647
Test Loss of 80312438573.608521, Test MSE of 80312439926.025253
Epoch 23: training loss 86944683429.647
Test Loss of 75984541179.498383, Test MSE of 75984540106.884277
Epoch 24: training loss 83693713219.765
Test Loss of 73952477184.000000, Test MSE of 73952476651.226349
Epoch 25: training loss 81166203392.000
Test Loss of 67904318786.695045, Test MSE of 67904317686.503922
Epoch 26: training loss 76446612148.706
Test Loss of 68647506427.972237, Test MSE of 68647505325.272675
Epoch 27: training loss 72415885417.412
Test Loss of 62658144063.614990, Test MSE of 62658145684.519478
Epoch 28: training loss 70213018352.941
Test Loss of 59120691724.083298, Test MSE of 59120690485.890663
Epoch 29: training loss 67374735766.588
Test Loss of 57634571386.728363, Test MSE of 57634570580.168747
Epoch 30: training loss 64413731102.118
Test Loss of 53173692162.961594, Test MSE of 53173691842.467407
Epoch 31: training loss 60751823036.235
Test Loss of 51345446746.150856, Test MSE of 51345445996.450516
Epoch 32: training loss 59043982516.706
Test Loss of 49442979853.267929, Test MSE of 49442979401.174873
Epoch 33: training loss 55083879928.471
Test Loss of 48430645509.567795, Test MSE of 48430644644.885727
Epoch 34: training loss 52617313716.706
Test Loss of 46359208934.885704, Test MSE of 46359208978.471085
Epoch 35: training loss 50034400346.353
Test Loss of 43978680370.228600, Test MSE of 43978680556.894363
Epoch 36: training loss 47354533933.176
Test Loss of 38933453297.547432, Test MSE of 38933454083.597397
Epoch 37: training loss 44996291011.765
Test Loss of 39463539758.911613, Test MSE of 39463539486.730232
Epoch 38: training loss 42956504734.118
Test Loss of 38535759376.347984, Test MSE of 38535759129.792938
Epoch 39: training loss 40533526445.176
Test Loss of 31038725704.262840, Test MSE of 31038725818.149441
Epoch 40: training loss 38815446061.176
Test Loss of 32063448021.826931, Test MSE of 32063448260.312428
Epoch 41: training loss 36807351258.353
Test Loss of 33419840883.975937, Test MSE of 33419840771.836815
Epoch 42: training loss 35682625686.588
Test Loss of 31862808545.673298, Test MSE of 31862807941.162220
Epoch 43: training loss 33704356660.706
Test Loss of 29808484528.273949, Test MSE of 29808485067.426010
Epoch 44: training loss 32639849253.647
Test Loss of 29353479297.362331, Test MSE of 29353478885.556698
Epoch 45: training loss 31101149658.353
Test Loss of 26578784256.000000, Test MSE of 26578784124.996979
Epoch 46: training loss 29789848237.176
Test Loss of 26893370239.585377, Test MSE of 26893370195.614601
Epoch 47: training loss 28324270976.000
Test Loss of 25963151087.059696, Test MSE of 25963150947.231598
Epoch 48: training loss 27111880146.824
Test Loss of 26170896176.451641, Test MSE of 26170896135.813747
Epoch 49: training loss 25622783088.941
Test Loss of 24607842720.044422, Test MSE of 24607842772.088539
Epoch 50: training loss 24972355791.059
Test Loss of 25054858755.080055, Test MSE of 25054858675.565342
Epoch 51: training loss 24076153456.941
Test Loss of 24507189478.293385, Test MSE of 24507189361.009560
Epoch 52: training loss 23383184233.412
Test Loss of 22475390051.983341, Test MSE of 22475390201.531727
Epoch 53: training loss 22405926343.529
Test Loss of 22617471381.619621, Test MSE of 22617471540.916172
Epoch 54: training loss 21945147508.706
Test Loss of 22813486436.812588, Test MSE of 22813487115.664940
Epoch 55: training loss 20506160794.353
Test Loss of 21496776628.183247, Test MSE of 21496776822.138241
Epoch 56: training loss 20161283538.824
Test Loss of 23957616833.332718, Test MSE of 23957616547.723003
Epoch 57: training loss 19754469726.118
Test Loss of 22052632503.974087, Test MSE of 22052632374.784290
Epoch 58: training loss 19133461029.647
Test Loss of 20680960623.592781, Test MSE of 20680960830.958069
Epoch 59: training loss 18001826405.647
Test Loss of 21536462282.217491, Test MSE of 21536462200.564987
Epoch 60: training loss 17904129618.824
Test Loss of 20235369528.388710, Test MSE of 20235369727.568016
Epoch 61: training loss 17370567954.824
Test Loss of 20507041598.667282, Test MSE of 20507041842.707237
Epoch 62: training loss 16851152048.941
Test Loss of 20127425638.826469, Test MSE of 20127425503.862957
Epoch 63: training loss 16445335905.882
Test Loss of 19023476572.993984, Test MSE of 19023476582.362743
Epoch 64: training loss 16051604359.529
Test Loss of 20640926309.167976, Test MSE of 20640926468.524864
Epoch 65: training loss 15525929667.765
Test Loss of 19417963281.651089, Test MSE of 19417963448.305038
Epoch 66: training loss 15169716284.235
Test Loss of 19967515363.213326, Test MSE of 19967515447.544060
Epoch 67: training loss 14977029353.412
Test Loss of 19376032975.074501, Test MSE of 19376033509.585079
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22518822648.385834, 'MSE - std': 3929908048.613791, 'R2 - mean': 0.8330425203359277, 'R2 - std': 0.025092020983469746} 
 

Saving model.....
Results After CV: {'MSE - mean': 22518822648.385834, 'MSE - std': 3929908048.613791, 'R2 - mean': 0.8330425203359277, 'R2 - std': 0.025092020983469746}
Train time: 93.8535352019986
Inference time: 0.07061270319827599
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 83 finished with value: 22518822648.385834 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005492 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525131444.706
Test Loss of 418111846530.279907, Test MSE of 418111850346.930847
Epoch 2: training loss 427503851279.059
Test Loss of 418092621810.735107, Test MSE of 418092628858.882385
Epoch 3: training loss 427475611286.588
Test Loss of 418067576909.220459, Test MSE of 418067582726.108398
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427498106759.529
Test Loss of 418073352453.033569, Test MSE of 418073351849.086548
Epoch 2: training loss 427480411919.059
Test Loss of 418072951361.495239, Test MSE of 418072950024.475220
Epoch 3: training loss 427479877270.588
Test Loss of 418072731361.857971, Test MSE of 418072733660.622314
Epoch 4: training loss 427479495860.706
Test Loss of 418072705885.742310, Test MSE of 418072709860.747131
Epoch 5: training loss 421156558125.176
Test Loss of 397404181553.506348, Test MSE of 397404181232.100281
Epoch 6: training loss 376630190200.471
Test Loss of 331758907565.390686, Test MSE of 331758910566.744324
Epoch 7: training loss 297671484656.941
Test Loss of 246274971427.708527, Test MSE of 246274968222.946320
Epoch 8: training loss 218723222799.059
Test Loss of 175239484124.409912, Test MSE of 175239481503.073792
Epoch 9: training loss 161080668190.118
Test Loss of 128029264287.474442, Test MSE of 128029267356.609009
Epoch 10: training loss 139471227904.000
Test Loss of 118862297873.943100, Test MSE of 118862299477.181305
Epoch 11: training loss 135537657554.824
Test Loss of 115510860704.066620, Test MSE of 115510861539.821106
Epoch 12: training loss 133389194721.882
Test Loss of 112889259759.596573, Test MSE of 112889260402.961945
Epoch 13: training loss 129679722736.941
Test Loss of 109780868458.651855, Test MSE of 109780867990.511398
Epoch 14: training loss 126689636201.412
Test Loss of 106912465530.818420, Test MSE of 106912468787.939545
Epoch 15: training loss 122629001743.059
Test Loss of 104230420061.209351, Test MSE of 104230421722.873322
Epoch 16: training loss 119264774053.647
Test Loss of 100980153365.555405, Test MSE of 100980153994.228973
Epoch 17: training loss 116421515444.706
Test Loss of 97776090127.870453, Test MSE of 97776090073.327011
Epoch 18: training loss 111859482654.118
Test Loss of 93701561587.268097, Test MSE of 93701561835.018417
Epoch 19: training loss 109192709135.059
Test Loss of 92641688197.714554, Test MSE of 92641688410.809433
Epoch 20: training loss 104570586172.235
Test Loss of 89476044934.069855, Test MSE of 89476045659.448441
Epoch 21: training loss 101359002428.235
Test Loss of 86918278484.859589, Test MSE of 86918277773.261627
Epoch 22: training loss 98845522153.412
Test Loss of 82389730989.272263, Test MSE of 82389729939.348145
Epoch 23: training loss 94992547026.824
Test Loss of 79550652834.790649, Test MSE of 79550654040.871582
Epoch 24: training loss 92277662464.000
Test Loss of 77467120858.159607, Test MSE of 77467122192.336029
Epoch 25: training loss 89187372649.412
Test Loss of 75063692350.297485, Test MSE of 75063693430.141541
Epoch 26: training loss 85680144406.588
Test Loss of 73450466908.261856, Test MSE of 73450467254.640961
Epoch 27: training loss 83221450985.412
Test Loss of 70536889735.550308, Test MSE of 70536889787.513901
Epoch 28: training loss 79323816658.824
Test Loss of 68014377741.205643, Test MSE of 68014378784.760063
Epoch 29: training loss 76113723030.588
Test Loss of 62883064630.895210, Test MSE of 62883064887.556046
Epoch 30: training loss 73144447232.000
Test Loss of 62784238296.619942, Test MSE of 62784236521.725090
Epoch 31: training loss 70094929859.765
Test Loss of 61331852916.896599, Test MSE of 61331853662.343414
Epoch 32: training loss 67195926648.471
Test Loss of 55166358173.875549, Test MSE of 55166358177.023972
Epoch 33: training loss 65574322755.765
Test Loss of 55854271916.028687, Test MSE of 55854272541.844604
Epoch 34: training loss 62334978176.000
Test Loss of 53404224684.206337, Test MSE of 53404225060.086372
Epoch 35: training loss 59750971128.471
Test Loss of 48864649223.579918, Test MSE of 48864649477.417046
Epoch 36: training loss 57407616240.941
Test Loss of 49693761026.250290, Test MSE of 49693761735.186615
Epoch 37: training loss 54820460717.176
Test Loss of 43576343415.324547, Test MSE of 43576343761.251503
Epoch 38: training loss 52666193859.765
Test Loss of 43564970612.185982, Test MSE of 43564970268.797089
Epoch 39: training loss 49593374765.176
Test Loss of 40373922436.767059, Test MSE of 40373921738.763184
Epoch 40: training loss 48111480628.706
Test Loss of 37918631224.908630, Test MSE of 37918631787.126053
Epoch 41: training loss 45636213594.353
Test Loss of 38337971749.781174, Test MSE of 38337970982.534378
Epoch 42: training loss 44294797778.824
Test Loss of 38533083013.773766, Test MSE of 38533083620.530655
Epoch 43: training loss 42282617434.353
Test Loss of 36314321639.779785, Test MSE of 36314320885.371689
Epoch 44: training loss 39905799258.353
Test Loss of 34536279330.642609, Test MSE of 34536280179.054070
Epoch 45: training loss 38475946977.882
Test Loss of 32602853254.721260, Test MSE of 32602853864.579414
Epoch 46: training loss 36807141985.882
Test Loss of 32097056527.811241, Test MSE of 32097057187.891830
Epoch 47: training loss 34708285281.882
Test Loss of 32807133942.939625, Test MSE of 32807134149.314987
Epoch 48: training loss 33612018228.706
Test Loss of 27466244692.918808, Test MSE of 27466245548.328365
Epoch 49: training loss 31801174889.412
Test Loss of 27482303212.990978, Test MSE of 27482303622.691460
Epoch 50: training loss 30606431947.294
Test Loss of 24172102158.330788, Test MSE of 24172102060.787914
Epoch 51: training loss 29264509903.059
Test Loss of 23336949525.022438, Test MSE of 23336949350.281815
Epoch 52: training loss 28226738277.647
Test Loss of 25127890712.812397, Test MSE of 25127891023.450451
Epoch 53: training loss 26728723173.647
Test Loss of 23016555020.909554, Test MSE of 23016554886.222031
Epoch 54: training loss 25807795663.059
Test Loss of 22880300068.241501, Test MSE of 22880300059.471958
Epoch 55: training loss 24675750332.235
Test Loss of 22401828944.062920, Test MSE of 22401828980.907906
Epoch 56: training loss 23771800003.765
Test Loss of 21982418533.263012, Test MSE of 21982418573.020409
Epoch 57: training loss 22815993404.235
Test Loss of 21367707843.182976, Test MSE of 21367708104.027523
Epoch 58: training loss 22192879649.882
Test Loss of 22057120116.363636, Test MSE of 22057120489.537727
Epoch 59: training loss 21114950945.882
Test Loss of 21179852397.790424, Test MSE of 21179852718.974449
Epoch 60: training loss 20567950561.882
Test Loss of 18632960308.881794, Test MSE of 18632960459.036205
Epoch 61: training loss 19905749744.941
Test Loss of 21676274295.502197, Test MSE of 21676274369.943867
Epoch 62: training loss 19025553509.647
Test Loss of 21127865601.480453, Test MSE of 21127865566.172058
Epoch 63: training loss 18306383450.353
Test Loss of 19385432878.367802, Test MSE of 19385433219.731335
Epoch 64: training loss 17727959499.294
Test Loss of 18394056635.780708, Test MSE of 18394057138.228817
Epoch 65: training loss 17588547489.882
Test Loss of 20072476201.097385, Test MSE of 20072476244.242153
Epoch 66: training loss 16518016357.647
Test Loss of 19568325140.015728, Test MSE of 19568324978.155529
Epoch 67: training loss 16265357920.000
Test Loss of 18255913558.813786, Test MSE of 18255913551.105705
Epoch 68: training loss 16145291506.824
Test Loss of 20753047326.497341, Test MSE of 20753047068.493603
Epoch 69: training loss 15368552730.353
Test Loss of 19340410648.812397, Test MSE of 19340410708.842888
Epoch 70: training loss 15051685492.706
Test Loss of 18806413247.096924, Test MSE of 18806413255.801163
Epoch 71: training loss 14536338556.235
Test Loss of 18949272105.334259, Test MSE of 18949272112.631588
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18949272112.631588, 'MSE - std': 0.0, 'R2 - mean': 0.8524399620785527, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005511 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918012416.000
Test Loss of 424556863012.359924, Test MSE of 424556862687.033936
Epoch 2: training loss 427897155102.118
Test Loss of 424539927573.081665, Test MSE of 424539925159.184326
Epoch 3: training loss 427868905712.941
Test Loss of 424517467371.688171, Test MSE of 424517461328.692566
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888730112.000
Test Loss of 424523591516.084229, Test MSE of 424523594023.028564
Epoch 2: training loss 427877163248.941
Test Loss of 424524371656.749451, Test MSE of 424524367294.265198
Epoch 3: training loss 427876535838.118
Test Loss of 424524444636.232239, Test MSE of 424524442098.217346
Epoch 4: training loss 427876078531.765
Test Loss of 424523822855.283813, Test MSE of 424523825574.252258
Epoch 5: training loss 420985608794.353
Test Loss of 403055323833.826538, Test MSE of 403055323433.273376
Epoch 6: training loss 375354996374.588
Test Loss of 337711932690.061523, Test MSE of 337711935043.841675
Epoch 7: training loss 296512896180.706
Test Loss of 252776252343.280121, Test MSE of 252776250229.872314
Epoch 8: training loss 217666268400.941
Test Loss of 184709373302.021759, Test MSE of 184709375107.671417
Epoch 9: training loss 158450284845.176
Test Loss of 138853365539.708527, Test MSE of 138853368150.682556
Epoch 10: training loss 138352821368.471
Test Loss of 130708806103.376358, Test MSE of 130708802267.528397
Epoch 11: training loss 133855185528.471
Test Loss of 127465443620.774460, Test MSE of 127465444006.642944
Epoch 12: training loss 131388569509.647
Test Loss of 124998007113.252838, Test MSE of 124998008818.670990
Epoch 13: training loss 127204540235.294
Test Loss of 121807066483.653015, Test MSE of 121807065495.394592
Epoch 14: training loss 123623549711.059
Test Loss of 118390216479.918579, Test MSE of 118390217531.873611
Epoch 15: training loss 120030212758.588
Test Loss of 114918625908.185989, Test MSE of 114918626484.461594
Epoch 16: training loss 117336393005.176
Test Loss of 111249382053.929214, Test MSE of 111249381250.467560
Epoch 17: training loss 113632071288.471
Test Loss of 108097073742.286377, Test MSE of 108097072372.426620
Epoch 18: training loss 110413191710.118
Test Loss of 103987017827.012726, Test MSE of 103987018422.781525
Epoch 19: training loss 105540455032.471
Test Loss of 101618301229.775620, Test MSE of 101618303822.602417
Epoch 20: training loss 103131469101.176
Test Loss of 97995360319.008102, Test MSE of 97995360386.124435
Epoch 21: training loss 98342284167.529
Test Loss of 94193739880.697662, Test MSE of 94193738942.278305
Epoch 22: training loss 95381823774.118
Test Loss of 89740942134.895218, Test MSE of 89740943511.952698
Epoch 23: training loss 92855125022.118
Test Loss of 88118127976.756882, Test MSE of 88118127373.290909
Epoch 24: training loss 89744477726.118
Test Loss of 85992387113.334259, Test MSE of 85992386035.379974
Epoch 25: training loss 86201860728.471
Test Loss of 81603115251.031235, Test MSE of 81603116782.507141
Epoch 26: training loss 83188242853.647
Test Loss of 78416177309.757111, Test MSE of 78416176293.451904
Epoch 27: training loss 79097238573.176
Test Loss of 76563574750.837845, Test MSE of 76563574142.069443
Epoch 28: training loss 76521812781.176
Test Loss of 71180021466.278046, Test MSE of 71180021615.368774
Epoch 29: training loss 73337771113.412
Test Loss of 70952531845.300018, Test MSE of 70952531161.636169
Epoch 30: training loss 70815912387.765
Test Loss of 66390717506.561188, Test MSE of 66390714144.388588
Epoch 31: training loss 69007815152.941
Test Loss of 64919900140.102707, Test MSE of 64919902334.275459
Epoch 32: training loss 65401308777.412
Test Loss of 60962746911.859360, Test MSE of 60962746173.771034
Epoch 33: training loss 61855313694.118
Test Loss of 58596094235.299561, Test MSE of 58596095675.881821
Epoch 34: training loss 59778380288.000
Test Loss of 56796822616.353455, Test MSE of 56796822157.194153
Epoch 35: training loss 57605000493.176
Test Loss of 53310735261.697891, Test MSE of 53310734500.527283
Epoch 36: training loss 54718856327.529
Test Loss of 52830884045.605370, Test MSE of 52830883739.432892
Epoch 37: training loss 51344270366.118
Test Loss of 50779973015.657646, Test MSE of 50779973121.889389
Epoch 38: training loss 49507020288.000
Test Loss of 48073413503.378212, Test MSE of 48073412789.840126
Epoch 39: training loss 47700432655.059
Test Loss of 47620205671.987045, Test MSE of 47620205267.221985
Epoch 40: training loss 45448615115.294
Test Loss of 45699283407.322693, Test MSE of 45699282019.179825
Epoch 41: training loss 43372070422.588
Test Loss of 43563558331.662270, Test MSE of 43563557974.796127
Epoch 42: training loss 41001706322.824
Test Loss of 43080798909.616470, Test MSE of 43080799569.466057
Epoch 43: training loss 38688076491.294
Test Loss of 40781788407.058060, Test MSE of 40781788230.351974
Epoch 44: training loss 36598460596.706
Test Loss of 40294171672.871620, Test MSE of 40294173302.476173
Epoch 45: training loss 35581085952.000
Test Loss of 35832583220.822578, Test MSE of 35832583719.745125
Epoch 46: training loss 33292167348.706
Test Loss of 36080117862.328941, Test MSE of 36080117679.085846
Epoch 47: training loss 32042868781.176
Test Loss of 32921048799.489243, Test MSE of 32921048977.333809
Epoch 48: training loss 30072410089.412
Test Loss of 34271747633.624798, Test MSE of 34271747740.503548
Epoch 49: training loss 28899792948.706
Test Loss of 33118933238.347443, Test MSE of 33118933708.045959
Epoch 50: training loss 27965427689.412
Test Loss of 30224087874.975712, Test MSE of 30224087901.533989
Epoch 51: training loss 26868344929.882
Test Loss of 31789535743.170944, Test MSE of 31789536326.323734
Epoch 52: training loss 25037803557.647
Test Loss of 28773107149.190838, Test MSE of 28773106860.496059
Epoch 53: training loss 23950760240.941
Test Loss of 31175345976.553318, Test MSE of 31175346139.407990
Epoch 54: training loss 23039139399.529
Test Loss of 29810252067.826973, Test MSE of 29810251265.200115
Epoch 55: training loss 22062875648.000
Test Loss of 30287917657.656258, Test MSE of 30287919434.457424
Epoch 56: training loss 20875317797.647
Test Loss of 29605592980.933613, Test MSE of 29605592743.061314
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24277432427.84645, 'MSE - std': 5328160315.214863, 'R2 - mean': 0.8205378429430525, 'R2 - std': 0.03190211913550006} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005608 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926849716.706
Test Loss of 447260078708.185974, Test MSE of 447260075812.986023
Epoch 2: training loss 421906275870.118
Test Loss of 447240995179.836243, Test MSE of 447241000559.004395
Epoch 3: training loss 421879272990.118
Test Loss of 447216051978.126282, Test MSE of 447216062886.889832
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900328598.588
Test Loss of 447224747754.859131, Test MSE of 447224744158.466553
Epoch 2: training loss 421888977257.412
Test Loss of 447226089476.737427, Test MSE of 447226091915.421082
Epoch 3: training loss 421888425261.176
Test Loss of 447225917912.797607, Test MSE of 447225915379.957153
Epoch 4: training loss 421888026864.941
Test Loss of 447225531912.882690, Test MSE of 447225538749.539673
Epoch 5: training loss 414855663134.118
Test Loss of 424883707629.227844, Test MSE of 424883712971.021118
Epoch 6: training loss 369044219422.118
Test Loss of 356911622493.150146, Test MSE of 356911619060.392395
Epoch 7: training loss 290206615552.000
Test Loss of 270818327662.619476, Test MSE of 270818323940.114838
Epoch 8: training loss 212465956683.294
Test Loss of 199729452979.727051, Test MSE of 199729454356.222168
Epoch 9: training loss 155586723358.118
Test Loss of 150057787651.612305, Test MSE of 150057788346.819916
Epoch 10: training loss 135724793524.706
Test Loss of 140736536077.620178, Test MSE of 140736533024.726501
Epoch 11: training loss 131731647608.471
Test Loss of 137535290175.422638, Test MSE of 137535286877.458832
Epoch 12: training loss 128933693018.353
Test Loss of 134860783574.310440, Test MSE of 134860784211.061813
Epoch 13: training loss 125109176530.824
Test Loss of 130646461244.580154, Test MSE of 130646465005.831940
Epoch 14: training loss 122387428773.647
Test Loss of 128143314785.769135, Test MSE of 128143312296.835785
Epoch 15: training loss 117561501003.294
Test Loss of 124660932537.885727, Test MSE of 124660936334.730698
Epoch 16: training loss 115391092555.294
Test Loss of 121372474076.883652, Test MSE of 121372474041.201797
Epoch 17: training loss 111240450469.647
Test Loss of 118925059853.916260, Test MSE of 118925058452.846771
Epoch 18: training loss 109104830192.941
Test Loss of 114585988971.244049, Test MSE of 114585989144.433090
Epoch 19: training loss 104912987105.882
Test Loss of 109784147286.280823, Test MSE of 109784146733.184601
Epoch 20: training loss 102345837447.529
Test Loss of 108993508619.429108, Test MSE of 108993510773.499893
Epoch 21: training loss 97939493647.059
Test Loss of 105151821503.274582, Test MSE of 105151822230.175949
Epoch 22: training loss 96012780303.059
Test Loss of 102334063022.160538, Test MSE of 102334063365.802658
Epoch 23: training loss 92209988638.118
Test Loss of 99483631250.268799, Test MSE of 99483632689.996063
Epoch 24: training loss 88777549974.588
Test Loss of 94640597229.346283, Test MSE of 94640596184.142120
Epoch 25: training loss 85686890661.647
Test Loss of 91714477174.199402, Test MSE of 91714476262.974609
Epoch 26: training loss 82560495254.588
Test Loss of 88521700162.738846, Test MSE of 88521699485.162643
Epoch 27: training loss 79214081551.059
Test Loss of 85359138571.073792, Test MSE of 85359138401.285248
Epoch 28: training loss 77338272225.882
Test Loss of 82940631782.358551, Test MSE of 82940631579.816238
Epoch 29: training loss 73208712583.529
Test Loss of 75935949997.627579, Test MSE of 75935949863.111084
Epoch 30: training loss 70392481370.353
Test Loss of 76479360522.777695, Test MSE of 76479359166.733688
Epoch 31: training loss 67368560911.059
Test Loss of 74115523998.290070, Test MSE of 74115524036.658112
Epoch 32: training loss 64680158840.471
Test Loss of 72334953833.704376, Test MSE of 72334954529.895508
Epoch 33: training loss 62356025720.471
Test Loss of 68491177871.248672, Test MSE of 68491178922.763115
Epoch 34: training loss 59949513743.059
Test Loss of 67010334148.663429, Test MSE of 67010334949.468529
Epoch 35: training loss 57541763900.235
Test Loss of 63899683955.356926, Test MSE of 63899683684.925217
Epoch 36: training loss 54386217532.235
Test Loss of 59754843482.544533, Test MSE of 59754843060.683411
Epoch 37: training loss 52679215149.176
Test Loss of 60586084964.552391, Test MSE of 60586085031.236908
Epoch 38: training loss 50584434266.353
Test Loss of 55277237968.329399, Test MSE of 55277238110.670547
Epoch 39: training loss 48466446102.588
Test Loss of 50437659519.851952, Test MSE of 50437659309.185173
Epoch 40: training loss 45717748720.941
Test Loss of 52786847476.807770, Test MSE of 52786847839.561127
Epoch 41: training loss 43530946959.059
Test Loss of 49837585936.936386, Test MSE of 49837586041.022301
Epoch 42: training loss 41918276886.588
Test Loss of 43360769407.970390, Test MSE of 43360770875.034523
Epoch 43: training loss 40061703830.588
Test Loss of 47657953084.343277, Test MSE of 47657953926.039162
Epoch 44: training loss 38008888169.412
Test Loss of 43586453692.550545, Test MSE of 43586454128.017281
Epoch 45: training loss 36176161024.000
Test Loss of 39070444856.434883, Test MSE of 39070445457.246918
Epoch 46: training loss 35101680677.647
Test Loss of 40303479100.935463, Test MSE of 40303478805.210144
Epoch 47: training loss 32996234089.412
Test Loss of 38849749919.592873, Test MSE of 38849749466.879051
Epoch 48: training loss 32001822027.294
Test Loss of 35420820005.544296, Test MSE of 35420819794.251228
Epoch 49: training loss 30099876513.882
Test Loss of 34293396674.472359, Test MSE of 34293396543.029167
Epoch 50: training loss 28582366682.353
Test Loss of 37109046737.928291, Test MSE of 37109046548.512199
Epoch 51: training loss 27490922063.059
Test Loss of 31436242157.583160, Test MSE of 31436242866.009392
Epoch 52: training loss 26267092592.941
Test Loss of 33338487400.816101, Test MSE of 33338487264.130783
Epoch 53: training loss 25426525492.706
Test Loss of 32225162152.120285, Test MSE of 32225161837.787411
Epoch 54: training loss 23990284276.706
Test Loss of 32868592725.984734, Test MSE of 32868593061.263893
Epoch 55: training loss 23345929103.059
Test Loss of 32095935493.448067, Test MSE of 32095935834.469959
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26883600230.054287, 'MSE - std': 5701790606.726102, 'R2 - mean': 0.8091383863935727, 'R2 - std': 0.030633184992815428} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004491 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109818157.176
Test Loss of 410763900633.262390, Test MSE of 410763898409.671570
Epoch 2: training loss 430088151521.882
Test Loss of 410745131817.343811, Test MSE of 410745130783.894714
Epoch 3: training loss 430060209573.647
Test Loss of 410721650489.928711, Test MSE of 410721648663.401184
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076311672.471
Test Loss of 410726310521.069885, Test MSE of 410726306230.905823
Epoch 2: training loss 430064600003.765
Test Loss of 410726442457.380859, Test MSE of 410726438185.127197
Epoch 3: training loss 430064210883.765
Test Loss of 410725242581.471558, Test MSE of 410725235463.420410
Epoch 4: training loss 430063882240.000
Test Loss of 410724987033.528931, Test MSE of 410724991522.902039
Epoch 5: training loss 423754277104.941
Test Loss of 390244936873.166138, Test MSE of 390244941171.138855
Epoch 6: training loss 379657942678.588
Test Loss of 325080373055.614990, Test MSE of 325080373140.170532
Epoch 7: training loss 300440579011.765
Test Loss of 239638021657.825073, Test MSE of 239638024868.030823
Epoch 8: training loss 220642477176.471
Test Loss of 168601433900.660797, Test MSE of 168601436709.615051
Epoch 9: training loss 162573026334.118
Test Loss of 121455281985.510406, Test MSE of 121455283131.550400
Epoch 10: training loss 142922555904.000
Test Loss of 113108393457.073578, Test MSE of 113108394007.471573
Epoch 11: training loss 136972764370.824
Test Loss of 109361904803.479874, Test MSE of 109361909594.061279
Epoch 12: training loss 134572853760.000
Test Loss of 107127504732.993988, Test MSE of 107127506382.968796
Epoch 13: training loss 131813058258.824
Test Loss of 104106285511.848221, Test MSE of 104106286409.430679
Epoch 14: training loss 127997605918.118
Test Loss of 101699889568.044418, Test MSE of 101699890722.875641
Epoch 15: training loss 122608244555.294
Test Loss of 98881107369.995377, Test MSE of 98881106603.812790
Epoch 16: training loss 120592938706.824
Test Loss of 95655381771.964828, Test MSE of 95655382810.215561
Epoch 17: training loss 117265600391.529
Test Loss of 92931435480.670059, Test MSE of 92931434636.376480
Epoch 18: training loss 112490144828.235
Test Loss of 90487319762.865341, Test MSE of 90487321319.054001
Epoch 19: training loss 110373135600.941
Test Loss of 87170489458.198975, Test MSE of 87170486151.517838
Epoch 20: training loss 106655997801.412
Test Loss of 84764894628.309113, Test MSE of 84764895372.350037
Epoch 21: training loss 102063903894.588
Test Loss of 80676696479.096710, Test MSE of 80676696176.950363
Epoch 22: training loss 99650452088.471
Test Loss of 78961730377.565948, Test MSE of 78961729919.847122
Epoch 23: training loss 95166212939.294
Test Loss of 76155822254.378525, Test MSE of 76155822651.579163
Epoch 24: training loss 92432599100.235
Test Loss of 73196933265.947250, Test MSE of 73196932736.797134
Epoch 25: training loss 89020660886.588
Test Loss of 70738115959.766769, Test MSE of 70738116881.959518
Epoch 26: training loss 86237644995.765
Test Loss of 67824281545.980568, Test MSE of 67824281621.735756
Epoch 27: training loss 82570403689.412
Test Loss of 67124219693.608513, Test MSE of 67124220398.725578
Epoch 28: training loss 79448458992.941
Test Loss of 64328560535.751968, Test MSE of 64328559923.336777
Epoch 29: training loss 77586247303.529
Test Loss of 59933325290.202682, Test MSE of 59933326411.252151
Epoch 30: training loss 74119083158.588
Test Loss of 59041250975.452103, Test MSE of 59041251239.681252
Epoch 31: training loss 70549507523.765
Test Loss of 58242471792.895882, Test MSE of 58242472045.162323
Epoch 32: training loss 67403464101.647
Test Loss of 54409956714.498840, Test MSE of 54409956930.228699
Epoch 33: training loss 65434076250.353
Test Loss of 53329749876.686722, Test MSE of 53329749122.234848
Epoch 34: training loss 62407298349.176
Test Loss of 49979871017.817680, Test MSE of 49979871964.640121
Epoch 35: training loss 59037687115.294
Test Loss of 46771176182.167519, Test MSE of 46771176547.161018
Epoch 36: training loss 56366530281.412
Test Loss of 46309037317.093941, Test MSE of 46309036953.670013
Epoch 37: training loss 54552298706.824
Test Loss of 45878302084.086998, Test MSE of 45878301347.942604
Epoch 38: training loss 52622605680.941
Test Loss of 44416882802.672836, Test MSE of 44416882233.473854
Epoch 39: training loss 49963111265.882
Test Loss of 40785121099.935219, Test MSE of 40785121267.217361
Epoch 40: training loss 47458808816.941
Test Loss of 39871317370.609901, Test MSE of 39871316920.565262
Epoch 41: training loss 45580413816.471
Test Loss of 34192405835.224434, Test MSE of 34192406029.707645
Epoch 42: training loss 43830582121.412
Test Loss of 36319538057.062469, Test MSE of 36319537614.218819
Epoch 43: training loss 41635405440.000
Test Loss of 32179726526.489590, Test MSE of 32179727368.011929
Epoch 44: training loss 39462310226.824
Test Loss of 33732625534.519203, Test MSE of 33732625897.108440
Epoch 45: training loss 37876930823.529
Test Loss of 31009931262.104580, Test MSE of 31009931050.430969
Epoch 46: training loss 35884121976.471
Test Loss of 30966832869.582600, Test MSE of 30966833042.009449
Epoch 47: training loss 34530974061.176
Test Loss of 27461669064.914391, Test MSE of 27461669269.692822
Epoch 48: training loss 33087366701.176
Test Loss of 25875057940.257290, Test MSE of 25875058174.800507
Epoch 49: training loss 31399985837.176
Test Loss of 28383995157.204998, Test MSE of 28383995076.451084
Epoch 50: training loss 30317346846.118
Test Loss of 24428460801.540028, Test MSE of 24428460692.098289
Epoch 51: training loss 29132698006.588
Test Loss of 26557034643.842667, Test MSE of 26557035183.713856
Epoch 52: training loss 27957038919.529
Test Loss of 24704097497.499306, Test MSE of 24704097424.612797
Epoch 53: training loss 26468842168.471
Test Loss of 20948747663.459511, Test MSE of 20948747851.156784
Epoch 54: training loss 25336609261.176
Test Loss of 22518745486.037945, Test MSE of 22518745018.266224
Epoch 55: training loss 24423719958.588
Test Loss of 21862665933.889866, Test MSE of 21862665785.794739
Epoch 56: training loss 23445754160.941
Test Loss of 20954933574.959740, Test MSE of 20954933836.975147
Epoch 57: training loss 22691641054.118
Test Loss of 21306933803.357704, Test MSE of 21306934319.145775
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25489433752.327156, 'MSE - std': 5496718347.802323, 'R2 - mean': 0.8128894814761366, 'R2 - std': 0.02731311333514257} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005457 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043675166.118
Test Loss of 431612855454.741333, Test MSE of 431612859507.579468
Epoch 2: training loss 424024338793.412
Test Loss of 431592534215.018982, Test MSE of 431592530349.049683
Epoch 3: training loss 423997617935.059
Test Loss of 431565157210.624695, Test MSE of 431565158055.605652
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011330258.824
Test Loss of 431570182162.954163, Test MSE of 431570175148.780212
Epoch 2: training loss 423999525586.824
Test Loss of 431572092077.904663, Test MSE of 431572084894.982788
Epoch 3: training loss 423999013948.235
Test Loss of 431571739590.663574, Test MSE of 431571737057.173279
Epoch 4: training loss 423998604709.647
Test Loss of 431570809743.696411, Test MSE of 431570805810.979736
Epoch 5: training loss 417357136474.353
Test Loss of 409932182408.114746, Test MSE of 409932175079.166199
Epoch 6: training loss 372658141184.000
Test Loss of 342066008153.084656, Test MSE of 342066008219.927795
Epoch 7: training loss 294028931312.941
Test Loss of 255197504014.926422, Test MSE of 255197503535.080048
Epoch 8: training loss 216212692961.882
Test Loss of 183955893030.500702, Test MSE of 183955892633.191284
Epoch 9: training loss 159148490450.824
Test Loss of 134037676619.579819, Test MSE of 134037676531.292374
Epoch 10: training loss 138160137216.000
Test Loss of 124451778311.700134, Test MSE of 124451780037.550323
Epoch 11: training loss 134291906078.118
Test Loss of 121189252013.075424, Test MSE of 121189250637.182861
Epoch 12: training loss 132758388916.706
Test Loss of 118067584004.264694, Test MSE of 118067585764.689590
Epoch 13: training loss 129091248549.647
Test Loss of 116094531842.724670, Test MSE of 116094530930.408020
Epoch 14: training loss 125554390046.118
Test Loss of 112417153770.794998, Test MSE of 112417154984.301971
Epoch 15: training loss 121935891034.353
Test Loss of 107862524370.273026, Test MSE of 107862526496.689102
Epoch 16: training loss 118429357387.294
Test Loss of 105619427686.234146, Test MSE of 105619427381.715012
Epoch 17: training loss 114879291331.765
Test Loss of 101509038565.227203, Test MSE of 101509037961.468613
Epoch 18: training loss 111763136361.412
Test Loss of 99367304533.649231, Test MSE of 99367304255.458267
Epoch 19: training loss 107875390976.000
Test Loss of 96014076411.024521, Test MSE of 96014075610.690414
Epoch 20: training loss 105172862388.706
Test Loss of 92889002922.232300, Test MSE of 92889001254.803711
Epoch 21: training loss 101841913916.235
Test Loss of 91413758865.591858, Test MSE of 91413760004.425842
Epoch 22: training loss 98493104850.824
Test Loss of 86525826496.740402, Test MSE of 86525826770.960693
Epoch 23: training loss 94027567706.353
Test Loss of 84061643919.577972, Test MSE of 84061645644.737015
Epoch 24: training loss 91465626112.000
Test Loss of 82382793499.128174, Test MSE of 82382793633.881454
Epoch 25: training loss 88735635320.471
Test Loss of 77287641908.716339, Test MSE of 77287641829.691437
Epoch 26: training loss 85647043312.941
Test Loss of 72934128685.963913, Test MSE of 72934129446.033295
Epoch 27: training loss 82379857603.765
Test Loss of 72205773851.009720, Test MSE of 72205773751.864624
Epoch 28: training loss 79534253402.353
Test Loss of 70884876983.618698, Test MSE of 70884877340.235458
Epoch 29: training loss 77019029850.353
Test Loss of 69346410331.098572, Test MSE of 69346411586.339111
Epoch 30: training loss 73220215875.765
Test Loss of 64405845014.271172, Test MSE of 64405844327.315239
Epoch 31: training loss 70293145419.294
Test Loss of 62964057150.074966, Test MSE of 62964057861.244934
Epoch 32: training loss 68660466100.706
Test Loss of 60063922878.252663, Test MSE of 60063922562.780075
Epoch 33: training loss 65191386601.412
Test Loss of 56897330701.504860, Test MSE of 56897330923.107910
Epoch 34: training loss 63129616240.941
Test Loss of 54714254432.192505, Test MSE of 54714254444.162407
Epoch 35: training loss 59800468645.647
Test Loss of 53368002784.607124, Test MSE of 53368002968.162125
Epoch 36: training loss 57545936052.706
Test Loss of 48398746947.642761, Test MSE of 48398746040.613632
Epoch 37: training loss 55732262727.529
Test Loss of 49432208455.552063, Test MSE of 49432208659.751144
Epoch 38: training loss 53375348973.176
Test Loss of 45218282225.428970, Test MSE of 45218281373.589149
Epoch 39: training loss 50640908897.882
Test Loss of 43191149224.455345, Test MSE of 43191149667.436996
Epoch 40: training loss 48026793072.941
Test Loss of 42605428936.914391, Test MSE of 42605428592.199852
Epoch 41: training loss 46174439363.765
Test Loss of 43765665066.054604, Test MSE of 43765666362.548813
Epoch 42: training loss 44268799376.941
Test Loss of 39660567273.847290, Test MSE of 39660567511.051247
Epoch 43: training loss 42689328941.176
Test Loss of 35658880845.830635, Test MSE of 35658880241.282227
Epoch 44: training loss 41113733993.412
Test Loss of 36117475936.429428, Test MSE of 36117474949.613190
Epoch 45: training loss 38558637854.118
Test Loss of 36012253246.548820, Test MSE of 36012253441.519974
Epoch 46: training loss 36737265076.706
Test Loss of 33633346496.977325, Test MSE of 33633346593.679577
Epoch 47: training loss 35678434590.118
Test Loss of 32609553214.193428, Test MSE of 32609552833.030079
Epoch 48: training loss 33828769581.176
Test Loss of 28787487516.075890, Test MSE of 28787486907.000999
Epoch 49: training loss 32496819599.059
Test Loss of 28631665237.056919, Test MSE of 28631665678.724079
Epoch 50: training loss 31203553916.235
Test Loss of 29080638312.840351, Test MSE of 29080638542.546753
Epoch 51: training loss 29405030377.412
Test Loss of 28995985620.286903, Test MSE of 28995985782.328457
Epoch 52: training loss 28345610198.588
Test Loss of 25594408350.622860, Test MSE of 25594408059.767345
Epoch 53: training loss 27219203354.353
Test Loss of 25695921306.002777, Test MSE of 25695921485.124176
Epoch 54: training loss 26152159096.471
Test Loss of 26111068449.999073, Test MSE of 26111068196.020222
Epoch 55: training loss 25057333963.294
Test Loss of 26972067038.237854, Test MSE of 26972067219.105675
Epoch 56: training loss 24283478663.529
Test Loss of 25669879923.620544, Test MSE of 25669880174.540188
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25525523036.769764, 'MSE - std': 4916944154.84452, 'R2 - mean': 0.8119708615418668, 'R2 - std': 0.02449857910943461} 
 

Saving model.....
Results After CV: {'MSE - mean': 25525523036.769764, 'MSE - std': 4916944154.84452, 'R2 - mean': 0.8119708615418668, 'R2 - std': 0.02449857910943461}
Train time: 89.76776573660027
Inference time: 0.06976038279972271
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 84 finished with value: 25525523036.769764 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005502 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525558753.882
Test Loss of 418112380960.214661, Test MSE of 418112386066.933838
Epoch 2: training loss 427504967318.588
Test Loss of 418094065838.811951, Test MSE of 418094074096.706360
Epoch 3: training loss 427477469906.824
Test Loss of 418070296183.265320, Test MSE of 418070295068.953735
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493512131.765
Test Loss of 418077290684.550537, Test MSE of 418077293695.758728
Epoch 2: training loss 427483359352.471
Test Loss of 418079133825.806152, Test MSE of 418079133164.181641
Epoch 3: training loss 423581196528.941
Test Loss of 405959064648.719849, Test MSE of 405959066261.598267
Epoch 4: training loss 396676448978.824
Test Loss of 364291656853.466553, Test MSE of 364291659987.848572
Epoch 5: training loss 330448162213.647
Test Loss of 277502644338.646301, Test MSE of 277502646313.535034
Epoch 6: training loss 247573849690.353
Test Loss of 201075577526.510284, Test MSE of 201075577317.746124
Epoch 7: training loss 178839121859.765
Test Loss of 141387329148.476532, Test MSE of 141387327075.768677
Epoch 8: training loss 145881431476.706
Test Loss of 123044307199.822342, Test MSE of 123044308765.685104
Epoch 9: training loss 137762816240.941
Test Loss of 117297176200.793900, Test MSE of 117297177071.373108
Epoch 10: training loss 134347755098.353
Test Loss of 114368838035.157074, Test MSE of 114368838678.744797
Epoch 11: training loss 132209757906.824
Test Loss of 111455186387.823273, Test MSE of 111455188569.534897
Epoch 12: training loss 128940008824.471
Test Loss of 108188824246.510300, Test MSE of 108188824664.420563
Epoch 13: training loss 124845664798.118
Test Loss of 104303956389.633118, Test MSE of 104303954144.163467
Epoch 14: training loss 119963195346.824
Test Loss of 101560809446.891510, Test MSE of 101560809387.906784
Epoch 15: training loss 116929021417.412
Test Loss of 99139774669.131622, Test MSE of 99139774471.485596
Epoch 16: training loss 113359470200.471
Test Loss of 94735514270.349289, Test MSE of 94735515177.285278
Epoch 17: training loss 109085783823.059
Test Loss of 91607311863.354156, Test MSE of 91607314057.527969
Epoch 18: training loss 104726413748.706
Test Loss of 89248167310.182739, Test MSE of 89248165538.228912
Epoch 19: training loss 101932492152.471
Test Loss of 83619738669.716400, Test MSE of 83619738435.380951
Epoch 20: training loss 97801420920.471
Test Loss of 82303208572.831833, Test MSE of 82303208805.153885
Epoch 21: training loss 94808877432.471
Test Loss of 80789706526.023590, Test MSE of 80789706901.192200
Epoch 22: training loss 91236016158.118
Test Loss of 76359895089.743240, Test MSE of 76359895386.555588
Epoch 23: training loss 86540809298.824
Test Loss of 73117869485.449921, Test MSE of 73117869656.367188
Epoch 24: training loss 83910192700.235
Test Loss of 71063608496.943787, Test MSE of 71063608211.622391
Epoch 25: training loss 80010746420.706
Test Loss of 69217701955.271805, Test MSE of 69217702705.642212
Epoch 26: training loss 77758850379.294
Test Loss of 66602408007.535507, Test MSE of 66602406855.690567
Epoch 27: training loss 73744631235.765
Test Loss of 62823448579.789963, Test MSE of 62823449731.441551
Epoch 28: training loss 70666915809.882
Test Loss of 60389016920.175804, Test MSE of 60389016609.103378
Epoch 29: training loss 67376322206.118
Test Loss of 57040587180.739304, Test MSE of 57040586527.449165
Epoch 30: training loss 65071409551.059
Test Loss of 55870165161.600739, Test MSE of 55870165229.538216
Epoch 31: training loss 61720682887.529
Test Loss of 52361532520.460793, Test MSE of 52361532792.482635
Epoch 32: training loss 59047189511.529
Test Loss of 52609010560.799446, Test MSE of 52609009455.966705
Epoch 33: training loss 57184173635.765
Test Loss of 47791384350.971085, Test MSE of 47791383966.684982
Epoch 34: training loss 54533355712.000
Test Loss of 47414365112.701363, Test MSE of 47414364975.621460
Epoch 35: training loss 51460051437.176
Test Loss of 44066605586.120750, Test MSE of 44066605803.947281
Epoch 36: training loss 49249983397.647
Test Loss of 40467745402.344666, Test MSE of 40467746313.486855
Epoch 37: training loss 47352081370.353
Test Loss of 39488889768.120285, Test MSE of 39488890125.439362
Epoch 38: training loss 45078129287.529
Test Loss of 38676080806.284523, Test MSE of 38676080961.380653
Epoch 39: training loss 43063420980.706
Test Loss of 35341009867.177422, Test MSE of 35341010288.754234
Epoch 40: training loss 40924013101.176
Test Loss of 36516150628.019432, Test MSE of 36516150672.927986
Epoch 41: training loss 38959284592.941
Test Loss of 33866574870.502892, Test MSE of 33866574520.010483
Epoch 42: training loss 37706596653.176
Test Loss of 34653417191.542908, Test MSE of 34653417302.761520
Epoch 43: training loss 35933070494.118
Test Loss of 32685950386.187370, Test MSE of 32685949703.516308
Epoch 44: training loss 34492104018.824
Test Loss of 28687762420.866989, Test MSE of 28687763316.970333
Epoch 45: training loss 32459351721.412
Test Loss of 27951589366.998844, Test MSE of 27951589957.449833
Epoch 46: training loss 31495135341.176
Test Loss of 27463872469.362942, Test MSE of 27463872267.140026
Epoch 47: training loss 29805651621.647
Test Loss of 24466990319.951885, Test MSE of 24466990202.749249
Epoch 48: training loss 28345093511.529
Test Loss of 25326660370.890587, Test MSE of 25326660332.422646
Epoch 49: training loss 27232146017.882
Test Loss of 26632383734.110573, Test MSE of 26632383987.156860
Epoch 50: training loss 26580275380.706
Test Loss of 24910377341.838539, Test MSE of 24910377795.774651
Epoch 51: training loss 25271994258.824
Test Loss of 23590377874.209576, Test MSE of 23590378005.181316
Epoch 52: training loss 24020734147.765
Test Loss of 24791170023.128384, Test MSE of 24791170242.788815
Epoch 53: training loss 22992025441.882
Test Loss of 23457033312.880871, Test MSE of 23457032809.034470
Epoch 54: training loss 22138439691.294
Test Loss of 24035682554.611149, Test MSE of 24035682684.670086
Epoch 55: training loss 21335870599.529
Test Loss of 23755199674.892437, Test MSE of 23755199618.934994
Epoch 56: training loss 20918155324.235
Test Loss of 21747172420.929909, Test MSE of 21747172230.918697
Epoch 57: training loss 20231838298.353
Test Loss of 21031542787.316216, Test MSE of 21031542683.091930
Epoch 58: training loss 19397735382.588
Test Loss of 21963585259.806614, Test MSE of 21963585988.550911
Epoch 59: training loss 18950265520.941
Test Loss of 23189605304.701366, Test MSE of 23189605110.681820
Epoch 60: training loss 18265143546.353
Test Loss of 20686243645.290771, Test MSE of 20686243838.340137
Epoch 61: training loss 17623898834.824
Test Loss of 20308535086.604671, Test MSE of 20308534922.695576
Epoch 62: training loss 16945162480.941
Test Loss of 20972530204.069397, Test MSE of 20972529850.106060
Epoch 63: training loss 16636734979.765
Test Loss of 21845691337.282444, Test MSE of 21845691471.973679
Epoch 64: training loss 16258175582.118
Test Loss of 21245642866.646309, Test MSE of 21245642902.595375
Epoch 65: training loss 15540982840.471
Test Loss of 22670066895.263474, Test MSE of 22670066899.908630
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22670066899.90863, 'MSE - std': 0.0, 'R2 - mean': 0.8234657293668628, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005463 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918230949.647
Test Loss of 424556677455.885254, Test MSE of 424556676230.203003
Epoch 2: training loss 427897406403.765
Test Loss of 424540159579.314392, Test MSE of 424540160901.280334
Epoch 3: training loss 427869421086.118
Test Loss of 424518124005.114990, Test MSE of 424518125849.768677
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890566324.706
Test Loss of 424524325479.868591, Test MSE of 424524328680.090393
Epoch 2: training loss 427877765842.824
Test Loss of 424525164645.381470, Test MSE of 424525166452.554504
Epoch 3: training loss 424030733010.824
Test Loss of 412758625888.051819, Test MSE of 412758639163.012573
Epoch 4: training loss 397299725854.118
Test Loss of 371697604486.247498, Test MSE of 371697607136.825439
Epoch 5: training loss 330844914025.412
Test Loss of 286173737701.884827, Test MSE of 286173742607.949219
Epoch 6: training loss 247239369065.412
Test Loss of 209697789712.758728, Test MSE of 209697792040.530121
Epoch 7: training loss 177583542272.000
Test Loss of 151116067053.820038, Test MSE of 151116066982.329437
Epoch 8: training loss 143631483120.941
Test Loss of 133579419163.832520, Test MSE of 133579419937.726700
Epoch 9: training loss 135075165093.647
Test Loss of 128613580174.656494, Test MSE of 128613579517.917084
Epoch 10: training loss 131265344722.824
Test Loss of 125427188625.380524, Test MSE of 125427189388.595795
Epoch 11: training loss 128338425313.882
Test Loss of 122064027306.903534, Test MSE of 122064029553.181778
Epoch 12: training loss 125341384372.706
Test Loss of 118487619797.659027, Test MSE of 118487620565.700668
Epoch 13: training loss 121221482556.235
Test Loss of 116435719302.780472, Test MSE of 116435717685.104523
Epoch 14: training loss 117300261195.294
Test Loss of 111615322080.259079, Test MSE of 111615324937.445145
Epoch 15: training loss 113576666714.353
Test Loss of 109182494174.719406, Test MSE of 109182495069.870850
Epoch 16: training loss 111530304512.000
Test Loss of 105652955349.659027, Test MSE of 105652954389.291260
Epoch 17: training loss 106467991250.824
Test Loss of 102206126924.687485, Test MSE of 102206127692.521652
Epoch 18: training loss 100892492589.176
Test Loss of 97289233536.148041, Test MSE of 97289235080.006073
Epoch 19: training loss 98158046388.706
Test Loss of 92827308845.894058, Test MSE of 92827307400.596680
Epoch 20: training loss 95397777814.588
Test Loss of 92026198758.358551, Test MSE of 92026200504.578156
Epoch 21: training loss 90497766098.824
Test Loss of 87693399775.489243, Test MSE of 87693398332.468414
Epoch 22: training loss 86838636664.471
Test Loss of 83888131890.394638, Test MSE of 83888132607.492767
Epoch 23: training loss 82735240688.941
Test Loss of 81280196390.077255, Test MSE of 81280197322.510880
Epoch 24: training loss 79944926162.824
Test Loss of 76635309447.550308, Test MSE of 76635307926.974533
Epoch 25: training loss 76362996118.588
Test Loss of 75896907863.642838, Test MSE of 75896907256.177231
Epoch 26: training loss 72339138755.765
Test Loss of 68716984381.113113, Test MSE of 68716984045.898193
Epoch 27: training loss 69063253880.471
Test Loss of 68366333431.354149, Test MSE of 68366333375.979645
Epoch 28: training loss 65568736240.941
Test Loss of 65441599664.233170, Test MSE of 65441601043.795799
Epoch 29: training loss 63022895721.412
Test Loss of 63954023987.993523, Test MSE of 63954022559.436890
Epoch 30: training loss 60838719427.765
Test Loss of 60047648420.744850, Test MSE of 60047649375.433853
Epoch 31: training loss 58174456711.529
Test Loss of 55575193801.341660, Test MSE of 55575195656.965164
Epoch 32: training loss 54332468796.235
Test Loss of 52492207510.236412, Test MSE of 52492208612.773727
Epoch 33: training loss 52481096493.176
Test Loss of 55719434825.548927, Test MSE of 55719434806.255249
Epoch 34: training loss 50152356487.529
Test Loss of 48948720292.981728, Test MSE of 48948720850.540497
Epoch 35: training loss 47682459587.765
Test Loss of 49318792754.572289, Test MSE of 49318793726.082489
Epoch 36: training loss 45328181293.176
Test Loss of 43227938575.811241, Test MSE of 43227938168.083611
Epoch 37: training loss 43434196562.824
Test Loss of 46097505786.196625, Test MSE of 46097503852.634377
Epoch 38: training loss 40884649306.353
Test Loss of 41909994712.264633, Test MSE of 41909995262.777756
Epoch 39: training loss 38304236378.353
Test Loss of 40640106966.902611, Test MSE of 40640106197.263039
Epoch 40: training loss 36568379851.294
Test Loss of 42240803279.796440, Test MSE of 42240804812.189568
Epoch 41: training loss 34710389541.647
Test Loss of 37315047270.032845, Test MSE of 37315046698.376793
Epoch 42: training loss 33485581086.118
Test Loss of 36396120713.030769, Test MSE of 36396120258.553543
Epoch 43: training loss 31852790791.529
Test Loss of 36358758508.961372, Test MSE of 36358759758.344742
Epoch 44: training loss 30407556442.353
Test Loss of 34526992428.058296, Test MSE of 34526992667.208122
Epoch 45: training loss 28657061782.588
Test Loss of 30459669193.933842, Test MSE of 30459668706.711388
Epoch 46: training loss 27328221722.353
Test Loss of 29623389884.432106, Test MSE of 29623388952.504021
Epoch 47: training loss 25614401317.647
Test Loss of 31730932771.767754, Test MSE of 31730932646.723442
Epoch 48: training loss 24882018247.529
Test Loss of 29201210990.974785, Test MSE of 29201211006.287792
Epoch 49: training loss 23552542979.765
Test Loss of 31623686292.755955, Test MSE of 31623686680.834972
Epoch 50: training loss 22380669778.824
Test Loss of 27821940523.762203, Test MSE of 27821940334.618206
Epoch 51: training loss 21539744210.824
Test Loss of 27645156911.019199, Test MSE of 27645156922.542065
Epoch 52: training loss 20774973440.000
Test Loss of 27526457772.976173, Test MSE of 27526458625.916084
Epoch 53: training loss 20144355738.353
Test Loss of 27372120770.353920, Test MSE of 27372121153.504704
Epoch 54: training loss 19325713822.118
Test Loss of 26570423881.785797, Test MSE of 26570424058.750538
Epoch 55: training loss 18305411535.059
Test Loss of 26889353107.986122, Test MSE of 26889353983.319607
Epoch 56: training loss 17627132224.000
Test Loss of 26523220823.820496, Test MSE of 26523219799.878510
Epoch 57: training loss 17323701138.824
Test Loss of 27220608163.442055, Test MSE of 27220608132.570755
Epoch 58: training loss 16526970669.176
Test Loss of 26621169713.743233, Test MSE of 26621170177.792133
Epoch 59: training loss 15783777972.706
Test Loss of 25303023926.776775, Test MSE of 25303023846.891788
Epoch 60: training loss 15585165888.000
Test Loss of 25315490825.474903, Test MSE of 25315491460.287479
Epoch 61: training loss 14879231055.059
Test Loss of 24978832907.488319, Test MSE of 24978832261.163761
Epoch 62: training loss 14565582211.765
Test Loss of 28835899184.973396, Test MSE of 28835899510.645298
Epoch 63: training loss 13818903552.000
Test Loss of 24094645378.990517, Test MSE of 24094645111.608810
Epoch 64: training loss 13790850654.118
Test Loss of 23881569910.317833, Test MSE of 23881569911.146168
Epoch 65: training loss 13142979911.529
Test Loss of 29636280440.331253, Test MSE of 29636280058.641739
Epoch 66: training loss 12650829436.235
Test Loss of 23049274785.369419, Test MSE of 23049274896.277977
Epoch 67: training loss 12490392685.176
Test Loss of 25160236471.872311, Test MSE of 25160236599.035400
Epoch 68: training loss 12099717285.647
Test Loss of 24839542689.014111, Test MSE of 24839542867.832844
Epoch 69: training loss 11757193543.529
Test Loss of 23483459268.485775, Test MSE of 23483459267.614380
Epoch 70: training loss 11684409569.882
Test Loss of 25460453616.188759, Test MSE of 25460453928.411232
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24065260414.15993, 'MSE - std': 1395193514.2513008, 'R2 - mean': 0.8208474961178747, 'R2 - std': 0.0026182332489880777} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005579 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926631785.412
Test Loss of 447257957057.880188, Test MSE of 447257965302.888000
Epoch 2: training loss 421905601234.824
Test Loss of 447239167054.404785, Test MSE of 447239176448.726013
Epoch 3: training loss 421878286336.000
Test Loss of 447214189515.651184, Test MSE of 447214191162.958496
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898139648.000
Test Loss of 447222063598.589844, Test MSE of 447222067811.528809
Epoch 2: training loss 421886899139.765
Test Loss of 447223249901.760803, Test MSE of 447223256046.343750
Epoch 3: training loss 418311652773.647
Test Loss of 435678493291.184814, Test MSE of 435678493178.754700
Epoch 4: training loss 392320941116.235
Test Loss of 394544412491.503113, Test MSE of 394544418506.664185
Epoch 5: training loss 326920583168.000
Test Loss of 307177339461.285217, Test MSE of 307177337172.820374
Epoch 6: training loss 244291818134.588
Test Loss of 228551622656.000000, Test MSE of 228551624284.871918
Epoch 7: training loss 175132258183.529
Test Loss of 165629987390.652802, Test MSE of 165629991502.758575
Epoch 8: training loss 142349785931.294
Test Loss of 145329351160.538513, Test MSE of 145329353420.522858
Epoch 9: training loss 134405316728.471
Test Loss of 139100378021.277802, Test MSE of 139100380363.033539
Epoch 10: training loss 130066847141.647
Test Loss of 135053020013.849640, Test MSE of 135053021442.880142
Epoch 11: training loss 127080870339.765
Test Loss of 131963047322.973862, Test MSE of 131963048972.118546
Epoch 12: training loss 122775483753.412
Test Loss of 128522149166.012497, Test MSE of 128522150388.905975
Epoch 13: training loss 119360357044.706
Test Loss of 124932211408.092529, Test MSE of 124932212191.795135
Epoch 14: training loss 114848072613.647
Test Loss of 121354203877.884796, Test MSE of 121354204479.359390
Epoch 15: training loss 112765006576.941
Test Loss of 119963338682.359467, Test MSE of 119963338430.114365
Epoch 16: training loss 108663913682.824
Test Loss of 114119082681.352768, Test MSE of 114119081866.403305
Epoch 17: training loss 105675086095.059
Test Loss of 112053597076.222992, Test MSE of 112053598893.931412
Epoch 18: training loss 100636103649.882
Test Loss of 107711349664.540359, Test MSE of 107711351063.165329
Epoch 19: training loss 98375539561.412
Test Loss of 103191497665.228775, Test MSE of 103191498978.698669
Epoch 20: training loss 93619385675.294
Test Loss of 99132094952.431183, Test MSE of 99132093909.729324
Epoch 21: training loss 89564278332.235
Test Loss of 95201576208.166550, Test MSE of 95201575537.296585
Epoch 22: training loss 86076983612.235
Test Loss of 96740119847.853806, Test MSE of 96740121940.225204
Epoch 23: training loss 83144732009.412
Test Loss of 89692635397.507294, Test MSE of 89692635058.631393
Epoch 24: training loss 79647392617.412
Test Loss of 86227865436.084198, Test MSE of 86227865748.862015
Epoch 25: training loss 76168943194.353
Test Loss of 82123449098.836914, Test MSE of 82123449173.193222
Epoch 26: training loss 72819195166.118
Test Loss of 77600161342.889664, Test MSE of 77600160207.590454
Epoch 27: training loss 68864019245.176
Test Loss of 77295528758.895218, Test MSE of 77295526252.343597
Epoch 28: training loss 66559359548.235
Test Loss of 70571991265.502655, Test MSE of 70571991222.598389
Epoch 29: training loss 63918588265.412
Test Loss of 67932744864.836456, Test MSE of 67932744592.224266
Epoch 30: training loss 60167880523.294
Test Loss of 69725757953.539673, Test MSE of 69725757327.088226
Epoch 31: training loss 58114116412.235
Test Loss of 66731380206.352997, Test MSE of 66731380095.014687
Epoch 32: training loss 55942133609.412
Test Loss of 58708989214.378906, Test MSE of 58708989166.067268
Epoch 33: training loss 52909786172.235
Test Loss of 59279338581.037239, Test MSE of 59279339039.924416
Epoch 34: training loss 50687708988.235
Test Loss of 55623024869.292618, Test MSE of 55623025085.649811
Epoch 35: training loss 47667356830.118
Test Loss of 51635779750.284523, Test MSE of 51635781042.606667
Epoch 36: training loss 46370435538.824
Test Loss of 52242157471.356003, Test MSE of 52242157790.502388
Epoch 37: training loss 43943357628.235
Test Loss of 50716684736.162849, Test MSE of 50716684294.273827
Epoch 38: training loss 41965818661.647
Test Loss of 47276035847.757576, Test MSE of 47276036395.034576
Epoch 39: training loss 39916533880.471
Test Loss of 41843193682.135551, Test MSE of 41843194488.632675
Epoch 40: training loss 38141746447.059
Test Loss of 42203430154.955353, Test MSE of 42203430212.590515
Epoch 41: training loss 36503142249.412
Test Loss of 44299686028.228546, Test MSE of 44299686660.710938
Epoch 42: training loss 34519374275.765
Test Loss of 38594195026.786957, Test MSE of 38594195515.852417
Epoch 43: training loss 33017019964.235
Test Loss of 38974925529.093681, Test MSE of 38974926000.207924
Epoch 44: training loss 31539324664.471
Test Loss of 36467024010.333565, Test MSE of 36467023652.849457
Epoch 45: training loss 30559994383.059
Test Loss of 36827722966.369652, Test MSE of 36827722825.434532
Epoch 46: training loss 29043213214.118
Test Loss of 33336379997.919964, Test MSE of 33336380212.327869
Epoch 47: training loss 27349539169.882
Test Loss of 34600415921.772842, Test MSE of 34600416279.643417
Epoch 48: training loss 26303169483.294
Test Loss of 34161452264.371964, Test MSE of 34161452400.476521
Epoch 49: training loss 25126162153.412
Test Loss of 30147109305.530418, Test MSE of 30147109662.719776
Epoch 50: training loss 24572198151.529
Test Loss of 32746283423.948185, Test MSE of 32746283496.909294
Epoch 51: training loss 23215560749.176
Test Loss of 30235934048.229469, Test MSE of 30235933905.443356
Epoch 52: training loss 22191002597.647
Test Loss of 27145908459.925053, Test MSE of 27145908276.634949
Epoch 53: training loss 21377956224.000
Test Loss of 28257191064.782791, Test MSE of 28257190946.475334
Epoch 54: training loss 20674116035.765
Test Loss of 26556924396.221142, Test MSE of 26556924427.891418
Epoch 55: training loss 20041245455.059
Test Loss of 26802303164.076797, Test MSE of 26802303659.950603
Epoch 56: training loss 19339654614.588
Test Loss of 26627659418.322460, Test MSE of 26627659908.559780
Epoch 57: training loss 18535544158.118
Test Loss of 25471832565.222298, Test MSE of 25471833367.895283
Epoch 58: training loss 17937166531.765
Test Loss of 25297534920.571827, Test MSE of 25297534824.447021
Epoch 59: training loss 17403206392.471
Test Loss of 23701300297.667362, Test MSE of 23701300250.330296
Epoch 60: training loss 16917231224.471
Test Loss of 24281417366.532501, Test MSE of 24281417662.195244
Epoch 61: training loss 16624658699.294
Test Loss of 25691412263.498497, Test MSE of 25691412428.770126
Epoch 62: training loss 16023819693.176
Test Loss of 22386065944.753181, Test MSE of 22386065664.097260
Epoch 63: training loss 15478522386.824
Test Loss of 21673516690.268795, Test MSE of 21673516346.925522
Epoch 64: training loss 15051835079.529
Test Loss of 24532456917.718250, Test MSE of 24532456982.564884
Epoch 65: training loss 14521448519.529
Test Loss of 25011527927.294933, Test MSE of 25011527678.975491
Epoch 66: training loss 14285180464.941
Test Loss of 23907055532.620865, Test MSE of 23907055103.020992
Epoch 67: training loss 13943993163.294
Test Loss of 23921810808.390469, Test MSE of 23921810661.635509
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24017443829.985123, 'MSE - std': 1141176065.6957757, 'R2 - mean': 0.8274830257871022, 'R2 - std': 0.00962447946019938} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005459 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110227275.294
Test Loss of 410764950801.414185, Test MSE of 410764947341.856995
Epoch 2: training loss 430088483297.882
Test Loss of 410746727608.329468, Test MSE of 410746722143.164917
Epoch 3: training loss 430060304022.588
Test Loss of 410723521541.212402, Test MSE of 410723524469.724060
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079090447.059
Test Loss of 410727227069.778809, Test MSE of 410727231129.158813
Epoch 2: training loss 430066703902.118
Test Loss of 410728616260.116638, Test MSE of 410728616736.264099
Epoch 3: training loss 426378115192.471
Test Loss of 399058554749.216125, Test MSE of 399058548705.213562
Epoch 4: training loss 400132600892.235
Test Loss of 358366033140.035156, Test MSE of 358366030955.698425
Epoch 5: training loss 334328629248.000
Test Loss of 272616923480.966217, Test MSE of 272616918193.708191
Epoch 6: training loss 250548931282.824
Test Loss of 195589722360.299866, Test MSE of 195589721451.243652
Epoch 7: training loss 182992167213.176
Test Loss of 135878942678.300781, Test MSE of 135878941590.506134
Epoch 8: training loss 148821862400.000
Test Loss of 116455359980.335037, Test MSE of 116455357852.636688
Epoch 9: training loss 141228536771.765
Test Loss of 111667664347.276260, Test MSE of 111667662998.209686
Epoch 10: training loss 138732272640.000
Test Loss of 108732516836.279495, Test MSE of 108732518012.975998
Epoch 11: training loss 133387901048.471
Test Loss of 106658273406.045349, Test MSE of 106658271517.754181
Epoch 12: training loss 129092502829.176
Test Loss of 103320970664.099960, Test MSE of 103320972686.929245
Epoch 13: training loss 127466464165.647
Test Loss of 100739261636.649704, Test MSE of 100739261588.182800
Epoch 14: training loss 122620075279.059
Test Loss of 97640191326.178619, Test MSE of 97640192213.492798
Epoch 15: training loss 119692591224.471
Test Loss of 94957807713.614075, Test MSE of 94957807680.719086
Epoch 16: training loss 114151627926.588
Test Loss of 91506295625.565948, Test MSE of 91506296682.532425
Epoch 17: training loss 109927948800.000
Test Loss of 88026207138.650620, Test MSE of 88026209490.085587
Epoch 18: training loss 107184490767.059
Test Loss of 84761466869.575195, Test MSE of 84761467180.619812
Epoch 19: training loss 103260609776.941
Test Loss of 80460587326.430359, Test MSE of 80460587727.898972
Epoch 20: training loss 98706506330.353
Test Loss of 78871141395.428040, Test MSE of 78871140671.825775
Epoch 21: training loss 94936639759.059
Test Loss of 75618352659.191116, Test MSE of 75618352785.039307
Epoch 22: training loss 91550090977.882
Test Loss of 73028322913.850998, Test MSE of 73028323686.596573
Epoch 23: training loss 88605878663.529
Test Loss of 68737923406.541412, Test MSE of 68737921899.568573
Epoch 24: training loss 84993042326.588
Test Loss of 66826368153.055069, Test MSE of 66826367742.911011
Epoch 25: training loss 81499565432.471
Test Loss of 63264367008.992134, Test MSE of 63264367328.928291
Epoch 26: training loss 78262338529.882
Test Loss of 62181221955.524292, Test MSE of 62181222433.489586
Epoch 27: training loss 74277084099.765
Test Loss of 58804365197.327164, Test MSE of 58804365021.995750
Epoch 28: training loss 70790697547.294
Test Loss of 56818955606.596947, Test MSE of 56818955293.121719
Epoch 29: training loss 68062466755.765
Test Loss of 52933595031.751968, Test MSE of 52933594234.657623
Epoch 30: training loss 65272793856.000
Test Loss of 50755675679.511337, Test MSE of 50755675770.991493
Epoch 31: training loss 62732562853.647
Test Loss of 47730422006.404442, Test MSE of 47730421088.750549
Epoch 32: training loss 59346966490.353
Test Loss of 46463502901.782509, Test MSE of 46463502606.208832
Epoch 33: training loss 57113384131.765
Test Loss of 43992195439.711243, Test MSE of 43992195530.956490
Epoch 34: training loss 54432702516.706
Test Loss of 42496507619.687180, Test MSE of 42496507900.880455
Epoch 35: training loss 52188231740.235
Test Loss of 39792372630.330406, Test MSE of 39792372020.680000
Epoch 36: training loss 49943713325.176
Test Loss of 39486574417.621475, Test MSE of 39486574416.755684
Epoch 37: training loss 47451721449.412
Test Loss of 36715267596.557152, Test MSE of 36715267215.190308
Epoch 38: training loss 45469689313.882
Test Loss of 36567744604.875519, Test MSE of 36567745041.733269
Epoch 39: training loss 43625521257.412
Test Loss of 37024189504.918091, Test MSE of 37024189500.374710
Epoch 40: training loss 40863542475.294
Test Loss of 32064895994.787598, Test MSE of 32064895950.583515
Epoch 41: training loss 40086441163.294
Test Loss of 31581890560.947708, Test MSE of 31581890453.870560
Epoch 42: training loss 37304521268.706
Test Loss of 32550795653.508560, Test MSE of 32550795685.411255
Epoch 43: training loss 36200577362.824
Test Loss of 26840093971.309578, Test MSE of 26840093603.851166
Epoch 44: training loss 34517892480.000
Test Loss of 28422177169.828781, Test MSE of 28422178078.181160
Epoch 45: training loss 32915192086.588
Test Loss of 28213516647.181862, Test MSE of 28213517230.397392
Epoch 46: training loss 30738653590.588
Test Loss of 26855117974.211941, Test MSE of 26855117898.912064
Epoch 47: training loss 30244713840.941
Test Loss of 26368301593.351227, Test MSE of 26368301884.949272
Epoch 48: training loss 28355520534.588
Test Loss of 24052799189.471542, Test MSE of 24052799383.765453
Epoch 49: training loss 27431482048.000
Test Loss of 25689828752.407219, Test MSE of 25689828885.827427
Epoch 50: training loss 26590602277.647
Test Loss of 24711188491.372513, Test MSE of 24711188676.121693
Epoch 51: training loss 24911936971.294
Test Loss of 21682109509.656639, Test MSE of 21682109391.692654
Epoch 52: training loss 24326147215.059
Test Loss of 21910472160.962517, Test MSE of 21910472318.251728
Epoch 53: training loss 23221377780.706
Test Loss of 20198856851.842667, Test MSE of 20198857154.120518
Epoch 54: training loss 22297004336.941
Test Loss of 22842465329.754742, Test MSE of 22842465330.513897
Epoch 55: training loss 21634354834.824
Test Loss of 21484805460.701527, Test MSE of 21484805387.015129
Epoch 56: training loss 21124909402.353
Test Loss of 19314787884.779270, Test MSE of 19314787802.962063
Epoch 57: training loss 20206712730.353
Test Loss of 19876088714.010181, Test MSE of 19876088703.288616
Epoch 58: training loss 19522146051.765
Test Loss of 19842321464.388710, Test MSE of 19842321414.195782
Epoch 59: training loss 18538072263.529
Test Loss of 21825896285.941692, Test MSE of 21825896202.825245
Epoch 60: training loss 18503827629.176
Test Loss of 19564448110.763535, Test MSE of 19564448099.974590
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22904194897.48249, 'MSE - std': 2166721409.605403, 'R2 - mean': 0.8302433727418937, 'R2 - std': 0.00960892811880339} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005361 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043071367.529
Test Loss of 431612292818.628418, Test MSE of 431612297260.673096
Epoch 2: training loss 424023628137.412
Test Loss of 431591854548.168457, Test MSE of 431591849192.753540
Epoch 3: training loss 423996575262.118
Test Loss of 431563987054.882019, Test MSE of 431563990383.420654
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011757447.529
Test Loss of 431565711628.201782, Test MSE of 431565705422.682678
Epoch 2: training loss 423998735058.824
Test Loss of 431568749908.701538, Test MSE of 431568746517.013428
Epoch 3: training loss 420556131629.176
Test Loss of 420177758123.653870, Test MSE of 420177763807.900452
Epoch 4: training loss 394961585453.176
Test Loss of 378883066150.737610, Test MSE of 378883066820.549255
Epoch 5: training loss 330269378560.000
Test Loss of 291504012197.493774, Test MSE of 291504011239.653809
Epoch 6: training loss 247805632331.294
Test Loss of 213027934479.044891, Test MSE of 213027931828.646820
Epoch 7: training loss 179486114695.529
Test Loss of 149584102088.677460, Test MSE of 149584102659.576599
Epoch 8: training loss 145657240877.176
Test Loss of 129545274318.245255, Test MSE of 129545277619.485107
Epoch 9: training loss 136429690759.529
Test Loss of 122718589403.750122, Test MSE of 122718585486.152939
Epoch 10: training loss 133323121001.412
Test Loss of 119479238144.236923, Test MSE of 119479236557.066254
Epoch 11: training loss 129499130096.941
Test Loss of 116254056091.661270, Test MSE of 116254054947.347519
Epoch 12: training loss 126277171772.235
Test Loss of 112694791977.817673, Test MSE of 112694790147.659729
Epoch 13: training loss 122276826428.235
Test Loss of 108756711656.188797, Test MSE of 108756711538.720230
Epoch 14: training loss 118780004894.118
Test Loss of 104970811863.011566, Test MSE of 104970812826.533691
Epoch 15: training loss 115349179587.765
Test Loss of 102122979676.757050, Test MSE of 102122979754.697647
Epoch 16: training loss 111962760779.294
Test Loss of 98700705689.647385, Test MSE of 98700704941.011368
Epoch 17: training loss 107296913106.824
Test Loss of 96850784830.311890, Test MSE of 96850784264.224060
Epoch 18: training loss 104307070328.471
Test Loss of 91875784322.073120, Test MSE of 91875783938.045807
Epoch 19: training loss 100805457543.529
Test Loss of 85874277308.238785, Test MSE of 85874277247.157700
Epoch 20: training loss 96612556272.941
Test Loss of 84684677214.770935, Test MSE of 84684676243.615662
Epoch 21: training loss 93330107452.235
Test Loss of 80036560437.308655, Test MSE of 80036560589.669418
Epoch 22: training loss 89649376195.765
Test Loss of 79058217383.626099, Test MSE of 79058216753.954086
Epoch 23: training loss 85787263066.353
Test Loss of 74202972681.240173, Test MSE of 74202972121.087112
Epoch 24: training loss 82433244446.118
Test Loss of 70502006031.518738, Test MSE of 70502003986.148087
Epoch 25: training loss 79969889505.882
Test Loss of 68080993796.975471, Test MSE of 68080995295.561340
Epoch 26: training loss 75409828171.294
Test Loss of 68048925007.489128, Test MSE of 68048924806.067131
Epoch 27: training loss 72725075937.882
Test Loss of 62481803048.869965, Test MSE of 62481803161.375519
Epoch 28: training loss 69581734023.529
Test Loss of 61168527668.005554, Test MSE of 61168526216.619896
Epoch 29: training loss 67018528662.588
Test Loss of 55935005178.076813, Test MSE of 55935004993.190643
Epoch 30: training loss 64130320564.706
Test Loss of 54441142128.895882, Test MSE of 54441141617.673401
Epoch 31: training loss 61329312346.353
Test Loss of 50514338019.450256, Test MSE of 50514336660.569221
Epoch 32: training loss 58508153479.529
Test Loss of 49443902598.100876, Test MSE of 49443903016.018135
Epoch 33: training loss 56500113920.000
Test Loss of 47816772526.970848, Test MSE of 47816771769.487930
Epoch 34: training loss 53495450721.882
Test Loss of 46644431313.325310, Test MSE of 46644430650.898705
Epoch 35: training loss 51650846283.294
Test Loss of 41513575880.322075, Test MSE of 41513576631.960663
Epoch 36: training loss 49478951484.235
Test Loss of 40232191644.608978, Test MSE of 40232191100.515297
Epoch 37: training loss 46784341571.765
Test Loss of 37835012340.509026, Test MSE of 37835012505.903648
Epoch 38: training loss 44485664602.353
Test Loss of 38726880476.342438, Test MSE of 38726880235.970589
Epoch 39: training loss 42955238249.412
Test Loss of 38162349420.394264, Test MSE of 38162349470.927620
Epoch 40: training loss 41041616602.353
Test Loss of 35510518235.750114, Test MSE of 35510518083.532944
Epoch 41: training loss 38964106081.882
Test Loss of 33717482991.178158, Test MSE of 33717482999.873741
Epoch 42: training loss 37214381786.353
Test Loss of 29486162886.663582, Test MSE of 29486162997.743927
Epoch 43: training loss 35447246546.824
Test Loss of 31965797060.886627, Test MSE of 31965796319.696163
Epoch 44: training loss 33946242767.059
Test Loss of 31942123315.294769, Test MSE of 31942122926.679401
Epoch 45: training loss 32880755305.412
Test Loss of 30413789981.971310, Test MSE of 30413789638.494038
Epoch 46: training loss 31480698044.235
Test Loss of 29916526101.086533, Test MSE of 29916526381.361969
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24306661194.258385, 'MSE - std': 3409309635.07909, 'R2 - mean': 0.8195111522467148, 'R2 - std': 0.023121146742807228} 
 

Saving model.....
Results After CV: {'MSE - mean': 24306661194.258385, 'MSE - std': 3409309635.07909, 'R2 - mean': 0.8195111522467148, 'R2 - std': 0.023121146742807228}
Train time: 95.15484615960013
Inference time: 0.06952330259809969
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 85 finished with value: 24306661194.258385 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005540 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525893180.235
Test Loss of 418111782305.606262, Test MSE of 418111777288.568665
Epoch 2: training loss 427506122752.000
Test Loss of 418093569217.524841, Test MSE of 418093571778.368286
Epoch 3: training loss 427479132641.882
Test Loss of 418069171580.654175, Test MSE of 418069172223.413574
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427495135232.000
Test Loss of 418074562640.536682, Test MSE of 418074553190.864197
Epoch 2: training loss 427481953581.176
Test Loss of 418075482191.115417, Test MSE of 418075483611.755188
Epoch 3: training loss 427481403994.353
Test Loss of 418074800218.011536, Test MSE of 418074795521.313782
Epoch 4: training loss 427480989575.529
Test Loss of 418074178141.446228, Test MSE of 418074175067.790710
Epoch 5: training loss 420888033159.529
Test Loss of 396976836146.572266, Test MSE of 396976835361.760071
Epoch 6: training loss 375725171531.294
Test Loss of 330041986788.700439, Test MSE of 330041985562.866760
Epoch 7: training loss 297481355986.824
Test Loss of 244957732536.168396, Test MSE of 244957732211.557404
Epoch 8: training loss 218527585581.176
Test Loss of 174335540383.888977, Test MSE of 174335539022.697540
Epoch 9: training loss 159996725669.647
Test Loss of 125824577485.783020, Test MSE of 125824576322.202759
Epoch 10: training loss 139130848346.353
Test Loss of 118056659061.962524, Test MSE of 118056659702.719925
Epoch 11: training loss 135303133394.824
Test Loss of 115435300788.674530, Test MSE of 115435299858.948929
Epoch 12: training loss 133060519168.000
Test Loss of 112325521329.358322, Test MSE of 112325521065.096603
Epoch 13: training loss 130427455367.529
Test Loss of 109073583619.197784, Test MSE of 109073582941.647827
Epoch 14: training loss 125262306936.471
Test Loss of 104888705711.877869, Test MSE of 104888704693.227631
Epoch 15: training loss 121369024240.941
Test Loss of 102568198272.858658, Test MSE of 102568200313.139313
Epoch 16: training loss 118329344813.176
Test Loss of 99683682590.615784, Test MSE of 99683682131.263885
Epoch 17: training loss 114127604630.588
Test Loss of 96269496091.654877, Test MSE of 96269494649.109375
Epoch 18: training loss 110551888376.471
Test Loss of 91642073369.404587, Test MSE of 91642074104.326477
Epoch 19: training loss 106043188811.294
Test Loss of 89385157511.905624, Test MSE of 89385159609.782883
Epoch 20: training loss 104538823122.824
Test Loss of 86697298066.387238, Test MSE of 86697297211.374802
Epoch 21: training loss 98978268882.824
Test Loss of 84370827236.049042, Test MSE of 84370827121.654053
Epoch 22: training loss 95442167958.588
Test Loss of 80732391591.232010, Test MSE of 80732391738.208466
Epoch 23: training loss 91606165323.294
Test Loss of 76662255582.837845, Test MSE of 76662255963.712753
Epoch 24: training loss 88080605048.471
Test Loss of 73862361754.085587, Test MSE of 73862361397.312775
Epoch 25: training loss 83148580833.882
Test Loss of 70844035404.569046, Test MSE of 70844034430.346252
Epoch 26: training loss 80530465035.294
Test Loss of 68085132581.485077, Test MSE of 68085134211.195290
Epoch 27: training loss 76916341805.176
Test Loss of 64254774455.576218, Test MSE of 64254772458.295067
Epoch 28: training loss 73274080504.471
Test Loss of 60490842839.672447, Test MSE of 60490843274.272453
Epoch 29: training loss 70768585298.824
Test Loss of 58683012433.543373, Test MSE of 58683012579.210403
Epoch 30: training loss 67278862569.412
Test Loss of 55935098672.499657, Test MSE of 55935098472.403259
Epoch 31: training loss 64364585080.471
Test Loss of 55061110770.735138, Test MSE of 55061111270.242760
Epoch 32: training loss 60720085421.176
Test Loss of 50178416559.463333, Test MSE of 50178417062.201294
Epoch 33: training loss 57739419271.529
Test Loss of 49864219411.838074, Test MSE of 49864219601.853760
Epoch 34: training loss 55482179248.941
Test Loss of 45928490531.886192, Test MSE of 45928489746.052322
Epoch 35: training loss 52519428544.000
Test Loss of 46161802100.955818, Test MSE of 46161802389.854362
Epoch 36: training loss 50154059429.647
Test Loss of 42002990208.148048, Test MSE of 42002989963.064743
Epoch 37: training loss 48008853936.941
Test Loss of 41088501324.628265, Test MSE of 41088500646.517799
Epoch 38: training loss 45270004246.588
Test Loss of 40106681040.092529, Test MSE of 40106681325.417053
Epoch 39: training loss 42965497652.706
Test Loss of 37823587356.661575, Test MSE of 37823586912.431732
Epoch 40: training loss 41201424587.294
Test Loss of 36120125558.436272, Test MSE of 36120125496.564278
Epoch 41: training loss 38907657456.941
Test Loss of 36062515628.265556, Test MSE of 36062515315.946037
Epoch 42: training loss 37324208730.353
Test Loss of 30431112635.188526, Test MSE of 30431113033.884644
Epoch 43: training loss 35093310223.059
Test Loss of 29461836956.572750, Test MSE of 29461836311.364639
Epoch 44: training loss 33605878957.176
Test Loss of 30820553386.192921, Test MSE of 30820553519.258926
Epoch 45: training loss 31597730590.118
Test Loss of 29124967415.235718, Test MSE of 29124967523.641865
Epoch 46: training loss 30372704000.000
Test Loss of 26185239443.749249, Test MSE of 26185239911.460526
Epoch 47: training loss 28933404039.529
Test Loss of 25769352536.649548, Test MSE of 25769352495.599075
Epoch 48: training loss 27781600015.059
Test Loss of 26036098259.527180, Test MSE of 26036098252.765049
Epoch 49: training loss 26743700728.471
Test Loss of 26824246474.052277, Test MSE of 26824247011.794086
Epoch 50: training loss 25350806467.765
Test Loss of 24024855138.657413, Test MSE of 24024855145.231735
Epoch 51: training loss 24366043309.176
Test Loss of 21143961018.241035, Test MSE of 21143961391.846416
Epoch 52: training loss 23144799382.588
Test Loss of 22479023738.581539, Test MSE of 22479023744.085213
Epoch 53: training loss 22328033076.706
Test Loss of 22413428446.541752, Test MSE of 22413428247.818249
Epoch 54: training loss 21508739636.706
Test Loss of 20909475495.587322, Test MSE of 20909475281.101254
Epoch 55: training loss 20669606016.000
Test Loss of 18426565874.320610, Test MSE of 18426565546.790306
Epoch 56: training loss 19743560161.882
Test Loss of 20592205130.910942, Test MSE of 20592204886.632244
Epoch 57: training loss 19140029281.882
Test Loss of 19555594853.499886, Test MSE of 19555594859.578033
Epoch 58: training loss 18561086720.000
Test Loss of 19223654448.795742, Test MSE of 19223654273.181381
Epoch 59: training loss 18001021948.235
Test Loss of 20044567354.685173, Test MSE of 20044567643.756187
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 20044567643.756187, 'MSE - std': 0.0, 'R2 - mean': 0.8439107769390243, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005397 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917843034.353
Test Loss of 424557330549.488770, Test MSE of 424557345119.256104
Epoch 2: training loss 427896966445.176
Test Loss of 424540837057.524841, Test MSE of 424540841887.616699
Epoch 3: training loss 427868758738.824
Test Loss of 424517747085.945862, Test MSE of 424517740832.207520
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887038223.059
Test Loss of 424524046046.067993, Test MSE of 424524040135.904419
Epoch 2: training loss 427877182042.353
Test Loss of 424524922444.154541, Test MSE of 424524929925.977173
Epoch 3: training loss 427876579689.412
Test Loss of 424524297631.237549, Test MSE of 424524298758.785461
Epoch 4: training loss 427876131297.882
Test Loss of 424524206437.203796, Test MSE of 424524216224.183350
Epoch 5: training loss 421025631533.176
Test Loss of 403314431590.210510, Test MSE of 403314429275.576355
Epoch 6: training loss 375714907919.059
Test Loss of 336817860978.468628, Test MSE of 336817852639.578674
Epoch 7: training loss 296171302671.059
Test Loss of 253434191635.127472, Test MSE of 253434188834.920197
Epoch 8: training loss 216662828212.706
Test Loss of 183724481261.938477, Test MSE of 183724483598.016327
Epoch 9: training loss 156461130450.824
Test Loss of 137111922038.495483, Test MSE of 137111916260.937485
Epoch 10: training loss 137139678991.059
Test Loss of 130153960647.209808, Test MSE of 130153960948.258331
Epoch 11: training loss 133137701827.765
Test Loss of 127511803310.397415, Test MSE of 127511801619.569901
Epoch 12: training loss 131315799311.059
Test Loss of 124634754961.143646, Test MSE of 124634754253.862320
Epoch 13: training loss 127657277018.353
Test Loss of 120907292220.994675, Test MSE of 120907290571.365204
Epoch 14: training loss 122709589202.824
Test Loss of 117435563274.007858, Test MSE of 117435561860.367371
Epoch 15: training loss 120191175649.882
Test Loss of 114429583869.749710, Test MSE of 114429583884.502640
Epoch 16: training loss 114883041520.941
Test Loss of 110692164688.773544, Test MSE of 110692162994.221603
Epoch 17: training loss 111652286885.647
Test Loss of 106865606259.949112, Test MSE of 106865605558.900009
Epoch 18: training loss 106378325684.706
Test Loss of 104515277495.457779, Test MSE of 104515278785.445953
Epoch 19: training loss 102813577607.529
Test Loss of 98725721074.972015, Test MSE of 98725722802.651154
Epoch 20: training loss 99747697332.706
Test Loss of 96475449695.045105, Test MSE of 96475450468.440964
Epoch 21: training loss 96304008975.059
Test Loss of 91040644408.434891, Test MSE of 91040645448.800919
Epoch 22: training loss 92673942377.412
Test Loss of 86602455820.258148, Test MSE of 86602455720.158142
Epoch 23: training loss 89385620871.529
Test Loss of 85923655895.080261, Test MSE of 85923655595.246124
Epoch 24: training loss 84903019444.706
Test Loss of 81981090027.451309, Test MSE of 81981090219.598083
Epoch 25: training loss 80886357895.529
Test Loss of 78965581650.135559, Test MSE of 78965582054.432404
Epoch 26: training loss 77241306955.294
Test Loss of 71585726577.698822, Test MSE of 71585726212.429153
Epoch 27: training loss 73442550136.471
Test Loss of 70431736720.433029, Test MSE of 70431737185.506973
Epoch 28: training loss 70418625867.294
Test Loss of 67648576975.085823, Test MSE of 67648577394.542015
Epoch 29: training loss 67070945114.353
Test Loss of 66340148176.862366, Test MSE of 66340149076.080467
Epoch 30: training loss 63662148864.000
Test Loss of 63335838141.320381, Test MSE of 63335838338.632645
Epoch 31: training loss 60065625344.000
Test Loss of 59683066415.966690, Test MSE of 59683067424.031082
Epoch 32: training loss 57560387079.529
Test Loss of 55718158835.327316, Test MSE of 55718157722.898804
Epoch 33: training loss 54788464323.765
Test Loss of 53608044205.272263, Test MSE of 53608042358.389168
Epoch 34: training loss 52179083279.059
Test Loss of 52287327320.353455, Test MSE of 52287329319.449173
Epoch 35: training loss 49043563158.588
Test Loss of 45898481977.145500, Test MSE of 45898481596.488449
Epoch 36: training loss 46497915851.294
Test Loss of 46081331986.890587, Test MSE of 46081332516.202782
Epoch 37: training loss 43690227757.176
Test Loss of 45238923783.224609, Test MSE of 45238924706.070251
Epoch 38: training loss 41640193453.176
Test Loss of 41436454605.960678, Test MSE of 41436453590.293991
Epoch 39: training loss 40026471725.176
Test Loss of 41739611581.083504, Test MSE of 41739611726.464485
Epoch 40: training loss 36750355975.529
Test Loss of 39638988474.063385, Test MSE of 39638988531.986481
Epoch 41: training loss 34842675892.706
Test Loss of 40312194085.662735, Test MSE of 40312193971.465363
Epoch 42: training loss 33263020928.000
Test Loss of 34218936604.720795, Test MSE of 34218937115.084171
Epoch 43: training loss 31966377487.059
Test Loss of 34069730563.375435, Test MSE of 34069730471.730831
Epoch 44: training loss 29420956988.235
Test Loss of 35377329355.473511, Test MSE of 35377328639.282188
Epoch 45: training loss 28161418880.000
Test Loss of 31379127215.700207, Test MSE of 31379127026.779591
Epoch 46: training loss 26692838083.765
Test Loss of 30866008491.318066, Test MSE of 30866009149.536736
Epoch 47: training loss 25525049965.176
Test Loss of 29399906375.535507, Test MSE of 29399907255.125916
Epoch 48: training loss 24335034571.294
Test Loss of 33678135020.280361, Test MSE of 33678134938.496181
Epoch 49: training loss 22876092641.882
Test Loss of 24683456448.991905, Test MSE of 24683456693.568359
Epoch 50: training loss 22066712094.118
Test Loss of 29127199691.177422, Test MSE of 29127198935.662136
Epoch 51: training loss 21104623247.059
Test Loss of 28999399918.826740, Test MSE of 28999400194.623772
Epoch 52: training loss 19882834703.059
Test Loss of 30836801070.545456, Test MSE of 30836801243.650986
Epoch 53: training loss 19685320911.059
Test Loss of 28698880727.672451, Test MSE of 28698880340.049576
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24371723991.90288, 'MSE - std': 4327156348.146694, 'R2 - mean': 0.8195099126559953, 'R2 - std': 0.024400864283029067} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005611 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926431563.294
Test Loss of 447257425008.040710, Test MSE of 447257432569.564087
Epoch 2: training loss 421903927898.353
Test Loss of 447238016589.101990, Test MSE of 447238016415.712585
Epoch 3: training loss 421875838735.059
Test Loss of 447213220650.104065, Test MSE of 447213224129.368774
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896586300.235
Test Loss of 447222308642.287292, Test MSE of 447222314214.340637
Epoch 2: training loss 421885936820.706
Test Loss of 447222840100.892883, Test MSE of 447222842639.958801
Epoch 3: training loss 421885425061.647
Test Loss of 447223098985.526733, Test MSE of 447223090652.315491
Epoch 4: training loss 421885086780.235
Test Loss of 447223077276.631958, Test MSE of 447223083062.460449
Epoch 5: training loss 415198673980.235
Test Loss of 425770834753.317627, Test MSE of 425770836640.707397
Epoch 6: training loss 369797575981.176
Test Loss of 358044353053.964355, Test MSE of 358044350629.580383
Epoch 7: training loss 291202340261.647
Test Loss of 271629150659.479065, Test MSE of 271629150450.070496
Epoch 8: training loss 212024397703.529
Test Loss of 198835632622.116119, Test MSE of 198835631275.802521
Epoch 9: training loss 153259018300.235
Test Loss of 148610516475.144104, Test MSE of 148610513277.482697
Epoch 10: training loss 134447924766.118
Test Loss of 140057258056.483002, Test MSE of 140057258107.751221
Epoch 11: training loss 130095395779.765
Test Loss of 135603660959.652100, Test MSE of 135603658316.482300
Epoch 12: training loss 127845235591.529
Test Loss of 133420254574.915573, Test MSE of 133420256309.259781
Epoch 13: training loss 124060624564.706
Test Loss of 129518996606.726807, Test MSE of 129518995036.447357
Epoch 14: training loss 120159951570.824
Test Loss of 125146545842.483459, Test MSE of 125146546906.100418
Epoch 15: training loss 116325371422.118
Test Loss of 122383898862.293777, Test MSE of 122383901871.687012
Epoch 16: training loss 110956097385.412
Test Loss of 118707131426.346512, Test MSE of 118707130802.194427
Epoch 17: training loss 108811360496.941
Test Loss of 113300635780.648621, Test MSE of 113300635215.268341
Epoch 18: training loss 105165811651.765
Test Loss of 110226604801.362015, Test MSE of 110226605062.351913
Epoch 19: training loss 100324367209.412
Test Loss of 106559688886.865601, Test MSE of 106559690688.080276
Epoch 20: training loss 96487034533.647
Test Loss of 101582542347.251450, Test MSE of 101582544572.119324
Epoch 21: training loss 92617151939.765
Test Loss of 98399266799.418915, Test MSE of 98399266609.935196
Epoch 22: training loss 88256404615.529
Test Loss of 93365944056.597733, Test MSE of 93365944808.814987
Epoch 23: training loss 84941168384.000
Test Loss of 90920499705.486008, Test MSE of 90920501557.295288
Epoch 24: training loss 80782477296.941
Test Loss of 84242877117.616470, Test MSE of 84242876806.472885
Epoch 25: training loss 76703506672.941
Test Loss of 80960124933.211197, Test MSE of 80960124090.054382
Epoch 26: training loss 74244944730.353
Test Loss of 74437302576.618088, Test MSE of 74437303054.001144
Epoch 27: training loss 70302616064.000
Test Loss of 73577808008.438583, Test MSE of 73577808461.313461
Epoch 28: training loss 67265284412.235
Test Loss of 68264920718.952576, Test MSE of 68264920177.230469
Epoch 29: training loss 63662994688.000
Test Loss of 69239919237.240799, Test MSE of 69239920315.238953
Epoch 30: training loss 60049177720.471
Test Loss of 61314684040.675453, Test MSE of 61314684548.582100
Epoch 31: training loss 57610063706.353
Test Loss of 58754307890.868378, Test MSE of 58754307763.615273
Epoch 32: training loss 54502788216.471
Test Loss of 57857148608.222069, Test MSE of 57857148946.306755
Epoch 33: training loss 51426776816.941
Test Loss of 49955643935.859360, Test MSE of 49955644896.143707
Epoch 34: training loss 48653322066.824
Test Loss of 52124347969.732132, Test MSE of 52124348243.848213
Epoch 35: training loss 45835291105.882
Test Loss of 48770627598.686096, Test MSE of 48770627401.842491
Epoch 36: training loss 44421239348.706
Test Loss of 46554514331.802917, Test MSE of 46554513758.567070
Epoch 37: training loss 41777017155.765
Test Loss of 43433949624.109184, Test MSE of 43433949993.578423
Epoch 38: training loss 38980348905.412
Test Loss of 42033260067.886192, Test MSE of 42033259359.909668
Epoch 39: training loss 37480658861.176
Test Loss of 42092447241.356468, Test MSE of 42092447769.288666
Epoch 40: training loss 35950267120.941
Test Loss of 36785687872.014801, Test MSE of 36785687864.718536
Epoch 41: training loss 33879866307.765
Test Loss of 35088365737.126999, Test MSE of 35088366349.449120
Epoch 42: training loss 31621277545.412
Test Loss of 32865616458.496414, Test MSE of 32865616379.609001
Epoch 43: training loss 30165458544.941
Test Loss of 32844545205.681240, Test MSE of 32844545047.780766
Epoch 44: training loss 28166432097.882
Test Loss of 34302717664.910480, Test MSE of 34302717479.517174
Epoch 45: training loss 27535187704.471
Test Loss of 30042516941.190838, Test MSE of 30042516897.592804
Epoch 46: training loss 25971084276.706
Test Loss of 28552127870.075409, Test MSE of 28552127531.308132
Epoch 47: training loss 24476252702.118
Test Loss of 27119193525.977329, Test MSE of 27119193972.073154
Epoch 48: training loss 23448954191.059
Test Loss of 30744792897.554478, Test MSE of 30744793134.411816
Epoch 49: training loss 22690638456.471
Test Loss of 26498299433.097385, Test MSE of 26498299492.344540
Epoch 50: training loss 21644782554.353
Test Loss of 26602709575.653946, Test MSE of 26602709216.607628
Epoch 51: training loss 20705681400.471
Test Loss of 25777843714.250290, Test MSE of 25777843562.105396
Epoch 52: training loss 19728446776.471
Test Loss of 22817253440.429333, Test MSE of 22817253719.993568
Epoch 53: training loss 19208260995.765
Test Loss of 22222700534.051353, Test MSE of 22222700069.213589
Epoch 54: training loss 18458365206.588
Test Loss of 23826006768.780941, Test MSE of 23826007136.916225
Epoch 55: training loss 17894013018.353
Test Loss of 23712659178.385380, Test MSE of 23712659361.273739
Epoch 56: training loss 17268666334.118
Test Loss of 22564865552.936386, Test MSE of 22564865957.776016
Epoch 57: training loss 16640405458.824
Test Loss of 21344507183.433727, Test MSE of 21344507329.032154
Epoch 58: training loss 16227998531.765
Test Loss of 22631915740.291466, Test MSE of 22631915914.209003
Epoch 59: training loss 15848276732.235
Test Loss of 21115094888.401573, Test MSE of 21115095222.115108
Epoch 60: training loss 15126385377.882
Test Loss of 22846890852.848484, Test MSE of 22846890664.314819
Epoch 61: training loss 14644444126.118
Test Loss of 22949535195.403191, Test MSE of 22949534893.552349
Epoch 62: training loss 14137898488.471
Test Loss of 23033651629.686790, Test MSE of 23033651689.488674
Epoch 63: training loss 14040334607.059
Test Loss of 19581255354.063381, Test MSE of 19581255406.481464
Epoch 64: training loss 13748893040.941
Test Loss of 21997098736.544067, Test MSE of 21997098764.256138
Epoch 65: training loss 13152407337.412
Test Loss of 22983769588.511681, Test MSE of 22983770038.413582
Epoch 66: training loss 12895638825.412
Test Loss of 23115835386.788803, Test MSE of 23115835308.315083
Epoch 67: training loss 12511199887.059
Test Loss of 21070827018.303955, Test MSE of 21070827044.305496
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23271425009.370422, 'MSE - std': 3860591976.557667, 'R2 - mean': 0.8329175734099378, 'R2 - std': 0.02750391824737252} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005355 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110363407.059
Test Loss of 410764412487.315125, Test MSE of 410764412381.409973
Epoch 2: training loss 430089534524.235
Test Loss of 410745995688.573792, Test MSE of 410745991725.455627
Epoch 3: training loss 430062044340.706
Test Loss of 410722399968.844055, Test MSE of 410722396891.285400
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079049968.941
Test Loss of 410728574378.943054, Test MSE of 410728570485.405640
Epoch 2: training loss 430065893857.882
Test Loss of 410729212877.297546, Test MSE of 410729207628.471558
Epoch 3: training loss 430065397760.000
Test Loss of 410729719424.177673, Test MSE of 410729715339.370789
Epoch 4: training loss 430065065984.000
Test Loss of 410729420372.583069, Test MSE of 410729419145.844177
Epoch 5: training loss 423422547365.647
Test Loss of 389741092465.962036, Test MSE of 389741094165.818237
Epoch 6: training loss 378304218895.059
Test Loss of 323718606485.975037, Test MSE of 323718612256.431213
Epoch 7: training loss 300159366204.235
Test Loss of 238977763683.391022, Test MSE of 238977763192.928375
Epoch 8: training loss 221447848960.000
Test Loss of 168654405632.000000, Test MSE of 168654405359.839966
Epoch 9: training loss 160389554959.059
Test Loss of 119926986104.714478, Test MSE of 119926986661.580933
Epoch 10: training loss 141024700867.765
Test Loss of 112654557028.575653, Test MSE of 112654558167.229965
Epoch 11: training loss 136661557940.706
Test Loss of 109911088860.105499, Test MSE of 109911087359.842575
Epoch 12: training loss 135256453210.353
Test Loss of 108161320955.261459, Test MSE of 108161320159.216049
Epoch 13: training loss 130558549564.235
Test Loss of 104863529169.443771, Test MSE of 104863529565.014389
Epoch 14: training loss 126961651591.529
Test Loss of 101701241320.070343, Test MSE of 101701238413.176575
Epoch 15: training loss 123639866488.471
Test Loss of 98035025642.321152, Test MSE of 98035026186.077042
Epoch 16: training loss 119889191152.941
Test Loss of 94662209321.817673, Test MSE of 94662210162.224701
Epoch 17: training loss 116149274443.294
Test Loss of 91979804817.947250, Test MSE of 91979803661.270935
Epoch 18: training loss 111277837552.941
Test Loss of 87686448578.635818, Test MSE of 87686448405.244705
Epoch 19: training loss 108667437477.647
Test Loss of 84911957905.117996, Test MSE of 84911958841.176453
Epoch 20: training loss 103692671759.059
Test Loss of 83152701655.130035, Test MSE of 83152701287.912460
Epoch 21: training loss 99715563971.765
Test Loss of 78071937123.509491, Test MSE of 78071936631.021744
Epoch 22: training loss 96751034895.059
Test Loss of 76946151606.907913, Test MSE of 76946153084.974228
Epoch 23: training loss 92848768572.235
Test Loss of 72854566420.138824, Test MSE of 72854566129.401810
Epoch 24: training loss 87997841995.294
Test Loss of 69755768394.158264, Test MSE of 69755770060.108810
Epoch 25: training loss 84998705227.294
Test Loss of 65966860120.255440, Test MSE of 65966859509.909966
Epoch 26: training loss 81232761584.941
Test Loss of 64596889878.626564, Test MSE of 64596889308.357643
Epoch 27: training loss 78271243655.529
Test Loss of 61958431686.663582, Test MSE of 61958431273.018318
Epoch 28: training loss 74263306691.765
Test Loss of 60087234633.921333, Test MSE of 60087235828.334961
Epoch 29: training loss 70709734550.588
Test Loss of 54959237968.199905, Test MSE of 54959236671.434448
Epoch 30: training loss 67631926362.353
Test Loss of 53872902352.496063, Test MSE of 53872901911.889328
Epoch 31: training loss 64303357620.706
Test Loss of 51652240514.310043, Test MSE of 51652239467.564499
Epoch 32: training loss 61880553456.941
Test Loss of 48343362789.819527, Test MSE of 48343362199.299927
Epoch 33: training loss 58838012491.294
Test Loss of 45677723204.472000, Test MSE of 45677723065.779266
Epoch 34: training loss 55755349398.588
Test Loss of 45737361972.834801, Test MSE of 45737361832.428841
Epoch 35: training loss 52929779538.824
Test Loss of 43424120710.693199, Test MSE of 43424120365.184097
Epoch 36: training loss 49889931392.000
Test Loss of 39750805614.881996, Test MSE of 39750804935.202675
Epoch 37: training loss 47726545189.647
Test Loss of 39202041031.492828, Test MSE of 39202040919.230721
Epoch 38: training loss 45720304368.941
Test Loss of 37482363891.205925, Test MSE of 37482364081.720917
Epoch 39: training loss 43104119446.588
Test Loss of 34143000476.490513, Test MSE of 34143000372.255516
Epoch 40: training loss 41042342083.765
Test Loss of 34458799407.740860, Test MSE of 34458799795.569656
Epoch 41: training loss 39159838622.118
Test Loss of 32917644132.101803, Test MSE of 32917643932.134907
Epoch 42: training loss 36535613349.647
Test Loss of 31684076515.094864, Test MSE of 31684076232.472191
Epoch 43: training loss 34931769035.294
Test Loss of 29416330027.239243, Test MSE of 29416329838.672260
Epoch 44: training loss 33171436619.294
Test Loss of 27354576896.000000, Test MSE of 27354577012.686417
Epoch 45: training loss 31605440165.647
Test Loss of 26828420861.275337, Test MSE of 26828420838.568062
Epoch 46: training loss 30230822317.176
Test Loss of 25997150686.593243, Test MSE of 25997150991.078789
Epoch 47: training loss 28918247574.588
Test Loss of 24414272725.708469, Test MSE of 24414272561.586857
Epoch 48: training loss 27673482800.941
Test Loss of 22826724631.100418, Test MSE of 22826724535.134274
Epoch 49: training loss 26006154100.706
Test Loss of 23225135121.058769, Test MSE of 23225134806.219704
Epoch 50: training loss 24926543984.941
Test Loss of 24398389229.993523, Test MSE of 24398389390.441772
Epoch 51: training loss 24095153950.118
Test Loss of 22476519978.409996, Test MSE of 22476520198.866871
Epoch 52: training loss 22798999149.176
Test Loss of 20966853214.534012, Test MSE of 20966852982.357174
Epoch 53: training loss 21974817980.235
Test Loss of 22789087188.405369, Test MSE of 22789087394.224895
Epoch 54: training loss 21273804939.294
Test Loss of 21929562379.254047, Test MSE of 21929562072.661087
Epoch 55: training loss 20865771922.824
Test Loss of 19999954944.947708, Test MSE of 19999954854.599659
Epoch 56: training loss 20014238204.235
Test Loss of 20118647752.559002, Test MSE of 20118647522.406376
Epoch 57: training loss 19108719326.118
Test Loss of 19698436731.913002, Test MSE of 19698436761.024231
Epoch 58: training loss 18305160414.118
Test Loss of 19518726723.050438, Test MSE of 19518726787.967716
Epoch 59: training loss 17835190159.059
Test Loss of 18429883579.172604, Test MSE of 18429884198.170616
Epoch 60: training loss 17454076969.412
Test Loss of 19657100077.134659, Test MSE of 19657100307.052567
Epoch 61: training loss 17034139190.588
Test Loss of 19671272768.799629, Test MSE of 19671273045.831898
Epoch 62: training loss 16435666055.529
Test Loss of 18409393413.093937, Test MSE of 18409393661.175648
Epoch 63: training loss 15893697411.765
Test Loss of 17633976549.345673, Test MSE of 17633976724.811104
Epoch 64: training loss 15507111156.706
Test Loss of 18729400051.798241, Test MSE of 18729400196.278648
Epoch 65: training loss 15322808854.588
Test Loss of 19390720452.057381, Test MSE of 19390720238.428379
Epoch 66: training loss 14619123011.765
Test Loss of 18290943322.387783, Test MSE of 18290943293.216820
Epoch 67: training loss 14118219896.471
Test Loss of 18306668547.790836, Test MSE of 18306668930.888050
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22030235989.74983, 'MSE - std': 3974893502.268489, 'R2 - mean': 0.8369145602059572, 'R2 - std': 0.024804774733129042} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005473 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043314959.059
Test Loss of 431613008816.866272, Test MSE of 431613009153.124146
Epoch 2: training loss 424024033882.353
Test Loss of 431593853729.762146, Test MSE of 431593853424.157898
Epoch 3: training loss 423997059072.000
Test Loss of 431567239957.441895, Test MSE of 431567238243.287231
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009475975.529
Test Loss of 431569016950.463684, Test MSE of 431569020055.516785
Epoch 2: training loss 424001116280.471
Test Loss of 431570755140.471985, Test MSE of 431570759672.344116
Epoch 3: training loss 424000573199.059
Test Loss of 431572346367.289246, Test MSE of 431572345722.838745
Epoch 4: training loss 424000207329.882
Test Loss of 431572276813.001404, Test MSE of 431572276649.057007
Epoch 5: training loss 417001965568.000
Test Loss of 408840113052.016663, Test MSE of 408840111764.924316
Epoch 6: training loss 371025413541.647
Test Loss of 340943933529.084656, Test MSE of 340943935932.374573
Epoch 7: training loss 292696533654.588
Test Loss of 254121017427.398438, Test MSE of 254121017444.171173
Epoch 8: training loss 215455123456.000
Test Loss of 182972289749.945404, Test MSE of 182972290895.407349
Epoch 9: training loss 157650811873.882
Test Loss of 133003758818.028687, Test MSE of 133003761472.001358
Epoch 10: training loss 137817566268.235
Test Loss of 124415236680.736694, Test MSE of 124415236556.013962
Epoch 11: training loss 135210174072.471
Test Loss of 121362656876.275803, Test MSE of 121362658089.767059
Epoch 12: training loss 131596971971.765
Test Loss of 118013998130.702454, Test MSE of 118013998173.417450
Epoch 13: training loss 128172789067.294
Test Loss of 114361244714.173065, Test MSE of 114361240317.078491
Epoch 14: training loss 123622468788.706
Test Loss of 111309359694.422958, Test MSE of 111309361450.073807
Epoch 15: training loss 119284987512.471
Test Loss of 108309927436.557144, Test MSE of 108309926723.749298
Epoch 16: training loss 118192343461.647
Test Loss of 103093027805.408600, Test MSE of 103093029773.700104
Epoch 17: training loss 112689388634.353
Test Loss of 100526150900.035172, Test MSE of 100526150544.289246
Epoch 18: training loss 109854361419.294
Test Loss of 96761451248.481262, Test MSE of 96761452261.550644
Epoch 19: training loss 104528973793.882
Test Loss of 93244376730.713562, Test MSE of 93244377787.311813
Epoch 20: training loss 102056677059.765
Test Loss of 89975383365.538177, Test MSE of 89975384206.552124
Epoch 21: training loss 97389228303.059
Test Loss of 87858293883.676071, Test MSE of 87858293676.313980
Epoch 22: training loss 92728988611.765
Test Loss of 82213489787.676071, Test MSE of 82213490143.167145
Epoch 23: training loss 89896690838.588
Test Loss of 79591108363.490982, Test MSE of 79591107554.953522
Epoch 24: training loss 86622084171.294
Test Loss of 75198431057.621475, Test MSE of 75198430637.420670
Epoch 25: training loss 82563895393.882
Test Loss of 70082661873.073578, Test MSE of 70082659714.164795
Epoch 26: training loss 78986788427.294
Test Loss of 71086110383.563171, Test MSE of 71086111787.892990
Epoch 27: training loss 75978277647.059
Test Loss of 64593272365.726974, Test MSE of 64593273181.965744
Epoch 28: training loss 73145651041.882
Test Loss of 62521259255.826004, Test MSE of 62521259792.103195
Epoch 29: training loss 69003008783.059
Test Loss of 60105035948.483109, Test MSE of 60105036603.785110
Epoch 30: training loss 66289311201.882
Test Loss of 56941583838.119392, Test MSE of 56941583465.757362
Epoch 31: training loss 63103153076.706
Test Loss of 53784234254.097176, Test MSE of 53784233982.056992
Epoch 32: training loss 59752251083.294
Test Loss of 48294649112.995834, Test MSE of 48294648816.818970
Epoch 33: training loss 56826028393.412
Test Loss of 49121200866.265617, Test MSE of 49121199755.572548
Epoch 34: training loss 53964053955.765
Test Loss of 47691742528.799629, Test MSE of 47691742257.877556
Epoch 35: training loss 51950539324.235
Test Loss of 45613035935.096718, Test MSE of 45613035948.444916
Epoch 36: training loss 49336802002.824
Test Loss of 39813432569.721428, Test MSE of 39813432082.854568
Epoch 37: training loss 46059214072.471
Test Loss of 39155641416.499771, Test MSE of 39155640719.789452
Epoch 38: training loss 44010418959.059
Test Loss of 36296238875.602036, Test MSE of 36296238660.189903
Epoch 39: training loss 42231445805.176
Test Loss of 36490922477.756592, Test MSE of 36490923437.626198
Epoch 40: training loss 40027389349.647
Test Loss of 31876548888.521980, Test MSE of 31876549268.461750
Epoch 41: training loss 38324930710.588
Test Loss of 32046030714.372974, Test MSE of 32046030287.483902
Epoch 42: training loss 35900899900.235
Test Loss of 31109182041.795464, Test MSE of 31109182054.390690
Epoch 43: training loss 34395308461.176
Test Loss of 27079802920.751503, Test MSE of 27079802821.878635
Epoch 44: training loss 32887151924.706
Test Loss of 28015602990.793152, Test MSE of 28015602886.547455
Epoch 45: training loss 31282632621.176
Test Loss of 30653816011.283665, Test MSE of 30653816450.219986
Epoch 46: training loss 29315657306.353
Test Loss of 26411640844.320221, Test MSE of 26411640610.658855
Epoch 47: training loss 28336538270.118
Test Loss of 23998852159.970383, Test MSE of 23998852688.092861
Epoch 48: training loss 26947882155.294
Test Loss of 25258755810.739471, Test MSE of 25258755875.608585
Epoch 49: training loss 25950465889.882
Test Loss of 25789992951.470615, Test MSE of 25789993495.281170
Epoch 50: training loss 24908828815.059
Test Loss of 23393463654.708004, Test MSE of 23393463402.704380
Epoch 51: training loss 23968365635.765
Test Loss of 23853551875.672375, Test MSE of 23853551771.098179
Epoch 52: training loss 22795760086.588
Test Loss of 23339415561.477093, Test MSE of 23339415869.427727
Epoch 53: training loss 22225865920.000
Test Loss of 21314681152.325775, Test MSE of 21314681021.577995
Epoch 54: training loss 21484824097.882
Test Loss of 23510414160.199909, Test MSE of 23510414149.802982
Epoch 55: training loss 20754329652.706
Test Loss of 20435092876.616383, Test MSE of 20435092657.063839
Epoch 56: training loss 19855912963.765
Test Loss of 20147630156.764462, Test MSE of 20147630302.103302
Epoch 57: training loss 19310305332.706
Test Loss of 20506181124.027763, Test MSE of 20506181071.013737
Epoch 58: training loss 18480128877.176
Test Loss of 20735336640.385006, Test MSE of 20735337028.022221
Epoch 59: training loss 18143417664.000
Test Loss of 21101274392.048126, Test MSE of 21101274532.574986
Epoch 60: training loss 17398286002.824
Test Loss of 22523924574.770939, Test MSE of 22523924581.228561
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22128973708.045574, 'MSE - std': 3560732962.6859136, 'R2 - mean': 0.835889747160022, 'R2 - std': 0.022280539642600652} 
 

Saving model.....
Results After CV: {'MSE - mean': 22128973708.045574, 'MSE - std': 3560732962.6859136, 'R2 - mean': 0.835889747160022, 'R2 - std': 0.022280539642600652}
Train time: 94.72436820019793
Inference time: 0.06978423039981863
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 86 finished with value: 22128973708.045574 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005261 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526100630.588
Test Loss of 418113179474.609314, Test MSE of 418113180986.415710
Epoch 2: training loss 427506578010.353
Test Loss of 418096571548.572754, Test MSE of 418096572380.557251
Epoch 3: training loss 427480414087.529
Test Loss of 418074923945.067749, Test MSE of 418074925098.011230
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496458962.824
Test Loss of 418080901994.059692, Test MSE of 418080910645.520386
Epoch 2: training loss 427487397888.000
Test Loss of 418082399619.760376, Test MSE of 418082406319.824585
Epoch 3: training loss 427486968048.941
Test Loss of 418081763012.485779, Test MSE of 418081765162.364807
Epoch 4: training loss 427486608926.118
Test Loss of 418080717589.733032, Test MSE of 418080720759.426086
Epoch 5: training loss 427486378947.765
Test Loss of 418081127943.935242, Test MSE of 418081127973.958252
Epoch 6: training loss 427486216432.941
Test Loss of 418079372752.507080, Test MSE of 418079376366.826599
Epoch 7: training loss 427486125959.529
Test Loss of 418080141136.714294, Test MSE of 418080144758.148071
Epoch 8: training loss 427486022475.294
Test Loss of 418077536941.035400, Test MSE of 418077540169.467957
Epoch 9: training loss 427485954288.941
Test Loss of 418078837776.817932, Test MSE of 418078836253.781860
Epoch 10: training loss 427485890560.000
Test Loss of 418076941773.190857, Test MSE of 418076941712.382141
Epoch 11: training loss 414080987617.882
Test Loss of 375354983900.824402, Test MSE of 375354985385.390808
Epoch 12: training loss 332034382546.824
Test Loss of 266081892421.640533, Test MSE of 266081896382.961731
Epoch 13: training loss 225625409987.765
Test Loss of 172464825826.509369, Test MSE of 172464825541.590210
Epoch 14: training loss 161013706842.353
Test Loss of 130958690495.866760, Test MSE of 130958691291.262146
Epoch 15: training loss 141926848587.294
Test Loss of 120299501414.032852, Test MSE of 120299500297.131744
Epoch 16: training loss 137798921336.471
Test Loss of 116452680705.184357, Test MSE of 116452679093.984161
Epoch 17: training loss 133868872975.059
Test Loss of 114457939402.348373, Test MSE of 114457940511.543350
Epoch 18: training loss 131014905856.000
Test Loss of 112036423788.487625, Test MSE of 112036420848.334106
Epoch 19: training loss 127587008451.765
Test Loss of 109267197677.227844, Test MSE of 109267198282.106461
Epoch 20: training loss 125708619715.765
Test Loss of 106711380527.966690, Test MSE of 106711379022.697189
Epoch 21: training loss 121711786209.882
Test Loss of 102282901486.471436, Test MSE of 102282901305.139481
Epoch 22: training loss 116494330081.882
Test Loss of 98157761960.712463, Test MSE of 98157758999.128448
Epoch 23: training loss 113271700178.824
Test Loss of 94264988967.853806, Test MSE of 94264988465.740311
Epoch 24: training loss 109467364668.235
Test Loss of 91557837944.094376, Test MSE of 91557835487.157074
Epoch 25: training loss 104966405850.353
Test Loss of 87803339695.700211, Test MSE of 87803340255.318390
Epoch 26: training loss 101196125846.588
Test Loss of 84265344650.688873, Test MSE of 84265346262.168228
Epoch 27: training loss 97899880824.471
Test Loss of 80137191439.159836, Test MSE of 80137189892.222641
Epoch 28: training loss 94143838042.353
Test Loss of 79146685049.870926, Test MSE of 79146685248.022736
Epoch 29: training loss 91119268050.824
Test Loss of 77182217759.622482, Test MSE of 77182217313.934357
Epoch 30: training loss 88005238053.647
Test Loss of 74053134173.505432, Test MSE of 74053134597.881760
Epoch 31: training loss 83549750832.941
Test Loss of 67613583836.113808, Test MSE of 67613584446.484329
Epoch 32: training loss 79696271977.412
Test Loss of 67271958584.849411, Test MSE of 67271958004.944801
Epoch 33: training loss 75648916525.176
Test Loss of 64617317600.792046, Test MSE of 64617316645.601028
Epoch 34: training loss 72342584816.941
Test Loss of 59779417019.070091, Test MSE of 59779416959.931236
Epoch 35: training loss 67748761050.353
Test Loss of 56714637960.083275, Test MSE of 56714637099.026833
Epoch 36: training loss 66117528779.294
Test Loss of 57013267783.594727, Test MSE of 57013267366.691582
Epoch 37: training loss 62579694848.000
Test Loss of 53244136965.092758, Test MSE of 53244136690.122421
Epoch 38: training loss 60100635143.529
Test Loss of 49066134207.511452, Test MSE of 49066134536.628349
Epoch 39: training loss 57334315700.706
Test Loss of 48397715569.225075, Test MSE of 48397716169.389328
Epoch 40: training loss 55039520026.353
Test Loss of 45360905722.670364, Test MSE of 45360905357.980583
Epoch 41: training loss 52135918042.353
Test Loss of 44061415845.159378, Test MSE of 44061416096.576164
Epoch 42: training loss 48624851584.000
Test Loss of 41922155293.786720, Test MSE of 41922154864.357582
Epoch 43: training loss 46624809765.647
Test Loss of 39597739657.741386, Test MSE of 39597739488.469452
Epoch 44: training loss 43691188630.588
Test Loss of 37278430702.826744, Test MSE of 37278430695.838875
Epoch 45: training loss 41525997402.353
Test Loss of 32718171554.553783, Test MSE of 32718171192.133652
Epoch 46: training loss 40206387072.000
Test Loss of 34329712632.893826, Test MSE of 34329712600.367172
Epoch 47: training loss 38103376406.588
Test Loss of 32685579760.247974, Test MSE of 32685580050.700134
Epoch 48: training loss 36028816534.588
Test Loss of 30882347809.102940, Test MSE of 30882347776.531841
Epoch 49: training loss 34346875489.882
Test Loss of 29277638915.612305, Test MSE of 29277638810.364750
Epoch 50: training loss 33322836163.765
Test Loss of 27335860369.913486, Test MSE of 27335860611.906757
Epoch 51: training loss 31438434650.353
Test Loss of 28268862303.637287, Test MSE of 28268862386.157349
Epoch 52: training loss 30016143457.882
Test Loss of 25560694041.641453, Test MSE of 25560693708.633259
Epoch 53: training loss 28859986575.059
Test Loss of 27154537100.820724, Test MSE of 27154537311.661854
Epoch 54: training loss 27118921773.176
Test Loss of 22518020555.058987, Test MSE of 22518020527.734829
Epoch 55: training loss 26035430241.882
Test Loss of 23859228241.839462, Test MSE of 23859227714.830463
Epoch 56: training loss 24445943401.412
Test Loss of 21404320888.094379, Test MSE of 21404320902.726368
Epoch 57: training loss 24044315256.471
Test Loss of 21733655772.528336, Test MSE of 21733656217.205929
Epoch 58: training loss 22718524272.941
Test Loss of 22187304713.178810, Test MSE of 22187305040.788319
Epoch 59: training loss 22037180685.176
Test Loss of 19607560539.728893, Test MSE of 19607560441.726089
Epoch 60: training loss 21299514733.176
Test Loss of 19936637548.606060, Test MSE of 19936637947.031944
Epoch 61: training loss 20389423649.882
Test Loss of 21655354810.241035, Test MSE of 21655354853.554672
Epoch 62: training loss 19540495721.412
Test Loss of 20376565408.481148, Test MSE of 20376565585.943905
Epoch 63: training loss 19020788600.471
Test Loss of 19439357437.986584, Test MSE of 19439357672.716293
Epoch 64: training loss 18344102915.765
Test Loss of 19427338624.681007, Test MSE of 19427338963.943264
Epoch 65: training loss 17672044894.118
Test Loss of 18556857120.155449, Test MSE of 18556857017.840553
Epoch 66: training loss 17117179738.353
Test Loss of 19471237716.445061, Test MSE of 19471237416.757614
Epoch 67: training loss 16722052434.824
Test Loss of 16866384177.091835, Test MSE of 16866384122.068110
Epoch 68: training loss 15989229936.941
Test Loss of 18461338841.212120, Test MSE of 18461338836.193134
Epoch 69: training loss 15553658432.000
Test Loss of 17719885914.485310, Test MSE of 17719886096.656372
Epoch 70: training loss 15303160564.706
Test Loss of 18127906443.636364, Test MSE of 18127906442.569260
Epoch 71: training loss 14823553697.882
Test Loss of 19889882296.997456, Test MSE of 19889882138.881649
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19889882138.88165, 'MSE - std': 0.0, 'R2 - mean': 0.8451153297487319, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005391 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918209987.765
Test Loss of 424556972663.502197, Test MSE of 424556972202.221252
Epoch 2: training loss 427897781067.294
Test Loss of 424541442844.839233, Test MSE of 424541443298.321655
Epoch 3: training loss 427870075000.471
Test Loss of 424519765853.979187, Test MSE of 424519766892.233032
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890847744.000
Test Loss of 424526912029.490662, Test MSE of 424526911527.246582
Epoch 2: training loss 427879097765.647
Test Loss of 424526767162.033752, Test MSE of 424526765044.759338
Epoch 3: training loss 427878565285.647
Test Loss of 424526879458.568604, Test MSE of 424526881031.116211
Epoch 4: training loss 427878165805.176
Test Loss of 424527058973.845947, Test MSE of 424527057820.702271
Epoch 5: training loss 427877916190.118
Test Loss of 424526688755.327332, Test MSE of 424526686218.275146
Epoch 6: training loss 427877768734.118
Test Loss of 424526504085.703430, Test MSE of 424526506247.978821
Epoch 7: training loss 427877635252.706
Test Loss of 424526313776.618103, Test MSE of 424526326354.434998
Epoch 8: training loss 427877562488.471
Test Loss of 424525725089.606262, Test MSE of 424525720211.169128
Epoch 9: training loss 427877444186.353
Test Loss of 424526054088.512634, Test MSE of 424526052890.853149
Epoch 10: training loss 427877390215.529
Test Loss of 424525295042.768433, Test MSE of 424525285654.173218
Epoch 11: training loss 414852811836.235
Test Loss of 383565650942.815613, Test MSE of 383565653644.857971
Epoch 12: training loss 333524555414.588
Test Loss of 275947416079.278259, Test MSE of 275947419026.393005
Epoch 13: training loss 226405101387.294
Test Loss of 183439623133.179749, Test MSE of 183439623852.496521
Epoch 14: training loss 161734867907.765
Test Loss of 142299275305.689575, Test MSE of 142299278423.270233
Epoch 15: training loss 140194752361.412
Test Loss of 131783825096.749481, Test MSE of 131783826470.406601
Epoch 16: training loss 134710534987.294
Test Loss of 128130632106.844315, Test MSE of 128130627875.787720
Epoch 17: training loss 131429910106.353
Test Loss of 125776107589.640533, Test MSE of 125776108025.810959
Epoch 18: training loss 128291771632.941
Test Loss of 122846627256.819794, Test MSE of 122846626755.218216
Epoch 19: training loss 125768097129.412
Test Loss of 120798909311.378204, Test MSE of 120798910062.724182
Epoch 20: training loss 123700918000.941
Test Loss of 118043888277.821884, Test MSE of 118043885539.929810
Epoch 21: training loss 118703034368.000
Test Loss of 114128368172.176727, Test MSE of 114128369962.783951
Epoch 22: training loss 113865954394.353
Test Loss of 109680868270.278976, Test MSE of 109680868503.496582
Epoch 23: training loss 108670476167.529
Test Loss of 105689581051.144119, Test MSE of 105689581253.285416
Epoch 24: training loss 105100012634.353
Test Loss of 99920503181.945877, Test MSE of 99920504123.048843
Epoch 25: training loss 101585159017.412
Test Loss of 97710870888.046265, Test MSE of 97710870753.433990
Epoch 26: training loss 97425177539.765
Test Loss of 92971174728.897522, Test MSE of 92971173792.490646
Epoch 27: training loss 94005585257.412
Test Loss of 90591893466.574142, Test MSE of 90591891755.574432
Epoch 28: training loss 90300129008.941
Test Loss of 87027619591.283829, Test MSE of 87027618600.288376
Epoch 29: training loss 85991467369.412
Test Loss of 84890854437.188995, Test MSE of 84890854989.335159
Epoch 30: training loss 82932549978.353
Test Loss of 80028767227.499420, Test MSE of 80028768082.222580
Epoch 31: training loss 80233027147.294
Test Loss of 76392073056.347900, Test MSE of 76392072027.811676
Epoch 32: training loss 75701760466.824
Test Loss of 72883047939.197784, Test MSE of 72883047440.409027
Epoch 33: training loss 72807098910.118
Test Loss of 68290253080.457092, Test MSE of 68290253603.201752
Epoch 34: training loss 68462183122.824
Test Loss of 68623874092.058296, Test MSE of 68623874538.916061
Epoch 35: training loss 64964725760.000
Test Loss of 59792039693.442520, Test MSE of 59792040311.043694
Epoch 36: training loss 62286300521.412
Test Loss of 59482090584.827202, Test MSE of 59482089327.145859
Epoch 37: training loss 59027989925.647
Test Loss of 55901744074.703674, Test MSE of 55901747314.779953
Epoch 38: training loss 55705697355.294
Test Loss of 53951021791.726112, Test MSE of 53951018750.335495
Epoch 39: training loss 52661385351.529
Test Loss of 51126543761.972702, Test MSE of 51126544066.364365
Epoch 40: training loss 50084032963.765
Test Loss of 50537476267.495720, Test MSE of 50537476545.103050
Epoch 41: training loss 47550832263.529
Test Loss of 46695995072.222069, Test MSE of 46695996818.524681
Epoch 42: training loss 45102356848.941
Test Loss of 44136402059.517929, Test MSE of 44136401875.955643
Epoch 43: training loss 42319172984.471
Test Loss of 44618376316.831833, Test MSE of 44618376250.498886
Epoch 44: training loss 40453502923.294
Test Loss of 38361597535.104324, Test MSE of 38361596539.803001
Epoch 45: training loss 37426616425.412
Test Loss of 36625316948.089752, Test MSE of 36625316258.356621
Epoch 46: training loss 35974811241.412
Test Loss of 38381775154.513069, Test MSE of 38381775819.953911
Epoch 47: training loss 33992404208.941
Test Loss of 34722107272.616241, Test MSE of 34722106498.327568
Epoch 48: training loss 31990905426.824
Test Loss of 35802343921.906082, Test MSE of 35802345194.164703
Epoch 49: training loss 30598906322.824
Test Loss of 32793040333.664585, Test MSE of 32793039560.170029
Epoch 50: training loss 28982177219.765
Test Loss of 32826414539.295860, Test MSE of 32826415507.547348
Epoch 51: training loss 27516333251.765
Test Loss of 29418733624.849411, Test MSE of 29418733962.113823
Epoch 52: training loss 25604025728.000
Test Loss of 30255018328.175804, Test MSE of 30255018616.973053
Epoch 53: training loss 24607971290.353
Test Loss of 32903121200.144344, Test MSE of 32903120913.109138
Epoch 54: training loss 23178451798.588
Test Loss of 31479967664.884571, Test MSE of 31479967366.325142
Epoch 55: training loss 22267765135.059
Test Loss of 27863169021.157528, Test MSE of 27863168966.611572
Epoch 56: training loss 21358597281.882
Test Loss of 27727390174.719406, Test MSE of 27727389209.917961
Epoch 57: training loss 20148273200.941
Test Loss of 28450760955.084896, Test MSE of 28450761452.773762
Epoch 58: training loss 19541458575.059
Test Loss of 27322663657.201019, Test MSE of 27322663676.826183
Epoch 59: training loss 19087355806.118
Test Loss of 26263338480.484848, Test MSE of 26263337711.630402
Epoch 60: training loss 17865677985.882
Test Loss of 27048652069.485081, Test MSE of 27048651639.463600
Epoch 61: training loss 17406735412.706
Test Loss of 23935046241.709923, Test MSE of 23935045981.150738
Epoch 62: training loss 16819103077.647
Test Loss of 25310231782.240112, Test MSE of 25310230822.779881
Epoch 63: training loss 16669440903.529
Test Loss of 27055323938.287300, Test MSE of 27055324474.584549
Epoch 64: training loss 15758287122.824
Test Loss of 26829347865.108490, Test MSE of 26829347634.249870
Epoch 65: training loss 14990124156.235
Test Loss of 26603011022.730511, Test MSE of 26603010895.102612
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23246446516.99213, 'MSE - std': 3356564378.1104813, 'R2 - mean': 0.8275937472257149, 'R2 - std': 0.01752158252301711} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005232 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927840105.412
Test Loss of 447259609950.689819, Test MSE of 447259608290.851746
Epoch 2: training loss 421908459279.059
Test Loss of 447241875618.020813, Test MSE of 447241876652.969055
Epoch 3: training loss 421881898405.647
Test Loss of 447217771604.563477, Test MSE of 447217770293.325134
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901306699.294
Test Loss of 447227740004.611633, Test MSE of 447227750808.448975
Epoch 2: training loss 421889706465.882
Test Loss of 447227675044.685608, Test MSE of 447227676948.835876
Epoch 3: training loss 421889245304.471
Test Loss of 447227415462.699036, Test MSE of 447227422853.440002
Epoch 4: training loss 421888944128.000
Test Loss of 447227360690.187378, Test MSE of 447227360246.641907
Epoch 5: training loss 421888727401.412
Test Loss of 447226732936.260925, Test MSE of 447226732874.542542
Epoch 6: training loss 421888586209.882
Test Loss of 447226950267.765930, Test MSE of 447226951291.617371
Epoch 7: training loss 421888477424.941
Test Loss of 447226623648.954895, Test MSE of 447226624553.235718
Epoch 8: training loss 421888380928.000
Test Loss of 447226575137.458252, Test MSE of 447226582554.130554
Epoch 9: training loss 421888298044.235
Test Loss of 447226263420.772583, Test MSE of 447226261707.193604
Epoch 10: training loss 421888219256.471
Test Loss of 447226631850.429810, Test MSE of 447226637974.260132
Epoch 11: training loss 409298793050.353
Test Loss of 406237355510.880432, Test MSE of 406237360743.544434
Epoch 12: training loss 329243283456.000
Test Loss of 296107115109.973633, Test MSE of 296107110706.411804
Epoch 13: training loss 222967048011.294
Test Loss of 197687363002.714783, Test MSE of 197687365837.811829
Epoch 14: training loss 158249928553.412
Test Loss of 154825572541.498047, Test MSE of 154825571864.627838
Epoch 15: training loss 137522082304.000
Test Loss of 142782890370.575989, Test MSE of 142782889054.056854
Epoch 16: training loss 133216894192.941
Test Loss of 138587931616.969696, Test MSE of 138587930827.170013
Epoch 17: training loss 130348662452.706
Test Loss of 135892931114.281754, Test MSE of 135892935142.405914
Epoch 18: training loss 127842614241.882
Test Loss of 133925942732.717102, Test MSE of 133925946533.343216
Epoch 19: training loss 123826111186.824
Test Loss of 129963823230.016190, Test MSE of 129963824482.391891
Epoch 20: training loss 121940381394.824
Test Loss of 128438841763.738144, Test MSE of 128438843424.917877
Epoch 21: training loss 118348793916.235
Test Loss of 121388407939.464264, Test MSE of 121388411945.645798
Epoch 22: training loss 113850018605.176
Test Loss of 119702281464.953049, Test MSE of 119702280999.815384
Epoch 23: training loss 108873788566.588
Test Loss of 114962817685.348145, Test MSE of 114962817357.524658
Epoch 24: training loss 104136880097.882
Test Loss of 112754681284.189682, Test MSE of 112754683565.953735
Epoch 25: training loss 102385979994.353
Test Loss of 108389602774.192001, Test MSE of 108389602629.296616
Epoch 26: training loss 98496662106.353
Test Loss of 103184405617.935699, Test MSE of 103184403582.943527
Epoch 27: training loss 95078554413.176
Test Loss of 99245519241.208420, Test MSE of 99245518669.479004
Epoch 28: training loss 91703509052.235
Test Loss of 97108874731.036774, Test MSE of 97108875509.467850
Epoch 29: training loss 87265231570.824
Test Loss of 92306414372.892899, Test MSE of 92306417290.047913
Epoch 30: training loss 83830712169.412
Test Loss of 91981519256.842010, Test MSE of 91981520460.430267
Epoch 31: training loss 80608489140.706
Test Loss of 86139901600.954895, Test MSE of 86139903176.255737
Epoch 32: training loss 77556357029.647
Test Loss of 81389673252.892899, Test MSE of 81389673164.833191
Epoch 33: training loss 73820137893.647
Test Loss of 77025451632.159149, Test MSE of 77025452038.712936
Epoch 34: training loss 69900416105.412
Test Loss of 76562583480.938232, Test MSE of 76562583464.432159
Epoch 35: training loss 66915792579.765
Test Loss of 70201680938.873932, Test MSE of 70201681165.563644
Epoch 36: training loss 63740101059.765
Test Loss of 66759411071.970390, Test MSE of 66759410003.950081
Epoch 37: training loss 60724530477.176
Test Loss of 64000479386.440895, Test MSE of 64000480045.711975
Epoch 38: training loss 58687068220.235
Test Loss of 60111100214.303032, Test MSE of 60111100988.307442
Epoch 39: training loss 55386611335.529
Test Loss of 55990979453.009483, Test MSE of 55990979646.705589
Epoch 40: training loss 51968899855.059
Test Loss of 53922116320.199860, Test MSE of 53922115924.480469
Epoch 41: training loss 50064325014.588
Test Loss of 52521433548.006477, Test MSE of 52521434385.368477
Epoch 42: training loss 48134026112.000
Test Loss of 50811217408.829056, Test MSE of 50811216524.311066
Epoch 43: training loss 45294843678.118
Test Loss of 48833584873.437889, Test MSE of 48833583833.080833
Epoch 44: training loss 42493845338.353
Test Loss of 43414197803.466110, Test MSE of 43414197950.718048
Epoch 45: training loss 40472696417.882
Test Loss of 46846874820.367340, Test MSE of 46846875052.832672
Epoch 46: training loss 38348334388.706
Test Loss of 40609421584.403427, Test MSE of 40609422171.482231
Epoch 47: training loss 36522256519.529
Test Loss of 42185128698.255844, Test MSE of 42185128154.566620
Epoch 48: training loss 34495610375.529
Test Loss of 37444655263.652092, Test MSE of 37444655708.145485
Epoch 49: training loss 33476770168.471
Test Loss of 36756532435.053436, Test MSE of 36756532452.005959
Epoch 50: training loss 31826865987.765
Test Loss of 32559374094.390007, Test MSE of 32559374295.495880
Epoch 51: training loss 30407851663.059
Test Loss of 34532254098.683319, Test MSE of 34532254189.686729
Epoch 52: training loss 28650844212.706
Test Loss of 34443814934.266022, Test MSE of 34443814328.447319
Epoch 53: training loss 27088804615.529
Test Loss of 29315684234.984962, Test MSE of 29315684593.664841
Epoch 54: training loss 25747017490.824
Test Loss of 30339122118.439972, Test MSE of 30339122251.260773
Epoch 55: training loss 24713283742.118
Test Loss of 28516416088.708767, Test MSE of 28516416286.585972
Epoch 56: training loss 23678661466.353
Test Loss of 27991867196.343281, Test MSE of 27991867127.446804
Epoch 57: training loss 22680083809.882
Test Loss of 28256770104.849411, Test MSE of 28256770517.378986
Epoch 58: training loss 21782007243.294
Test Loss of 26341346184.379368, Test MSE of 26341346621.739521
Epoch 59: training loss 21026865509.647
Test Loss of 29457018556.905853, Test MSE of 29457018986.381737
Epoch 60: training loss 20301073528.471
Test Loss of 26105817322.030071, Test MSE of 26105817505.709282
Epoch 61: training loss 19681145475.765
Test Loss of 23912254270.001389, Test MSE of 23912254270.829266
Epoch 62: training loss 19129353197.176
Test Loss of 24032032573.764515, Test MSE of 24032032461.775364
Epoch 63: training loss 18250532976.941
Test Loss of 23841449446.773075, Test MSE of 23841449161.384277
Epoch 64: training loss 17659494927.059
Test Loss of 22779489717.977329, Test MSE of 22779489703.175282
Epoch 65: training loss 17235147975.529
Test Loss of 24793589439.748322, Test MSE of 24793589639.452030
Epoch 66: training loss 16659439600.941
Test Loss of 23617475731.571594, Test MSE of 23617476352.803242
Epoch 67: training loss 16067640929.882
Test Loss of 23369294550.251213, Test MSE of 23369294911.074444
Epoch 68: training loss 15452868005.647
Test Loss of 21997924278.569511, Test MSE of 21997924230.416630
Epoch 69: training loss 15298834258.824
Test Loss of 22556652283.203331, Test MSE of 22556652263.623005
Epoch 70: training loss 14592781827.765
Test Loss of 23881204047.885265, Test MSE of 23881204388.737385
Epoch 71: training loss 14619315422.118
Test Loss of 22986031627.488319, Test MSE of 22986031189.926464
Epoch 72: training loss 14028138846.118
Test Loss of 20327878038.236408, Test MSE of 20327878267.168533
Epoch 73: training loss 13645445240.471
Test Loss of 23094786584.753181, Test MSE of 23094786630.562813
Epoch 74: training loss 13299153328.941
Test Loss of 22161588783.492943, Test MSE of 22161588994.814106
Epoch 75: training loss 12938952510.118
Test Loss of 19782002211.886189, Test MSE of 19782002114.907665
Epoch 76: training loss 12799697426.824
Test Loss of 23103393562.707378, Test MSE of 23103392812.367424
Epoch 77: training loss 12185995734.588
Test Loss of 23064303573.836689, Test MSE of 23064303649.587643
Epoch 78: training loss 11970975019.294
Test Loss of 19436303033.352764, Test MSE of 19436303295.061806
Epoch 79: training loss 11652984576.000
Test Loss of 21608495962.899837, Test MSE of 21608496334.245010
Epoch 80: training loss 11329896768.000
Test Loss of 21073280207.263474, Test MSE of 21073280828.333725
Epoch 81: training loss 11016392820.706
Test Loss of 21205016311.650242, Test MSE of 21205016286.786770
Epoch 82: training loss 10903602768.941
Test Loss of 22756405048.079575, Test MSE of 22756404925.265984
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23083099319.75008, 'MSE - std': 2750341959.9437327, 'R2 - mean': 0.8345665362180044, 'R2 - std': 0.017375561633203515} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005410 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109653112.471
Test Loss of 410764718074.313721, Test MSE of 410764725522.466248
Epoch 2: training loss 430088036352.000
Test Loss of 410745437995.713074, Test MSE of 410745439375.399353
Epoch 3: training loss 430060386906.353
Test Loss of 410720920222.978271, Test MSE of 410720917387.916443
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430073106191.059
Test Loss of 410723994804.538635, Test MSE of 410723990440.682739
Epoch 2: training loss 430064481942.588
Test Loss of 410724131274.691345, Test MSE of 410724127780.311218
Epoch 3: training loss 430064023672.471
Test Loss of 410724349096.218445, Test MSE of 410724354036.018616
Epoch 4: training loss 430063690209.882
Test Loss of 410724303863.944458, Test MSE of 410724304841.098816
Epoch 5: training loss 430063463484.235
Test Loss of 410724003447.648315, Test MSE of 410724003789.181396
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 119993325437.10791, 'MSE - std': 167870334079.68408, 'R2 - mean': 0.028445088026483267, 'R2 - std': 1.3963243893557604} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005453 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043124495.059
Test Loss of 431612322296.181396, Test MSE of 431612327256.828308
Epoch 2: training loss 424023432011.294
Test Loss of 431592197088.251709, Test MSE of 431592189906.459167
Epoch 3: training loss 423996407567.059
Test Loss of 431564861434.313721, Test MSE of 431564864202.559143
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010850304.000
Test Loss of 431567000058.076843, Test MSE of 431566999796.259949
Epoch 2: training loss 423998766019.765
Test Loss of 431569934560.607117, Test MSE of 431569936081.839539
Epoch 3: training loss 423998058255.059
Test Loss of 431569135558.189697, Test MSE of 431569136830.218689
Epoch 4: training loss 423997662509.176
Test Loss of 431568262240.192505, Test MSE of 431568260847.975586
Epoch 5: training loss 423997412532.706
Test Loss of 431567937517.045837, Test MSE of 431567934060.671509
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 182308247161.82062, 'MSE - std': 195133177930.99176, 'R2 - mean': -0.4218370004768982, 'R2 - std': 1.539738054720886} 
 

Saving model.....
Results After CV: {'MSE - mean': 182308247161.82062, 'MSE - std': 195133177930.99176, 'R2 - mean': -0.4218370004768982, 'R2 - std': 1.539738054720886}
Train time: 71.52707532420027
Inference time: 0.0689845018016058
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 87 finished with value: 182308247161.82062 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 10, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005352 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[98]	valid_0's l2: 1.21004e+10
Model Interpreting...
[(20,), (20,), (20,), (19,), (19,)]

Train embedding model...
Epoch 1: training loss 427525887518.118
Test Loss of 418110637949.720093, Test MSE of 418110634794.835266
Epoch 2: training loss 427505596295.529
Test Loss of 418091538223.078430, Test MSE of 418091532504.756897
Epoch 3: training loss 427478166467.765
Test Loss of 418066685025.354614, Test MSE of 418066689853.755005
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490655774.118
Test Loss of 418070216215.095093, Test MSE of 418070217479.504883
Epoch 2: training loss 427478449814.588
Test Loss of 418071572732.979858, Test MSE of 418071575022.311951
Epoch 3: training loss 427478114906.353
Test Loss of 418071161946.722168, Test MSE of 418071158204.769287
Epoch 4: training loss 422102064308.706
Test Loss of 401223411079.550293, Test MSE of 401223414022.224976
Epoch 5: training loss 385620592640.000
Test Loss of 346634794891.932434, Test MSE of 346634795820.850342
Epoch 6: training loss 317974618834.824
Test Loss of 269103337075.001617, Test MSE of 269103334949.001373
Epoch 7: training loss 228939374501.647
Test Loss of 175087887516.572754, Test MSE of 175087888926.892426
Epoch 8: training loss 163871606844.235
Test Loss of 133122348532.511688, Test MSE of 133122350130.341370
Epoch 9: training loss 142823600850.824
Test Loss of 121040938959.204254, Test MSE of 121040939881.377029
Epoch 10: training loss 137180937607.529
Test Loss of 116713556698.041168, Test MSE of 116713555035.009811
Epoch 11: training loss 133798303322.353
Test Loss of 113785172270.249359, Test MSE of 113785173967.400223
Epoch 12: training loss 130301459215.059
Test Loss of 110192348074.489014, Test MSE of 110192348243.663773
Epoch 13: training loss 128706216688.941
Test Loss of 107527367757.457321, Test MSE of 107527369566.023392
Epoch 14: training loss 124628463525.647
Test Loss of 105552476381.238953, Test MSE of 105552477940.444885
Epoch 15: training loss 121520523459.765
Test Loss of 102411379029.096466, Test MSE of 102411378640.025360
Epoch 16: training loss 118602171000.471
Test Loss of 100729215964.705994, Test MSE of 100729214362.075165
Epoch 17: training loss 113915988630.588
Test Loss of 96888014516.852188, Test MSE of 96888015593.261200
Epoch 18: training loss 110677612709.647
Test Loss of 94122193738.318756, Test MSE of 94122193446.601196
Epoch 19: training loss 107253335988.706
Test Loss of 90649242877.690491, Test MSE of 90649242432.277267
Epoch 20: training loss 104865246313.412
Test Loss of 88629522256.714325, Test MSE of 88629521827.161209
Epoch 21: training loss 101639427139.765
Test Loss of 85424629951.866760, Test MSE of 85424629877.755722
Epoch 22: training loss 99177809859.765
Test Loss of 82790506810.566742, Test MSE of 82790506061.005737
Epoch 23: training loss 94562306748.235
Test Loss of 80240746846.571365, Test MSE of 80240747281.295334
Epoch 24: training loss 91337353012.706
Test Loss of 77237114172.698593, Test MSE of 77237115253.367401
Epoch 25: training loss 89785191062.588
Test Loss of 75862274263.080261, Test MSE of 75862273730.115631
Epoch 26: training loss 85257356483.765
Test Loss of 72245456764.298874, Test MSE of 72245455592.316330
Epoch 27: training loss 82669093797.647
Test Loss of 70168484271.581772, Test MSE of 70168483398.747055
Epoch 28: training loss 79845969212.235
Test Loss of 68427292781.908859, Test MSE of 68427292481.037621
Epoch 29: training loss 76777339678.118
Test Loss of 66162434874.211426, Test MSE of 66162434681.026184
Epoch 30: training loss 73963744865.882
Test Loss of 61975532621.931068, Test MSE of 61975533376.325371
Epoch 31: training loss 71266391235.765
Test Loss of 58524227812.818878, Test MSE of 58524227235.724541
Epoch 32: training loss 67891297784.471
Test Loss of 56561397548.709694, Test MSE of 56561397639.692726
Epoch 33: training loss 65968771968.000
Test Loss of 54794290494.593567, Test MSE of 54794290220.410172
Epoch 34: training loss 62775579256.471
Test Loss of 48786207388.928062, Test MSE of 48786208118.964958
Epoch 35: training loss 60762081024.000
Test Loss of 49719665439.681702, Test MSE of 49719664230.810730
Epoch 36: training loss 57250809607.529
Test Loss of 49054324413.379601, Test MSE of 49054324070.746132
Epoch 37: training loss 55070473170.824
Test Loss of 46119706770.860977, Test MSE of 46119706097.273872
Epoch 38: training loss 53172151408.941
Test Loss of 43902954047.600281, Test MSE of 43902953845.658829
Epoch 39: training loss 50736989974.588
Test Loss of 40870187547.121902, Test MSE of 40870187577.155045
Epoch 40: training loss 48462517720.471
Test Loss of 39835160741.810776, Test MSE of 39835160600.053482
Epoch 41: training loss 46659869733.647
Test Loss of 37647023366.217903, Test MSE of 37647022672.753883
Epoch 42: training loss 43928458368.000
Test Loss of 33356847710.867455, Test MSE of 33356847553.526745
Epoch 43: training loss 42046848865.882
Test Loss of 36209463169.036316, Test MSE of 36209463329.630394
Epoch 44: training loss 40485941014.588
Test Loss of 30677734190.841545, Test MSE of 30677734680.704426
Epoch 45: training loss 38361423006.118
Test Loss of 35574143599.922279, Test MSE of 35574143842.553810
Epoch 46: training loss 37048124905.412
Test Loss of 32611098384.284988, Test MSE of 32611098805.773270
Epoch 47: training loss 35150674044.235
Test Loss of 29178400032.747627, Test MSE of 29178400379.498432
Epoch 48: training loss 33471858379.294
Test Loss of 29304404811.739994, Test MSE of 29304404792.787937
Epoch 49: training loss 31991316141.176
Test Loss of 29266390702.693501, Test MSE of 29266391233.607632
Epoch 50: training loss 30367500897.882
Test Loss of 25286041838.293777, Test MSE of 25286041947.349564
Epoch 51: training loss 29350094102.588
Test Loss of 22657258507.606754, Test MSE of 22657258497.964718
Epoch 52: training loss 28112797568.000
Test Loss of 24599439880.172104, Test MSE of 24599440418.580578
Epoch 53: training loss 26590365184.000
Test Loss of 26569509336.797596, Test MSE of 26569509915.589336
Epoch 54: training loss 25964924024.471
Test Loss of 20938347045.544296, Test MSE of 20938347368.062740
Epoch 55: training loss 24774181116.235
Test Loss of 21842881519.182049, Test MSE of 21842881767.427147
Epoch 56: training loss 23644826735.059
Test Loss of 23933325997.035393, Test MSE of 23933325695.448467
Epoch 57: training loss 23337752033.882
Test Loss of 22005986212.330326, Test MSE of 22005986139.879387
Epoch 58: training loss 22168316088.471
Test Loss of 21433348523.791813, Test MSE of 21433348014.126343
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21433348014.126343, 'MSE - std': 0.0, 'R2 - mean': 0.8330961935134279, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005306 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[92]	valid_0's l2: 1.91103e+10
Model Interpreting...
[(19,), (19,), (18,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918108310.588
Test Loss of 424555998408.631042, Test MSE of 424555998599.934143
Epoch 2: training loss 427897886599.529
Test Loss of 424539405369.559998, Test MSE of 424539411856.611389
Epoch 3: training loss 427870080783.059
Test Loss of 424516633269.562805, Test MSE of 424516638401.086182
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884935047.529
Test Loss of 424524029870.515869, Test MSE of 424524035365.990295
Epoch 2: training loss 427875403414.588
Test Loss of 424525107636.792969, Test MSE of 424525115178.978027
Epoch 3: training loss 427874993814.588
Test Loss of 424524701686.525085, Test MSE of 424524705379.300110
Epoch 4: training loss 422503158482.824
Test Loss of 407765818052.248901, Test MSE of 407765821004.376465
Epoch 5: training loss 385955883128.471
Test Loss of 353932738120.364563, Test MSE of 353932728334.110291
Epoch 6: training loss 317648809743.059
Test Loss of 277231162887.461487, Test MSE of 277231162970.842712
Epoch 7: training loss 226959039006.118
Test Loss of 184488535666.527863, Test MSE of 184488541802.413116
Epoch 8: training loss 162631784869.647
Test Loss of 143832962213.100159, Test MSE of 143832963351.543335
Epoch 9: training loss 141003149733.647
Test Loss of 132251942868.415451, Test MSE of 132251938857.313202
Epoch 10: training loss 134590443760.941
Test Loss of 127860265868.643066, Test MSE of 127860262347.148422
Epoch 11: training loss 130976671081.412
Test Loss of 124907969779.268097, Test MSE of 124907971430.104904
Epoch 12: training loss 129132621191.529
Test Loss of 121992408181.251907, Test MSE of 121992410351.400040
Epoch 13: training loss 125933306398.118
Test Loss of 119043598543.026596, Test MSE of 119043601664.130753
Epoch 14: training loss 122195734497.882
Test Loss of 115572156494.167938, Test MSE of 115572157678.097733
Epoch 15: training loss 119253200685.176
Test Loss of 113129500307.216278, Test MSE of 113129498826.015930
Epoch 16: training loss 115532387448.471
Test Loss of 110044351089.817261, Test MSE of 110044346983.322891
Epoch 17: training loss 112278248387.765
Test Loss of 106424022077.823730, Test MSE of 106424021383.688904
Epoch 18: training loss 109811046460.235
Test Loss of 104416934226.727737, Test MSE of 104416936272.792114
Epoch 19: training loss 105113100016.941
Test Loss of 100126904169.349060, Test MSE of 100126903004.922485
Epoch 20: training loss 101951241908.706
Test Loss of 97233214793.489700, Test MSE of 97233216163.717651
Epoch 21: training loss 98069177103.059
Test Loss of 93105902442.533432, Test MSE of 93105904822.955246
Epoch 22: training loss 94475939719.529
Test Loss of 89336054011.084900, Test MSE of 89336052980.694656
Epoch 23: training loss 92824691305.412
Test Loss of 87861385528.198013, Test MSE of 87861385390.080002
Epoch 24: training loss 89404479969.882
Test Loss of 83057588646.106873, Test MSE of 83057585775.046570
Epoch 25: training loss 86354185321.412
Test Loss of 81270232537.034470, Test MSE of 81270232016.243118
Epoch 26: training loss 82978202051.765
Test Loss of 75397878322.809158, Test MSE of 75397879326.623169
Epoch 27: training loss 79711479536.941
Test Loss of 74460669178.374283, Test MSE of 74460667851.598434
Epoch 28: training loss 77262093447.529
Test Loss of 72634244079.655792, Test MSE of 72634243073.312607
Epoch 29: training loss 73614716656.941
Test Loss of 70033003132.713394, Test MSE of 70033003164.476593
Epoch 30: training loss 70817820672.000
Test Loss of 69136765966.449219, Test MSE of 69136767588.066818
Epoch 31: training loss 68204615695.059
Test Loss of 64661689725.127922, Test MSE of 64661688038.338600
Epoch 32: training loss 64969294802.824
Test Loss of 62896438724.663429, Test MSE of 62896437387.756073
Epoch 33: training loss 62290112828.235
Test Loss of 60077109251.789963, Test MSE of 60077109605.665672
Epoch 34: training loss 59320594913.882
Test Loss of 56862114079.089523, Test MSE of 56862114379.784309
Epoch 35: training loss 56888995809.882
Test Loss of 52757390615.746475, Test MSE of 52757392578.622009
Epoch 36: training loss 54316201396.706
Test Loss of 51432515319.176498, Test MSE of 51432516566.903412
Epoch 37: training loss 51737330401.882
Test Loss of 50146707944.431183, Test MSE of 50146707193.719368
Epoch 38: training loss 49344752346.353
Test Loss of 49959243622.032845, Test MSE of 49959246392.642288
Epoch 39: training loss 47160010458.353
Test Loss of 45750213335.672447, Test MSE of 45750214809.466400
Epoch 40: training loss 45274229662.118
Test Loss of 47116842017.872772, Test MSE of 47116842909.898743
Epoch 41: training loss 43326445718.588
Test Loss of 46685008250.522324, Test MSE of 46685008379.081009
Epoch 42: training loss 40611464478.118
Test Loss of 44557634447.485542, Test MSE of 44557633950.301033
Epoch 43: training loss 38793860201.412
Test Loss of 39708755781.344437, Test MSE of 39708756145.157822
Epoch 44: training loss 37087155365.647
Test Loss of 37835928748.206337, Test MSE of 37835927846.055756
Epoch 45: training loss 35164823115.294
Test Loss of 39815835306.903542, Test MSE of 39815836886.114792
Epoch 46: training loss 33551737140.706
Test Loss of 38406312898.650009, Test MSE of 38406313139.210815
Epoch 47: training loss 32215680256.000
Test Loss of 35178211148.450615, Test MSE of 35178211253.749809
Epoch 48: training loss 30071644724.706
Test Loss of 34229346983.350452, Test MSE of 34229347001.982700
Epoch 49: training loss 29053631796.706
Test Loss of 32900879773.816330, Test MSE of 32900880309.923374
Epoch 50: training loss 27858112888.471
Test Loss of 31533973540.952118, Test MSE of 31533974398.638111
Epoch 51: training loss 26400861876.706
Test Loss of 32430198859.799213, Test MSE of 32430199826.305199
Epoch 52: training loss 25437299636.706
Test Loss of 28709388752.743927, Test MSE of 28709389216.721611
Epoch 53: training loss 24029452024.471
Test Loss of 30484205399.820496, Test MSE of 30484204928.643597
Epoch 54: training loss 22781107200.000
Test Loss of 30829049240.605137, Test MSE of 30829049507.465858
Epoch 55: training loss 21587241592.471
Test Loss of 30057102934.340042, Test MSE of 30057102712.527088
Epoch 56: training loss 20973766855.529
Test Loss of 26439286213.847790, Test MSE of 26439286221.070507
Epoch 57: training loss 19987076992.000
Test Loss of 27941360491.480915, Test MSE of 27941360640.230499
Epoch 58: training loss 19359446445.176
Test Loss of 26650359523.279205, Test MSE of 26650359633.134129
Epoch 59: training loss 18478142497.882
Test Loss of 27601033786.862827, Test MSE of 27601033971.549042
Epoch 60: training loss 17714045808.941
Test Loss of 28283048962.131851, Test MSE of 28283048075.982628
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24858198045.054485, 'MSE - std': 3424850030.9281425, 'R2 - mean': 0.8155870040844968, 'R2 - std': 0.017509189428931038} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005317 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.47566e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421928108152.471
Test Loss of 447257995048.445984, Test MSE of 447257987243.380432
Epoch 2: training loss 421908878034.824
Test Loss of 447240028042.511230, Test MSE of 447240028250.226379
Epoch 3: training loss 421882840365.176
Test Loss of 447215811330.783264, Test MSE of 447215809127.339417
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897800884.706
Test Loss of 447226731723.473511, Test MSE of 447226733933.968628
Epoch 2: training loss 421887654249.412
Test Loss of 447227034829.368469, Test MSE of 447227040552.380310
Epoch 3: training loss 421887365481.412
Test Loss of 447227055295.156128, Test MSE of 447227049517.092834
Epoch 4: training loss 416517349978.353
Test Loss of 430207253417.541504, Test MSE of 430207248669.287720
Epoch 5: training loss 379980354740.706
Test Loss of 373241021083.033081, Test MSE of 373241020300.412170
Epoch 6: training loss 312293741989.647
Test Loss of 296383453581.472107, Test MSE of 296383451240.589966
Epoch 7: training loss 223454347023.059
Test Loss of 200062664294.684235, Test MSE of 200062657455.430267
Epoch 8: training loss 159003753291.294
Test Loss of 155853637793.547089, Test MSE of 155853636222.500214
Epoch 9: training loss 138647998795.294
Test Loss of 143087449738.215118, Test MSE of 143087450919.076447
Epoch 10: training loss 132895656297.412
Test Loss of 138742405237.015045, Test MSE of 138742404394.723572
Epoch 11: training loss 130133704282.353
Test Loss of 135123593170.994217, Test MSE of 135123594457.414871
Epoch 12: training loss 126190111744.000
Test Loss of 132486236583.054367, Test MSE of 132486237045.922363
Epoch 13: training loss 123932449822.118
Test Loss of 129188517053.734909, Test MSE of 129188516258.574249
Epoch 14: training loss 120808196698.353
Test Loss of 126094337483.769608, Test MSE of 126094339643.002213
Epoch 15: training loss 117663492397.176
Test Loss of 123104405345.295395, Test MSE of 123104404743.139038
Epoch 16: training loss 115302247966.118
Test Loss of 119602467495.350449, Test MSE of 119602467135.361374
Epoch 17: training loss 110527170469.647
Test Loss of 116061935611.736298, Test MSE of 116061935285.220856
Epoch 18: training loss 108430518663.529
Test Loss of 113078438000.277588, Test MSE of 113078438105.662308
Epoch 19: training loss 104360417008.941
Test Loss of 110425701566.208649, Test MSE of 110425701323.924240
Epoch 20: training loss 101416164412.235
Test Loss of 106510123375.626190, Test MSE of 106510120293.553894
Epoch 21: training loss 97768345931.294
Test Loss of 104310872855.628036, Test MSE of 104310873093.155807
Epoch 22: training loss 95015453063.529
Test Loss of 100121727471.537354, Test MSE of 100121727802.678818
Epoch 23: training loss 91925650883.765
Test Loss of 97674561919.259781, Test MSE of 97674563486.978714
Epoch 24: training loss 88801470494.118
Test Loss of 94804095412.792969, Test MSE of 94804095839.002289
Epoch 25: training loss 86266425328.941
Test Loss of 90152654515.904694, Test MSE of 90152655838.483353
Epoch 26: training loss 83507504504.471
Test Loss of 87280560786.031921, Test MSE of 87280560007.848083
Epoch 27: training loss 79796685056.000
Test Loss of 86128462038.606522, Test MSE of 86128463558.915482
Epoch 28: training loss 77599772928.000
Test Loss of 79492681403.010880, Test MSE of 79492681258.288086
Epoch 29: training loss 73848454836.706
Test Loss of 79364767745.894974, Test MSE of 79364768959.179916
Epoch 30: training loss 71223489626.353
Test Loss of 75384128249.782089, Test MSE of 75384129154.945328
Epoch 31: training loss 68278956016.941
Test Loss of 74849520660.607910, Test MSE of 74849520969.644485
Epoch 32: training loss 66042994989.176
Test Loss of 69095177849.634048, Test MSE of 69095178623.843201
Epoch 33: training loss 63411631811.765
Test Loss of 66356117305.027061, Test MSE of 66356117609.542305
Epoch 34: training loss 61165376271.059
Test Loss of 64829333810.986816, Test MSE of 64829333617.532463
Epoch 35: training loss 58158928489.412
Test Loss of 63119740805.773766, Test MSE of 63119741868.462067
Epoch 36: training loss 55582538157.176
Test Loss of 60365855039.777931, Test MSE of 60365855673.596809
Epoch 37: training loss 53123690608.941
Test Loss of 54645023311.233864, Test MSE of 54645024281.586800
Epoch 38: training loss 51183975439.059
Test Loss of 52461855921.417534, Test MSE of 52461856114.225975
Epoch 39: training loss 49056327913.412
Test Loss of 45167761658.611153, Test MSE of 45167762490.322548
Epoch 40: training loss 46645827252.706
Test Loss of 47302753901.316681, Test MSE of 47302754686.147041
Epoch 41: training loss 44076594304.000
Test Loss of 44524875654.721260, Test MSE of 44524875187.130547
Epoch 42: training loss 42426999996.235
Test Loss of 45496918550.384453, Test MSE of 45496918939.502457
Epoch 43: training loss 40864284920.471
Test Loss of 46341818303.096924, Test MSE of 46341818929.196274
Epoch 44: training loss 38442759393.882
Test Loss of 43898278120.135094, Test MSE of 43898278550.255539
Epoch 45: training loss 36664897566.118
Test Loss of 38042807994.063385, Test MSE of 38042808860.234718
Epoch 46: training loss 35342054437.647
Test Loss of 40640233660.550545, Test MSE of 40640233856.265640
Epoch 47: training loss 33610858834.824
Test Loss of 38274268084.911407, Test MSE of 38274268717.030495
Epoch 48: training loss 31782394842.353
Test Loss of 39060442937.974556, Test MSE of 39060443745.673256
Epoch 49: training loss 30862385377.882
Test Loss of 35435522122.377975, Test MSE of 35435522219.773094
Epoch 50: training loss 29356851998.118
Test Loss of 34333273174.932224, Test MSE of 34333272766.591423
Epoch 51: training loss 28043926000.941
Test Loss of 32242415180.628269, Test MSE of 32242415439.511189
Epoch 52: training loss 26949908705.882
Test Loss of 31311634432.947491, Test MSE of 31311634955.794960
Epoch 53: training loss 25843985182.118
Test Loss of 31029881311.903770, Test MSE of 31029881512.051048
Epoch 54: training loss 24658381989.647
Test Loss of 34735540026.448303, Test MSE of 34735540608.611382
Epoch 55: training loss 23412881513.412
Test Loss of 27469778311.550312, Test MSE of 27469778263.237331
Epoch 56: training loss 22831729219.765
Test Loss of 28074510917.285217, Test MSE of 28074510926.203785
Epoch 57: training loss 21871272632.471
Test Loss of 27715498657.665508, Test MSE of 27715498728.740005
Epoch 58: training loss 21081656888.471
Test Loss of 27013541962.851723, Test MSE of 27013542312.655334
Epoch 59: training loss 20355712274.824
Test Loss of 25401935422.889660, Test MSE of 25401935607.271282
Epoch 60: training loss 19525957534.118
Test Loss of 27915567321.922737, Test MSE of 27915567020.932194
Epoch 61: training loss 18981572570.353
Test Loss of 27650626821.744160, Test MSE of 27650626977.433937
Epoch 62: training loss 18176912971.294
Test Loss of 27378425323.510525, Test MSE of 27378425613.669376
Epoch 63: training loss 17499363561.412
Test Loss of 26598319946.555634, Test MSE of 26598319958.134251
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25438238682.74774, 'MSE - std': 2914211060.587444, 'R2 - mean': 0.8180369158472569, 'R2 - std': 0.014710040049664983} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005343 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's l2: 1.31903e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (19,)]

Train embedding model...
Epoch 1: training loss 430111677982.118
Test Loss of 410764627442.968994, Test MSE of 410764633641.654419
Epoch 2: training loss 430092516894.118
Test Loss of 410747273868.024048, Test MSE of 410747274164.506531
Epoch 3: training loss 430065903977.412
Test Loss of 410723936440.803345, Test MSE of 410723935500.367676
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430077834902.588
Test Loss of 410729913863.818604, Test MSE of 410729911958.108521
Epoch 2: training loss 430067332276.706
Test Loss of 410730000006.811646, Test MSE of 410730002712.494324
Epoch 3: training loss 430067085312.000
Test Loss of 410729272484.427551, Test MSE of 410729267477.471191
Epoch 4: training loss 424803414256.941
Test Loss of 394204991281.399353, Test MSE of 394204995777.417175
Epoch 5: training loss 388417100739.765
Test Loss of 339576956968.751526, Test MSE of 339576949628.614746
Epoch 6: training loss 320540166023.529
Test Loss of 262712995440.066650, Test MSE of 262712995753.281403
Epoch 7: training loss 230708442533.647
Test Loss of 168550985282.576599, Test MSE of 168550987598.401154
Epoch 8: training loss 165913569280.000
Test Loss of 126850259570.435913, Test MSE of 126850259404.279999
Epoch 9: training loss 145671692589.176
Test Loss of 115334029763.109665, Test MSE of 115334029364.788788
Epoch 10: training loss 139723864545.882
Test Loss of 110854098005.767700, Test MSE of 110854095858.139175
Epoch 11: training loss 136559205345.882
Test Loss of 108831115354.032394, Test MSE of 108831117589.827408
Epoch 12: training loss 133865251870.118
Test Loss of 105862821532.608978, Test MSE of 105862824795.441040
Epoch 13: training loss 130637315584.000
Test Loss of 103894396103.492828, Test MSE of 103894395866.820221
Epoch 14: training loss 126882357639.529
Test Loss of 100714274535.478012, Test MSE of 100714276289.178970
Epoch 15: training loss 124245452016.941
Test Loss of 97854457555.576126, Test MSE of 97854459339.891815
Epoch 16: training loss 121080763904.000
Test Loss of 94364743347.354004, Test MSE of 94364743296.181885
Epoch 17: training loss 116914871868.235
Test Loss of 92969971194.550674, Test MSE of 92969970228.423477
Epoch 18: training loss 113118229413.647
Test Loss of 90444000167.389175, Test MSE of 90444000856.198700
Epoch 19: training loss 110986836871.529
Test Loss of 88265632368.540497, Test MSE of 88265631258.707306
Epoch 20: training loss 108070543360.000
Test Loss of 84875281350.663574, Test MSE of 84875281587.537720
Epoch 21: training loss 104558678678.588
Test Loss of 81635906299.853775, Test MSE of 81635907692.802460
Epoch 22: training loss 100327741680.941
Test Loss of 79650904690.435913, Test MSE of 79650904692.628250
Epoch 23: training loss 97768804653.176
Test Loss of 76445511299.020828, Test MSE of 76445510787.234299
Epoch 24: training loss 94740604400.941
Test Loss of 74636661871.355850, Test MSE of 74636661423.854874
Epoch 25: training loss 92321571689.412
Test Loss of 71493144934.234146, Test MSE of 71493145716.081665
Epoch 26: training loss 88333416463.059
Test Loss of 69645384408.788528, Test MSE of 69645385948.561981
Epoch 27: training loss 85391365376.000
Test Loss of 66667350337.273483, Test MSE of 66667350406.768456
Epoch 28: training loss 82215724001.882
Test Loss of 61519588752.407219, Test MSE of 61519588981.786789
Epoch 29: training loss 79530328636.235
Test Loss of 61985029897.595558, Test MSE of 61985029094.106400
Epoch 30: training loss 76108784850.824
Test Loss of 61848157127.611290, Test MSE of 61848156726.033752
Epoch 31: training loss 74691530089.412
Test Loss of 57938558028.290604, Test MSE of 57938557079.270836
Epoch 32: training loss 70947848071.529
Test Loss of 56500256237.282738, Test MSE of 56500255646.058769
Epoch 33: training loss 68010056222.118
Test Loss of 52484281045.945396, Test MSE of 52484280332.007744
Epoch 34: training loss 65658080210.824
Test Loss of 53221469792.903282, Test MSE of 53221469322.169800
Epoch 35: training loss 62519668871.529
Test Loss of 49508653758.252663, Test MSE of 49508654756.556847
Epoch 36: training loss 59964041652.706
Test Loss of 46568074217.728828, Test MSE of 46568074604.655914
Epoch 37: training loss 56866471627.294
Test Loss of 42583577681.029152, Test MSE of 42583578342.882599
Epoch 38: training loss 55800868336.941
Test Loss of 42720001604.472000, Test MSE of 42720000896.247231
Epoch 39: training loss 52835934049.882
Test Loss of 39613721719.885239, Test MSE of 39613721605.755463
Epoch 40: training loss 50534863864.471
Test Loss of 40132899706.846832, Test MSE of 40132899860.914566
Epoch 41: training loss 48239691316.706
Test Loss of 38843069246.193428, Test MSE of 38843069202.762558
Epoch 42: training loss 46115158934.588
Test Loss of 37362804240.347984, Test MSE of 37362804651.504784
Epoch 43: training loss 44792400158.118
Test Loss of 35186672675.539101, Test MSE of 35186672729.737778
Epoch 44: training loss 42066096722.824
Test Loss of 33121762224.866264, Test MSE of 33121762054.286541
Epoch 45: training loss 39720217291.294
Test Loss of 32585988900.605274, Test MSE of 32585989038.402069
Epoch 46: training loss 38429989195.294
Test Loss of 32258903167.466915, Test MSE of 32258902402.575047
Epoch 47: training loss 36719373379.765
Test Loss of 29092537777.577049, Test MSE of 29092537603.423088
Epoch 48: training loss 35259023367.529
Test Loss of 28677308050.184174, Test MSE of 28677308140.903858
Epoch 49: training loss 33600896165.647
Test Loss of 26636158908.712631, Test MSE of 26636158211.191296
Epoch 50: training loss 31945986514.824
Test Loss of 28240002520.433132, Test MSE of 28240002178.766941
Epoch 51: training loss 30851763998.118
Test Loss of 26325354723.450256, Test MSE of 26325354561.879887
Epoch 52: training loss 29403536293.647
Test Loss of 25373609489.295696, Test MSE of 25373609089.606075
Epoch 53: training loss 28047651459.765
Test Loss of 25246826533.908375, Test MSE of 25246826623.582729
Epoch 54: training loss 27111314831.059
Test Loss of 23003866892.438686, Test MSE of 23003866888.481960
Epoch 55: training loss 25733216730.353
Test Loss of 23115667898.580288, Test MSE of 23115667637.563160
Epoch 56: training loss 24850272613.647
Test Loss of 23978016205.060619, Test MSE of 23978015952.254906
Epoch 57: training loss 23847738364.235
Test Loss of 23860410183.196667, Test MSE of 23860410882.741032
Epoch 58: training loss 23184182908.235
Test Loss of 22610713369.232761, Test MSE of 22610713583.717861
Epoch 59: training loss 22302095691.294
Test Loss of 20644353205.486347, Test MSE of 20644353305.473919
Epoch 60: training loss 21646973515.294
Test Loss of 20049042427.735310, Test MSE of 20049042735.409805
Epoch 61: training loss 20471343813.647
Test Loss of 20541490185.950947, Test MSE of 20541490340.113209
Epoch 62: training loss 19726337904.941
Test Loss of 21354351724.512726, Test MSE of 21354351566.636299
Epoch 63: training loss 19727871759.059
Test Loss of 19283187379.354004, Test MSE of 19283187399.072838
Epoch 64: training loss 18975663627.294
Test Loss of 19278360149.530773, Test MSE of 19278360075.566402
Epoch 65: training loss 18206562962.824
Test Loss of 20450804860.623787, Test MSE of 20450804680.443062
Epoch 66: training loss 17516278304.000
Test Loss of 19459612889.973160, Test MSE of 19459612574.334503
Epoch 67: training loss 16927877564.235
Test Loss of 18813949457.295696, Test MSE of 18813949212.274799
Epoch 68: training loss 16526375653.647
Test Loss of 18532259248.629337, Test MSE of 18532259256.888065
Epoch 69: training loss 16040898595.765
Test Loss of 18922301552.777416, Test MSE of 18922301729.560089
Epoch 70: training loss 15808064553.412
Test Loss of 20051142224.792225, Test MSE of 20051142173.658768
Epoch 71: training loss 15443231307.294
Test Loss of 19971655063.041183, Test MSE of 19971655022.123547
Epoch 72: training loss 15286796307.765
Test Loss of 18554510620.786674, Test MSE of 18554510899.417442
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23717306736.91517, 'MSE - std': 3905674060.3638344, 'R2 - mean': 0.8252426748009949, 'R2 - std': 0.01783417631950472} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.7, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 128, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005552 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[79]	valid_0's l2: 1.67475e+10
Model Interpreting...
[(16,), (16,), (16,), (16,), (15,)]

Train embedding model...
Epoch 1: training loss 424044013206.588
Test Loss of 431613235706.076843, Test MSE of 431613237656.919373
Epoch 2: training loss 424025410861.176
Test Loss of 431593529992.233215, Test MSE of 431593527576.649597
Epoch 3: training loss 423998297509.647
Test Loss of 431566508750.363708, Test MSE of 431566509538.437317
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424014596939.294
Test Loss of 431565695979.624268, Test MSE of 431565694612.787659
Epoch 2: training loss 424000209618.824
Test Loss of 431567459871.511353, Test MSE of 431567462198.166138
Epoch 3: training loss 423999757131.294
Test Loss of 431567872026.535889, Test MSE of 431567875882.756470
Epoch 4: training loss 418394916020.706
Test Loss of 413480718106.654297, Test MSE of 413480718696.098694
Epoch 5: training loss 381379891440.941
Test Loss of 357510602240.236938, Test MSE of 357510603915.306091
Epoch 6: training loss 313546011226.353
Test Loss of 279093577529.928711, Test MSE of 279093582132.159851
Epoch 7: training loss 225483614840.471
Test Loss of 182868172614.248962, Test MSE of 182868175114.234161
Epoch 8: training loss 162994028152.471
Test Loss of 140032558792.203613, Test MSE of 140032556776.812958
Epoch 9: training loss 140142397289.412
Test Loss of 126909501529.084686, Test MSE of 126909498911.429031
Epoch 10: training loss 136730385076.706
Test Loss of 122300909305.484497, Test MSE of 122300909497.086517
Epoch 11: training loss 133212941613.176
Test Loss of 118675636659.946320, Test MSE of 118675636518.225357
Epoch 12: training loss 128854966181.647
Test Loss of 116420830745.825089, Test MSE of 116420832469.745346
Epoch 13: training loss 125704599943.529
Test Loss of 113606441537.155014, Test MSE of 113606440599.897583
Epoch 14: training loss 123060454098.824
Test Loss of 110381496540.816284, Test MSE of 110381498183.677521
Epoch 15: training loss 120014834627.765
Test Loss of 105804924089.277191, Test MSE of 105804925404.903503
Epoch 16: training loss 116624886663.529
Test Loss of 102454028025.484497, Test MSE of 102454029154.325104
Epoch 17: training loss 113535820197.647
Test Loss of 101090811589.360474, Test MSE of 101090812402.147537
Epoch 18: training loss 109693145374.118
Test Loss of 98571359214.467377, Test MSE of 98571359905.660980
Epoch 19: training loss 107634023454.118
Test Loss of 93632978815.111526, Test MSE of 93632978428.667282
Epoch 20: training loss 104112334607.059
Test Loss of 91801449488.584915, Test MSE of 91801448771.332458
Epoch 21: training loss 101080760681.412
Test Loss of 88485545683.576126, Test MSE of 88485545401.150299
Epoch 22: training loss 97172564028.235
Test Loss of 84911700655.089310, Test MSE of 84911700085.782715
Epoch 23: training loss 94545927499.294
Test Loss of 82280599572.849609, Test MSE of 82280598578.595657
Epoch 24: training loss 91393205443.765
Test Loss of 79573992765.956497, Test MSE of 79573994723.551849
Epoch 25: training loss 88013558829.176
Test Loss of 77884378815.674225, Test MSE of 77884378811.752625
Epoch 26: training loss 85134106601.412
Test Loss of 73974647215.207779, Test MSE of 73974647880.664093
Epoch 27: training loss 82242474270.118
Test Loss of 71409164397.934296, Test MSE of 71409163630.590973
Epoch 28: training loss 78709867791.059
Test Loss of 68333360227.983337, Test MSE of 68333359720.150841
Epoch 29: training loss 76069750874.353
Test Loss of 66579696803.479874, Test MSE of 66579696670.624756
Epoch 30: training loss 73275982080.000
Test Loss of 65833595632.481262, Test MSE of 65833595749.039436
Epoch 31: training loss 70966089908.706
Test Loss of 60305587115.180008, Test MSE of 60305586182.701004
Epoch 32: training loss 67024405940.706
Test Loss of 60926318405.301247, Test MSE of 60926319627.434143
Epoch 33: training loss 65249668314.353
Test Loss of 58593342490.062012, Test MSE of 58593343339.380051
Epoch 34: training loss 63095373989.647
Test Loss of 56492244614.811661, Test MSE of 56492244329.110611
Epoch 35: training loss 59745210533.647
Test Loss of 51467066624.355392, Test MSE of 51467066457.031319
Epoch 36: training loss 57237597063.529
Test Loss of 48933894296.581215, Test MSE of 48933894250.537384
Epoch 37: training loss 54747089904.941
Test Loss of 46141516309.560387, Test MSE of 46141515839.801712
Epoch 38: training loss 52613023849.412
Test Loss of 42916031084.749657, Test MSE of 42916032380.661705
Epoch 39: training loss 50455759774.118
Test Loss of 45313033135.918556, Test MSE of 45313033804.544273
Epoch 40: training loss 47969351213.176
Test Loss of 42765403851.994446, Test MSE of 42765405019.029999
Epoch 41: training loss 45447383484.235
Test Loss of 41910811328.148079, Test MSE of 41910810628.810387
Epoch 42: training loss 44089991563.294
Test Loss of 42838554312.203606, Test MSE of 42838554764.391525
Epoch 43: training loss 41894334362.353
Test Loss of 37385837794.028694, Test MSE of 37385837040.502853
Epoch 44: training loss 40060613684.706
Test Loss of 34850675188.390556, Test MSE of 34850675337.806625
Epoch 45: training loss 38334864180.706
Test Loss of 32956127556.116611, Test MSE of 32956127515.320831
Epoch 46: training loss 36339916114.824
Test Loss of 32825154699.787136, Test MSE of 32825155022.041515
Epoch 47: training loss 35093180498.824
Test Loss of 33195521313.525219, Test MSE of 33195521382.119923
Epoch 48: training loss 33596884464.941
Test Loss of 29404500059.927811, Test MSE of 29404500160.806900
Epoch 49: training loss 31978532965.647
Test Loss of 31358974776.507172, Test MSE of 31358974389.994957
Epoch 50: training loss 30959137031.529
Test Loss of 28503328831.496529, Test MSE of 28503328379.062759
Epoch 51: training loss 29425443136.000
Test Loss of 28145329993.092087, Test MSE of 28145330176.565140
Epoch 52: training loss 28212788389.647
Test Loss of 24521234119.255901, Test MSE of 24521234471.566345
Epoch 53: training loss 26818182369.882
Test Loss of 25803684200.129570, Test MSE of 25803684391.315628
Epoch 54: training loss 26106955120.941
Test Loss of 25030370793.491901, Test MSE of 25030371237.054352
Epoch 55: training loss 24960825780.706
Test Loss of 22406189671.063396, Test MSE of 22406189644.526596
Epoch 56: training loss 24153346661.647
Test Loss of 21764153699.391022, Test MSE of 21764153855.445324
Epoch 57: training loss 23033413440.000
Test Loss of 21861365619.739010, Test MSE of 21861365413.956112
Epoch 58: training loss 21979254923.294
Test Loss of 23362671938.695049, Test MSE of 23362671909.420746
Epoch 59: training loss 21494526678.588
Test Loss of 23140062131.709393, Test MSE of 23140062000.926758
Epoch 60: training loss 20766705140.706
Test Loss of 23531146765.504860, Test MSE of 23531147067.211678
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23680074802.97447, 'MSE - std': 3494134622.5151362, 'R2 - mean': 0.8250478438519098, 'R2 - std': 0.015956130871581365} 
 

Saving model.....
Results After CV: {'MSE - mean': 23680074802.97447, 'MSE - std': 3494134622.5151362, 'R2 - mean': 0.8250478438519098, 'R2 - std': 0.015956130871581365}
Train time: 97.56579590160109
Inference time: 0.06910258639982203
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 88 finished with value: 23680074802.97447 and parameters: {'n_trees': 100, 'maxleaf': 128, 'loss_de': 3, 'loss_dr': 0.7}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005717 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525277816.471
Test Loss of 418112233636.863281, Test MSE of 418112238858.812866
Epoch 2: training loss 427504560368.941
Test Loss of 418094322055.313416, Test MSE of 418094326429.057373
Epoch 3: training loss 427476978748.235
Test Loss of 418070238628.922485, Test MSE of 418070237097.514709
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496701470.118
Test Loss of 418074395548.750427, Test MSE of 418074398827.666016
Epoch 2: training loss 427482615446.588
Test Loss of 418075291889.609985, Test MSE of 418075294954.733521
Epoch 3: training loss 423632945874.824
Test Loss of 405876722189.383301, Test MSE of 405876728058.985962
Epoch 4: training loss 396952286147.765
Test Loss of 364445124132.833679, Test MSE of 364445127544.828491
Epoch 5: training loss 330852834605.176
Test Loss of 277862286403.508667, Test MSE of 277862289299.360962
Epoch 6: training loss 247319373583.059
Test Loss of 201262699277.679382, Test MSE of 201262701547.046051
Epoch 7: training loss 179520630603.294
Test Loss of 140462638052.759644, Test MSE of 140462637569.173889
Epoch 8: training loss 146886708736.000
Test Loss of 122735729253.499878, Test MSE of 122735728367.533890
Epoch 9: training loss 136745907764.706
Test Loss of 116943424581.877396, Test MSE of 116943423430.058197
Epoch 10: training loss 134674936711.529
Test Loss of 113776241373.831131, Test MSE of 113776243315.798599
Epoch 11: training loss 131158454633.412
Test Loss of 110700838182.432571, Test MSE of 110700839286.647873
Epoch 12: training loss 127316798464.000
Test Loss of 107577481528.198013, Test MSE of 107577482288.056320
Epoch 13: training loss 123815038780.235
Test Loss of 104853019142.750870, Test MSE of 104853020134.468887
Epoch 14: training loss 119602465189.647
Test Loss of 100764491935.178345, Test MSE of 100764492145.862137
Epoch 15: training loss 116120661052.235
Test Loss of 97899040136.024063, Test MSE of 97899042126.906845
Epoch 16: training loss 112080392975.059
Test Loss of 94154859111.868607, Test MSE of 94154858121.256119
Epoch 17: training loss 108628672768.000
Test Loss of 91873027592.408981, Test MSE of 91873027341.580811
Epoch 18: training loss 104073168474.353
Test Loss of 88103169433.078888, Test MSE of 88103168961.713577
Epoch 19: training loss 100837911521.882
Test Loss of 85902564132.892899, Test MSE of 85902565766.398926
Epoch 20: training loss 96640291538.824
Test Loss of 81511063298.072632, Test MSE of 81511061414.739563
Epoch 21: training loss 92488700197.647
Test Loss of 78375264669.105713, Test MSE of 78375264681.895508
Epoch 22: training loss 89487003768.471
Test Loss of 76308151897.419388, Test MSE of 76308153216.455124
Epoch 23: training loss 85237086162.824
Test Loss of 71439943286.317841, Test MSE of 71439942887.056442
Epoch 24: training loss 82402831661.176
Test Loss of 70093228307.245895, Test MSE of 70093227989.108032
Epoch 25: training loss 78875610608.941
Test Loss of 65903240706.724037, Test MSE of 65903239451.901917
Epoch 26: training loss 75312118859.294
Test Loss of 64658543411.578995, Test MSE of 64658543301.987297
Epoch 27: training loss 71977292015.059
Test Loss of 60872492129.828362, Test MSE of 60872491260.239098
Epoch 28: training loss 69269168587.294
Test Loss of 57923941535.415222, Test MSE of 57923941593.949112
Epoch 29: training loss 66494153298.824
Test Loss of 55310949168.262779, Test MSE of 55310948692.913460
Epoch 30: training loss 63816100690.824
Test Loss of 54225968500.126762, Test MSE of 54225967198.172623
Epoch 31: training loss 60346783329.882
Test Loss of 49720445559.502197, Test MSE of 49720445984.503647
Epoch 32: training loss 58020053925.647
Test Loss of 45385864337.202866, Test MSE of 45385864653.725578
Epoch 33: training loss 55131311503.059
Test Loss of 46663825423.633591, Test MSE of 46663825431.629349
Epoch 34: training loss 52749023962.353
Test Loss of 45902992972.628265, Test MSE of 45902993054.060364
Epoch 35: training loss 50081200135.529
Test Loss of 41282117093.825584, Test MSE of 41282116673.108170
Epoch 36: training loss 48238942599.529
Test Loss of 41369007338.266945, Test MSE of 41369007037.234329
Epoch 37: training loss 46031960109.176
Test Loss of 37998420145.180664, Test MSE of 37998420649.183350
Epoch 38: training loss 43301676069.647
Test Loss of 37104382578.291000, Test MSE of 37104382690.390343
Epoch 39: training loss 41965265325.176
Test Loss of 35748284028.002777, Test MSE of 35748284743.508659
Epoch 40: training loss 39850787892.706
Test Loss of 34316167166.578766, Test MSE of 34316167838.414280
Epoch 41: training loss 38168227538.824
Test Loss of 34190138582.843395, Test MSE of 34190138233.374176
Epoch 42: training loss 35613833268.706
Test Loss of 33914308418.975712, Test MSE of 33914308089.010380
Epoch 43: training loss 34316118603.294
Test Loss of 29251428214.613926, Test MSE of 29251427949.559887
Epoch 44: training loss 32916632109.176
Test Loss of 27281051306.903538, Test MSE of 27281051554.112495
Epoch 45: training loss 31517170812.235
Test Loss of 28418871933.660885, Test MSE of 28418872373.549900
Epoch 46: training loss 30302780280.471
Test Loss of 28746422664.734676, Test MSE of 28746422831.819622
Epoch 47: training loss 28587797120.000
Test Loss of 26733271225.708073, Test MSE of 26733270967.911621
Epoch 48: training loss 28288071122.824
Test Loss of 23828552116.319221, Test MSE of 23828551885.332012
Epoch 49: training loss 26701500807.529
Test Loss of 24036708510.704605, Test MSE of 24036708444.168179
Epoch 50: training loss 25467601462.588
Test Loss of 26678529398.969234, Test MSE of 26678529232.568993
Epoch 51: training loss 24695783966.118
Test Loss of 22034371765.207497, Test MSE of 22034371889.266228
Epoch 52: training loss 23251391540.706
Test Loss of 21517684996.086052, Test MSE of 21517685045.706425
Epoch 53: training loss 22271183706.353
Test Loss of 21904161592.316448, Test MSE of 21904161584.998409
Epoch 54: training loss 21730352139.294
Test Loss of 23046112386.043026, Test MSE of 23046112731.197861
Epoch 55: training loss 20833743951.059
Test Loss of 19448560686.663891, Test MSE of 19448560566.004196
Epoch 56: training loss 20183289069.176
Test Loss of 19801909906.031921, Test MSE of 19801910162.231194
Epoch 57: training loss 19714146620.235
Test Loss of 20906365918.600971, Test MSE of 20906366228.993599
Epoch 58: training loss 19101168773.647
Test Loss of 21945945643.466110, Test MSE of 21945945769.159191
Epoch 59: training loss 18318762989.176
Test Loss of 19299772934.277122, Test MSE of 19299772706.811024
Epoch 60: training loss 17620143973.647
Test Loss of 18704719544.168400, Test MSE of 18704719543.731037
Epoch 61: training loss 17162299301.647
Test Loss of 18243703510.251213, Test MSE of 18243703435.167973
Epoch 62: training loss 16522647977.412
Test Loss of 19418912842.377979, Test MSE of 19418912871.184471
Epoch 63: training loss 16259256670.118
Test Loss of 18619607505.691418, Test MSE of 18619607390.404408
Epoch 64: training loss 15696546853.647
Test Loss of 19918783653.573906, Test MSE of 19918783720.155045
Epoch 65: training loss 15301672417.882
Test Loss of 16950904757.148277, Test MSE of 16950904813.608858
Epoch 66: training loss 14723545445.647
Test Loss of 19348758940.158222, Test MSE of 19348758830.362041
Epoch 67: training loss 14421799883.294
Test Loss of 19036007745.436039, Test MSE of 19036007482.536713
Epoch 68: training loss 14113030189.176
Test Loss of 18638119850.962757, Test MSE of 18638120038.557755
Epoch 69: training loss 13892056079.059
Test Loss of 19478676765.431412, Test MSE of 19478676800.408100
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19478676800.4081, 'MSE - std': 0.0, 'R2 - mean': 0.8483174303348653, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005417 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918037714.824
Test Loss of 424557091948.724487, Test MSE of 424557095576.553162
Epoch 2: training loss 427897768297.412
Test Loss of 424541264618.148499, Test MSE of 424541266087.583130
Epoch 3: training loss 427870327506.824
Test Loss of 424518832644.382141, Test MSE of 424518831903.955505
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887620577.882
Test Loss of 424525006209.865356, Test MSE of 424525000318.065002
Epoch 2: training loss 427878276879.059
Test Loss of 424526538885.122375, Test MSE of 424526543573.923462
Epoch 3: training loss 423980767834.353
Test Loss of 412290816814.367798, Test MSE of 412290818566.166321
Epoch 4: training loss 397029165056.000
Test Loss of 371201163101.031677, Test MSE of 371201159980.455383
Epoch 5: training loss 330402492175.059
Test Loss of 286011219192.953064, Test MSE of 286011219176.817627
Epoch 6: training loss 246719419693.176
Test Loss of 210369080562.320618, Test MSE of 210369081700.716431
Epoch 7: training loss 177782551190.588
Test Loss of 151909544041.171417, Test MSE of 151909546818.746857
Epoch 8: training loss 144171660830.118
Test Loss of 134174332671.703903, Test MSE of 134174331957.894165
Epoch 9: training loss 134776996502.588
Test Loss of 128711080960.710617, Test MSE of 128711079425.890533
Epoch 10: training loss 133520196969.412
Test Loss of 125662678633.526718, Test MSE of 125662678310.134674
Epoch 11: training loss 128913422366.118
Test Loss of 122249079622.055054, Test MSE of 122249079661.415604
Epoch 12: training loss 126144398275.765
Test Loss of 118729188084.097153, Test MSE of 118729187632.805954
Epoch 13: training loss 121971828916.706
Test Loss of 116367712286.319687, Test MSE of 116367712994.686386
Epoch 14: training loss 117796306913.882
Test Loss of 112438421980.113815, Test MSE of 112438421301.594955
Epoch 15: training loss 115001273193.412
Test Loss of 109276687280.173950, Test MSE of 109276689128.308121
Epoch 16: training loss 110159179896.471
Test Loss of 105431691280.107330, Test MSE of 105431690956.622086
Epoch 17: training loss 106896970992.941
Test Loss of 103333840801.724731, Test MSE of 103333841611.165863
Epoch 18: training loss 103208671653.647
Test Loss of 100117704561.876480, Test MSE of 100117705644.495483
Epoch 19: training loss 100702793637.647
Test Loss of 95831515440.144348, Test MSE of 95831514771.572876
Epoch 20: training loss 95249837357.176
Test Loss of 90092024906.141113, Test MSE of 90092025448.706284
Epoch 21: training loss 91905247232.000
Test Loss of 90319553763.634506, Test MSE of 90319554106.521194
Epoch 22: training loss 87716377103.059
Test Loss of 83962601724.032379, Test MSE of 83962600959.666977
Epoch 23: training loss 85265582607.059
Test Loss of 79417838011.899139, Test MSE of 79417836310.377319
Epoch 24: training loss 82296907535.059
Test Loss of 78961745600.695816, Test MSE of 78961745018.989761
Epoch 25: training loss 77559999171.765
Test Loss of 75128267747.812164, Test MSE of 75128268439.139908
Epoch 26: training loss 73764842300.235
Test Loss of 70919656917.244507, Test MSE of 70919656977.082642
Epoch 27: training loss 70888034153.412
Test Loss of 70113666864.973404, Test MSE of 70113666209.497711
Epoch 28: training loss 68385180160.000
Test Loss of 64383542285.501732, Test MSE of 64383543649.862495
Epoch 29: training loss 64585671499.294
Test Loss of 62291923151.500343, Test MSE of 62291923220.380005
Epoch 30: training loss 61933587380.706
Test Loss of 58467832692.955818, Test MSE of 58467831310.508873
Epoch 31: training loss 59012682752.000
Test Loss of 59700115697.136246, Test MSE of 59700115626.961388
Epoch 32: training loss 56829071360.000
Test Loss of 55946357585.424934, Test MSE of 55946357656.704590
Epoch 33: training loss 53458207849.412
Test Loss of 52589855911.942635, Test MSE of 52589858297.964272
Epoch 34: training loss 50842106006.588
Test Loss of 50774589714.298401, Test MSE of 50774587813.448967
Epoch 35: training loss 48609858620.235
Test Loss of 46430026022.669441, Test MSE of 46430027452.800583
Epoch 36: training loss 46005265121.882
Test Loss of 46905678075.558640, Test MSE of 46905679784.971039
Epoch 37: training loss 44079954334.118
Test Loss of 45100726676.578300, Test MSE of 45100726970.145081
Epoch 38: training loss 42309604698.353
Test Loss of 43255770956.924355, Test MSE of 43255770402.411278
Epoch 39: training loss 40417056655.059
Test Loss of 43971138960.077721, Test MSE of 43971138582.548965
Epoch 40: training loss 38296426081.882
Test Loss of 40554417826.849876, Test MSE of 40554417107.993874
Epoch 41: training loss 36319891696.941
Test Loss of 40765672454.395561, Test MSE of 40765671797.717033
Epoch 42: training loss 34808055785.412
Test Loss of 36086314622.845245, Test MSE of 36086314011.210068
Epoch 43: training loss 32866353272.471
Test Loss of 39860755974.040253, Test MSE of 39860756806.176674
Epoch 44: training loss 31298771373.176
Test Loss of 35931110740.859589, Test MSE of 35931109779.768318
Epoch 45: training loss 29657805199.059
Test Loss of 35634480678.254913, Test MSE of 35634481019.554955
Epoch 46: training loss 27957173353.412
Test Loss of 33518018063.515152, Test MSE of 33518017672.766720
Epoch 47: training loss 27174683233.882
Test Loss of 31348647003.669674, Test MSE of 31348646444.729553
Epoch 48: training loss 25714131719.529
Test Loss of 32286770741.414757, Test MSE of 32286769257.474857
Epoch 49: training loss 24836106330.353
Test Loss of 32321586393.212120, Test MSE of 32321586902.751431
Epoch 50: training loss 23391431371.294
Test Loss of 30775033936.062920, Test MSE of 30775033752.083050
Epoch 51: training loss 22110578861.176
Test Loss of 31566005126.010639, Test MSE of 31566005598.684391
Epoch 52: training loss 21602094192.941
Test Loss of 33340966448.203564, Test MSE of 33340966011.336903
Epoch 53: training loss 20337442032.941
Test Loss of 31201593459.593800, Test MSE of 31201593774.608093
Epoch 54: training loss 19779458191.059
Test Loss of 29242165498.611149, Test MSE of 29242165470.096951
Epoch 55: training loss 19341419166.118
Test Loss of 29828026943.126534, Test MSE of 29828026351.288925
Epoch 56: training loss 18270345238.588
Test Loss of 29928025293.368496, Test MSE of 29928024261.744629
Epoch 57: training loss 17640742234.353
Test Loss of 28171211559.261623, Test MSE of 28171211894.977421
Epoch 58: training loss 16977788329.412
Test Loss of 25694929113.922737, Test MSE of 25694929311.071823
Epoch 59: training loss 16482314443.294
Test Loss of 28406469561.411983, Test MSE of 28406470154.244228
Epoch 60: training loss 15778460431.059
Test Loss of 27818244816.092529, Test MSE of 27818245682.307671
Epoch 61: training loss 15236624173.176
Test Loss of 28480328194.487164, Test MSE of 28480328341.123444
Epoch 62: training loss 14896893824.000
Test Loss of 26823945658.714783, Test MSE of 26823946263.282074
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23151311531.845085, 'MSE - std': 3672634731.436987, 'R2 - mean': 0.8284061315970106, 'R2 - std': 0.019911298737854743} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926117616.941
Test Loss of 447258666720.436707, Test MSE of 447258670392.175781
Epoch 2: training loss 421904907685.647
Test Loss of 447240772330.148499, Test MSE of 447240771773.918945
Epoch 3: training loss 421877479183.059
Test Loss of 447217045872.573669, Test MSE of 447217049347.125854
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898036886.588
Test Loss of 447224779681.487854, Test MSE of 447224787235.982910
Epoch 2: training loss 421889193381.647
Test Loss of 447225137741.812622, Test MSE of 447225132652.871277
Epoch 3: training loss 418044804035.765
Test Loss of 434919764402.898010, Test MSE of 434919765987.902954
Epoch 4: training loss 391272042977.882
Test Loss of 393092951953.143677, Test MSE of 393092954930.968628
Epoch 5: training loss 325276946552.471
Test Loss of 304424838785.687744, Test MSE of 304424839758.536621
Epoch 6: training loss 241857754955.294
Test Loss of 226251563332.041626, Test MSE of 226251561790.656128
Epoch 7: training loss 174579507681.882
Test Loss of 164036646460.520935, Test MSE of 164036642875.952362
Epoch 8: training loss 141957562368.000
Test Loss of 145488541905.869080, Test MSE of 145488543933.430328
Epoch 9: training loss 132533820928.000
Test Loss of 138718157572.678223, Test MSE of 138718159125.756714
Epoch 10: training loss 128700947275.294
Test Loss of 135001103411.875092, Test MSE of 135001100918.695465
Epoch 11: training loss 126128686742.588
Test Loss of 131751993107.127457, Test MSE of 131751992460.468506
Epoch 12: training loss 123614922541.176
Test Loss of 128839686219.325470, Test MSE of 128839686546.687363
Epoch 13: training loss 119178095947.294
Test Loss of 125197945164.095306, Test MSE of 125197944199.957489
Epoch 14: training loss 116522410255.059
Test Loss of 121684438690.376129, Test MSE of 121684439551.623932
Epoch 15: training loss 111325530925.176
Test Loss of 118310452319.459641, Test MSE of 118310452206.678375
Epoch 16: training loss 107861611580.235
Test Loss of 114004402204.424698, Test MSE of 114004403945.195816
Epoch 17: training loss 103488853534.118
Test Loss of 110770620456.268326, Test MSE of 110770622703.399109
Epoch 18: training loss 101101864658.824
Test Loss of 108500249789.261154, Test MSE of 108500249166.528564
Epoch 19: training loss 95947461120.000
Test Loss of 102527987347.216278, Test MSE of 102527986564.685257
Epoch 20: training loss 92503639341.176
Test Loss of 101679015551.792740, Test MSE of 101679017093.821182
Epoch 21: training loss 88481412788.706
Test Loss of 94954421682.661118, Test MSE of 94954424594.029495
Epoch 22: training loss 86103603651.765
Test Loss of 92587561745.469345, Test MSE of 92587562662.958008
Epoch 23: training loss 81973209886.118
Test Loss of 87869447269.144577, Test MSE of 87869448180.178436
Epoch 24: training loss 78089540577.882
Test Loss of 84254213051.070084, Test MSE of 84254212831.267929
Epoch 25: training loss 74969582486.588
Test Loss of 80118224219.018280, Test MSE of 80118223823.761505
Epoch 26: training loss 71373239898.353
Test Loss of 75950209567.385605, Test MSE of 75950207406.622986
Epoch 27: training loss 67733074838.588
Test Loss of 72345202975.089523, Test MSE of 72345202460.704788
Epoch 28: training loss 64982975051.294
Test Loss of 68152413377.051125, Test MSE of 68152412205.228951
Epoch 29: training loss 62587119480.471
Test Loss of 67650743811.197777, Test MSE of 67650746185.729179
Epoch 30: training loss 59833245108.706
Test Loss of 64159887624.349754, Test MSE of 64159888393.520576
Epoch 31: training loss 57777605526.588
Test Loss of 63755173541.218597, Test MSE of 63755172479.113945
Epoch 32: training loss 54117313385.412
Test Loss of 59663284111.722412, Test MSE of 59663284604.547386
Epoch 33: training loss 51567328451.765
Test Loss of 54855678303.992599, Test MSE of 54855678316.451309
Epoch 34: training loss 49596514838.588
Test Loss of 56002107114.859123, Test MSE of 56002107478.737602
Epoch 35: training loss 47202072380.235
Test Loss of 48341614763.495720, Test MSE of 48341615336.782867
Epoch 36: training loss 44170366464.000
Test Loss of 47293927038.371498, Test MSE of 47293927747.241936
Epoch 37: training loss 43307684946.824
Test Loss of 48740878723.049736, Test MSE of 48740879088.002197
Epoch 38: training loss 40716295582.118
Test Loss of 44446432933.218597, Test MSE of 44446432610.864967
Epoch 39: training loss 38843322533.647
Test Loss of 47671557150.082809, Test MSE of 47671557056.655205
Epoch 40: training loss 37112623465.412
Test Loss of 43409836228.604210, Test MSE of 43409836430.355179
Epoch 41: training loss 35277524577.882
Test Loss of 38215697544.675453, Test MSE of 38215697220.484955
Epoch 42: training loss 33756372080.941
Test Loss of 40195182706.646309, Test MSE of 40195182685.006538
Epoch 43: training loss 32012356924.235
Test Loss of 37978094913.909782, Test MSE of 37978095194.096146
Epoch 44: training loss 30375168971.294
Test Loss of 37251867727.826050, Test MSE of 37251868414.949730
Epoch 45: training loss 29275075343.059
Test Loss of 32621152373.962524, Test MSE of 32621151888.342411
Epoch 46: training loss 27943601840.941
Test Loss of 32190829322.836918, Test MSE of 32190829507.944889
Epoch 47: training loss 26940019922.824
Test Loss of 33620258036.215591, Test MSE of 33620258304.447865
Epoch 48: training loss 25479962304.000
Test Loss of 29927337703.542912, Test MSE of 29927338360.925728
Epoch 49: training loss 24382948291.765
Test Loss of 28927407050.229935, Test MSE of 28927406721.631950
Epoch 50: training loss 23284060201.412
Test Loss of 28345995659.577145, Test MSE of 28345995456.663681
Epoch 51: training loss 22598031088.941
Test Loss of 25326125092.004627, Test MSE of 25326125180.534485
Epoch 52: training loss 21827818458.353
Test Loss of 29775068545.391624, Test MSE of 29775068249.508858
Epoch 53: training loss 20731072301.176
Test Loss of 25853918052.848484, Test MSE of 25853917986.946983
Epoch 54: training loss 19930399939.765
Test Loss of 25746322376.808697, Test MSE of 25746322505.444988
Epoch 55: training loss 19239333184.000
Test Loss of 25100649216.888271, Test MSE of 25100649614.756290
Epoch 56: training loss 18638477933.176
Test Loss of 27192221581.353691, Test MSE of 27192221438.388287
Epoch 57: training loss 17890549063.529
Test Loss of 26135445840.359009, Test MSE of 26135446022.711670
Epoch 58: training loss 17344975533.176
Test Loss of 22323385600.059219, Test MSE of 22323385829.488335
Epoch 59: training loss 16817953483.294
Test Loss of 24814669911.169094, Test MSE of 24814669490.075901
Epoch 60: training loss 16640257404.235
Test Loss of 21967127457.014111, Test MSE of 21967127843.913029
Epoch 61: training loss 15959853760.000
Test Loss of 23778073150.179043, Test MSE of 23778073139.553509
Epoch 62: training loss 15224970285.176
Test Loss of 25615542404.174877, Test MSE of 25615541891.129513
Epoch 63: training loss 15225369630.118
Test Loss of 21203086763.081192, Test MSE of 21203087063.262062
Epoch 64: training loss 14690128143.059
Test Loss of 22715773593.138100, Test MSE of 22715773859.124748
Epoch 65: training loss 14175800207.059
Test Loss of 21414582064.499653, Test MSE of 21414582189.937561
Epoch 66: training loss 13826745091.765
Test Loss of 23390016548.241501, Test MSE of 23390016428.273037
Epoch 67: training loss 13349566656.000
Test Loss of 22145802273.162155, Test MSE of 22145802351.995445
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22816141805.22854, 'MSE - std': 3035925131.650193, 'R2 - mean': 0.8364630394768854, 'R2 - std': 0.01985281023234844} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005300 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109831408.941
Test Loss of 410765593211.913025, Test MSE of 410765596548.791077
Epoch 2: training loss 430088912896.000
Test Loss of 410747587764.538635, Test MSE of 410747590967.461548
Epoch 3: training loss 430061733044.706
Test Loss of 410724280330.424805, Test MSE of 410724276271.898560
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078021632.000
Test Loss of 410727476567.544678, Test MSE of 410727474203.451355
Epoch 2: training loss 430067343119.059
Test Loss of 410727873923.139282, Test MSE of 410727869112.227295
Epoch 3: training loss 426252544361.412
Test Loss of 398787567271.033752, Test MSE of 398787567629.720032
Epoch 4: training loss 399530895962.353
Test Loss of 357362726139.616821, Test MSE of 357362726158.326965
Epoch 5: training loss 332963816508.235
Test Loss of 271559410475.239227, Test MSE of 271559413774.641876
Epoch 6: training loss 250164557161.412
Test Loss of 192687636722.613617, Test MSE of 192687639467.224548
Epoch 7: training loss 182477379343.059
Test Loss of 136015369812.583069, Test MSE of 136015370578.243866
Epoch 8: training loss 149695582659.765
Test Loss of 116588743072.044418, Test MSE of 116588744297.981110
Epoch 9: training loss 139539726546.824
Test Loss of 111422102600.499771, Test MSE of 111422101062.591949
Epoch 10: training loss 136345410198.588
Test Loss of 108433661345.465988, Test MSE of 108433660692.091690
Epoch 11: training loss 132470255616.000
Test Loss of 105606443946.232300, Test MSE of 105606444150.148056
Epoch 12: training loss 130035360406.588
Test Loss of 103201442947.257751, Test MSE of 103201443110.964066
Epoch 13: training loss 124992756766.118
Test Loss of 100033010400.370193, Test MSE of 100033010103.580627
Epoch 14: training loss 121794255299.765
Test Loss of 96994587649.421570, Test MSE of 96994588111.199142
Epoch 15: training loss 119227606949.647
Test Loss of 93680937057.614075, Test MSE of 93680936317.453812
Epoch 16: training loss 114534840801.882
Test Loss of 90261288301.341965, Test MSE of 90261290583.473999
Epoch 17: training loss 109081584670.118
Test Loss of 87087842913.850998, Test MSE of 87087841721.494217
Epoch 18: training loss 107068505419.294
Test Loss of 84503478533.567795, Test MSE of 84503477573.090057
Epoch 19: training loss 102744070174.118
Test Loss of 81491214957.697357, Test MSE of 81491212783.328232
Epoch 20: training loss 98416243169.882
Test Loss of 78655056811.180008, Test MSE of 78655058229.933517
Epoch 21: training loss 95392825916.235
Test Loss of 75493303181.801025, Test MSE of 75493303722.405548
Epoch 22: training loss 92315853568.000
Test Loss of 72610405592.551590, Test MSE of 72610405542.187073
Epoch 23: training loss 87987494927.059
Test Loss of 68504588236.823692, Test MSE of 68504587310.978539
Epoch 24: training loss 84341035851.294
Test Loss of 66899677868.246185, Test MSE of 66899677037.627403
Epoch 25: training loss 80377114337.882
Test Loss of 64440320253.986115, Test MSE of 64440321159.212944
Epoch 26: training loss 77789285767.529
Test Loss of 62327164558.393333, Test MSE of 62327164173.543877
Epoch 27: training loss 74777217099.294
Test Loss of 61001288478.919022, Test MSE of 61001288308.169975
Epoch 28: training loss 71155350663.529
Test Loss of 56246738030.408142, Test MSE of 56246737896.760323
Epoch 29: training loss 68478750147.765
Test Loss of 56237798192.925499, Test MSE of 56237799105.044830
Epoch 30: training loss 65091059568.941
Test Loss of 52999108106.661728, Test MSE of 52999109287.986290
Epoch 31: training loss 62316483734.588
Test Loss of 51354289507.864876, Test MSE of 51354288522.119980
Epoch 32: training loss 60161143205.647
Test Loss of 48928368786.894958, Test MSE of 48928367181.129654
Epoch 33: training loss 56893647676.235
Test Loss of 45246755011.228134, Test MSE of 45246754797.380974
Epoch 34: training loss 54329036754.824
Test Loss of 42723805920.370201, Test MSE of 42723805238.020866
Epoch 35: training loss 52085264225.882
Test Loss of 41320603177.936142, Test MSE of 41320603267.115570
Epoch 36: training loss 49753824233.412
Test Loss of 39078897771.091164, Test MSE of 39078897407.572235
Epoch 37: training loss 46734190832.941
Test Loss of 38729157056.266541, Test MSE of 38729157027.098495
Epoch 38: training loss 45179967134.118
Test Loss of 36592247854.911613, Test MSE of 36592247939.712967
Epoch 39: training loss 42997483429.647
Test Loss of 34317284806.426655, Test MSE of 34317284329.357590
Epoch 40: training loss 41364249366.588
Test Loss of 34343601475.642757, Test MSE of 34343601755.725597
Epoch 41: training loss 39021236141.176
Test Loss of 32942608200.144379, Test MSE of 32942608217.748581
Epoch 42: training loss 37759911680.000
Test Loss of 32621615618.606201, Test MSE of 32621616056.462978
Epoch 43: training loss 35701443764.706
Test Loss of 29704305503.363258, Test MSE of 29704305658.097836
Epoch 44: training loss 34114279555.765
Test Loss of 27702977473.925034, Test MSE of 27702977316.199886
Epoch 45: training loss 32432703887.059
Test Loss of 25597013832.144379, Test MSE of 25597013884.945213
Epoch 46: training loss 31541327804.235
Test Loss of 26373248253.512264, Test MSE of 26373248000.324127
Epoch 47: training loss 29789435843.765
Test Loss of 26719069936.007404, Test MSE of 26719070070.176865
Epoch 48: training loss 28473474462.118
Test Loss of 25213715204.383156, Test MSE of 25213715158.612171
Epoch 49: training loss 27222133146.353
Test Loss of 25296980898.650623, Test MSE of 25296980954.161114
Epoch 50: training loss 26346606226.824
Test Loss of 22447973719.544655, Test MSE of 22447973835.706905
Epoch 51: training loss 25153127796.706
Test Loss of 25249643832.270245, Test MSE of 25249643891.241207
Epoch 52: training loss 23871548242.824
Test Loss of 22890402856.277649, Test MSE of 22890402531.380428
Epoch 53: training loss 23355227200.000
Test Loss of 21589839125.204998, Test MSE of 21589838969.118393
Epoch 54: training loss 22152517586.824
Test Loss of 22572145097.743637, Test MSE of 22572145113.290657
Epoch 55: training loss 21600017084.235
Test Loss of 22136537642.409996, Test MSE of 22136537614.501881
Epoch 56: training loss 20695374121.412
Test Loss of 21822646947.716797, Test MSE of 21822646590.889324
Epoch 57: training loss 20213866413.176
Test Loss of 18863551521.643684, Test MSE of 18863551425.428036
Epoch 58: training loss 19608479834.353
Test Loss of 19695901351.507637, Test MSE of 19695901049.890625
Epoch 59: training loss 18857405323.294
Test Loss of 21470988469.960205, Test MSE of 21470988094.834961
Epoch 60: training loss 18002289637.647
Test Loss of 18411771493.167976, Test MSE of 18411771459.303932
Epoch 61: training loss 17918348457.412
Test Loss of 19545156610.369274, Test MSE of 19545156396.293522
Epoch 62: training loss 17085145660.235
Test Loss of 20031525244.979176, Test MSE of 20031525220.510391
Epoch 63: training loss 16672165285.647
Test Loss of 18960984185.306801, Test MSE of 18960984151.359863
Epoch 64: training loss 16180834409.412
Test Loss of 18382949809.577049, Test MSE of 18382949601.181660
Epoch 65: training loss 15694604129.882
Test Loss of 18819148038.515503, Test MSE of 18819147821.789307
Epoch 66: training loss 15575955437.176
Test Loss of 18342754253.771400, Test MSE of 18342754090.208218
Epoch 67: training loss 15166664948.706
Test Loss of 18123840237.638130, Test MSE of 18123840217.951954
Epoch 68: training loss 14579379237.647
Test Loss of 18852925064.707081, Test MSE of 18852925059.425152
Epoch 69: training loss 14279174091.294
Test Loss of 18085441837.371586, Test MSE of 18085441788.892525
Epoch 70: training loss 13987198196.706
Test Loss of 18745145193.314205, Test MSE of 18745145605.452301
Epoch 71: training loss 13359887075.765
Test Loss of 17314819367.211475, Test MSE of 17314819092.060600
Epoch 72: training loss 13174112160.000
Test Loss of 18415206529.836185, Test MSE of 18415206585.715252
Epoch 73: training loss 12899147064.471
Test Loss of 17410477147.453957, Test MSE of 17410476943.360947
Epoch 74: training loss 12629885129.412
Test Loss of 18433302378.261917, Test MSE of 18433302737.672848
Epoch 75: training loss 12260447092.706
Test Loss of 18679090758.367424, Test MSE of 18679090729.869953
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21781879036.388893, 'MSE - std': 3181466561.0002074, 'R2 - mean': 0.838805211956502, 'R2 - std': 0.01766515981711038} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005391 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042617675.294
Test Loss of 431613008892.683044, Test MSE of 431613014360.586060
Epoch 2: training loss 424022316815.059
Test Loss of 431591899613.645508, Test MSE of 431591908310.440186
Epoch 3: training loss 423994543525.647
Test Loss of 431563338518.389648, Test MSE of 431563339405.367249
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009688606.118
Test Loss of 431567851358.889404, Test MSE of 431567854662.606445
Epoch 2: training loss 423997884536.471
Test Loss of 431570550010.669128, Test MSE of 431570547411.348511
Epoch 3: training loss 420158057050.353
Test Loss of 419109523173.582581, Test MSE of 419109526242.295715
Epoch 4: training loss 393620581918.118
Test Loss of 376825174490.802429, Test MSE of 376825176913.435303
Epoch 5: training loss 327728588860.235
Test Loss of 289724369274.136047, Test MSE of 289724366769.969971
Epoch 6: training loss 245392026804.706
Test Loss of 211024935504.318359, Test MSE of 211024936499.256348
Epoch 7: training loss 177779340348.235
Test Loss of 148139192397.712158, Test MSE of 148139192973.800476
Epoch 8: training loss 144589140239.059
Test Loss of 128847280075.875977, Test MSE of 128847278399.510391
Epoch 9: training loss 135953039631.059
Test Loss of 122971671565.741791, Test MSE of 122971673925.048798
Epoch 10: training loss 133158764062.118
Test Loss of 119387869947.853775, Test MSE of 119387869439.140732
Epoch 11: training loss 130549451053.176
Test Loss of 116611252007.922256, Test MSE of 116611251022.384567
Epoch 12: training loss 127029534659.765
Test Loss of 113802257714.583984, Test MSE of 113802256489.521408
Epoch 13: training loss 123046824688.941
Test Loss of 109518085101.519669, Test MSE of 109518087547.420761
Epoch 14: training loss 119007773906.824
Test Loss of 106245537378.798706, Test MSE of 106245537162.272812
Epoch 15: training loss 116496148419.765
Test Loss of 103549415895.959274, Test MSE of 103549413451.101151
Epoch 16: training loss 112229975416.471
Test Loss of 99009294929.266083, Test MSE of 99009295612.056778
Epoch 17: training loss 107654001438.118
Test Loss of 94771247317.234619, Test MSE of 94771246044.770370
Epoch 18: training loss 104464055567.059
Test Loss of 92769480793.084686, Test MSE of 92769480543.533783
Epoch 19: training loss 100518256173.176
Test Loss of 88698327015.359558, Test MSE of 88698327485.245193
Epoch 20: training loss 96783280338.824
Test Loss of 84385776351.422485, Test MSE of 84385778547.741013
Epoch 21: training loss 93274445312.000
Test Loss of 83042586830.600647, Test MSE of 83042585000.729874
Epoch 22: training loss 89712325963.294
Test Loss of 77459371109.878754, Test MSE of 77459371430.718475
Epoch 23: training loss 86067153739.294
Test Loss of 76083126243.568726, Test MSE of 76083127171.524612
Epoch 24: training loss 82428960481.882
Test Loss of 71762021706.276718, Test MSE of 71762022164.372467
Epoch 25: training loss 79575455849.412
Test Loss of 69258324179.339188, Test MSE of 69258325544.258652
Epoch 26: training loss 76055389680.941
Test Loss of 64520806572.483109, Test MSE of 64520806468.329597
Epoch 27: training loss 72611505001.412
Test Loss of 64035277079.574272, Test MSE of 64035278300.265099
Epoch 28: training loss 69329624530.824
Test Loss of 60341415102.015732, Test MSE of 60341414651.694633
Epoch 29: training loss 66894402236.235
Test Loss of 55247228351.792686, Test MSE of 55247228986.057426
Epoch 30: training loss 64212459143.529
Test Loss of 52377421955.731606, Test MSE of 52377421444.899567
Epoch 31: training loss 60661227760.941
Test Loss of 52652323657.565941, Test MSE of 52652322367.922142
Epoch 32: training loss 58497034906.353
Test Loss of 49364340895.215179, Test MSE of 49364339604.938660
Epoch 33: training loss 55972151992.471
Test Loss of 50540776691.087456, Test MSE of 50540777413.691040
Epoch 34: training loss 53588595117.176
Test Loss of 46831976853.619621, Test MSE of 46831976413.743019
Epoch 35: training loss 51115116672.000
Test Loss of 45781889539.553909, Test MSE of 45781889234.697464
Epoch 36: training loss 48254915275.294
Test Loss of 41782472362.824615, Test MSE of 41782472109.114067
Epoch 37: training loss 46787724480.000
Test Loss of 40762991374.807961, Test MSE of 40762990580.808784
Epoch 38: training loss 44864548562.824
Test Loss of 37873737646.496994, Test MSE of 37873737825.713089
Epoch 39: training loss 42406505547.294
Test Loss of 34014711375.844517, Test MSE of 34014711772.769104
Epoch 40: training loss 40172190366.118
Test Loss of 34370058244.738548, Test MSE of 34370058211.963417
Epoch 41: training loss 39171756084.706
Test Loss of 35693455142.026840, Test MSE of 35693456068.689651
Epoch 42: training loss 36732111367.529
Test Loss of 32064394355.146690, Test MSE of 32064394344.522114
Epoch 43: training loss 35675315538.824
Test Loss of 31724049567.689034, Test MSE of 31724050202.046516
Epoch 44: training loss 33952315075.765
Test Loss of 29193319028.331329, Test MSE of 29193318582.633995
Epoch 45: training loss 32291736997.647
Test Loss of 30022760852.671909, Test MSE of 30022760826.482235
Epoch 46: training loss 31084577807.059
Test Loss of 28263507975.581676, Test MSE of 28263508089.579964
Epoch 47: training loss 29489493010.824
Test Loss of 25042550084.590466, Test MSE of 25042550072.130295
Epoch 48: training loss 28650985464.471
Test Loss of 27537656481.821381, Test MSE of 27537656611.762619
Epoch 49: training loss 27413500272.941
Test Loss of 27617579371.920406, Test MSE of 27617579367.699986
Epoch 50: training loss 26238084984.471
Test Loss of 24106218727.714947, Test MSE of 24106218951.017406
Epoch 51: training loss 25080023672.471
Test Loss of 21845689863.818604, Test MSE of 21845690470.734550
Epoch 52: training loss 24492008470.588
Test Loss of 24262448970.039795, Test MSE of 24262448934.746941
Epoch 53: training loss 23284106996.706
Test Loss of 23853297590.078667, Test MSE of 23853297721.695107
Epoch 54: training loss 22388021462.588
Test Loss of 22981080578.132347, Test MSE of 22981080937.760227
Epoch 55: training loss 21594809468.235
Test Loss of 21030221887.496529, Test MSE of 21030222032.762825
Epoch 56: training loss 21055711774.118
Test Loss of 20303991167.822304, Test MSE of 20303990739.211143
Epoch 57: training loss 20174206377.412
Test Loss of 20284728437.989819, Test MSE of 20284728441.024841
Epoch 58: training loss 19759870674.824
Test Loss of 21068081399.352150, Test MSE of 21068081338.118690
Epoch 59: training loss 18799433110.588
Test Loss of 21055408999.892643, Test MSE of 21055408997.259312
Epoch 60: training loss 18479739169.882
Test Loss of 20037301503.407681, Test MSE of 20037301567.382511
Epoch 61: training loss 17822320478.118
Test Loss of 20698667505.547432, Test MSE of 20698667891.635883
Epoch 62: training loss 17571326720.000
Test Loss of 19879172025.869503, Test MSE of 19879171942.748692
Epoch 63: training loss 17018243523.765
Test Loss of 20396294561.939842, Test MSE of 20396294321.361664
Epoch 64: training loss 16257273656.471
Test Loss of 19490879964.697826, Test MSE of 19490879965.030365
Epoch 65: training loss 15913533455.059
Test Loss of 20859408140.438686, Test MSE of 20859407851.513000
Epoch 66: training loss 15772903367.529
Test Loss of 19909271303.700138, Test MSE of 19909271220.323463
Epoch 67: training loss 15266874959.059
Test Loss of 20473226717.171680, Test MSE of 20473226939.530651
Epoch 68: training loss 14883954680.471
Test Loss of 19308925980.905136, Test MSE of 19308925802.084965
Epoch 69: training loss 14510753950.118
Test Loss of 19137477867.505783, Test MSE of 19137477936.211071
Epoch 70: training loss 14519615239.529
Test Loss of 19717826839.574272, Test MSE of 19717826839.163769
Epoch 71: training loss 13814425449.412
Test Loss of 19702471024.658955, Test MSE of 19702470834.111534
Epoch 72: training loss 13765946782.118
Test Loss of 20266563361.288292, Test MSE of 20266563488.612129
Epoch 73: training loss 13371067873.882
Test Loss of 20142823442.006477, Test MSE of 20142823883.199806
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21454068005.751076, 'MSE - std': 2920141070.2637815, 'R2 - mean': 0.8409586982437718, 'R2 - std': 0.01637670021246884} 
 

Saving model.....
Results After CV: {'MSE - mean': 21454068005.751076, 'MSE - std': 2920141070.2637815, 'R2 - mean': 0.8409586982437718, 'R2 - std': 0.01637670021246884}
Train time: 105.11389027480182
Inference time: 0.06849738799792249
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 89 finished with value: 21454068005.751076 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003962 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[111]	valid_0's l2: 1.2134e+10
Model Interpreting...
[(23,), (22,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 427524512707.765
Test Loss of 418111138595.945435, Test MSE of 418111140703.992859
Epoch 2: training loss 427502876190.118
Test Loss of 418093119523.530884, Test MSE of 418093125961.682068
Epoch 3: training loss 427475752116.706
Test Loss of 418069632038.136475, Test MSE of 418069632159.876709
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427493779094.588
Test Loss of 418073804451.323608, Test MSE of 418073805734.850098
Epoch 2: training loss 427481889370.353
Test Loss of 418075849468.624573, Test MSE of 418075851430.474121
Epoch 3: training loss 427481496636.235
Test Loss of 418075414995.112671, Test MSE of 418075413453.985413
Epoch 4: training loss 422085235651.765
Test Loss of 401237168924.839233, Test MSE of 401237172913.512390
Epoch 5: training loss 385672528956.235
Test Loss of 346241758452.689331, Test MSE of 346241764287.138245
Epoch 6: training loss 318022140024.471
Test Loss of 268256601092.974335, Test MSE of 268256604247.355591
Epoch 7: training loss 225895026507.294
Test Loss of 170638749820.121216, Test MSE of 170638750750.477081
Epoch 8: training loss 161407604103.529
Test Loss of 130858113269.873703, Test MSE of 130858113620.177658
Epoch 9: training loss 141868299866.353
Test Loss of 120405197095.616928, Test MSE of 120405199808.312286
Epoch 10: training loss 136489515685.647
Test Loss of 115994527477.281525, Test MSE of 115994525862.575333
Epoch 11: training loss 132815803843.765
Test Loss of 113032321599.600281, Test MSE of 113032320714.159500
Epoch 12: training loss 130159879514.353
Test Loss of 109921236269.538742, Test MSE of 109921235544.029694
Epoch 13: training loss 126620880655.059
Test Loss of 107315034139.950958, Test MSE of 107315035185.708542
Epoch 14: training loss 122363381940.706
Test Loss of 103763141043.845474, Test MSE of 103763140881.667877
Epoch 15: training loss 118608224677.647
Test Loss of 100544669679.418915, Test MSE of 100544668397.361099
Epoch 16: training loss 114931603034.353
Test Loss of 97002374025.800598, Test MSE of 97002373917.677399
Epoch 17: training loss 111409927875.765
Test Loss of 92772038996.622711, Test MSE of 92772040360.501663
Epoch 18: training loss 107205793942.588
Test Loss of 90623266987.495728, Test MSE of 90623269891.365021
Epoch 19: training loss 102884493869.176
Test Loss of 85954136928.584778, Test MSE of 85954137055.966400
Epoch 20: training loss 99026296485.647
Test Loss of 84493813250.724030, Test MSE of 84493814091.228638
Epoch 21: training loss 95484940958.118
Test Loss of 81201200555.081192, Test MSE of 81201199958.706573
Epoch 22: training loss 93291497615.059
Test Loss of 77647233827.471664, Test MSE of 77647233420.220078
Epoch 23: training loss 89394094802.824
Test Loss of 73936353840.440430, Test MSE of 73936353826.437088
Epoch 24: training loss 83490114785.882
Test Loss of 72144382176.081421, Test MSE of 72144381851.561218
Epoch 25: training loss 81036266194.824
Test Loss of 69537102600.468201, Test MSE of 69537102988.446533
Epoch 26: training loss 76527179271.529
Test Loss of 65048147330.812859, Test MSE of 65048146783.339615
Epoch 27: training loss 73762679461.647
Test Loss of 62602598861.901459, Test MSE of 62602598898.732231
Epoch 28: training loss 70365112395.294
Test Loss of 60284362668.620865, Test MSE of 60284363571.819557
Epoch 29: training loss 67080064820.706
Test Loss of 58957612101.403656, Test MSE of 58957612118.730682
Epoch 30: training loss 63874038283.294
Test Loss of 52784612514.257690, Test MSE of 52784613149.963776
Epoch 31: training loss 60888892506.353
Test Loss of 52672744066.872078, Test MSE of 52672743661.054329
Epoch 32: training loss 58753831713.882
Test Loss of 52040195035.758499, Test MSE of 52040195162.771339
Epoch 33: training loss 55122790490.353
Test Loss of 45898846136.938240, Test MSE of 45898846579.481110
Epoch 34: training loss 52596580013.176
Test Loss of 45400218403.945412, Test MSE of 45400218515.487305
Epoch 35: training loss 50382305566.118
Test Loss of 42220067912.719872, Test MSE of 42220068225.524025
Epoch 36: training loss 46959983969.882
Test Loss of 38937036243.349525, Test MSE of 38937035993.664177
Epoch 37: training loss 45181803928.471
Test Loss of 39541217086.238258, Test MSE of 39541217420.022522
Epoch 38: training loss 42948970251.294
Test Loss of 33962060038.928520, Test MSE of 33962060456.211281
Epoch 39: training loss 40896285688.471
Test Loss of 33442842297.589638, Test MSE of 33442842004.013859
Epoch 40: training loss 38911096018.824
Test Loss of 34281105816.842007, Test MSE of 34281106500.432831
Epoch 41: training loss 36914601682.824
Test Loss of 29618409568.643997, Test MSE of 29618409653.480076
Epoch 42: training loss 35285860186.353
Test Loss of 26539829955.301411, Test MSE of 26539829653.118958
Epoch 43: training loss 34064945543.529
Test Loss of 27516472554.740688, Test MSE of 27516472682.047874
Epoch 44: training loss 32466690913.882
Test Loss of 29042608102.891510, Test MSE of 29042608183.583977
Epoch 45: training loss 30316741601.882
Test Loss of 25787667731.009022, Test MSE of 25787667902.569550
Epoch 46: training loss 29215695006.118
Test Loss of 25863392304.085125, Test MSE of 25863392872.918407
Epoch 47: training loss 27765498345.412
Test Loss of 25287107699.830673, Test MSE of 25287107973.825722
Epoch 48: training loss 26838326896.941
Test Loss of 27850399900.335876, Test MSE of 27850400015.818535
Epoch 49: training loss 25725925560.471
Test Loss of 22483421512.305344, Test MSE of 22483421463.376362
Epoch 50: training loss 24798760459.294
Test Loss of 24560395900.002777, Test MSE of 24560396102.293003
Epoch 51: training loss 23917421756.235
Test Loss of 22047568340.533890, Test MSE of 22047568496.201714
Epoch 52: training loss 22727569163.294
Test Loss of 21831162278.580616, Test MSE of 21831162346.333202
Epoch 53: training loss 21786502343.529
Test Loss of 21480569314.746243, Test MSE of 21480568943.846127
Epoch 54: training loss 21245509421.176
Test Loss of 23408985674.733288, Test MSE of 23408985450.702526
Epoch 55: training loss 20356717526.588
Test Loss of 22744881196.058292, Test MSE of 22744881712.522114
Epoch 56: training loss 20055892536.471
Test Loss of 21742351009.191765, Test MSE of 21742351331.756058
Epoch 57: training loss 18922787595.294
Test Loss of 19890572756.770760, Test MSE of 19890572757.825027
Epoch 58: training loss 18315518023.529
Test Loss of 21750728845.649780, Test MSE of 21750728772.494606
Epoch 59: training loss 17581943612.235
Test Loss of 20318457289.400879, Test MSE of 20318457253.770264
Epoch 60: training loss 17380126238.118
Test Loss of 19896158328.804996, Test MSE of 19896158482.161633
Epoch 61: training loss 16844996073.412
Test Loss of 19439864210.209576, Test MSE of 19439863962.433773
Epoch 62: training loss 16394455529.412
Test Loss of 19692587043.057137, Test MSE of 19692587042.127922
Epoch 63: training loss 15681559958.588
Test Loss of 20214707754.518620, Test MSE of 20214707705.885605
Epoch 64: training loss 15260057897.412
Test Loss of 19440407836.010178, Test MSE of 19440408097.313606
Epoch 65: training loss 14962571888.941
Test Loss of 19641787294.645386, Test MSE of 19641787237.876816
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19641787237.876816, 'MSE - std': 0.0, 'R2 - mean': 0.8470472716609485, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005577 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[125]	valid_0's l2: 1.86498e+10
Model Interpreting...
[(25,), (25,), (25,), (25,), (25,)]

Train embedding model...
Epoch 1: training loss 427916448225.882
Test Loss of 424556284906.918335, Test MSE of 424556291753.645386
Epoch 2: training loss 427893947452.235
Test Loss of 424538463374.834167, Test MSE of 424538466991.377380
Epoch 3: training loss 427866187173.647
Test Loss of 424515785364.400635, Test MSE of 424515787086.470459
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884173914.353
Test Loss of 424521704108.087891, Test MSE of 424521703691.758240
Epoch 2: training loss 427874818891.294
Test Loss of 424522489812.415466, Test MSE of 424522490994.213135
Epoch 3: training loss 427874299663.059
Test Loss of 424522145153.391602, Test MSE of 424522139848.580933
Epoch 4: training loss 422189132619.294
Test Loss of 407141286949.188965, Test MSE of 407141290283.319580
Epoch 5: training loss 384774920914.824
Test Loss of 352670029334.621338, Test MSE of 352670033135.540833
Epoch 6: training loss 315958355124.706
Test Loss of 275339652531.608582, Test MSE of 275339647246.255798
Epoch 7: training loss 223296643312.941
Test Loss of 179940165236.422852, Test MSE of 179940164310.703400
Epoch 8: training loss 159959249016.471
Test Loss of 141621122521.981964, Test MSE of 141621123502.246918
Epoch 9: training loss 139745958038.588
Test Loss of 131905456249.752487, Test MSE of 131905456601.969437
Epoch 10: training loss 133967451346.824
Test Loss of 127173881750.828598, Test MSE of 127173882473.258347
Epoch 11: training loss 130680447337.412
Test Loss of 124346530263.376358, Test MSE of 124346533277.245941
Epoch 12: training loss 128059424256.000
Test Loss of 121514415521.132553, Test MSE of 121514415740.036423
Epoch 13: training loss 125189889867.294
Test Loss of 118670813411.871384, Test MSE of 118670814204.279297
Epoch 14: training loss 120555404800.000
Test Loss of 113269252603.854736, Test MSE of 113269252889.296585
Epoch 15: training loss 115883169370.353
Test Loss of 110805701529.907928, Test MSE of 110805700811.240509
Epoch 16: training loss 113107925925.647
Test Loss of 107376747950.397415, Test MSE of 107376747795.044144
Epoch 17: training loss 108320264402.824
Test Loss of 103934208516.855881, Test MSE of 103934208726.963272
Epoch 18: training loss 104864604280.471
Test Loss of 99284873223.579926, Test MSE of 99284872208.381134
Epoch 19: training loss 100333406509.176
Test Loss of 95350613106.883179, Test MSE of 95350612017.373154
Epoch 20: training loss 97007533808.941
Test Loss of 91035388938.185516, Test MSE of 91035391306.601898
Epoch 21: training loss 92247992621.176
Test Loss of 88300328854.354843, Test MSE of 88300332025.792740
Epoch 22: training loss 87558811738.353
Test Loss of 85725322166.806381, Test MSE of 85725321729.210495
Epoch 23: training loss 84887351898.353
Test Loss of 80667318845.705292, Test MSE of 80667318504.955139
Epoch 24: training loss 80780250563.765
Test Loss of 77439374758.817490, Test MSE of 77439376246.325577
Epoch 25: training loss 76812115862.588
Test Loss of 76229351565.649780, Test MSE of 76229353430.837677
Epoch 26: training loss 74304324803.765
Test Loss of 71091540283.514221, Test MSE of 71091540500.227417
Epoch 27: training loss 70809449110.588
Test Loss of 66522592488.371964, Test MSE of 66522592869.120201
Epoch 28: training loss 66322864609.882
Test Loss of 64603162122.303955, Test MSE of 64603160784.370125
Epoch 29: training loss 63542733914.353
Test Loss of 61925820388.049042, Test MSE of 61925820472.050301
Epoch 30: training loss 60810910132.706
Test Loss of 59137939904.636597, Test MSE of 59137939429.650284
Epoch 31: training loss 57672816971.294
Test Loss of 56309089773.405502, Test MSE of 56309090601.562035
Epoch 32: training loss 54237022464.000
Test Loss of 52644123040.421928, Test MSE of 52644123543.957146
Epoch 33: training loss 51945723158.588
Test Loss of 51240933135.811241, Test MSE of 51240934252.741310
Epoch 34: training loss 48923883956.706
Test Loss of 48300703944.394173, Test MSE of 48300704680.157005
Epoch 35: training loss 46344537321.412
Test Loss of 46737577050.722183, Test MSE of 46737577530.677788
Epoch 36: training loss 44350367736.471
Test Loss of 44989123133.231552, Test MSE of 44989123944.725105
Epoch 37: training loss 41526354605.176
Test Loss of 39460054273.954201, Test MSE of 39460054420.484322
Epoch 38: training loss 39275287446.588
Test Loss of 38840633937.602592, Test MSE of 38840635112.680824
Epoch 39: training loss 37663742464.000
Test Loss of 39185279210.977562, Test MSE of 39185279318.803131
Epoch 40: training loss 35156623819.294
Test Loss of 39701331809.769142, Test MSE of 39701332186.452042
Epoch 41: training loss 33333889129.412
Test Loss of 37590378813.409210, Test MSE of 37590377565.482635
Epoch 42: training loss 31982283324.235
Test Loss of 34566002442.126305, Test MSE of 34566002602.016396
Epoch 43: training loss 30206263100.235
Test Loss of 34319493302.865604, Test MSE of 34319493841.776146
Epoch 44: training loss 28628362209.882
Test Loss of 33199642723.486469, Test MSE of 33199642753.319736
Epoch 45: training loss 26934848474.353
Test Loss of 35504866445.886650, Test MSE of 35504867119.477242
Epoch 46: training loss 25731091072.000
Test Loss of 29943630870.029148, Test MSE of 29943630288.794155
Epoch 47: training loss 24813547045.647
Test Loss of 31152484496.018505, Test MSE of 31152485752.744236
Epoch 48: training loss 23921215081.412
Test Loss of 28196737290.718483, Test MSE of 28196736997.687798
Epoch 49: training loss 22696944301.176
Test Loss of 29417029326.908165, Test MSE of 29417027854.958805
Epoch 50: training loss 21120570096.941
Test Loss of 29047104044.650475, Test MSE of 29047104368.170181
Epoch 51: training loss 20527826055.529
Test Loss of 28960884201.141800, Test MSE of 28960884480.673767
Epoch 52: training loss 19746246249.412
Test Loss of 29043645438.105019, Test MSE of 29043645226.605751
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24342716232.241283, 'MSE - std': 4700928994.364468, 'R2 - mean': 0.8198474638215468, 'R2 - std': 0.02719980783940168} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003635 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[112]	valid_0's l2: 1.44998e+10
Model Interpreting...
[(23,), (23,), (22,), (22,), (22,)]

Train embedding model...
Epoch 1: training loss 421926098100.706
Test Loss of 447256754048.562561, Test MSE of 447256759641.867798
Epoch 2: training loss 421904315693.176
Test Loss of 447237867719.446655, Test MSE of 447237871383.659363
Epoch 3: training loss 421876856470.588
Test Loss of 447213217163.340271, Test MSE of 447213220540.453247
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898409020.235
Test Loss of 447222860763.758484, Test MSE of 447222862583.715393
Epoch 2: training loss 421885835504.941
Test Loss of 447224737976.760559, Test MSE of 447224749028.062195
Epoch 3: training loss 421885398678.588
Test Loss of 447225315718.365967, Test MSE of 447225312109.559570
Epoch 4: training loss 416415919043.765
Test Loss of 429868147186.379822, Test MSE of 429868148876.189087
Epoch 5: training loss 379671221549.176
Test Loss of 373390522433.850586, Test MSE of 373390518755.675781
Epoch 6: training loss 311510537276.235
Test Loss of 294876141294.649109, Test MSE of 294876142979.708191
Epoch 7: training loss 219799926241.882
Test Loss of 195118140576.836456, Test MSE of 195118138378.949738
Epoch 8: training loss 155984784956.235
Test Loss of 153913661371.306976, Test MSE of 153913662940.457886
Epoch 9: training loss 135753113298.824
Test Loss of 142608620592.085114, Test MSE of 142608621924.917297
Epoch 10: training loss 131418493560.471
Test Loss of 137310464019.423553, Test MSE of 137310465066.486115
Epoch 11: training loss 129120251422.118
Test Loss of 134715775130.914642, Test MSE of 134715774336.472595
Epoch 12: training loss 123422195290.353
Test Loss of 131072941646.049500, Test MSE of 131072942934.451828
Epoch 13: training loss 122448939369.412
Test Loss of 127379773670.713852, Test MSE of 127379773763.532272
Epoch 14: training loss 118242588370.824
Test Loss of 123713490285.494339, Test MSE of 123713490975.091202
Epoch 15: training loss 113948988626.824
Test Loss of 119984784061.142731, Test MSE of 119984787450.674057
Epoch 16: training loss 111233158806.588
Test Loss of 115709045998.530655, Test MSE of 115709047343.720673
Epoch 17: training loss 106716896075.294
Test Loss of 111331436426.984970, Test MSE of 111331434805.913345
Epoch 18: training loss 102547223732.706
Test Loss of 107271252552.127686, Test MSE of 107271253841.189285
Epoch 19: training loss 99969686076.235
Test Loss of 105332468690.283600, Test MSE of 105332468753.553467
Epoch 20: training loss 94931492999.529
Test Loss of 101493557705.164001, Test MSE of 101493556648.220398
Epoch 21: training loss 91682396280.471
Test Loss of 97014337927.313446, Test MSE of 97014338642.521011
Epoch 22: training loss 87248479096.471
Test Loss of 92810954381.057602, Test MSE of 92810955054.008759
Epoch 23: training loss 82631030753.882
Test Loss of 88875176590.715714, Test MSE of 88875177533.294189
Epoch 24: training loss 80272082100.706
Test Loss of 87522068855.679855, Test MSE of 87522068863.158066
Epoch 25: training loss 76661240154.353
Test Loss of 83241483304.505203, Test MSE of 83241483898.774948
Epoch 26: training loss 74022592843.294
Test Loss of 78269177971.593796, Test MSE of 78269177657.723053
Epoch 27: training loss 69932969246.118
Test Loss of 73977686372.730042, Test MSE of 73977686440.694656
Epoch 28: training loss 66940796295.529
Test Loss of 71772879141.248215, Test MSE of 71772880439.532852
Epoch 29: training loss 63999084122.353
Test Loss of 67488373573.107567, Test MSE of 67488373351.506477
Epoch 30: training loss 60571384176.941
Test Loss of 66912076187.210732, Test MSE of 66912074958.408112
Epoch 31: training loss 57471112192.000
Test Loss of 60739122801.580383, Test MSE of 60739122992.230408
Epoch 32: training loss 55042896037.647
Test Loss of 58146823286.199402, Test MSE of 58146823962.116928
Epoch 33: training loss 51832396626.824
Test Loss of 56400417356.391396, Test MSE of 56400417843.689911
Epoch 34: training loss 49513622256.941
Test Loss of 55850743847.320839, Test MSE of 55850743060.615852
Epoch 35: training loss 46756389955.765
Test Loss of 51714805789.372192, Test MSE of 51714806437.027832
Epoch 36: training loss 44407467248.941
Test Loss of 48921925451.503120, Test MSE of 48921926666.064201
Epoch 37: training loss 42089151646.118
Test Loss of 47082264294.121674, Test MSE of 47082265019.189293
Epoch 38: training loss 39734556333.176
Test Loss of 43406252262.713860, Test MSE of 43406252822.191978
Epoch 39: training loss 38280683663.059
Test Loss of 38267146837.866295, Test MSE of 38267147208.145111
Epoch 40: training loss 36359033893.647
Test Loss of 40152089938.490860, Test MSE of 40152089581.371010
Epoch 41: training loss 34087504775.529
Test Loss of 38523390070.910019, Test MSE of 38523390838.472374
Epoch 42: training loss 32415100284.235
Test Loss of 35652395415.420776, Test MSE of 35652396423.452301
Epoch 43: training loss 31135350535.529
Test Loss of 36242040656.240570, Test MSE of 36242041090.002342
Epoch 44: training loss 29032817679.059
Test Loss of 34166483783.950035, Test MSE of 34166484547.974483
Epoch 45: training loss 28136823134.118
Test Loss of 33669319779.249596, Test MSE of 33669320482.377537
Epoch 46: training loss 26789832572.235
Test Loss of 31426568719.752026, Test MSE of 31426568479.412476
Epoch 47: training loss 25389176056.471
Test Loss of 30603893259.488319, Test MSE of 30603893337.344334
Epoch 48: training loss 24838669255.529
Test Loss of 27818580000.688412, Test MSE of 27818580299.406910
Epoch 49: training loss 23746353615.059
Test Loss of 28100008080.965996, Test MSE of 28100008153.143101
Epoch 50: training loss 22546411324.235
Test Loss of 28289114859.569744, Test MSE of 28289115389.531406
Epoch 51: training loss 21829478317.176
Test Loss of 24738226584.842007, Test MSE of 24738226733.972328
Epoch 52: training loss 20713450898.824
Test Loss of 26033371781.714550, Test MSE of 26033371708.460777
Epoch 53: training loss 20196654113.882
Test Loss of 26435916397.079807, Test MSE of 26435916728.487782
Epoch 54: training loss 19477897204.706
Test Loss of 25380543477.814480, Test MSE of 25380543486.300968
Epoch 55: training loss 18941150452.706
Test Loss of 22464249824.732826, Test MSE of 22464249712.985703
Epoch 56: training loss 18227793603.765
Test Loss of 24272034867.401340, Test MSE of 24272035085.221264
Epoch 57: training loss 17700302693.647
Test Loss of 24751485661.831135, Test MSE of 24751485809.408974
Epoch 58: training loss 16968214411.294
Test Loss of 26492106643.512375, Test MSE of 26492106617.130402
Epoch 59: training loss 16525398283.294
Test Loss of 25240367148.532040, Test MSE of 25240367242.894032
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24641933235.792202, 'MSE - std': 3861547691.0774593, 'R2 - mean': 0.8238904895414204, 'R2 - std': 0.02293276720350243} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005389 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[198]	valid_0's l2: 1.26679e+10
Model Interpreting...
[(40,), (40,), (40,), (39,), (39,)]

Train embedding model...
Epoch 1: training loss 430105752997.647
Test Loss of 410762093662.297058, Test MSE of 410762091485.531189
Epoch 2: training loss 430082530364.235
Test Loss of 410742008788.879211, Test MSE of 410742012729.302612
Epoch 3: training loss 430055767582.118
Test Loss of 410717707412.316528, Test MSE of 410717702623.724243
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076128075.294
Test Loss of 410720672317.364197, Test MSE of 410720674796.822754
Epoch 2: training loss 430059895868.235
Test Loss of 410721879304.884766, Test MSE of 410721874029.586060
Epoch 3: training loss 430059509157.647
Test Loss of 410721971331.257751, Test MSE of 410721972794.508484
Epoch 4: training loss 424865818021.647
Test Loss of 393966279661.519653, Test MSE of 393966277199.528503
Epoch 5: training loss 389009640267.294
Test Loss of 339814838986.572876, Test MSE of 339814839733.047791
Epoch 6: training loss 321137732306.824
Test Loss of 262388406140.268402, Test MSE of 262388402180.958984
Epoch 7: training loss 229007697618.824
Test Loss of 164591439437.949097, Test MSE of 164591439317.532166
Epoch 8: training loss 163902843211.294
Test Loss of 124507889296.288757, Test MSE of 124507889453.058746
Epoch 9: training loss 144016554345.412
Test Loss of 114064928542.445160, Test MSE of 114064931389.283813
Epoch 10: training loss 138189486682.353
Test Loss of 109466304743.241089, Test MSE of 109466301061.279510
Epoch 11: training loss 135886415329.882
Test Loss of 107040039131.868576, Test MSE of 107040038631.507278
Epoch 12: training loss 132117665671.529
Test Loss of 104104185186.917175, Test MSE of 104104184411.392624
Epoch 13: training loss 127198904681.412
Test Loss of 101691219903.555756, Test MSE of 101691221512.277130
Epoch 14: training loss 125074894125.176
Test Loss of 98826786335.511337, Test MSE of 98826785972.159409
Epoch 15: training loss 120004705340.235
Test Loss of 95215735987.590927, Test MSE of 95215736192.833694
Epoch 16: training loss 116725844841.412
Test Loss of 91873562879.881531, Test MSE of 91873560879.339600
Epoch 17: training loss 112000289189.647
Test Loss of 90377701458.450714, Test MSE of 90377703198.304810
Epoch 18: training loss 108743337953.882
Test Loss of 86075582516.124023, Test MSE of 86075580586.492676
Epoch 19: training loss 105937846031.059
Test Loss of 83092114369.925034, Test MSE of 83092115994.889816
Epoch 20: training loss 101498519973.647
Test Loss of 79505972398.852386, Test MSE of 79505971898.012894
Epoch 21: training loss 97900368007.529
Test Loss of 77738668151.411377, Test MSE of 77738669876.896637
Epoch 22: training loss 93479680888.471
Test Loss of 74755829481.373444, Test MSE of 74755828854.944321
Epoch 23: training loss 89428065069.176
Test Loss of 71601630938.210083, Test MSE of 71601630896.888855
Epoch 24: training loss 85820926795.294
Test Loss of 70045938242.102737, Test MSE of 70045937754.322052
Epoch 25: training loss 82422882650.353
Test Loss of 64297324943.933365, Test MSE of 64297324857.915131
Epoch 26: training loss 79022855107.765
Test Loss of 63185925417.106895, Test MSE of 63185925251.911850
Epoch 27: training loss 75212985690.353
Test Loss of 60935134545.858398, Test MSE of 60935134605.920624
Epoch 28: training loss 72217642209.882
Test Loss of 56849171421.408607, Test MSE of 56849171748.865402
Epoch 29: training loss 68667567917.176
Test Loss of 55880664472.936600, Test MSE of 55880664898.763565
Epoch 30: training loss 65830649268.706
Test Loss of 52521099243.150391, Test MSE of 52521099163.137955
Epoch 31: training loss 62297965312.000
Test Loss of 49082385189.552986, Test MSE of 49082386373.395218
Epoch 32: training loss 59577227008.000
Test Loss of 50527902599.167053, Test MSE of 50527902011.790169
Epoch 33: training loss 56331976960.000
Test Loss of 45221929452.808884, Test MSE of 45221929640.171753
Epoch 34: training loss 53729433584.941
Test Loss of 44299286851.168900, Test MSE of 44299288015.133392
Epoch 35: training loss 51130462298.353
Test Loss of 40939247808.385010, Test MSE of 40939247933.353600
Epoch 36: training loss 47981231006.118
Test Loss of 41062990394.521057, Test MSE of 41062990928.188095
Epoch 37: training loss 45876228886.588
Test Loss of 39011230814.297081, Test MSE of 39011230827.813774
Epoch 38: training loss 44232061364.706
Test Loss of 37993366749.290146, Test MSE of 37993366578.878891
Epoch 39: training loss 41562315798.588
Test Loss of 35142957280.607124, Test MSE of 35142957443.839027
Epoch 40: training loss 38954389274.353
Test Loss of 32514987070.074966, Test MSE of 32514987162.712605
Epoch 41: training loss 37721044728.471
Test Loss of 32857221556.894032, Test MSE of 32857221108.632114
Epoch 42: training loss 36317861383.529
Test Loss of 31816517145.825081, Test MSE of 31816517329.096012
Epoch 43: training loss 34200131335.529
Test Loss of 29964482917.286442, Test MSE of 29964483179.296333
Epoch 44: training loss 32541069221.647
Test Loss of 29008589052.090698, Test MSE of 29008589081.419090
Epoch 45: training loss 30961251267.765
Test Loss of 27505913107.309578, Test MSE of 27505913244.443768
Epoch 46: training loss 29578749616.941
Test Loss of 27100104733.378990, Test MSE of 27100104822.280460
Epoch 47: training loss 28626589906.824
Test Loss of 27309409724.949562, Test MSE of 27309409437.670288
Epoch 48: training loss 27331858221.176
Test Loss of 23994752454.426655, Test MSE of 23994752644.035877
Epoch 49: training loss 26450979753.412
Test Loss of 24773733361.310505, Test MSE of 24773733239.008728
Epoch 50: training loss 25065691200.000
Test Loss of 23181760490.202682, Test MSE of 23181760261.809498
Epoch 51: training loss 24026659542.588
Test Loss of 23600955436.542343, Test MSE of 23600954981.807983
Epoch 52: training loss 23174516005.647
Test Loss of 21331312544.281353, Test MSE of 21331312481.309319
Epoch 53: training loss 22115588212.706
Test Loss of 22700099538.509949, Test MSE of 22700099226.062439
Epoch 54: training loss 21683282048.000
Test Loss of 23158561452.246181, Test MSE of 23158561662.105793
Epoch 55: training loss 21031007917.176
Test Loss of 22026192376.655251, Test MSE of 22026192540.589848
Epoch 56: training loss 20140364321.882
Test Loss of 20569519761.236465, Test MSE of 20569519745.901051
Epoch 57: training loss 19171096417.882
Test Loss of 20738235980.053680, Test MSE of 20738236196.965736
Epoch 58: training loss 18276711928.471
Test Loss of 21197624889.099491, Test MSE of 21197625235.978539
Epoch 59: training loss 18173445150.118
Test Loss of 21463000796.105507, Test MSE of 21463000528.740875
Epoch 60: training loss 17977969133.176
Test Loss of 19576021673.403053, Test MSE of 19576021937.355312
Epoch 61: training loss 17014012604.235
Test Loss of 21023230937.617771, Test MSE of 21023231129.454197
Epoch 62: training loss 16763323079.529
Test Loss of 19635823409.873207, Test MSE of 19635823607.969891
Epoch 63: training loss 16320972062.118
Test Loss of 21195465587.265156, Test MSE of 21195465163.854870
Epoch 64: training loss 16005176956.235
Test Loss of 19127129681.739937, Test MSE of 19127129696.948208
Epoch 65: training loss 15244350038.588
Test Loss of 19177264110.467377, Test MSE of 19177264108.067291
Epoch 66: training loss 14966325498.353
Test Loss of 19448486625.317909, Test MSE of 19448486763.392548
Epoch 67: training loss 14437303292.235
Test Loss of 18558103113.210552, Test MSE of 18558102977.233780
Epoch 68: training loss 14014696224.000
Test Loss of 18907823479.766773, Test MSE of 18907823574.459808
Epoch 69: training loss 13749800060.235
Test Loss of 20077643558.974548, Test MSE of 20077643509.406517
Epoch 70: training loss 13670208054.588
Test Loss of 20488328661.590004, Test MSE of 20488328724.409599
Epoch 71: training loss 13193326403.765
Test Loss of 18647538349.193893, Test MSE of 18647538555.365368
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23143334565.685493, 'MSE - std': 4233326924.534181, 'R2 - mean': 0.8294409036398895, 'R2 - std': 0.02206479432718517} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003786 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043194368.000
Test Loss of 431612341674.943054, Test MSE of 431612342258.532288
Epoch 2: training loss 424023419000.471
Test Loss of 431592097267.916687, Test MSE of 431592092696.073608
Epoch 3: training loss 423996202767.059
Test Loss of 431564647994.994934, Test MSE of 431564644563.707336
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010978605.176
Test Loss of 431565654977.925049, Test MSE of 431565661803.229431
Epoch 2: training loss 423998642416.941
Test Loss of 431568568797.645508, Test MSE of 431568564201.565308
Epoch 3: training loss 423998054640.941
Test Loss of 431568972476.357239, Test MSE of 431568973537.652100
Epoch 4: training loss 418847362590.118
Test Loss of 414552284869.360474, Test MSE of 414552284688.958557
Epoch 5: training loss 383064976564.706
Test Loss of 358861978685.127258, Test MSE of 358861975060.105652
Epoch 6: training loss 315141594172.235
Test Loss of 280996224716.942139, Test MSE of 280996223636.454651
Epoch 7: training loss 224660271887.059
Test Loss of 180900660087.056000, Test MSE of 180900658598.905151
Epoch 8: training loss 160574500864.000
Test Loss of 138132126546.095337, Test MSE of 138132127868.144623
Epoch 9: training loss 141211563520.000
Test Loss of 126633671634.983810, Test MSE of 126633673643.892075
Epoch 10: training loss 135431184414.118
Test Loss of 121267625965.045807, Test MSE of 121267624217.626846
Epoch 11: training loss 131483143122.824
Test Loss of 118711741760.799637, Test MSE of 118711739699.256424
Epoch 12: training loss 128914985050.353
Test Loss of 114868114933.812119, Test MSE of 114868115264.616348
Epoch 13: training loss 124355806479.059
Test Loss of 111704861944.773712, Test MSE of 111704863315.594543
Epoch 14: training loss 121457637074.824
Test Loss of 107444541857.939850, Test MSE of 107444541556.068741
Epoch 15: training loss 117818875437.176
Test Loss of 104970941217.762146, Test MSE of 104970942744.956650
Epoch 16: training loss 113757148777.412
Test Loss of 102836235554.472931, Test MSE of 102836235150.849548
Epoch 17: training loss 110284143450.353
Test Loss of 98009605782.922714, Test MSE of 98009605056.663727
Epoch 18: training loss 106051802112.000
Test Loss of 94464323017.269775, Test MSE of 94464323042.124100
Epoch 19: training loss 101674608685.176
Test Loss of 88956024222.622864, Test MSE of 88956023613.108276
Epoch 20: training loss 98402556762.353
Test Loss of 86834928693.545578, Test MSE of 86834929262.347275
Epoch 21: training loss 94420136869.647
Test Loss of 82539290203.217026, Test MSE of 82539291741.475098
Epoch 22: training loss 91333851090.824
Test Loss of 78625900717.430817, Test MSE of 78625900770.673218
Epoch 23: training loss 87151995919.059
Test Loss of 76476609730.754288, Test MSE of 76476610702.324188
Epoch 24: training loss 83503463619.765
Test Loss of 72074259477.323456, Test MSE of 72074260238.503967
Epoch 25: training loss 80388388201.412
Test Loss of 70719335126.893112, Test MSE of 70719335277.377197
Epoch 26: training loss 75724914070.588
Test Loss of 64343617824.103653, Test MSE of 64343616102.440750
Epoch 27: training loss 73863371670.588
Test Loss of 64060969181.764000, Test MSE of 64060968735.795029
Epoch 28: training loss 68854574866.824
Test Loss of 61580038805.501160, Test MSE of 61580039622.277283
Epoch 29: training loss 66773882955.294
Test Loss of 57281485454.867188, Test MSE of 57281484858.140396
Epoch 30: training loss 63249199902.118
Test Loss of 55043785539.879684, Test MSE of 55043784686.317032
Epoch 31: training loss 61124995809.882
Test Loss of 53953188509.082832, Test MSE of 53953189604.881462
Epoch 32: training loss 57763265792.000
Test Loss of 47624051651.820450, Test MSE of 47624051754.967751
Epoch 33: training loss 54425948295.529
Test Loss of 48521402916.723740, Test MSE of 48521403619.469254
Epoch 34: training loss 53432878275.765
Test Loss of 45717717837.356781, Test MSE of 45717717874.570770
Epoch 35: training loss 49492490736.941
Test Loss of 45274078739.191116, Test MSE of 45274078981.871811
Epoch 36: training loss 47641128071.529
Test Loss of 42081820021.871353, Test MSE of 42081819161.686737
Epoch 37: training loss 45218629963.294
Test Loss of 38515156362.720963, Test MSE of 38515156635.074158
Epoch 38: training loss 42776036562.824
Test Loss of 35802681935.370659, Test MSE of 35802682160.628220
Epoch 39: training loss 41150179305.412
Test Loss of 38066479847.951874, Test MSE of 38066479860.698563
Epoch 40: training loss 38984446618.353
Test Loss of 35725070115.183708, Test MSE of 35725070016.931862
Epoch 41: training loss 36701879905.882
Test Loss of 34472008339.605736, Test MSE of 34472008193.844559
Epoch 42: training loss 35661855450.353
Test Loss of 33961187454.045349, Test MSE of 33961186735.209576
Epoch 43: training loss 33484393261.176
Test Loss of 30099106347.357704, Test MSE of 30099106395.614269
Epoch 44: training loss 32219763817.412
Test Loss of 30274212401.991669, Test MSE of 30274211832.577621
Epoch 45: training loss 30572746179.765
Test Loss of 26976264097.229061, Test MSE of 26976263820.159454
Epoch 46: training loss 29319280692.706
Test Loss of 28014406520.951412, Test MSE of 28014406940.598072
Epoch 47: training loss 28014009769.412
Test Loss of 25076613055.555759, Test MSE of 25076612848.898441
Epoch 48: training loss 27409092679.529
Test Loss of 24094901984.844055, Test MSE of 24094902121.041546
Epoch 49: training loss 25924138624.000
Test Loss of 24880340297.329014, Test MSE of 24880339922.057426
Epoch 50: training loss 24460205944.471
Test Loss of 23824687883.964832, Test MSE of 23824688337.659374
Epoch 51: training loss 24045029985.882
Test Loss of 25636867503.681629, Test MSE of 25636867610.691261
Epoch 52: training loss 23003510701.176
Test Loss of 23335860724.390560, Test MSE of 23335860886.287453
Epoch 53: training loss 21890227211.294
Test Loss of 25137947350.893105, Test MSE of 25137947273.160706
Epoch 54: training loss 21354528752.941
Test Loss of 23458131491.302174, Test MSE of 23458131314.229271
Epoch 55: training loss 20410236638.118
Test Loss of 23600256357.760296, Test MSE of 23600256098.160618
Epoch 56: training loss 20068450488.471
Test Loss of 23279262172.697826, Test MSE of 23279262039.396400
Epoch 57: training loss 19584240873.412
Test Loss of 22511514899.309578, Test MSE of 22511515247.658512
Epoch 58: training loss 18754825814.588
Test Loss of 22929256813.815826, Test MSE of 22929256596.010983
Epoch 59: training loss 18169187568.941
Test Loss of 21410942250.054604, Test MSE of 21410942277.387966
Epoch 60: training loss 17660366976.000
Test Loss of 21316616684.808884, Test MSE of 21316616694.063152
Epoch 61: training loss 17244143728.941
Test Loss of 23194008093.142063, Test MSE of 23194008120.597279
Epoch 62: training loss 16869446260.706
Test Loss of 20828245700.412773, Test MSE of 20828245174.301071
Epoch 63: training loss 16196521231.059
Test Loss of 22196173151.600185, Test MSE of 22196173565.368534
Epoch 64: training loss 15707935868.235
Test Loss of 19811186897.443775, Test MSE of 19811187241.399982
Epoch 65: training loss 15468781918.118
Test Loss of 20735235808.844055, Test MSE of 20735235836.769463
Epoch 66: training loss 15108456293.647
Test Loss of 21029366616.729294, Test MSE of 21029366737.324913
Epoch 67: training loss 14630598592.000
Test Loss of 20533796221.453030, Test MSE of 20533796141.370598
Epoch 68: training loss 14244341688.471
Test Loss of 19960001631.244793, Test MSE of 19960001556.849499
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22506667963.918297, 'MSE - std': 3994774452.6083064, 'R2 - mean': 0.8337403163746562, 'R2 - std': 0.021527283117390404} 
 

Saving model.....
Results After CV: {'MSE - mean': 22506667963.918297, 'MSE - std': 3994774452.6083064, 'R2 - mean': 0.8337403163746562, 'R2 - std': 0.021527283117390404}
Train time: 97.9695016468002
Inference time: 0.07032931999856373
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 90 finished with value: 22506667963.918297 and parameters: {'n_trees': 200, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003751 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524781839.059
Test Loss of 418109650771.083069, Test MSE of 418109659290.687866
Epoch 2: training loss 427503684306.824
Test Loss of 418091408689.802429, Test MSE of 418091410683.584534
Epoch 3: training loss 427476536741.647
Test Loss of 418067513525.918091, Test MSE of 418067516482.915222
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490516269.176
Test Loss of 418073088565.651611, Test MSE of 418073095021.397766
Epoch 2: training loss 427479957744.941
Test Loss of 418074585819.936157, Test MSE of 418074598957.770874
Epoch 3: training loss 423540113167.059
Test Loss of 405783918657.850586, Test MSE of 405783911995.741516
Epoch 4: training loss 396608622471.529
Test Loss of 364267733348.256287, Test MSE of 364267735799.449219
Epoch 5: training loss 330360186277.647
Test Loss of 277597619078.010620, Test MSE of 277597622576.266357
Epoch 6: training loss 247719749812.706
Test Loss of 200835979036.128601, Test MSE of 200835983360.232117
Epoch 7: training loss 179564583424.000
Test Loss of 140919379785.608154, Test MSE of 140919379733.824799
Epoch 8: training loss 146314165699.765
Test Loss of 122174549655.006241, Test MSE of 122174549903.933090
Epoch 9: training loss 138291817773.176
Test Loss of 117083069992.149902, Test MSE of 117083070436.951416
Epoch 10: training loss 134652485820.235
Test Loss of 113264110349.205643, Test MSE of 113264111178.404083
Epoch 11: training loss 129769388001.882
Test Loss of 110053972043.325470, Test MSE of 110053973779.342010
Epoch 12: training loss 127105471036.235
Test Loss of 107920462791.387466, Test MSE of 107920463661.442108
Epoch 13: training loss 123096230038.588
Test Loss of 104128436251.950958, Test MSE of 104128435031.422241
Epoch 14: training loss 119228246497.882
Test Loss of 100781934374.551010, Test MSE of 100781934236.039932
Epoch 15: training loss 115979370872.471
Test Loss of 97725308020.304413, Test MSE of 97725306895.999451
Epoch 16: training loss 112441024195.765
Test Loss of 94481318329.530426, Test MSE of 94481318478.490555
Epoch 17: training loss 107888101752.471
Test Loss of 91398853744.040710, Test MSE of 91398854062.135757
Epoch 18: training loss 104319293432.471
Test Loss of 89444868042.940552, Test MSE of 89444868839.584030
Epoch 19: training loss 100865962420.706
Test Loss of 84933413459.497574, Test MSE of 84933414614.326050
Epoch 20: training loss 96750877003.294
Test Loss of 82844670529.495255, Test MSE of 82844670361.293564
Epoch 21: training loss 92549464320.000
Test Loss of 80227310939.492020, Test MSE of 80227310147.631454
Epoch 22: training loss 89247266936.471
Test Loss of 75944386421.666443, Test MSE of 75944384762.771027
Epoch 23: training loss 85827947520.000
Test Loss of 71625954372.456161, Test MSE of 71625955691.087021
Epoch 24: training loss 81511815499.294
Test Loss of 67901862099.290306, Test MSE of 67901863169.928551
Epoch 25: training loss 78014496888.471
Test Loss of 65859440366.175339, Test MSE of 65859439750.168922
Epoch 26: training loss 75795527785.412
Test Loss of 62234602744.005554, Test MSE of 62234603126.061226
Epoch 27: training loss 72019691580.235
Test Loss of 60076904776.779091, Test MSE of 60076904975.283997
Epoch 28: training loss 68647624286.118
Test Loss of 57014105899.288460, Test MSE of 57014106527.351456
Epoch 29: training loss 66063103322.353
Test Loss of 56551605616.810547, Test MSE of 56551605582.406204
Epoch 30: training loss 62929743811.765
Test Loss of 56519057023.318993, Test MSE of 56519057485.571129
Epoch 31: training loss 60600076920.471
Test Loss of 52135552654.005089, Test MSE of 52135552750.177376
Epoch 32: training loss 57409064169.412
Test Loss of 49762316380.854034, Test MSE of 49762316670.339500
Epoch 33: training loss 54719101560.471
Test Loss of 47313262462.667595, Test MSE of 47313262821.960732
Epoch 34: training loss 52792691275.294
Test Loss of 46344590001.772842, Test MSE of 46344590137.653214
Epoch 35: training loss 49648694957.176
Test Loss of 43706349331.601204, Test MSE of 43706348808.108604
Epoch 36: training loss 48007222708.706
Test Loss of 40993881687.524406, Test MSE of 40993881589.513634
Epoch 37: training loss 45516249012.706
Test Loss of 36520541592.131393, Test MSE of 36520541304.672768
Epoch 38: training loss 43283415085.176
Test Loss of 36844446355.453156, Test MSE of 36844446626.106758
Epoch 39: training loss 41185162669.176
Test Loss of 35549651851.695580, Test MSE of 35549651970.436607
Epoch 40: training loss 39351999510.588
Test Loss of 34590008106.104095, Test MSE of 34590008281.951988
Epoch 41: training loss 37758023860.706
Test Loss of 33548203933.934769, Test MSE of 33548203931.120834
Epoch 42: training loss 35900476013.176
Test Loss of 33128437481.911636, Test MSE of 33128438233.706989
Epoch 43: training loss 34079527717.647
Test Loss of 28579810686.549156, Test MSE of 28579810436.305088
Epoch 44: training loss 32600614648.471
Test Loss of 30635547836.313671, Test MSE of 30635547459.288399
Epoch 45: training loss 31495810876.235
Test Loss of 28629763060.866989, Test MSE of 28629762689.495232
Epoch 46: training loss 30198113603.765
Test Loss of 28041031345.535969, Test MSE of 28041032501.432373
Epoch 47: training loss 28691152873.412
Test Loss of 24444143765.229702, Test MSE of 24444143632.374241
Epoch 48: training loss 27309767311.059
Test Loss of 25718543747.760353, Test MSE of 25718544474.279587
Epoch 49: training loss 25684787715.765
Test Loss of 23704517037.686790, Test MSE of 23704516609.048950
Epoch 50: training loss 25619983672.471
Test Loss of 22323865358.390007, Test MSE of 22323865276.729843
Epoch 51: training loss 24214986691.765
Test Loss of 24933533611.199631, Test MSE of 24933533891.263729
Epoch 52: training loss 23285666806.588
Test Loss of 21437183771.654869, Test MSE of 21437184070.197788
Epoch 53: training loss 22220309191.529
Test Loss of 19702293075.971317, Test MSE of 19702293155.876682
Epoch 54: training loss 21241432082.824
Test Loss of 19274054345.933842, Test MSE of 19274054168.371849
Epoch 55: training loss 20783713528.471
Test Loss of 20204034202.914642, Test MSE of 20204034505.240677
Epoch 56: training loss 19829240896.000
Test Loss of 21992520569.693268, Test MSE of 21992521015.591789
Epoch 57: training loss 19106950735.059
Test Loss of 20418626631.772381, Test MSE of 20418626524.013309
Epoch 58: training loss 18792835896.471
Test Loss of 19025952586.081886, Test MSE of 19025952648.494209
Epoch 59: training loss 18265436856.471
Test Loss of 22173662155.651169, Test MSE of 22173662313.408504
Epoch 60: training loss 17563335228.235
Test Loss of 19587913979.558640, Test MSE of 19587914180.427605
Epoch 61: training loss 16958693338.353
Test Loss of 19665672229.899609, Test MSE of 19665672058.970261
Epoch 62: training loss 16475727834.353
Test Loss of 19497862127.182049, Test MSE of 19497862301.908478
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19497862301.908478, 'MSE - std': 0.0, 'R2 - mean': 0.8481680307530708, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917885199.059
Test Loss of 424556511205.470276, Test MSE of 424556512670.466431
Epoch 2: training loss 427896658281.412
Test Loss of 424540944411.950989, Test MSE of 424540934740.282776
Epoch 3: training loss 427868383593.412
Test Loss of 424519335412.985413, Test MSE of 424519332726.604431
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427890692577.882
Test Loss of 424526393805.427734, Test MSE of 424526400521.094482
Epoch 2: training loss 427878838031.059
Test Loss of 424527377224.423767, Test MSE of 424527376455.727844
Epoch 3: training loss 424136530522.353
Test Loss of 412755870609.380493, Test MSE of 412755875100.036499
Epoch 4: training loss 397471572329.412
Test Loss of 371956183353.619263, Test MSE of 371956189823.504333
Epoch 5: training loss 331156748288.000
Test Loss of 286933978037.858887, Test MSE of 286933978861.146301
Epoch 6: training loss 248132398863.059
Test Loss of 210854903637.451782, Test MSE of 210854904692.644348
Epoch 7: training loss 178654252032.000
Test Loss of 151416150487.376373, Test MSE of 151416150757.639435
Epoch 8: training loss 144505203651.765
Test Loss of 133853570250.052277, Test MSE of 133853564889.872345
Epoch 9: training loss 134878274620.235
Test Loss of 128566722656.643997, Test MSE of 128566719808.745743
Epoch 10: training loss 131856738906.353
Test Loss of 125009262316.754105, Test MSE of 125009262467.214859
Epoch 11: training loss 128978888432.941
Test Loss of 122032502995.764053, Test MSE of 122032501104.986328
Epoch 12: training loss 125071128726.588
Test Loss of 119385787249.639603, Test MSE of 119385784729.193665
Epoch 13: training loss 121130124679.529
Test Loss of 115380736969.282440, Test MSE of 115380733891.062042
Epoch 14: training loss 118350032474.353
Test Loss of 112771277413.973633, Test MSE of 112771275518.890182
Epoch 15: training loss 112892378172.235
Test Loss of 108606823629.368500, Test MSE of 108606822465.562943
Epoch 16: training loss 109073457573.647
Test Loss of 103476232263.061768, Test MSE of 103476231952.925079
Epoch 17: training loss 105920770379.294
Test Loss of 101350127690.377975, Test MSE of 101350128371.221924
Epoch 18: training loss 101796854512.941
Test Loss of 97062539259.262543, Test MSE of 97062541427.205765
Epoch 19: training loss 98181599382.588
Test Loss of 93607752317.660889, Test MSE of 93607751603.420700
Epoch 20: training loss 93509600978.824
Test Loss of 89807233232.447830, Test MSE of 89807235076.931442
Epoch 21: training loss 90755855239.529
Test Loss of 86099083086.819336, Test MSE of 86099084486.095276
Epoch 22: training loss 86474895465.412
Test Loss of 81306837082.959061, Test MSE of 81306837026.311859
Epoch 23: training loss 82880469865.412
Test Loss of 78088177226.733292, Test MSE of 78088176761.434052
Epoch 24: training loss 79423445187.765
Test Loss of 76306154247.994446, Test MSE of 76306156570.567230
Epoch 25: training loss 75944248079.059
Test Loss of 71675171227.210739, Test MSE of 71675169951.635681
Epoch 26: training loss 72219053312.000
Test Loss of 69089446069.444366, Test MSE of 69089444622.929153
Epoch 27: training loss 68962482793.412
Test Loss of 63034124644.730049, Test MSE of 63034125367.759705
Epoch 28: training loss 65740922880.000
Test Loss of 64424100387.649315, Test MSE of 64424101984.134636
Epoch 29: training loss 62824675659.294
Test Loss of 59717839116.613464, Test MSE of 59717838590.125572
Epoch 30: training loss 60107502667.294
Test Loss of 56228665130.814713, Test MSE of 56228665641.331612
Epoch 31: training loss 57597817434.353
Test Loss of 55766907795.275505, Test MSE of 55766909005.407646
Epoch 32: training loss 53895376052.706
Test Loss of 52037789163.984268, Test MSE of 52037787513.017815
Epoch 33: training loss 51587625878.588
Test Loss of 49790756107.429100, Test MSE of 49790757205.702805
Epoch 34: training loss 49285262336.000
Test Loss of 47831291014.306732, Test MSE of 47831291095.141365
Epoch 35: training loss 47171218936.471
Test Loss of 44046018431.615082, Test MSE of 44046019083.524490
Epoch 36: training loss 44535622528.000
Test Loss of 44345507637.710846, Test MSE of 44345507275.453720
Epoch 37: training loss 42111734708.706
Test Loss of 43852986156.235947, Test MSE of 43852985402.257805
Epoch 38: training loss 39892943894.588
Test Loss of 40666345611.281052, Test MSE of 40666345730.668648
Epoch 39: training loss 38180797568.000
Test Loss of 38271703367.357857, Test MSE of 38271703330.092628
Epoch 40: training loss 36329898315.294
Test Loss of 37914994933.399956, Test MSE of 37914995369.024292
Epoch 41: training loss 34247388400.941
Test Loss of 38461255301.477676, Test MSE of 38461255925.219849
Epoch 42: training loss 32956788291.765
Test Loss of 34979118434.124451, Test MSE of 34979118641.347931
Epoch 43: training loss 30787893421.176
Test Loss of 33867483342.552856, Test MSE of 33867482496.373322
Epoch 44: training loss 29759443651.765
Test Loss of 34578763694.515846, Test MSE of 34578763368.144463
Epoch 45: training loss 27973821959.529
Test Loss of 31348759703.361553, Test MSE of 31348759456.096138
Epoch 46: training loss 26894459971.765
Test Loss of 31037298384.329399, Test MSE of 31037298099.954689
Epoch 47: training loss 25632825193.412
Test Loss of 30200547821.168633, Test MSE of 30200547649.328316
Epoch 48: training loss 24301268156.235
Test Loss of 31026361610.481609, Test MSE of 31026360791.343250
Epoch 49: training loss 23109122989.176
Test Loss of 27306683109.884804, Test MSE of 27306682872.877457
Epoch 50: training loss 22317826089.412
Test Loss of 28075027879.291233, Test MSE of 28075028631.722633
Epoch 51: training loss 21449541263.059
Test Loss of 25621488708.929909, Test MSE of 25621488701.340797
Epoch 52: training loss 20318852517.647
Test Loss of 26219004834.672218, Test MSE of 26219004095.618294
Epoch 53: training loss 19605215040.000
Test Loss of 27943077423.256073, Test MSE of 27943077132.287445
Epoch 54: training loss 18605591277.176
Test Loss of 27474883348.548695, Test MSE of 27474883419.166218
Epoch 55: training loss 17935585633.882
Test Loss of 25778776555.747398, Test MSE of 25778777349.934235
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22638319825.921356, 'MSE - std': 3140457524.0128784, 'R2 - mean': 0.8320623378681172, 'R2 - std': 0.01610569288495356} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003590 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927426168.471
Test Loss of 447256462054.832275, Test MSE of 447256465419.179626
Epoch 2: training loss 421906093116.235
Test Loss of 447237430221.546143, Test MSE of 447237435153.425964
Epoch 3: training loss 421878851584.000
Test Loss of 447212803936.347900, Test MSE of 447212807847.159668
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897912199.529
Test Loss of 447220163585.184387, Test MSE of 447220160479.761475
Epoch 2: training loss 421885806953.412
Test Loss of 447220493417.882019, Test MSE of 447220493825.000244
Epoch 3: training loss 417892629684.706
Test Loss of 434814581290.755493, Test MSE of 434814583791.593445
Epoch 4: training loss 390770537652.706
Test Loss of 392354578231.132080, Test MSE of 392354572727.093933
Epoch 5: training loss 324169168775.529
Test Loss of 304498785504.318298, Test MSE of 304498790158.270874
Epoch 6: training loss 241850417573.647
Test Loss of 226171715227.743683, Test MSE of 226171716217.706635
Epoch 7: training loss 174140567371.294
Test Loss of 163915186236.639374, Test MSE of 163915187318.021210
Epoch 8: training loss 140954365560.471
Test Loss of 145178551717.869995, Test MSE of 145178552186.942413
Epoch 9: training loss 132301695668.706
Test Loss of 138631023079.483704, Test MSE of 138631024926.050781
Epoch 10: training loss 129788282096.941
Test Loss of 135620487642.218826, Test MSE of 135620486045.801926
Epoch 11: training loss 126306268160.000
Test Loss of 131974497616.595886, Test MSE of 131974499629.574249
Epoch 12: training loss 123191150501.647
Test Loss of 128514463118.182739, Test MSE of 128514463730.458237
Epoch 13: training loss 119654017656.471
Test Loss of 125819389632.222061, Test MSE of 125819392120.914978
Epoch 14: training loss 115273203952.941
Test Loss of 121488379548.691193, Test MSE of 121488379565.805511
Epoch 15: training loss 112985579520.000
Test Loss of 118296670491.299561, Test MSE of 118296672753.026642
Epoch 16: training loss 107362215845.647
Test Loss of 113940206107.121902, Test MSE of 113940204937.735809
Epoch 17: training loss 104733708679.529
Test Loss of 111311059002.270645, Test MSE of 111311056423.539993
Epoch 18: training loss 101115927642.353
Test Loss of 106269897123.738144, Test MSE of 106269898004.747391
Epoch 19: training loss 97681638791.529
Test Loss of 104401904243.238495, Test MSE of 104401904442.043671
Epoch 20: training loss 93686683151.059
Test Loss of 99773578009.523010, Test MSE of 99773578876.128616
Epoch 21: training loss 90533619922.824
Test Loss of 95310203325.794128, Test MSE of 95310202286.482605
Epoch 22: training loss 86117398302.118
Test Loss of 92244097967.226456, Test MSE of 92244095269.694336
Epoch 23: training loss 81978635776.000
Test Loss of 86145412950.399261, Test MSE of 86145413418.237167
Epoch 24: training loss 80842083930.353
Test Loss of 82854214694.610229, Test MSE of 82854215970.467468
Epoch 25: training loss 75187019715.765
Test Loss of 82227331445.311127, Test MSE of 82227330925.140472
Epoch 26: training loss 72383000847.059
Test Loss of 77582011490.302109, Test MSE of 77582012081.463638
Epoch 27: training loss 69592077824.000
Test Loss of 73128214372.611618, Test MSE of 73128216121.524750
Epoch 28: training loss 66142996510.118
Test Loss of 72550073652.171173, Test MSE of 72550072743.985168
Epoch 29: training loss 63281153325.176
Test Loss of 65394221749.562805, Test MSE of 65394221280.430138
Epoch 30: training loss 59830354522.353
Test Loss of 64538848850.786957, Test MSE of 64538849452.596443
Epoch 31: training loss 58474550091.294
Test Loss of 64039007874.635208, Test MSE of 64039006629.112877
Epoch 32: training loss 54918562981.647
Test Loss of 60498011690.518623, Test MSE of 60498011216.579735
Epoch 33: training loss 52288134942.118
Test Loss of 56318513168.107330, Test MSE of 56318514024.024292
Epoch 34: training loss 50565929697.882
Test Loss of 55250255824.625488, Test MSE of 55250254971.372040
Epoch 35: training loss 47769420205.176
Test Loss of 50687302091.058990, Test MSE of 50687302729.567924
Epoch 36: training loss 46307848267.294
Test Loss of 50965241167.174644, Test MSE of 50965241338.498146
Epoch 37: training loss 44019484242.824
Test Loss of 47179210651.566040, Test MSE of 47179211747.506447
Epoch 38: training loss 41489667975.529
Test Loss of 47069522650.988663, Test MSE of 47069523094.667473
Epoch 39: training loss 39690125839.059
Test Loss of 44398449606.439972, Test MSE of 44398450859.861717
Epoch 40: training loss 37799940894.118
Test Loss of 42942565931.939857, Test MSE of 42942566493.622169
Epoch 41: training loss 36496292171.294
Test Loss of 40674983141.055748, Test MSE of 40674982786.479050
Epoch 42: training loss 34183635004.235
Test Loss of 37587238635.332870, Test MSE of 37587238665.622459
Epoch 43: training loss 33020323734.588
Test Loss of 35852900555.236641, Test MSE of 35852900048.851624
Epoch 44: training loss 31080904342.588
Test Loss of 36372067130.211426, Test MSE of 36372067464.775620
Epoch 45: training loss 29956387749.647
Test Loss of 35332020690.638908, Test MSE of 35332019939.247849
Epoch 46: training loss 28852404705.882
Test Loss of 33782307574.465881, Test MSE of 33782307948.920780
Epoch 47: training loss 27384254765.176
Test Loss of 31398958927.293083, Test MSE of 31398959237.389137
Epoch 48: training loss 26375482917.647
Test Loss of 28843540988.802219, Test MSE of 28843541164.215988
Epoch 49: training loss 24833207401.412
Test Loss of 30010894945.236179, Test MSE of 30010895294.257923
Epoch 50: training loss 23739061530.353
Test Loss of 29532543582.393707, Test MSE of 29532543620.107689
Epoch 51: training loss 23112123474.824
Test Loss of 29005432341.673836, Test MSE of 29005432476.455185
Epoch 52: training loss 22194653680.941
Test Loss of 30739553445.573906, Test MSE of 30739554042.016804
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25338731231.286503, 'MSE - std': 4599937589.055431, 'R2 - mean': 0.819831164369972, 'R2 - std': 0.02172860110130782} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005752 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110698556.235
Test Loss of 410766167781.582581, Test MSE of 410766168442.048584
Epoch 2: training loss 430089756431.059
Test Loss of 410748396501.826904, Test MSE of 410748396727.543152
Epoch 3: training loss 430062053496.471
Test Loss of 410724793594.669128, Test MSE of 410724794482.057861
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430080711740.235
Test Loss of 410728362122.365601, Test MSE of 410728359923.406189
Epoch 2: training loss 430067677304.471
Test Loss of 410729779653.005066, Test MSE of 410729774387.249817
Epoch 3: training loss 426316579056.941
Test Loss of 398769611802.062012, Test MSE of 398769609504.278442
Epoch 4: training loss 399656458240.000
Test Loss of 357633926465.747314, Test MSE of 357633932183.305664
Epoch 5: training loss 333444690281.412
Test Loss of 271694276072.544189, Test MSE of 271694279167.662415
Epoch 6: training loss 251379163678.118
Test Loss of 195031877903.992584, Test MSE of 195031878292.588440
Epoch 7: training loss 182622881792.000
Test Loss of 135064368099.094864, Test MSE of 135064368211.718185
Epoch 8: training loss 148748014381.176
Test Loss of 117071670933.975006, Test MSE of 117071670779.006546
Epoch 9: training loss 140351972592.941
Test Loss of 111564229061.952805, Test MSE of 111564228633.855240
Epoch 10: training loss 135423369155.765
Test Loss of 108788382265.573349, Test MSE of 108788381401.766907
Epoch 11: training loss 133930103386.353
Test Loss of 106084072694.878296, Test MSE of 106084073926.559402
Epoch 12: training loss 130325142949.647
Test Loss of 102558795604.464600, Test MSE of 102558795933.663986
Epoch 13: training loss 125444932306.824
Test Loss of 100109549461.856552, Test MSE of 100109550778.828552
Epoch 14: training loss 123344610002.824
Test Loss of 97727246235.542801, Test MSE of 97727245488.188889
Epoch 15: training loss 118770252890.353
Test Loss of 94039220697.854691, Test MSE of 94039221228.212173
Epoch 16: training loss 115395540871.529
Test Loss of 90813007844.516434, Test MSE of 90813009359.371185
Epoch 17: training loss 110496389541.647
Test Loss of 88075308352.799637, Test MSE of 88075310524.189636
Epoch 18: training loss 107620473313.882
Test Loss of 85612752196.590469, Test MSE of 85612751042.622345
Epoch 19: training loss 104210762932.706
Test Loss of 81127137643.920410, Test MSE of 81127137186.056000
Epoch 20: training loss 99583723459.765
Test Loss of 79885191963.128174, Test MSE of 79885191517.908463
Epoch 21: training loss 96818451004.235
Test Loss of 76424239275.535400, Test MSE of 76424239072.978516
Epoch 22: training loss 92538902543.059
Test Loss of 73071983593.728836, Test MSE of 73071983690.432175
Epoch 23: training loss 88658458744.471
Test Loss of 70867832185.662201, Test MSE of 70867832078.221863
Epoch 24: training loss 84442778955.294
Test Loss of 68364336098.147156, Test MSE of 68364334800.588181
Epoch 25: training loss 82506937404.235
Test Loss of 65184757415.981491, Test MSE of 65184757383.140083
Epoch 26: training loss 79045628431.059
Test Loss of 62568772617.003242, Test MSE of 62568772763.702026
Epoch 27: training loss 75292769355.294
Test Loss of 59756851584.770012, Test MSE of 59756851330.097443
Epoch 28: training loss 71576170496.000
Test Loss of 57355325574.574738, Test MSE of 57355324790.462143
Epoch 29: training loss 69878304783.059
Test Loss of 54186322711.337341, Test MSE of 54186323083.187645
Epoch 30: training loss 66672574177.882
Test Loss of 52326885553.221657, Test MSE of 52326886624.119171
Epoch 31: training loss 63743339203.765
Test Loss of 48932561881.617767, Test MSE of 48932562257.845062
Epoch 32: training loss 60576180118.588
Test Loss of 48948314566.900513, Test MSE of 48948314054.564308
Epoch 33: training loss 57606973447.529
Test Loss of 49707043084.201759, Test MSE of 49707042861.316910
Epoch 34: training loss 55518790881.882
Test Loss of 45534975834.150856, Test MSE of 45534976524.879166
Epoch 35: training loss 53367228973.176
Test Loss of 43312454220.527534, Test MSE of 43312454768.512978
Epoch 36: training loss 50448115779.765
Test Loss of 40753735197.615921, Test MSE of 40753734752.466232
Epoch 37: training loss 48158534106.353
Test Loss of 39078469199.844513, Test MSE of 39078469155.876701
Epoch 38: training loss 46511621345.882
Test Loss of 36695942780.386856, Test MSE of 36695942641.525864
Epoch 39: training loss 44033241321.412
Test Loss of 37026203201.628876, Test MSE of 37026202828.107254
Epoch 40: training loss 41799195316.706
Test Loss of 34692077821.038406, Test MSE of 34692078023.097885
Epoch 41: training loss 40435559348.706
Test Loss of 33095739002.491440, Test MSE of 33095739663.210819
Epoch 42: training loss 38529801268.706
Test Loss of 32888519303.285515, Test MSE of 32888519402.107517
Epoch 43: training loss 37021631126.588
Test Loss of 31229899413.501156, Test MSE of 31229899040.553932
Epoch 44: training loss 35035934343.529
Test Loss of 31425435702.019436, Test MSE of 31425435552.912811
Epoch 45: training loss 33272562371.765
Test Loss of 28221073573.849144, Test MSE of 28221073639.695286
Epoch 46: training loss 31959615864.471
Test Loss of 29374324936.914391, Test MSE of 29374325109.538280
Epoch 47: training loss 30769472835.765
Test Loss of 26484043177.995373, Test MSE of 26484043519.839413
Epoch 48: training loss 28922357865.412
Test Loss of 27557660077.786209, Test MSE of 27557660877.186226
Epoch 49: training loss 28125745219.765
Test Loss of 26059765327.844517, Test MSE of 26059766133.847435
Epoch 50: training loss 27170844589.176
Test Loss of 26460322006.182323, Test MSE of 26460322083.671925
Epoch 51: training loss 25945393118.118
Test Loss of 25951103861.160572, Test MSE of 25951103939.314678
Epoch 52: training loss 24641883335.529
Test Loss of 24535303884.942158, Test MSE of 24535303544.006241
Epoch 53: training loss 23660404333.176
Test Loss of 21891886732.497917, Test MSE of 21891886588.009766
Epoch 54: training loss 23074329317.647
Test Loss of 20883091497.699215, Test MSE of 20883090944.656075
Epoch 55: training loss 22065737219.765
Test Loss of 23972480629.752892, Test MSE of 23972480157.759327
Epoch 56: training loss 21348301104.941
Test Loss of 19241247336.958817, Test MSE of 19241247003.304779
Epoch 57: training loss 20993592624.941
Test Loss of 22413371403.372513, Test MSE of 22413371419.735844
Epoch 58: training loss 20151666337.882
Test Loss of 23566682260.790375, Test MSE of 23566682381.385059
Epoch 59: training loss 19300124751.059
Test Loss of 21263435388.386856, Test MSE of 21263435622.475819
Epoch 60: training loss 18782169061.647
Test Loss of 20692128868.931049, Test MSE of 20692129083.460377
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24177080694.32997, 'MSE - std': 4462943566.463508, 'R2 - mean': 0.8221776419640706, 'R2 - std': 0.01925141425100079} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005461 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042951981.176
Test Loss of 431611912173.045837, Test MSE of 431611910945.720947
Epoch 2: training loss 424023216368.941
Test Loss of 431591377190.737610, Test MSE of 431591374873.929688
Epoch 3: training loss 423995730763.294
Test Loss of 431563236590.348938, Test MSE of 431563243480.386963
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009486817.882
Test Loss of 431565007730.317444, Test MSE of 431565003391.352478
Epoch 2: training loss 423997380126.118
Test Loss of 431567173284.190674, Test MSE of 431567166878.399658
Epoch 3: training loss 420070709488.941
Test Loss of 418990412054.626587, Test MSE of 418990414871.373413
Epoch 4: training loss 393388700611.765
Test Loss of 376652302607.992615, Test MSE of 376652309832.444458
Epoch 5: training loss 327909865712.941
Test Loss of 288740500126.978271, Test MSE of 288740502874.287781
Epoch 6: training loss 244965965763.765
Test Loss of 211003840266.543274, Test MSE of 211003837031.610138
Epoch 7: training loss 177757551947.294
Test Loss of 149838761421.060608, Test MSE of 149838761164.166718
Epoch 8: training loss 144614027911.529
Test Loss of 129205998297.736237, Test MSE of 129205999440.142609
Epoch 9: training loss 136282053300.706
Test Loss of 122556179830.819061, Test MSE of 122556179336.272552
Epoch 10: training loss 132508143977.412
Test Loss of 119841764118.389633, Test MSE of 119841763755.333496
Epoch 11: training loss 129613302964.706
Test Loss of 116924725764.501617, Test MSE of 116924727022.914963
Epoch 12: training loss 124970081159.529
Test Loss of 113529806840.418320, Test MSE of 113529807052.220520
Epoch 13: training loss 122931955456.000
Test Loss of 109990981952.325775, Test MSE of 109990980055.541870
Epoch 14: training loss 119664602488.471
Test Loss of 106176712728.640442, Test MSE of 106176711479.479736
Epoch 15: training loss 116113550712.471
Test Loss of 102886509687.885239, Test MSE of 102886509861.785004
Epoch 16: training loss 111850909530.353
Test Loss of 98733815896.136978, Test MSE of 98733815784.574326
Epoch 17: training loss 108855014053.647
Test Loss of 97170078712.892181, Test MSE of 97170079498.094162
Epoch 18: training loss 104664488673.882
Test Loss of 93092141390.541412, Test MSE of 93092139817.232132
Epoch 19: training loss 101003637097.412
Test Loss of 89685829475.627945, Test MSE of 89685830534.901291
Epoch 20: training loss 95676882507.294
Test Loss of 85715487321.795471, Test MSE of 85715487530.881973
Epoch 21: training loss 94105768086.588
Test Loss of 84346130721.999069, Test MSE of 84346129538.000458
Epoch 22: training loss 89521818503.529
Test Loss of 76907912198.160110, Test MSE of 76907913060.864227
Epoch 23: training loss 85522315625.412
Test Loss of 78829249291.017120, Test MSE of 78829249483.462585
Epoch 24: training loss 82757554597.647
Test Loss of 73552424061.571487, Test MSE of 73552423244.722260
Epoch 25: training loss 78728679077.647
Test Loss of 70375030323.413239, Test MSE of 70375030703.538071
Epoch 26: training loss 76053488293.647
Test Loss of 67075235618.236000, Test MSE of 67075235656.494812
Epoch 27: training loss 72416057569.882
Test Loss of 60875319880.736694, Test MSE of 60875320083.794205
Epoch 28: training loss 69539317760.000
Test Loss of 60445274826.572884, Test MSE of 60445273775.050537
Epoch 29: training loss 66903376911.059
Test Loss of 55727058608.510872, Test MSE of 55727058522.264465
Epoch 30: training loss 63146466198.588
Test Loss of 58655014040.107361, Test MSE of 58655014438.393974
Epoch 31: training loss 61358347233.882
Test Loss of 55362179244.483109, Test MSE of 55362180591.578636
Epoch 32: training loss 58231777212.235
Test Loss of 49644606158.363724, Test MSE of 49644606469.838051
Epoch 33: training loss 55081377716.706
Test Loss of 48376950329.099487, Test MSE of 48376950401.835144
Epoch 34: training loss 53127336199.529
Test Loss of 43751036527.118927, Test MSE of 43751036449.542633
Epoch 35: training loss 50852550384.941
Test Loss of 43032764243.516891, Test MSE of 43032763728.986481
Epoch 36: training loss 48939780404.706
Test Loss of 42196505174.952339, Test MSE of 42196505193.489052
Epoch 37: training loss 46035615706.353
Test Loss of 38408540387.450256, Test MSE of 38408541154.530167
Epoch 38: training loss 44592669492.706
Test Loss of 39678515299.509483, Test MSE of 39678515638.931755
Epoch 39: training loss 42558197511.529
Test Loss of 38823727735.174454, Test MSE of 38823727286.168457
Epoch 40: training loss 40757289366.588
Test Loss of 35473535589.167976, Test MSE of 35473535809.698479
Epoch 41: training loss 38403823096.471
Test Loss of 31665975710.149006, Test MSE of 31665975242.498615
Epoch 42: training loss 37261148498.824
Test Loss of 31041411801.736233, Test MSE of 31041411308.831322
Epoch 43: training loss 35240962198.588
Test Loss of 30719116311.218880, Test MSE of 30719116267.674801
Epoch 44: training loss 33929160756.706
Test Loss of 29784344968.351688, Test MSE of 29784344858.261993
Epoch 45: training loss 32542671277.176
Test Loss of 27028185986.428505, Test MSE of 27028185967.737766
Epoch 46: training loss 31220608180.706
Test Loss of 27681665198.378529, Test MSE of 27681665460.697697
Epoch 47: training loss 29633183472.941
Test Loss of 26240207201.495605, Test MSE of 26240206348.532501
Epoch 48: training loss 28300227604.706
Test Loss of 25918281539.405830, Test MSE of 25918281532.311832
Epoch 49: training loss 27209362665.412
Test Loss of 23820118676.553448, Test MSE of 23820118963.072128
Epoch 50: training loss 26010050416.941
Test Loss of 23439405522.273022, Test MSE of 23439405299.100971
Epoch 51: training loss 24781269729.882
Test Loss of 23478247330.176769, Test MSE of 23478247423.948101
Epoch 52: training loss 24303827388.235
Test Loss of 23396046011.172604, Test MSE of 23396046153.947796
Epoch 53: training loss 23271268958.118
Test Loss of 23392350495.155945, Test MSE of 23392350378.322006
Epoch 54: training loss 22340881449.412
Test Loss of 21850299079.729755, Test MSE of 21850299207.157372
Epoch 55: training loss 21514706454.588
Test Loss of 22241210073.262379, Test MSE of 22241210659.991173
Epoch 56: training loss 21007261315.765
Test Loss of 24143771735.189262, Test MSE of 24143772101.957760
Epoch 57: training loss 20326066379.294
Test Loss of 17778446884.723740, Test MSE of 17778447163.289745
Epoch 58: training loss 19713010736.941
Test Loss of 21280004745.180935, Test MSE of 21280004619.794201
Epoch 59: training loss 18892738439.529
Test Loss of 20963479127.426193, Test MSE of 20963479146.989803
Epoch 60: training loss 18130666864.941
Test Loss of 19788277295.622398, Test MSE of 19788277230.409679
Epoch 61: training loss 17694478848.000
Test Loss of 20081618200.048126, Test MSE of 20081617933.137508
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23357988142.09148, 'MSE - std': 4314851406.303662, 'R2 - mean': 0.8277480599113213, 'R2 - std': 0.020508822125103122} 
 

Saving model.....
Results After CV: {'MSE - mean': 23357988142.09148, 'MSE - std': 4314851406.303662, 'R2 - mean': 0.8277480599113213, 'R2 - std': 0.020508822125103122}
Train time: 89.63078688719979
Inference time: 0.07030694720160682
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 91 finished with value: 23357988142.09148 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005555 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526509507.765
Test Loss of 418113218829.560974, Test MSE of 418113223374.252747
Epoch 2: training loss 427506676193.882
Test Loss of 418095850061.338867, Test MSE of 418095857321.546021
Epoch 3: training loss 427480036171.294
Test Loss of 418072934753.887573, Test MSE of 418072936785.930969
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427498124107.294
Test Loss of 418077438242.879456, Test MSE of 418077433707.944275
Epoch 2: training loss 427485608779.294
Test Loss of 418078213032.120300, Test MSE of 418078211655.432922
Epoch 3: training loss 423694720903.529
Test Loss of 406106776448.325684, Test MSE of 406106784013.465393
Epoch 4: training loss 397150708555.294
Test Loss of 364852347137.717346, Test MSE of 364852348557.494324
Epoch 5: training loss 331121275482.353
Test Loss of 278755098946.146667, Test MSE of 278755098265.522339
Epoch 6: training loss 247915771663.059
Test Loss of 199923283973.211182, Test MSE of 199923284233.927551
Epoch 7: training loss 179645665249.882
Test Loss of 142387963968.903076, Test MSE of 142387965067.322113
Epoch 8: training loss 145872815465.412
Test Loss of 122483870835.356934, Test MSE of 122483873544.954605
Epoch 9: training loss 138485772559.059
Test Loss of 117428246650.699982, Test MSE of 117428247639.970795
Epoch 10: training loss 135292973658.353
Test Loss of 114032137657.767288, Test MSE of 114032139602.872925
Epoch 11: training loss 132528851501.176
Test Loss of 111815276098.205872, Test MSE of 111815276709.853973
Epoch 12: training loss 128307298183.529
Test Loss of 109174341133.620178, Test MSE of 109174340625.421844
Epoch 13: training loss 125551419331.765
Test Loss of 105687324277.370346, Test MSE of 105687325472.694687
Epoch 14: training loss 121967040331.294
Test Loss of 103335592957.394409, Test MSE of 103335594652.355118
Epoch 15: training loss 117604032813.176
Test Loss of 100392337565.283371, Test MSE of 100392338164.538712
Epoch 16: training loss 112948898966.588
Test Loss of 96887900935.520706, Test MSE of 96887900140.179443
Epoch 17: training loss 111169984286.118
Test Loss of 91667071821.161224, Test MSE of 91667073264.564835
Epoch 18: training loss 106214521479.529
Test Loss of 89676754679.650238, Test MSE of 89676752909.753128
Epoch 19: training loss 103502312207.059
Test Loss of 87071796022.895218, Test MSE of 87071794709.116516
Epoch 20: training loss 98776604596.706
Test Loss of 81537360857.389771, Test MSE of 81537359754.512573
Epoch 21: training loss 95407775051.294
Test Loss of 81178410634.688873, Test MSE of 81178411294.512634
Epoch 22: training loss 91845097923.765
Test Loss of 77043075062.998840, Test MSE of 77043077630.156799
Epoch 23: training loss 89276260156.235
Test Loss of 74722606150.824890, Test MSE of 74722606084.886703
Epoch 24: training loss 84203596709.647
Test Loss of 74108383812.811478, Test MSE of 74108383870.607727
Epoch 25: training loss 81421489965.176
Test Loss of 68785346731.969467, Test MSE of 68785347092.548691
Epoch 26: training loss 77947515075.765
Test Loss of 68051669984.022209, Test MSE of 68051669426.859589
Epoch 27: training loss 75103814377.412
Test Loss of 66839840518.336342, Test MSE of 66839841319.481483
Epoch 28: training loss 72067235983.059
Test Loss of 61924014215.964836, Test MSE of 61924014893.470840
Epoch 29: training loss 69409934960.941
Test Loss of 58846980310.606522, Test MSE of 58846981219.003891
Epoch 30: training loss 66186416150.588
Test Loss of 55541049149.290771, Test MSE of 55541048628.595749
Epoch 31: training loss 62812268867.765
Test Loss of 56499745130.888733, Test MSE of 56499745940.008850
Epoch 32: training loss 60815328000.000
Test Loss of 48932223521.280594, Test MSE of 48932223757.116180
Epoch 33: training loss 57803489008.941
Test Loss of 50621298140.824425, Test MSE of 50621297536.005264
Epoch 34: training loss 55650486949.647
Test Loss of 48554716065.014114, Test MSE of 48554716820.894936
Epoch 35: training loss 53912984583.529
Test Loss of 45646372778.725883, Test MSE of 45646373106.578865
Epoch 36: training loss 50912752041.412
Test Loss of 44029740554.303955, Test MSE of 44029739304.680115
Epoch 37: training loss 48766303427.765
Test Loss of 41494468355.493874, Test MSE of 41494468550.647469
Epoch 38: training loss 46766324419.765
Test Loss of 39082313827.012726, Test MSE of 39082313397.099861
Epoch 39: training loss 44459650006.588
Test Loss of 43295740937.711777, Test MSE of 43295741835.918137
Epoch 40: training loss 43131976463.059
Test Loss of 36956652999.979645, Test MSE of 36956653815.735626
Epoch 41: training loss 41097032022.588
Test Loss of 35150195578.167015, Test MSE of 35150194795.741905
Epoch 42: training loss 39160777607.529
Test Loss of 32949121160.438583, Test MSE of 32949121681.288963
Epoch 43: training loss 37493920587.294
Test Loss of 35400386830.508446, Test MSE of 35400387647.797813
Epoch 44: training loss 35324587986.824
Test Loss of 31097947030.117973, Test MSE of 31097947026.043060
Epoch 45: training loss 33906736141.176
Test Loss of 32422118694.906315, Test MSE of 32422118726.109898
Epoch 46: training loss 32733546080.000
Test Loss of 28758521122.168865, Test MSE of 28758521774.819923
Epoch 47: training loss 31187016975.059
Test Loss of 28550867782.055054, Test MSE of 28550868013.874851
Epoch 48: training loss 29941531881.412
Test Loss of 27012286026.259541, Test MSE of 27012285930.439312
Epoch 49: training loss 28511363252.706
Test Loss of 26430670829.523941, Test MSE of 26430670858.617634
Epoch 50: training loss 27422909345.882
Test Loss of 23860433479.653946, Test MSE of 23860433598.225204
Epoch 51: training loss 25878715429.647
Test Loss of 25507754238.874855, Test MSE of 25507754172.762745
Epoch 52: training loss 24926696500.706
Test Loss of 24748428880.418228, Test MSE of 24748429293.893978
Epoch 53: training loss 24257966168.471
Test Loss of 23169477963.858433, Test MSE of 23169478240.766621
Epoch 54: training loss 22966575439.059
Test Loss of 22347413646.597271, Test MSE of 22347413637.546474
Epoch 55: training loss 22294712914.824
Test Loss of 21961149165.464722, Test MSE of 21961148908.791580
Epoch 56: training loss 21739318260.706
Test Loss of 23358919529.112190, Test MSE of 23358919539.065033
Epoch 57: training loss 20744066104.471
Test Loss of 22004276488.112885, Test MSE of 22004276476.086029
Epoch 58: training loss 20127720444.235
Test Loss of 21328052783.019199, Test MSE of 21328052544.774376
Epoch 59: training loss 19413771137.882
Test Loss of 20167858004.030533, Test MSE of 20167857872.759750
Epoch 60: training loss 18842109315.765
Test Loss of 21223214356.904003, Test MSE of 21223214497.393513
Epoch 61: training loss 18360288952.471
Test Loss of 19359894314.104095, Test MSE of 19359894083.241154
Epoch 62: training loss 17537669451.294
Test Loss of 22228417002.799908, Test MSE of 22228417325.812702
Epoch 63: training loss 17317919286.588
Test Loss of 19301978196.326626, Test MSE of 19301978620.240292
Epoch 64: training loss 16744375021.176
Test Loss of 19875866818.235485, Test MSE of 19875867188.653446
Epoch 65: training loss 16136487751.529
Test Loss of 19403922661.529495, Test MSE of 19403922665.968094
Epoch 66: training loss 15678582772.706
Test Loss of 19382335419.306961, Test MSE of 19382335429.038628
Epoch 67: training loss 14971576478.118
Test Loss of 17951582310.328938, Test MSE of 17951582116.682777
Epoch 68: training loss 14792619207.529
Test Loss of 18246039592.268333, Test MSE of 18246040126.435486
Epoch 69: training loss 14351943996.235
Test Loss of 18895595748.345131, Test MSE of 18895595751.036556
Epoch 70: training loss 14164391979.294
Test Loss of 19776435803.788109, Test MSE of 19776435751.733257
Epoch 71: training loss 13812707866.353
Test Loss of 19556730375.935230, Test MSE of 19556730507.674252
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19556730507.67425, 'MSE - std': 0.0, 'R2 - mean': 0.847709617647621, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005500 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918513814.588
Test Loss of 424556793110.325256, Test MSE of 424556798165.855469
Epoch 2: training loss 427899195632.941
Test Loss of 424541438516.704163, Test MSE of 424541434458.046265
Epoch 3: training loss 427872178898.824
Test Loss of 424520062918.676819, Test MSE of 424520063724.884705
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889426913.882
Test Loss of 424529461680.055542, Test MSE of 424529466424.711975
Epoch 2: training loss 427879214381.176
Test Loss of 424530551930.463135, Test MSE of 424530555539.639526
Epoch 3: training loss 424248812965.647
Test Loss of 413007934473.711792, Test MSE of 413007937812.318542
Epoch 4: training loss 397884714767.059
Test Loss of 372648482887.535522, Test MSE of 372648482714.732788
Epoch 5: training loss 331788063563.294
Test Loss of 287496046552.679138, Test MSE of 287496044345.370056
Epoch 6: training loss 247601157662.118
Test Loss of 210950908101.551697, Test MSE of 210950911402.614380
Epoch 7: training loss 178530492897.882
Test Loss of 151597060750.715698, Test MSE of 151597061825.326263
Epoch 8: training loss 143121481005.176
Test Loss of 134115371605.392548, Test MSE of 134115371889.776260
Epoch 9: training loss 135731927160.471
Test Loss of 128725208755.430954, Test MSE of 128725212512.728470
Epoch 10: training loss 132532179184.941
Test Loss of 125837993101.412903, Test MSE of 125837991406.325851
Epoch 11: training loss 129292843941.647
Test Loss of 122801782098.017120, Test MSE of 122801779316.180222
Epoch 12: training loss 126071802970.353
Test Loss of 119026532257.250977, Test MSE of 119026531374.139633
Epoch 13: training loss 121790202789.647
Test Loss of 115865322591.933380, Test MSE of 115865324104.061798
Epoch 14: training loss 118972191623.529
Test Loss of 113196159746.309509, Test MSE of 113196160206.544464
Epoch 15: training loss 115148210657.882
Test Loss of 109745807197.979187, Test MSE of 109745807359.791748
Epoch 16: training loss 110902120658.824
Test Loss of 106517640710.513992, Test MSE of 106517639138.179123
Epoch 17: training loss 108542812129.882
Test Loss of 101462900916.970627, Test MSE of 101462903436.749512
Epoch 18: training loss 102491888338.824
Test Loss of 98497536270.034698, Test MSE of 98497537728.997620
Epoch 19: training loss 99792189680.941
Test Loss of 96977435787.991669, Test MSE of 96977434864.204712
Epoch 20: training loss 95589940464.941
Test Loss of 91575237463.109879, Test MSE of 91575238979.434860
Epoch 21: training loss 92048959638.588
Test Loss of 88225958739.319916, Test MSE of 88225958527.195435
Epoch 22: training loss 87994685485.176
Test Loss of 86316892807.135788, Test MSE of 86316891804.896057
Epoch 23: training loss 84412338176.000
Test Loss of 81268776107.258850, Test MSE of 81268777187.590576
Epoch 24: training loss 81693425091.765
Test Loss of 78387204527.108032, Test MSE of 78387203856.358261
Epoch 25: training loss 77427482503.529
Test Loss of 74037676100.693039, Test MSE of 74037675948.352936
Epoch 26: training loss 74219499399.529
Test Loss of 72415753558.280823, Test MSE of 72415754595.467606
Epoch 27: training loss 71460694272.000
Test Loss of 69080239323.580841, Test MSE of 69080239541.988144
Epoch 28: training loss 68664378955.294
Test Loss of 67705010566.839691, Test MSE of 67705013265.569717
Epoch 29: training loss 65007311390.118
Test Loss of 62954453082.485313, Test MSE of 62954452664.000504
Epoch 30: training loss 61670347173.647
Test Loss of 63174716240.477448, Test MSE of 63174715564.405220
Epoch 31: training loss 59917819888.941
Test Loss of 59284891406.863754, Test MSE of 59284892671.862694
Epoch 32: training loss 56997971531.294
Test Loss of 56915052756.711540, Test MSE of 56915052593.190147
Epoch 33: training loss 53790974087.529
Test Loss of 53003414327.368958, Test MSE of 53003413041.157303
Epoch 34: training loss 52332799269.647
Test Loss of 50886680710.069862, Test MSE of 50886681479.211792
Epoch 35: training loss 49011516950.588
Test Loss of 51079332781.805229, Test MSE of 51079333067.192558
Epoch 36: training loss 46889888685.176
Test Loss of 48610272774.987740, Test MSE of 48610274102.429115
Epoch 37: training loss 44597976892.235
Test Loss of 44057720026.633354, Test MSE of 44057719932.422508
Epoch 38: training loss 42190152335.059
Test Loss of 43200348791.502197, Test MSE of 43200349184.985886
Epoch 39: training loss 39997730273.882
Test Loss of 42515494207.541061, Test MSE of 42515494575.474327
Epoch 40: training loss 38572482228.706
Test Loss of 38251970262.488091, Test MSE of 38251970518.244576
Epoch 41: training loss 36282233208.471
Test Loss of 39467390862.774925, Test MSE of 39467390340.461021
Epoch 42: training loss 34779009317.647
Test Loss of 38353416246.006943, Test MSE of 38353415458.613594
Epoch 43: training loss 32999111115.294
Test Loss of 35530303658.548233, Test MSE of 35530302932.590065
Epoch 44: training loss 31916129701.647
Test Loss of 33851011604.015728, Test MSE of 33851011221.261944
Epoch 45: training loss 30133466149.647
Test Loss of 33607224759.398567, Test MSE of 33607225947.588486
Epoch 46: training loss 28776258725.647
Test Loss of 32569180916.570900, Test MSE of 32569181201.546337
Epoch 47: training loss 27014674665.412
Test Loss of 33741138859.910248, Test MSE of 33741138963.762260
Epoch 48: training loss 25976978861.176
Test Loss of 34108445506.738838, Test MSE of 34108445917.093540
Epoch 49: training loss 25097677409.882
Test Loss of 29249926716.047188, Test MSE of 29249926185.953075
Epoch 50: training loss 23756526509.176
Test Loss of 32697264106.681473, Test MSE of 32697264455.789722
Epoch 51: training loss 22685627429.647
Test Loss of 30454689395.238491, Test MSE of 30454689176.372261
Epoch 52: training loss 21901022991.059
Test Loss of 28812524561.291695, Test MSE of 28812524989.672260
Epoch 53: training loss 20998154526.118
Test Loss of 29185722206.216053, Test MSE of 29185722699.598774
Epoch 54: training loss 19997169159.529
Test Loss of 31781376520.645847, Test MSE of 31781376262.821106
Epoch 55: training loss 19169212175.059
Test Loss of 29057739392.977100, Test MSE of 29057738128.686619
Epoch 56: training loss 18523392308.706
Test Loss of 27815753616.196159, Test MSE of 27815753331.493511
Epoch 57: training loss 17948646765.176
Test Loss of 27908714901.999538, Test MSE of 27908714553.597965
Epoch 58: training loss 17050740321.882
Test Loss of 28233768998.373352, Test MSE of 28233769209.458969
Epoch 59: training loss 16638022260.706
Test Loss of 25533036943.130234, Test MSE of 25533037375.524628
Epoch 60: training loss 16025897351.529
Test Loss of 27755311134.793430, Test MSE of 27755311445.768837
Epoch 61: training loss 15496835576.471
Test Loss of 25079144431.892666, Test MSE of 25079144980.935726
Epoch 62: training loss 14952596212.706
Test Loss of 27217638173.076103, Test MSE of 27217637666.692669
Epoch 63: training loss 14560492709.647
Test Loss of 27277840433.269489, Test MSE of 27277840878.125412
Epoch 64: training loss 14248567996.235
Test Loss of 26199119817.519314, Test MSE of 26199119619.211685
Epoch 65: training loss 13755493078.588
Test Loss of 26652576598.873005, Test MSE of 26652576727.557991
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23104653617.61612, 'MSE - std': 3547923109.9418697, 'R2 - mean': 0.8287139576073749, 'R2 - std': 0.01899566004024611} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005430 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927526640.941
Test Loss of 447259028919.398560, Test MSE of 447259033465.010559
Epoch 2: training loss 421906993874.824
Test Loss of 447240404840.401550, Test MSE of 447240405242.863037
Epoch 3: training loss 421879583322.353
Test Loss of 447215809238.724976, Test MSE of 447215812481.693848
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896638584.471
Test Loss of 447224178578.564880, Test MSE of 447224181607.999878
Epoch 2: training loss 421887959401.412
Test Loss of 447225415151.774231, Test MSE of 447225419023.176758
Epoch 3: training loss 418106535092.706
Test Loss of 435217895027.001648, Test MSE of 435217896004.966125
Epoch 4: training loss 391488899433.412
Test Loss of 393184470612.208191, Test MSE of 393184467779.278320
Epoch 5: training loss 325263153392.941
Test Loss of 304460595859.453186, Test MSE of 304460596714.991455
Epoch 6: training loss 242832535913.412
Test Loss of 226676591155.282898, Test MSE of 226676589506.745361
Epoch 7: training loss 174970495879.529
Test Loss of 164848420714.770294, Test MSE of 164848422664.239349
Epoch 8: training loss 141481499648.000
Test Loss of 145834650328.146210, Test MSE of 145834652829.910248
Epoch 9: training loss 132696771734.588
Test Loss of 139531929871.455933, Test MSE of 139531932325.773651
Epoch 10: training loss 130437848847.059
Test Loss of 136748234192.507050, Test MSE of 136748232949.434998
Epoch 11: training loss 127054718403.765
Test Loss of 132470430233.700668, Test MSE of 132470433801.562454
Epoch 12: training loss 123983310185.412
Test Loss of 129845045460.711548, Test MSE of 129845044937.248810
Epoch 13: training loss 120767941541.647
Test Loss of 126810368917.644226, Test MSE of 126810368596.103287
Epoch 14: training loss 116107032214.588
Test Loss of 123705281840.854965, Test MSE of 123705285094.890488
Epoch 15: training loss 113744508235.294
Test Loss of 120023167623.135788, Test MSE of 120023165511.053986
Epoch 16: training loss 110468067478.588
Test Loss of 116088562825.622940, Test MSE of 116088564828.859558
Epoch 17: training loss 106594420404.706
Test Loss of 112026533584.803146, Test MSE of 112026535593.182739
Epoch 18: training loss 101888804487.529
Test Loss of 109010829880.494095, Test MSE of 109010831659.798584
Epoch 19: training loss 98676968779.294
Test Loss of 103269280695.753876, Test MSE of 103269280141.377090
Epoch 20: training loss 94769617257.412
Test Loss of 101816595768.908630, Test MSE of 101816593941.580902
Epoch 21: training loss 91603018676.706
Test Loss of 96388394832.714325, Test MSE of 96388394871.814011
Epoch 22: training loss 87300714962.824
Test Loss of 95113521895.306030, Test MSE of 95113521970.689255
Epoch 23: training loss 83457371542.588
Test Loss of 91185503060.267410, Test MSE of 91185503724.137070
Epoch 24: training loss 80283995934.118
Test Loss of 87039187242.933151, Test MSE of 87039187659.336624
Epoch 25: training loss 77641799574.588
Test Loss of 85210288623.537354, Test MSE of 85210288089.109558
Epoch 26: training loss 74137307376.941
Test Loss of 81469319672.538513, Test MSE of 81469320687.692627
Epoch 27: training loss 71330539489.882
Test Loss of 75611000999.705765, Test MSE of 75611000957.526978
Epoch 28: training loss 67669855939.765
Test Loss of 73942319054.730515, Test MSE of 73942318679.085434
Epoch 29: training loss 64179877029.647
Test Loss of 70905537868.805923, Test MSE of 70905538123.700607
Epoch 30: training loss 62099604766.118
Test Loss of 67979616521.297249, Test MSE of 67979618121.619682
Epoch 31: training loss 59596573364.706
Test Loss of 64645066430.800835, Test MSE of 64645066319.091202
Epoch 32: training loss 57343413248.000
Test Loss of 61606859473.040016, Test MSE of 61606859627.947983
Epoch 33: training loss 54285279096.471
Test Loss of 60948659407.263474, Test MSE of 60948658736.024696
Epoch 34: training loss 51683879258.353
Test Loss of 57112895353.456398, Test MSE of 57112895970.035179
Epoch 35: training loss 50129216368.941
Test Loss of 55453512636.017578, Test MSE of 55453512445.385185
Epoch 36: training loss 47190846057.412
Test Loss of 52857396062.689796, Test MSE of 52857396192.386574
Epoch 37: training loss 45494978710.588
Test Loss of 52034156181.585007, Test MSE of 52034157569.070404
Epoch 38: training loss 42763017682.824
Test Loss of 44133444611.553085, Test MSE of 44133444335.116089
Epoch 39: training loss 41443339361.882
Test Loss of 43164663122.017120, Test MSE of 43164662245.817253
Epoch 40: training loss 38971056459.294
Test Loss of 43039291956.230392, Test MSE of 43039291598.216660
Epoch 41: training loss 36939045910.588
Test Loss of 42504409821.357391, Test MSE of 42504410250.552666
Epoch 42: training loss 35657473016.471
Test Loss of 39470133148.513535, Test MSE of 39470133964.210625
Epoch 43: training loss 34217030520.471
Test Loss of 38216969507.590103, Test MSE of 38216969561.374260
Epoch 44: training loss 32848712161.882
Test Loss of 36181716423.505898, Test MSE of 36181716356.529510
Epoch 45: training loss 31256536259.765
Test Loss of 35899663616.059219, Test MSE of 35899663246.083649
Epoch 46: training loss 29787444931.765
Test Loss of 34441591274.326164, Test MSE of 34441592140.336807
Epoch 47: training loss 28655170891.294
Test Loss of 32899530564.396946, Test MSE of 32899531161.455723
Epoch 48: training loss 27437709786.353
Test Loss of 29652473519.640991, Test MSE of 29652473369.008789
Epoch 49: training loss 25947893338.353
Test Loss of 32158117620.807774, Test MSE of 32158118274.383148
Epoch 50: training loss 24820665874.824
Test Loss of 31808873493.081657, Test MSE of 31808873765.616219
Epoch 51: training loss 24043994044.235
Test Loss of 30094437254.484386, Test MSE of 30094437286.780071
Epoch 52: training loss 23128572739.765
Test Loss of 30134130512.240574, Test MSE of 30134130840.393204
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25447812691.87515, 'MSE - std': 4401434746.07291, 'R2 - mean': 0.8189423316290791, 'R2 - std': 0.02077320586948031} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005356 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110328229.647
Test Loss of 410764214757.227234, Test MSE of 410764213828.614868
Epoch 2: training loss 430088520643.765
Test Loss of 410745451187.827881, Test MSE of 410745452239.705627
Epoch 3: training loss 430059979956.706
Test Loss of 410720954310.189697, Test MSE of 410720954354.635803
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076054347.294
Test Loss of 410723132995.524292, Test MSE of 410723140422.127258
Epoch 2: training loss 430064141251.765
Test Loss of 410724225681.710327, Test MSE of 410724227641.139648
Epoch 3: training loss 426260432293.647
Test Loss of 399009162849.851013, Test MSE of 399009160555.422302
Epoch 4: training loss 399689622226.824
Test Loss of 356929145452.749634, Test MSE of 356929146771.362000
Epoch 5: training loss 333851092510.118
Test Loss of 271518056091.661255, Test MSE of 271518052840.491089
Epoch 6: training loss 250624811730.824
Test Loss of 194880598950.441467, Test MSE of 194880599235.751434
Epoch 7: training loss 182698546959.059
Test Loss of 135247512211.605743, Test MSE of 135247511483.147324
Epoch 8: training loss 148760883742.118
Test Loss of 116643414304.103653, Test MSE of 116643415747.034714
Epoch 9: training loss 140203455909.647
Test Loss of 111038898240.444244, Test MSE of 111038895953.765350
Epoch 10: training loss 135942271518.118
Test Loss of 108823513966.526611, Test MSE of 108823514267.188110
Epoch 11: training loss 133651667094.588
Test Loss of 105469724451.183716, Test MSE of 105469723802.650391
Epoch 12: training loss 128472824681.412
Test Loss of 103025199982.526611, Test MSE of 103025198974.578598
Epoch 13: training loss 127041014091.294
Test Loss of 100479181330.717255, Test MSE of 100479181407.647812
Epoch 14: training loss 121949659166.118
Test Loss of 96609254032.762604, Test MSE of 96609254571.062149
Epoch 15: training loss 117540117443.765
Test Loss of 93446240317.601105, Test MSE of 93446240527.695862
Epoch 16: training loss 114957954469.647
Test Loss of 90385533389.534470, Test MSE of 90385533487.182251
Epoch 17: training loss 110799377603.765
Test Loss of 87477226160.984726, Test MSE of 87477227476.281036
Epoch 18: training loss 106445152647.529
Test Loss of 86550837768.292450, Test MSE of 86550837847.195251
Epoch 19: training loss 102219299659.294
Test Loss of 81725546981.227203, Test MSE of 81725546517.679459
Epoch 20: training loss 98781410394.353
Test Loss of 78100086992.496063, Test MSE of 78100086949.513596
Epoch 21: training loss 94468713005.176
Test Loss of 75491006146.517349, Test MSE of 75491006721.798752
Epoch 22: training loss 91942019794.824
Test Loss of 72199836889.499313, Test MSE of 72199836995.827591
Epoch 23: training loss 87445523862.588
Test Loss of 68775474340.427582, Test MSE of 68775474956.843613
Epoch 24: training loss 84259109722.353
Test Loss of 65909781304.507172, Test MSE of 65909782471.041359
Epoch 25: training loss 80361243783.529
Test Loss of 62645413640.647850, Test MSE of 62645413300.535629
Epoch 26: training loss 77102259922.824
Test Loss of 61998167799.589081, Test MSE of 61998166597.865135
Epoch 27: training loss 73642880195.765
Test Loss of 59839301085.171677, Test MSE of 59839301832.722984
Epoch 28: training loss 70608497016.471
Test Loss of 55244123659.609444, Test MSE of 55244122459.763306
Epoch 29: training loss 67878739320.471
Test Loss of 54594700122.624710, Test MSE of 54594700240.341934
Epoch 30: training loss 65060711830.588
Test Loss of 50048880825.751038, Test MSE of 50048881437.926094
Epoch 31: training loss 62609564626.824
Test Loss of 47707608520.322075, Test MSE of 47707610023.270187
Epoch 32: training loss 59400515373.176
Test Loss of 46579705161.329018, Test MSE of 46579704479.359901
Epoch 33: training loss 56706912926.118
Test Loss of 46009656031.422493, Test MSE of 46009657119.982170
Epoch 34: training loss 54668734810.353
Test Loss of 45043297894.589539, Test MSE of 45043299087.503021
Epoch 35: training loss 51975265920.000
Test Loss of 42220721987.405830, Test MSE of 42220721963.067314
Epoch 36: training loss 49098536816.941
Test Loss of 40272618427.291069, Test MSE of 40272617817.819038
Epoch 37: training loss 47306794428.235
Test Loss of 39014750390.907913, Test MSE of 39014749866.667007
Epoch 38: training loss 45116983461.647
Test Loss of 35404810498.724663, Test MSE of 35404810241.183907
Epoch 39: training loss 42752511224.471
Test Loss of 33037229369.217957, Test MSE of 33037230359.428253
Epoch 40: training loss 41182616207.059
Test Loss of 33407811955.502083, Test MSE of 33407811906.962494
Epoch 41: training loss 38942002861.176
Test Loss of 30125286270.637669, Test MSE of 30125286461.686234
Epoch 42: training loss 37299414166.588
Test Loss of 30074529197.786209, Test MSE of 30074529508.119518
Epoch 43: training loss 35902276513.882
Test Loss of 29691483978.987507, Test MSE of 29691484800.243092
Epoch 44: training loss 34124227870.118
Test Loss of 28798769712.096252, Test MSE of 28798770291.299206
Epoch 45: training loss 32401921543.529
Test Loss of 28018593623.307728, Test MSE of 28018593065.409744
Epoch 46: training loss 31376310558.118
Test Loss of 27381834074.861637, Test MSE of 27381833956.035606
Epoch 47: training loss 29379275847.529
Test Loss of 23815680682.350765, Test MSE of 23815680980.532436
Epoch 48: training loss 28302291512.471
Test Loss of 22958231683.731606, Test MSE of 22958231647.565788
Epoch 49: training loss 26822489528.471
Test Loss of 25838123164.372051, Test MSE of 25838123402.922230
Epoch 50: training loss 26068036743.529
Test Loss of 23525813140.434982, Test MSE of 23525813558.427109
Epoch 51: training loss 24963813519.059
Test Loss of 24263942709.782509, Test MSE of 24263942537.795853
Epoch 52: training loss 23923540272.941
Test Loss of 21153552626.613605, Test MSE of 21153552737.839779
Epoch 53: training loss 23382742407.529
Test Loss of 22373804872.618233, Test MSE of 22373804901.948093
Epoch 54: training loss 22019700532.706
Test Loss of 20803517509.656639, Test MSE of 20803517927.313641
Epoch 55: training loss 21577440884.706
Test Loss of 21391978199.366959, Test MSE of 21391978203.342289
Epoch 56: training loss 20447041707.294
Test Loss of 21581618828.497917, Test MSE of 21581618838.224701
Epoch 57: training loss 20306387824.941
Test Loss of 22244870558.149006, Test MSE of 22244870685.626595
Epoch 58: training loss 19159575491.765
Test Loss of 20524577467.409534, Test MSE of 20524577885.552029
Epoch 59: training loss 18867434808.471
Test Loss of 20634217737.832485, Test MSE of 20634217562.767860
Epoch 60: training loss 18458419738.353
Test Loss of 19731156651.772327, Test MSE of 19731156302.583874
Epoch 61: training loss 17656880207.059
Test Loss of 20032466903.248497, Test MSE of 20032467156.434414
Epoch 62: training loss 17272921910.588
Test Loss of 19506258788.101803, Test MSE of 19506258883.820034
Epoch 63: training loss 16656109225.412
Test Loss of 18769121753.854698, Test MSE of 18769121977.687813
Epoch 64: training loss 16250661402.353
Test Loss of 19779773086.978252, Test MSE of 19779773113.645638
Epoch 65: training loss 15931435840.000
Test Loss of 20621995510.759834, Test MSE of 20621995588.417553
Epoch 66: training loss 15085040903.529
Test Loss of 18612708086.167515, Test MSE of 18612708211.816757
Epoch 67: training loss 14992747873.882
Test Loss of 18253116757.649235, Test MSE of 18253116528.293415
Epoch 68: training loss 14496318117.647
Test Loss of 19471409803.550209, Test MSE of 19471409747.043499
Epoch 69: training loss 14038770659.765
Test Loss of 17385931929.055069, Test MSE of 17385932109.748306
Epoch 70: training loss 13804814245.647
Test Loss of 18307224370.347061, Test MSE of 18307224572.245125
Epoch 71: training loss 13603862592.000
Test Loss of 18161882395.838963, Test MSE of 18161882042.662983
Epoch 72: training loss 13247480587.294
Test Loss of 18545219179.801945, Test MSE of 18545219524.460838
Epoch 73: training loss 12970351314.824
Test Loss of 19612198521.543728, Test MSE of 19612198447.357368
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23988909130.745705, 'MSE - std': 4573255915.644997, 'R2 - mean': 0.8237393249977358, 'R2 - std': 0.01981610450387937} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005431 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042920899.765
Test Loss of 431611073791.407654, Test MSE of 431611077441.562744
Epoch 2: training loss 424022900013.176
Test Loss of 431591106130.687622, Test MSE of 431591105140.258545
Epoch 3: training loss 423996200478.118
Test Loss of 431564010664.218445, Test MSE of 431564005159.204224
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424006826706.824
Test Loss of 431567120015.341064, Test MSE of 431567124711.464294
Epoch 2: training loss 423998352564.706
Test Loss of 431569041333.130981, Test MSE of 431569044418.581665
Epoch 3: training loss 420387335710.118
Test Loss of 419884140270.112000, Test MSE of 419884137198.952820
Epoch 4: training loss 394377494889.412
Test Loss of 378290641590.670959, Test MSE of 378290635201.029968
Epoch 5: training loss 328916080158.118
Test Loss of 291026461191.344727, Test MSE of 291026456624.459656
Epoch 6: training loss 246632368248.471
Test Loss of 211854396801.243866, Test MSE of 211854396383.960663
Epoch 7: training loss 178588853639.529
Test Loss of 149173573341.053223, Test MSE of 149173573485.732300
Epoch 8: training loss 145102608353.882
Test Loss of 129151731828.568253, Test MSE of 129151732959.106659
Epoch 9: training loss 136281587802.353
Test Loss of 122931069572.442383, Test MSE of 122931070477.924835
Epoch 10: training loss 133100409765.647
Test Loss of 119822307527.018967, Test MSE of 119822305854.812286
Epoch 11: training loss 129731534305.882
Test Loss of 116414867724.201752, Test MSE of 116414869084.319489
Epoch 12: training loss 127533540924.235
Test Loss of 113434935561.358627, Test MSE of 113434934214.560989
Epoch 13: training loss 121978903040.000
Test Loss of 110506886075.291061, Test MSE of 110506884809.124054
Epoch 14: training loss 118965698861.176
Test Loss of 106764487765.767700, Test MSE of 106764488899.456818
Epoch 15: training loss 114886967988.706
Test Loss of 101891161547.165207, Test MSE of 101891160698.386108
Epoch 16: training loss 111726105027.765
Test Loss of 100146234447.133728, Test MSE of 100146234364.255722
Epoch 17: training loss 107978083207.529
Test Loss of 95542177282.132339, Test MSE of 95542178906.103302
Epoch 18: training loss 103605038004.706
Test Loss of 92422523811.598328, Test MSE of 92422523810.564682
Epoch 19: training loss 99209879040.000
Test Loss of 87035485434.669128, Test MSE of 87035484523.555557
Epoch 20: training loss 96334986526.118
Test Loss of 84765564522.380386, Test MSE of 84765566405.484100
Epoch 21: training loss 92040234398.118
Test Loss of 80584344791.603882, Test MSE of 80584343986.802673
Epoch 22: training loss 88800698744.471
Test Loss of 76882720207.903748, Test MSE of 76882721320.752258
Epoch 23: training loss 85862176865.882
Test Loss of 77517928215.811203, Test MSE of 77517927385.754074
Epoch 24: training loss 82063715162.353
Test Loss of 73497483410.894958, Test MSE of 73497482542.641083
Epoch 25: training loss 78578014509.176
Test Loss of 68216190766.556221, Test MSE of 68216191014.244331
Epoch 26: training loss 75314136289.882
Test Loss of 64677591539.442848, Test MSE of 64677590945.819969
Epoch 27: training loss 72655935216.941
Test Loss of 62174971510.700600, Test MSE of 62174972249.796738
Epoch 28: training loss 68735905724.235
Test Loss of 58340169787.705688, Test MSE of 58340168745.141945
Epoch 29: training loss 64977902418.824
Test Loss of 56318695654.293381, Test MSE of 56318695000.824379
Epoch 30: training loss 62823910264.471
Test Loss of 53175609145.928734, Test MSE of 53175608248.349487
Epoch 31: training loss 60733478181.647
Test Loss of 54218640084.997688, Test MSE of 54218640122.483643
Epoch 32: training loss 57416143510.588
Test Loss of 49205388715.890793, Test MSE of 49205389015.190079
Epoch 33: training loss 55394620935.529
Test Loss of 48001393106.273026, Test MSE of 48001393359.347694
Epoch 34: training loss 52622661511.529
Test Loss of 45955993440.784821, Test MSE of 45955993960.755898
Epoch 35: training loss 50062777935.059
Test Loss of 38922711983.918556, Test MSE of 38922711305.229332
Epoch 36: training loss 48025167104.000
Test Loss of 35946757311.911156, Test MSE of 35946757126.182846
Epoch 37: training loss 45992478945.882
Test Loss of 35698029623.441002, Test MSE of 35698029121.172142
Epoch 38: training loss 44304038294.588
Test Loss of 37192293361.310501, Test MSE of 37192293179.942909
Epoch 39: training loss 41465017133.176
Test Loss of 34865357533.527069, Test MSE of 34865357750.977745
Epoch 40: training loss 39650682115.765
Test Loss of 34075136665.291996, Test MSE of 34075136149.973232
Epoch 41: training loss 37571044803.765
Test Loss of 28714015523.657566, Test MSE of 28714015668.135426
Epoch 42: training loss 36615920180.706
Test Loss of 33418998321.043961, Test MSE of 33418999032.681507
Epoch 43: training loss 34784839280.941
Test Loss of 31060992853.886166, Test MSE of 31060992527.720814
Epoch 44: training loss 33177971621.647
Test Loss of 26113342036.109207, Test MSE of 26113341598.208916
Epoch 45: training loss 32128871371.294
Test Loss of 27499713708.009254, Test MSE of 27499713514.068371
Epoch 46: training loss 30364437647.059
Test Loss of 24427515527.759369, Test MSE of 24427515096.553600
Epoch 47: training loss 29086442420.706
Test Loss of 24983998173.053215, Test MSE of 24983997894.836750
Epoch 48: training loss 27974345938.824
Test Loss of 23844510584.003700, Test MSE of 23844510418.869366
Epoch 49: training loss 26732484555.294
Test Loss of 26029688488.929199, Test MSE of 26029688356.982735
Epoch 50: training loss 25785354251.294
Test Loss of 23722107778.428505, Test MSE of 23722107357.612644
Epoch 51: training loss 24822356054.588
Test Loss of 22088572130.028690, Test MSE of 22088571873.135666
Epoch 52: training loss 23567517225.412
Test Loss of 21173638082.872746, Test MSE of 21173638202.305534
Epoch 53: training loss 22617902546.824
Test Loss of 22836595612.016659, Test MSE of 22836595845.570858
Epoch 54: training loss 22012362349.176
Test Loss of 20701018000.170292, Test MSE of 20701018090.423481
Epoch 55: training loss 21320022829.176
Test Loss of 20514283827.057842, Test MSE of 20514283623.908340
Epoch 56: training loss 20406723139.765
Test Loss of 22618156196.427578, Test MSE of 22618155794.228432
Epoch 57: training loss 19927819681.882
Test Loss of 22697289618.065712, Test MSE of 22697289246.137814
Epoch 58: training loss 19399258315.294
Test Loss of 20970510191.948174, Test MSE of 20970510049.979351
Epoch 59: training loss 18635531459.765
Test Loss of 20371016493.134659, Test MSE of 20371016645.063068
Epoch 60: training loss 17961580178.824
Test Loss of 18392915935.777882, Test MSE of 18392916401.180717
Epoch 61: training loss 17555822290.824
Test Loss of 20008246458.224895, Test MSE of 20008246362.676353
Epoch 62: training loss 16811611538.824
Test Loss of 21247601498.624710, Test MSE of 21247601505.790615
Epoch 63: training loss 16490647905.882
Test Loss of 19759729186.828320, Test MSE of 19759729428.862041
Epoch 64: training loss 16269672176.941
Test Loss of 19900033308.312817, Test MSE of 19900033453.175091
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23171133995.231583, 'MSE - std': 4405310479.982782, 'R2 - mean': 0.8292686222658714, 'R2 - std': 0.02089102465856041} 
 

Saving model.....
Results After CV: {'MSE - mean': 23171133995.231583, 'MSE - std': 4405310479.982782, 'R2 - mean': 0.8292686222658714, 'R2 - std': 0.02089102465856041}
Train time: 99.96809033440077
Inference time: 0.06972619019943523
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 92 finished with value: 23171133995.231583 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005563 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525668623.059
Test Loss of 418111878964.763367, Test MSE of 418111883942.307312
Epoch 2: training loss 427505351619.765
Test Loss of 418093610444.717102, Test MSE of 418093610918.159851
Epoch 3: training loss 427478461259.294
Test Loss of 418069767483.751099, Test MSE of 418069772525.862000
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494178093.176
Test Loss of 418075016708.145264, Test MSE of 418075021291.066406
Epoch 2: training loss 427482127420.235
Test Loss of 418075656984.101807, Test MSE of 418075665808.496704
Epoch 3: training loss 423639015183.059
Test Loss of 406001000116.378418, Test MSE of 406001005181.810181
Epoch 4: training loss 396878313231.059
Test Loss of 364680305897.793213, Test MSE of 364680307845.226501
Epoch 5: training loss 330983677229.176
Test Loss of 278345699609.404602, Test MSE of 278345698666.835999
Epoch 6: training loss 247379368237.176
Test Loss of 201208917382.129089, Test MSE of 201208923025.466797
Epoch 7: training loss 179311948378.353
Test Loss of 142016870014.845245, Test MSE of 142016870554.659912
Epoch 8: training loss 146443797112.471
Test Loss of 122265679608.123993, Test MSE of 122265679529.909286
Epoch 9: training loss 138002886249.412
Test Loss of 116834208540.365494, Test MSE of 116834208955.687286
Epoch 10: training loss 135970650985.412
Test Loss of 114164007781.795975, Test MSE of 114164009166.830032
Epoch 11: training loss 131043120429.176
Test Loss of 110785580306.535278, Test MSE of 110785582074.117371
Epoch 12: training loss 127649094927.059
Test Loss of 108368914572.702286, Test MSE of 108368914333.890793
Epoch 13: training loss 123326156679.529
Test Loss of 105036237202.209579, Test MSE of 105036237818.470032
Epoch 14: training loss 120715166418.824
Test Loss of 101574733144.649551, Test MSE of 101574733523.473434
Epoch 15: training loss 116783914240.000
Test Loss of 98993425422.922974, Test MSE of 98993427030.073288
Epoch 16: training loss 111991653767.529
Test Loss of 94935922662.891510, Test MSE of 94935921472.355194
Epoch 17: training loss 108882030080.000
Test Loss of 91766200222.882263, Test MSE of 91766199538.945038
Epoch 18: training loss 105143852348.235
Test Loss of 87622834933.518387, Test MSE of 87622833241.094559
Epoch 19: training loss 101041237202.824
Test Loss of 85940251401.415680, Test MSE of 85940251192.108063
Epoch 20: training loss 97662788502.588
Test Loss of 82729128670.304886, Test MSE of 82729129701.601852
Epoch 21: training loss 94224696681.412
Test Loss of 79094430159.796432, Test MSE of 79094430797.911148
Epoch 22: training loss 90212946401.882
Test Loss of 76649397030.787872, Test MSE of 76649396314.257202
Epoch 23: training loss 86864768233.412
Test Loss of 71755230787.627106, Test MSE of 71755231323.469376
Epoch 24: training loss 83153517025.882
Test Loss of 68957234107.070084, Test MSE of 68957235324.910568
Epoch 25: training loss 79956968899.765
Test Loss of 67149718720.577377, Test MSE of 67149719005.156288
Epoch 26: training loss 75806914160.941
Test Loss of 64863954654.541756, Test MSE of 64863953582.615860
Epoch 27: training loss 73750193935.059
Test Loss of 61690729106.505669, Test MSE of 61690728962.173416
Epoch 28: training loss 70326850763.294
Test Loss of 60328190569.763588, Test MSE of 60328190345.802162
Epoch 29: training loss 67200307410.824
Test Loss of 56603993291.947258, Test MSE of 56603993470.954697
Epoch 30: training loss 63930698081.882
Test Loss of 54090405528.427483, Test MSE of 54090405297.276787
Epoch 31: training loss 61077851572.706
Test Loss of 52854028710.106873, Test MSE of 52854029337.270493
Epoch 32: training loss 58676016798.118
Test Loss of 49744292059.343971, Test MSE of 49744291782.437614
Epoch 33: training loss 56082584094.118
Test Loss of 46766828955.921349, Test MSE of 46766829938.577736
Epoch 34: training loss 53651232406.588
Test Loss of 45826278963.993523, Test MSE of 45826278603.997818
Epoch 35: training loss 51352382324.706
Test Loss of 45300108344.612541, Test MSE of 45300108324.320221
Epoch 36: training loss 48580822633.412
Test Loss of 44299496375.753876, Test MSE of 44299495852.468948
Epoch 37: training loss 46981443388.235
Test Loss of 38556719911.735367, Test MSE of 38556719884.363251
Epoch 38: training loss 44252196404.706
Test Loss of 38407123708.624565, Test MSE of 38407124036.093330
Epoch 39: training loss 42533128308.706
Test Loss of 36820865448.475594, Test MSE of 36820865272.478592
Epoch 40: training loss 40114240173.176
Test Loss of 33523270710.243813, Test MSE of 33523270236.588161
Epoch 41: training loss 38758171964.235
Test Loss of 33707588006.106869, Test MSE of 33707588443.375645
Epoch 42: training loss 37179604141.176
Test Loss of 32125412067.989822, Test MSE of 32125412375.400715
Epoch 43: training loss 35521660574.118
Test Loss of 28748284868.544991, Test MSE of 28748284529.498234
Epoch 44: training loss 33993738224.941
Test Loss of 28525159155.149666, Test MSE of 28525159438.847820
Epoch 45: training loss 31981395422.118
Test Loss of 27264690416.188759, Test MSE of 27264690370.242100
Epoch 46: training loss 30901644216.471
Test Loss of 27117100733.853344, Test MSE of 27117101189.510445
Epoch 47: training loss 29155825283.765
Test Loss of 29333868981.740459, Test MSE of 29333868285.618168
Epoch 48: training loss 27931638230.588
Test Loss of 29047819021.205643, Test MSE of 29047819504.680485
Epoch 49: training loss 27380210861.176
Test Loss of 26888245254.632431, Test MSE of 26888245103.276062
Epoch 50: training loss 25820309564.235
Test Loss of 23856272466.194771, Test MSE of 23856271964.186481
Epoch 51: training loss 24862817253.647
Test Loss of 22543577380.774464, Test MSE of 22543577538.510777
Epoch 52: training loss 24012486659.765
Test Loss of 24450463406.693501, Test MSE of 24450463510.463245
Epoch 53: training loss 22871656918.588
Test Loss of 20352734446.530651, Test MSE of 20352734591.170490
Epoch 54: training loss 22247340954.353
Test Loss of 23209361046.058754, Test MSE of 23209361234.497139
Epoch 55: training loss 21421499610.353
Test Loss of 21296363926.473282, Test MSE of 21296364030.236122
Epoch 56: training loss 20525024357.647
Test Loss of 23135538976.866066, Test MSE of 23135538696.434067
Epoch 57: training loss 19570631781.647
Test Loss of 22746383223.798286, Test MSE of 22746383329.786453
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22746383329.786453, 'MSE - std': 0.0, 'R2 - mean': 0.8228714450471345, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005408 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918534294.588
Test Loss of 424556925470.911865, Test MSE of 424556929483.459229
Epoch 2: training loss 427898204882.824
Test Loss of 424539884807.165405, Test MSE of 424539884109.332031
Epoch 3: training loss 427870476649.412
Test Loss of 424516891676.661560, Test MSE of 424516885774.781311
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427884819154.824
Test Loss of 424521955465.859802, Test MSE of 424521960238.816895
Epoch 2: training loss 427876350072.471
Test Loss of 424522154681.352783, Test MSE of 424522150843.693359
Epoch 3: training loss 424406157914.353
Test Loss of 413471450386.061523, Test MSE of 413471459031.474121
Epoch 4: training loss 398605646908.235
Test Loss of 373253995050.755493, Test MSE of 373253993703.138794
Epoch 5: training loss 332798621214.118
Test Loss of 287301724816.136963, Test MSE of 287301720920.845764
Epoch 6: training loss 249544617562.353
Test Loss of 212318390338.798065, Test MSE of 212318391415.891846
Epoch 7: training loss 179622031661.176
Test Loss of 151690924284.979889, Test MSE of 151690918749.331787
Epoch 8: training loss 144975574919.529
Test Loss of 134526252505.981964, Test MSE of 134526252429.039764
Epoch 9: training loss 135551532995.765
Test Loss of 129037039418.448303, Test MSE of 129037040349.108032
Epoch 10: training loss 133060155452.235
Test Loss of 126271403432.712463, Test MSE of 126271401694.888885
Epoch 11: training loss 129927311360.000
Test Loss of 122659752785.424942, Test MSE of 122659753984.441498
Epoch 12: training loss 125284051817.412
Test Loss of 120428101539.145966, Test MSE of 120428100133.081009
Epoch 13: training loss 121614551190.588
Test Loss of 116560835480.012955, Test MSE of 116560835363.647369
Epoch 14: training loss 118990135024.941
Test Loss of 114300980339.356934, Test MSE of 114300978710.385986
Epoch 15: training loss 114951777972.706
Test Loss of 110315221677.982880, Test MSE of 110315219763.273254
Epoch 16: training loss 111030822761.412
Test Loss of 106582903743.333801, Test MSE of 106582903717.965424
Epoch 17: training loss 108131580265.412
Test Loss of 102651868128.022202, Test MSE of 102651870177.891113
Epoch 18: training loss 104625652344.471
Test Loss of 100129084611.419846, Test MSE of 100129085913.933029
Epoch 19: training loss 99398419877.647
Test Loss of 95502778655.089523, Test MSE of 95502779099.596313
Epoch 20: training loss 97271072496.941
Test Loss of 92709645500.313675, Test MSE of 92709643686.706879
Epoch 21: training loss 94359951480.471
Test Loss of 87450319745.746933, Test MSE of 87450320030.950928
Epoch 22: training loss 89172071695.059
Test Loss of 84681094222.878555, Test MSE of 84681093237.133331
Epoch 23: training loss 85822270915.765
Test Loss of 81614826704.210968, Test MSE of 81614827272.381302
Epoch 24: training loss 81738282616.471
Test Loss of 76574595258.181824, Test MSE of 76574594213.121414
Epoch 25: training loss 78254447826.824
Test Loss of 71957948152.123993, Test MSE of 71957947932.011002
Epoch 26: training loss 74647184489.412
Test Loss of 70988182462.860046, Test MSE of 70988183097.781769
Epoch 27: training loss 72420682601.412
Test Loss of 66295347247.848251, Test MSE of 66295347095.191505
Epoch 28: training loss 68573971998.118
Test Loss of 66308658595.027527, Test MSE of 66308656301.028641
Epoch 29: training loss 66039972080.941
Test Loss of 60240037992.697662, Test MSE of 60240040250.504028
Epoch 30: training loss 62228246543.059
Test Loss of 60142967694.064308, Test MSE of 60142968684.198914
Epoch 31: training loss 61093011486.118
Test Loss of 57810320893.512840, Test MSE of 57810323524.187019
Epoch 32: training loss 56917104896.000
Test Loss of 54827992676.315521, Test MSE of 54827992287.568672
Epoch 33: training loss 54038845093.647
Test Loss of 53221395533.457321, Test MSE of 53221394565.746674
Epoch 34: training loss 52790981601.882
Test Loss of 50004149360.988205, Test MSE of 50004149477.975266
Epoch 35: training loss 49812629165.176
Test Loss of 47673298857.778397, Test MSE of 47673299455.446304
Epoch 36: training loss 46998044453.647
Test Loss of 44469898267.240341, Test MSE of 44469898231.225700
Epoch 37: training loss 45390553840.941
Test Loss of 45166728083.275505, Test MSE of 45166727860.197403
Epoch 38: training loss 42379920918.588
Test Loss of 40451443757.242653, Test MSE of 40451444905.621460
Epoch 39: training loss 40914429906.824
Test Loss of 42655527005.801529, Test MSE of 42655525993.441711
Epoch 40: training loss 38794341707.294
Test Loss of 38195389153.621094, Test MSE of 38195389076.676140
Epoch 41: training loss 37317948212.706
Test Loss of 41924938774.976639, Test MSE of 41924937663.305260
Epoch 42: training loss 35524578823.529
Test Loss of 35013837545.201019, Test MSE of 35013837526.670319
Epoch 43: training loss 33241103698.824
Test Loss of 36058918704.499657, Test MSE of 36058917794.693024
Epoch 44: training loss 31735981296.941
Test Loss of 32171210345.526718, Test MSE of 32171210159.181152
Epoch 45: training loss 30740505720.471
Test Loss of 32856368871.542912, Test MSE of 32856368922.408455
Epoch 46: training loss 29021771550.118
Test Loss of 32696047141.781170, Test MSE of 32696046568.386524
Epoch 47: training loss 27673654881.882
Test Loss of 28821150601.326855, Test MSE of 28821150701.441448
Epoch 48: training loss 25940816805.647
Test Loss of 31454323077.418461, Test MSE of 31454322906.215508
Epoch 49: training loss 24673967111.529
Test Loss of 30885074100.260006, Test MSE of 30885074073.557728
Epoch 50: training loss 24195621436.235
Test Loss of 30797566298.781403, Test MSE of 30797566152.561352
Epoch 51: training loss 23005050944.000
Test Loss of 27251604727.768681, Test MSE of 27251605125.935738
Epoch 52: training loss 21618724133.647
Test Loss of 30726236706.701828, Test MSE of 30726236731.121662
Epoch 53: training loss 21437703536.941
Test Loss of 25859913061.914410, Test MSE of 25859912499.625435
Epoch 54: training loss 20086593035.294
Test Loss of 30150036209.254684, Test MSE of 30150036035.997654
Epoch 55: training loss 19318823706.353
Test Loss of 29855896096.333103, Test MSE of 29855895968.606480
Epoch 56: training loss 18818828427.294
Test Loss of 27364733920.969696, Test MSE of 27364734485.739445
Epoch 57: training loss 17998302731.294
Test Loss of 26612879623.165394, Test MSE of 26612879331.536537
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24679631330.661495, 'MSE - std': 1933248000.875042, 'R2 - mean': 0.8164365778328144, 'R2 - std': 0.006434867214320016} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003673 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927245221.647
Test Loss of 447258865844.970642, Test MSE of 447258871023.318787
Epoch 2: training loss 421907134584.471
Test Loss of 447240176154.174438, Test MSE of 447240176659.489136
Epoch 3: training loss 421880440229.647
Test Loss of 447215768701.305603, Test MSE of 447215770907.850769
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421900374618.353
Test Loss of 447222903362.916504, Test MSE of 447222905161.374084
Epoch 2: training loss 421888603798.588
Test Loss of 447224935085.035400, Test MSE of 447224940074.600342
Epoch 3: training loss 418146990802.824
Test Loss of 435130658432.266479, Test MSE of 435130670651.770081
Epoch 4: training loss 391662912451.765
Test Loss of 392907630395.395813, Test MSE of 392907632362.822144
Epoch 5: training loss 326089160463.059
Test Loss of 305504919834.352051, Test MSE of 305504917254.269409
Epoch 6: training loss 242577971260.235
Test Loss of 227074630691.294006, Test MSE of 227074630697.699738
Epoch 7: training loss 174962242499.765
Test Loss of 164867180474.359467, Test MSE of 164867180049.634918
Epoch 8: training loss 141515003994.353
Test Loss of 145431638564.596802, Test MSE of 145431638207.435150
Epoch 9: training loss 132365921882.353
Test Loss of 138807891410.402039, Test MSE of 138807890739.458099
Epoch 10: training loss 130187952399.059
Test Loss of 135886010236.772614, Test MSE of 135886008578.891418
Epoch 11: training loss 126940677782.588
Test Loss of 132076704870.565811, Test MSE of 132076707271.243057
Epoch 12: training loss 122979923727.059
Test Loss of 129163532981.089066, Test MSE of 129163533711.488785
Epoch 13: training loss 120001386616.471
Test Loss of 125357173793.872772, Test MSE of 125357174518.302887
Epoch 14: training loss 115645190927.059
Test Loss of 121876635577.885727, Test MSE of 121876635388.181900
Epoch 15: training loss 113016158569.412
Test Loss of 119792904605.579453, Test MSE of 119792905475.791229
Epoch 16: training loss 109008901029.647
Test Loss of 115454385454.012497, Test MSE of 115454384349.195465
Epoch 17: training loss 105005878904.471
Test Loss of 113707938139.728897, Test MSE of 113707941577.567841
Epoch 18: training loss 102430721987.765
Test Loss of 107427989309.290771, Test MSE of 107427991318.520187
Epoch 19: training loss 98436678113.882
Test Loss of 102616809081.634048, Test MSE of 102616807749.040222
Epoch 20: training loss 93464724570.353
Test Loss of 101782217854.253067, Test MSE of 101782216657.716858
Epoch 21: training loss 89565621519.059
Test Loss of 98275404318.438126, Test MSE of 98275406618.988754
Epoch 22: training loss 87020933481.412
Test Loss of 93740180676.367340, Test MSE of 93740183029.973419
Epoch 23: training loss 83142847503.059
Test Loss of 90898258230.303024, Test MSE of 90898256098.488052
Epoch 24: training loss 79843028811.294
Test Loss of 87877418551.309738, Test MSE of 87877419807.035812
Epoch 25: training loss 77822992911.059
Test Loss of 81302044914.557480, Test MSE of 81302046389.597031
Epoch 26: training loss 73939075252.706
Test Loss of 77508441283.419846, Test MSE of 77508442317.747192
Epoch 27: training loss 70159844096.000
Test Loss of 78150527084.487625, Test MSE of 78150526566.954102
Epoch 28: training loss 66560672843.294
Test Loss of 73041256760.671753, Test MSE of 73041257070.327118
Epoch 29: training loss 64546527156.706
Test Loss of 69932785131.510529, Test MSE of 69932787219.101486
Epoch 30: training loss 61549319122.824
Test Loss of 66182521538.827667, Test MSE of 66182519681.848846
Epoch 31: training loss 59504133240.471
Test Loss of 65451695688.364563, Test MSE of 65451693706.533279
Epoch 32: training loss 57258139437.176
Test Loss of 61926294142.608376, Test MSE of 61926294493.686890
Epoch 33: training loss 53661639845.647
Test Loss of 58292567846.077263, Test MSE of 58292567402.907875
Epoch 34: training loss 51190804864.000
Test Loss of 58986425870.567665, Test MSE of 58986426362.560112
Epoch 35: training loss 49131308754.824
Test Loss of 52969358782.504745, Test MSE of 52969359393.185783
Epoch 36: training loss 46863155049.412
Test Loss of 56029814450.483459, Test MSE of 56029815055.237869
Epoch 37: training loss 44588400941.176
Test Loss of 50508015807.866760, Test MSE of 50508016394.205498
Epoch 38: training loss 42977303198.118
Test Loss of 49830770372.959518, Test MSE of 49830770858.918549
Epoch 39: training loss 40219984745.412
Test Loss of 45877058263.198708, Test MSE of 45877058679.621658
Epoch 40: training loss 39028312741.647
Test Loss of 43134101519.633591, Test MSE of 43134101471.899536
Epoch 41: training loss 37206815465.412
Test Loss of 42946455153.580383, Test MSE of 42946454814.697319
Epoch 42: training loss 35269607329.882
Test Loss of 46041122257.454544, Test MSE of 46041123104.353256
Epoch 43: training loss 33698475090.824
Test Loss of 37516960639.378212, Test MSE of 37516960457.598442
Epoch 44: training loss 32237890123.294
Test Loss of 37092644817.099236, Test MSE of 37092644575.451996
Epoch 45: training loss 30734976843.294
Test Loss of 34995996591.937080, Test MSE of 34995996699.968323
Epoch 46: training loss 29540579245.176
Test Loss of 34624989922.568588, Test MSE of 34624990369.827690
Epoch 47: training loss 27945425392.941
Test Loss of 34885769109.407356, Test MSE of 34885768528.031212
Epoch 48: training loss 27075532235.294
Test Loss of 32956264265.134396, Test MSE of 32956264484.204330
Epoch 49: training loss 25723888240.941
Test Loss of 35362872645.699745, Test MSE of 35362872416.636375
Epoch 50: training loss 24341144180.706
Test Loss of 30231199993.189915, Test MSE of 30231200077.401478
Epoch 51: training loss 23412657336.471
Test Loss of 29751979853.871849, Test MSE of 29751979438.887875
Epoch 52: training loss 22996276668.235
Test Loss of 30804526829.701595, Test MSE of 30804526988.310909
Epoch 53: training loss 21852669662.118
Test Loss of 31705038310.773075, Test MSE of 31705038906.934299
Epoch 54: training loss 21030270949.647
Test Loss of 29111761900.813324, Test MSE of 29111761709.010582
Epoch 55: training loss 20289942128.941
Test Loss of 29015242742.998844, Test MSE of 29015242590.675957
Epoch 56: training loss 19512426925.176
Test Loss of 28270511339.688179, Test MSE of 28270510877.789707
Epoch 57: training loss 18664858368.000
Test Loss of 24500930911.518852, Test MSE of 24500930956.524265
Epoch 58: training loss 18307230373.647
Test Loss of 27134045804.369186, Test MSE of 27134045754.988720
Epoch 59: training loss 17708566520.471
Test Loss of 26679200238.116123, Test MSE of 26679200090.451221
Epoch 60: training loss 17274345280.000
Test Loss of 28498480267.517929, Test MSE of 28498480839.724770
Epoch 61: training loss 16735298703.059
Test Loss of 25365486428.794819, Test MSE of 25365486647.227303
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24908249769.516766, 'MSE - std': 1611261825.3284612, 'R2 - mean': 0.8213389283431004, 'R2 - std': 0.008698913251781655} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005462 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110267512.471
Test Loss of 410765820677.330872, Test MSE of 410765817449.376465
Epoch 2: training loss 430089673065.412
Test Loss of 410747552418.769104, Test MSE of 410747547096.101196
Epoch 3: training loss 430062262151.529
Test Loss of 410723911269.641846, Test MSE of 410723904825.357544
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430082419049.412
Test Loss of 410728868305.325317, Test MSE of 410728869239.733948
Epoch 2: training loss 430067278064.941
Test Loss of 410729840882.613586, Test MSE of 410729839822.816284
Epoch 3: training loss 426179117537.882
Test Loss of 398648772591.888916, Test MSE of 398648773272.112793
Epoch 4: training loss 399229415303.529
Test Loss of 357217177810.391479, Test MSE of 357217175411.160706
Epoch 5: training loss 332854854113.882
Test Loss of 271019988450.857941, Test MSE of 271019987103.841095
Epoch 6: training loss 249764816474.353
Test Loss of 194305255297.954651, Test MSE of 194305255356.120697
Epoch 7: training loss 182259321012.706
Test Loss of 135131133626.935684, Test MSE of 135131136005.168091
Epoch 8: training loss 148531113020.235
Test Loss of 117371260516.694122, Test MSE of 117371258662.431808
Epoch 9: training loss 140436864210.824
Test Loss of 111856363060.360947, Test MSE of 111856363442.443054
Epoch 10: training loss 136264324909.176
Test Loss of 108841567481.721420, Test MSE of 108841565309.106369
Epoch 11: training loss 134195474341.647
Test Loss of 106433199491.139282, Test MSE of 106433200608.110413
Epoch 12: training loss 129965352357.647
Test Loss of 103204664696.714478, Test MSE of 103204665352.445572
Epoch 13: training loss 126504417249.882
Test Loss of 99999392278.508102, Test MSE of 99999392852.183945
Epoch 14: training loss 123186255811.765
Test Loss of 97126014422.537720, Test MSE of 97126017155.410004
Epoch 15: training loss 119330121065.412
Test Loss of 93197014730.099030, Test MSE of 93197014779.684326
Epoch 16: training loss 115227192410.353
Test Loss of 91184417498.683945, Test MSE of 91184417889.542572
Epoch 17: training loss 110591863024.941
Test Loss of 87355255859.650162, Test MSE of 87355256248.703751
Epoch 18: training loss 107470205861.647
Test Loss of 84610880828.061081, Test MSE of 84610880236.166107
Epoch 19: training loss 103553783958.588
Test Loss of 81829558398.045349, Test MSE of 81829560010.698105
Epoch 20: training loss 99529509827.765
Test Loss of 79493591239.018967, Test MSE of 79493590152.742188
Epoch 21: training loss 96471533552.941
Test Loss of 77035141162.646927, Test MSE of 77035138960.039307
Epoch 22: training loss 92844634413.176
Test Loss of 72255606948.901428, Test MSE of 72255605590.795212
Epoch 23: training loss 89639384756.706
Test Loss of 69463150616.640442, Test MSE of 69463150258.429657
Epoch 24: training loss 84749388754.824
Test Loss of 66005979350.182320, Test MSE of 66005980178.765572
Epoch 25: training loss 81587615653.647
Test Loss of 64855747400.618233, Test MSE of 64855746737.824669
Epoch 26: training loss 79229536120.471
Test Loss of 60877759661.904671, Test MSE of 60877759151.867287
Epoch 27: training loss 75511066729.412
Test Loss of 57594710977.925034, Test MSE of 57594711713.471939
Epoch 28: training loss 72763770398.118
Test Loss of 54472308219.498383, Test MSE of 54472309443.544060
Epoch 29: training loss 69404583680.000
Test Loss of 53115743909.138359, Test MSE of 53115744002.592087
Epoch 30: training loss 66640823175.529
Test Loss of 51306941855.570572, Test MSE of 51306942401.127617
Epoch 31: training loss 63604580668.235
Test Loss of 49290784826.757980, Test MSE of 49290784576.783897
Epoch 32: training loss 60771697626.353
Test Loss of 45115147820.779266, Test MSE of 45115147084.851067
Epoch 33: training loss 58520363776.000
Test Loss of 44622866988.305412, Test MSE of 44622866504.364922
Epoch 34: training loss 55628193468.235
Test Loss of 42073797265.710320, Test MSE of 42073797744.291824
Epoch 35: training loss 52886611026.824
Test Loss of 40665218469.256828, Test MSE of 40665218309.430702
Epoch 36: training loss 51016611373.176
Test Loss of 38976266607.711243, Test MSE of 38976266455.295219
Epoch 37: training loss 48707053560.471
Test Loss of 37041159763.635353, Test MSE of 37041158990.995171
Epoch 38: training loss 46564373436.235
Test Loss of 34807531949.312355, Test MSE of 34807532425.894905
Epoch 39: training loss 44019535751.529
Test Loss of 31850097594.343361, Test MSE of 31850097553.024174
Epoch 40: training loss 42179730703.059
Test Loss of 31939526583.974087, Test MSE of 31939526488.796539
Epoch 41: training loss 40146196111.059
Test Loss of 30411099752.484962, Test MSE of 30411100040.327938
Epoch 42: training loss 38861665972.706
Test Loss of 32628556924.623787, Test MSE of 32628556610.151913
Epoch 43: training loss 37263790893.176
Test Loss of 28353704123.646461, Test MSE of 28353704042.972557
Epoch 44: training loss 35470435998.118
Test Loss of 27857123761.103191, Test MSE of 27857124277.404297
Epoch 45: training loss 33764610108.235
Test Loss of 27855266004.286903, Test MSE of 27855266832.282898
Epoch 46: training loss 32289322307.765
Test Loss of 26638865131.742710, Test MSE of 26638864926.538670
Epoch 47: training loss 30479032545.882
Test Loss of 25973158225.384544, Test MSE of 25973158305.118385
Epoch 48: training loss 29453284257.882
Test Loss of 27017632958.015732, Test MSE of 27017633117.244148
Epoch 49: training loss 28241690119.529
Test Loss of 24619502383.977787, Test MSE of 24619502523.373177
Epoch 50: training loss 27069360146.824
Test Loss of 23714699899.913002, Test MSE of 23714699526.247597
Epoch 51: training loss 25785254878.118
Test Loss of 21912289621.175381, Test MSE of 21912289228.099045
Epoch 52: training loss 25120539282.824
Test Loss of 22518718385.340118, Test MSE of 22518718649.525826
Epoch 53: training loss 23692641140.706
Test Loss of 21616563766.256363, Test MSE of 21616563577.861790
Epoch 54: training loss 22826319303.529
Test Loss of 20951764475.498379, Test MSE of 20951764631.599449
Epoch 55: training loss 22156587704.471
Test Loss of 20748674713.765850, Test MSE of 20748674666.783283
Epoch 56: training loss 21315496463.059
Test Loss of 19919027698.021286, Test MSE of 19919027900.534515
Epoch 57: training loss 20650631318.588
Test Loss of 19545625274.461823, Test MSE of 19545625361.891953
Epoch 58: training loss 19738417581.176
Test Loss of 19209668557.771400, Test MSE of 19209668576.517735
Epoch 59: training loss 19400283904.000
Test Loss of 19844902146.250809, Test MSE of 19844902376.464146
Epoch 60: training loss 19002006505.412
Test Loss of 19438914957.564091, Test MSE of 19438915039.297054
Epoch 61: training loss 18165154729.412
Test Loss of 18455210248.410919, Test MSE of 18455210454.874123
Epoch 62: training loss 17553170204.235
Test Loss of 19275484781.223507, Test MSE of 19275485045.299168
Epoch 63: training loss 16986119124.706
Test Loss of 18951568945.517815, Test MSE of 18951568640.404224
Epoch 64: training loss 16466126232.471
Test Loss of 17832570681.928738, Test MSE of 17832570531.956982
Epoch 65: training loss 15988211764.706
Test Loss of 17802907791.104118, Test MSE of 17802907885.141235
Epoch 66: training loss 15773251580.235
Test Loss of 18509439273.580750, Test MSE of 18509439047.236248
Epoch 67: training loss 15178143949.176
Test Loss of 17932202314.750580, Test MSE of 17932202749.388466
Epoch 68: training loss 14584443290.353
Test Loss of 19621555272.499767, Test MSE of 19621555312.032722
Epoch 69: training loss 14458223898.353
Test Loss of 17848156314.476631, Test MSE of 17848156090.451519
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23143226349.750454, 'MSE - std': 3360512835.757557, 'R2 - mean': 0.8291766627635967, 'R2 - std': 0.015525577649807944} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 2, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005479 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043408203.294
Test Loss of 431612959915.061523, Test MSE of 431612967674.278870
Epoch 2: training loss 424023638618.353
Test Loss of 431592762211.154114, Test MSE of 431592769636.937744
Epoch 3: training loss 423996796928.000
Test Loss of 431565481342.400757, Test MSE of 431565482465.710205
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010953908.706
Test Loss of 431566073334.759827, Test MSE of 431566078417.045837
Epoch 2: training loss 423999523056.941
Test Loss of 431569085898.217468, Test MSE of 431569091956.860291
Epoch 3: training loss 420400796370.824
Test Loss of 419595467943.270691, Test MSE of 419595465380.800293
Epoch 4: training loss 394488230008.471
Test Loss of 378352453411.183716, Test MSE of 378352452776.289001
Epoch 5: training loss 329240981925.647
Test Loss of 291062782055.300354, Test MSE of 291062780317.733459
Epoch 6: training loss 246694279529.412
Test Loss of 212272179839.703827, Test MSE of 212272179828.031799
Epoch 7: training loss 178993750196.706
Test Loss of 149576111397.316071, Test MSE of 149576109158.387665
Epoch 8: training loss 144868752896.000
Test Loss of 129337307308.483109, Test MSE of 129337308931.093048
Epoch 9: training loss 136110315339.294
Test Loss of 122530606620.668213, Test MSE of 122530606200.697372
Epoch 10: training loss 134433102215.529
Test Loss of 119611441891.213333, Test MSE of 119611442332.513809
Epoch 11: training loss 129659297054.118
Test Loss of 115755380984.299866, Test MSE of 115755379267.884155
Epoch 12: training loss 126754859730.824
Test Loss of 112426689110.952332, Test MSE of 112426686676.592575
Epoch 13: training loss 123182448128.000
Test Loss of 109127422276.116608, Test MSE of 109127424048.372894
Epoch 14: training loss 118637103104.000
Test Loss of 106521587626.706161, Test MSE of 106521588769.083221
Epoch 15: training loss 115258596954.353
Test Loss of 101601574343.374359, Test MSE of 101601574878.657013
Epoch 16: training loss 111616258048.000
Test Loss of 99647285728.488663, Test MSE of 99647286862.539825
Epoch 17: training loss 108473023186.824
Test Loss of 95211070490.062012, Test MSE of 95211071853.843307
Epoch 18: training loss 103580585984.000
Test Loss of 92193202365.541885, Test MSE of 92193203965.320938
Epoch 19: training loss 99968881317.647
Test Loss of 88220026617.484497, Test MSE of 88220025527.419479
Epoch 20: training loss 97738648304.941
Test Loss of 84409912690.080521, Test MSE of 84409913091.834534
Epoch 21: training loss 93724564389.647
Test Loss of 81228539034.950485, Test MSE of 81228539020.293182
Epoch 22: training loss 89378048858.353
Test Loss of 77206961925.804718, Test MSE of 77206963489.192337
Epoch 23: training loss 86133653820.235
Test Loss of 74025303730.406296, Test MSE of 74025303521.594376
Epoch 24: training loss 82862568192.000
Test Loss of 70904341249.540024, Test MSE of 70904343046.779327
Epoch 25: training loss 78919625200.941
Test Loss of 67103153614.956039, Test MSE of 67103153903.876923
Epoch 26: training loss 76556909296.941
Test Loss of 64434065768.129570, Test MSE of 64434066078.716637
Epoch 27: training loss 72008116299.294
Test Loss of 65696583427.909302, Test MSE of 65696584001.831383
Epoch 28: training loss 69347954740.706
Test Loss of 60088562200.403519, Test MSE of 60088561962.872948
Epoch 29: training loss 67380605319.529
Test Loss of 59109981599.570572, Test MSE of 59109982048.885597
Epoch 30: training loss 63516088982.588
Test Loss of 58134578311.996300, Test MSE of 58134576932.197800
Epoch 31: training loss 61076268438.588
Test Loss of 55837852013.815826, Test MSE of 55837851186.357414
Epoch 32: training loss 57600569298.824
Test Loss of 52164010167.855621, Test MSE of 52164010925.718010
Epoch 33: training loss 55955422358.588
Test Loss of 50593245351.744560, Test MSE of 50593246450.744896
Epoch 34: training loss 52986749703.529
Test Loss of 45954458820.649696, Test MSE of 45954458772.466568
Epoch 35: training loss 50897209622.588
Test Loss of 47277884940.557152, Test MSE of 47277884493.819992
Epoch 36: training loss 48571561456.941
Test Loss of 46209161649.103195, Test MSE of 46209161865.774933
Epoch 37: training loss 46964461650.824
Test Loss of 42286682259.842667, Test MSE of 42286681829.832344
Epoch 38: training loss 44515302339.765
Test Loss of 39436559374.689499, Test MSE of 39436559919.058723
Epoch 39: training loss 42466334817.882
Test Loss of 37614762840.729294, Test MSE of 37614763366.655312
Epoch 40: training loss 40597075779.765
Test Loss of 35220578325.797318, Test MSE of 35220578706.513809
Epoch 41: training loss 39059051463.529
Test Loss of 31545766209.273483, Test MSE of 31545765534.456062
Epoch 42: training loss 37365110942.118
Test Loss of 33336360019.872280, Test MSE of 33336360226.625137
Epoch 43: training loss 35218781251.765
Test Loss of 32752105628.372051, Test MSE of 32752105109.975533
Epoch 44: training loss 34034283979.294
Test Loss of 29753594345.965755, Test MSE of 29753594492.964268
Epoch 45: training loss 32381536112.941
Test Loss of 29483869211.957428, Test MSE of 29483868741.638153
Epoch 46: training loss 30802782900.706
Test Loss of 27889740986.224895, Test MSE of 27889740533.778320
Epoch 47: training loss 29239974347.294
Test Loss of 24740053035.594631, Test MSE of 24740053164.654282
Epoch 48: training loss 28501880417.882
Test Loss of 26468143292.594170, Test MSE of 26468142837.099976
Epoch 49: training loss 27577460156.235
Test Loss of 24230185329.606663, Test MSE of 24230186032.862671
Epoch 50: training loss 26192483162.353
Test Loss of 22939833813.590004, Test MSE of 22939833747.042343
Epoch 51: training loss 25128927849.412
Test Loss of 24543520214.537716, Test MSE of 24543520429.972343
Epoch 52: training loss 24391715230.118
Test Loss of 23212157310.874596, Test MSE of 23212157205.202473
Epoch 53: training loss 23638842322.824
Test Loss of 23711904175.207775, Test MSE of 23711904027.058083
Epoch 54: training loss 22149511668.706
Test Loss of 25847757507.938919, Test MSE of 25847756986.525436
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23684132477.10545, 'MSE - std': 3194488217.206906, 'R2 - mean': 0.8247349284446388, 'R2 - std': 0.01648486773162008} 
 

Saving model.....
Results After CV: {'MSE - mean': 23684132477.10545, 'MSE - std': 3194488217.206906, 'R2 - mean': 0.8247349284446388, 'R2 - std': 0.01648486773162008}
Train time: 92.4412381926013
Inference time: 0.07109741180029232
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 93 finished with value: 23684132477.10545 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 2, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005534 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525357086.118
Test Loss of 418111845431.191284, Test MSE of 418111851943.088745
Epoch 2: training loss 427504832391.529
Test Loss of 418093699091.897278, Test MSE of 418093704027.681152
Epoch 3: training loss 427477471954.824
Test Loss of 418069191000.412659, Test MSE of 418069193881.523621
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427492421391.059
Test Loss of 418074005592.116577, Test MSE of 418074003177.279053
Epoch 2: training loss 427481730951.529
Test Loss of 418073530907.121887, Test MSE of 418073531835.554382
Epoch 3: training loss 427481357854.118
Test Loss of 418072554203.936157, Test MSE of 418072554412.034363
Epoch 4: training loss 421936129807.059
Test Loss of 400593854314.296570, Test MSE of 400593855498.569153
Epoch 5: training loss 385102421654.588
Test Loss of 345389010175.585449, Test MSE of 345389017714.633362
Epoch 6: training loss 316961600572.235
Test Loss of 267268922355.445770, Test MSE of 267268919313.622955
Epoch 7: training loss 225796738469.647
Test Loss of 170940277734.654633, Test MSE of 170940278644.602661
Epoch 8: training loss 162406366027.294
Test Loss of 131375100532.185989, Test MSE of 131375101148.645706
Epoch 9: training loss 141542493696.000
Test Loss of 120473150764.591263, Test MSE of 120473150819.660263
Epoch 10: training loss 136305162209.882
Test Loss of 116513188081.136246, Test MSE of 116513188663.573196
Epoch 11: training loss 132282227681.882
Test Loss of 113518381846.680542, Test MSE of 113518382513.263641
Epoch 12: training loss 131325425995.294
Test Loss of 110093217984.340500, Test MSE of 110093219549.786423
Epoch 13: training loss 127203430189.176
Test Loss of 106764913597.438812, Test MSE of 106764913834.556580
Epoch 14: training loss 123284104131.765
Test Loss of 103789275900.150818, Test MSE of 103789278342.689560
Epoch 15: training loss 118902285914.353
Test Loss of 99964537012.496872, Test MSE of 99964537180.274658
Epoch 16: training loss 115158250390.588
Test Loss of 97214576144.936386, Test MSE of 97214576298.044785
Epoch 17: training loss 111836626582.588
Test Loss of 94110097955.649323, Test MSE of 94110099322.722839
Epoch 18: training loss 108223734362.353
Test Loss of 90472262898.320618, Test MSE of 90472265050.149109
Epoch 19: training loss 103172821232.941
Test Loss of 87941276478.001389, Test MSE of 87941276827.283173
Epoch 20: training loss 100282111631.059
Test Loss of 83599019388.654175, Test MSE of 83599021590.005096
Epoch 21: training loss 95421543348.706
Test Loss of 82459223816.231323, Test MSE of 82459226728.413086
Epoch 22: training loss 92383859576.471
Test Loss of 77028202491.736298, Test MSE of 77028203486.543976
Epoch 23: training loss 87377954032.941
Test Loss of 75904098726.580612, Test MSE of 75904101230.709641
Epoch 24: training loss 84173630433.882
Test Loss of 72456058636.968765, Test MSE of 72456058869.456955
Epoch 25: training loss 80691123011.765
Test Loss of 69959711007.563263, Test MSE of 69959710328.093079
Epoch 26: training loss 77097650981.647
Test Loss of 63514816973.427711, Test MSE of 63514816786.007790
Epoch 27: training loss 74146288820.706
Test Loss of 61638401792.177658, Test MSE of 61638402250.171455
Epoch 28: training loss 71597250145.882
Test Loss of 60765156316.232246, Test MSE of 60765156469.977821
Epoch 29: training loss 67327822682.353
Test Loss of 56618301486.900764, Test MSE of 56618302588.584457
Epoch 30: training loss 64801370725.647
Test Loss of 54573589442.650009, Test MSE of 54573589951.168961
Epoch 31: training loss 62091704816.941
Test Loss of 55647795070.667595, Test MSE of 55647794962.903603
Epoch 32: training loss 58600639638.588
Test Loss of 52411088398.567665, Test MSE of 52411088525.458435
Epoch 33: training loss 55799306556.235
Test Loss of 46266183325.638680, Test MSE of 46266183259.926590
Epoch 34: training loss 53075945592.471
Test Loss of 46216864848.773537, Test MSE of 46216864679.814423
Epoch 35: training loss 51090860152.471
Test Loss of 44597744546.435349, Test MSE of 44597744325.519257
Epoch 36: training loss 47254175337.412
Test Loss of 42537042812.535736, Test MSE of 42537042719.243752
Epoch 37: training loss 45002312146.824
Test Loss of 39066945747.764053, Test MSE of 39066945295.367584
Epoch 38: training loss 43279820747.294
Test Loss of 37268217665.554474, Test MSE of 37268217970.093117
Epoch 39: training loss 40634338266.353
Test Loss of 34973185371.965767, Test MSE of 34973184300.492416
Epoch 40: training loss 39330551875.765
Test Loss of 34597845190.499191, Test MSE of 34597845969.599205
Epoch 41: training loss 37484343687.529
Test Loss of 32357930808.553318, Test MSE of 32357931002.081039
Epoch 42: training loss 35457489758.118
Test Loss of 28711518456.242424, Test MSE of 28711519264.273849
Epoch 43: training loss 34320140069.647
Test Loss of 30632804166.291927, Test MSE of 30632803811.052139
Epoch 44: training loss 32883411072.000
Test Loss of 30083021533.831135, Test MSE of 30083021019.948021
Epoch 45: training loss 30864303548.235
Test Loss of 27812671032.257229, Test MSE of 27812671301.002956
Epoch 46: training loss 29386841562.353
Test Loss of 23445462439.764977, Test MSE of 23445462798.423042
Epoch 47: training loss 28440856726.588
Test Loss of 26861973720.501503, Test MSE of 26861973688.791748
Epoch 48: training loss 27076653545.412
Test Loss of 24020987484.261856, Test MSE of 24020987594.461617
Epoch 49: training loss 26343441306.353
Test Loss of 23983459783.505898, Test MSE of 23983459880.014927
Epoch 50: training loss 24791752207.059
Test Loss of 24965779479.687256, Test MSE of 24965779636.621162
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24965779636.621162, 'MSE - std': 0.0, 'R2 - mean': 0.805588765203145, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005542 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918042292.706
Test Loss of 424557161805.279663, Test MSE of 424557164892.601501
Epoch 2: training loss 427896993189.647
Test Loss of 424541101513.400879, Test MSE of 424541097866.967102
Epoch 3: training loss 427868676577.882
Test Loss of 424518891449.411987, Test MSE of 424518888850.969055
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427889485944.471
Test Loss of 424526291309.731201, Test MSE of 424526287107.740906
Epoch 2: training loss 427878036178.824
Test Loss of 424527392065.909790, Test MSE of 424527392003.150269
Epoch 3: training loss 427877608026.353
Test Loss of 424526593157.359253, Test MSE of 424526593569.070496
Epoch 4: training loss 422653677808.941
Test Loss of 408098521296.447815, Test MSE of 408098518345.848083
Epoch 5: training loss 386482211779.765
Test Loss of 354566453457.869080, Test MSE of 354566450180.094055
Epoch 6: training loss 318547671401.412
Test Loss of 278139380483.257019, Test MSE of 278139377131.817017
Epoch 7: training loss 224984916269.176
Test Loss of 181638190804.119354, Test MSE of 181638192383.591705
Epoch 8: training loss 160172730789.647
Test Loss of 142539262032.062927, Test MSE of 142539263365.489716
Epoch 9: training loss 139799309462.588
Test Loss of 131986348827.417999, Test MSE of 131986349113.188995
Epoch 10: training loss 133945993637.647
Test Loss of 127678951739.514221, Test MSE of 127678948123.114151
Epoch 11: training loss 131566473396.706
Test Loss of 124757948582.284531, Test MSE of 124757950726.270233
Epoch 12: training loss 128088895156.706
Test Loss of 121094132487.994446, Test MSE of 121094132878.326126
Epoch 13: training loss 123918570556.235
Test Loss of 118530347566.071716, Test MSE of 118530347874.455521
Epoch 14: training loss 119601317586.824
Test Loss of 114640126641.772842, Test MSE of 114640123570.236069
Epoch 15: training loss 116822563960.471
Test Loss of 110912093180.920654, Test MSE of 110912094722.183548
Epoch 16: training loss 112839214501.647
Test Loss of 106969053353.126999, Test MSE of 106969054525.266388
Epoch 17: training loss 107495032862.118
Test Loss of 102964915799.524399, Test MSE of 102964913409.358490
Epoch 18: training loss 104841997312.000
Test Loss of 99685135850.089294, Test MSE of 99685134662.597931
Epoch 19: training loss 100633278674.824
Test Loss of 98841642607.685410, Test MSE of 98841642787.101379
Epoch 20: training loss 97020041788.235
Test Loss of 93324360912.210968, Test MSE of 93324360663.095947
Epoch 21: training loss 91803279525.647
Test Loss of 88960061248.606979, Test MSE of 88960060788.172684
Epoch 22: training loss 88046989673.412
Test Loss of 84549845028.004623, Test MSE of 84549845579.719543
Epoch 23: training loss 85495916754.824
Test Loss of 82001366829.657181, Test MSE of 82001366863.126511
Epoch 24: training loss 81235897856.000
Test Loss of 78503170783.252365, Test MSE of 78503169526.388779
Epoch 25: training loss 77329972856.471
Test Loss of 74869046310.610229, Test MSE of 74869045332.905518
Epoch 26: training loss 74427035211.294
Test Loss of 72106854481.010406, Test MSE of 72106851602.939682
Epoch 27: training loss 70597902110.118
Test Loss of 69583595250.439041, Test MSE of 69583595584.721924
Epoch 28: training loss 67002871792.941
Test Loss of 67621755166.615776, Test MSE of 67621755727.689827
Epoch 29: training loss 64104687766.588
Test Loss of 59832788800.133240, Test MSE of 59832789739.059937
Epoch 30: training loss 61218093854.118
Test Loss of 59993489420.791115, Test MSE of 59993490161.002991
Epoch 31: training loss 57686891008.000
Test Loss of 55518699400.142494, Test MSE of 55518697694.272598
Epoch 32: training loss 54759513765.647
Test Loss of 52196030615.124680, Test MSE of 52196029659.573418
Epoch 33: training loss 52685382784.000
Test Loss of 54649374614.354843, Test MSE of 54649373647.989136
Epoch 34: training loss 48852579267.765
Test Loss of 50188116260.774460, Test MSE of 50188116861.308571
Epoch 35: training loss 46577429504.000
Test Loss of 47237233587.963913, Test MSE of 47237233467.018951
Epoch 36: training loss 44605523117.176
Test Loss of 46004004535.220909, Test MSE of 46004004102.377853
Epoch 37: training loss 42265385577.412
Test Loss of 44431251114.903542, Test MSE of 44431251408.751839
Epoch 38: training loss 39663225667.765
Test Loss of 42441021372.728195, Test MSE of 42441019590.170799
Epoch 39: training loss 37420346616.471
Test Loss of 39131953626.218826, Test MSE of 39131953777.547722
Epoch 40: training loss 35786918023.529
Test Loss of 36625779883.258850, Test MSE of 36625780366.886826
Epoch 41: training loss 33659865419.294
Test Loss of 35340986801.950500, Test MSE of 35340985953.667534
Epoch 42: training loss 31519549507.765
Test Loss of 36396345472.858665, Test MSE of 36396345236.580208
Epoch 43: training loss 30514838287.059
Test Loss of 35399907355.003471, Test MSE of 35399907139.167343
Epoch 44: training loss 28864343055.059
Test Loss of 34305824091.018276, Test MSE of 34305824068.880955
Epoch 45: training loss 27351960764.235
Test Loss of 33833370957.279667, Test MSE of 33833370977.054359
Epoch 46: training loss 25952706605.176
Test Loss of 32917939580.654175, Test MSE of 32917940064.857071
Epoch 47: training loss 24724922526.118
Test Loss of 31662651772.417301, Test MSE of 31662651431.233055
Epoch 48: training loss 23775192432.941
Test Loss of 28377698488.760582, Test MSE of 28377698309.929546
Epoch 49: training loss 22454264918.588
Test Loss of 29080786804.718945, Test MSE of 29080786922.426178
Epoch 50: training loss 21849931757.176
Test Loss of 27753610557.409206, Test MSE of 27753610960.411194
Epoch 51: training loss 20405390753.882
Test Loss of 29191836307.216286, Test MSE of 29191835789.074005
Epoch 52: training loss 19842262016.000
Test Loss of 28406878263.191303, Test MSE of 28406877997.544567
Epoch 53: training loss 19278580818.824
Test Loss of 27500263896.323849, Test MSE of 27500265048.457478
Epoch 54: training loss 17903164683.294
Test Loss of 25909958398.756420, Test MSE of 25909959641.209946
Epoch 55: training loss 17534587614.118
Test Loss of 25581181919.548462, Test MSE of 25581181486.001339
Epoch 56: training loss 17100220894.118
Test Loss of 24979770262.591721, Test MSE of 24979770074.005745
Epoch 57: training loss 16044640944.941
Test Loss of 26858356143.581772, Test MSE of 26858357266.532192
Epoch 58: training loss 15641042398.118
Test Loss of 24135973959.772381, Test MSE of 24135973926.360462
Epoch 59: training loss 15489353065.412
Test Loss of 24650012945.350914, Test MSE of 24650013193.355259
Epoch 60: training loss 14977404758.588
Test Loss of 24652708906.163311, Test MSE of 24652708971.266285
Epoch 61: training loss 14390434123.294
Test Loss of 25888628084.363636, Test MSE of 25888628167.217697
Epoch 62: training loss 14122661880.471
Test Loss of 23190998240.318298, Test MSE of 23190998473.636753
Epoch 63: training loss 13607349537.882
Test Loss of 27277748951.435577, Test MSE of 27277749229.069309
Epoch 64: training loss 12970478294.588
Test Loss of 24531738630.395557, Test MSE of 24531738715.691124
Epoch 65: training loss 12652017159.529
Test Loss of 22827197853.579460, Test MSE of 22827197640.703571
Epoch 66: training loss 12398432022.588
Test Loss of 23781813692.136017, Test MSE of 23781813933.476357
Epoch 67: training loss 11809231405.176
Test Loss of 25170120389.670135, Test MSE of 25170120515.807980
Epoch 68: training loss 11604022501.647
Test Loss of 23261778685.808929, Test MSE of 23261778548.535126
Epoch 69: training loss 11395952417.882
Test Loss of 24520787750.787880, Test MSE of 24520787726.859348
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24743283681.740257, 'MSE - std': 222495954.88090706, 'R2 - mean': 0.8152633104400301, 'R2 - std': 0.009674545236885046} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005439 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927212935.529
Test Loss of 447259072594.905396, Test MSE of 447259073414.705750
Epoch 2: training loss 421906517172.706
Test Loss of 447240663209.600769, Test MSE of 447240664683.141296
Epoch 3: training loss 421879137942.588
Test Loss of 447216219327.629883, Test MSE of 447216224763.665344
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421897247683.765
Test Loss of 447223472251.173706, Test MSE of 447223482734.015320
Epoch 2: training loss 421888437910.588
Test Loss of 447224414496.036987, Test MSE of 447224421259.342346
Epoch 3: training loss 421887988314.353
Test Loss of 447224507380.393250, Test MSE of 447224502196.939087
Epoch 4: training loss 416779514458.353
Test Loss of 430598019609.700684, Test MSE of 430598026472.742798
Epoch 5: training loss 381015704997.647
Test Loss of 375808232438.288208, Test MSE of 375808227973.682068
Epoch 6: training loss 313537173744.941
Test Loss of 297648338880.991882, Test MSE of 297648337614.387146
Epoch 7: training loss 221916012483.765
Test Loss of 197036647087.877869, Test MSE of 197036647756.525391
Epoch 8: training loss 156016446433.882
Test Loss of 154220517758.312286, Test MSE of 154220520519.934753
Epoch 9: training loss 137685187553.882
Test Loss of 142535498258.120758, Test MSE of 142535497667.057556
Epoch 10: training loss 132030982866.824
Test Loss of 137370023740.343277, Test MSE of 137370021632.856583
Epoch 11: training loss 128078458066.824
Test Loss of 133939355043.501266, Test MSE of 133939352095.900528
Epoch 12: training loss 127139138469.647
Test Loss of 132207759918.071716, Test MSE of 132207759626.873322
Epoch 13: training loss 121895140231.529
Test Loss of 127588259941.855194, Test MSE of 127588260694.064423
Epoch 14: training loss 118491697212.235
Test Loss of 124909393438.201248, Test MSE of 124909397105.595413
Epoch 15: training loss 114995835934.118
Test Loss of 120477714256.240570, Test MSE of 120477714557.518326
Epoch 16: training loss 110739934720.000
Test Loss of 118296687909.958832, Test MSE of 118296690832.790359
Epoch 17: training loss 107229661003.294
Test Loss of 114191083596.272964, Test MSE of 114191084504.284073
Epoch 18: training loss 103527847213.176
Test Loss of 111932694033.410126, Test MSE of 111932694535.071396
Epoch 19: training loss 100181978533.647
Test Loss of 105428487874.590790, Test MSE of 105428487022.882629
Epoch 20: training loss 94759856760.471
Test Loss of 102278990735.959290, Test MSE of 102278991391.701981
Epoch 21: training loss 91618304632.471
Test Loss of 99948969561.419388, Test MSE of 99948970281.863297
Epoch 22: training loss 87812435817.412
Test Loss of 93005361795.345825, Test MSE of 93005362575.365097
Epoch 23: training loss 85150334509.176
Test Loss of 92038247359.807541, Test MSE of 92038249776.808746
Epoch 24: training loss 80813377069.176
Test Loss of 88911241665.110336, Test MSE of 88911243576.239151
Epoch 25: training loss 78162462027.294
Test Loss of 84113505517.583160, Test MSE of 84113506588.233948
Epoch 26: training loss 74798443655.529
Test Loss of 83653965641.608139, Test MSE of 83653965812.203903
Epoch 27: training loss 70505720666.353
Test Loss of 78192240778.807312, Test MSE of 78192241700.944351
Epoch 28: training loss 67851315727.059
Test Loss of 77104254220.850342, Test MSE of 77104255557.623901
Epoch 29: training loss 64614554337.882
Test Loss of 69132463804.905853, Test MSE of 69132463423.558197
Epoch 30: training loss 61705063710.118
Test Loss of 66680057862.632431, Test MSE of 66680058200.014816
Epoch 31: training loss 58297168037.647
Test Loss of 60393491332.589409, Test MSE of 60393490884.074532
Epoch 32: training loss 55757579640.471
Test Loss of 61635963623.306038, Test MSE of 61635964466.922752
Epoch 33: training loss 53189265370.353
Test Loss of 59601203572.363640, Test MSE of 59601204070.875496
Epoch 34: training loss 50648383036.235
Test Loss of 53353997026.094841, Test MSE of 53353996833.036217
Epoch 35: training loss 48165530187.294
Test Loss of 53292542818.479759, Test MSE of 53292541389.392281
Epoch 36: training loss 45812363971.765
Test Loss of 49661364873.504509, Test MSE of 49661364755.429764
Epoch 37: training loss 43479513216.000
Test Loss of 46968891520.384918, Test MSE of 46968891092.795471
Epoch 38: training loss 40867113268.706
Test Loss of 45929938218.222534, Test MSE of 45929938339.460274
Epoch 39: training loss 39442887220.706
Test Loss of 40976090023.172798, Test MSE of 40976090292.113724
Epoch 40: training loss 36995365782.588
Test Loss of 40637504494.708305, Test MSE of 40637505338.842041
Epoch 41: training loss 35185911220.706
Test Loss of 40588016208.655098, Test MSE of 40588016294.995667
Epoch 42: training loss 33349925383.529
Test Loss of 38530015538.039322, Test MSE of 38530015273.410324
Epoch 43: training loss 32110728809.412
Test Loss of 38479872696.405273, Test MSE of 38479872366.159340
Epoch 44: training loss 30675810439.529
Test Loss of 37178628032.991905, Test MSE of 37178628353.568352
Epoch 45: training loss 29401220299.294
Test Loss of 34380449179.447609, Test MSE of 34380449035.636871
Epoch 46: training loss 28045544210.824
Test Loss of 35216862765.834839, Test MSE of 35216862867.586411
Epoch 47: training loss 26734302866.824
Test Loss of 30314725408.214664, Test MSE of 30314725421.353508
Epoch 48: training loss 25894731203.765
Test Loss of 31023723358.216053, Test MSE of 31023723915.667606
Epoch 49: training loss 24055808169.412
Test Loss of 31049319267.900993, Test MSE of 31049319044.767803
Epoch 50: training loss 23357068800.000
Test Loss of 28197712783.959286, Test MSE of 28197713139.915894
Epoch 51: training loss 22618239811.765
Test Loss of 29765743688.246124, Test MSE of 29765743542.753288
Epoch 52: training loss 21723685910.588
Test Loss of 25353981574.898914, Test MSE of 25353981635.082565
Epoch 53: training loss 20797267629.176
Test Loss of 34534708880.847557, Test MSE of 34534708851.190033
Epoch 54: training loss 20202082236.235
Test Loss of 28119970317.857044, Test MSE of 28119970248.165337
Epoch 55: training loss 19523295710.118
Test Loss of 26646090194.165165, Test MSE of 26646089734.233837
Epoch 56: training loss 18518110750.118
Test Loss of 26790839509.895905, Test MSE of 26790839389.282936
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25425802250.921154, 'MSE - std': 982174201.0351557, 'R2 - mean': 0.8173939237016136, 'R2 - std': 0.008454401778633514} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002493 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109601551.059
Test Loss of 410765008194.695068, Test MSE of 410765007274.161072
Epoch 2: training loss 430087131858.824
Test Loss of 410745179733.530762, Test MSE of 410745183507.675049
Epoch 3: training loss 430058758384.941
Test Loss of 410720362590.770935, Test MSE of 410720361415.626831
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430075224786.824
Test Loss of 410726368748.335022, Test MSE of 410726369719.301270
Epoch 2: training loss 430063494806.588
Test Loss of 410726647056.466431, Test MSE of 410726647052.990906
Epoch 3: training loss 430062979915.294
Test Loss of 410726297768.692261, Test MSE of 410726294508.254150
Epoch 4: training loss 424942410330.353
Test Loss of 394196330142.504395, Test MSE of 394196336564.154297
Epoch 5: training loss 388943774659.765
Test Loss of 340191805258.039795, Test MSE of 340191806076.187866
Epoch 6: training loss 321316099252.706
Test Loss of 262050521554.273010, Test MSE of 262050519635.722107
Epoch 7: training loss 229280783119.059
Test Loss of 165456468768.814423, Test MSE of 165456469808.415253
Epoch 8: training loss 164057196393.412
Test Loss of 125186399956.523834, Test MSE of 125186400606.036240
Epoch 9: training loss 144104883440.941
Test Loss of 114146201219.020828, Test MSE of 114146201237.397949
Epoch 10: training loss 138416342467.765
Test Loss of 110053608431.415085, Test MSE of 110053608005.719467
Epoch 11: training loss 134844914657.882
Test Loss of 107215632961.155014, Test MSE of 107215631652.660538
Epoch 12: training loss 131850574516.706
Test Loss of 104711825255.418793, Test MSE of 104711823518.507050
Epoch 13: training loss 128266572860.235
Test Loss of 101192430578.258209, Test MSE of 101192432499.419708
Epoch 14: training loss 123627673027.765
Test Loss of 98175322220.986588, Test MSE of 98175325340.477188
Epoch 15: training loss 120796700822.588
Test Loss of 95750967267.568726, Test MSE of 95750968999.234207
Epoch 16: training loss 117013369615.059
Test Loss of 91036138413.075424, Test MSE of 91036137751.451828
Epoch 17: training loss 112623922206.118
Test Loss of 88906429267.043030, Test MSE of 88906431078.717514
Epoch 18: training loss 108519911544.471
Test Loss of 86455260392.662659, Test MSE of 86455260154.009109
Epoch 19: training loss 104808048308.706
Test Loss of 82767482995.620544, Test MSE of 82767483107.999817
Epoch 20: training loss 100742930703.059
Test Loss of 80624590434.798706, Test MSE of 80624589722.383209
Epoch 21: training loss 96279598049.882
Test Loss of 77502264461.682556, Test MSE of 77502264025.129929
Epoch 22: training loss 93287429857.882
Test Loss of 72651615875.020828, Test MSE of 72651617137.270538
Epoch 23: training loss 89075098503.529
Test Loss of 70852535193.173538, Test MSE of 70852534769.269714
Epoch 24: training loss 85686818695.529
Test Loss of 68626498699.787132, Test MSE of 68626497662.187065
Epoch 25: training loss 81622956589.176
Test Loss of 65368549189.301247, Test MSE of 65368548324.892212
Epoch 26: training loss 77618469857.882
Test Loss of 63153318520.122162, Test MSE of 63153319364.173416
Epoch 27: training loss 74964431104.000
Test Loss of 57286133171.472466, Test MSE of 57286133864.489937
Epoch 28: training loss 70989154725.647
Test Loss of 56553888848.081444, Test MSE of 56553887962.344009
Epoch 29: training loss 67527441438.118
Test Loss of 53808375079.211479, Test MSE of 53808374802.952072
Epoch 30: training loss 64120541741.176
Test Loss of 52222146399.837112, Test MSE of 52222146503.002777
Epoch 31: training loss 61641531557.647
Test Loss of 49967545192.840355, Test MSE of 49967545084.801918
Epoch 32: training loss 58724826880.000
Test Loss of 49921166435.035629, Test MSE of 49921165913.818184
Epoch 33: training loss 55232843090.824
Test Loss of 46453302923.550209, Test MSE of 46453303404.467300
Epoch 34: training loss 52853946887.529
Test Loss of 45047595205.123550, Test MSE of 45047595419.806786
Epoch 35: training loss 50786161837.176
Test Loss of 41231651253.841743, Test MSE of 41231651289.210037
Epoch 36: training loss 47949953536.000
Test Loss of 40809849049.973160, Test MSE of 40809848996.950325
Epoch 37: training loss 45562242831.059
Test Loss of 38831848092.135124, Test MSE of 38831847989.837456
Epoch 38: training loss 43648335314.824
Test Loss of 35926677537.169830, Test MSE of 35926676989.825462
Epoch 39: training loss 40719375104.000
Test Loss of 33315631140.012959, Test MSE of 33315631744.717838
Epoch 40: training loss 39350667512.471
Test Loss of 33201697075.057842, Test MSE of 33201696871.640793
Epoch 41: training loss 37370973244.235
Test Loss of 31124375319.811199, Test MSE of 31124374660.470921
Epoch 42: training loss 35108183408.941
Test Loss of 28724426154.943081, Test MSE of 28724426219.045849
Epoch 43: training loss 33633396329.412
Test Loss of 31037668587.505783, Test MSE of 31037668501.584621
Epoch 44: training loss 31797331666.824
Test Loss of 27105978028.720036, Test MSE of 27105977746.876442
Epoch 45: training loss 30305340318.118
Test Loss of 27998603648.770016, Test MSE of 27998603452.591366
Epoch 46: training loss 29401426808.471
Test Loss of 25369665574.856087, Test MSE of 25369665228.440647
Epoch 47: training loss 28017060148.706
Test Loss of 24737601594.757984, Test MSE of 24737601675.576488
Epoch 48: training loss 26870130499.765
Test Loss of 24430086209.391949, Test MSE of 24430086272.236534
Epoch 49: training loss 25424905505.882
Test Loss of 23474250919.270706, Test MSE of 23474250594.316849
Epoch 50: training loss 24620677266.824
Test Loss of 25045307250.791302, Test MSE of 25045307160.954777
Epoch 51: training loss 23764093176.471
Test Loss of 21380198941.142063, Test MSE of 21380198809.011162
Epoch 52: training loss 22572338804.706
Test Loss of 21854455904.666359, Test MSE of 21854455864.316505
Epoch 53: training loss 21757252608.000
Test Loss of 19340213790.563629, Test MSE of 19340213668.931507
Epoch 54: training loss 21115423954.824
Test Loss of 19599673173.412308, Test MSE of 19599673287.459965
Epoch 55: training loss 20428242729.412
Test Loss of 20921497128.040722, Test MSE of 20921497613.875217
Epoch 56: training loss 19529229342.118
Test Loss of 19996128134.219341, Test MSE of 19996127896.857979
Epoch 57: training loss 19121878765.176
Test Loss of 22076438564.012959, Test MSE of 22076438825.217190
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24588461394.495163, 'MSE - std': 1681344328.3292437, 'R2 - mean': 0.8174933540261073, 'R2 - std': 0.007323751855723127} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004907 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043462776.471
Test Loss of 431611409962.883850, Test MSE of 431611410073.306885
Epoch 2: training loss 424024083034.353
Test Loss of 431592051065.188354, Test MSE of 431592049859.409485
Epoch 3: training loss 423996444431.059
Test Loss of 431564660792.862549, Test MSE of 431564668299.047302
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424011855992.471
Test Loss of 431567686442.291504, Test MSE of 431567685649.719788
Epoch 2: training loss 423998700122.353
Test Loss of 431570464747.150391, Test MSE of 431570465200.007202
Epoch 3: training loss 423998108009.412
Test Loss of 431570087270.708008, Test MSE of 431570087731.620789
Epoch 4: training loss 419045219388.235
Test Loss of 415005895796.094421, Test MSE of 415005899907.425598
Epoch 5: training loss 383577607589.647
Test Loss of 360232583243.816772, Test MSE of 360232581399.811890
Epoch 6: training loss 316461951216.941
Test Loss of 281659951049.980591, Test MSE of 281659953192.087280
Epoch 7: training loss 224910903235.765
Test Loss of 179975856086.300781, Test MSE of 179975853527.438812
Epoch 8: training loss 159151395719.529
Test Loss of 137951887913.462280, Test MSE of 137951884736.945587
Epoch 9: training loss 139985364931.765
Test Loss of 126621748518.263763, Test MSE of 126621748936.499313
Epoch 10: training loss 137304064225.882
Test Loss of 121741257802.395187, Test MSE of 121741257857.318954
Epoch 11: training loss 132462549232.941
Test Loss of 118667445562.165665, Test MSE of 118667444615.706024
Epoch 12: training loss 127965317285.647
Test Loss of 114594345594.965286, Test MSE of 114594344086.356613
Epoch 13: training loss 126185677071.059
Test Loss of 111857528837.686249, Test MSE of 111857529847.549744
Epoch 14: training loss 122006375996.235
Test Loss of 108264770808.773712, Test MSE of 108264771796.285080
Epoch 15: training loss 117379519518.118
Test Loss of 105054210312.410919, Test MSE of 105054207940.096405
Epoch 16: training loss 113628981940.706
Test Loss of 99548217450.143448, Test MSE of 99548219084.961105
Epoch 17: training loss 109870022324.706
Test Loss of 96450833145.010651, Test MSE of 96450835518.889862
Epoch 18: training loss 106738471213.176
Test Loss of 92403932854.197128, Test MSE of 92403934635.001785
Epoch 19: training loss 102742074307.765
Test Loss of 91209810071.633499, Test MSE of 91209809580.762787
Epoch 20: training loss 98480295137.882
Test Loss of 85310406891.031937, Test MSE of 85310406606.863373
Epoch 21: training loss 95644361667.765
Test Loss of 82140448588.882919, Test MSE of 82140448096.366501
Epoch 22: training loss 91460136508.235
Test Loss of 77765647610.669128, Test MSE of 77765649537.126556
Epoch 23: training loss 86902540197.647
Test Loss of 73136043490.857941, Test MSE of 73136042406.283493
Epoch 24: training loss 84129519299.765
Test Loss of 72772429881.810272, Test MSE of 72772428048.701767
Epoch 25: training loss 80973037854.118
Test Loss of 67482296873.462288, Test MSE of 67482296460.667831
Epoch 26: training loss 77052152515.765
Test Loss of 66526930128.022209, Test MSE of 66526930947.331505
Epoch 27: training loss 74349658029.176
Test Loss of 63168022339.405830, Test MSE of 63168021782.932251
Epoch 28: training loss 71099214501.647
Test Loss of 61351412991.407684, Test MSE of 61351413677.625328
Epoch 29: training loss 67478406746.353
Test Loss of 57886661983.126328, Test MSE of 57886661476.754051
Epoch 30: training loss 64089773748.706
Test Loss of 54699107007.200371, Test MSE of 54699107327.731148
Epoch 31: training loss 61158633178.353
Test Loss of 50455869194.543266, Test MSE of 50455867593.793159
Epoch 32: training loss 58719880116.706
Test Loss of 49668642960.999535, Test MSE of 49668642854.473640
Epoch 33: training loss 55539602236.235
Test Loss of 47035073569.643684, Test MSE of 47035074293.230194
Epoch 34: training loss 52928160512.000
Test Loss of 46780824851.309578, Test MSE of 46780825200.739212
Epoch 35: training loss 50666013620.706
Test Loss of 44612378081.910225, Test MSE of 44612377716.090904
Epoch 36: training loss 47800915275.294
Test Loss of 40797405985.762146, Test MSE of 40797406394.208244
Epoch 37: training loss 45785000568.471
Test Loss of 39509176981.501160, Test MSE of 39509177199.795357
Epoch 38: training loss 43157130307.765
Test Loss of 36031008600.255440, Test MSE of 36031009499.600868
Epoch 39: training loss 41402723689.412
Test Loss of 35079420825.173531, Test MSE of 35079420867.776344
Epoch 40: training loss 39482773082.353
Test Loss of 35546862100.612679, Test MSE of 35546862054.767570
Epoch 41: training loss 37516826255.059
Test Loss of 32326772500.020359, Test MSE of 32326773304.899017
Epoch 42: training loss 36225544003.765
Test Loss of 31125867874.917168, Test MSE of 31125867431.773766
Epoch 43: training loss 33834430125.176
Test Loss of 28319953813.382694, Test MSE of 28319954448.555073
Epoch 44: training loss 33235263216.941
Test Loss of 27853060520.573807, Test MSE of 27853060955.706707
Epoch 45: training loss 31122599130.353
Test Loss of 27611639002.447014, Test MSE of 27611639331.427731
Epoch 46: training loss 30023421869.176
Test Loss of 24295615015.566868, Test MSE of 24295615224.993374
Epoch 47: training loss 28638733473.882
Test Loss of 27153934595.672375, Test MSE of 27153934979.852917
Epoch 48: training loss 27526195648.000
Test Loss of 27249387952.155483, Test MSE of 27249387318.958843
Epoch 49: training loss 26309527066.353
Test Loss of 23969597404.934753, Test MSE of 23969597320.182091
Epoch 50: training loss 25093806976.000
Test Loss of 24971185131.624249, Test MSE of 24971184841.658764
Epoch 51: training loss 24354796129.882
Test Loss of 21691320916.109207, Test MSE of 21691320894.952572
Epoch 52: training loss 23162028099.765
Test Loss of 24797073440.222118, Test MSE of 24797073565.208206
Epoch 53: training loss 22407191405.176
Test Loss of 23938638014.489590, Test MSE of 23938638668.643494
Epoch 54: training loss 21516637135.059
Test Loss of 24835310854.989357, Test MSE of 24835310913.010715
Epoch 55: training loss 20757477537.882
Test Loss of 22971986947.316982, Test MSE of 22971987784.677715
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24265166672.531673, 'MSE - std': 1636952323.4724123, 'R2 - mean': 0.8196835516884005, 'R2 - std': 0.007880211684658132} 
 

Saving model.....
Results After CV: {'MSE - mean': 24265166672.531673, 'MSE - std': 1636952323.4724123, 'R2 - mean': 0.8196835516884005, 'R2 - std': 0.007880211684658132}
Train time: 90.04702587139909
Inference time: 0.07099612840102054
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 94 finished with value: 24265166672.531673 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005572 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525562127.059
Test Loss of 418111452904.964172, Test MSE of 418111454253.864807
Epoch 2: training loss 427505226450.824
Test Loss of 418093308324.211914, Test MSE of 418093318035.749390
Epoch 3: training loss 427477422561.882
Test Loss of 418069016101.307434, Test MSE of 418069016687.725525
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427490447119.059
Test Loss of 418073540912.618103, Test MSE of 418073547520.460388
Epoch 2: training loss 427481450255.059
Test Loss of 418074816151.006226, Test MSE of 418074810322.536316
Epoch 3: training loss 427481050413.176
Test Loss of 418074608673.399048, Test MSE of 418074608202.898193
Epoch 4: training loss 427480723576.471
Test Loss of 418074165225.260254, Test MSE of 418074164126.796448
Epoch 5: training loss 420187118893.176
Test Loss of 395327788744.749451, Test MSE of 395327796359.946777
Epoch 6: training loss 373670344463.059
Test Loss of 328179936001.835754, Test MSE of 328179937620.409302
Epoch 7: training loss 294722000474.353
Test Loss of 243044929923.523468, Test MSE of 243044931816.833801
Epoch 8: training loss 217914285598.118
Test Loss of 174034059548.247040, Test MSE of 174034058185.501434
Epoch 9: training loss 158316847435.294
Test Loss of 125846313213.453613, Test MSE of 125846312525.977371
Epoch 10: training loss 139737402699.294
Test Loss of 118416370469.840393, Test MSE of 118416372266.533920
Epoch 11: training loss 135928099312.941
Test Loss of 115287311891.778854, Test MSE of 115287312052.520386
Epoch 12: training loss 133255366339.765
Test Loss of 112583136980.119354, Test MSE of 112583136990.515518
Epoch 13: training loss 129294370590.118
Test Loss of 109059817968.484848, Test MSE of 109059818348.380463
Epoch 14: training loss 125223345950.118
Test Loss of 106158935996.728195, Test MSE of 106158937397.207932
Epoch 15: training loss 121083418337.882
Test Loss of 101872166215.120987, Test MSE of 101872165260.643127
Epoch 16: training loss 118898024508.235
Test Loss of 99297246978.072632, Test MSE of 99297246071.157181
Epoch 17: training loss 114388282119.529
Test Loss of 96148146583.894516, Test MSE of 96148146408.554535
Epoch 18: training loss 108714879262.118
Test Loss of 93016797050.403885, Test MSE of 93016799166.411667
Epoch 19: training loss 105906784677.647
Test Loss of 89332346579.645615, Test MSE of 89332346391.565491
Epoch 20: training loss 103099238851.765
Test Loss of 85835179698.957199, Test MSE of 85835180725.946732
Epoch 21: training loss 98672371230.118
Test Loss of 84530899204.086044, Test MSE of 84530899704.604767
Epoch 22: training loss 95381719823.059
Test Loss of 80552518911.111725, Test MSE of 80552520935.639725
Epoch 23: training loss 90389546729.412
Test Loss of 77176943729.935699, Test MSE of 77176944013.711899
Epoch 24: training loss 87643019535.059
Test Loss of 74023779684.966919, Test MSE of 74023780230.170471
Epoch 25: training loss 84002561249.882
Test Loss of 70189381629.631271, Test MSE of 70189382307.354645
Epoch 26: training loss 80230433995.294
Test Loss of 66635044166.173492, Test MSE of 66635044627.098442
Epoch 27: training loss 76734226469.647
Test Loss of 64461297074.187370, Test MSE of 64461297217.915871
Epoch 28: training loss 73404905513.412
Test Loss of 60665975201.843163, Test MSE of 60665975040.618690
Epoch 29: training loss 69829988510.118
Test Loss of 58036586051.153366, Test MSE of 58036585563.456497
Epoch 30: training loss 66388071499.294
Test Loss of 56368549556.378441, Test MSE of 56368549286.649017
Epoch 31: training loss 63994753400.471
Test Loss of 54142690972.217445, Test MSE of 54142692115.098709
Epoch 32: training loss 60275099888.941
Test Loss of 49405609411.242195, Test MSE of 49405609706.787872
Epoch 33: training loss 57363149628.235
Test Loss of 49900741118.697205, Test MSE of 49900741839.867424
Epoch 34: training loss 54207617528.471
Test Loss of 49452049844.082352, Test MSE of 49452048831.053307
Epoch 35: training loss 52266163463.529
Test Loss of 42835424049.684013, Test MSE of 42835423915.858215
Epoch 36: training loss 49813480361.412
Test Loss of 43195073436.750404, Test MSE of 43195073476.626205
Epoch 37: training loss 47177875200.000
Test Loss of 38439726431.281982, Test MSE of 38439726872.342720
Epoch 38: training loss 44814725451.294
Test Loss of 37843864260.248901, Test MSE of 37843863758.948486
Epoch 39: training loss 42513516265.412
Test Loss of 32858243321.189915, Test MSE of 32858243531.766830
Epoch 40: training loss 40504486904.471
Test Loss of 35496314878.578766, Test MSE of 35496314951.073456
Epoch 41: training loss 38583457739.294
Test Loss of 32251779817.201019, Test MSE of 32251779838.656334
Epoch 42: training loss 36213642812.235
Test Loss of 32408933393.291695, Test MSE of 32408933167.605534
Epoch 43: training loss 34861576636.235
Test Loss of 31750775521.147350, Test MSE of 31750775319.581051
Epoch 44: training loss 32856170842.353
Test Loss of 28809094457.856117, Test MSE of 28809094132.732864
Epoch 45: training loss 31679197530.353
Test Loss of 27750611416.323849, Test MSE of 27750611403.896423
Epoch 46: training loss 29994797534.118
Test Loss of 26883791783.646542, Test MSE of 26883791959.689594
Epoch 47: training loss 28402561118.118
Test Loss of 26823127598.071709, Test MSE of 26823127877.784309
Epoch 48: training loss 27114414140.235
Test Loss of 26526019926.043949, Test MSE of 26526020118.290558
Epoch 49: training loss 26067082130.824
Test Loss of 25009408865.058525, Test MSE of 25009409108.160061
Epoch 50: training loss 24718952504.471
Test Loss of 26222950879.903770, Test MSE of 26222951400.665741
Epoch 51: training loss 24026824700.235
Test Loss of 21789637530.381680, Test MSE of 21789637864.245655
Epoch 52: training loss 23035477108.706
Test Loss of 24505219708.239647, Test MSE of 24505219391.599850
Epoch 53: training loss 21979663009.882
Test Loss of 20720268432.255379, Test MSE of 20720268135.066483
Epoch 54: training loss 21000855363.765
Test Loss of 20645578749.157528, Test MSE of 20645578939.644058
Epoch 55: training loss 20502444958.118
Test Loss of 21038604338.690723, Test MSE of 21038604340.126820
Epoch 56: training loss 19670580058.353
Test Loss of 21704296935.009945, Test MSE of 21704297570.125340
Epoch 57: training loss 18944755757.176
Test Loss of 19709637015.894516, Test MSE of 19709636982.942371
Epoch 58: training loss 18490547463.529
Test Loss of 20214514783.933380, Test MSE of 20214514923.961262
Epoch 59: training loss 17591306834.824
Test Loss of 20264559864.953041, Test MSE of 20264559937.135056
Epoch 60: training loss 17341725530.353
Test Loss of 20198891757.820034, Test MSE of 20198891478.575123
Epoch 61: training loss 16932399702.588
Test Loss of 19969799390.897060, Test MSE of 19969799283.958733
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19969799283.958733, 'MSE - std': 0.0, 'R2 - mean': 0.8444930062690724, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005378 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917504752.941
Test Loss of 424555848591.485535, Test MSE of 424555857661.989014
Epoch 2: training loss 427896206516.706
Test Loss of 424538952143.322693, Test MSE of 424538963006.390381
Epoch 3: training loss 427868022904.471
Test Loss of 424516357792.481140, Test MSE of 424516357957.033936
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887735024.941
Test Loss of 424521363898.477905, Test MSE of 424521372103.942383
Epoch 2: training loss 427876139971.765
Test Loss of 424522890972.173035, Test MSE of 424522895293.624146
Epoch 3: training loss 427875580747.294
Test Loss of 424522935322.292847, Test MSE of 424522945539.925171
Epoch 4: training loss 427875139584.000
Test Loss of 424523269968.240601, Test MSE of 424523273797.499634
Epoch 5: training loss 421119691113.412
Test Loss of 403128383352.982666, Test MSE of 403128386673.267761
Epoch 6: training loss 375659705524.706
Test Loss of 338162756888.220215, Test MSE of 338162754343.355774
Epoch 7: training loss 296382301967.059
Test Loss of 253471952961.376831, Test MSE of 253471951172.120636
Epoch 8: training loss 217115884905.412
Test Loss of 184723017516.709686, Test MSE of 184723014288.204468
Epoch 9: training loss 156284767232.000
Test Loss of 137206337590.006943, Test MSE of 137206337349.450012
Epoch 10: training loss 136659759585.882
Test Loss of 129571183014.106873, Test MSE of 129571183301.307556
Epoch 11: training loss 132789648414.118
Test Loss of 126655464549.618317, Test MSE of 126655464411.198669
Epoch 12: training loss 130328186066.824
Test Loss of 123970963249.920883, Test MSE of 123970963893.935455
Epoch 13: training loss 127099811568.941
Test Loss of 120816682703.381912, Test MSE of 120816682185.590805
Epoch 14: training loss 121764733048.471
Test Loss of 115910180877.264862, Test MSE of 115910181832.266541
Epoch 15: training loss 118273572141.176
Test Loss of 113833172423.505905, Test MSE of 113833170171.294159
Epoch 16: training loss 114878546582.588
Test Loss of 109419856579.775162, Test MSE of 109419857052.079987
Epoch 17: training loss 109571508645.647
Test Loss of 106750947699.416153, Test MSE of 106750949392.594864
Epoch 18: training loss 106426412845.176
Test Loss of 100801316885.318527, Test MSE of 100801314377.902344
Epoch 19: training loss 102943605970.824
Test Loss of 97826502285.057602, Test MSE of 97826503707.682877
Epoch 20: training loss 97855081953.882
Test Loss of 96166966630.388153, Test MSE of 96166967383.090759
Epoch 21: training loss 94734113039.059
Test Loss of 89956897833.926437, Test MSE of 89956897176.842789
Epoch 22: training loss 89915265024.000
Test Loss of 85223545279.689102, Test MSE of 85223545233.121277
Epoch 23: training loss 85576495872.000
Test Loss of 83960041515.584549, Test MSE of 83960040310.671524
Epoch 24: training loss 82699757056.000
Test Loss of 78367431231.600281, Test MSE of 78367429844.073334
Epoch 25: training loss 79448721317.647
Test Loss of 77630216253.113113, Test MSE of 77630215848.057800
Epoch 26: training loss 75116838580.706
Test Loss of 74375594964.652328, Test MSE of 74375595700.503387
Epoch 27: training loss 70976992195.765
Test Loss of 69067699470.271576, Test MSE of 69067699229.774582
Epoch 28: training loss 68321294230.588
Test Loss of 65984481033.178810, Test MSE of 65984482073.320412
Epoch 29: training loss 63766378270.118
Test Loss of 64831478046.615776, Test MSE of 64831479364.922798
Epoch 30: training loss 61441839239.529
Test Loss of 60751485380.426552, Test MSE of 60751485866.833092
Epoch 31: training loss 57297269985.882
Test Loss of 55138796494.493637, Test MSE of 55138797527.064110
Epoch 32: training loss 54765893150.118
Test Loss of 53404991116.110107, Test MSE of 53404991979.172577
Epoch 33: training loss 51955182351.059
Test Loss of 51606549236.097153, Test MSE of 51606549430.970749
Epoch 34: training loss 50078618300.235
Test Loss of 49599614379.081192, Test MSE of 49599614006.135323
Epoch 35: training loss 46597171990.588
Test Loss of 45622325140.459869, Test MSE of 45622325107.124977
Epoch 36: training loss 44053329995.294
Test Loss of 46458134795.665970, Test MSE of 46458135480.157829
Epoch 37: training loss 42172436992.000
Test Loss of 42206858499.849182, Test MSE of 42206858186.581154
Epoch 38: training loss 39306290725.647
Test Loss of 40094958710.673141, Test MSE of 40094959173.611382
Epoch 39: training loss 37360147930.353
Test Loss of 39812831925.089058, Test MSE of 39812832343.664673
Epoch 40: training loss 35083145999.059
Test Loss of 37577302215.446678, Test MSE of 37577302789.994392
Epoch 41: training loss 33363031800.471
Test Loss of 35474982809.197319, Test MSE of 35474984029.675400
Epoch 42: training loss 31295343879.529
Test Loss of 36760629737.378670, Test MSE of 36760629654.330719
Epoch 43: training loss 29984588032.000
Test Loss of 35602344885.385147, Test MSE of 35602344730.954834
Epoch 44: training loss 28040360139.294
Test Loss of 30556950076.047188, Test MSE of 30556950885.746117
Epoch 45: training loss 26655601253.647
Test Loss of 28829291279.574371, Test MSE of 28829291560.631935
Epoch 46: training loss 24961691328.000
Test Loss of 29823241969.965302, Test MSE of 29823242546.839840
Epoch 47: training loss 23914158486.588
Test Loss of 27100289648.396023, Test MSE of 27100289521.229046
Epoch 48: training loss 22739592658.824
Test Loss of 29840823581.905159, Test MSE of 29840825249.530582
Epoch 49: training loss 21849537513.412
Test Loss of 26674026304.133240, Test MSE of 26674025314.533878
Epoch 50: training loss 20792399322.353
Test Loss of 26579883680.954891, Test MSE of 26579883810.951714
Epoch 51: training loss 19850055337.412
Test Loss of 25424469237.636826, Test MSE of 25424468768.352272
Epoch 52: training loss 18986582053.647
Test Loss of 25436105706.444599, Test MSE of 25436105440.493431
Epoch 53: training loss 18129123892.706
Test Loss of 25736311837.135323, Test MSE of 25736311724.150970
Epoch 54: training loss 17585236329.412
Test Loss of 25096004288.222069, Test MSE of 25096004116.778244
Epoch 55: training loss 16828623280.941
Test Loss of 26350366717.631275, Test MSE of 26350367409.644295
Epoch 56: training loss 16294678640.941
Test Loss of 24455528158.778625, Test MSE of 24455528168.926247
Epoch 57: training loss 15772685560.471
Test Loss of 24336469637.951424, Test MSE of 24336469082.631767
Epoch 58: training loss 15084624045.176
Test Loss of 24128868410.270645, Test MSE of 24128868198.455650
Epoch 59: training loss 14752787783.529
Test Loss of 23602906499.049732, Test MSE of 23602906194.333817
Epoch 60: training loss 14342119420.235
Test Loss of 25064546561.954197, Test MSE of 25064546926.778255
Epoch 61: training loss 13791531870.118
Test Loss of 22060244044.983578, Test MSE of 22060244246.958862
Epoch 62: training loss 13564806539.294
Test Loss of 24490298065.040020, Test MSE of 24490298685.436737
Epoch 63: training loss 13034961298.824
Test Loss of 22664430335.940781, Test MSE of 22664430357.760395
Epoch 64: training loss 12416166467.765
Test Loss of 21367096536.264629, Test MSE of 21367096804.223030
Epoch 65: training loss 12245653673.412
Test Loss of 23230007436.228546, Test MSE of 23230007450.569817
Epoch 66: training loss 12091078471.529
Test Loss of 23925332656.588482, Test MSE of 23925332973.127228
Epoch 67: training loss 11837855239.529
Test Loss of 23054602057.371269, Test MSE of 23054602431.914234
Epoch 68: training loss 11286782076.235
Test Loss of 24169646171.195930, Test MSE of 24169646137.741856
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22069722710.850296, 'MSE - std': 2099923426.8915615, 'R2 - mean': 0.8359688898797113, 'R2 - std': 0.008524116389361125} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005462 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421926136771.765
Test Loss of 447256908845.716431, Test MSE of 447256915347.315063
Epoch 2: training loss 421905357522.824
Test Loss of 447237873131.510498, Test MSE of 447237867510.673889
Epoch 3: training loss 421878355004.235
Test Loss of 447212897381.618347, Test MSE of 447212900957.062744
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898278550.588
Test Loss of 447220126610.328003, Test MSE of 447220125969.456482
Epoch 2: training loss 421886071988.706
Test Loss of 447221010353.358337, Test MSE of 447221011015.985413
Epoch 3: training loss 421885465539.765
Test Loss of 447221472880.159119, Test MSE of 447221474462.569824
Epoch 4: training loss 421885044133.647
Test Loss of 447222035355.802917, Test MSE of 447222032990.595398
Epoch 5: training loss 414845506499.765
Test Loss of 425021426575.959290, Test MSE of 425021419388.721924
Epoch 6: training loss 368924307094.588
Test Loss of 356908912769.806152, Test MSE of 356908909060.853455
Epoch 7: training loss 290169346288.941
Test Loss of 270382912277.259308, Test MSE of 270382912924.952454
Epoch 8: training loss 211997126896.941
Test Loss of 199349590424.842010, Test MSE of 199349589430.923126
Epoch 9: training loss 154001910211.765
Test Loss of 148920701012.326630, Test MSE of 148920701806.315704
Epoch 10: training loss 134585031499.294
Test Loss of 140081652293.758972, Test MSE of 140081652638.716400
Epoch 11: training loss 130495815258.353
Test Loss of 136534307645.053894, Test MSE of 136534308244.227249
Epoch 12: training loss 127894062140.235
Test Loss of 133370153746.890579, Test MSE of 133370151286.441620
Epoch 13: training loss 124642231958.588
Test Loss of 129878946086.669449, Test MSE of 129878945940.187836
Epoch 14: training loss 120417722368.000
Test Loss of 126062005189.018738, Test MSE of 126062009055.431763
Epoch 15: training loss 116741914443.294
Test Loss of 123110370822.987747, Test MSE of 123110371777.486542
Epoch 16: training loss 112705868950.588
Test Loss of 119574188775.779785, Test MSE of 119574189192.723114
Epoch 17: training loss 109310650157.176
Test Loss of 114297633459.194077, Test MSE of 114297633325.970779
Epoch 18: training loss 104948000346.353
Test Loss of 112507157898.866531, Test MSE of 112507158345.471542
Epoch 19: training loss 101861962029.176
Test Loss of 107808815590.536209, Test MSE of 107808816971.896011
Epoch 20: training loss 96518195320.471
Test Loss of 105226988771.160767, Test MSE of 105226988329.780991
Epoch 21: training loss 93744806505.412
Test Loss of 98654707295.104324, Test MSE of 98654709776.453568
Epoch 22: training loss 88613050789.647
Test Loss of 97744334403.627106, Test MSE of 97744334762.589096
Epoch 23: training loss 85113020431.059
Test Loss of 92142011933.253754, Test MSE of 92142012453.864487
Epoch 24: training loss 81938130763.294
Test Loss of 87590864968.483002, Test MSE of 87590865955.812546
Epoch 25: training loss 77992644186.353
Test Loss of 83484556464.943787, Test MSE of 83484557559.389984
Epoch 26: training loss 74128000376.471
Test Loss of 79475530959.263474, Test MSE of 79475530082.216766
Epoch 27: training loss 70999841264.941
Test Loss of 79676851023.056213, Test MSE of 79676851432.581802
Epoch 28: training loss 67781550561.882
Test Loss of 69144471004.824432, Test MSE of 69144470970.645782
Epoch 29: training loss 64412429161.412
Test Loss of 68883274366.134628, Test MSE of 68883274339.670151
Epoch 30: training loss 61244609792.000
Test Loss of 69223625459.149658, Test MSE of 69223623931.799042
Epoch 31: training loss 58276135469.176
Test Loss of 64170029566.697205, Test MSE of 64170030631.066307
Epoch 32: training loss 55724276103.529
Test Loss of 59932337990.765671, Test MSE of 59932337073.187805
Epoch 33: training loss 52585031183.059
Test Loss of 55014899268.574600, Test MSE of 55014899746.785912
Epoch 34: training loss 49797406659.765
Test Loss of 57272470395.114502, Test MSE of 57272471324.082047
Epoch 35: training loss 47987378665.412
Test Loss of 52241395104.658798, Test MSE of 52241394972.391426
Epoch 36: training loss 44385153377.882
Test Loss of 45884028514.420540, Test MSE of 45884028161.951363
Epoch 37: training loss 42521502772.706
Test Loss of 48875179275.902847, Test MSE of 48875179276.801620
Epoch 38: training loss 40351189714.824
Test Loss of 45544805308.017578, Test MSE of 45544805005.729446
Epoch 39: training loss 37944300845.176
Test Loss of 40512248556.043488, Test MSE of 40512248325.798012
Epoch 40: training loss 35935597123.765
Test Loss of 40717995505.195465, Test MSE of 40717995076.620789
Epoch 41: training loss 34255953686.588
Test Loss of 37732692753.469353, Test MSE of 37732692388.573418
Epoch 42: training loss 32061342945.882
Test Loss of 34649330258.076332, Test MSE of 34649330389.801239
Epoch 43: training loss 30584123075.765
Test Loss of 33466640859.166321, Test MSE of 33466640483.505508
Epoch 44: training loss 29235171139.765
Test Loss of 35641928274.550079, Test MSE of 35641927722.390556
Epoch 45: training loss 27497141263.059
Test Loss of 33666962267.373585, Test MSE of 33666962866.448689
Epoch 46: training loss 26381466917.647
Test Loss of 35019226264.309044, Test MSE of 35019227014.824646
Epoch 47: training loss 25412746202.353
Test Loss of 30026599387.758499, Test MSE of 30026599539.514240
Epoch 48: training loss 24314713008.941
Test Loss of 29385953378.065231, Test MSE of 29385953636.814575
Epoch 49: training loss 22947247446.588
Test Loss of 30514833597.024288, Test MSE of 30514833295.077759
Epoch 50: training loss 21690232602.353
Test Loss of 27740760640.547768, Test MSE of 27740761207.143921
Epoch 51: training loss 21049160011.294
Test Loss of 27592956833.961601, Test MSE of 27592956861.143883
Epoch 52: training loss 20520059904.000
Test Loss of 26030047924.141567, Test MSE of 26030047969.741489
Epoch 53: training loss 19537608015.059
Test Loss of 27412243725.324081, Test MSE of 27412243754.271671
Epoch 54: training loss 18772511815.529
Test Loss of 25923799874.265095, Test MSE of 25923799789.951118
Epoch 55: training loss 18146065415.529
Test Loss of 26265218342.906315, Test MSE of 26265218304.715473
Epoch 56: training loss 17733759126.588
Test Loss of 26215040257.954197, Test MSE of 26215040470.185955
Epoch 57: training loss 16890713886.118
Test Loss of 25649746241.199165, Test MSE of 25649746494.533100
Epoch 58: training loss 16544945833.412
Test Loss of 25673931746.864677, Test MSE of 25673931545.025799
Epoch 59: training loss 15828632515.765
Test Loss of 23629482134.650936, Test MSE of 23629482138.828800
Epoch 60: training loss 15652143216.941
Test Loss of 21713642414.752720, Test MSE of 21713642421.111332
Epoch 61: training loss 15043999201.882
Test Loss of 23961173440.873466, Test MSE of 23961173327.843803
Epoch 62: training loss 14569188299.294
Test Loss of 24275259770.996067, Test MSE of 24275259463.025742
Epoch 63: training loss 14113985264.941
Test Loss of 25048342065.624798, Test MSE of 25048341679.862675
Epoch 64: training loss 13749648594.824
Test Loss of 22632720946.335415, Test MSE of 22632721003.802582
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22257388808.501057, 'MSE - std': 1734999345.135399, 'R2 - mean': 0.8404244165549234, 'R2 - std': 0.009388493456707296} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005605 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109556254.118
Test Loss of 410764982750.593262, Test MSE of 410764984216.084229
Epoch 2: training loss 430088130319.059
Test Loss of 410745729420.142517, Test MSE of 410745733169.218445
Epoch 3: training loss 430060357752.471
Test Loss of 410720770181.627014, Test MSE of 410720773582.884216
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078283776.000
Test Loss of 410727078014.045349, Test MSE of 410727067247.442749
Epoch 2: training loss 430063972833.882
Test Loss of 410726829501.897278, Test MSE of 410726827557.501404
Epoch 3: training loss 430063492638.118
Test Loss of 410726761100.024048, Test MSE of 410726763502.750977
Epoch 4: training loss 430063129298.824
Test Loss of 410726330066.628418, Test MSE of 410726336336.201538
Epoch 5: training loss 423448254584.471
Test Loss of 389698964493.741760, Test MSE of 389698963007.103577
Epoch 6: training loss 378352473991.529
Test Loss of 323598871774.237854, Test MSE of 323598876056.440796
Epoch 7: training loss 299256128210.824
Test Loss of 238012599302.160126, Test MSE of 238012598022.230804
Epoch 8: training loss 220483203072.000
Test Loss of 168403456839.670532, Test MSE of 168403456032.339355
Epoch 9: training loss 161060348024.471
Test Loss of 120401347727.577972, Test MSE of 120401349203.047394
Epoch 10: training loss 141393513712.941
Test Loss of 112677232916.731140, Test MSE of 112677234963.325027
Epoch 11: training loss 137815106891.294
Test Loss of 109582519241.506714, Test MSE of 109582518203.522614
Epoch 12: training loss 135137556239.059
Test Loss of 107230741147.187408, Test MSE of 107230741619.611313
Epoch 13: training loss 130076212163.765
Test Loss of 104367314527.955582, Test MSE of 104367315511.192413
Epoch 14: training loss 127565254716.235
Test Loss of 100474968694.226746, Test MSE of 100474969925.831833
Epoch 15: training loss 122906701763.765
Test Loss of 97892946911.304031, Test MSE of 97892946565.704895
Epoch 16: training loss 118743167969.882
Test Loss of 93642877203.783432, Test MSE of 93642878416.788254
Epoch 17: training loss 114975186522.353
Test Loss of 91235967267.894501, Test MSE of 91235967001.308136
Epoch 18: training loss 111018738627.765
Test Loss of 88692582891.861176, Test MSE of 88692582159.311981
Epoch 19: training loss 106212043264.000
Test Loss of 84296445696.118469, Test MSE of 84296445394.590469
Epoch 20: training loss 103589399371.294
Test Loss of 81767743935.318832, Test MSE of 81767744160.497635
Epoch 21: training loss 99000858880.000
Test Loss of 77886036708.161041, Test MSE of 77886035396.241547
Epoch 22: training loss 94635591920.941
Test Loss of 75172109213.912079, Test MSE of 75172109399.002747
Epoch 23: training loss 90864126554.353
Test Loss of 72378750669.889862, Test MSE of 72378751501.318542
Epoch 24: training loss 88405257095.529
Test Loss of 68710898908.342438, Test MSE of 68710898849.620911
Epoch 25: training loss 83542855680.000
Test Loss of 66735485233.162422, Test MSE of 66735485112.139328
Epoch 26: training loss 80001108705.882
Test Loss of 62956043901.334564, Test MSE of 62956045370.913063
Epoch 27: training loss 77017162285.176
Test Loss of 60491235987.605736, Test MSE of 60491236328.563232
Epoch 28: training loss 73361127092.706
Test Loss of 57459770250.010178, Test MSE of 57459769202.723648
Epoch 29: training loss 69660905411.765
Test Loss of 56006465582.437759, Test MSE of 56006464851.347923
Epoch 30: training loss 66269348653.176
Test Loss of 52099819399.640907, Test MSE of 52099820877.768784
Epoch 31: training loss 63024126208.000
Test Loss of 50492259442.198982, Test MSE of 50492259646.099724
Epoch 32: training loss 60234099358.118
Test Loss of 48546269393.443779, Test MSE of 48546269232.367691
Epoch 33: training loss 57818742934.588
Test Loss of 47268555984.969925, Test MSE of 47268556009.001099
Epoch 34: training loss 54682348122.353
Test Loss of 44042082149.523369, Test MSE of 44042081700.859940
Epoch 35: training loss 51138089193.412
Test Loss of 41295767086.674690, Test MSE of 41295766567.268341
Epoch 36: training loss 49198523098.353
Test Loss of 41286057578.380379, Test MSE of 41286058045.179214
Epoch 37: training loss 46439959017.412
Test Loss of 35902268940.557152, Test MSE of 35902269105.409630
Epoch 38: training loss 44163125082.353
Test Loss of 37421816271.429893, Test MSE of 37421816896.285912
Epoch 39: training loss 42234764815.059
Test Loss of 35454007992.566406, Test MSE of 35454008462.510841
Epoch 40: training loss 40135514480.941
Test Loss of 32614042315.520592, Test MSE of 32614042666.493507
Epoch 41: training loss 37855124803.765
Test Loss of 31064663422.400742, Test MSE of 31064663491.129181
Epoch 42: training loss 36212889592.471
Test Loss of 29847576064.236927, Test MSE of 29847576040.096851
Epoch 43: training loss 33893306142.118
Test Loss of 29571522994.050903, Test MSE of 29571523056.462734
Epoch 44: training loss 32385855759.059
Test Loss of 26984882429.986118, Test MSE of 26984882357.782043
Epoch 45: training loss 30781737637.647
Test Loss of 24763484428.201759, Test MSE of 24763483659.702663
Epoch 46: training loss 29498119702.588
Test Loss of 22846935102.548820, Test MSE of 22846934937.004505
Epoch 47: training loss 28052140483.765
Test Loss of 25618493350.915318, Test MSE of 25618493556.909897
Epoch 48: training loss 26763207216.941
Test Loss of 23195397533.675152, Test MSE of 23195397404.278683
Epoch 49: training loss 25408235068.235
Test Loss of 24061117368.447941, Test MSE of 24061117316.585964
Epoch 50: training loss 24350802917.647
Test Loss of 21913290651.542805, Test MSE of 21913290808.892387
Epoch 51: training loss 23499441392.941
Test Loss of 22342796546.250809, Test MSE of 22342796399.037968
Epoch 52: training loss 22366713136.941
Test Loss of 22182954683.883389, Test MSE of 22182954757.674191
Epoch 53: training loss 21562352655.059
Test Loss of 19687210234.669136, Test MSE of 19687210155.731174
Epoch 54: training loss 20679444024.471
Test Loss of 21832877424.185101, Test MSE of 21832877505.360832
Epoch 55: training loss 20088068171.294
Test Loss of 21542539225.143913, Test MSE of 21542539080.558849
Epoch 56: training loss 19245780611.765
Test Loss of 21007506468.486813, Test MSE of 21007506325.963806
Epoch 57: training loss 18889538593.882
Test Loss of 19889412466.554375, Test MSE of 19889412572.414417
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21665394749.479397, 'MSE - std': 1819076123.6039627, 'R2 - mean': 0.8392788905303069, 'R2 - std': 0.008369262014012783} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003767 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043661793.882
Test Loss of 431613255979.476196, Test MSE of 431613257335.237854
Epoch 2: training loss 424023974369.882
Test Loss of 431592821712.140686, Test MSE of 431592820842.508545
Epoch 3: training loss 423996660916.706
Test Loss of 431564826816.385010, Test MSE of 431564826524.041931
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424008615092.706
Test Loss of 431568817795.020813, Test MSE of 431568819383.162964
Epoch 2: training loss 423998929618.824
Test Loss of 431571032311.352173, Test MSE of 431571040807.180847
Epoch 3: training loss 423998297389.176
Test Loss of 431571181761.332703, Test MSE of 431571178986.270081
Epoch 4: training loss 423997880079.059
Test Loss of 431571402691.346619, Test MSE of 431571405593.523621
Epoch 5: training loss 417293899655.529
Test Loss of 409887946335.955566, Test MSE of 409887947547.930542
Epoch 6: training loss 372206428400.941
Test Loss of 342691940260.072205, Test MSE of 342691936862.738342
Epoch 7: training loss 294329330928.941
Test Loss of 256162833070.141602, Test MSE of 256162829264.310150
Epoch 8: training loss 215752513746.824
Test Loss of 183669216564.479401, Test MSE of 183669216838.473541
Epoch 9: training loss 158015364065.882
Test Loss of 132917839579.157791, Test MSE of 132917838950.398941
Epoch 10: training loss 138252123196.235
Test Loss of 124036643882.173065, Test MSE of 124036645874.951035
Epoch 11: training loss 134300544843.294
Test Loss of 121131158299.128174, Test MSE of 121131156844.724930
Epoch 12: training loss 131636600169.412
Test Loss of 118548912489.077286, Test MSE of 118548913150.648346
Epoch 13: training loss 127954283369.412
Test Loss of 114774416001.599258, Test MSE of 114774415162.094925
Epoch 14: training loss 124649966832.941
Test Loss of 110181799629.889862, Test MSE of 110181799120.811340
Epoch 15: training loss 120727076020.706
Test Loss of 106492007743.851913, Test MSE of 106492009008.508392
Epoch 16: training loss 117455385088.000
Test Loss of 104297279933.423416, Test MSE of 104297277980.260376
Epoch 17: training loss 113522045711.059
Test Loss of 99994424408.610825, Test MSE of 99994424847.015594
Epoch 18: training loss 109434485775.059
Test Loss of 95499837985.880615, Test MSE of 95499836430.361755
Epoch 19: training loss 105048364754.824
Test Loss of 93063040832.562698, Test MSE of 93063040347.582947
Epoch 20: training loss 101924397206.588
Test Loss of 89849654861.949097, Test MSE of 89849655920.416351
Epoch 21: training loss 98522140310.588
Test Loss of 86682896094.948639, Test MSE of 86682897676.252792
Epoch 22: training loss 94721136564.706
Test Loss of 81733448640.977325, Test MSE of 81733449096.798538
Epoch 23: training loss 90870998196.706
Test Loss of 76672776037.523361, Test MSE of 76672775596.879852
Epoch 24: training loss 87073978074.353
Test Loss of 73543012365.267929, Test MSE of 73543013389.855453
Epoch 25: training loss 84051987365.647
Test Loss of 71336479388.135117, Test MSE of 71336479079.496246
Epoch 26: training loss 79991890070.588
Test Loss of 67889126747.335495, Test MSE of 67889126650.623421
Epoch 27: training loss 76598738266.353
Test Loss of 65019727955.398430, Test MSE of 65019726942.709946
Epoch 28: training loss 73482072335.059
Test Loss of 58780059844.175842, Test MSE of 58780058788.127777
Epoch 29: training loss 69393986394.353
Test Loss of 58723167145.284592, Test MSE of 58723167064.634705
Epoch 30: training loss 66560573409.882
Test Loss of 54466917320.558998, Test MSE of 54466916917.084679
Epoch 31: training loss 62895698861.176
Test Loss of 53555356826.950485, Test MSE of 53555356718.537125
Epoch 32: training loss 61044533571.765
Test Loss of 49796192560.214714, Test MSE of 49796192523.627441
Epoch 33: training loss 57915752448.000
Test Loss of 51185139704.418327, Test MSE of 51185139557.553291
Epoch 34: training loss 54429939915.294
Test Loss of 45746928729.558540, Test MSE of 45746929402.800240
Epoch 35: training loss 51932466740.706
Test Loss of 43286046580.686722, Test MSE of 43286046375.740807
Epoch 36: training loss 49696172581.647
Test Loss of 42512737154.428505, Test MSE of 42512737083.248985
Epoch 37: training loss 47760900713.412
Test Loss of 38883513459.146690, Test MSE of 38883513820.043869
Epoch 38: training loss 44796327664.941
Test Loss of 37018678155.905602, Test MSE of 37018677828.145523
Epoch 39: training loss 42867345114.353
Test Loss of 32738490509.208698, Test MSE of 32738490597.211853
Epoch 40: training loss 40390396969.412
Test Loss of 32084085326.422951, Test MSE of 32084084312.203568
Epoch 41: training loss 38124056252.235
Test Loss of 32315173561.514114, Test MSE of 32315172665.277496
Epoch 42: training loss 36967130104.471
Test Loss of 29723611224.136974, Test MSE of 29723611528.230591
Epoch 43: training loss 35103006607.059
Test Loss of 28214113060.605274, Test MSE of 28214113822.465366
Epoch 44: training loss 33633595971.765
Test Loss of 25852384738.857937, Test MSE of 25852384746.261604
Epoch 45: training loss 31889115663.059
Test Loss of 28702141772.172142, Test MSE of 28702141791.526997
Epoch 46: training loss 30006859587.765
Test Loss of 27517654511.178158, Test MSE of 27517654322.321033
Epoch 47: training loss 29158603211.294
Test Loss of 25501987820.571957, Test MSE of 25501987177.762421
Epoch 48: training loss 27565905833.412
Test Loss of 23792569137.873207, Test MSE of 23792569199.745117
Epoch 49: training loss 26350654426.353
Test Loss of 23417050116.264690, Test MSE of 23417049682.296658
Epoch 50: training loss 25294040214.588
Test Loss of 24806810132.612679, Test MSE of 24806810904.843483
Epoch 51: training loss 24398079269.647
Test Loss of 24432666605.045811, Test MSE of 24432666756.200912
Epoch 52: training loss 23723878373.647
Test Loss of 20566883358.800556, Test MSE of 20566883401.975243
Epoch 53: training loss 22650226179.765
Test Loss of 19878514906.920872, Test MSE of 19878515441.940567
Epoch 54: training loss 21775972419.765
Test Loss of 22257690357.693661, Test MSE of 22257690521.173710
Epoch 55: training loss 21355589820.235
Test Loss of 21364435315.502083, Test MSE of 21364435626.716873
Epoch 56: training loss 20252568982.588
Test Loss of 18370718870.211941, Test MSE of 18370719736.880283
Epoch 57: training loss 19677407416.471
Test Loss of 21743848706.250809, Test MSE of 21743848635.134106
Epoch 58: training loss 18846440865.882
Test Loss of 19004844347.587227, Test MSE of 19004844252.682503
Epoch 59: training loss 18647497313.882
Test Loss of 20013051268.086998, Test MSE of 20013051196.241177
Epoch 60: training loss 17792702814.118
Test Loss of 20341751650.680241, Test MSE of 20341751793.410198
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21400666158.265556, 'MSE - std': 1711010012.4771545, 'R2 - mean': 0.8410405208989967, 'R2 - std': 0.008273391296857552} 
 

Saving model.....
Results After CV: {'MSE - mean': 21400666158.265556, 'MSE - std': 1711010012.4771545, 'R2 - mean': 0.8410405208989967, 'R2 - std': 0.008273391296857552}
Train time: 97.99285024379932
Inference time: 0.08018765480082948
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 95 finished with value: 21400666158.265556 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005440 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526021360.941
Test Loss of 418112672976.684692, Test MSE of 418112677210.224365
Epoch 2: training loss 427505891087.059
Test Loss of 418094705599.333801, Test MSE of 418094711447.564209
Epoch 3: training loss 427478784000.000
Test Loss of 418071213649.365723, Test MSE of 418071209299.565369
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496565097.412
Test Loss of 418076107314.809143, Test MSE of 418076113529.257019
Epoch 2: training loss 427483853281.882
Test Loss of 418077215385.611816, Test MSE of 418077220944.604431
Epoch 3: training loss 427483399589.647
Test Loss of 418076778350.086487, Test MSE of 418076787400.078674
Epoch 4: training loss 427483056489.412
Test Loss of 418076229973.096436, Test MSE of 418076236359.972778
Epoch 5: training loss 420963074529.882
Test Loss of 397282503228.757812, Test MSE of 397282506908.733887
Epoch 6: training loss 376418344357.647
Test Loss of 330751624028.321045, Test MSE of 330751626723.526367
Epoch 7: training loss 297521224161.882
Test Loss of 245986563583.881561, Test MSE of 245986567031.772888
Epoch 8: training loss 218864757519.059
Test Loss of 175638895728.514465, Test MSE of 175638895758.621582
Epoch 9: training loss 159494733522.824
Test Loss of 126503829295.552170, Test MSE of 126503830797.213181
Epoch 10: training loss 140670470595.765
Test Loss of 118629870902.539902, Test MSE of 118629874567.630844
Epoch 11: training loss 135883344624.941
Test Loss of 115529903099.973160, Test MSE of 115529902719.590591
Epoch 12: training loss 133772714029.176
Test Loss of 113212181397.644226, Test MSE of 113212181318.315552
Epoch 13: training loss 129652193566.118
Test Loss of 109533229151.696503, Test MSE of 109533230171.143387
Epoch 14: training loss 125145526317.176
Test Loss of 106910125379.804764, Test MSE of 106910125933.624832
Epoch 15: training loss 121708728214.588
Test Loss of 103043561431.731674, Test MSE of 103043565102.362320
Epoch 16: training loss 118914001558.588
Test Loss of 99756178622.208649, Test MSE of 99756178494.204483
Epoch 17: training loss 114799286452.706
Test Loss of 95265058318.567657, Test MSE of 95265057155.743637
Epoch 18: training loss 110591038629.647
Test Loss of 92688235086.760117, Test MSE of 92688234818.719635
Epoch 19: training loss 107798674100.706
Test Loss of 90280493980.750412, Test MSE of 90280493213.645508
Epoch 20: training loss 102963560448.000
Test Loss of 86250242654.393707, Test MSE of 86250243377.805008
Epoch 21: training loss 99308741157.647
Test Loss of 85708729173.214890, Test MSE of 85708729406.387894
Epoch 22: training loss 94603309138.824
Test Loss of 80677566579.120056, Test MSE of 80677566035.222992
Epoch 23: training loss 90727541217.882
Test Loss of 77182705086.030991, Test MSE of 77182705282.500229
Epoch 24: training loss 88112251256.471
Test Loss of 74241325811.623413, Test MSE of 74241325796.870285
Epoch 25: training loss 84825047446.588
Test Loss of 71327515857.632202, Test MSE of 71327514137.790283
Epoch 26: training loss 80760265946.353
Test Loss of 68346753445.633125, Test MSE of 68346752710.637871
Epoch 27: training loss 77251644476.235
Test Loss of 66378563381.237106, Test MSE of 66378562792.402573
Epoch 28: training loss 74058030110.118
Test Loss of 60820511638.591721, Test MSE of 60820510852.420517
Epoch 29: training loss 70992358592.000
Test Loss of 59573508849.728432, Test MSE of 59573509562.512978
Epoch 30: training loss 66800800730.353
Test Loss of 54484963314.735138, Test MSE of 54484962993.640266
Epoch 31: training loss 64135388107.294
Test Loss of 54319204336.366409, Test MSE of 54319203556.184189
Epoch 32: training loss 61636074947.765
Test Loss of 51737113408.843857, Test MSE of 51737114014.667030
Epoch 33: training loss 58556602729.412
Test Loss of 48936799025.210274, Test MSE of 48936799458.814262
Epoch 34: training loss 54821636683.294
Test Loss of 45814597343.489243, Test MSE of 45814598034.834618
Epoch 35: training loss 53218661078.588
Test Loss of 45370805418.311356, Test MSE of 45370804803.243767
Epoch 36: training loss 50524351476.706
Test Loss of 40153868717.686790, Test MSE of 40153868172.954803
Epoch 37: training loss 48055674473.412
Test Loss of 41450875092.711540, Test MSE of 41450874986.013794
Epoch 38: training loss 46284017490.824
Test Loss of 37856782439.750175, Test MSE of 37856782435.373032
Epoch 39: training loss 42924174810.353
Test Loss of 35911860756.489471, Test MSE of 35911860839.871643
Epoch 40: training loss 40920446603.294
Test Loss of 34140509041.165855, Test MSE of 34140509360.312225
Epoch 41: training loss 38822713129.412
Test Loss of 34962655456.081429, Test MSE of 34962655607.723900
Epoch 42: training loss 37048285801.412
Test Loss of 30537604146.927597, Test MSE of 30537604500.589230
Epoch 43: training loss 35261292363.294
Test Loss of 28101930044.402500, Test MSE of 28101930068.102188
Epoch 44: training loss 33430395730.824
Test Loss of 28467852302.449226, Test MSE of 28467852816.443302
Epoch 45: training loss 32330672376.471
Test Loss of 29301145264.588482, Test MSE of 29301145092.732826
Epoch 46: training loss 30392935928.471
Test Loss of 24943400733.549850, Test MSE of 24943400996.577847
Epoch 47: training loss 29349251328.000
Test Loss of 25238005699.834373, Test MSE of 25238006453.989841
Epoch 48: training loss 27898967860.706
Test Loss of 24748008247.368958, Test MSE of 24748008584.197010
Epoch 49: training loss 26489040000.000
Test Loss of 25607382804.548695, Test MSE of 25607382878.770088
Epoch 50: training loss 25180046061.176
Test Loss of 20544517785.848717, Test MSE of 20544517566.393433
Epoch 51: training loss 24138562492.235
Test Loss of 20699759892.904003, Test MSE of 20699759854.388069
Epoch 52: training loss 23674777415.529
Test Loss of 23217598274.975712, Test MSE of 23217598378.768497
Epoch 53: training loss 22637935815.529
Test Loss of 19859466488.716167, Test MSE of 19859466192.027321
Epoch 54: training loss 21619536651.294
Test Loss of 18001395407.855656, Test MSE of 18001395518.135582
Epoch 55: training loss 20484329027.765
Test Loss of 21515015264.880871, Test MSE of 21515015375.478874
Epoch 56: training loss 20301283651.765
Test Loss of 20748634005.881100, Test MSE of 20748633680.862457
Epoch 57: training loss 19184117662.118
Test Loss of 22171167992.479298, Test MSE of 22171167902.485413
Epoch 58: training loss 18530015597.176
Test Loss of 19256336402.002312, Test MSE of 19256336593.201893
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 19256336593.201893, 'MSE - std': 0.0, 'R2 - mean': 0.850048817652109, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005416 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917325251.765
Test Loss of 424555801580.813354, Test MSE of 424555803215.606018
Epoch 2: training loss 427895799567.059
Test Loss of 424539349353.941223, Test MSE of 424539353131.785583
Epoch 3: training loss 427867990377.412
Test Loss of 424517196404.659729, Test MSE of 424517192580.465332
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427888155467.294
Test Loss of 424522878783.659485, Test MSE of 424522875550.961121
Epoch 2: training loss 427876589086.118
Test Loss of 424523766604.687500, Test MSE of 424523775662.197327
Epoch 3: training loss 427875970590.118
Test Loss of 424523579562.548218, Test MSE of 424523582598.660522
Epoch 4: training loss 427875524848.941
Test Loss of 424523100898.094849, Test MSE of 424523105729.338074
Epoch 5: training loss 421380807499.294
Test Loss of 404086876960.155457, Test MSE of 404086879236.590820
Epoch 6: training loss 376331968993.882
Test Loss of 338718532040.690247, Test MSE of 338718532191.181030
Epoch 7: training loss 297838257694.118
Test Loss of 254368973080.457092, Test MSE of 254368970368.300720
Epoch 8: training loss 218739049833.412
Test Loss of 185068261289.778381, Test MSE of 185068262853.952423
Epoch 9: training loss 157659907192.471
Test Loss of 137021941489.491562, Test MSE of 137021942056.119446
Epoch 10: training loss 136586502053.647
Test Loss of 129445418403.027527, Test MSE of 129445421286.397614
Epoch 11: training loss 132777344180.706
Test Loss of 126340134670.863754, Test MSE of 126340135506.945236
Epoch 12: training loss 130206187218.824
Test Loss of 124225732018.661118, Test MSE of 124225731588.906662
Epoch 13: training loss 125942488274.824
Test Loss of 120250907826.838776, Test MSE of 120250905996.632950
Epoch 14: training loss 122721555486.118
Test Loss of 116981586495.600281, Test MSE of 116981583922.085159
Epoch 15: training loss 117726357775.059
Test Loss of 113924751558.972931, Test MSE of 113924750481.778305
Epoch 16: training loss 114920299670.588
Test Loss of 109490813103.759430, Test MSE of 109490813698.527527
Epoch 17: training loss 111437882608.941
Test Loss of 105948882326.710159, Test MSE of 105948881430.020096
Epoch 18: training loss 106686457765.647
Test Loss of 102013547405.590561, Test MSE of 102013548954.808319
Epoch 19: training loss 102257802661.647
Test Loss of 97858565988.137863, Test MSE of 97858565467.288177
Epoch 20: training loss 98135593953.882
Test Loss of 94891656766.415909, Test MSE of 94891657734.539124
Epoch 21: training loss 94396892370.824
Test Loss of 90199168584.838303, Test MSE of 90199169438.317688
Epoch 22: training loss 90407035497.412
Test Loss of 87230773254.158691, Test MSE of 87230774326.677765
Epoch 23: training loss 87672450755.765
Test Loss of 82195668183.790894, Test MSE of 82195668654.917206
Epoch 24: training loss 83160741752.471
Test Loss of 79740928222.660187, Test MSE of 79740930304.587418
Epoch 25: training loss 79263390027.294
Test Loss of 76192160989.475830, Test MSE of 76192162562.109634
Epoch 26: training loss 76430960896.000
Test Loss of 71718521663.422623, Test MSE of 71718521797.072205
Epoch 27: training loss 72061591868.235
Test Loss of 67748156068.507980, Test MSE of 67748156692.995979
Epoch 28: training loss 67501018940.235
Test Loss of 63934162908.469116, Test MSE of 63934162973.997772
Epoch 29: training loss 65608846245.647
Test Loss of 62714324892.039787, Test MSE of 62714323401.234894
Epoch 30: training loss 62227397767.529
Test Loss of 59762619255.798286, Test MSE of 59762618645.059441
Epoch 31: training loss 58676013688.471
Test Loss of 53933583252.222992, Test MSE of 53933581377.268440
Epoch 32: training loss 55621776376.471
Test Loss of 53461022956.398796, Test MSE of 53461023205.325081
Epoch 33: training loss 52562101172.706
Test Loss of 51199290482.883186, Test MSE of 51199290999.251884
Epoch 34: training loss 50402099764.706
Test Loss of 51078815696.625488, Test MSE of 51078815942.229210
Epoch 35: training loss 46426995425.882
Test Loss of 46730350655.481842, Test MSE of 46730351515.163414
Epoch 36: training loss 45036609001.412
Test Loss of 43516203116.961372, Test MSE of 43516204150.149406
Epoch 37: training loss 41827734512.941
Test Loss of 43148182805.377747, Test MSE of 43148182299.090385
Epoch 38: training loss 39911159966.118
Test Loss of 35590716853.503586, Test MSE of 35590716240.850609
Epoch 39: training loss 37461088248.471
Test Loss of 37943971024.684708, Test MSE of 37943970855.802597
Epoch 40: training loss 35616181330.824
Test Loss of 38307776238.649086, Test MSE of 38307775839.320435
Epoch 41: training loss 33562522639.059
Test Loss of 32211513903.019199, Test MSE of 32211513898.399990
Epoch 42: training loss 31688641822.118
Test Loss of 31031604732.091602, Test MSE of 31031605352.587917
Epoch 43: training loss 29967566042.353
Test Loss of 34830495063.228317, Test MSE of 34830495532.552719
Epoch 44: training loss 28731052453.647
Test Loss of 33552956870.558407, Test MSE of 33552956549.249928
Epoch 45: training loss 26710382080.000
Test Loss of 30536952845.975479, Test MSE of 30536953939.536602
Epoch 46: training loss 25662793050.353
Test Loss of 29764639050.674068, Test MSE of 29764638656.849457
Epoch 47: training loss 24407529035.294
Test Loss of 25372399015.054359, Test MSE of 25372399515.116570
Epoch 48: training loss 23265349150.118
Test Loss of 26253433603.493870, Test MSE of 26253433951.714134
Epoch 49: training loss 22222453665.882
Test Loss of 26944590621.786724, Test MSE of 26944590476.537033
Epoch 50: training loss 20717341108.706
Test Loss of 25519352044.635670, Test MSE of 25519352458.411495
Epoch 51: training loss 19941129739.294
Test Loss of 28905411012.426556, Test MSE of 28905411327.893425
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24080873960.54766, 'MSE - std': 4824537367.345766, 'R2 - mean': 0.8218416859468264, 'R2 - std': 0.028207131705282662} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003573 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927356175.059
Test Loss of 447259139601.410156, Test MSE of 447259141138.941528
Epoch 2: training loss 421906337430.588
Test Loss of 447239865544.157288, Test MSE of 447239862079.499817
Epoch 3: training loss 421878569200.941
Test Loss of 447214326075.514221, Test MSE of 447214334491.335999
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898782720.000
Test Loss of 447221785809.632202, Test MSE of 447221787674.178162
Epoch 2: training loss 421887195979.294
Test Loss of 447223056871.483704, Test MSE of 447223051566.526245
Epoch 3: training loss 421886645669.647
Test Loss of 447222736953.323181, Test MSE of 447222734857.790222
Epoch 4: training loss 421886286185.412
Test Loss of 447222466865.565552, Test MSE of 447222459251.091492
Epoch 5: training loss 415007824353.882
Test Loss of 424714182013.601685, Test MSE of 424714184267.299011
Epoch 6: training loss 369461528937.412
Test Loss of 357562548534.776794, Test MSE of 357562546412.360840
Epoch 7: training loss 290792706529.882
Test Loss of 271393361644.280365, Test MSE of 271393363477.747162
Epoch 8: training loss 212402934482.824
Test Loss of 197639909610.030060, Test MSE of 197639911695.434113
Epoch 9: training loss 153797952572.235
Test Loss of 149192372909.035400, Test MSE of 149192371861.086182
Epoch 10: training loss 134816970420.706
Test Loss of 140063948167.076569, Test MSE of 140063949131.673645
Epoch 11: training loss 130814790776.471
Test Loss of 137199370486.347443, Test MSE of 137199371399.919205
Epoch 12: training loss 127995791269.647
Test Loss of 133853168851.527176, Test MSE of 133853168291.266190
Epoch 13: training loss 123634150490.353
Test Loss of 129993895525.736755, Test MSE of 129993896656.972321
Epoch 14: training loss 121332150723.765
Test Loss of 126933425363.053436, Test MSE of 126933422396.473450
Epoch 15: training loss 117482962251.294
Test Loss of 121813710659.923203, Test MSE of 121813708405.037125
Epoch 16: training loss 114437629711.059
Test Loss of 119119265087.067322, Test MSE of 119119264292.973236
Epoch 17: training loss 110659200963.765
Test Loss of 115146893846.858200, Test MSE of 115146893426.894745
Epoch 18: training loss 106159632082.824
Test Loss of 110141199746.812866, Test MSE of 110141199909.465897
Epoch 19: training loss 101878579049.412
Test Loss of 107248971524.915100, Test MSE of 107248970930.558792
Epoch 20: training loss 99271299463.529
Test Loss of 103929311272.742081, Test MSE of 103929310286.151230
Epoch 21: training loss 94894815036.235
Test Loss of 100932719881.534119, Test MSE of 100932719207.478241
Epoch 22: training loss 90625502659.765
Test Loss of 95037385043.675232, Test MSE of 95037385408.143967
Epoch 23: training loss 87038151664.941
Test Loss of 90273081590.821182, Test MSE of 90273081977.485703
Epoch 24: training loss 83778519597.176
Test Loss of 90069661733.425858, Test MSE of 90069662825.542206
Epoch 25: training loss 79719941948.235
Test Loss of 87279367943.283829, Test MSE of 87279368122.999527
Epoch 26: training loss 75804349168.941
Test Loss of 77845534097.972702, Test MSE of 77845533839.054306
Epoch 27: training loss 71968503868.235
Test Loss of 75989321508.656021, Test MSE of 75989321934.977921
Epoch 28: training loss 69433831439.059
Test Loss of 69708097716.496872, Test MSE of 69708097328.199875
Epoch 29: training loss 67057504346.353
Test Loss of 68443977066.178116, Test MSE of 68443977270.226868
Epoch 30: training loss 63437877217.882
Test Loss of 64768974813.890350, Test MSE of 64768975118.584244
Epoch 31: training loss 60122554428.235
Test Loss of 64263492763.625259, Test MSE of 64263492903.584297
Epoch 32: training loss 57148426224.941
Test Loss of 61232627198.460327, Test MSE of 61232626724.709946
Epoch 33: training loss 54442265005.176
Test Loss of 53906624920.368263, Test MSE of 53906625025.446884
Epoch 34: training loss 51154675983.059
Test Loss of 55680032421.218597, Test MSE of 55680032491.185616
Epoch 35: training loss 49108284438.588
Test Loss of 52510137483.044182, Test MSE of 52510139021.049507
Epoch 36: training loss 46339855698.824
Test Loss of 48012682396.572754, Test MSE of 48012682148.621124
Epoch 37: training loss 44830450228.706
Test Loss of 51860584229.603516, Test MSE of 51860584047.782524
Epoch 38: training loss 42278393720.471
Test Loss of 46490479033.056671, Test MSE of 46490479655.425430
Epoch 39: training loss 39867952692.706
Test Loss of 41625297462.599121, Test MSE of 41625298683.338524
Epoch 40: training loss 37427308092.235
Test Loss of 39973347699.416145, Test MSE of 39973347163.651894
Epoch 41: training loss 35509178744.471
Test Loss of 35278711988.733749, Test MSE of 35278712318.496078
Epoch 42: training loss 33501040798.118
Test Loss of 36574042928.262779, Test MSE of 36574043038.368858
Epoch 43: training loss 32304945347.765
Test Loss of 35706857191.069168, Test MSE of 35706858123.158165
Epoch 44: training loss 30594204920.471
Test Loss of 34023592233.511913, Test MSE of 34023592173.813381
Epoch 45: training loss 28873421985.882
Test Loss of 30810138960.832756, Test MSE of 30810138670.763165
Epoch 46: training loss 27784220016.941
Test Loss of 34248837291.969467, Test MSE of 34248837755.358475
Epoch 47: training loss 26806511232.000
Test Loss of 32633195914.866528, Test MSE of 32633195812.186539
Epoch 48: training loss 25705215819.294
Test Loss of 28445999197.801525, Test MSE of 28445999510.323257
Epoch 49: training loss 23766101153.882
Test Loss of 25959952465.484154, Test MSE of 25959952592.759102
Epoch 50: training loss 22853934336.000
Test Loss of 27201695798.480686, Test MSE of 27201696027.457207
Epoch 51: training loss 22175761253.647
Test Loss of 24274702388.585705, Test MSE of 24274702373.470486
Epoch 52: training loss 21412140653.176
Test Loss of 26281721149.646080, Test MSE of 26281721430.267139
Epoch 53: training loss 20530783190.588
Test Loss of 26406888245.000233, Test MSE of 26406888303.798592
Epoch 54: training loss 19507691200.000
Test Loss of 25051793087.274578, Test MSE of 25051793526.741604
Epoch 55: training loss 19160369253.647
Test Loss of 24228861047.146889, Test MSE of 24228861803.111130
Epoch 56: training loss 18363831488.000
Test Loss of 24248844513.976406, Test MSE of 24248844970.258049
Epoch 57: training loss 17607195260.235
Test Loss of 24974191252.874393, Test MSE of 24974191163.482010
Epoch 58: training loss 17140230934.588
Test Loss of 23895696370.972012, Test MSE of 23895695863.467884
Epoch 59: training loss 16450933711.059
Test Loss of 24183544937.882027, Test MSE of 24183545213.229923
Epoch 60: training loss 16271781808.941
Test Loss of 23804333863.972240, Test MSE of 23804333605.316143
Epoch 61: training loss 15794904730.353
Test Loss of 23670005199.085819, Test MSE of 23670005693.847206
Epoch 62: training loss 15044140314.353
Test Loss of 24173344414.112423, Test MSE of 24173344467.949387
Epoch 63: training loss 14812793886.118
Test Loss of 25641766496.051815, Test MSE of 25641766890.682621
Epoch 64: training loss 14660964397.176
Test Loss of 24441917283.190376, Test MSE of 24441917359.999168
Epoch 65: training loss 14099261519.059
Test Loss of 23485717623.620632, Test MSE of 23485717876.765736
Epoch 66: training loss 13545275169.882
Test Loss of 22498423845.188988, Test MSE of 22498424130.825138
Epoch 67: training loss 13270296278.588
Test Loss of 24149334609.602592, Test MSE of 24149334277.471523
Epoch 68: training loss 12829271728.941
Test Loss of 22756553750.976635, Test MSE of 22756553780.157463
Epoch 69: training loss 12711297722.353
Test Loss of 23712541697.658108, Test MSE of 23712541422.727142
Epoch 70: training loss 12016378646.588
Test Loss of 24184531684.463566, Test MSE of 24184532069.425224
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24115426663.506847, 'MSE - std': 3939521331.0554175, 'R2 - mean': 0.8275628459818783, 'R2 - std': 0.024410889584762382} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005523 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110465807.059
Test Loss of 410765269080.136963, Test MSE of 410765267958.641907
Epoch 2: training loss 430088822302.118
Test Loss of 410746460081.340149, Test MSE of 410746467553.409790
Epoch 3: training loss 430060899388.235
Test Loss of 410722741614.289673, Test MSE of 410722740249.605042
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078601336.471
Test Loss of 410728406202.224915, Test MSE of 410728407455.016296
Epoch 2: training loss 430065967104.000
Test Loss of 410728737263.652039, Test MSE of 410728737176.976013
Epoch 3: training loss 430065503051.294
Test Loss of 410727974107.394714, Test MSE of 410727973759.911987
Epoch 4: training loss 430065160914.824
Test Loss of 410727901808.540466, Test MSE of 410727902401.101562
Epoch 5: training loss 423468880112.941
Test Loss of 389329915295.570557, Test MSE of 389329915504.375732
Epoch 6: training loss 378204797530.353
Test Loss of 323283724450.058289, Test MSE of 323283722694.662903
Epoch 7: training loss 299558239292.235
Test Loss of 238472924712.040710, Test MSE of 238472927684.339264
Epoch 8: training loss 221966068073.412
Test Loss of 168701297496.729279, Test MSE of 168701294673.374023
Epoch 9: training loss 162694751111.529
Test Loss of 120195040975.785278, Test MSE of 120195040896.923431
Epoch 10: training loss 141062235346.824
Test Loss of 111688181922.058304, Test MSE of 111688182091.616928
Epoch 11: training loss 137771064711.529
Test Loss of 109099738084.990280, Test MSE of 109099735376.646149
Epoch 12: training loss 133682986315.294
Test Loss of 106938216082.184174, Test MSE of 106938216937.720871
Epoch 13: training loss 130090602556.235
Test Loss of 102953155531.402130, Test MSE of 102953155271.317032
Epoch 14: training loss 126318802642.824
Test Loss of 100520629670.204529, Test MSE of 100520630081.633041
Epoch 15: training loss 123728792756.706
Test Loss of 97036076736.621933, Test MSE of 97036075647.529404
Epoch 16: training loss 118745364239.059
Test Loss of 94519537525.634430, Test MSE of 94519539538.454681
Epoch 17: training loss 115765776112.941
Test Loss of 88956342527.881531, Test MSE of 88956340904.384064
Epoch 18: training loss 111312718908.235
Test Loss of 87255933773.356781, Test MSE of 87255933641.096970
Epoch 19: training loss 107511827425.882
Test Loss of 83635902673.443771, Test MSE of 83635901011.701004
Epoch 20: training loss 103147597251.765
Test Loss of 80651842675.146698, Test MSE of 80651843440.568085
Epoch 21: training loss 98854433159.529
Test Loss of 79536003906.931976, Test MSE of 79536003621.957916
Epoch 22: training loss 95880814290.824
Test Loss of 75477714173.512268, Test MSE of 75477712842.529053
Epoch 23: training loss 91740624609.882
Test Loss of 70829840536.581207, Test MSE of 70829840993.686646
Epoch 24: training loss 87932265697.882
Test Loss of 69738710034.480331, Test MSE of 69738711197.718063
Epoch 25: training loss 85006606938.353
Test Loss of 64983022634.173065, Test MSE of 64983021722.889053
Epoch 26: training loss 80723864365.176
Test Loss of 63602043273.299400, Test MSE of 63602043196.118683
Epoch 27: training loss 76362399864.471
Test Loss of 60486791162.313744, Test MSE of 60486791993.273087
Epoch 28: training loss 73322612329.412
Test Loss of 58822501295.918556, Test MSE of 58822501779.097481
Epoch 29: training loss 70506323350.588
Test Loss of 53481846464.621933, Test MSE of 53481846539.560249
Epoch 30: training loss 66402967055.059
Test Loss of 55619434220.216568, Test MSE of 55619434873.578911
Epoch 31: training loss 62882461093.647
Test Loss of 49343130326.893105, Test MSE of 49343130238.054329
Epoch 32: training loss 60601380653.176
Test Loss of 50014361314.739471, Test MSE of 50014360714.041122
Epoch 33: training loss 57537360203.294
Test Loss of 45949623366.604347, Test MSE of 45949622704.202904
Epoch 34: training loss 54804308781.176
Test Loss of 42259567014.678391, Test MSE of 42259567434.463196
Epoch 35: training loss 52294548773.647
Test Loss of 41159778053.804718, Test MSE of 41159777570.028358
Epoch 36: training loss 49748982264.471
Test Loss of 38091541957.478943, Test MSE of 38091541497.655602
Epoch 37: training loss 46401526716.235
Test Loss of 39356337476.590469, Test MSE of 39356336727.355927
Epoch 38: training loss 44184109982.118
Test Loss of 34573196201.284592, Test MSE of 34573195468.300674
Epoch 39: training loss 42283421123.765
Test Loss of 36124320706.872742, Test MSE of 36124321429.650917
Epoch 40: training loss 39714123760.941
Test Loss of 31121924660.834797, Test MSE of 31121924706.227261
Epoch 41: training loss 38176318561.882
Test Loss of 32541816362.883850, Test MSE of 32541816387.292458
Epoch 42: training loss 36005589760.000
Test Loss of 29493366602.039795, Test MSE of 29493366365.178387
Epoch 43: training loss 34046676178.824
Test Loss of 27273200788.316521, Test MSE of 27273200969.843224
Epoch 44: training loss 32794392478.118
Test Loss of 28446842198.596947, Test MSE of 28446842196.361759
Epoch 45: training loss 30977985280.000
Test Loss of 25114797330.361870, Test MSE of 25114797225.716625
Epoch 46: training loss 29451638791.529
Test Loss of 26825551661.608513, Test MSE of 26825551984.780403
Epoch 47: training loss 28320137246.118
Test Loss of 26008835404.172142, Test MSE of 26008835122.095154
Epoch 48: training loss 27182997376.000
Test Loss of 23574934935.041183, Test MSE of 23574934715.793285
Epoch 49: training loss 25977645914.353
Test Loss of 21349559318.271172, Test MSE of 21349559086.925198
Epoch 50: training loss 24622453534.118
Test Loss of 23021162998.285980, Test MSE of 23021163393.824635
Epoch 51: training loss 23499017212.235
Test Loss of 20705847693.090237, Test MSE of 20705847425.959545
Epoch 52: training loss 22728666507.294
Test Loss of 21294922273.880611, Test MSE of 21294922320.534943
Epoch 53: training loss 21573354932.706
Test Loss of 20676262108.816288, Test MSE of 20676262101.607784
Epoch 54: training loss 20932157824.000
Test Loss of 18179429801.047665, Test MSE of 18179429718.848686
Epoch 55: training loss 20492420894.118
Test Loss of 19709702729.684406, Test MSE of 19709702352.584156
Epoch 56: training loss 19766474942.118
Test Loss of 20625921407.348450, Test MSE of 20625921346.278721
Epoch 57: training loss 18922795482.353
Test Loss of 18425982682.210087, Test MSE of 18425982762.163464
Epoch 58: training loss 18168428250.353
Test Loss of 17912744850.539566, Test MSE of 17912744658.526855
Epoch 59: training loss 17847841765.647
Test Loss of 19089209733.508560, Test MSE of 19089209838.690956
Epoch 60: training loss 17100904387.765
Test Loss of 17120946099.235538, Test MSE of 17120945790.999884
Epoch 61: training loss 16822574155.294
Test Loss of 19366242981.612217, Test MSE of 19366243044.412106
Epoch 62: training loss 16248565739.294
Test Loss of 18472999789.578899, Test MSE of 18472999748.768291
Epoch 63: training loss 15808623036.235
Test Loss of 19708950985.743637, Test MSE of 19708950626.772800
Epoch 64: training loss 15320135009.882
Test Loss of 18053997040.599724, Test MSE of 18053996568.999298
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22600069139.87996, 'MSE - std': 4304508858.350547, 'R2 - mean': 0.8334198738063162, 'R2 - std': 0.023448517491481465} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424042592617.412
Test Loss of 431612422616.906982, Test MSE of 431612422874.585205
Epoch 2: training loss 424022308502.588
Test Loss of 431591841386.380371, Test MSE of 431591854586.085022
Epoch 3: training loss 423994530273.882
Test Loss of 431563630003.472473, Test MSE of 431563632044.037842
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009093481.412
Test Loss of 431566183435.846375, Test MSE of 431566183350.738586
Epoch 2: training loss 423997604924.235
Test Loss of 431568312461.208679, Test MSE of 431568320810.374451
Epoch 3: training loss 423997041121.882
Test Loss of 431569480554.735779, Test MSE of 431569480005.598938
Epoch 4: training loss 423996691998.118
Test Loss of 431570152003.050415, Test MSE of 431570154691.117676
Epoch 5: training loss 417263047017.412
Test Loss of 409628582051.006042, Test MSE of 409628574745.407715
Epoch 6: training loss 372344550339.765
Test Loss of 342008797757.364197, Test MSE of 342008791550.958801
Epoch 7: training loss 293554182806.588
Test Loss of 254864085915.068939, Test MSE of 254864084493.231598
Epoch 8: training loss 216118149180.235
Test Loss of 183763644804.560852, Test MSE of 183763647528.976929
Epoch 9: training loss 157278411294.118
Test Loss of 132274399359.940765, Test MSE of 132274397684.771744
Epoch 10: training loss 137381422411.294
Test Loss of 123920693154.176773, Test MSE of 123920694298.483810
Epoch 11: training loss 133669028472.471
Test Loss of 120232209076.775574, Test MSE of 120232211881.919662
Epoch 12: training loss 130765008655.059
Test Loss of 117922436745.654785, Test MSE of 117922435779.652130
Epoch 13: training loss 127686327868.235
Test Loss of 114704070569.758438, Test MSE of 114704071659.631302
Epoch 14: training loss 124457665144.471
Test Loss of 110495616718.837570, Test MSE of 110495616806.805191
Epoch 15: training loss 119404215265.882
Test Loss of 107043295927.618698, Test MSE of 107043295756.982330
Epoch 16: training loss 116259486418.824
Test Loss of 104984431588.042572, Test MSE of 104984431316.822311
Epoch 17: training loss 113863681084.235
Test Loss of 100040031467.505783, Test MSE of 100040031243.049942
Epoch 18: training loss 107872903710.118
Test Loss of 94835968215.130035, Test MSE of 94835967244.615631
Epoch 19: training loss 103901031725.176
Test Loss of 94272204896.666351, Test MSE of 94272205429.035202
Epoch 20: training loss 100324601042.824
Test Loss of 87666641103.548355, Test MSE of 87666641096.555450
Epoch 21: training loss 97817426944.000
Test Loss of 83782666292.124023, Test MSE of 83782665959.219940
Epoch 22: training loss 92913166426.353
Test Loss of 83275116447.333649, Test MSE of 83275116614.737137
Epoch 23: training loss 88481353005.176
Test Loss of 78513923812.634888, Test MSE of 78513923904.925476
Epoch 24: training loss 85455414151.529
Test Loss of 75676818487.440994, Test MSE of 75676817489.743469
Epoch 25: training loss 82353000975.059
Test Loss of 70030282520.758911, Test MSE of 70030280895.937271
Epoch 26: training loss 78313430949.647
Test Loss of 69972230198.967148, Test MSE of 69972230130.451828
Epoch 27: training loss 74384217592.471
Test Loss of 65623107649.391945, Test MSE of 65623106221.085899
Epoch 28: training loss 72530083403.294
Test Loss of 60279078773.634430, Test MSE of 60279079855.143120
Epoch 29: training loss 68194547343.059
Test Loss of 57946592439.381767, Test MSE of 57946592830.216873
Epoch 30: training loss 64870285869.176
Test Loss of 56372700523.446556, Test MSE of 56372700165.781349
Epoch 31: training loss 61300518475.294
Test Loss of 52829076152.566406, Test MSE of 52829075972.570312
Epoch 32: training loss 58679161246.118
Test Loss of 51061972842.261917, Test MSE of 51061973323.519264
Epoch 33: training loss 56167664640.000
Test Loss of 48855301061.715874, Test MSE of 48855301105.899788
Epoch 34: training loss 53077555937.882
Test Loss of 43415733895.759369, Test MSE of 43415732737.785904
Epoch 35: training loss 50445954319.059
Test Loss of 44478815782.145302, Test MSE of 44478815546.896637
Epoch 36: training loss 47274377456.941
Test Loss of 39516432059.883385, Test MSE of 39516431728.896736
Epoch 37: training loss 45518916837.647
Test Loss of 40455561379.006012, Test MSE of 40455561001.750206
Epoch 38: training loss 43464662957.176
Test Loss of 38331660939.076355, Test MSE of 38331661872.440277
Epoch 39: training loss 40905653074.824
Test Loss of 33221555105.702915, Test MSE of 33221554790.867378
Epoch 40: training loss 39481869801.412
Test Loss of 31031719701.915779, Test MSE of 31031719632.004063
Epoch 41: training loss 37842408090.353
Test Loss of 29682204414.223045, Test MSE of 29682204138.180611
Epoch 42: training loss 35271108950.588
Test Loss of 31237323948.956963, Test MSE of 31237324225.791904
Epoch 43: training loss 33536649869.176
Test Loss of 27471330002.154556, Test MSE of 27471329799.729626
Epoch 44: training loss 31584773669.647
Test Loss of 26188419071.052292, Test MSE of 26188419109.393314
Epoch 45: training loss 30288994386.824
Test Loss of 28820542938.328552, Test MSE of 28820542125.819187
Epoch 46: training loss 29247470704.941
Test Loss of 27558418957.031006, Test MSE of 27558419289.187714
Epoch 47: training loss 27879173586.824
Test Loss of 25520489512.277649, Test MSE of 25520489000.857540
Epoch 48: training loss 26544326452.706
Test Loss of 26177362748.298012, Test MSE of 26177362937.895695
Epoch 49: training loss 25629674089.412
Test Loss of 24188988723.531700, Test MSE of 24188989179.029694
Epoch 50: training loss 24487250232.471
Test Loss of 24504075218.036095, Test MSE of 24504075470.717243
Epoch 51: training loss 23853711254.588
Test Loss of 20954448162.946785, Test MSE of 20954448150.102890
Epoch 52: training loss 22537649005.176
Test Loss of 22395060125.438225, Test MSE of 22395060657.976883
Epoch 53: training loss 21469588382.118
Test Loss of 20851756667.439148, Test MSE of 20851756461.556919
Epoch 54: training loss 21021650631.529
Test Loss of 22852894998.626560, Test MSE of 22852893987.743992
Epoch 55: training loss 19895824568.471
Test Loss of 22270031049.388245, Test MSE of 22270030710.882858
Epoch 56: training loss 19564975277.176
Test Loss of 20434374227.635353, Test MSE of 20434374225.379063
Epoch 57: training loss 18967019508.706
Test Loss of 19626508020.745953, Test MSE of 19626508254.497280
Epoch 58: training loss 18516315083.294
Test Loss of 22466640017.947247, Test MSE of 22466640278.395470
Epoch 59: training loss 17949215826.824
Test Loss of 20576961729.806572, Test MSE of 20576961512.331654
Epoch 60: training loss 17328547459.765
Test Loss of 19344557262.126793, Test MSE of 19344557066.188065
Epoch 61: training loss 16637209935.059
Test Loss of 19843189390.867191, Test MSE of 19843189398.972015
Epoch 62: training loss 16119798264.471
Test Loss of 19539850362.728367, Test MSE of 19539850143.190327
Epoch 63: training loss 15637810955.294
Test Loss of 18849336577.303101, Test MSE of 18849336303.400654
Epoch 64: training loss 15205797302.588
Test Loss of 20545011683.568718, Test MSE of 20545011789.558205
Epoch 65: training loss 14839271043.765
Test Loss of 20526970916.012959, Test MSE of 20526971204.706322
Epoch 66: training loss 14801403256.471
Test Loss of 20641240498.524757, Test MSE of 20641240832.777946
Epoch 67: training loss 14366658511.059
Test Loss of 19064945568.755207, Test MSE of 19064945709.966366
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21893044453.89724, 'MSE - std': 4101532986.0493026, 'R2 - mean': 0.8382603545668925, 'R2 - std': 0.02309951068773288} 
 

Saving model.....
Results After CV: {'MSE - mean': 21893044453.89724, 'MSE - std': 4101532986.0493026, 'R2 - mean': 0.8382603545668925, 'R2 - std': 0.02309951068773288}
Train time: 97.91114939079998
Inference time: 0.07375148579885718
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 96 finished with value: 21893044453.89724 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005599 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427525610435.765
Test Loss of 418112496546.435364, Test MSE of 418112497155.601868
Epoch 2: training loss 427504955512.471
Test Loss of 418094425521.239868, Test MSE of 418094432012.017090
Epoch 3: training loss 427477418224.941
Test Loss of 418070629616.425659, Test MSE of 418070625875.989502
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427496267896.471
Test Loss of 418076976981.925537, Test MSE of 418076983319.892639
Epoch 2: training loss 427483238761.412
Test Loss of 418077428040.305359, Test MSE of 418077422233.753418
Epoch 3: training loss 427482756879.059
Test Loss of 418076586691.775146, Test MSE of 418076589431.539795
Epoch 4: training loss 427482429801.412
Test Loss of 418075934447.122803, Test MSE of 418075930010.509888
Epoch 5: training loss 427482201389.176
Test Loss of 418074975647.711304, Test MSE of 418074976428.014343
Epoch 6: training loss 419703356476.235
Test Loss of 393074744686.441833, Test MSE of 393074745475.733276
Epoch 7: training loss 367144372103.529
Test Loss of 316633752546.864685, Test MSE of 316633754214.054504
Epoch 8: training loss 280277893481.412
Test Loss of 226423072791.213501, Test MSE of 226423073841.515747
Epoch 9: training loss 201598625370.353
Test Loss of 160938297323.628967, Test MSE of 160938297110.851074
Epoch 10: training loss 157805787256.471
Test Loss of 130791415116.332184, Test MSE of 130791414759.271332
Epoch 11: training loss 140516990750.118
Test Loss of 118192060091.484619, Test MSE of 118192060188.149460
Epoch 12: training loss 134190140310.588
Test Loss of 114007899896.597733, Test MSE of 114007900365.791489
Epoch 13: training loss 130902219956.706
Test Loss of 111587612152.301636, Test MSE of 111587612628.713730
Epoch 14: training loss 128080072613.647
Test Loss of 108661888388.707840, Test MSE of 108661887302.990692
Epoch 15: training loss 124094977837.176
Test Loss of 105724925600.954895, Test MSE of 105724925927.497345
Epoch 16: training loss 121268016338.824
Test Loss of 102275584146.860977, Test MSE of 102275584685.489395
Epoch 17: training loss 116528803568.941
Test Loss of 98070402432.917877, Test MSE of 98070403175.089584
Epoch 18: training loss 113278705679.059
Test Loss of 95935060839.217209, Test MSE of 95935062296.743469
Epoch 19: training loss 109901123855.059
Test Loss of 92554460263.039551, Test MSE of 92554460212.704849
Epoch 20: training loss 105670129332.706
Test Loss of 90052242365.438812, Test MSE of 90052243506.885986
Epoch 21: training loss 101268940649.412
Test Loss of 84630845401.626648, Test MSE of 84630845044.647293
Epoch 22: training loss 97428876769.882
Test Loss of 82968970190.019897, Test MSE of 82968971338.145477
Epoch 23: training loss 92720745125.647
Test Loss of 78714636441.967148, Test MSE of 78714636870.813629
Epoch 24: training loss 89997984481.882
Test Loss of 75152638713.782089, Test MSE of 75152638462.487991
Epoch 25: training loss 85462408003.765
Test Loss of 72925748325.144577, Test MSE of 72925746971.056259
Epoch 26: training loss 82838953984.000
Test Loss of 70199023361.598892, Test MSE of 70199022055.799042
Epoch 27: training loss 78389500559.059
Test Loss of 66930855916.102707, Test MSE of 66930856029.843658
Epoch 28: training loss 75479216589.176
Test Loss of 65223335791.981491, Test MSE of 65223336579.879166
Epoch 29: training loss 71571463107.765
Test Loss of 59181039206.684250, Test MSE of 59181039249.848320
Epoch 30: training loss 68069254505.412
Test Loss of 57931121502.216057, Test MSE of 57931122776.777863
Epoch 31: training loss 65644702354.824
Test Loss of 56115521846.303032, Test MSE of 56115522008.212425
Epoch 32: training loss 62417776052.706
Test Loss of 52773919558.055054, Test MSE of 52773920776.410660
Epoch 33: training loss 59954532758.588
Test Loss of 48005884738.265091, Test MSE of 48005884272.358604
Epoch 34: training loss 56768646144.000
Test Loss of 45677858205.816330, Test MSE of 45677857984.736984
Epoch 35: training loss 53355868645.647
Test Loss of 46493422557.179733, Test MSE of 46493422628.257812
Epoch 36: training loss 51267736267.294
Test Loss of 43814224457.548927, Test MSE of 43814224119.611298
Epoch 37: training loss 48306803527.529
Test Loss of 41545485617.802452, Test MSE of 41545486111.169456
Epoch 38: training loss 45787382302.118
Test Loss of 37229888150.295631, Test MSE of 37229888625.744911
Epoch 39: training loss 43572771504.941
Test Loss of 36922495685.670135, Test MSE of 36922496108.696625
Epoch 40: training loss 41288199529.412
Test Loss of 34491022004.141571, Test MSE of 34491022857.164757
Epoch 41: training loss 39547161148.235
Test Loss of 32520447377.972706, Test MSE of 32520446914.928394
Epoch 42: training loss 37488271439.059
Test Loss of 30198705484.332176, Test MSE of 30198705379.161324
Epoch 43: training loss 35356895578.353
Test Loss of 30910587097.922737, Test MSE of 30910587541.465801
Epoch 44: training loss 33704336444.235
Test Loss of 31348937637.277817, Test MSE of 31348936875.810787
Epoch 45: training loss 32265519529.412
Test Loss of 30662017644.606060, Test MSE of 30662017650.215580
Epoch 46: training loss 30490362119.529
Test Loss of 25398700853.000233, Test MSE of 25398701073.462742
Epoch 47: training loss 29118723392.000
Test Loss of 25500250671.019199, Test MSE of 25500250920.418682
Epoch 48: training loss 27336759476.706
Test Loss of 25305111018.799908, Test MSE of 25305110901.953197
Epoch 49: training loss 26503907132.235
Test Loss of 25503144650.407589, Test MSE of 25503144619.949051
Epoch 50: training loss 25092247160.471
Test Loss of 26477373430.051353, Test MSE of 26477373819.057137
Epoch 51: training loss 24461163154.824
Test Loss of 23081578883.523479, Test MSE of 23081578696.147541
Epoch 52: training loss 23404178529.882
Test Loss of 22269616777.978256, Test MSE of 22269616607.402000
Epoch 53: training loss 22168720896.000
Test Loss of 21972608111.093224, Test MSE of 21972607965.680901
Epoch 54: training loss 21800725549.176
Test Loss of 22708997433.856117, Test MSE of 22708997484.720989
Epoch 55: training loss 20397814226.824
Test Loss of 19457546136.249828, Test MSE of 19457546140.651340
Epoch 56: training loss 20023751653.647
Test Loss of 21752642573.501736, Test MSE of 21752642709.716949
Epoch 57: training loss 18947246825.412
Test Loss of 22275001531.366180, Test MSE of 22275001513.889370
Epoch 58: training loss 18463963960.471
Test Loss of 19885808946.276196, Test MSE of 19885809181.065060
Epoch 59: training loss 17707865773.176
Test Loss of 18632058880.947491, Test MSE of 18632059113.578350
Epoch 60: training loss 16941543292.235
Test Loss of 19835036872.631042, Test MSE of 19835037031.391762
Epoch 61: training loss 16497207036.235
Test Loss of 20469128417.739532, Test MSE of 20469128572.084606
Epoch 62: training loss 16203551081.412
Test Loss of 19721918834.468655, Test MSE of 19721918900.197651
Epoch 63: training loss 15600653918.118
Test Loss of 21188094042.011566, Test MSE of 21188094281.177067
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21188094281.177067, 'MSE - std': 0.0, 'R2 - mean': 0.8350060109417315, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005329 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427918352624.941
Test Loss of 424556046586.611145, Test MSE of 424556047805.961243
Epoch 2: training loss 427898269214.118
Test Loss of 424540509006.108704, Test MSE of 424540506311.853394
Epoch 3: training loss 427871025754.353
Test Loss of 424518510156.628296, Test MSE of 424518503971.454529
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887797669.647
Test Loss of 424527045845.422180, Test MSE of 424527048306.325867
Epoch 2: training loss 427878107497.412
Test Loss of 424527458913.236206, Test MSE of 424527454085.553406
Epoch 3: training loss 427877570680.471
Test Loss of 424526811102.837830, Test MSE of 424526816612.513062
Epoch 4: training loss 427877193848.471
Test Loss of 424526324312.708740, Test MSE of 424526318214.543823
Epoch 5: training loss 427876945920.000
Test Loss of 424525439690.407593, Test MSE of 424525438482.451904
Epoch 6: training loss 419402927405.176
Test Loss of 398501712581.670166, Test MSE of 398501721267.304688
Epoch 7: training loss 365184709571.765
Test Loss of 321535074377.904236, Test MSE of 321535071921.429871
Epoch 8: training loss 276966432888.471
Test Loss of 232405012215.650238, Test MSE of 232405015840.113281
Epoch 9: training loss 198870202428.235
Test Loss of 169567320440.627350, Test MSE of 169567320627.385956
Epoch 10: training loss 155178740736.000
Test Loss of 140930747801.078888, Test MSE of 140930746380.027130
Epoch 11: training loss 138169690834.824
Test Loss of 129822290627.775162, Test MSE of 129822292025.705338
Epoch 12: training loss 132428299504.941
Test Loss of 126433546417.180664, Test MSE of 126433546605.276138
Epoch 13: training loss 129441926746.353
Test Loss of 123993417758.793427, Test MSE of 123993415913.302460
Epoch 14: training loss 126192534106.353
Test Loss of 120676813520.566269, Test MSE of 120676814748.518692
Epoch 15: training loss 123938312824.471
Test Loss of 118405539127.961136, Test MSE of 118405539276.463654
Epoch 16: training loss 119313898044.235
Test Loss of 114018340707.664124, Test MSE of 114018339467.411316
Epoch 17: training loss 116035186718.118
Test Loss of 110052160663.835297, Test MSE of 110052161288.990707
Epoch 18: training loss 111438049972.706
Test Loss of 107715590171.003464, Test MSE of 107715592103.111374
Epoch 19: training loss 107777996800.000
Test Loss of 102691616078.700897, Test MSE of 102691616885.776077
Epoch 20: training loss 104044041788.235
Test Loss of 99718926226.091141, Test MSE of 99718926880.658051
Epoch 21: training loss 99464504741.647
Test Loss of 96060609687.835297, Test MSE of 96060607966.613373
Epoch 22: training loss 96000919431.529
Test Loss of 92712847209.822815, Test MSE of 92712845998.615784
Epoch 23: training loss 92088118302.118
Test Loss of 88664085899.340271, Test MSE of 88664085729.960922
Epoch 24: training loss 89164736090.353
Test Loss of 85182632838.010635, Test MSE of 85182635005.195084
Epoch 25: training loss 84393259535.059
Test Loss of 79864419510.154984, Test MSE of 79864419668.903198
Epoch 26: training loss 79840829455.059
Test Loss of 80471092742.040253, Test MSE of 80471094185.294998
Epoch 27: training loss 77220555264.000
Test Loss of 74053925310.741608, Test MSE of 74053926204.766037
Epoch 28: training loss 73238339056.941
Test Loss of 72770690828.495026, Test MSE of 72770689485.061081
Epoch 29: training loss 69593996724.706
Test Loss of 66046922083.071938, Test MSE of 66046922438.913010
Epoch 30: training loss 66889646742.588
Test Loss of 62073279830.280823, Test MSE of 62073279721.862267
Epoch 31: training loss 63232505871.059
Test Loss of 62012750049.976402, Test MSE of 62012750231.318283
Epoch 32: training loss 59532580848.941
Test Loss of 60180047034.892433, Test MSE of 60180045033.921608
Epoch 33: training loss 56932496941.176
Test Loss of 53788168813.079803, Test MSE of 53788168456.609482
Epoch 34: training loss 54313054388.706
Test Loss of 53223356172.968773, Test MSE of 53223354622.320526
Epoch 35: training loss 50988839664.941
Test Loss of 51234167865.560028, Test MSE of 51234166698.301140
Epoch 36: training loss 49221308807.529
Test Loss of 46441382504.342354, Test MSE of 46441382234.630562
Epoch 37: training loss 45990990110.118
Test Loss of 44490419155.704834, Test MSE of 44490418243.728699
Epoch 38: training loss 43300235851.294
Test Loss of 41778949050.596344, Test MSE of 41778949329.061447
Epoch 39: training loss 40880863872.000
Test Loss of 41448051536.951195, Test MSE of 41448052617.303566
Epoch 40: training loss 38894056192.000
Test Loss of 41056790226.224380, Test MSE of 41056789207.518517
Epoch 41: training loss 36646763279.059
Test Loss of 36566066798.264168, Test MSE of 36566067234.672791
Epoch 42: training loss 34879705381.647
Test Loss of 34507971017.637749, Test MSE of 34507971744.927238
Epoch 43: training loss 32660364702.118
Test Loss of 36016781727.474442, Test MSE of 36016782197.329521
Epoch 44: training loss 31536349522.824
Test Loss of 36888555097.893127, Test MSE of 36888554117.088814
Epoch 45: training loss 29581532897.882
Test Loss of 31807400762.211426, Test MSE of 31807401525.901066
Epoch 46: training loss 28013267230.118
Test Loss of 31091284675.775158, Test MSE of 31091285227.158333
Epoch 47: training loss 26743205338.353
Test Loss of 30189920120.982651, Test MSE of 30189919607.075733
Epoch 48: training loss 25189464493.176
Test Loss of 30986638352.817951, Test MSE of 30986638644.116890
Epoch 49: training loss 24280624075.294
Test Loss of 28493155725.709000, Test MSE of 28493155215.155567
Epoch 50: training loss 22571846226.824
Test Loss of 25447600809.245430, Test MSE of 25447601670.207073
Epoch 51: training loss 21751371561.412
Test Loss of 28555079488.606987, Test MSE of 28555079619.625332
Epoch 52: training loss 20964252352.000
Test Loss of 29631580230.114273, Test MSE of 29631581247.641964
Epoch 53: training loss 20053288463.059
Test Loss of 27008873156.248901, Test MSE of 27008873190.766228
Epoch 54: training loss 19045867681.882
Test Loss of 27789740432.788342, Test MSE of 27789740669.776001
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24488917475.476532, 'MSE - std': 3300823194.299467, 'R2 - mean': 0.8183028564784147, 'R2 - std': 0.016703154463316883} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003568 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927667350.588
Test Loss of 447259797887.259766, Test MSE of 447259795848.704590
Epoch 2: training loss 421907156269.176
Test Loss of 447241211920.107361, Test MSE of 447241215176.778015
Epoch 3: training loss 421879646810.353
Test Loss of 447216125382.084656, Test MSE of 447216121752.323853
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421898893793.882
Test Loss of 447224827859.467957, Test MSE of 447224833008.775757
Epoch 2: training loss 421888282864.941
Test Loss of 447225297147.558655, Test MSE of 447225285435.894226
Epoch 3: training loss 421887914345.412
Test Loss of 447225140925.379578, Test MSE of 447225141193.677429
Epoch 4: training loss 421887599435.294
Test Loss of 447224835287.790894, Test MSE of 447224831177.767517
Epoch 5: training loss 421887377528.471
Test Loss of 447224448590.523254, Test MSE of 447224452008.809814
Epoch 6: training loss 413918248960.000
Test Loss of 421094401373.860718, Test MSE of 421094407685.687988
Epoch 7: training loss 360741239506.824
Test Loss of 343857497686.103149, Test MSE of 343857496616.084778
Epoch 8: training loss 274769819286.588
Test Loss of 250902778118.454773, Test MSE of 250902786866.040924
Epoch 9: training loss 195308667482.353
Test Loss of 184845662584.627350, Test MSE of 184845661687.329803
Epoch 10: training loss 151936953856.000
Test Loss of 153553978242.220673, Test MSE of 153553977269.735260
Epoch 11: training loss 135629957511.529
Test Loss of 140144817881.804291, Test MSE of 140144817725.841827
Epoch 12: training loss 130113167510.588
Test Loss of 136173253342.304886, Test MSE of 136173254995.222534
Epoch 13: training loss 127549627994.353
Test Loss of 132983783655.187607, Test MSE of 132983782248.983871
Epoch 14: training loss 124712465016.471
Test Loss of 129860564117.940323, Test MSE of 129860563710.862686
Epoch 15: training loss 121143945456.941
Test Loss of 126444566923.577148, Test MSE of 126444564815.547012
Epoch 16: training loss 117438658590.118
Test Loss of 122972536001.998611, Test MSE of 122972536271.950150
Epoch 17: training loss 114306390377.412
Test Loss of 119017254221.279663, Test MSE of 119017253352.588928
Epoch 18: training loss 110431470351.059
Test Loss of 115043315650.413132, Test MSE of 115043314730.215988
Epoch 19: training loss 105441977283.765
Test Loss of 111132837124.796661, Test MSE of 111132836805.552551
Epoch 20: training loss 101674057366.588
Test Loss of 107407104969.756195, Test MSE of 107407102466.026840
Epoch 21: training loss 98156694648.471
Test Loss of 104897535975.365250, Test MSE of 104897538811.767853
Epoch 22: training loss 93418552500.706
Test Loss of 99950193273.397171, Test MSE of 99950194618.066330
Epoch 23: training loss 89158359808.000
Test Loss of 98220645242.167007, Test MSE of 98220647122.868286
Epoch 24: training loss 86589422411.294
Test Loss of 92234050211.086746, Test MSE of 92234052023.320953
Epoch 25: training loss 83110745645.176
Test Loss of 89794886215.180206, Test MSE of 89794888693.443939
Epoch 26: training loss 80289759834.353
Test Loss of 86422011115.688187, Test MSE of 86422010389.197830
Epoch 27: training loss 75566876235.294
Test Loss of 80663309154.479767, Test MSE of 80663307849.513046
Epoch 28: training loss 71720696696.471
Test Loss of 78827724462.693497, Test MSE of 78827726758.499908
Epoch 29: training loss 69152387162.353
Test Loss of 71657285549.094604, Test MSE of 71657284437.872955
Epoch 30: training loss 65844198144.000
Test Loss of 70105977287.742767, Test MSE of 70105978280.198471
Epoch 31: training loss 62937029496.471
Test Loss of 67285481681.869072, Test MSE of 67285482445.250755
Epoch 32: training loss 59508369694.118
Test Loss of 66360765966.567665, Test MSE of 66360766462.705482
Epoch 33: training loss 56413634695.529
Test Loss of 64146280907.532730, Test MSE of 64146280715.508484
Epoch 34: training loss 54055455525.647
Test Loss of 60352570882.724037, Test MSE of 60352570701.484108
Epoch 35: training loss 50946854573.176
Test Loss of 55877897478.217903, Test MSE of 55877896897.762230
Epoch 36: training loss 48646374174.118
Test Loss of 53991115955.786263, Test MSE of 53991116726.517059
Epoch 37: training loss 46284843926.588
Test Loss of 52957441383.809395, Test MSE of 52957442497.198402
Epoch 38: training loss 43584233005.176
Test Loss of 48878541899.562340, Test MSE of 48878541718.755264
Epoch 39: training loss 41419101312.000
Test Loss of 47676293503.733521, Test MSE of 47676293525.811096
Epoch 40: training loss 39322652175.059
Test Loss of 45084525204.400650, Test MSE of 45084525706.594337
Epoch 41: training loss 37235364525.176
Test Loss of 41470035494.728661, Test MSE of 41470035648.080887
Epoch 42: training loss 35783469891.765
Test Loss of 41587950474.748093, Test MSE of 41587950630.183884
Epoch 43: training loss 33468983521.882
Test Loss of 36490057834.592644, Test MSE of 36490057774.664116
Epoch 44: training loss 32360811346.824
Test Loss of 34460024115.697433, Test MSE of 34460024560.692215
Epoch 45: training loss 30398517330.824
Test Loss of 41137616788.459869, Test MSE of 41137617283.196060
Epoch 46: training loss 28980138992.941
Test Loss of 36407112529.898682, Test MSE of 36407112244.136772
Epoch 47: training loss 27323874213.647
Test Loss of 35036572118.428871, Test MSE of 35036571880.330353
Epoch 48: training loss 25556719744.000
Test Loss of 33416673146.877632, Test MSE of 33416673280.801281
Epoch 49: training loss 25017814377.412
Test Loss of 29787134860.169327, Test MSE of 29787135202.535591
Epoch 50: training loss 23717976071.529
Test Loss of 29329186040.479298, Test MSE of 29329186084.183113
Epoch 51: training loss 22796710313.412
Test Loss of 30551796446.778625, Test MSE of 30551796906.474842
Epoch 52: training loss 21739730488.471
Test Loss of 29601895860.082352, Test MSE of 29601895698.616589
Epoch 53: training loss 21208080508.235
Test Loss of 26530104088.101780, Test MSE of 26530104313.190262
Epoch 54: training loss 20304921720.471
Test Loss of 25569668872.231319, Test MSE of 25569669026.430340
Epoch 55: training loss 19723786736.941
Test Loss of 24206353454.190147, Test MSE of 24206353380.780865
Epoch 56: training loss 18950000282.353
Test Loss of 27610437967.411518, Test MSE of 27610437848.177814
Epoch 57: training loss 18124203900.235
Test Loss of 23610311237.758965, Test MSE of 23610311609.099167
Epoch 58: training loss 17371825656.471
Test Loss of 25552679109.077953, Test MSE of 25552679106.537868
Epoch 59: training loss 16984569336.471
Test Loss of 25059203257.234329, Test MSE of 25059203085.442490
Epoch 60: training loss 16645996720.941
Test Loss of 23769490913.561878, Test MSE of 23769491024.065514
Epoch 61: training loss 15886951883.294
Test Loss of 24488986079.666897, Test MSE of 24488986403.976536
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24488940451.6432, 'MSE - std': 2695110852.5883245, 'R2 - mean': 0.8245280488653051, 'R2 - std': 0.016232774044851687} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005370 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110529174.588
Test Loss of 410764929694.030518, Test MSE of 410764931526.100403
Epoch 2: training loss 430089661018.353
Test Loss of 410746948689.976868, Test MSE of 410746944237.207214
Epoch 3: training loss 430062348408.471
Test Loss of 410723098620.209167, Test MSE of 410723104123.228027
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430076553818.353
Test Loss of 410726603264.710754, Test MSE of 410726603619.585449
Epoch 2: training loss 430066513076.706
Test Loss of 410727925463.366943, Test MSE of 410727928197.784851
Epoch 3: training loss 430066141545.412
Test Loss of 410727861837.949097, Test MSE of 410727863217.094360
Epoch 4: training loss 430065808082.824
Test Loss of 410727618951.403992, Test MSE of 410727610606.030762
Epoch 5: training loss 430065571478.588
Test Loss of 410727668808.499756, Test MSE of 410727677542.976379
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 121048624724.4765, 'MSE - std': 167262564841.89328, 'R2 - mean': 0.02090864216078714, 'R2 - std': 1.391980631945451} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 5, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003598 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043122928.941
Test Loss of 431612873817.558533, Test MSE of 431612880055.733154
Epoch 2: training loss 424023098548.706
Test Loss of 431592957742.556213, Test MSE of 431592959054.344177
Epoch 3: training loss 423995513795.765
Test Loss of 431565571579.498352, Test MSE of 431565572212.200806
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012079826.824
Test Loss of 431569802502.989380, Test MSE of 431569799863.338867
Epoch 2: training loss 424000001807.059
Test Loss of 431573113071.770447, Test MSE of 431573110885.039978
Epoch 3: training loss 423999494384.941
Test Loss of 431574269034.617310, Test MSE of 431574271060.501160
Epoch 4: training loss 423999051535.059
Test Loss of 431574169001.995361, Test MSE of 431574162617.587585
Epoch 5: training loss 423998792041.412
Test Loss of 431573967829.826904, Test MSE of 431573968503.733093
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 183153693480.32782, 'MSE - std': 194446832678.41345, 'R2 - mean': -0.42787517025842037, 'R2 - std': 1.534834096874017} 
 

Saving model.....
Results After CV: {'MSE - mean': 183153693480.32782, 'MSE - std': 194446832678.41345, 'R2 - mean': -0.42787517025842037, 'R2 - std': 1.534834096874017}
Train time: 61.14367724280019
Inference time: 0.06988811379851541
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 97 finished with value: 183153693480.32782 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 5, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003632 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526454452.706
Test Loss of 418113225272.494080, Test MSE of 418113223765.787170
Epoch 2: training loss 427507046279.529
Test Loss of 418095948107.621582, Test MSE of 418095947343.700806
Epoch 3: training loss 427480380596.706
Test Loss of 418071947825.387939, Test MSE of 418071950438.186157
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427494127375.059
Test Loss of 418075051446.924805, Test MSE of 418075053762.175598
Epoch 2: training loss 427484548758.588
Test Loss of 418076905677.605347, Test MSE of 418076907461.086487
Epoch 3: training loss 427484131809.882
Test Loss of 418077180715.051575, Test MSE of 418077178259.930298
Epoch 4: training loss 427483826778.353
Test Loss of 418077226520.516296, Test MSE of 418077224912.872803
Epoch 5: training loss 420504741165.176
Test Loss of 396218185460.334045, Test MSE of 396218190833.297607
Epoch 6: training loss 374898947614.118
Test Loss of 328885682877.142700, Test MSE of 328885685309.406067
Epoch 7: training loss 295940392478.118
Test Loss of 243663190756.463562, Test MSE of 243663192743.234863
Epoch 8: training loss 217091199608.471
Test Loss of 174160117540.182281, Test MSE of 174160120161.937378
Epoch 9: training loss 159552532269.176
Test Loss of 126463809562.055984, Test MSE of 126463807929.315231
Epoch 10: training loss 140061732231.529
Test Loss of 119065176784.566269, Test MSE of 119065177213.800629
Epoch 11: training loss 136637388890.353
Test Loss of 115894635658.096695, Test MSE of 115894635457.269516
Epoch 12: training loss 133164074074.353
Test Loss of 113334711686.602829, Test MSE of 113334710826.103821
Epoch 13: training loss 130392423378.824
Test Loss of 109748147569.758041, Test MSE of 109748148383.169174
Epoch 14: training loss 125599935247.059
Test Loss of 106345477688.730972, Test MSE of 106345476344.705444
Epoch 15: training loss 122903174535.529
Test Loss of 102976469861.322235, Test MSE of 102976467160.505447
Epoch 16: training loss 118150940446.118
Test Loss of 100316667399.935226, Test MSE of 100316666355.290482
Epoch 17: training loss 115515191235.765
Test Loss of 96203932167.224609, Test MSE of 96203932125.505066
Epoch 18: training loss 110255797515.294
Test Loss of 91958449995.503128, Test MSE of 91958451175.056900
Epoch 19: training loss 107384654576.941
Test Loss of 90130215551.555862, Test MSE of 90130216255.841736
Epoch 20: training loss 102654410962.824
Test Loss of 87492011003.499420, Test MSE of 87492011308.638687
Epoch 21: training loss 99416978108.235
Test Loss of 84191749358.056900, Test MSE of 84191748915.681732
Epoch 22: training loss 95219816824.471
Test Loss of 80408483555.279205, Test MSE of 80408484937.919266
Epoch 23: training loss 91546916382.118
Test Loss of 75859471092.570892, Test MSE of 75859470815.057922
Epoch 24: training loss 87935359469.176
Test Loss of 73612542507.229233, Test MSE of 73612543848.963013
Epoch 25: training loss 85078604800.000
Test Loss of 71233185038.271576, Test MSE of 71233185194.499542
Epoch 26: training loss 81308779083.294
Test Loss of 67264187503.566971, Test MSE of 67264188261.275993
Epoch 27: training loss 76447094358.588
Test Loss of 65788159977.733978, Test MSE of 65788159208.856552
Epoch 28: training loss 74179096131.765
Test Loss of 65163450757.892204, Test MSE of 65163450956.474236
Epoch 29: training loss 71849003512.471
Test Loss of 63537056170.844322, Test MSE of 63537056103.585609
Epoch 30: training loss 67824603693.176
Test Loss of 57998491920.877167, Test MSE of 57998492206.468788
Epoch 31: training loss 64632160941.176
Test Loss of 54081904861.712700, Test MSE of 54081904291.294662
Epoch 32: training loss 60825959256.471
Test Loss of 52711599145.452698, Test MSE of 52711600032.855667
Epoch 33: training loss 58717983111.529
Test Loss of 50437907885.449921, Test MSE of 50437907911.130959
Epoch 34: training loss 55697390524.235
Test Loss of 48534022983.002548, Test MSE of 48534023363.561859
Epoch 35: training loss 53042016124.235
Test Loss of 47330891200.399719, Test MSE of 47330891511.037308
Epoch 36: training loss 50201131971.765
Test Loss of 44116731196.698586, Test MSE of 44116731092.658112
Epoch 37: training loss 48559075456.000
Test Loss of 41017953640.756882, Test MSE of 41017953547.375343
Epoch 38: training loss 45235954492.235
Test Loss of 40655734533.388847, Test MSE of 40655734914.820801
Epoch 39: training loss 43414679235.765
Test Loss of 38296390811.151512, Test MSE of 38296391121.568436
Epoch 40: training loss 41294139309.176
Test Loss of 36724309743.122833, Test MSE of 36724310406.690712
Epoch 41: training loss 39185866804.706
Test Loss of 34812016268.110107, Test MSE of 34812016554.840836
Epoch 42: training loss 37138782528.000
Test Loss of 32045102780.432106, Test MSE of 32045102528.711819
Epoch 43: training loss 35571929795.765
Test Loss of 32925367760.980801, Test MSE of 32925368074.597206
Epoch 44: training loss 33915909018.353
Test Loss of 32577157205.747860, Test MSE of 32577156848.049099
Epoch 45: training loss 32405097573.647
Test Loss of 27905067137.332409, Test MSE of 27905067733.513618
Epoch 46: training loss 30700981187.765
Test Loss of 27103536853.066853, Test MSE of 27103536609.602951
Epoch 47: training loss 28960853323.294
Test Loss of 31161249654.613926, Test MSE of 31161249914.437534
Epoch 48: training loss 28066306676.706
Test Loss of 25599696201.015961, Test MSE of 25599696283.975163
Epoch 49: training loss 26879138612.706
Test Loss of 27823845144.812397, Test MSE of 27823844845.480202
Epoch 50: training loss 25669448929.882
Test Loss of 25561480429.109413, Test MSE of 25561480373.899464
Epoch 51: training loss 24285392811.294
Test Loss of 25290641346.413139, Test MSE of 25290641098.460770
Epoch 52: training loss 23750936677.647
Test Loss of 24277399734.391857, Test MSE of 24277399788.147720
Epoch 53: training loss 22602643288.471
Test Loss of 23876964786.897987, Test MSE of 23876964445.685696
Epoch 54: training loss 21833280775.529
Test Loss of 21790836764.424706, Test MSE of 21790836679.377235
Epoch 55: training loss 20887434812.235
Test Loss of 25318622154.229935, Test MSE of 25318622278.527515
Epoch 56: training loss 19837090597.647
Test Loss of 22968806861.190838, Test MSE of 22968806515.067558
Epoch 57: training loss 19577224862.118
Test Loss of 23316104235.821419, Test MSE of 23316104353.534542
Epoch 58: training loss 18659460743.529
Test Loss of 22751586962.979412, Test MSE of 22751586729.952168
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22751586729.952168, 'MSE - std': 0.0, 'R2 - mean': 0.8228309256054794, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005553 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917632933.647
Test Loss of 424556038316.916931, Test MSE of 424556029001.501648
Epoch 2: training loss 427896609370.353
Test Loss of 424539544423.217224, Test MSE of 424539539342.315186
Epoch 3: training loss 427868316129.882
Test Loss of 424516670941.771912, Test MSE of 424516674585.540894
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427887740325.647
Test Loss of 424519968025.641479, Test MSE of 424519966915.556763
Epoch 2: training loss 427875728444.235
Test Loss of 424520418621.646057, Test MSE of 424520422408.064636
Epoch 3: training loss 427875242224.941
Test Loss of 424520495300.130493, Test MSE of 424520502491.113342
Epoch 4: training loss 427874808530.824
Test Loss of 424519632098.687012, Test MSE of 424519628347.948059
Epoch 5: training loss 420868297908.706
Test Loss of 402951656129.880188, Test MSE of 402951658698.697327
Epoch 6: training loss 374872898740.706
Test Loss of 335749374222.982178, Test MSE of 335749373566.142151
Epoch 7: training loss 295739172020.706
Test Loss of 252930868969.911621, Test MSE of 252930868617.799164
Epoch 8: training loss 216669929351.529
Test Loss of 184069496860.424713, Test MSE of 184069495171.611725
Epoch 9: training loss 156875162021.647
Test Loss of 137049346299.084900, Test MSE of 137049345518.462936
Epoch 10: training loss 136784631657.412
Test Loss of 129572965447.772385, Test MSE of 129572966690.202927
Epoch 11: training loss 133506443324.235
Test Loss of 126768524505.685867, Test MSE of 126768527420.706573
Epoch 12: training loss 129121527145.412
Test Loss of 123738997442.827667, Test MSE of 123738998569.618866
Epoch 13: training loss 126517530322.824
Test Loss of 120373218959.426331, Test MSE of 120373216771.496353
Epoch 14: training loss 121599717315.765
Test Loss of 116781351236.041641, Test MSE of 116781352386.988953
Epoch 15: training loss 119113825641.412
Test Loss of 113067770828.124908, Test MSE of 113067770954.568176
Epoch 16: training loss 113622445146.353
Test Loss of 109384417467.603058, Test MSE of 109384419472.284439
Epoch 17: training loss 110754697908.706
Test Loss of 105653635163.432800, Test MSE of 105653631658.638641
Epoch 18: training loss 106624348250.353
Test Loss of 102707300624.403427, Test MSE of 102707299742.612442
Epoch 19: training loss 102691824308.706
Test Loss of 97282119573.407349, Test MSE of 97282118890.600830
Epoch 20: training loss 97875404980.706
Test Loss of 93536107591.772385, Test MSE of 93536105020.916748
Epoch 21: training loss 95087150983.529
Test Loss of 90646324072.638443, Test MSE of 90646324460.772842
Epoch 22: training loss 90885011425.882
Test Loss of 86758951769.952347, Test MSE of 86758951145.909256
Epoch 23: training loss 86799570432.000
Test Loss of 80946896549.218597, Test MSE of 80946895897.003174
Epoch 24: training loss 82969232037.647
Test Loss of 80366973632.932678, Test MSE of 80366973846.223938
Epoch 25: training loss 79045437274.353
Test Loss of 73662750805.274109, Test MSE of 73662751917.403015
Epoch 26: training loss 75689929758.118
Test Loss of 69494488310.110565, Test MSE of 69494490669.787491
Epoch 27: training loss 72334554970.353
Test Loss of 68519970703.722412, Test MSE of 68519969408.195351
Epoch 28: training loss 68244061846.588
Test Loss of 66952360621.035393, Test MSE of 66952358205.661850
Epoch 29: training loss 65622006031.059
Test Loss of 61171029237.636826, Test MSE of 61171029159.198708
Epoch 30: training loss 62052162665.412
Test Loss of 60830961737.904236, Test MSE of 60830962143.660362
Epoch 31: training loss 57661297694.118
Test Loss of 56370320252.535736, Test MSE of 56370320476.509415
Epoch 32: training loss 55001610112.000
Test Loss of 54583347951.833450, Test MSE of 54583346455.232040
Epoch 33: training loss 52131608109.176
Test Loss of 52913711237.122368, Test MSE of 52913709895.168747
Epoch 34: training loss 50083932483.765
Test Loss of 50269918612.815178, Test MSE of 50269917122.538010
Epoch 35: training loss 46849220299.294
Test Loss of 46368632673.769142, Test MSE of 46368631012.955917
Epoch 36: training loss 44371061451.294
Test Loss of 45311155750.728661, Test MSE of 45311156163.481972
Epoch 37: training loss 41855330394.353
Test Loss of 46133423441.543373, Test MSE of 46133423628.296310
Epoch 38: training loss 39667013797.647
Test Loss of 40990480563.075645, Test MSE of 40990481829.554634
Epoch 39: training loss 37457815536.941
Test Loss of 40412067145.015961, Test MSE of 40412067803.696045
Epoch 40: training loss 35357085184.000
Test Loss of 37438515818.237335, Test MSE of 37438515608.670341
Epoch 41: training loss 33719632587.294
Test Loss of 35733309148.173027, Test MSE of 35733308400.490013
Epoch 42: training loss 31432379218.824
Test Loss of 33071014489.182510, Test MSE of 33071013597.719234
Epoch 43: training loss 30352081837.176
Test Loss of 35704753128.075874, Test MSE of 35704753004.400253
Epoch 44: training loss 28794949933.176
Test Loss of 31485719970.553783, Test MSE of 31485720020.397476
Epoch 45: training loss 27023712361.412
Test Loss of 30797244309.881100, Test MSE of 30797245433.293934
Epoch 46: training loss 25670460615.529
Test Loss of 33600363262.519547, Test MSE of 33600363288.301048
Epoch 47: training loss 24185592007.529
Test Loss of 31802143051.621559, Test MSE of 31802143046.075394
Epoch 48: training loss 23093868468.706
Test Loss of 30638882489.115891, Test MSE of 30638882553.552135
Epoch 49: training loss 22254807800.471
Test Loss of 29099245945.101086, Test MSE of 29099246292.686363
Epoch 50: training loss 20673598422.588
Test Loss of 27139849852.002777, Test MSE of 27139850223.076775
Epoch 51: training loss 20080214095.059
Test Loss of 27848275115.258846, Test MSE of 27848275317.323765
Epoch 52: training loss 19058320146.824
Test Loss of 24985102069.755264, Test MSE of 24985102040.222794
Epoch 53: training loss 18146959040.000
Test Loss of 29725153649.994911, Test MSE of 29725154480.686333
Epoch 54: training loss 17676025167.059
Test Loss of 30618289902.885960, Test MSE of 30618288709.442005
Epoch 55: training loss 16896712847.059
Test Loss of 27604386981.100163, Test MSE of 27604386425.826271
Epoch 56: training loss 16414933289.412
Test Loss of 27078424332.495026, Test MSE of 27078424564.109257
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24915005647.030712, 'MSE - std': 2163418917.0785446, 'R2 - mean': 0.8147544761760466, 'R2 - std': 0.008076449429432808} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003592 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927548807.529
Test Loss of 447258864889.900513, Test MSE of 447258877989.120056
Epoch 2: training loss 421907157835.294
Test Loss of 447241583093.696045, Test MSE of 447241577325.509644
Epoch 3: training loss 421879992681.412
Test Loss of 447217867945.363892, Test MSE of 447217875981.399475
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421902655728.941
Test Loss of 447224458808.257202, Test MSE of 447224454439.089172
Epoch 2: training loss 421889794048.000
Test Loss of 447225567311.115417, Test MSE of 447225571118.871948
Epoch 3: training loss 421889328429.176
Test Loss of 447225482370.516785, Test MSE of 447225483498.980896
Epoch 4: training loss 421889002676.706
Test Loss of 447225361000.816101, Test MSE of 447225359023.148804
Epoch 5: training loss 415268130936.471
Test Loss of 425885276511.992615, Test MSE of 425885283771.494141
Epoch 6: training loss 370211658932.706
Test Loss of 358575619667.734436, Test MSE of 358575620143.411743
Epoch 7: training loss 291236059376.941
Test Loss of 271827459358.852661, Test MSE of 271827463478.496185
Epoch 8: training loss 213163563610.353
Test Loss of 200122160306.365021, Test MSE of 200122162112.372589
Epoch 9: training loss 153710082499.765
Test Loss of 149114349625.323151, Test MSE of 149114350236.939423
Epoch 10: training loss 135491583367.529
Test Loss of 140101101405.505432, Test MSE of 140101099852.427399
Epoch 11: training loss 131737485010.824
Test Loss of 137199257340.861435, Test MSE of 137199256141.758957
Epoch 12: training loss 127862386115.765
Test Loss of 133508182268.979874, Test MSE of 133508182076.279053
Epoch 13: training loss 125995250507.294
Test Loss of 130593200862.778625, Test MSE of 130593202766.058380
Epoch 14: training loss 120397183548.235
Test Loss of 126201497265.062225, Test MSE of 126201496454.420929
Epoch 15: training loss 118129632256.000
Test Loss of 123210867008.251678, Test MSE of 123210867459.976334
Epoch 16: training loss 113817314514.824
Test Loss of 120581049629.905151, Test MSE of 120581048422.212860
Epoch 17: training loss 109879630185.412
Test Loss of 116621346799.418915, Test MSE of 116621346642.365692
Epoch 18: training loss 104614775506.824
Test Loss of 114782008424.460785, Test MSE of 114782005932.495941
Epoch 19: training loss 101814462072.471
Test Loss of 108792198854.854492, Test MSE of 108792198960.847855
Epoch 20: training loss 97811857076.706
Test Loss of 106239095302.277115, Test MSE of 106239094235.962036
Epoch 21: training loss 94525333624.471
Test Loss of 102939558699.051590, Test MSE of 102939559130.001266
Epoch 22: training loss 90911450864.941
Test Loss of 98346264485.514694, Test MSE of 98346263909.500198
Epoch 23: training loss 86690269771.294
Test Loss of 95004676259.915802, Test MSE of 95004676408.771729
Epoch 24: training loss 83492569946.353
Test Loss of 92720793729.332413, Test MSE of 92720794144.221420
Epoch 25: training loss 80123284269.176
Test Loss of 85358112505.545227, Test MSE of 85358114138.391052
Epoch 26: training loss 75819407329.882
Test Loss of 82729794555.025681, Test MSE of 82729794305.333832
Epoch 27: training loss 72752655887.059
Test Loss of 79661943750.913712, Test MSE of 79661943518.069534
Epoch 28: training loss 69201073317.647
Test Loss of 75358730341.855194, Test MSE of 75358730276.776718
Epoch 29: training loss 66953654000.941
Test Loss of 72910290883.360626, Test MSE of 72910291239.606995
Epoch 30: training loss 62962639375.059
Test Loss of 66789103871.348602, Test MSE of 66789103533.815002
Epoch 31: training loss 59958339478.588
Test Loss of 63606589413.944023, Test MSE of 63606590401.048599
Epoch 32: training loss 57440165647.059
Test Loss of 64145761161.800598, Test MSE of 64145761262.886314
Epoch 33: training loss 54304961551.059
Test Loss of 59813109703.150589, Test MSE of 59813110133.661179
Epoch 34: training loss 51549288719.059
Test Loss of 54339730322.801758, Test MSE of 54339729519.139580
Epoch 35: training loss 49214693790.118
Test Loss of 54870235993.952347, Test MSE of 54870235622.474007
Epoch 36: training loss 46440352662.588
Test Loss of 51696673508.937309, Test MSE of 51696673620.823257
Epoch 37: training loss 44266477643.294
Test Loss of 47911905751.376358, Test MSE of 47911906484.597496
Epoch 38: training loss 42495437409.882
Test Loss of 45047176338.624107, Test MSE of 45047177119.225830
Epoch 39: training loss 39665793106.824
Test Loss of 42583284338.764748, Test MSE of 42583284941.918633
Epoch 40: training loss 37694318561.882
Test Loss of 42157410334.793434, Test MSE of 42157410472.481483
Epoch 41: training loss 36382881754.353
Test Loss of 42604104857.256538, Test MSE of 42604104712.882736
Epoch 42: training loss 33947947105.882
Test Loss of 37724011120.869766, Test MSE of 37724010997.809952
Epoch 43: training loss 32104362616.471
Test Loss of 36536271897.819107, Test MSE of 36536271655.988663
Epoch 44: training loss 31046426962.824
Test Loss of 32720306269.801525, Test MSE of 32720306895.397438
Epoch 45: training loss 29225915760.941
Test Loss of 30768095665.003006, Test MSE of 30768095605.824654
Epoch 46: training loss 27877494979.765
Test Loss of 31605708776.786491, Test MSE of 31605708927.067192
Epoch 47: training loss 26552656918.588
Test Loss of 29691886298.988667, Test MSE of 29691885600.666039
Epoch 48: training loss 25212784666.353
Test Loss of 29898489662.712006, Test MSE of 29898489654.736408
Epoch 49: training loss 24564361133.176
Test Loss of 27699688326.958130, Test MSE of 27699688401.880024
Epoch 50: training loss 23055664109.176
Test Loss of 26105285088.377514, Test MSE of 26105285383.145893
Epoch 51: training loss 21761551431.529
Test Loss of 26292421592.916031, Test MSE of 26292421814.192764
Epoch 52: training loss 21461331572.706
Test Loss of 23522967349.237103, Test MSE of 23522967400.424465
Epoch 53: training loss 20372609882.353
Test Loss of 25050102546.890587, Test MSE of 25050103242.633919
Epoch 54: training loss 19619963294.118
Test Loss of 26139824998.743465, Test MSE of 26139825080.335140
Epoch 55: training loss 19254692988.235
Test Loss of 22726547582.016193, Test MSE of 22726547080.179081
Epoch 56: training loss 18313764754.824
Test Loss of 23695921224.956741, Test MSE of 23695921695.968876
Epoch 57: training loss 17697458469.647
Test Loss of 22560969506.050426, Test MSE of 22560969206.266647
Epoch 58: training loss 17179213839.059
Test Loss of 23941700090.433495, Test MSE of 23941700341.664307
Epoch 59: training loss 16853841035.294
Test Loss of 24597168617.378674, Test MSE of 24597169040.137165
Epoch 60: training loss 16053564521.412
Test Loss of 21952599364.752254, Test MSE of 21952599145.022678
Epoch 61: training loss 15636786243.765
Test Loss of 21983363012.781864, Test MSE of 21983362720.413616
Epoch 62: training loss 15234731248.941
Test Loss of 21602404336.840157, Test MSE of 21602404635.426174
Epoch 63: training loss 14820345287.529
Test Loss of 20733058561.776543, Test MSE of 20733058326.149841
Epoch 64: training loss 14416239902.118
Test Loss of 21907767322.055981, Test MSE of 21907767376.040417
Epoch 65: training loss 14130463006.118
Test Loss of 22687978632.675457, Test MSE of 22687978958.106670
Epoch 66: training loss 13592947840.000
Test Loss of 24158227769.856117, Test MSE of 24158228231.283760
Epoch 67: training loss 13277005820.235
Test Loss of 22455752487.024750, Test MSE of 22455752567.006599
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24095254620.356007, 'MSE - std': 2112874289.9939246, 'R2 - mean': 0.826674163154102, 'R2 - std': 0.018100936409491106} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005427 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109771173.647
Test Loss of 410764640225.673279, Test MSE of 410764634776.189026
Epoch 2: training loss 430088670027.294
Test Loss of 410746219196.357239, Test MSE of 410746220903.777649
Epoch 3: training loss 430061540773.647
Test Loss of 410722925363.768616, Test MSE of 410722921295.440857
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430079052860.235
Test Loss of 410728222589.216125, Test MSE of 410728223801.303284
Epoch 2: training loss 430065913374.118
Test Loss of 410728849002.380371, Test MSE of 410728846067.458557
Epoch 3: training loss 430065433419.294
Test Loss of 410727844703.363281, Test MSE of 410727843186.091370
Epoch 4: training loss 430065110558.118
Test Loss of 410727683016.559021, Test MSE of 410727683224.360901
Epoch 5: training loss 422955225088.000
Test Loss of 388705666654.060181, Test MSE of 388705666567.617065
Epoch 6: training loss 377051096847.059
Test Loss of 321996384880.540466, Test MSE of 321996386154.931030
Epoch 7: training loss 297344827030.588
Test Loss of 236675095457.702911, Test MSE of 236675097905.770142
Epoch 8: training loss 218628434763.294
Test Loss of 166200832744.899597, Test MSE of 166200835194.805878
Epoch 9: training loss 160009833773.176
Test Loss of 119793841100.349838, Test MSE of 119793841547.432648
Epoch 10: training loss 141572618721.882
Test Loss of 112203609339.616837, Test MSE of 112203609078.586731
Epoch 11: training loss 137365325763.765
Test Loss of 109710303902.978256, Test MSE of 109710304268.088043
Epoch 12: training loss 134094045967.059
Test Loss of 106523653466.387787, Test MSE of 106523652515.911041
Epoch 13: training loss 129988113679.059
Test Loss of 103971519158.670990, Test MSE of 103971518738.486801
Epoch 14: training loss 128345549432.471
Test Loss of 100576170850.206390, Test MSE of 100576171506.517288
Epoch 15: training loss 123002189673.412
Test Loss of 97309941885.097641, Test MSE of 97309942131.337204
Epoch 16: training loss 119010567198.118
Test Loss of 94190249179.394730, Test MSE of 94190248622.415070
Epoch 17: training loss 115646214083.765
Test Loss of 90867037094.441467, Test MSE of 90867037615.458282
Epoch 18: training loss 111417968911.059
Test Loss of 88050441747.191116, Test MSE of 88050441825.218338
Epoch 19: training loss 107562187595.294
Test Loss of 84098811956.124023, Test MSE of 84098811877.780243
Epoch 20: training loss 103010760628.706
Test Loss of 80619807538.820923, Test MSE of 80619808985.341309
Epoch 21: training loss 99375633016.471
Test Loss of 78839995737.440079, Test MSE of 78839995939.122482
Epoch 22: training loss 94144061319.529
Test Loss of 73672044492.823700, Test MSE of 73672044333.222763
Epoch 23: training loss 91583777295.059
Test Loss of 72299875599.518738, Test MSE of 72299875658.216705
Epoch 24: training loss 87190839401.412
Test Loss of 69140096026.062012, Test MSE of 69140096666.712494
Epoch 25: training loss 82567464568.471
Test Loss of 65984913051.661270, Test MSE of 65984911053.643639
Epoch 26: training loss 80999034849.882
Test Loss of 64222485682.643219, Test MSE of 64222487097.543518
Epoch 27: training loss 76302619617.882
Test Loss of 60061673577.195740, Test MSE of 60061673951.826561
Epoch 28: training loss 73427775412.706
Test Loss of 56666308630.745026, Test MSE of 56666309460.495506
Epoch 29: training loss 69129935631.059
Test Loss of 56198329193.788063, Test MSE of 56198329146.993240
Epoch 30: training loss 66532325466.353
Test Loss of 51974848043.831558, Test MSE of 51974847803.994797
Epoch 31: training loss 62921247864.471
Test Loss of 51215836762.269318, Test MSE of 51215836046.547478
Epoch 32: training loss 60219959311.059
Test Loss of 47371001950.770943, Test MSE of 47371002687.778908
Epoch 33: training loss 56800370605.176
Test Loss of 45124332527.415085, Test MSE of 45124334071.651360
Epoch 34: training loss 53898173793.882
Test Loss of 43199999340.394264, Test MSE of 43199999273.575645
Epoch 35: training loss 51484590486.588
Test Loss of 42032023109.419716, Test MSE of 42032023158.490280
Epoch 36: training loss 48177239740.235
Test Loss of 41831247017.166130, Test MSE of 41831246041.471474
Epoch 37: training loss 46222911096.471
Test Loss of 39972292091.498383, Test MSE of 39972292116.203827
Epoch 38: training loss 43936586112.000
Test Loss of 35239005988.131424, Test MSE of 35239006180.020699
Epoch 39: training loss 41084616199.529
Test Loss of 34783412817.266083, Test MSE of 34783412660.168976
Epoch 40: training loss 39404559781.647
Test Loss of 33197422520.921795, Test MSE of 33197422820.284199
Epoch 41: training loss 36807172517.647
Test Loss of 31378431342.289680, Test MSE of 31378431411.350338
Epoch 42: training loss 35663141428.706
Test Loss of 30054272681.876907, Test MSE of 30054272658.439503
Epoch 43: training loss 34140352180.706
Test Loss of 29737819587.583527, Test MSE of 29737819616.753632
Epoch 44: training loss 32453267516.235
Test Loss of 27187479162.965294, Test MSE of 27187479205.931610
Epoch 45: training loss 30616895284.706
Test Loss of 25835252706.147156, Test MSE of 25835252535.306042
Epoch 46: training loss 29127875267.765
Test Loss of 25470557643.165203, Test MSE of 25470557324.717361
Epoch 47: training loss 27630933925.647
Test Loss of 25351105290.069412, Test MSE of 25351105879.002205
Epoch 48: training loss 26450018906.353
Test Loss of 23731481233.710320, Test MSE of 23731481171.384838
Epoch 49: training loss 25389016606.118
Test Loss of 22864900756.553448, Test MSE of 22864900989.066578
Epoch 50: training loss 24003730876.235
Test Loss of 24197999548.238777, Test MSE of 24197999950.704411
Epoch 51: training loss 23393214490.353
Test Loss of 23980073171.339195, Test MSE of 23980072929.489372
Epoch 52: training loss 22415742825.412
Test Loss of 19590642243.050438, Test MSE of 19590642301.656418
Epoch 53: training loss 21337683422.118
Test Loss of 22003653063.848217, Test MSE of 22003652816.914673
Epoch 54: training loss 20669580656.000
Test Loss of 20750024810.143452, Test MSE of 20750024876.388927
Epoch 55: training loss 20065043440.941
Test Loss of 20868432836.768162, Test MSE of 20868433054.908482
Epoch 56: training loss 19307519412.706
Test Loss of 19392993853.364182, Test MSE of 19392993802.960915
Epoch 57: training loss 18635014085.647
Test Loss of 22049911462.559925, Test MSE of 22049911619.456585
Epoch 58: training loss 18195030981.647
Test Loss of 21519041816.995834, Test MSE of 21519041388.852295
Epoch 59: training loss 17587479717.647
Test Loss of 21934920891.172604, Test MSE of 21934921026.105354
Epoch 60: training loss 17155695627.294
Test Loss of 19218273274.787598, Test MSE of 19218273219.680256
Epoch 61: training loss 16332657281.882
Test Loss of 20032402389.826931, Test MSE of 20032402131.181015
Epoch 62: training loss 15996966539.294
Test Loss of 19175095961.765850, Test MSE of 19175095858.830894
Epoch 63: training loss 15893722797.176
Test Loss of 20228596663.500233, Test MSE of 20228596985.252583
Epoch 64: training loss 15387913394.824
Test Loss of 19802245780.553448, Test MSE of 19802245743.916698
Epoch 65: training loss 14643483241.412
Test Loss of 19444594282.380379, Test MSE of 19444594005.010757
Epoch 66: training loss 14394681701.647
Test Loss of 20846755150.541416, Test MSE of 20846755576.031677
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 23283129859.274925, 'MSE - std': 2307990079.4134746, 'R2 - mean': 0.8269908377950513, 'R2 - std': 0.015685463737486107} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003784 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043944658.824
Test Loss of 431613247776.103638, Test MSE of 431613250455.867798
Epoch 2: training loss 424024337468.235
Test Loss of 431592858938.165649, Test MSE of 431592858071.401428
Epoch 3: training loss 423996864632.471
Test Loss of 431564863799.796387, Test MSE of 431564862636.072449
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424012523279.059
Test Loss of 431569575917.045837, Test MSE of 431569577600.727173
Epoch 2: training loss 423999183932.235
Test Loss of 431571126870.004639, Test MSE of 431571128122.980957
Epoch 3: training loss 423998659523.765
Test Loss of 431570703842.857910, Test MSE of 431570706725.454712
Epoch 4: training loss 423998223299.765
Test Loss of 431570026450.509949, Test MSE of 431570029645.573547
Epoch 5: training loss 417074104801.882
Test Loss of 409091556685.119873, Test MSE of 409091562127.677917
Epoch 6: training loss 371653436476.235
Test Loss of 340623920240.303589, Test MSE of 340623919635.278931
Epoch 7: training loss 293137469199.059
Test Loss of 255015590067.117065, Test MSE of 255015593290.509521
Epoch 8: training loss 215222433280.000
Test Loss of 183358479834.328552, Test MSE of 183358481107.826294
Epoch 9: training loss 158538540604.235
Test Loss of 132705743350.285980, Test MSE of 132705742903.402939
Epoch 10: training loss 138406042849.882
Test Loss of 124662426171.468765, Test MSE of 124662425588.441422
Epoch 11: training loss 134963382347.294
Test Loss of 120918845279.837112, Test MSE of 120918843761.566818
Epoch 12: training loss 132277657600.000
Test Loss of 117489455033.869507, Test MSE of 117489455119.036591
Epoch 13: training loss 128332182076.235
Test Loss of 114342612535.204071, Test MSE of 114342612707.755402
Epoch 14: training loss 124084450394.353
Test Loss of 110876765239.440994, Test MSE of 110876766652.066635
Epoch 15: training loss 120959261485.176
Test Loss of 108175658558.785751, Test MSE of 108175660485.146713
Epoch 16: training loss 118151966614.588
Test Loss of 105333120951.974091, Test MSE of 105333121688.581696
Epoch 17: training loss 113901654497.882
Test Loss of 101782904665.203140, Test MSE of 101782901222.423477
Epoch 18: training loss 108916086528.000
Test Loss of 97990338570.424805, Test MSE of 97990340594.465958
Epoch 19: training loss 106233365293.176
Test Loss of 91854769810.184174, Test MSE of 91854768991.399612
Epoch 20: training loss 101212090593.882
Test Loss of 88389676493.060623, Test MSE of 88389675945.024994
Epoch 21: training loss 97559499384.471
Test Loss of 87343226633.595551, Test MSE of 87343226214.785782
Epoch 22: training loss 93937659256.471
Test Loss of 80097753498.358170, Test MSE of 80097754411.506653
Epoch 23: training loss 90438655563.294
Test Loss of 78989633345.510406, Test MSE of 78989634122.659760
Epoch 24: training loss 87168076559.059
Test Loss of 77207223539.087463, Test MSE of 77207224116.720673
Epoch 25: training loss 82902889276.235
Test Loss of 72777083536.762604, Test MSE of 72777085394.217606
Epoch 26: training loss 79294548374.588
Test Loss of 68816405133.919479, Test MSE of 68816407530.223648
Epoch 27: training loss 76619255838.118
Test Loss of 65559907546.447014, Test MSE of 65559907797.640823
Epoch 28: training loss 73087892992.000
Test Loss of 63902541487.563164, Test MSE of 63902542062.430489
Epoch 29: training loss 69718470128.941
Test Loss of 59797895557.508560, Test MSE of 59797895779.155243
Epoch 30: training loss 65863091474.824
Test Loss of 56561818016.044426, Test MSE of 56561818450.085243
Epoch 31: training loss 63487939087.059
Test Loss of 54593698727.389175, Test MSE of 54593697861.857979
Epoch 32: training loss 60543011139.765
Test Loss of 52436615535.711243, Test MSE of 52436616863.497093
Epoch 33: training loss 57276774174.118
Test Loss of 48609284135.803795, Test MSE of 48609284166.339180
Epoch 34: training loss 55007634898.824
Test Loss of 45076866752.621933, Test MSE of 45076867469.737335
Epoch 35: training loss 52998567981.176
Test Loss of 43739822304.607124, Test MSE of 43739821983.892937
Epoch 36: training loss 49258757564.235
Test Loss of 38589350407.344749, Test MSE of 38589350151.163979
Epoch 37: training loss 47544285906.824
Test Loss of 34206938786.295235, Test MSE of 34206938952.485424
Epoch 38: training loss 45336476423.529
Test Loss of 37816516496.170288, Test MSE of 37816516092.112885
Epoch 39: training loss 42863326727.529
Test Loss of 33069648010.365570, Test MSE of 33069648037.719913
Epoch 40: training loss 40492188348.235
Test Loss of 32465246144.503471, Test MSE of 32465246584.641190
Epoch 41: training loss 39248781793.882
Test Loss of 34956563920.851456, Test MSE of 34956563981.256264
Epoch 42: training loss 36982694490.353
Test Loss of 29158193272.359093, Test MSE of 29158193398.918056
Epoch 43: training loss 35306339862.588
Test Loss of 31026177585.517815, Test MSE of 31026177970.584381
Epoch 44: training loss 33265472165.647
Test Loss of 29017447279.474316, Test MSE of 29017447151.492455
Epoch 45: training loss 31652839845.647
Test Loss of 28229158397.393799, Test MSE of 28229159015.002300
Epoch 46: training loss 30485543958.588
Test Loss of 24195223001.380840, Test MSE of 24195223073.565617
Epoch 47: training loss 28835150275.765
Test Loss of 23890118702.437759, Test MSE of 23890119125.852577
Epoch 48: training loss 28107119646.118
Test Loss of 25749464881.399353, Test MSE of 25749464511.760525
Epoch 49: training loss 26404243568.941
Test Loss of 24013477696.562702, Test MSE of 24013477605.315857
Epoch 50: training loss 25567179975.529
Test Loss of 23821000087.988895, Test MSE of 23820999701.718361
Epoch 51: training loss 24543173688.471
Test Loss of 21761467198.193428, Test MSE of 21761466790.915359
Epoch 52: training loss 23403445816.471
Test Loss of 22561283934.889404, Test MSE of 22561284468.943016
Epoch 53: training loss 22476039216.941
Test Loss of 20950697428.642296, Test MSE of 20950697634.548820
Epoch 54: training loss 21587602526.118
Test Loss of 20965300191.777882, Test MSE of 20965300125.190594
Epoch 55: training loss 20975118855.529
Test Loss of 20480432880.481258, Test MSE of 20480432519.661709
Epoch 56: training loss 19977553276.235
Test Loss of 21028522229.456734, Test MSE of 21028522105.003529
Epoch 57: training loss 19696869643.294
Test Loss of 20030919627.402130, Test MSE of 20030919677.132339
Epoch 58: training loss 19055895122.824
Test Loss of 20303276560.821842, Test MSE of 20303276562.729820
Epoch 59: training loss 18168317699.765
Test Loss of 21293143234.754280, Test MSE of 21293143023.446121
Epoch 60: training loss 17634343205.647
Test Loss of 20186590400.858860, Test MSE of 20186590550.231972
Epoch 61: training loss 17086797232.941
Test Loss of 20003870754.117538, Test MSE of 20003870173.939552
Epoch 62: training loss 16983585016.471
Test Loss of 20096368750.882000, Test MSE of 20096368293.943142
Epoch 63: training loss 16560760643.765
Test Loss of 20186995209.240166, Test MSE of 20186995214.487610
Epoch 64: training loss 15979826232.471
Test Loss of 20889627904.355392, Test MSE of 20889627849.258293
Epoch 65: training loss 15948212848.941
Test Loss of 18013859891.176308, Test MSE of 18013859994.660797
Epoch 66: training loss 15263883271.529
Test Loss of 19804941679.237389, Test MSE of 19804941433.439568
Epoch 67: training loss 14758244378.353
Test Loss of 19258913190.678391, Test MSE of 19258913469.808865
Epoch 68: training loss 14348527872.000
Test Loss of 20293255950.807961, Test MSE of 20293256249.892597
Epoch 69: training loss 14299367164.235
Test Loss of 18111829386.720963, Test MSE of 18111829417.367580
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22248869770.893456, 'MSE - std': 2922367240.2715216, 'R2 - mean': 0.8345407073298212, 'R2 - std': 0.020611383701638654} 
 

Saving model.....
Results After CV: {'MSE - mean': 22248869770.893456, 'MSE - std': 2922367240.2715216, 'R2 - mean': 0.8345407073298212, 'R2 - std': 0.020611383701638654}
Train time: 99.92998366339744
Inference time: 0.07194269260071451
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 98 finished with value: 22248869770.893456 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005495 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427526575646.118
Test Loss of 418113534427.166321, Test MSE of 418113530240.029053
Epoch 2: training loss 427507365285.647
Test Loss of 418096470455.161682, Test MSE of 418096470215.230957
Epoch 3: training loss 427480756224.000
Test Loss of 418073283407.529968, Test MSE of 418073283070.361389
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427497805462.588
Test Loss of 418079354871.472595, Test MSE of 418079364149.092285
Epoch 2: training loss 427486107888.941
Test Loss of 418080161511.542908, Test MSE of 418080158580.520508
Epoch 3: training loss 427485666364.235
Test Loss of 418079251003.810303, Test MSE of 418079258845.580872
Epoch 4: training loss 427485287604.706
Test Loss of 418078468930.265076, Test MSE of 418078472927.121033
Epoch 5: training loss 420696566241.882
Test Loss of 396041116663.235718, Test MSE of 396041120113.779907
Epoch 6: training loss 375639584888.471
Test Loss of 330309964425.030762, Test MSE of 330309964753.983459
Epoch 7: training loss 297016144173.176
Test Loss of 244934299103.193146, Test MSE of 244934298378.448486
Epoch 8: training loss 218568700175.059
Test Loss of 174427904477.061310, Test MSE of 174427909283.050446
Epoch 9: training loss 159408917082.353
Test Loss of 126745715675.284760, Test MSE of 126745714814.105759
Epoch 10: training loss 140192334576.941
Test Loss of 119038733975.006241, Test MSE of 119038735492.534271
Epoch 11: training loss 135582510170.353
Test Loss of 115927955713.243576, Test MSE of 115927954984.054031
Epoch 12: training loss 134089903917.176
Test Loss of 113498476171.636368, Test MSE of 113498474721.291229
Epoch 13: training loss 130989067956.706
Test Loss of 109828646126.056900, Test MSE of 109828645998.796204
Epoch 14: training loss 126784162379.294
Test Loss of 106707510544.166550, Test MSE of 106707509671.693359
Epoch 15: training loss 123157014949.647
Test Loss of 103536266281.452698, Test MSE of 103536266558.552368
Epoch 16: training loss 119121243843.765
Test Loss of 100473572039.091370, Test MSE of 100473570728.579926
Epoch 17: training loss 114897225758.118
Test Loss of 98007293046.199402, Test MSE of 98007292352.623764
Epoch 18: training loss 110603989428.706
Test Loss of 93707300271.818649, Test MSE of 93707300672.646652
Epoch 19: training loss 107981890567.529
Test Loss of 90551191535.418915, Test MSE of 90551192378.764450
Epoch 20: training loss 103572084751.059
Test Loss of 87832714331.669678, Test MSE of 87832714664.327988
Epoch 21: training loss 100494458744.471
Test Loss of 84136702980.026840, Test MSE of 84136702812.878250
Epoch 22: training loss 97052274349.176
Test Loss of 82530541778.342819, Test MSE of 82530542247.719635
Epoch 23: training loss 92251351371.294
Test Loss of 79172740064.969696, Test MSE of 79172740549.257950
Epoch 24: training loss 89267808557.176
Test Loss of 75473187929.300949, Test MSE of 75473186015.187744
Epoch 25: training loss 85208987843.765
Test Loss of 72870515700.866989, Test MSE of 72870515889.115219
Epoch 26: training loss 82223481103.059
Test Loss of 70892505274.181824, Test MSE of 70892503285.324966
Epoch 27: training loss 78993343367.529
Test Loss of 66424524759.257919, Test MSE of 66424524902.901184
Epoch 28: training loss 74860157541.647
Test Loss of 64530069043.046036, Test MSE of 64530069626.640686
Epoch 29: training loss 72067537776.941
Test Loss of 63288742809.907936, Test MSE of 63288743241.009789
Epoch 30: training loss 68945676167.529
Test Loss of 59391271533.553551, Test MSE of 59391272361.377068
Epoch 31: training loss 65957077089.882
Test Loss of 56542595285.659035, Test MSE of 56542595692.828056
Epoch 32: training loss 63100342640.941
Test Loss of 52939157359.981491, Test MSE of 52939157125.460426
Epoch 33: training loss 60490969923.765
Test Loss of 51997714790.625031, Test MSE of 51997715311.002327
Epoch 34: training loss 56404320903.529
Test Loss of 49385342864.669907, Test MSE of 49385343440.168312
Epoch 35: training loss 54576354823.529
Test Loss of 46565605027.323616, Test MSE of 46565604954.770676
Epoch 36: training loss 51621618635.294
Test Loss of 44293900892.972473, Test MSE of 44293900317.411064
Epoch 37: training loss 49464177464.471
Test Loss of 43356794847.785332, Test MSE of 43356794460.679474
Epoch 38: training loss 46254183905.882
Test Loss of 41139266224.825356, Test MSE of 41139267036.728500
Epoch 39: training loss 44795509880.471
Test Loss of 36985827868.069397, Test MSE of 36985827969.453957
Epoch 40: training loss 42266563866.353
Test Loss of 37195093962.466805, Test MSE of 37195093968.305344
Epoch 41: training loss 40493009069.176
Test Loss of 34978719678.149437, Test MSE of 34978720420.202530
Epoch 42: training loss 37939118558.118
Test Loss of 34088983172.767059, Test MSE of 34088983654.564445
Epoch 43: training loss 36603947855.059
Test Loss of 32221320203.606754, Test MSE of 32221320708.321861
Epoch 44: training loss 34989661936.941
Test Loss of 32550096009.149200, Test MSE of 32550096520.704212
Epoch 45: training loss 33043428449.882
Test Loss of 29434561096.601433, Test MSE of 29434561246.691208
Epoch 46: training loss 31391594992.941
Test Loss of 28445269606.684246, Test MSE of 28445268925.610855
Epoch 47: training loss 30197577054.118
Test Loss of 28158862305.917187, Test MSE of 28158862970.143925
Epoch 48: training loss 28507737152.000
Test Loss of 26837269606.565811, Test MSE of 26837269666.264801
Epoch 49: training loss 27745706725.647
Test Loss of 25727648675.382835, Test MSE of 25727648496.183006
Epoch 50: training loss 26307581933.176
Test Loss of 22237677622.954430, Test MSE of 22237677457.508575
Epoch 51: training loss 25254273197.176
Test Loss of 22846770791.631737, Test MSE of 22846770827.360706
Epoch 52: training loss 24335518245.647
Test Loss of 23804744766.297478, Test MSE of 23804744705.035160
Epoch 53: training loss 23235454799.059
Test Loss of 24172091196.343281, Test MSE of 24172091247.331516
Epoch 54: training loss 22193634541.176
Test Loss of 21521747626.903538, Test MSE of 21521747647.733910
Epoch 55: training loss 21468988754.824
Test Loss of 20467162436.278511, Test MSE of 20467162369.312962
Epoch 56: training loss 20645756867.765
Test Loss of 19787625342.430721, Test MSE of 19787625146.551975
Epoch 57: training loss 19928650544.941
Test Loss of 18902805076.445061, Test MSE of 18902805106.236492
Epoch 58: training loss 19279926302.118
Test Loss of 19294217949.594265, Test MSE of 19294218197.403515
Epoch 59: training loss 18576326006.588
Test Loss of 19865437587.867683, Test MSE of 19865437479.103851
Epoch 60: training loss 18044681633.882
Test Loss of 20052151194.855423, Test MSE of 20052151255.214581
Epoch 61: training loss 17594949756.235
Test Loss of 18919596785.254684, Test MSE of 18919596818.952953
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 18919596818.952953, 'MSE - std': 0.0, 'R2 - mean': 0.852671046810173, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003773 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917437771.294
Test Loss of 424555891501.420288, Test MSE of 424555897543.952209
Epoch 2: training loss 427896691531.294
Test Loss of 424539758495.356018, Test MSE of 424539757995.692139
Epoch 3: training loss 427869137016.471
Test Loss of 424518223931.218140, Test MSE of 424518217531.031128
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886669583.059
Test Loss of 424524852117.644226, Test MSE of 424524861344.468506
Epoch 2: training loss 427877326125.176
Test Loss of 424525175082.933167, Test MSE of 424525177603.432251
Epoch 3: training loss 427876833882.353
Test Loss of 424524467557.914429, Test MSE of 424524466916.170349
Epoch 4: training loss 427876458014.118
Test Loss of 424523912192.236877, Test MSE of 424523910765.256653
Epoch 5: training loss 420980364950.588
Test Loss of 403023834505.919067, Test MSE of 403023837999.372986
Epoch 6: training loss 375462064730.353
Test Loss of 337512606227.068237, Test MSE of 337512610300.911316
Epoch 7: training loss 295918101564.235
Test Loss of 253233546409.837616, Test MSE of 253233549146.782318
Epoch 8: training loss 215992801280.000
Test Loss of 183851631035.425385, Test MSE of 183851629364.957489
Epoch 9: training loss 157711433035.294
Test Loss of 137564760197.122375, Test MSE of 137564755680.447144
Epoch 10: training loss 136815355482.353
Test Loss of 129684094293.807083, Test MSE of 129684094152.728409
Epoch 11: training loss 132702608353.882
Test Loss of 126587521837.420303, Test MSE of 126587522394.663742
Epoch 12: training loss 130201078663.529
Test Loss of 124016778318.404816, Test MSE of 124016781361.486359
Epoch 13: training loss 126892877552.941
Test Loss of 120371005629.971771, Test MSE of 120371006463.907501
Epoch 14: training loss 122434180758.588
Test Loss of 117170557647.145035, Test MSE of 117170559759.511047
Epoch 15: training loss 119192872176.941
Test Loss of 114377071599.655792, Test MSE of 114377073850.600052
Epoch 16: training loss 115176145016.471
Test Loss of 110857238489.626648, Test MSE of 110857239448.318344
Epoch 17: training loss 110873214855.529
Test Loss of 107188747770.670364, Test MSE of 107188747921.653397
Epoch 18: training loss 106294783879.529
Test Loss of 103512819376.114731, Test MSE of 103512821694.362244
Epoch 19: training loss 102840962168.471
Test Loss of 98967536167.439285, Test MSE of 98967538389.023132
Epoch 20: training loss 99040262565.647
Test Loss of 94098542095.515152, Test MSE of 94098541505.302567
Epoch 21: training loss 95077118855.529
Test Loss of 92180822678.769379, Test MSE of 92180820855.601639
Epoch 22: training loss 91169055834.353
Test Loss of 87648520156.469116, Test MSE of 87648521120.559616
Epoch 23: training loss 87243820272.941
Test Loss of 82767153182.793427, Test MSE of 82767153293.682999
Epoch 24: training loss 83207104256.000
Test Loss of 77467940580.226700, Test MSE of 77467940820.873489
Epoch 25: training loss 79900471040.000
Test Loss of 74957741870.604675, Test MSE of 74957740677.087036
Epoch 26: training loss 76116779309.176
Test Loss of 74257864912.921585, Test MSE of 74257862499.257690
Epoch 27: training loss 72863732208.941
Test Loss of 67678322359.220909, Test MSE of 67678320844.654449
Epoch 28: training loss 68794077891.765
Test Loss of 64616124951.331947, Test MSE of 64616124979.499901
Epoch 29: training loss 65897525308.235
Test Loss of 63434276430.996994, Test MSE of 63434276664.674576
Epoch 30: training loss 62117600015.059
Test Loss of 59874100724.511681, Test MSE of 59874100219.197906
Epoch 31: training loss 59011595128.471
Test Loss of 55338577389.405502, Test MSE of 55338577166.298180
Epoch 32: training loss 56695021086.118
Test Loss of 52546163305.289848, Test MSE of 52546163246.073997
Epoch 33: training loss 53009436626.824
Test Loss of 51276276645.751564, Test MSE of 51276275620.609749
Epoch 34: training loss 50422520485.647
Test Loss of 47043052616.719872, Test MSE of 47043053125.300018
Epoch 35: training loss 46369718844.235
Test Loss of 43665602547.208885, Test MSE of 43665602481.979919
Epoch 36: training loss 44960174260.706
Test Loss of 43859805549.968079, Test MSE of 43859805672.493279
Epoch 37: training loss 42671479040.000
Test Loss of 43216452172.865143, Test MSE of 43216451661.752472
Epoch 38: training loss 40089106793.412
Test Loss of 37893993580.250755, Test MSE of 37893993912.838943
Epoch 39: training loss 38601013632.000
Test Loss of 37567633220.870689, Test MSE of 37567634234.405357
Epoch 40: training loss 35670006234.353
Test Loss of 37094784074.141106, Test MSE of 37094784771.660149
Epoch 41: training loss 34307803700.706
Test Loss of 34400646534.602821, Test MSE of 34400646341.345306
Epoch 42: training loss 31844421308.235
Test Loss of 32710809215.318993, Test MSE of 32710808306.873814
Epoch 43: training loss 30323438674.824
Test Loss of 30695111121.691418, Test MSE of 30695110611.825897
Epoch 44: training loss 28292633088.000
Test Loss of 30043343479.975945, Test MSE of 30043342925.472389
Epoch 45: training loss 26918723320.471
Test Loss of 31074982632.727272, Test MSE of 31074982610.611206
Epoch 46: training loss 25839391051.294
Test Loss of 29930048233.201019, Test MSE of 29930047789.543121
Epoch 47: training loss 24659867753.412
Test Loss of 28152780595.815868, Test MSE of 28152780465.466175
Epoch 48: training loss 23351794846.118
Test Loss of 27635958103.938931, Test MSE of 27635957862.623585
Epoch 49: training loss 22208315715.765
Test Loss of 26764548286.445522, Test MSE of 26764548599.206436
Epoch 50: training loss 21332607378.824
Test Loss of 27519366908.861439, Test MSE of 27519367122.341805
Epoch 51: training loss 20543992918.588
Test Loss of 26480848796.987278, Test MSE of 26480849384.212807
Epoch 52: training loss 19266934241.882
Test Loss of 25388403844.411751, Test MSE of 25388403287.791031
Epoch 53: training loss 18417384801.882
Test Loss of 26835810136.767986, Test MSE of 26835810416.669052
Epoch 54: training loss 17663085688.471
Test Loss of 23487720111.877861, Test MSE of 23487720469.819447
Epoch 55: training loss 17019950354.824
Test Loss of 26155412536.612537, Test MSE of 26155412542.795780
Epoch 56: training loss 16736136395.294
Test Loss of 25682900903.409668, Test MSE of 25682901426.270153
Epoch 57: training loss 16015710878.118
Test Loss of 23037631737.900532, Test MSE of 23037631782.824722
Epoch 58: training loss 15529154725.647
Test Loss of 23422894978.457554, Test MSE of 23422895182.670639
Epoch 59: training loss 15339312892.235
Test Loss of 23601289426.816563, Test MSE of 23601289757.232124
Epoch 60: training loss 14674895141.647
Test Loss of 24097734283.399490, Test MSE of 24097733881.772324
Epoch 61: training loss 14092909296.941
Test Loss of 23269924177.069626, Test MSE of 23269924366.345703
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21094760592.64933, 'MSE - std': 2175163773.696375, 'R2 - mean': 0.8432696181987401, 'R2 - std': 0.009401428611432894} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005486 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421928543051.294
Test Loss of 447259904021.318542, Test MSE of 447259904755.287964
Epoch 2: training loss 421908510238.118
Test Loss of 447242007720.890137, Test MSE of 447242006241.269714
Epoch 3: training loss 421881450736.941
Test Loss of 447218389155.915771, Test MSE of 447218383798.609436
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421901918689.882
Test Loss of 447226669598.438110, Test MSE of 447226669052.211304
Epoch 2: training loss 421890455672.471
Test Loss of 447227536241.165833, Test MSE of 447227538523.583496
Epoch 3: training loss 421889989089.882
Test Loss of 447227550264.020386, Test MSE of 447227550044.014465
Epoch 4: training loss 421889635870.118
Test Loss of 447227012028.965088, Test MSE of 447227014802.730896
Epoch 5: training loss 414910063676.235
Test Loss of 424794974432.081421, Test MSE of 424794985843.496765
Epoch 6: training loss 369293363440.941
Test Loss of 356405409937.439758, Test MSE of 356405409485.893860
Epoch 7: training loss 290349921340.235
Test Loss of 270830178452.755951, Test MSE of 270830177752.561584
Epoch 8: training loss 212262081897.412
Test Loss of 199744404855.916718, Test MSE of 199744402715.780548
Epoch 9: training loss 153673453417.412
Test Loss of 148349121917.127930, Test MSE of 148349120512.694702
Epoch 10: training loss 135216714360.471
Test Loss of 140944622470.010651, Test MSE of 140944622152.378662
Epoch 11: training loss 130840614339.765
Test Loss of 137539333161.452698, Test MSE of 137539331884.010742
Epoch 12: training loss 128998958411.294
Test Loss of 134865872502.317841, Test MSE of 134865872833.268616
Epoch 13: training loss 126243658450.824
Test Loss of 131144637408.495956, Test MSE of 131144635742.715500
Epoch 14: training loss 121472035599.059
Test Loss of 127579290897.587784, Test MSE of 127579292606.172348
Epoch 15: training loss 118049575303.529
Test Loss of 124025099730.402039, Test MSE of 124025099191.739944
Epoch 16: training loss 113834403599.059
Test Loss of 121127424012.317368, Test MSE of 121127424201.877686
Epoch 17: training loss 110434173138.824
Test Loss of 116405789791.933380, Test MSE of 116405789727.870895
Epoch 18: training loss 106148562883.765
Test Loss of 112527472353.384216, Test MSE of 112527472899.575165
Epoch 19: training loss 101827047152.941
Test Loss of 109986846306.420547, Test MSE of 109986844214.954453
Epoch 20: training loss 100065181093.647
Test Loss of 104854583445.229706, Test MSE of 104854584071.457001
Epoch 21: training loss 94653433524.706
Test Loss of 102784422681.049271, Test MSE of 102784421658.579910
Epoch 22: training loss 91488067102.118
Test Loss of 98557121729.524872, Test MSE of 98557122590.486008
Epoch 23: training loss 88581630554.353
Test Loss of 93523989146.085587, Test MSE of 93523990678.771194
Epoch 24: training loss 83879532122.353
Test Loss of 87931535464.223923, Test MSE of 87931536151.168106
Epoch 25: training loss 81128900909.176
Test Loss of 87478111860.896606, Test MSE of 87478114739.071274
Epoch 26: training loss 76958187730.824
Test Loss of 83000050067.630814, Test MSE of 83000051910.380920
Epoch 27: training loss 74636220295.529
Test Loss of 77711993313.798752, Test MSE of 77711995173.799683
Epoch 28: training loss 71245302452.706
Test Loss of 76270764725.562805, Test MSE of 76270766509.672333
Epoch 29: training loss 68609907832.471
Test Loss of 70663960348.128616, Test MSE of 70663960921.045822
Epoch 30: training loss 64021119638.588
Test Loss of 69436939133.956970, Test MSE of 69436939882.727097
Epoch 31: training loss 61834886640.941
Test Loss of 63656774127.063614, Test MSE of 63656774857.573776
Epoch 32: training loss 58361642812.235
Test Loss of 61552379660.021278, Test MSE of 61552379177.693443
Epoch 33: training loss 54827931813.647
Test Loss of 56065930048.843857, Test MSE of 56065930302.784264
Epoch 34: training loss 52786439920.941
Test Loss of 58355760651.725189, Test MSE of 58355760611.002113
Epoch 35: training loss 50204204137.412
Test Loss of 51456368358.595421, Test MSE of 51456368534.555412
Epoch 36: training loss 48040738936.471
Test Loss of 53558007955.334724, Test MSE of 53558007742.700356
Epoch 37: training loss 45391713069.176
Test Loss of 50304384175.048805, Test MSE of 50304384136.695816
Epoch 38: training loss 42956585336.471
Test Loss of 48116991118.360397, Test MSE of 48116991808.970039
Epoch 39: training loss 40264747836.235
Test Loss of 43022152588.643074, Test MSE of 43022152868.438698
Epoch 40: training loss 38453326960.941
Test Loss of 42933280606.926674, Test MSE of 42933281104.572708
Epoch 41: training loss 36773946518.588
Test Loss of 39967952260.234100, Test MSE of 39967952450.759399
Epoch 42: training loss 35007843945.412
Test Loss of 39150934453.266716, Test MSE of 39150934779.796570
Epoch 43: training loss 33313615232.000
Test Loss of 35225829730.361320, Test MSE of 35225830546.282082
Epoch 44: training loss 32028658379.294
Test Loss of 36210858324.622719, Test MSE of 36210858353.994217
Epoch 45: training loss 29943333590.588
Test Loss of 36894736857.745087, Test MSE of 36894736692.881218
Epoch 46: training loss 28867428992.000
Test Loss of 34802974583.561417, Test MSE of 34802974738.477600
Epoch 47: training loss 27456108352.000
Test Loss of 30135469794.805458, Test MSE of 30135469787.389832
Epoch 48: training loss 26101199461.647
Test Loss of 30160230969.915337, Test MSE of 30160230797.529102
Epoch 49: training loss 24844035090.824
Test Loss of 27781942053.366642, Test MSE of 27781941823.603901
Epoch 50: training loss 23702583898.353
Test Loss of 28773910830.249363, Test MSE of 28773910970.761955
Epoch 51: training loss 22838282857.412
Test Loss of 26850616790.665741, Test MSE of 26850616198.185383
Epoch 52: training loss 21980775548.235
Test Loss of 25387083442.483459, Test MSE of 25387083444.284988
Epoch 53: training loss 21037627915.294
Test Loss of 24099856286.882259, Test MSE of 24099856459.507538
Epoch 54: training loss 20212967536.941
Test Loss of 23473345199.877861, Test MSE of 23473345438.628334
Epoch 55: training loss 19841069782.588
Test Loss of 24982449855.748322, Test MSE of 24982450230.061607
Epoch 56: training loss 18875959868.235
Test Loss of 23707228829.875549, Test MSE of 23707228996.238644
Epoch 57: training loss 18295602612.706
Test Loss of 23646781851.210732, Test MSE of 23646781674.920109
Epoch 58: training loss 17700962518.588
Test Loss of 23844499313.876476, Test MSE of 23844499679.929157
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 22011340288.40927, 'MSE - std': 2198740921.290974, 'R2 - mean': 0.8426026585420607, 'R2 - std': 0.007733966877214762} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006978 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430109707806.118
Test Loss of 410764252256.192505, Test MSE of 410764247160.730713
Epoch 2: training loss 430088354153.412
Test Loss of 410745870954.854248, Test MSE of 410745869828.579834
Epoch 3: training loss 430060695552.000
Test Loss of 410722244180.583069, Test MSE of 410722239076.240173
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430082145340.235
Test Loss of 410728239951.252197, Test MSE of 410728237597.589050
Epoch 2: training loss 430065648579.765
Test Loss of 410727553142.463684, Test MSE of 410727556377.808167
Epoch 3: training loss 430065168384.000
Test Loss of 410727128401.858398, Test MSE of 410727130063.143066
Epoch 4: training loss 430064844077.176
Test Loss of 410725983220.153625, Test MSE of 410725989177.021667
Epoch 5: training loss 423234850575.059
Test Loss of 389295117393.029175, Test MSE of 389295123152.969910
Epoch 6: training loss 377754302223.059
Test Loss of 323046648455.285522, Test MSE of 323046652283.822449
Epoch 7: training loss 298903718128.941
Test Loss of 237453344555.713104, Test MSE of 237453344988.418793
Epoch 8: training loss 220266910780.235
Test Loss of 167556399960.729279, Test MSE of 167556403208.994049
Epoch 9: training loss 161530143503.059
Test Loss of 119925649955.776031, Test MSE of 119925649604.908630
Epoch 10: training loss 141190417709.176
Test Loss of 112618918727.670517, Test MSE of 112618916605.990936
Epoch 11: training loss 138194410285.176
Test Loss of 109606287588.397964, Test MSE of 109606290216.780304
Epoch 12: training loss 134864931508.706
Test Loss of 106632192705.095795, Test MSE of 106632193267.330246
Epoch 13: training loss 130570696975.059
Test Loss of 103661962417.695511, Test MSE of 103661962856.316147
Epoch 14: training loss 125351082224.941
Test Loss of 100197975331.420639, Test MSE of 100197976627.127533
Epoch 15: training loss 124357553633.882
Test Loss of 98471636986.313751, Test MSE of 98471636611.748596
Epoch 16: training loss 119039482337.882
Test Loss of 94137180921.958359, Test MSE of 94137180989.760361
Epoch 17: training loss 115691098895.059
Test Loss of 91145331582.637665, Test MSE of 91145332690.748520
Epoch 18: training loss 111608128421.647
Test Loss of 86061961252.012955, Test MSE of 86061959746.870712
Epoch 19: training loss 106787862196.706
Test Loss of 83974297737.417862, Test MSE of 83974295474.991196
Epoch 20: training loss 103283378176.000
Test Loss of 81915334862.600647, Test MSE of 81915334732.131622
Epoch 21: training loss 98784018025.412
Test Loss of 76603286421.382690, Test MSE of 76603285144.524200
Epoch 22: training loss 94724467350.588
Test Loss of 76125293327.755676, Test MSE of 76125293798.206726
Epoch 23: training loss 91189156276.706
Test Loss of 71862047703.722351, Test MSE of 71862048970.740829
Epoch 24: training loss 88440738966.588
Test Loss of 68973148358.071259, Test MSE of 68973149564.461151
Epoch 25: training loss 83757140931.765
Test Loss of 66935003265.836189, Test MSE of 66935002721.257690
Epoch 26: training loss 80209163083.294
Test Loss of 63881162454.419250, Test MSE of 63881162677.404945
Epoch 27: training loss 75672849904.941
Test Loss of 59451870778.994911, Test MSE of 59451870968.410240
Epoch 28: training loss 73142875361.882
Test Loss of 55544373953.095787, Test MSE of 55544375274.213051
Epoch 29: training loss 69092487469.176
Test Loss of 54314037206.300789, Test MSE of 54314036940.983871
Epoch 30: training loss 65744049061.647
Test Loss of 51386450503.788986, Test MSE of 51386450500.430679
Epoch 31: training loss 62834048225.882
Test Loss of 50124481903.237389, Test MSE of 50124481720.817085
Epoch 32: training loss 59908147328.000
Test Loss of 46900335247.814903, Test MSE of 46900335703.410156
Epoch 33: training loss 56874725376.000
Test Loss of 45126897921.776955, Test MSE of 45126898045.907860
Epoch 34: training loss 54215198704.941
Test Loss of 43755427092.257286, Test MSE of 43755426630.322395
Epoch 35: training loss 51467361332.706
Test Loss of 42954712194.310043, Test MSE of 42954712158.192703
Epoch 36: training loss 49247837696.000
Test Loss of 41109835954.169365, Test MSE of 41109835895.180267
Epoch 37: training loss 46604128655.059
Test Loss of 34397124523.653862, Test MSE of 34397124626.717010
Epoch 38: training loss 44001503698.824
Test Loss of 35821241853.393799, Test MSE of 35821241478.733574
Epoch 39: training loss 41922085850.353
Test Loss of 33481560970.010181, Test MSE of 33481561131.670753
Epoch 40: training loss 39462441517.176
Test Loss of 32654965726.356316, Test MSE of 32654965961.204491
Epoch 41: training loss 37906129686.588
Test Loss of 30704544520.173992, Test MSE of 30704544361.308533
Epoch 42: training loss 35949242571.294
Test Loss of 28698365811.739010, Test MSE of 28698366128.193291
Epoch 43: training loss 33324755282.824
Test Loss of 29969217893.286442, Test MSE of 29969217672.283775
Epoch 44: training loss 32038993927.529
Test Loss of 26269974469.242016, Test MSE of 26269975193.280594
Epoch 45: training loss 30749548160.000
Test Loss of 27117020995.405830, Test MSE of 27117020720.551903
Epoch 46: training loss 29079406245.647
Test Loss of 26553275865.380840, Test MSE of 26553275733.857243
Epoch 47: training loss 28171787915.294
Test Loss of 24178261967.666821, Test MSE of 24178262010.055031
Epoch 48: training loss 26416237741.176
Test Loss of 24679379822.526608, Test MSE of 24679380350.298096
Epoch 49: training loss 25533969524.706
Test Loss of 22991468254.000927, Test MSE of 22991468489.640503
Epoch 50: training loss 24309788897.882
Test Loss of 21873146379.135586, Test MSE of 21873146577.786552
Epoch 51: training loss 23120369893.647
Test Loss of 23352355511.618694, Test MSE of 23352355169.173710
Epoch 52: training loss 22323366640.941
Test Loss of 21449722446.422951, Test MSE of 21449722107.808479
Epoch 53: training loss 21358277605.647
Test Loss of 19585123351.218880, Test MSE of 19585123013.259109
Epoch 54: training loss 20839748205.176
Test Loss of 19935763474.480331, Test MSE of 19935763505.126549
Epoch 55: training loss 20009581767.529
Test Loss of 20074492603.409534, Test MSE of 20074492291.132114
Epoch 56: training loss 19083999749.647
Test Loss of 20543076334.941231, Test MSE of 20543076094.971439
Epoch 57: training loss 18491917312.000
Test Loss of 20608065267.798241, Test MSE of 20608065375.469906
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21660521560.17443, 'MSE - std': 1998766511.99197, 'R2 - mean': 0.8394297179913272, 'R2 - std': 0.00866391008487857} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 4, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005480 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043661312.000
Test Loss of 431611991568.347961, Test MSE of 431611996555.886658
Epoch 2: training loss 424023806915.765
Test Loss of 431591339661.445618, Test MSE of 431591330668.712891
Epoch 3: training loss 423995860028.235
Test Loss of 431562550782.341492, Test MSE of 431562552981.583252
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424009264790.588
Test Loss of 431566784556.542358, Test MSE of 431566786839.889282
Epoch 2: training loss 423997239175.529
Test Loss of 431569132904.603455, Test MSE of 431569133529.243774
Epoch 3: training loss 423996541168.941
Test Loss of 431569178091.387329, Test MSE of 431569183972.920288
Epoch 4: training loss 423996097355.294
Test Loss of 431568177916.801453, Test MSE of 431568180975.894531
Epoch 5: training loss 417377672011.294
Test Loss of 409744648839.285522, Test MSE of 409744655753.570801
Epoch 6: training loss 372416821850.353
Test Loss of 342701940216.655273, Test MSE of 342701941959.394958
Epoch 7: training loss 294277229748.706
Test Loss of 255667044549.123566, Test MSE of 255667042535.920410
Epoch 8: training loss 216141007209.412
Test Loss of 183774228466.732056, Test MSE of 183774230890.864227
Epoch 9: training loss 156481660160.000
Test Loss of 132684760313.721420, Test MSE of 132684759094.113022
Epoch 10: training loss 137779507350.588
Test Loss of 124195497971.679779, Test MSE of 124195498148.634979
Epoch 11: training loss 133000929340.235
Test Loss of 120547136149.501160, Test MSE of 120547136011.705383
Epoch 12: training loss 131112967589.647
Test Loss of 117256201609.773254, Test MSE of 117256202302.735596
Epoch 13: training loss 127829339497.412
Test Loss of 113235723854.896805, Test MSE of 113235723940.331848
Epoch 14: training loss 124160336624.941
Test Loss of 110151649419.313278, Test MSE of 110151650778.561218
Epoch 15: training loss 120629497404.235
Test Loss of 105775835226.032394, Test MSE of 105775833803.070175
Epoch 16: training loss 116353376918.588
Test Loss of 101178907220.109207, Test MSE of 101178906138.028976
Epoch 17: training loss 113908424658.824
Test Loss of 100141529077.575195, Test MSE of 100141530183.581879
Epoch 18: training loss 108914254516.706
Test Loss of 97433511775.837112, Test MSE of 97433512109.796066
Epoch 19: training loss 104959754571.294
Test Loss of 92334972969.699219, Test MSE of 92334973376.186340
Epoch 20: training loss 101352859376.941
Test Loss of 89350040824.773712, Test MSE of 89350041360.140137
Epoch 21: training loss 98224556182.588
Test Loss of 86728449346.695053, Test MSE of 86728448755.188828
Epoch 22: training loss 93942922142.118
Test Loss of 80865285576.322067, Test MSE of 80865286177.888199
Epoch 23: training loss 89775571516.235
Test Loss of 76209042753.747345, Test MSE of 76209042829.304565
Epoch 24: training loss 86232304564.706
Test Loss of 74921135172.708939, Test MSE of 74921136122.992050
Epoch 25: training loss 82846998799.059
Test Loss of 69927400170.794998, Test MSE of 69927401074.620224
Epoch 26: training loss 79848472591.059
Test Loss of 68408945488.199905, Test MSE of 68408945414.482445
Epoch 27: training loss 75412957891.765
Test Loss of 63783619677.349373, Test MSE of 63783621080.671417
Epoch 28: training loss 71967169987.765
Test Loss of 60991715395.287369, Test MSE of 60991714793.812561
Epoch 29: training loss 68986124912.941
Test Loss of 57851002921.699211, Test MSE of 57851002845.194092
Epoch 30: training loss 65857975009.882
Test Loss of 56382926279.374367, Test MSE of 56382926870.041245
Epoch 31: training loss 62613687092.706
Test Loss of 52866313809.739937, Test MSE of 52866314305.568108
Epoch 32: training loss 59403776549.647
Test Loss of 50511408871.478020, Test MSE of 50511409752.018829
Epoch 33: training loss 56881208982.588
Test Loss of 50185880952.714485, Test MSE of 50185879800.550262
Epoch 34: training loss 53519296022.588
Test Loss of 44532553302.478485, Test MSE of 44532552998.919983
Epoch 35: training loss 51190844137.412
Test Loss of 43338286979.850067, Test MSE of 43338287359.734642
Epoch 36: training loss 49099351009.882
Test Loss of 38773778097.458588, Test MSE of 38773778197.019608
Epoch 37: training loss 46208135363.765
Test Loss of 38738960088.314667, Test MSE of 38738959705.516068
Epoch 38: training loss 44750950226.824
Test Loss of 35731890725.671448, Test MSE of 35731890622.193764
Epoch 39: training loss 41749752508.235
Test Loss of 31059205631.289219, Test MSE of 31059205814.872448
Epoch 40: training loss 39949460325.647
Test Loss of 33095394034.376678, Test MSE of 33095394493.745953
Epoch 41: training loss 38330593739.294
Test Loss of 29740719346.139751, Test MSE of 29740719555.515125
Epoch 42: training loss 36143932144.941
Test Loss of 27109949489.280888, Test MSE of 27109949611.353466
Epoch 43: training loss 33866117368.471
Test Loss of 31320625258.143452, Test MSE of 31320624943.564411
Epoch 44: training loss 32425302625.882
Test Loss of 24307795962.313744, Test MSE of 24307795859.667145
Epoch 45: training loss 31466138179.765
Test Loss of 29212440992.992134, Test MSE of 29212441586.509411
Epoch 46: training loss 29563319439.059
Test Loss of 28497678686.652477, Test MSE of 28497678626.558052
Epoch 47: training loss 28100326610.824
Test Loss of 22877338137.351227, Test MSE of 22877338341.771297
Epoch 48: training loss 26728042627.765
Test Loss of 24971687783.418789, Test MSE of 24971687673.237267
Epoch 49: training loss 26142458398.118
Test Loss of 23056044524.808884, Test MSE of 23056044678.563984
Epoch 50: training loss 25077674480.941
Test Loss of 23647296033.880611, Test MSE of 23647295975.240742
Epoch 51: training loss 23912090733.176
Test Loss of 21753379087.992596, Test MSE of 21753378970.516846
Epoch 52: training loss 22864703216.941
Test Loss of 23039842207.333641, Test MSE of 23039842149.585350
Epoch 53: training loss 22168625528.471
Test Loss of 19704527857.310505, Test MSE of 19704527822.423920
Epoch 54: training loss 21299205952.000
Test Loss of 21833525607.181862, Test MSE of 21833525442.517326
Epoch 55: training loss 20370415028.706
Test Loss of 21292497821.912079, Test MSE of 21292497731.652348
Epoch 56: training loss 20154775683.765
Test Loss of 21066219962.580288, Test MSE of 21066219760.498112
Epoch 57: training loss 19227832154.353
Test Loss of 19552196063.067097, Test MSE of 19552195929.448200
Epoch 58: training loss 18394223981.176
Test Loss of 19072356678.012032, Test MSE of 19072357054.438042
Epoch 59: training loss 18069535804.235
Test Loss of 18798009474.310043, Test MSE of 18798009557.723579
Epoch 60: training loss 17584078452.706
Test Loss of 20343172464.185101, Test MSE of 20343172572.349766
Epoch 61: training loss 17005623612.235
Test Loss of 22632569636.605274, Test MSE of 22632569716.035645
Epoch 62: training loss 16646957206.588
Test Loss of 18321756486.012032, Test MSE of 18321756336.813469
Epoch 63: training loss 15996998994.824
Test Loss of 19688700733.245720, Test MSE of 19688701033.031563
Epoch 64: training loss 15596484958.118
Test Loss of 18999842847.274410, Test MSE of 18999843310.836258
Epoch 65: training loss 15429823661.176
Test Loss of 20599602081.702915, Test MSE of 20599602300.659264
Epoch 66: training loss 14857176640.000
Test Loss of 19370756584.070339, Test MSE of 19370756416.497070
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 21202568531.438957, 'MSE - std': 2008715500.433151, 'R2 - mean': 0.8426114687728455, 'R2 - std': 0.010027204122279634} 
 

Saving model.....
Results After CV: {'MSE - mean': 21202568531.438957, 'MSE - std': 2008715500.433151, 'R2 - mean': 0.8426114687728455, 'R2 - std': 0.010027204122279634}
Train time: 94.79461892600084
Inference time: 0.07052256079914514
Finished cross validation
Hyperparam was saved!!! Hurrah!!!
Trial 99 finished with value: 21202568531.438957 and parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 4, 'loss_dr': 0.9}. Best is trial 6 with value: 20640313426.4125.
Best parameters: {'n_trees': 100, 'maxleaf': 64, 'loss_de': 3, 'loss_dr': 0.9}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003707 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2528
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 90
[LightGBM] [Info] Start training from score 540549.777501
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.22039e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427524936402.824
Test Loss of 418110811787.636353, Test MSE of 418110820676.871704
Epoch 2: training loss 427504215943.529
Test Loss of 418092350472.290527, Test MSE of 418092351564.303894
Epoch 3: training loss 427477178368.000
Test Loss of 418068545888.466370, Test MSE of 418068546719.232910
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427491931196.235
Test Loss of 418072669062.484375, Test MSE of 418072666167.523804
Epoch 2: training loss 427480849829.647
Test Loss of 418074115243.258850, Test MSE of 418074112761.549377
Epoch 3: training loss 427480436013.176
Test Loss of 418073996367.352295, Test MSE of 418073991811.115540
Epoch 4: training loss 421905702550.588
Test Loss of 400939761561.671082, Test MSE of 400939753732.159119
Epoch 5: training loss 385115799190.588
Test Loss of 345990036185.093689, Test MSE of 345990030000.682007
Epoch 6: training loss 317539363900.235
Test Loss of 268932368833.584076, Test MSE of 268932369369.864105
Epoch 7: training loss 225412210627.765
Test Loss of 171269048424.697662, Test MSE of 171269049489.238159
Epoch 8: training loss 160654984643.765
Test Loss of 131323196759.228317, Test MSE of 131323195741.495377
Epoch 9: training loss 141817133718.588
Test Loss of 120400849866.703674, Test MSE of 120400850014.890610
Epoch 10: training loss 136021638957.176
Test Loss of 116217522168.893829, Test MSE of 116217521279.446655
Epoch 11: training loss 132847537152.000
Test Loss of 113081744075.828827, Test MSE of 113081745069.086090
Epoch 12: training loss 129379800380.235
Test Loss of 110412114795.954666, Test MSE of 110412112811.383148
Epoch 13: training loss 127096891346.824
Test Loss of 105682433634.420547, Test MSE of 105682434286.394348
Epoch 14: training loss 122539787580.235
Test Loss of 104866836913.003006, Test MSE of 104866836249.984497
Epoch 15: training loss 119358940656.941
Test Loss of 100710375471.374512, Test MSE of 100710374124.469849
Epoch 16: training loss 114876911826.824
Test Loss of 97318184297.230621, Test MSE of 97318184384.549393
Epoch 17: training loss 110451517500.235
Test Loss of 93626815298.738846, Test MSE of 93626815132.408340
Epoch 18: training loss 106990562364.235
Test Loss of 90697930124.287766, Test MSE of 90697930460.275940
Epoch 19: training loss 103415613620.706
Test Loss of 87687678702.885956, Test MSE of 87687679083.841385
Epoch 20: training loss 99107152971.294
Test Loss of 84641241218.279892, Test MSE of 84641242567.366302
Epoch 21: training loss 95372225626.353
Test Loss of 81931248722.668518, Test MSE of 81931249139.576523
Epoch 22: training loss 92281890891.294
Test Loss of 75456870473.430481, Test MSE of 75456870935.902695
Epoch 23: training loss 87569519811.765
Test Loss of 74681766984.246124, Test MSE of 74681766701.432281
Epoch 24: training loss 84228620724.706
Test Loss of 71357867467.769608, Test MSE of 71357868376.774002
Epoch 25: training loss 80797050021.647
Test Loss of 67771134491.121902, Test MSE of 67771134606.071915
Epoch 26: training loss 77101474153.412
Test Loss of 65382188577.043716, Test MSE of 65382190287.234879
Epoch 27: training loss 74357233242.353
Test Loss of 62761123398.469582, Test MSE of 62761123048.676468
Epoch 28: training loss 70056985577.412
Test Loss of 60649502079.022903, Test MSE of 60649500481.001457
Epoch 29: training loss 67461124336.941
Test Loss of 58568018905.863518, Test MSE of 58568017796.790611
Epoch 30: training loss 64267607024.941
Test Loss of 56680860362.881332, Test MSE of 56680860988.715248
Epoch 31: training loss 60644676604.235
Test Loss of 51753738216.312744, Test MSE of 51753738392.688499
Epoch 32: training loss 58012977430.588
Test Loss of 48551851748.226692, Test MSE of 48551851562.542236
Epoch 33: training loss 55042037940.706
Test Loss of 46436455987.046036, Test MSE of 46436456876.475052
Epoch 34: training loss 52771114089.412
Test Loss of 44623563051.880638, Test MSE of 44623563572.682899
Epoch 35: training loss 50006611817.412
Test Loss of 41138647171.701134, Test MSE of 41138647812.753181
Epoch 36: training loss 47901282413.176
Test Loss of 39708269311.940781, Test MSE of 39708270048.718613
Epoch 37: training loss 44850268280.471
Test Loss of 38183317604.433960, Test MSE of 38183317831.006218
Epoch 38: training loss 42915650518.588
Test Loss of 36406157683.416145, Test MSE of 36406156915.505669
Epoch 39: training loss 40988454377.412
Test Loss of 34818836421.729355, Test MSE of 34818836506.790092
Epoch 40: training loss 38964684540.235
Test Loss of 31612853906.268795, Test MSE of 31612853858.378910
Epoch 41: training loss 37150928030.118
Test Loss of 31562486068.881794, Test MSE of 31562486679.400501
Epoch 42: training loss 35739295454.118
Test Loss of 26296422345.993061, Test MSE of 26296422362.084354
Epoch 43: training loss 33672287939.765
Test Loss of 30083032817.491558, Test MSE of 30083032808.421856
Epoch 44: training loss 32295482597.647
Test Loss of 27571046463.955585, Test MSE of 27571046560.786491
Epoch 45: training loss 30755775608.471
Test Loss of 27872912585.104790, Test MSE of 27872912413.488678
Epoch 46: training loss 29141862264.471
Test Loss of 27619734099.971317, Test MSE of 27619734425.032646
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 27619734425.032646, 'MSE - std': 0.0, 'R2 - mean': 0.7849221313138806, 'R2 - std': 0.0} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003832 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2523
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 541764.934644
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[93]	valid_0's l2: 1.89395e+10
Model Interpreting...
[(19,), (19,), (19,), (18,), (18,)]

Train embedding model...
Epoch 1: training loss 427917571252.706
Test Loss of 424556857266.779541, Test MSE of 424556850441.360840
Epoch 2: training loss 427896507452.235
Test Loss of 424539854532.959534, Test MSE of 424539849423.382080
Epoch 3: training loss 427868664771.765
Test Loss of 424517462869.214905, Test MSE of 424517462523.532043
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 427886450567.529
Test Loss of 424525899117.020569, Test MSE of 424525894086.787231
Epoch 2: training loss 427876706665.412
Test Loss of 424526858401.547058, Test MSE of 424526861130.874878
Epoch 3: training loss 427876188400.941
Test Loss of 424525947348.060120, Test MSE of 424525947817.119690
Epoch 4: training loss 422312277654.588
Test Loss of 407394590244.359924, Test MSE of 407394584715.153625
Epoch 5: training loss 385387907674.353
Test Loss of 353386331136.710632, Test MSE of 353386332553.146545
Epoch 6: training loss 316613411779.765
Test Loss of 276926130718.438110, Test MSE of 276926139007.389221
Epoch 7: training loss 224474117421.176
Test Loss of 180213592874.104095, Test MSE of 180213592477.838806
Epoch 8: training loss 159251012186.353
Test Loss of 141440241740.746704, Test MSE of 141440245111.739349
Epoch 9: training loss 139183611904.000
Test Loss of 132196278891.658569, Test MSE of 132196280256.506851
Epoch 10: training loss 133830539113.412
Test Loss of 127696617777.091827, Test MSE of 127696619209.408905
Epoch 11: training loss 130737323489.882
Test Loss of 124727106086.965530, Test MSE of 124727105881.599289
Epoch 12: training loss 126418219339.294
Test Loss of 121942138573.013184, Test MSE of 121942137577.416809
Epoch 13: training loss 124281213108.706
Test Loss of 118546067254.184601, Test MSE of 118546068320.801468
Epoch 14: training loss 120340482590.118
Test Loss of 114305710856.468201, Test MSE of 114305712546.334686
Epoch 15: training loss 116742259169.882
Test Loss of 111050327475.134857, Test MSE of 111050325240.331543
Epoch 16: training loss 111586796634.353
Test Loss of 107800782011.839920, Test MSE of 107800782869.760330
Epoch 17: training loss 107564406784.000
Test Loss of 103377867845.403656, Test MSE of 103377871808.798157
Epoch 18: training loss 104789669376.000
Test Loss of 100731830889.763596, Test MSE of 100731829404.284439
Epoch 19: training loss 100370765101.176
Test Loss of 97556395182.575058, Test MSE of 97556393124.987381
Epoch 20: training loss 95650397876.706
Test Loss of 90849285489.758041, Test MSE of 90849289316.356964
Epoch 21: training loss 92278555587.765
Test Loss of 89446779175.143188, Test MSE of 89446780963.049728
Epoch 22: training loss 88415018992.941
Test Loss of 87534487733.444366, Test MSE of 87534487535.064331
Epoch 23: training loss 84053980521.412
Test Loss of 80299761776.751328, Test MSE of 80299761558.845764
Epoch 24: training loss 80922496978.824
Test Loss of 79123553909.844086, Test MSE of 79123554917.912827
Epoch 25: training loss 78671240252.235
Test Loss of 73409166384.085129, Test MSE of 73409167437.008743
Epoch 26: training loss 74014001242.353
Test Loss of 70493383615.333801, Test MSE of 70493381585.080261
Epoch 27: training loss 70677273163.294
Test Loss of 67062237867.614159, Test MSE of 67062235829.628502
Epoch 28: training loss 67129553784.471
Test Loss of 65847573025.280594, Test MSE of 65847574870.090012
Epoch 29: training loss 64052102264.471
Test Loss of 60759104029.490631, Test MSE of 60759102616.840538
Epoch 30: training loss 61042807823.059
Test Loss of 56249674661.751564, Test MSE of 56249675886.998375
Epoch 31: training loss 57572168929.882
Test Loss of 54257455274.074486, Test MSE of 54257455015.132133
Epoch 32: training loss 54441001968.941
Test Loss of 49831692419.227386, Test MSE of 49831691854.442551
Epoch 33: training loss 51784891105.882
Test Loss of 52067821184.266479, Test MSE of 52067822765.502594
Epoch 34: training loss 49705285255.529
Test Loss of 49732519945.001160, Test MSE of 49732520537.685257
Epoch 35: training loss 46825945298.824
Test Loss of 45748964800.873466, Test MSE of 45748966097.774086
Epoch 36: training loss 43708525711.059
Test Loss of 44706623920.055519, Test MSE of 44706624384.177338
Epoch 37: training loss 42137302400.000
Test Loss of 44612883714.901688, Test MSE of 44612884764.388199
Epoch 38: training loss 39493456489.412
Test Loss of 42055343566.138329, Test MSE of 42055343073.749870
Epoch 39: training loss 37425500265.412
Test Loss of 40610772305.069626, Test MSE of 40610771721.627831
Epoch 40: training loss 35081661131.294
Test Loss of 40040525007.263474, Test MSE of 40040525436.074387
Epoch 41: training loss 33611846354.824
Test Loss of 37011050658.968307, Test MSE of 37011050840.127632
Epoch 42: training loss 32214806384.941
Test Loss of 34595981964.820724, Test MSE of 34595983027.308296
Epoch 43: training loss 29826539527.529
Test Loss of 36187425217.110336, Test MSE of 36187426216.970039
Epoch 44: training loss 28759075501.176
Test Loss of 33568247982.338192, Test MSE of 33568248100.116291
Epoch 45: training loss 27633329031.529
Test Loss of 35292109832.764282, Test MSE of 35292108918.535019
Epoch 46: training loss 26073333353.412
Test Loss of 35736602246.898911, Test MSE of 35736601090.685577
Epoch 47: training loss 24450616365.176
Test Loss of 32114791546.936848, Test MSE of 32114791552.367924
Epoch 48: training loss 23606704176.941
Test Loss of 31047505628.173027, Test MSE of 31047505185.764332
Epoch 49: training loss 22460289231.059
Test Loss of 30407629348.123062, Test MSE of 30407629119.259060
Epoch 50: training loss 21769945001.412
Test Loss of 29250206656.281284, Test MSE of 29250207335.230396
Epoch 51: training loss 20643605760.000
Test Loss of 28505168015.071014, Test MSE of 28505168111.404774
Epoch 52: training loss 20201235629.176
Test Loss of 29823969048.812397, Test MSE of 29823968571.966381
Epoch 53: training loss 18948662787.765
Test Loss of 32501333446.558407, Test MSE of 32501333643.325302
Epoch 54: training loss 18292149556.706
Test Loss of 26569476059.758499, Test MSE of 26569475711.625935
Epoch 55: training loss 17454171478.588
Test Loss of 27737614327.472588, Test MSE of 27737614462.628925
Epoch 56: training loss 16972117357.176
Test Loss of 27190000411.891743, Test MSE of 27189998971.203663
Epoch 57: training loss 16312384741.647
Test Loss of 27424063796.408051, Test MSE of 27424064529.895683
Epoch 58: training loss 15974611851.294
Test Loss of 29413326335.881565, Test MSE of 29413326731.130852
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 28516530578.08175, 'MSE - std': 896796153.0491028, 'R2 - mean': 0.7874652533978781, 'R2 - std': 0.00254312208399754} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003764 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17290, number of used features: 89
[LightGBM] [Info] Start training from score 538854.447600
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.46142e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 421927164747.294
Test Loss of 447258970084.049011, Test MSE of 447258966288.120789
Epoch 2: training loss 421905713392.941
Test Loss of 447240045749.207520, Test MSE of 447240052559.299377
Epoch 3: training loss 421878191887.059
Test Loss of 447215351077.958801, Test MSE of 447215347730.506714
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 421896123813.647
Test Loss of 447225664485.707153, Test MSE of 447225667807.660828
Epoch 2: training loss 421888170586.353
Test Loss of 447225432722.031921, Test MSE of 447225435824.155151
Epoch 3: training loss 421887689185.882
Test Loss of 447225367110.232727, Test MSE of 447225367536.627136
Epoch 4: training loss 416850343815.529
Test Loss of 430805322795.347656, Test MSE of 430805323559.056458
Epoch 5: training loss 381028292608.000
Test Loss of 375928328674.272522, Test MSE of 375928326056.835938
Epoch 6: training loss 313214118851.765
Test Loss of 297415241677.783020, Test MSE of 297415237880.229919
Epoch 7: training loss 221930265539.765
Test Loss of 196401640408.442291, Test MSE of 196401639317.804474
Epoch 8: training loss 156690370831.059
Test Loss of 154196062343.491089, Test MSE of 154196061445.671631
Epoch 9: training loss 135547952730.353
Test Loss of 141900631477.503571, Test MSE of 141900631312.986115
Epoch 10: training loss 132098964389.647
Test Loss of 136972847561.164001, Test MSE of 136972848933.167877
Epoch 11: training loss 128172568003.765
Test Loss of 133737821239.191299, Test MSE of 133737825200.745514
Epoch 12: training loss 124966580374.588
Test Loss of 130448325609.970856, Test MSE of 130448325081.263306
Epoch 13: training loss 122012264297.412
Test Loss of 127199170735.285675, Test MSE of 127199173521.564774
Epoch 14: training loss 116924246377.412
Test Loss of 123300448099.190384, Test MSE of 123300452179.241135
Epoch 15: training loss 114345281295.059
Test Loss of 120321513259.288452, Test MSE of 120321514170.465103
Epoch 16: training loss 111344782366.118
Test Loss of 115451360750.826736, Test MSE of 115451362065.909180
Epoch 17: training loss 107764524664.471
Test Loss of 111330245761.569275, Test MSE of 111330245469.038101
Epoch 18: training loss 101615186974.118
Test Loss of 108205444957.979187, Test MSE of 108205443975.462524
Epoch 19: training loss 98430126351.059
Test Loss of 104849337984.266479, Test MSE of 104849340264.199127
Epoch 20: training loss 95718358558.118
Test Loss of 101260053221.411057, Test MSE of 101260054412.606689
Epoch 21: training loss 91453904052.706
Test Loss of 94695989990.832291, Test MSE of 94695990251.248550
Epoch 22: training loss 87716811038.118
Test Loss of 92245867262.993286, Test MSE of 92245867641.569229
Epoch 23: training loss 84243127672.471
Test Loss of 88604349504.666199, Test MSE of 88604350376.701004
Epoch 24: training loss 80267311254.588
Test Loss of 86390852560.388626, Test MSE of 86390853760.703156
Epoch 25: training loss 77379608199.529
Test Loss of 80604174043.699280, Test MSE of 80604174013.395798
Epoch 26: training loss 73939215028.706
Test Loss of 78479434677.385147, Test MSE of 78479434555.589203
Epoch 27: training loss 69827308483.765
Test Loss of 74053078354.964615, Test MSE of 74053076626.093933
Epoch 28: training loss 66754210499.765
Test Loss of 71471822936.590332, Test MSE of 71471823593.015198
Epoch 29: training loss 62975843855.059
Test Loss of 67504397017.093681, Test MSE of 67504398394.234329
Epoch 30: training loss 60680563471.059
Test Loss of 63715070252.354385, Test MSE of 63715070029.351364
Epoch 31: training loss 58290417332.706
Test Loss of 61803850003.245895, Test MSE of 61803849912.977631
Epoch 32: training loss 55074227794.824
Test Loss of 57308562429.394402, Test MSE of 57308563147.523895
Epoch 33: training loss 51846039770.353
Test Loss of 57112900837.529495, Test MSE of 57112900399.110771
Epoch 34: training loss 49542054279.529
Test Loss of 51987146271.148743, Test MSE of 51987146203.127449
Epoch 35: training loss 47722259297.882
Test Loss of 49781272692.541290, Test MSE of 49781272593.664108
Epoch 36: training loss 44438300295.529
Test Loss of 48160812140.724495, Test MSE of 48160811371.953949
Epoch 37: training loss 42047191785.412
Test Loss of 47391640800.318298, Test MSE of 47391641001.943077
Epoch 38: training loss 39807945321.412
Test Loss of 43265810525.801529, Test MSE of 43265810189.911674
Epoch 39: training loss 37601810974.118
Test Loss of 42366420584.105484, Test MSE of 42366420741.354652
Epoch 40: training loss 36297175680.000
Test Loss of 44349258802.927597, Test MSE of 44349258569.988602
Epoch 41: training loss 34197286437.647
Test Loss of 36552673550.508446, Test MSE of 36552673747.013329
Epoch 42: training loss 32270967905.882
Test Loss of 37740476981.177887, Test MSE of 37740477317.079361
Epoch 43: training loss 31412848120.471
Test Loss of 37035022496.599586, Test MSE of 37035022461.629898
Epoch 44: training loss 29980708999.529
Test Loss of 33548822800.403423, Test MSE of 33548822897.639538
Epoch 45: training loss 28060277248.000
Test Loss of 30728702628.034237, Test MSE of 30728702496.972061
Epoch 46: training loss 27051606896.941
Test Loss of 30794741013.851494, Test MSE of 30794741318.867958
Epoch 47: training loss 25778213575.529
Test Loss of 30943823577.804302, Test MSE of 30943823836.721375
Epoch 48: training loss 24721448071.529
Test Loss of 32139893820.402500, Test MSE of 32139893799.767181
Epoch 49: training loss 23901761761.882
Test Loss of 30003959794.261391, Test MSE of 30003960135.477627
Epoch 50: training loss 22740789955.765
Test Loss of 25957839199.992599, Test MSE of 25957839520.988724
Epoch 51: training loss 21885906319.059
Test Loss of 26432017381.233402, Test MSE of 26432017353.061878
Epoch 52: training loss 21123267218.824
Test Loss of 25188027989.155678, Test MSE of 25188027549.806591
Epoch 53: training loss 20358054336.000
Test Loss of 24823638295.509602, Test MSE of 24823638655.119152
Epoch 54: training loss 19681404178.824
Test Loss of 24214818049.954197, Test MSE of 24214817795.634331
Epoch 55: training loss 18963168154.353
Test Loss of 23639807175.209808, Test MSE of 23639807389.085922
Epoch 56: training loss 18166963056.941
Test Loss of 25421649782.613926, Test MSE of 25421650365.973598
Epoch 57: training loss 17736653349.647
Test Loss of 22321790794.081886, Test MSE of 22321790884.570686
Epoch 58: training loss 17100216357.647
Test Loss of 23138331061.266712, Test MSE of 23138331692.981964
Epoch 59: training loss 16521556449.882
Test Loss of 22264398046.186443, Test MSE of 22264397922.518417
Epoch 60: training loss 16041206415.059
Test Loss of 23728269669.203793, Test MSE of 23728269819.831554
Epoch 61: training loss 15378315373.176
Test Loss of 22258803799.879715, Test MSE of 22258803580.211330
Epoch 62: training loss 15088300043.294
Test Loss of 22542338281.082581, Test MSE of 22542337768.476357
Epoch 63: training loss 14877832286.118
Test Loss of 23758764199.232014, Test MSE of 23758764042.742031
Epoch 64: training loss 14230579369.412
Test Loss of 20282163989.733055, Test MSE of 20282164119.767849
Epoch 65: training loss 13849663683.765
Test Loss of 22122687120.610687, Test MSE of 22122686751.281517
Epoch 66: training loss 13776968090.353
Test Loss of 21212078127.374508, Test MSE of 21212077798.731281
Epoch 67: training loss 13190375081.412
Test Loss of 20368371648.044415, Test MSE of 20368371504.534454
Epoch 68: training loss 12667125263.059
Test Loss of 21486348261.944019, Test MSE of 21486348276.265079
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 26173136477.476192, 'MSE - std': 3393987926.945964, 'R2 - mean': 0.8106324347648011, 'R2 - std': 0.032829076008906236} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002440 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2529
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 90
[LightGBM] [Info] Start training from score 540570.395813
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.33122e+10
Model Interpreting...
[(20,), (20,), (20,), (20,), (20,)]

Train embedding model...
Epoch 1: training loss 430110988408.471
Test Loss of 410766511110.160095, Test MSE of 410766512192.485168
Epoch 2: training loss 430090759710.118
Test Loss of 410748329631.452087, Test MSE of 410748330567.071899
Epoch 3: training loss 430063558896.941
Test Loss of 410724353645.223511, Test MSE of 410724350910.330872
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 430078550497.882
Test Loss of 410727388332.009277, Test MSE of 410727385250.229431
Epoch 2: training loss 430067717059.765
Test Loss of 410728540594.998596, Test MSE of 410728538640.505005
Epoch 3: training loss 430067305050.353
Test Loss of 410727592915.931519, Test MSE of 410727592823.826355
Epoch 4: training loss 424902147132.235
Test Loss of 394439994658.472900, Test MSE of 394439993609.427856
Epoch 5: training loss 388566779060.706
Test Loss of 340030471047.167053, Test MSE of 340030470095.465332
Epoch 6: training loss 320673481547.294
Test Loss of 262941240345.114288, Test MSE of 262941240486.771912
Epoch 7: training loss 229071156826.353
Test Loss of 164891949648.318359, Test MSE of 164891948838.316162
Epoch 8: training loss 164668991819.294
Test Loss of 125419593451.268860, Test MSE of 125419593253.035812
Epoch 9: training loss 144365493699.765
Test Loss of 114798880050.583984, Test MSE of 114798879040.023788
Epoch 10: training loss 139579739136.000
Test Loss of 110530273026.013885, Test MSE of 110530271086.771667
Epoch 11: training loss 135771646343.529
Test Loss of 107369456715.342896, Test MSE of 107369457187.799545
Epoch 12: training loss 132569509345.882
Test Loss of 105230730664.099960, Test MSE of 105230730363.233200
Epoch 13: training loss 129079988193.882
Test Loss of 101588676079.178162, Test MSE of 101588675856.424561
Epoch 14: training loss 126610206057.412
Test Loss of 98258694506.024994, Test MSE of 98258695683.525406
Epoch 15: training loss 121759239951.059
Test Loss of 96072877991.863022, Test MSE of 96072879322.279663
Epoch 16: training loss 117493222068.706
Test Loss of 92724097977.395645, Test MSE of 92724098689.853668
Epoch 17: training loss 114021951518.118
Test Loss of 90403757246.015732, Test MSE of 90403757171.524872
Epoch 18: training loss 110639268894.118
Test Loss of 87214952722.835724, Test MSE of 87214951781.094376
Epoch 19: training loss 106286122767.059
Test Loss of 83041089832.633041, Test MSE of 83041090291.650223
Epoch 20: training loss 102819578895.059
Test Loss of 81347010491.764923, Test MSE of 81347011860.508942
Epoch 21: training loss 98083226262.588
Test Loss of 77704082342.441467, Test MSE of 77704080861.156387
Epoch 22: training loss 95075513193.412
Test Loss of 74544742494.770935, Test MSE of 74544742102.537338
Epoch 23: training loss 90576240865.882
Test Loss of 73997224932.042572, Test MSE of 73997226660.023575
Epoch 24: training loss 86944289731.765
Test Loss of 68650354031.237389, Test MSE of 68650351921.871933
Epoch 25: training loss 84202509613.176
Test Loss of 66878554502.456268, Test MSE of 66878555622.534660
Epoch 26: training loss 80275682665.412
Test Loss of 64589144249.751038, Test MSE of 64589144637.569351
Epoch 27: training loss 76943652246.588
Test Loss of 60214581438.489586, Test MSE of 60214582995.593811
Epoch 28: training loss 72697401961.412
Test Loss of 60003932426.306343, Test MSE of 60003931476.164703
Epoch 29: training loss 70835766302.118
Test Loss of 56358849887.126328, Test MSE of 56358850023.180298
Epoch 30: training loss 67072308329.412
Test Loss of 53366338834.361870, Test MSE of 53366338712.361046
Epoch 31: training loss 63499803617.882
Test Loss of 50029228924.742249, Test MSE of 50029229288.176758
Epoch 32: training loss 61036264914.824
Test Loss of 47801236167.729752, Test MSE of 47801235568.072159
Epoch 33: training loss 58420429108.706
Test Loss of 49207872298.291534, Test MSE of 49207872521.380188
Epoch 34: training loss 55488486580.706
Test Loss of 47153624387.642761, Test MSE of 47153624240.827667
Epoch 35: training loss 53023586921.412
Test Loss of 45255448897.273483, Test MSE of 45255448789.929863
Epoch 36: training loss 50377603019.294
Test Loss of 42238100174.363724, Test MSE of 42238099337.601730
Epoch 37: training loss 47227173707.294
Test Loss of 39728949715.220734, Test MSE of 39728950032.339256
Epoch 38: training loss 45648565609.412
Test Loss of 38707412840.366493, Test MSE of 38707413301.579231
Epoch 39: training loss 43307804536.471
Test Loss of 33960515378.347061, Test MSE of 33960515794.162411
Epoch 40: training loss 41089500935.529
Test Loss of 34695723461.952797, Test MSE of 34695724071.460365
Epoch 41: training loss 39161885236.706
Test Loss of 32817879848.396114, Test MSE of 32817880430.942844
Epoch 42: training loss 37586020269.176
Test Loss of 31713609086.400742, Test MSE of 31713608999.412930
Epoch 43: training loss 35767559122.824
Test Loss of 31061623629.356781, Test MSE of 31061624367.452911
Epoch 44: training loss 34149283779.765
Test Loss of 27500322264.433132, Test MSE of 27500322019.586658
Epoch 45: training loss 32169132024.471
Test Loss of 28162731548.668209, Test MSE of 28162731147.679863
Epoch 46: training loss 31001254904.471
Test Loss of 28362337696.518280, Test MSE of 28362337440.658745
Epoch 47: training loss 29664311815.529
Test Loss of 26776001991.374363, Test MSE of 26776002019.258488
Epoch 48: training loss 28214673479.529
Test Loss of 24959996200.633041, Test MSE of 24959996437.886433
Epoch 49: training loss 27204038915.765
Test Loss of 25283588444.757057, Test MSE of 25283589040.311260
Epoch 50: training loss 26175340566.588
Test Loss of 22209743536.037022, Test MSE of 22209743162.780312
Epoch 51: training loss 25118994691.765
Test Loss of 24033752931.627949, Test MSE of 24033753858.760765
Epoch 52: training loss 23714054671.059
Test Loss of 24558546778.150856, Test MSE of 24558546566.432510
Epoch 53: training loss 23007462501.647
Test Loss of 23221858658.443314, Test MSE of 23221858540.287369
Epoch 54: training loss 22355118546.824
Test Loss of 22874436165.893566, Test MSE of 22874435687.888390
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 25348461280.07924, 'MSE - std': 3267970789.311973, 'R2 - mean': 0.8107756663254084, 'R2 - std': 0.028431896165025106} 
 

In get_device
{'task': 'regression', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 500, 'early_stopping': 3, 'loss_de': 3, 'loss_dr': 0.9, 'device': device(type='cuda'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 100, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
X type b4: float64
X type after: float64
Preprocess data for CatNN...
X type b4: float64
X type after: float64
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005313 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 2525
[LightGBM] [Info] Number of data points in the train set: 17291, number of used features: 89
[LightGBM] [Info] Start training from score 538701.205598
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[84]	valid_0's l2: 1.64622e+10
Model Interpreting...
[(17,), (17,), (17,), (17,), (16,)]

Train embedding model...
Epoch 1: training loss 424043706247.529
Test Loss of 431612749614.556213, Test MSE of 431612749328.500000
Epoch 2: training loss 424023619222.588
Test Loss of 431592131961.662170, Test MSE of 431592128012.413818
Epoch 3: training loss 423995980980.706
Test Loss of 431564282497.599243, Test MSE of 431564276210.567444
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 424010570571.294
Test Loss of 431567960807.951904, Test MSE of 431567959785.174377
Epoch 2: training loss 423998482552.471
Test Loss of 431569730279.951904, Test MSE of 431569726269.527405
Epoch 3: training loss 423997927544.471
Test Loss of 431570285698.310059, Test MSE of 431570291699.634216
Epoch 4: training loss 418267840030.118
Test Loss of 413523657030.012024, Test MSE of 413523665220.230896
Epoch 5: training loss 381165348743.529
Test Loss of 357255235272.203613, Test MSE of 357255232420.912048
Epoch 6: training loss 313529192749.176
Test Loss of 278920663917.578918, Test MSE of 278920658673.644653
Epoch 7: training loss 222491187561.412
Test Loss of 179643300189.704773, Test MSE of 179643296365.318024
Epoch 8: training loss 159407077767.529
Test Loss of 137441821714.480347, Test MSE of 137441818748.311371
Epoch 9: training loss 140013416116.706
Test Loss of 125381638304.162888, Test MSE of 125381640291.875427
Epoch 10: training loss 134616429778.824
Test Loss of 120656927896.107361, Test MSE of 120656927336.199707
Epoch 11: training loss 131575034970.353
Test Loss of 117567283389.541885, Test MSE of 117567282014.491364
Epoch 12: training loss 128888866906.353
Test Loss of 115000791887.252197, Test MSE of 115000790872.805420
Epoch 13: training loss 124743914405.647
Test Loss of 111312208235.920410, Test MSE of 111312208264.522095
Epoch 14: training loss 120635568519.529
Test Loss of 108096188148.745956, Test MSE of 108096186654.939407
Epoch 15: training loss 117842761968.941
Test Loss of 104321167979.801941, Test MSE of 104321170139.468704
Epoch 16: training loss 113412641295.059
Test Loss of 99187457123.035629, Test MSE of 99187458269.872162
Epoch 17: training loss 109834462599.529
Test Loss of 95887516425.121704, Test MSE of 95887518283.951965
Epoch 18: training loss 105230054896.941
Test Loss of 92018153556.346130, Test MSE of 92018154131.952164
Epoch 19: training loss 102044870023.529
Test Loss of 88623921107.457657, Test MSE of 88623919845.390503
Epoch 20: training loss 97890670245.647
Test Loss of 83169006983.877838, Test MSE of 83169006742.565918
Epoch 21: training loss 93030579983.059
Test Loss of 85040732869.834335, Test MSE of 85040731787.058517
Epoch 22: training loss 90971155877.647
Test Loss of 78352801113.440079, Test MSE of 78352802411.047028
Epoch 23: training loss 86554672353.882
Test Loss of 74346707136.385010, Test MSE of 74346707118.018539
Epoch 24: training loss 83227734091.294
Test Loss of 72033916559.341049, Test MSE of 72033915137.106628
Epoch 25: training loss 79387886697.412
Test Loss of 66947044290.872742, Test MSE of 66947043904.913376
Epoch 26: training loss 75089352192.000
Test Loss of 63390572758.182320, Test MSE of 63390572455.479729
Epoch 27: training loss 73524365025.882
Test Loss of 58287956002.117538, Test MSE of 58287957212.012016
Epoch 28: training loss 68833111868.235
Test Loss of 55654837834.158257, Test MSE of 55654838538.825981
Epoch 29: training loss 65897006095.059
Test Loss of 54887144950.759834, Test MSE of 54887144720.615601
Epoch 30: training loss 62917036092.235
Test Loss of 53172976955.113373, Test MSE of 53172976545.479324
Epoch 31: training loss 60185572321.882
Test Loss of 49551216706.339661, Test MSE of 49551216475.327843
Epoch 32: training loss 57088975375.059
Test Loss of 47574040702.993057, Test MSE of 47574040367.635796
Epoch 33: training loss 54119179090.824
Test Loss of 43642504487.211479, Test MSE of 43642504325.463806
Epoch 34: training loss 51577497916.235
Test Loss of 41688825687.307732, Test MSE of 41688825012.260490
Epoch 35: training loss 48879623996.235
Test Loss of 42143163711.851921, Test MSE of 42143164305.711044
Epoch 36: training loss 46397681355.294
Test Loss of 35366899934.711708, Test MSE of 35366900893.777306
Epoch 37: training loss 44208009656.471
Test Loss of 37057465977.543732, Test MSE of 37057465993.871590
Epoch 38: training loss 42078006678.588
Test Loss of 35676830072.240631, Test MSE of 35676830488.454842
Epoch 39: training loss 39694452227.765
Test Loss of 31447414678.804256, Test MSE of 31447414936.985401
Epoch 40: training loss 38129943710.118
Test Loss of 33164605674.084221, Test MSE of 33164606016.162418
Epoch 41: training loss 35806743070.118
Test Loss of 32339444186.802406, Test MSE of 32339444417.025120
Epoch 42: training loss 34856248071.529
Test Loss of 31011579784.588615, Test MSE of 31011579358.309017
Epoch 43: training loss 32931794940.235
Test Loss of 30754792006.841278, Test MSE of 30754791590.269550
Epoch 44: training loss 31638064880.941
Test Loss of 28010551418.254513, Test MSE of 28010551233.211430
Epoch 45: training loss 30243315425.882
Test Loss of 27360632810.202682, Test MSE of 27360632900.113041
Epoch 46: training loss 29124983130.353
Test Loss of 27957481374.385933, Test MSE of 27957481222.408867
Epoch 47: training loss 27662770247.529
Test Loss of 25034162005.412308, Test MSE of 25034162327.774548
Epoch 48: training loss 26026758953.412
Test Loss of 25434229441.095791, Test MSE of 25434229391.252518
Epoch 49: training loss 25680517914.353
Test Loss of 26276264119.855621, Test MSE of 26276264619.543324
Epoch 50: training loss 24355035949.176
Test Loss of 24041582091.609440, Test MSE of 24041581946.883579
Epoch 51: training loss 23292443339.294
Test Loss of 22432473507.835262, Test MSE of 22432473452.875317
Epoch 52: training loss 22494494479.059
Test Loss of 25232192018.717262, Test MSE of 25232191933.259853
Epoch 53: training loss 21743406317.176
Test Loss of 21800834060.794075, Test MSE of 21800833590.548038
Epoch 54: training loss 21119851599.059
Test Loss of 22686833284.442387, Test MSE of 22686833315.369678
Epoch 55: training loss 20221390528.000
Test Loss of 22764629013.323460, Test MSE of 22764629176.445232
Epoch 56: training loss 19438592224.000
Test Loss of 21981772488.677464, Test MSE of 21981772484.017796
Epoch 57: training loss 18843359198.118
Test Loss of 21695107709.808422, Test MSE of 21695107828.315117
Epoch 58: training loss 18310043173.647
Test Loss of 20737685492.627487, Test MSE of 20737685259.586056
Epoch 59: training loss 17955575638.588
Test Loss of 21401009722.047199, Test MSE of 21401009537.072834
Epoch 60: training loss 17330418725.647
Test Loss of 22615011722.247108, Test MSE of 22615011878.638233
Epoch 61: training loss 17021871612.235
Test Loss of 21067896072.884777, Test MSE of 21067895905.898380
Epoch 62: training loss 16704129347.765
Test Loss of 20939652490.720963, Test MSE of 20939652517.294056
Early stopping applies!
Finished Training
X type b4: float64
X type after: float64

################### Make predictions #######################################

Saved Losses
{'MSE - mean': 24466699527.5222, 'MSE - std': 3413754768.129499, 'R2 - mean': 0.8173449125742046, 'R2 - std': 0.028623734220752914} 
 

Saving model.....
Results After CV: {'MSE - mean': 24466699527.5222, 'MSE - std': 3413754768.129499, 'R2 - mean': 0.8173449125742046, 'R2 - std': 0.028623734220752914}
Train time: 91.56853106079943
Inference time: 0.07287758919992485
Finished cross validation
